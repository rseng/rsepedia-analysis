---
authors: M. Torrent
---

# How to install ABINIT on macOS

This file describes how to install ABINIT on macOS using one of the following approaches:

 - Homebrew package manager
 - MacPorts package manager
 - Compilation from source
 - Example : MacPorts + ABINIT source

## Using [homebrew](http://brew.sh)

Tested with:

1. macOS 10.13 (High Sierra)
2. macOS 10.14 (Mojave)
3. macOS 10.15 (Catalina)

A Homebrew official formula for ABINIT is available in this [github repository](https://github.com/abinit/homebrew-tap).

### Prerequisites

- Homebrew installed (see: <http://brew.sh/#install>)

### Installing ABINIT

Before the first installation, type:

    brew tap abinit/tap

To install ABINIT just type

    brew install abinit

and ABINIT should install smoothly with all its dependencies.

Note that:

* the LibXC and netCDF fallbacks are enabled by default.
  Wannier90 and BigDFT are not available in Homebrew.
  AtomPAW can be installed as a separate formula.

* the following extra options are available for the ABINIT formula:

      * --with-testsuite    --> Run full test suite (time consuming)
      * --without-check     --> Skip build-time tests (not recommended)
      * --without-openmp    --> Disable openMP multithreading
      * --without-netcdf    --> Build without netcdf support
      * --without-libxc     --> Build without libXC support
      * --without-fftw      --> Build without fftw support
      * --without-scalapack --> Build without scalapack support

## Using [macports](http://www.macports.org)

ABINIT is available on the MacPorts project, but it is not necessarily the latest version.
The procedure has been tested with Mac OS X v10.8 (Mountain Lion) and v10.15 (Catalina)

### Prerequisites:

1. MacPorts installed (see <https://www.macports.org/install.php>)

2. Some basic ports already installed:

    1. gcc (last version) with Fortran variant (Fortran compiler),
    2. mpich or openmpi (MPI)

3. Before starting, it is preferable to update the MacPorts system:

        sudo port selfupdate
        sudo port upgrade outdated

### Installing ABINIT

To install ABINIT, just type:

    sudo port install abinit

### ABINIT port variants

By default, ABINIT is installed with libXC and Wannier90

To activate support for the FFTW3 library:

     sudo port install abinit @X.Y.Z +fftw3

where X.Y.X is to be replaced by the version of ABINIT that has been installed (you might skip X.Y.Z if there is only one version installed).

To link ABINIT with the parallel Linear Algebra ScaLapack:

     sudo port install abinit @X.Y.Z +scalapack

To install a multi-threaded (openMP) version of ABINIT:

     sudo port install abinit @X.Y.Z +threads

It is possible to mix all previous variants:

    sudo port install abinit @X.Y.Z +fftw3+threads+scalapack

Other options available, see:

    port info abinit

## Compiling from source under macOS

### Prerequisites

1. macOS

2. Xcode installed with "Xcode command line tools"; just type:

        xcode-select --install

3. A Fortran compiler installed. Possible options:

      - [gfortran-for-macOS project](https://github.com/fxcoudert/gfortran-for-macOS/releases).
      - gfortran binary from [http://hpc.sourceforge.net](http://hpc.sourceforge.net)
      - gfortran binary from [https://gcc.gnu.org/wiki/GFortranBinaries#MacOS](https://gcc.gnu.org/wiki/GFortranBinaries#MacOS)
      - gfortran installed via a package manager (MacPorts, Homebrew, Fink)
      - intel Fortran compiler

4. Mandatory libraries.

      - [HDF5](https://www.hdfgroup.org/solutions/hdf5/)
        (High-performance data management and storage suite)

      - [NetCDF](https://www.unidata.ucar.edu/software/netcdf/)
        (Network Common Data Form) 
      - [libXC](https://tddft.org/programs/libxc/download/) (library of exchange-correlation functionals)

5. A MPI library installed (if you want to benefit from parallelism; recommended).
   Possible options:

      - mpich from [http://www.mpich.org](http://www.mpich.org), or via package manager
      - open-mpi from [http://www.open-mpi.org](http://www.open-mpi.org), or via package manager

6. A Linear Algebra library installed.
   By default the `accelerate` Framework is installed on macOS and the ABINIT build system should find it.
   But you might want to install a parallel library: `scalapack`, `atlas`, `mkl`, etc.
   If ABINIT is linked with the accelerate library, make sure the code is configured with
   `--enable-zdot-bugfix="yes"`. 
   Use `otool -L abinit` to print the shared libraries required by the program.

### Installing ABINIT from source

For normal users it is advised to get the newest version from our website (replace 9.4.2 by the newest version available).

    wget https://www.abinit.org/sites/default/files/packages/abinit-9.4.2.tar.gz
    tar xzf abinit-9.4.2.tar.gz
    cd abinit-9.4.2

Create a working directory:

    mkdir build && cd build

To configure the sequential version, use:

    ../configure FC=gfortran CC=clang FCFLAGS_EXTRA="-ffree-line-length-none"

For the parallel version (only if MPI installed):

    ../configure FC=mpif90 CC=mpicc FCFLAGS_EXTRA="-ffree-line-length-none" \
                 --enable-mpi --enable-mpi-io

For the GNU compiler version 10 or beyond, "-ffree-line-length-none" needs to be replaced
by "-ffree-line-length-none -fallow-argument-mismatch".

Compile with:

    make -j4

Install (optional):

    make install

Remember that on MacOs, the environment variant LD_LIBRARY_PATH should be replaced by DYLD_LIBRARY_PATH.

## Example : [macports](http://www.macports.org) + ABINIT source

We give an example of using ABINIT sources (to get the latest ABINIT version) combined with the ease of installation of Macports.
Indeed, the libraries (mandatory or optional) used by ABINIT are available from the MacPorts project.
The procedure has been tested with Mac OS X v11.5 (Big Sur) 

Start with the prerequisite of the macports approach, as explained in the section [using macports](#using-macports). Then,
install the libraries needed by ABINIT. There are different choices for the linear algebra and mpi,
as mentioned in the section [Compiling from source](#compiling-from-source-under-macos). Focusing on the choice
of OpenBLAS and openmpi, and supposing that GNU compiler version 11 is to be used, one might issue 

     sudo port install gcc11
     sudo port install OpenBLAS +gcc11+fortran
     sudo port install openmpi-gcc11 +gfortran
     sudo port install fftw-3 +gfortran
     sudo port install fftw-3-single +gfortran
     sudo port install fftw-3-long +gfortran
     sudo port install hdf5 +cxx+gcc11+hl+openmpi
     sudo port install netcdf +cdf5+dap+x+gcc11+netcdf4+openmpi 
     sudo port install netcdf-fortran +gcc11+openmpi 
     sudo port install libxc4 +gcc11

and optionally

     sudo port install wannier90 +accelerate+gcc11
     sudo port install atompaw +accelerate+gcc11+libxc

After this step, one can follow the steps described in the subsection "Installing ABINIT from source" of
[Compiling from source](#compiling-from-source-under-macos).

## Troubleshooting

When switching from GNU compiler version 10 to GNU compiler version 11 on the same machine, with MacOs Big Sur v11.5, it has been seen that Xcode needs to be reinstalled.
Indeed, at the level of the abinit compilation, the system library (-lSystem) was not found, which was quite hard to debug.
Actually, the reinstallation of Xcode might be good practice to solve other problems.
To do this, use

    sudo rm -rf /Library/Developer/CommandLineTools
    sudo xcode-select --install

You might have to accept the licence on the screen. Also, possibly first using
 
    sudo xcodebuild -license


## Comments

To benefit from the optional "fallbacks" (`Wannier90`, `libPSML`, ...),
consult the [abinit-fallbacks Project](https://gitlab.abinit.org/buildbot/abinit-fallbacks)
---
authors: M. Giantomassi
---

# How to install ABINIT with EasyBuild

This page describes how to compile and install ABINIT with [EasyBuild](https://github.com/easybuilders/easybuild),
a python framework for managing scientific software on clusters
that allows one to build an entire software stack and the associated modules with a single command.

For further information about EasyBuild, please consult the [official documentation](https://easybuild.readthedocs.io/en/latest/).
We also recommended to read this [tutorial](https://ulhpc-tutorials.readthedocs.io/en/latest/tools/easybuild/) in which
the different steps required to build and install software are discussed in more detail.
For an introduction to Lmod and Environment Modules,
see the [Lmod user guide](https://lmod.readthedocs.io/en/latest/index.html).

Note also that EasyBuild configuration files (*easyconfigs*) for different ABINIT versions and different toolchains
are available on [github](https://github.com/easybuilders/easybuild-easyconfigs/tree/develop/easybuild/easyconfigs/a/ABINIT).
If you are not interested in learning how to use EasyBuild,
feel free to ask your sysadmin to build an ABINIT module using one of the *easyconfigs* already available.
If, on the other hand, you love learning how to use new tools to facilitate your work
and you like to organize your own software stack in a clean way with automatically-generated modules, continue reading.

## Getting started with EasyBuild

First of all, let's use the `module spider` command to check whether an ABINIT module is already installed:

```sh
$ module spider abinit

------------------------------------------------------------------------------------------------------------------------
  ABINIT:
------------------------------------------------------------------------------------------------------------------------
    Description:
      ABINIT is a package whose main program allows one to find the total energy, charge density and electronic
      structure of systems made of electrons and nuclei (molecules and periodic solids) within Density Functional
      Theory (DFT), using pseudopotentials and a planewave or wavelet basis.

     Versions:
        ABINIT/8.2.2-foss-2016b
        ABINIT/8.4.4-intel-2017b
        ABINIT/8.6.3-intel-2018a
        ABINIT/8.10.2-intel-2017b
        ABINIT/8.10.3-intel-2018b

------------------------------------------------------------------------------------------------------------------------
  For detailed information about a specific "ABINIT" package (including how to load the modules) use the module's full name.
  Note that names that have a trailing (E) are extensions provided by other modules.
  For example:

     $ module spider ABINIT/8.10.3-intel-2018b
------------------------------------------------------------------------------------------------------------------------
```

The output shows that there are five ABINIT versions already installed.
The good news is that we can directly load one of these modules with

```sh
$ module load ABINIT/8.10.3-intel-2018b
```

to activate the 8.10.3 executable compiled with the intel toolchain 2018b.
The bad news is that all the available versions are rather old so
we have to use the EasyBuild command line interface to build a more recent version.

To build software with EasyBuild, we need the `eb` python script.
If `eb` is not already in $PATH, you will get the following error message:

```sh
$ which eb
bash: eb: command not found...
```

In this case, issue `module spider EasyBuild` and follow the instructions printed to the terminal
to load the EasyBuild module, e.g.:

```sh
$ module load EasyBuild/4.2.2

$ which eb
/opt/sw/arch/easybuild/software/EasyBuild/4.2.2/bin/eb
```

Now execute `eb` with the `-S` option to search for ABINIT easyconfigs:

```sh
$ eb -S abinit

== found valid index for /usr/easybuild/easyconfigs, so using it...
== found valid index for /usr/easybuild/easyconfigs, so using it...
CFGS1=/usr/easybuild/easyconfigs
CFGS2=/usr/easybuild/easyconfigs
 * $CFGS1/a/ABINIT/ABINIT-8.0.8-intel-2016a.eb
 * $CFGS1/a/ABINIT/ABINIT-8.0.8b-foss-2016b.eb
 * $CFGS1/a/ABINIT/ABINIT-8.0.8b-intel-2016b.eb
 * $CFGS1/a/ABINIT/ABINIT-8.10.2-intel-2018b.eb
 * $CFGS1/a/ABINIT/ABINIT-8.10.3-intel-2018b.eb
 * $CFGS1/a/ABINIT/ABINIT-8.2.2-foss-2016b.eb
 * $CFGS1/a/ABINIT/ABINIT-8.2.2-intel-2016b.eb
 * $CFGS1/a/ABINIT/ABINIT-8.4.4-intel-2017b.eb
 * $CFGS1/a/ABINIT/ABINIT-8.6.3-intel-2018a.eb
 * $CFGS2/a/ABINIT/ABINIT-8.0.8-intel-2016a.eb
 * $CFGS2/a/ABINIT/ABINIT-8.0.8b-foss-2016b.eb
 * $CFGS2/a/ABINIT/ABINIT-8.0.8b-intel-2016b.eb
 * $CFGS2/a/ABINIT/ABINIT-8.10.2-intel-2018b.eb
 * $CFGS2/a/ABINIT/ABINIT-8.10.3-intel-2018b.eb
 * $CFGS2/a/ABINIT/ABINIT-8.2.2-foss-2016b.eb
 * $CFGS2/a/ABINIT/ABINIT-8.2.2-intel-2016b.eb
 * $CFGS2/a/ABINIT/ABINIT-8.4.4-intel-2017b.eb
 * $CFGS2/a/ABINIT/ABINIT-8.6.3-intel-2018a.eb
 * $CFGS2/a/ABINIT/ABINIT-9.0.4-foss-2019b.eb
 * $CFGS2/a/ABINIT/ABINIT-9.0.4-intel-2019b.eb
```

Various *easyconfig* files are found for different ABINIT versions
and different toolchains: `foss 2016b`, `intel 2018a`, etc.
Note that the output may change depending on the version of EasyBuild installed on your machine as
these `eb` files are shipped with the EasyBuild installation.
More recent *easyconfigs* files can be found in the
[github repository](https://github.com/easybuilders/easybuild-easyconfigs/tree/develop/easybuild/easyconfigs/a/ABINIT).

!!! important

    `eb` files typically follow the naming scheme `<name>-<version>[-<toolchain>][<versionsuffix>].eb`.
    The name reflects the fact each easyconfig file is associated to a particular version of the library/application
    and a particular [toolchain](https://easybuild.readthedocs.io/en/latest/Common-toolchains.html).
    For example, `foss` stands for Free and Open Source Software toolchain based on 
    GCC, OpenMPI, OpenBLAS/LAPACK, ScaLAPACK, and FFTW3
    whereas the `intel` toolchain is based on the intel compilers, MKL and intel MPI.

    Applications based on `foss` are relatively easy to build since all the basic building blocks
    (including the GCC compiler suite) can be compiled from source if needed.
    On the contrary, the `intel` toolchain requires licensed software and valid licenses must be available to install and use it.

    It is worth to mention that applications compiled with the `intel` toolchain are usually more performant
    than the `foss` version especially on intel hardware.
    Unfortunately, compiling and linking with the intel toolchain requires a pre-existent licensed installation
    that is usually done by the sysadmin.
    As a consequence, building software with an intel easyconfig is not that easy if the sysadmin hasn't
    already installed the intel toolchain.

We are lucky as our installation kindly provides two configuration files for ABINIT 9.0.4:

```sh
* $CFGS2/a/ABINIT/ABINIT/9.0.4-foss-2019b.eb
* $CFGS2/a/ABINIT/ABINIT/9.0.4-intel-2019b.eb
```

but before discussing the build process, let's have a look at the `Meta Modules` section that lists the
releases available on our machine:

```sh
$ module avail

------------------ Meta Modules ------------------
   releases/elic-2017b        releases/2016b (S,L)
   releases/2016a      (S)    releases/2017b (S)
   releases/2018a (S)         releases/2019b (S,D)    use.own
   releases/2018b (S)         tis/2018.01    (S,L)

<snip>
```

The `(D)` after the name of the module stands for Default.
It means that EasyBuild uses this release by default.

Since we want to build `9.0.4-foss-2019b.eb` and `2019b` is the default release,
we can proceed directly with the next steps.
If you want to build a recipe associated to a different release e.g. `2016a`,
you need to load the associated module before building the code using e.g.:

```sh
module load releases/2016a`
```

Now invoke `eb` with the name of the easyconfig file and the two options `--dry-run` and `--robot`
(or just `-Dr` if you prefer the short version):

```sh
eb ABINIT-9.1.0-foss-2019b.eb --robot --dry-run
```

`--dry-run` tells `eb` to print all the required dependencies **without actually building software**.
We **strongly** suggest to use this option to understand what's going on before performing a full installation.
The `--robot` option tells `eb` to automatically build and install all dependencies
while searching for easyconfigs files in a set of pre-defined directories.
To prepend additional directories to search for eb files (like the current directory $PWD),
use the syntax `--robot-paths=$PWD`.
Multiple directories can be specified using `--robot-paths=PATH1:PATH2`

On my machine, I get the following results:

```sh
eb ABINIT-9.1.0-foss-2019b.eb --robot --dry-run

== temporary log file in case of crash /tmp/eb-ttdRNl/easybuild-hVtLoS.log
== found valid index for /usr/easybuild/easyconfigs, so using it...
Dry run: printing build status of easyconfigs and dependencies
 * [x] /usr/easybuild/easyconfigs/m/M4/M4-1.4.18.eb (module: M4/1.4.18)
 * [x] /usr/easybuild/easyconfigs/b/Bison/Bison-3.3.2.eb (module: Bison/3.3.2)
 * [x] /usr/easybuild/easyconfigs/z/zlib/zlib-1.2.11.eb (module: zlib/1.2.11)
 * [x] /usr/easybuild/easyconfigs/h/help2man/help2man-1.47.4.eb (module: help2man/1.47.4)
 * [x] /usr/easybuild/easyconfigs/f/flex/flex-2.6.4.eb (module: flex/2.6.4)
 * [x] /usr/easybuild/easyconfigs/b/binutils/binutils-2.32.eb (module: binutils/2.32)
 * [x] /usr/easybuild/easyconfigs/g/GCCcore/GCCcore-8.3.0.eb (module: GCCcore/8.3.0)
 * [x] /usr/easybuild/easyconfigs/h/help2man/help2man-1.47.8-GCCcore-8.3.0.eb (module: help2man/1.47.8-GCCcore-8.3.0)
 * [x] /usr/easybuild/easyconfigs/m/M4/M4-1.4.18-GCCcore-8.3.0.eb (module: M4/1.4.18-GCCcore-8.3.0)
 * [x] /usr/easybuild/easyconfigs/z/zlib/zlib-1.2.11-GCCcore-8.3.0.eb (module: zlib/1.2.11-GCCcore-8.3.0)
 * [x] /usr/easybuild/easyconfigs/b/Bison/Bison-3.3.2-GCCcore-8.3.0.eb (module: Bison/3.3.2-GCCcore-8.3.0)
 * [x] /usr/easybuild/easyconfigs/f/flex/flex-2.6.4-GCCcore-8.3.0.eb (module: flex/2.6.4-GCCcore-8.3.0)
 * [x] /usr/easybuild/easyconfigs/b/binutils/binutils-2.32-GCCcore-8.3.0.eb (module: binutils/2.32-GCCcore-8.3.0)
 * [x] /usr/easybuild/easyconfigs/g/GCC/GCC-8.3.0.eb (module: GCC/8.3.0)
 * [x] /usr/easybuild/easyconfigs/s/Szip/Szip-2.1.1-GCCcore-8.3.0.eb (module: Szip/2.1.1-GCCcore-8.3.0)
 * [x] /usr/easybuild/easyconfigs/n/ncurses/ncurses-6.1-GCCcore-8.3.0.eb (module: ncurses/6.1-GCCcore-8.3.0)
 * [x] /usr/easybuild/easyconfigs/e/expat/expat-2.2.7-GCCcore-8.3.0.eb (module: expat/2.2.7-GCCcore-8.3.0)
 * [x] /usr/easybuild/easyconfigs/b/bzip2/bzip2-1.0.8-GCCcore-8.3.0.eb (module: bzip2/1.0.8-GCCcore-8.3.0)
 * [x] /usr/easybuild/easyconfigs/l/libreadline/libreadline-8.0-GCCcore-8.3.0.eb (module: libreadline/8.0-GCCcore-8.3.0)
 * [x] /usr/easybuild/easyconfigs/c/cURL/cURL-7.66.0-GCCcore-8.3.0.eb (module: cURL/7.66.0-GCCcore-8.3.0)
 * [x] /usr/easybuild/easyconfigs/p/pkg-config/pkg-config-0.29.2-GCCcore-8.3.0.eb (module: pkg-config/0.29.2-GCCcore-8.3.0)
 * [x] /usr/easybuild/easyconfigs/l/libtool/libtool-2.4.6-GCCcore-8.3.0.eb (module: libtool/2.4.6-GCCcore-8.3.0)
 * [x] /usr/easybuild/easyconfigs/o/OpenBLAS/OpenBLAS-0.3.7-GCC-8.3.0.eb (module: OpenBLAS/0.3.7-GCC-8.3.0)
 * [x] /usr/easybuild/easyconfigs/c/CMake/CMake-3.15.3-GCCcore-8.3.0.eb (module: CMake/3.15.3-GCCcore-8.3.0)
 * [x] /usr/easybuild/easyconfigs/p/Perl/Perl-5.30.0-GCCcore-8.3.0.eb (module: Perl/5.30.0-GCCcore-8.3.0)
 * [x] /usr/easybuild/easyconfigs/d/Doxygen/Doxygen-1.8.16-GCCcore-8.3.0.eb (module: Doxygen/1.8.16-GCCcore-8.3.0)
 * [x] /usr/easybuild/easyconfigs/a/Autoconf/Autoconf-2.69-GCCcore-8.3.0.eb (module: Autoconf/2.69-GCCcore-8.3.0)
 * [x] /usr/easybuild/easyconfigs/a/Automake/Automake-1.16.1-GCCcore-8.3.0.eb (module: Automake/1.16.1-GCCcore-8.3.0)
 * [ ] /usr/easybuild/easyconfigs/l/libxc/libxc-4.3.4-GCC-8.3.0.eb (module: libxc/4.3.4-GCC-8.3.0)
 * [x] /usr/easybuild/easyconfigs/a/Autotools/Autotools-20180311-GCCcore-8.3.0.eb (module: Autotools/20180311-GCCcore-8.3.0)
 * [x] /usr/easybuild/easyconfigs/n/numactl/numactl-2.0.12-GCCcore-8.3.0.eb (module: numactl/2.0.12-GCCcore-8.3.0)
 * [x] /usr/easybuild/easyconfigs/x/xorg-macros/xorg-macros-1.19.2-GCCcore-8.3.0.eb (module: xorg-macros/1.19.2-GCCcore-8.3.0)
 * [x] /usr/easybuild/easyconfigs/l/libpciaccess/libpciaccess-0.14-GCCcore-8.3.0.eb (module: libpciaccess/0.14-GCCcore-8.3.0)
 * [x] /usr/easybuild/easyconfigs/n/ncurses/ncurses-6.0.eb (module: ncurses/6.0)
 * [x] /usr/easybuild/easyconfigs/g/gettext/gettext-0.19.8.1.eb (module: gettext/0.19.8.1)
 * [x] /usr/easybuild/easyconfigs/x/XZ/XZ-5.2.4-GCCcore-8.3.0.eb (module: XZ/5.2.4-GCCcore-8.3.0)
 * [x] /usr/easybuild/easyconfigs/l/libxml2/libxml2-2.9.9-GCCcore-8.3.0.eb (module: libxml2/2.9.9-GCCcore-8.3.0)
 * [x] /usr/easybuild/easyconfigs/h/hwloc/hwloc-1.11.12-GCCcore-8.3.0.eb (module: hwloc/1.11.12-GCCcore-8.3.0)
 * [x] /usr/easybuild/easyconfigs/o/OpenMPI/OpenMPI-3.1.4-GCC-8.3.0.eb (module: OpenMPI/3.1.4-GCC-8.3.0)
 * [x] /usr/easybuild/easyconfigs/g/gompi/gompi-2019b.eb (module: gompi/2019b)
 * [x] /usr/easybuild/easyconfigs/f/FFTW/FFTW-3.3.8-gompi-2019b.eb (module: FFTW/3.3.8-gompi-2019b)
 * [x] /usr/easybuild/easyconfigs/s/ScaLAPACK/ScaLAPACK-2.0.2-gompi-2019b.eb (module: ScaLAPACK/2.0.2-gompi-2019b)
 * [x] /usr/easybuild/easyconfigs/h/HDF5/HDF5-1.10.5-gompi-2019b.eb (module: HDF5/1.10.5-gompi-2019b)
 * [x] /usr/easybuild/easyconfigs/n/netCDF/netCDF-4.7.1-gompi-2019b.eb (module: netCDF/4.7.1-gompi-2019b)
 * [x] /usr/easybuild/easyconfigs/n/netCDF-Fortran/netCDF-Fortran-4.5.2-gompi-2019b.eb (module: netCDF-Fortran/4.5.2-gompi-2019b)
 * [x] /usr/easybuild/easyconfigs/f/foss/foss-2019b.eb (module: foss/2019b)
 * [ ] /home/users/g/m/gmatteo/try_eb/ABINIT-9.1.0-foss-2019b.eb (module: ABINIT/9.0.4-foss-2019b)
== Temporary log file(s) /tmp/eb-ttdRNl/easybuild-hVtLoS.log* have been removed.
== Temporary directory /tmp/eb-ttdRNl has been removed.
```

The output indicates that most of the dependencies are already installed (**checked boxes**).
Only *libxc/4.3.4-GCC-8.3.0* and *ABINIT-9.1.0-foss-2019b.eb* must be built from source (**unchecked boxes**).

Now we can finally build and install ABINIT v9.1.0 with *foss-2019b*
by executing the same command without `--dry-run`:

```sh
eb ABINIT-9.1.0-foss-2019b.eb --robot

== temporary log file in case of crash /tmp/eb-4IOAjW/easybuild-vBwXik.log
== found valid index for /usr/easybuild/easyconfigs, so using it...
== resolving dependencies ...
== processing EasyBuild easyconfig /usr/easybuild/easyconfigs/l/libxc/libxc-4.3.4-GCC-8.3.0.eb
== building and installing libxc/4.3.4-GCC-8.3.0...
== fetching files...
== creating build dir, resetting environment...
== starting iteration #0 ...
== unpacking...
== patching...
== preparing...
== configuring...
== building...
== testing...
== installing...
== taking care of extensions...
== creating build dir, resetting environment...
== starting iteration #1 ...
== unpacking...
== patching...
== preparing...
== configuring...
== building...
== testing...
== installing...
== taking care of extensions...
== restore after iterating...
== postprocessing...
== sanity checking...
== cleaning up...
== creating module...
== permissions...
== packaging...
== COMPLETED: Installation ended successfully (took 18 min 29 sec)
== Results of the build can be found in the log file(s) /home/ucl/modl/gmatteo/.local/easybuild/software/libxc/4.3.4-GCC-8.3.0/easybuild/easybuild-libxc-4.3.4-20200904.010629.log
== processing EasyBuild easyconfig /home/users/g/m/gmatteo/try_eb/ABINIT-9.1.0-foss-2019b.eb
== building and installing ABINIT/9.0.4-foss-2019b...
== fetching files...
== creating build dir, resetting environment...
== unpacking...
== patching...
== preparing...
== configuring...
== building...
== testing...
== installing...
== taking care of extensions...
== restore after iterating...
== postprocessing...
== sanity checking...
== cleaning up...
== creating module...
== permissions...
== packaging...
== COMPLETED: Installation ended successfully (took 7 min 4 sec)
== Results of the build can be found in the log file(s) /home/ucl/modl/gmatteo/.local/easybuild/software/ABINIT/9.0.4-foss-2019b/easybuild/easybuild-ABINIT-9.0.4-20200904.011333.log
== Build succeeded for 2 out of 2
== Temporary log file(s) /tmp/eb-4IOAjW/easybuild-vBwXik.log* have been removed.
== Temporary directory /tmp/eb-4IOAjW has been removed.
```

As you can see from the log messages, *eb* has compiled *libxc* and finally *abinit*.
Executables, libraries and modules are now installed in our `~/.local/easybuild/` directory.

```sh
$ ls ~/.local/easybuild/
build  ebfiles_repo  modules  software  sources

$ tree  ~/.local/easybuild/modules/
/home/ucl/modl/gmatteo/.local/easybuild/modules/
|-- all
|   |-- ABINIT
|   |   `-- 9.0.4-foss-2019b.lua
|   `-- libxc
|       `-- 4.3.4-GCC-8.3.0.lua
`-- chem
    |-- ABINIT
    |   `-- 9.0.4-foss-2019b.lua -> /home/ucl/modl/gmatteo/.local/easybuild/modules/all/ABINIT/9.0.4-foss-2019b.lua
    `-- libxc
        `-- 4.3.4-GCC-8.3.0.lua -> /home/ucl/modl/gmatteo/.local/easybuild/modules/all/libxc/4.3.4-GCC-8.3.0.lua
```

Since we need to load modules installed in `~/.local/easybuild/modules/all`,
we must add this directory to $MODULEPATH.
With a recent version of EasyBuild, issue:

```sh
module use use.own
```

If the `use.own` module is not available, execute the command:

```sh
module use ~/.local/easybuild/modules/all
```

At this point, we should have our version of ABINIT and the dependencies available as `User Modules`

```sh
$ module avail

------------- User Modules -------------
   ABINIT/9.0.4-foss-2019b    libxc/4.3.4-GCC-8.3.0
```

We can finally load our brand-new ABINIT module with:

```sh
module load ABINIT/9.0.4-foss-2019b
```

and check that the abinit executable is in $PATH

```sh
$ which abinit
~/.local/easybuild/software/ABINIT/9.0.4-foss-2019b/bin/abinit
```

and that we have the correct version:

```sh
$ abinit -v
9.0.4
```

The output of `ldd` shows that the executable is dynamically linked to our version of libxc
whereas *openblas*, *fftw3*, *netcdf*, *hdf5* and MPI are provided by the `2019b` release of `foss` as expected:

```sh
$ ldd `which abinit`

libxc.so.5 => /home/ucl/modl/gmatteo/.local/easybuild/software/libxc/4.3.4-GCC-8.3.0/lib64/libxc.so.5 (0x00002aaaaaccf000)
libfftw3f.so.3 => /opt/sw/arch/easybuild/2019b/software/FFTW/3.3.8-gompi-2019b/lib/libfftw3f.so.3 (0x00002aaaab2d9000)
libfftw3.so.3 => /opt/sw/arch/easybuild/2019b/software/FFTW/3.3.8-gompi-2019b/lib/libfftw3.so.3 (0x00002aaaab5c0000)
libopenblas.so.0 => /opt/sw/arch/easybuild/2019b/software/OpenBLAS/0.3.7-GCC-8.3.0/lib/libopenblas.so.0 (0x00002aaaab856000)
libnetcdff.so.7 => /opt/sw/arch/easybuild/2019b/software/netCDF-Fortran/4.5.2-gompi-2019b/lib/libnetcdff.so.7 (0x00002aaaaaad3000)
libnetcdf.so.15 => /opt/sw/arch/easybuild/2019b/software/netCDF/4.7.1-gompi-2019b/lib64/libnetcdf.so.15 (0x00002aaaaab59000)
libhdf5_hl.so.100 => /opt/sw/arch/easybuild/2019b/software/HDF5/1.10.5-gompi-2019b/lib/libhdf5_hl.so.100 (0x00002aaaaac95000)
libhdf5.so.103 => /opt/sw/arch/easybuild/2019b/software/HDF5/1.10.5-gompi-2019b/lib/libhdf5.so.103 (0x00002aaaac607000)
libmpi.so.40 => /opt/sw/arch/easybuild/2019b/software/OpenMPI/3.1.4-GCC-8.3.0/lib/libmpi.so.40 (0x00002aaaad432000)
```

The binary works out of the box because the ABINIT module is automatically loading all the dependencies we need
and our local installation is prepended to `LD_LIBRARY_PATH` and `PATH`.
To see this, use `module show`:

```sh
module show ABINIT/9.0.4-foss-2019b

whatis("Description: ABINIT is a package whose main program allows one to find the total energy,
 charge density and  electronic structure of systems made of electrons and nuclei (molecules
 and periodic solids) within Density Functional  Theory (DFT), using pseudopotentials and a
 planewave or wavelet basis.")
whatis("Homepage: https://www.abinit.org/")
whatis("URL: https://www.abinit.org/")
conflict("ABINIT")
load("foss/2019b")
load("libxc/4.3.4-GCC-8.3.0")
load("HDF5/1.10.5-gompi-2019b")
load("netCDF/4.7.1-gompi-2019b")
load("netCDF-Fortran/4.5.2-gompi-2019b")
prepend_path("CMAKE_PREFIX_PATH","/home/ucl/modl/gmatteo/.local/easybuild/software/ABINIT/9.0.4-foss-2019b")
prepend_path("LD_LIBRARY_PATH","/home/ucl/modl/gmatteo/.local/easybuild/software/ABINIT/9.0.4-foss-2019b/lib")
prepend_path("LIBRARY_PATH","/home/ucl/modl/gmatteo/.local/easybuild/software/ABINIT/9.0.4-foss-2019b/lib")
prepend_path("PATH","/home/ucl/modl/gmatteo/.local/easybuild/software/ABINIT/9.0.4-foss-2019b/bin")
prepend_path("PKG_CONFIG_PATH","/home/ucl/modl/gmatteo/.local/easybuild/software/ABINIT/9.0.4-foss-2019b/lib/pkgconfig")
setenv("EBROOTABINIT","/home/ucl/modl/gmatteo/.local/easybuild/software/ABINIT/9.0.4-foss-2019b")
setenv("EBVERSIONABINIT","9.0.4")
setenv("EBDEVELABINIT","/home/ucl/modl/gmatteo/.local/easybuild/software/ABINIT/9.0.4-foss-2019b/easybuild/ABINIT-9.0.4-foss
-2019b-easybuild-devel")
```

Q: What happens if you run

```sh
module load releases/2016a
eb ABINIT-9.1.0-foss-2019b.eb -Dr
```

## Using a customized easyconfig

In the previous example we used one of the official easyconfig files shipped with the EasyBuild package.
There are cases, however, in which we need to perform a customized build.
For example, we may want to compile the latest stable version of ABINIT recently released,
or use a different version, or maybe we just need to activate configuration options that are not present in the official file.
In this case, we can simply copy the original easyconfig file,
change it according to our needs and finally pass this customized file to the `eb` script.

First of all, let's have a look at `ABINIT-9.0.4-foss-2019b.eb`:

```python
easyblock = 'ConfigureMake'

name = 'ABINIT'
version = '9.0.4'

homepage = 'https://www.abinit.org/'
description = """ABINIT is a package whose main program allows one to find the total energy,
 charge density and  electronic structure of systems made of electrons and nuclei (molecules
 and periodic solids) within Density Functional  Theory (DFT), using pseudopotentials and a
 planewave or wavelet basis."""

toolchain = {'name': 'foss', 'version': '2019b'}
toolchainopts = {'usempi': True, 'pic': True}

source_urls = ['https://www.abinit.org/sites/default/files/packages/']
sources = [SOURCELOWER_TAR_GZ]
#checksums = ['82e8d071088ab8dc1b3a24380e30b68c544685678314df1213180b449c84ca65']

dependencies = [
    ('libxc', '4.3.4'),
    ('netCDF-Fortran', '4.5.2'),
]

# Ensure MPI.
configopts = '--with-mpi="yes" --enable-openmp="no" '

# BLAS/Lapack
configopts += '--with-linalg-flavor="openblas"  LINALG_LIBS="-L${EBROOTOPENBLAS}/lib -lopenblas" '

# FFTW3 support
configopts += '--with-fft-flavor=fftw3 FFTW3_LIBS="-L${EBROOTFFTW} -lfftw3f -lfftw3" '

# libxc support
configopts += '--with-libxc=${EBROOTLIBXC} '

# hdf5/netcdf4.
configopts += 'with_netcdf="${EBROOTNETCDF}" '
configopts += 'with_netcdf_fortran="${EBROOTNETCDFMINFORTRAN}" '
configopts += 'with_hdf5="${EBROOTHDF5}" '

# make sure --free-line-length-none is added to FCFLAGS
configopts += 'FCFLAGS="${FCFLAGS} --free-line-length-none" '

runtest = 'check'

sanity_check_paths = {
    'files': ['bin/%s' % x for x in ['abinit', 'aim', 'cut3d', 'conducti', 'mrgddb', 'mrgscr', 'optic']],
    'dirs': ['lib/pkgconfig'],
}

moduleclass = 'chem'
```

The meaning of the different easyconfig parameters
is explained [here](https://easybuild.readthedocs.io/en/latest/Writing_easyconfig_files.html).

In most of the cases, a customized build requires changing one of the following entries:

- `version`: string with the version number. Used to download the tarball from `source_urls`
- toolchain version
- dependencies
- `configopts`: string with the options passed to the ABINIT configure script

Changing `version` and/or toolchain version is straightforward.
To use a different version of ABINIT (e.g. 9.2.0), simply change the value of `version`.

```diff
- version = '9.0.4'
+ version = '9.2.0'
```

save the new file as `ABINIT-9.2.0-foss-2019b.eb` and execute:

```
eb ./ABINIT-9.2.0-foss-2019b.eb --robot
```

to install a new module for version 9.2.0.
Everything should work out of the box provided version 9.2.0 is still supporting
the version of the libraries listed in `dependencies`.
By the same token, one can compile with version `2020a` of the `foss` toolchain by just changing

```diff
- toolchain = {'name': 'foss', 'version': '2019b'}
+ toolchain = {'name': 'foss', 'version': '2020a'}
```

provided `releases/2020a` is available on our cluster.

Changing `dependencies` and `configopts` requires some basic understanding of the ABINIT build system
and of the logic used by EasyBuild to automate the compilation.
If you need a specialized `eb` file, feel free to contact the ABINIT developers on the forum or the EasyBuild 
developers to ask for support.
README for ABINIT
=================

ABINIT is an atomic-scale simulation software suite.

Most of the relevant information can be found on the ABINIT [website](https://www.abinit.org)
and the [Forum](https://forum.abinit.org).

Many documentation files can be found in the doc directory.
See the file [INSTALL](INSTALL) for generic compilation and installation instructions.
Please see the file [COPYING](COPYING) for copying conditions.

## LICENSE

	Copyright (C) 2006-2021 ABINIT Group

	This file is part of ABINIT.

	This program is free software; you can redistribute it and/or modify
	it under the terms of the GNU General Public License as published by
	the Free Software Foundation; either version 2, or (at your option)
	any later version.

	This program is distributed in the hope that it will be useful,
	but WITHOUT ANY WARRANTY; without even the implied warranty of
	MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
	GNU General Public License for more details.

	You should have received a copy of the GNU General Public License
	along with this program; see the file COPYING. If not, write to
	the Free Software Foundation, Inc., 59 Temple Place - Suite 330,
	Boston, MA 02111-1307, USA.
---
authors: J. Van Bever
---

# How to install ABINIT on Ubuntu

This file describes how to install ABINIT on Ubuntu.
The procedure for other Linux-based systems is similar, but the package manager and
the location of the libraries may differ.
Explicit testing was done with Ubuntu 19.10.

This page discusses how to install ABINIT with two different approaches:

- Using the apt package manager to install a precompiled version of abinit
- Compiling abinit from source using external libraries installed via apt.

Other possibilities (such as installation using the binary files provided by *conda*) are not discussed.

## Using apt

To install a precompiled version of ABINIT, just type:

    sudo apt install abinit

then enter your user password and ABINIT should install smoothly with all its dependencies.

!!! warning

    This version of ABINIT is most likely an old one.
    Use `abinit -v` to print the version number.

## Compiling from source under Ubuntu

### Introduction

The source code of the latest stable release is available at [https://www.abinit.org/packages](https://www.abinit.org/packages).
Compiling from source enables one to optimize the build, activate MPI parallelism, customize linear algebra libraries etc.
All the required packages are available via *apt* and can be installed with the syntax:

    sudo apt install [package]

Use the commands `apt search` and `apt show` to search for a package or to get info about a particular package.

Ubuntu packages install their executables in the bin folder (e.g. /usr/bin).
The location of the executable can be found via the unix `which` command.
For example

    which gfortran

will show the location of the gfortran executable provided the binary is in $PATH.

Other packages of interest when compiling code from source
are the so-called develop packages (denoted by the `-dev` suffix) that contain among others:

  - header files (extension `.h`) providing the declaration of prototypes and named constants.
    These files are usually installed in an include folder (e.g. /usr/include).
  - shared library files (extension `.so`).
    that are usually installed in a lib folder (e.g. /usr/lib).

To obtain the location of such files, use the command

    dpkg -L [package]

from the *dpkg* package

### Prerequisites

The prerequisites are first discussed qualitatively, because the installation may
depend on the linux distribution. Then we discuss how to compile the source code.

A possible list of prerequisites (tested for Ubuntu 19.10) is:

1. Ubuntu or similar Ubuntu-based distributions

2. A Fortran compiler. Possible options:

      - gfortran, the GNU compiler. It is open source and available via e.g. apt (gfortran package).
      - ifort, the intel compiler. This is a commercial compiler, slightly more complicated
        to use but more optimized for intel architecture.

3. A MPI library installed (If you want to benefit from parallelism; recommended).
   Possible options:

      - Open MPI from apt (`libopenmpi-dev` package) or [http://www.open-mpi.org](http://www.open-mpi.org)
      - MPICH from apt (`libmpich-dev` package) or [http://www.mpich.org](http://www.mpich.org)

    Depending on your distribution, you might need to manually add the `mpi-default-dev` package,
    a metapackage for both MPI libraries.

4. A Linear Algebra library installed.
   A fallback (see next point) is available inside ABINIT (basic version of lapack),
   but you might want to install a math library yourself, especially for parallel computations:
   You can choose among:

     - `blas` (libblas-dev)
     - `lapack` (liblapack-dev)
     - `scalapack` (libscalapack-...-dev)
     - `atlas` (libatlas-base-dev),
     - `mkl` from Intel (or you might try the libmkl-full-dev package).

5. Some mandatory libraries:

      - HDF5, NetCDF and NetCDF-Fortran, libraries to write/read binary files in netcdf4 format.
       These libraries are available via the `libhdf5-dev`, `libnetcdf-dev` and `libnetcdff-dev` packages from apt.
       For parallel IO, the `libpnetcdf-dev` is required.

      - LIBXC, a library containing exchange-correlation potentials, from the `libxc-dev` package.

   Note that it is also possible to generate these libraries via the ABINIT fallbacks:

   ```sh
   cd fallbacks
   ./build-abinit-fallbacks.sh
   ```

   In the latter case, the ABINIT configuration file (see later) should contain `with_fallbacks="yes"`.

These are the commands required to install the required packages from apt 
assuming a relatively simple ABINIT build with MPI support.
The list of commands may change depending on your linux distribution,
the exact ABINIT version you want to compile and the libraries you want to use.

```sh
# 1 # compiler
sudo apt install gfortran

# 2 # MPI libraries - choice for Open MPI
sudo apt install mpi-default-dev libopenmpi-dev

# 3 # math libraries - choice for lapack and blas
sudo apt install liblapack-dev libblas-dev

# 4 # mandatory libraries
sudo apt install libhdf5-dev libnetcdf-dev libnetcdff-dev libpnetcdf-dev libxc-dev
```

### Compiling ABINIT

For normal users it is advised to download the latest stable version from our website
(replace 9.0.4 by the newest version available).

```sh
wget https://www.abinit.org/sites/default/files/packages/abinit-9.0.4.tar.gz
tar xzf abinit-9.0.4.tar.gz
cd abinit-9.0.4
```

Create a working directory:

```sh
mkdir build && cd build
```

To configure, use:

```sh
../configure --with-config-file='my_config_file.ac9'
```

where 'my_config_file.ac9' is a configuration file that is discussed in more details in the next section.

Compile with:

```sh
make -j4
```

Install (optional):

```sh
make install
```

### The configuration file

The configure script accepts variables and flags to customize the configuration.
For example

```sh
../configure FC=mpifort --with-mpi="yes"
```

tells ABINIT to configure for a MPI build with the *mpifort* MPI wrapper.
To obtain the documentation for the different variables and flags, use:

```sh
../configure --help
```

Most configuration options are detected automatically by configure.
For example, if `with_mpi` is set to 'yes', configure will try to use the parallel fortran compiler (mpifort)
and automatically detect the MPI installation with libraries (.so) and header (.h) files.
When you install the Open MPI package via apt, these directories can be printed to terminal 
by using `dpkg -L 'libopenmpi-dev'`.

When a lot of options must be passed to configure, it is advised to use an external configuration file with
the syntax:

```sh
../configure --with-config-file='my_config_file.ac9'
```

An example of configuration file to build abinit with MPI, lapack and blas and automatic 
detection for libxc, hdf5, and netcdf:

```sh
# MPI settings
with_mpi="yes"
enable_mpi_io="yes"

# linear algebra settings
with_linalg_flavor="netlib"
LINALG_LIBS="-L/usr/lib/x86_64-linux-gnu -llapack -lblas"

# mandatory libraries
with_libxc="yes"
with_hdf5="yes"
with_netcdf="yes"
with_netcdf_fortran="yes"
```

Note that:

  - one uses '-' when typing a flag but '_' inside the config file, e.g. `--with-mpi="yes"` becomes `with_mpi="yes"`.

  - the LINALG_LIBS variable was explicitly set for this linux distrubution.
    The directory was extracted via `dpkg -L liblapack-dev` and `dpkg -L libblas-dev`.

  - when fine tuning variables and flags for a particular linux distribution, it is advised to
    take a look at the template file `~abinit/doc/build/config-template.ac9`.
    For example, the setting of `LINALG_LIBS` in this template file is given
    by the line `#LINALG_LIBS="-L/usr/local/lib -llapack -lblas"`.

More specialized libraries might be harder to detect.
For example, the following section was added to the config file to detect customized FFT and XML libraries.
These libraries are available via apt (`libfftw3-dev `and `libxml2-dev`).
The directories for the corresponding library and header files can be found by using `dpkg -L [package]`
and other flags can be extracted from the `~abinit/doc/build/config-template.ac9` template

```sh
# fast fourier settings
with_fft_flavor="fftw3"
FFTW3_CPPFLAGS="-I/usr/include"
FFTW3_FCFLAGS="-I/usr/include"
FFTW3_LIBS="-L/usr/lib/x86_64-linux-gnu -lfftw3 -lfftw3f"

# XML2 library (used by multibinit)
with_libxml2="yes"
LIBXML2_FCFLAGS="-I/usr/lib/x86_64-linux-gnu/"
LIBXML2_LIBS="-L/usr/include/libxml2/libxml/ -lxmlf90"
```
# GPU support in Abinit

  IMPORTANT: GPU support is currently highly *EXPERIMENTAL* and should
             be used by experienced developers only. If you encounter
             any problem, please report it to Yann Pouillon before doing
             anything else.

## GPU-related parameters

GPU support is activated by the --enable-gpu option of configure.
Another option of importance is the --with-gpu-flavor one, which selects
the kind of GPU support that will be activated. A convenience option,
codename --with-gpu-prefix, is also provided, in order to set
automatically all relevant parameters whenever possible. A few other
options are available as well, mainly for fine-tuning of the build
parameters and testing purposes.

Full descriptions of all these options can be found in the
~abinit/doc/build/config-template.ac9 file. Do not hesitate to ask
questions on https://forum.abinit.org/.

In addition, the permitted GPU-related preprocessiong options are:

  * HAVE_GPU        : generic use;
  * HAVE_GPU_SERIAL : serial GPU support;
  * HAVE_GPU_MPI    : MPI-aware GPU support.


## Cuda support

At present it is possible to ask for single- or double-precision Cuda
support. The configure script will check that the Cuda libraries are
properly working, but however not whether double-precision is actually
supported by your version of Cuda (this might be added in the future).

All calls to Cuda routines should be carefully embedded within
'#if defined HAVE_GPU_CUDA ... #else ... #endif' preprocessing blocks.
When a feature does require Cuda and will not work without it, the
corresponding '#else' part should display an error and cause Abinit to
abort.

The permitted Cuda-related preprocessing options are :

  * HAVE_GPU_CUDA    : generic use;
  * HAVE_GPU_CUDA_SP : single-precision calculations;
  * HAVE_GPU_CUDA_DP : double-precision calculations.

All high-level routines directly accessing Cuda features have to be put
in ~abinit/src/52_manage_cuda/, and low-level ones in
~abinit/shared/common/src/17_gpu_toolbox/. 

All files belonging to nVidia must *not* be distributed with Abinit.
Please discuss with Yann Pouillon if you need them inside the Abinit
source tree during the build.

In any case, all Cuda-related developments should be done in good
coordination with:

  * Marc Torrent
  * Yann Pouillon

## Cuda version

To take advantage of the multiple FFT in cuda (FFT in batch), ABINIT
have to be compiled with a Cuda version>=3.0.
It is possible to build with previous versions (>2.1 tested) but you
make some changes. 
cuda implementation support devices with capabilty (revision)>1.0

## Magma support

The MAGMA project aims to develop a dense linear algebra library similar
to LAPACK but for heterogeneous/hybrid architectures, starting
with current "Multicore+GPU" systems.
It is recommended to take advantage of MAGMA when using ABINIT with
Cuda.
Magma is not distributed within ABINIT package; it has to be preliminary
installed. To activate MAGMA support during building process, use
--wih-linalg-flavor="...+magma" at configure level.

## OpenCL support

OpenCL support is currently under discussion. More info will come once
decisions have been taken.


## S_GPU support

The S_GPU library provides higher performance and better load balancing
when each GPU of a hybrid computer is shared by several processes, e.g.
MPI tasks.

It will be supported in Abinit in the future, from its version 2.

See http://ligforge.imag.fr/projects/sgpu/ for details.
# Installation instructions specific to ABINIT

Please refer to the official Web [Installation Notes](https://docs.abinit.org/installation),
or the file ~abinit/doc/installation.md .

Please report problems or comments on the [ABINIT forum](https://forum.abinit.org).

---
authors: J-M Beuken
---

# How to install ABINIT v9 on CentOS

This step-by-step recipe describes how to build ABINIT on Fedora/RHEL/CentOS *nix distributions.
Tested with __CentOS 8.2__

[Quick Guide for the Impatient](#quick-guide-for-the-impatient)

[Quick Guide for the Impatient (MKL version)](#quick-guide-for-the-impatient-mkl-version)

## Prerequisites

1.  __Fortran compiler__

    Possible options:

    - gfortran, the GNU compiler. ([https://gcc.gnu.org/](https://gcc.gnu.org))
    - ifort, the intel compiler. This is a commercial compiler, slightly more complicated
      to use but more optimized for intel architecture.

2.  __Python interpreter__ (**v3.7+ recommended**)

3.  __MPI library__ (if you want to benefit from parallelism; **recommended**).

     Possible options:

     - [Open MPI](http://www.open-mpi.org)
     - [MPICH](http://www.mpich.org)

4.  __Linear Algebra library__

    Possible options:

    - MKL (Intel® Math Kernel Library): [Free Download](https://software.intel.com/content/www/us/en/develop/tools/math-kernel-library/choose-download/linux.html), **recommended** for performance
    - [OpenBLAS](https://www.openblas.net): An optimized BLAS library, **recommended with GNU**.
    - [Netlib](https://www.netlib.org): blas, lapack, scalapack
    - [ATLAS](http://math-atlas.sourceforge.net/): Automatically Tuned Linear Algebra Software

5.  __Mandatory libraries:__

    - [HDF5](https://www.hdfgroup.org/solutions/hdf5/): High-performance data management and storage suite
    - [NetCDF](https://www.unidata.ucar.edu/software/netcdf): Network Common Data Form
    - [libXC](https://tddft.org/programs/libxc/download/): Library of exchange-correlation functionals

6.  __Optional libraries:__

    - [FFTW3](http://www.fftw.org/): Library for computing the discrete Fourier transform, **recommended with GNU**
    - [libxml2](http://xmlsoft.org/downloads.html): XML C parser, recommended for multibinit
    - [Wannier90](http://www.wannier.org)
    - [LibPSML](https://esl.cecam.org/PSML) + [xmlf90](https://gitlab.com/siesta-project/libraries/xmlf90)
      to read pseudopotentials in psml format

## Installation of tools and libraries

All mandatory libraries are installed through the DNF package manager.
For other optional libraries, compilation from source is needed.

The steps required to install MPICH, fftw3 and OpenBLAS with dnf and compile
__a relatively simple parallel version of ABINIT__ are summarized below:

1. __Install the compiler__

    `sudo dnf install gcc-gfortran`

2. __Install the MPI library__ (MPICH)

    `sudo dnf install mpich mpich-devel`

3. __Install the linear algebra library__  (OpenBLAS)

    `sudo dnf install openblas`

4. __Install other mandatory libraries__  (use hdf5 with support for parallel MPI-IO)

    `sudo dnf install hdf5-mpich hdf5-mpich-devel`

    `sudo dnf install netcdf-mpich-devel netcdf-fortran-mpich-devel`

    `sudo dnf install libxc libxc-devel`

5. __Install fftw3__

    `sudo dnf install fftw fftw-devel`

6. __Install the python interpreter__

    `sudo dnf install python3`

!!! important

    Before continuing, it is important to test whether your development environment is properly configured.
    To check whether the MPICH package is installed, execute the following command:

    ```sh
    mpif90 --version
    ```

    If the output is:

    ```sh
    bash: mpif90: command not found...
    ```

    then, you need to find out where the MPI wrappers are installed.

    If you installed the MPICH package via dnf, the installation directories can be obtained by using e.g.

    ```sh
    rpm -ql mpich-devel | grep mpif90
    ```

    that should print

    ```sh
    /usr/lib64/mpich/bin/mpif90
    ```

    The $PATH variable needs to be updated:

    ```sh
    export PATH=/usr/lib64/mpich/bin:$PATH

    mpif90 --version

    GNU Fortran (GCC) 8.3.1 20191121 (Red Hat 8.3.1-5)
    /usr/lib64/mpich/bin/mpif90
    ```

## Compiling, testing and installing ABINIT

__Download ABINIT__.

For normal users, it is advised to get the latest stable version
from our [website](https://www.abinit.org/packages) (replace 9.0.4 by the newest version available).

```sh
wget https://www.abinit.org/sites/default/files/packages/abinit-9.0.4.tar.gz
tar xzf abinit-9.0.4.tar.gz
cd abinit-9.0.4
```

__Create a working directory__:

```sh
mkdir build && cd build
```

__Configure with__:

```sh
../configure --with-config-file='my_config_file.ac'
```

where `my_config_file.ac` is an external file providing all the configuration flags and options.
More on the configure options is presented in [next section](#the-config-file).

__Compile with__:

```
make -j 4
```

where `-j 4` means that 4 cores are used to compile. Adjust this value according to number of
physical cores available on your machine.

To run the test suite, issue:

```sh
cd tests
./runtests.py fast -j 4
```

!!! Important

    At the end of the test, one should get something like:

    ```
    Suite   failed  passed  succeeded  skipped  disabled  run_etime  tot_etime
    fast         0       0         11        0         0      27.72      27.98

    Completed in 9.95 [s]. Average time for test=2.52 [s], stdev=2.94 [s]
    Summary: failed=0, succeeded=11, passed=0, skipped=0, disabled=0
    ```

otherwise there is a __problem__ with the compilation: see [Troubleshooting](#troubleshooting)

__Install__ (optional):

    make install

## The configuration file

The configure command takes in input variables and flags.
For example:

```sh
../configure --with-mpi="yes"
```

tells ABINIT to enable MPI support.
All the variables and flags supported by the script can be found by typing:

```sh
../configure --help
```

Some options are detected automatically by the script.
For example, with the option `--with-mpi="yes"`, ABINIT will try to use the parallel fortran compiler
found in $PATH (e.g. mpifort) and will try to detect the directories containing the libraries
and the associated header files required by MPI.

When a lot of options are needed, it is advised to use a config file.

The `.ac` file for __our simple parallel ABINIT__ build based on OpenBLAS is:

```sh
# installation location
prefix=$HOME/local

# MPI settings
with_mpi="yes"
enable_mpi_io="yes"

# linear algebra settings
with_linalg_flavor="openblas"
LINALG_LIBS="-L/usr/lib64 -lopenblas"

# mandatory libraries
with_hdf5="yes"
with_netcdf="yes"
with_netcdf_fortran="yes"
with_libxc="yes"

# FFT flavor
with_fft_flavor="fftw3"
FFTW3_LIBS="-L/usr/lib64 -lfftw3 -lfftw3f"

# Enable Netcdf mode in Abinit (use netcdf as default I/O library)
enable_netcdf_default="yes"
```

!!! Important

    The name of the options in the `.ac` files is in normalized form that is
    the initial `--` is removed from the option name and all the other `-` characters
    in the string are replaced by an underscore `_`.
    Following these simple rules, the configure option `--with-mpi` becomes `with_mpi`
    in the ac file.

<!--
## To go further

- compiling optional libraries with the fallback project: Wannier90, libPSML/XMLF90.
- enabling OpenMP
- using libxml2
-->

## Quick Guide for the impatient

We will build ABINIT with the following components:

- GNU compilers
- MPICH
- OpenBLAS
- FFTW3

### Installing required packages

```sh
sudo dnf install gcc-gfortran
sudo dnf install mpich mpich-devel
sudo dnf install openblas
sudo dnf install hdf5-mpich hdf5-mpich-devel
sudo dnf install netcdf-mpich-devel netcdf-fortran-mpich-devel
sudo dnf install libxc libxc-devel
sudo dnf install fftw fftw-devel
sudo dnf install python3
```

### Getting the ABINIT tarball

```sh
wget https://www.abinit.org/sites/default/files/packages/abinit-9.0.4.tar.gz
tar xzf abinit-9.0.4.tar.gz
cd abinit-9.0.4
mkdir build && cd build
export PATH=/usr/lib64/mpich/bin:$PATH
```

### Creating a config file

Edit a `config.ac` file:

```sh
# installation location
prefix=$HOME/local

# MPI settings
with_mpi="yes"
enable_mpi_io="yes"

# linear algebra settings
with_linalg_flavor="openblas"
LINALG_LIBS="-L/usr/lib64 -lopenblas"

# mandatory libraries
with_hdf5="yes"
with_netcdf="yes"
with_netcdf_fortran="yes"
with_libxc="yes"

# FFT flavor
with_fft_flavor="fftw3"
FFTW3_LIBS="-L/usr/lib64 -lfftw3 -lfftw3f"

# Enable Netcdf mode in Abinit (use netcdf as default I/O library)
enable_netcdf_default="yes"
```

### Compiling ABINIT

```sh
../configure -q --with-config-file='config.ac'
make -j 8
```

### Testing ABINIT

```sh
cd tests
export OPENBLAS_NUM_THREADS=1
./runtest.py fast -j 8 --no-logo
```

### Installing ABINIT

```sh
make install
```

## Quick Guide for the impatient (MKL version)

We will build ABINIT with the following components:

- GNU compilers
- MPICH
- MKL

### Installing needed packages

```sh
sudo dnf install gcc-gfortran
sudo dnf install mpich mpich-devel
sudo dnf install hdf5-mpich hdf5-mpich-devel
sudo dnf install netcdf-mpich-devel netcdf-fortran-mpich-devel
sudo dnf install libxc libxc-devel
sudo dnf install python3
```

### Getting the ABINIT tarball

```sh
wget https://www.abinit.org/sites/default/files/packages/abinit-9.0.4.tar.gz
tar xzf abinit-9.0.4.tar.gz
cd abinit-9.0.4
mkdir build && cd build
export PATH=/usr/lib64/mpich/bin:$PATH
```

### Creating a configuration file

Edit a `config.ac` file:

```sh
# installation location
prefix=$HOME/local

# MPI settings
with_mpi="yes"
enable_mpi_io="yes"

# linear algebra settings
with_linalg_flavor="mkl"
LINALG_CPPFLAGS="-I${MKLROOT}/include"
LINALG_FCFLAGS="-I${MKLROOT}/include"
LINALG_LIBS="-L${MKLROOT}/lib/intel64 -Wl,--start-group  -lmkl_gf_lp64 -lmkl_sequential -lmkl_core -Wl,--end-group"

# mandatory libraries
with_hdf5="yes"
with_netcdf="yes"
with_netcdf_fortran="yes"
with_libxc="yes"

# FFT flavor
with_fft_flavor="dfti"
FFT_FCFLAGS="-I${MKLROOT}/include"

# Enable Netcdf mode in Abinit (use netcdf as default I/O library)
enable_netcdf_default="yes"
```

### Compiling ABINIT

```sh
../configure --with-config-file='config.ac'
make -j 8
```

### Testing ABINIT

```sh
cd tests
export MKL_NUM_THREADS=1
./runtest.py fast -j 8 --no-logo
```

### Installing ABINIT

```sh
make install
```
Adding a new directory
======================

Instructions can be adapted to move a directory inside src (see point 4)
Do this only if you have the autotools.

1. create your directory, fill it, and add the files with git

2. in `config/specs/corelibs.conf`, declare the new directory,
   its attributes and its dependencies upon the external libraries

3. in `config/specs/binaries.conf`, specify the main codes that
   depends on the routines contained in this new directory

Then, in order for the make to work, issue:

    ~abinit/config/scripts/makemake

then `configure` and `make`

In case of a move, check that the doc (or other files) does not need any update.
Advice: find the occurrences thanks to:

    grep name_of_directory *
    grep name_of_directory */*


### Harmonic lattice potential added and tested
### Abstract lattice mover added and tested
### Velocity Verlet NVE mover added and tested
### Langevin NVT mover added and tested
### Berendsen NVT mover added and tested
---
This file list the things to do for multibinit
---
# Todo list for Multibinit

## Temporary: for short term todos. Will be moved to other sections or removed once it is done.
### [TODO] add energy table in manager.
  - State: waiting for approvement of Multibinit devs. 
  - State: Implemented. Need to be tested
### [TODO] Check acoustic sum rule of Oiju and Tijuv term. 
   \sum_u Oiju = 0, \sum_u Tijuv=0, \sum_v Tijuv=0, \sum_uv Tijuv=0
  - state: Oiju bug found and partialy fixed. Now cutoff on (Riu+Rju) is disabled. 

### [DONE?] check if Oiju parameter fitting has correct sign for ij permutation.
 - state: Oiju=Ojiu is correct if there is no SOC. 
### [TODO] implement sorting so that  (@Nicole, Can you confirm which ones?)  
 - Oiju could be grouped by ju?
 - Tijuv could be grouped by uv? ju? 
 
### [TODO] clean spin hist data structure.
 

## Preparing for Abinit9

### [TODO] Update the automatic tests (hexu) [time: November, 2019]

### [TODO] Add more autotests. (Nicole) [time: November, 2019??]

      - list of SLC tests?
[TODO] Autotests with Multibinit --F03.

### [Ongoing] Finalize the format of potential netcdf file and spin hist file. (hexu & Nicole) [time: December]

      - Oiju format updated.
### [TODO] documentation for the file formats. (Nicole & hexu)

#### [TODO] use reduced coordinates instead of cartersian? Do we have the final decision? (hexu)  [time: November]
### [Ongoing] Full workflow of fitting parameters. (hexu) [time: November]
### [TODO] Update the tutorials and other documentations. (Nicole & hexu) [time: November]
### [TODO] Tutorial for spin-lattice coupling???


## Genenal structure

### [TODO] Clean lattice part in 98_main/multibinit.F90 to manager or lattice code

State: 


### [TODO] Separate the 78_effpot directory into layers 

State: 

### [DONE] Make a example harmonic lattice potential. (hexu) [time: July, 2019]
#### [DONE] implement primitive potentail (read from netcdf file). Tested.
#### [DONE] implement supercell potential, fill_supercell
#### [DONE] add the data structure to the manager.
###  [DONE] move random number generator to manager and use pointer in each mover.

### [ISSUE] Where to store the total energy.
	When there are multiple potential, each mover get part of the energies but not
	   all of them.
	e.g. For spin+lattice. Spin mover get E(spin) + E(spin-lattice-coupling).
	        Lattice_mover get E(lattice) + E(spin-lattice-couping) 
	    Another problem is from the kinetic energy, which is not calculated by the potential
	but the lattic mover.
	    Also note that the energy is related to specific heat, susceptibility, etc. 

---

## Documenation
### [DONE] Document general data structure  (hexu) [time: July, 2019]


#### [DONE] Make a developer's guide on how to add new things. (hexu) [time: July, 2019]

State: First draft in multibinit_note.md

### [TODO?] Make a full technical note (including all the methods used) (everyone) 

State: One file exist for spin/spin-lattice coupling.

---

## Lattice

### [TODO] Adapt to general data structure

State: 

### [TODO] Mover

#### [TODO] set intial state:  (hexu) [time: July 2019]
       * [DONE] implement get_temperature and get_kinetic_energy
       * [TODO] More options other than Maxwell-Boltzmann distribution
       * [DONE] make center of mass fixed.
       * [TODO] make sure there is no global rotation. (useful for non-periodic structure only.)

#### [TODO] implement movers. (hexu) [time: July 2019]

      * [DONE] implement Velocity Verlet mover
      * [DONE] implement Langevin mover
      * [DONE] implement Berendsen NVT
      * [Pending] implement Berendsen NPT  (Pending due to no testcase available)
      * [DONE] add friction parameter for langevin, taut for Berendsen, (taup) for Berendsen NPT
      * [DONE] Add documentation to input variables

---

## Spin

### [TODO] Documentaion of codes (hexu) [time: July, 2019] 

### [TODO] Implement dipole-dipole 

### [TODO?] Implement IMP/SIB integration mover.

### [TODO?] biqudratic terms (as computed from TB2J)

---

## Spin-Lattice coupling

### [DONE] Test two dynamics without coupling (hexu) [time: July,2019]

### [TODO] Implement primitive Oiju potential (Nicole) 
 - Other terms are also implemented. Test to be done.

### [TODO] Implement supercell Oiju potential (Nicole)
 - Other terms are also implemented. Test to be done.

### [TODO??] Implement advanced integration algorithm for spin/lattice dynamics

### [TODO] Optimize Tijuv terms so the time is acceptable. (Nicole & hexu) [time: Now]
    - The firt time consuming part is the sorting of the ijuv indices. Merge sort (O(NlogN)) is now (07/11/2019) instead of insertion sort (O(N^2)). More test needed to see if calculating the force and bfield is fast enough, especially on larger supercell. 


### [TODO] Implement full procedure of coupled SLC dynamics. (Nicole)

---
## Lattice wannier function

---

## Electron

---


## Long term issues
### [ISSUE] damping parameter for spin dynamics. (1) how to get the damping parameter for spin dynamics. (2) Counting spin-phonon coupling


# Multibinit Enhancement Proposals

## MEP0: keep a list of proposed changes as MEP.
If some changes to be basic structure of multibinit, put them here.

## MEP1: add label property to potentials (for debugging)
For each potential, it has a default label, which can be modified.
After it is finalized, it becomes "Destroyed potential". 

## MEP2: add get_delta_energy method to each potential. (For Monte Carlo)
- get_delta_energy( displacement, strain, spin, lwf, ispin, dS, ilatt, dtau, ilwf, dlwf, energy)
all params are optional, but should be given in pairs (ispin, dS).
This is limited to one move per step (simple metropolis-Hastings)
possible extension: arrays of changes for other MC methods.

## MEP3: Do not use mover list
- We need very fine control of each mover. Putting them into a list and loop over them is not useful. 
And FORTRAN polymorphous  pointer is quite verbose to use. 

## MEP4: save global state in a type (extends Multibinit_dtset_type).
- The potentials and movers need to share some states, mostly the input parameters and things derived from them, e.g. current temperature from temperature range.  . We can extend multibinit_dtset_type and add some attributes. And a pointer to it can be added to the movers and potentials if necessary. 

## MEP5: specify which data structures are mpi aware.
- If yes, it has to be outside if(iam_master) 
     else it has to be inside if(iam_master)
     
## MEP6: Unit test mode
 Unit tests are useful but not so easy in Abinit/Multibinit. We can make a module m_multibinit unit test and call unit test functions from there.
 Unit test functions can be implemented in the modules they are for.
 By running "multibinit --unittest", the unit tests will run instead of the main Multibinit.
 
## MEP7: use/not use sub type in primitive/supercell?
- The lattice part use crystal_t from abinit. we can either extend this type, or make one type for each nature. Which one is better?

     


- 
---
Copyright (C) 2001-2021 ABINIT Group (hexu)
This file is distributed under the terms of the
GNU General Public License, see ~abinit/COPYING or http://www.gnu.org/copyleft/gpl.txt.
For the initials of contributors, see ~abinit/doc/developers/contributors.txt .
---

# Developer's notes on Multibinit

## Manager
### The whole procedure
* Read files (outside of manager) file and input file.

* Allocation of data structure (using information from input params). 

* Read primitive potential and put them to primitive_potential_list. This includes the reading of primitive cell. So it works like this: 1. allocate a object for the primitive potential. 2. set the primitive pointer. 3. call load_from_files method of primitive potential. 4. append the potential to the primitive potential list.

* Build supercell and supercell potential. 

* Initialize movers, which use information from the input file and the supercell.

* Call the movers to run the dynamics. The movers will do: 1. call supercell potentials to calculate energies and the derivatives. 2. update system states. 3. calculate observables if needed. 4. write to hist file.

* Finalize. One thing to note is that not all the objects in the manager are allocated or initialized because they are not needed in that calculation. So for pointers, they should be initially null() and the manager should check assciated(pointer) before finalising it.  And for objects, there should be a tag to tell if it is initialized, which should be taken care of inside the objects themselves. 

## Potential
A potential usually has a primitive cell and a supercell presentation.
The primitive cell potential is the one saved in a file (xml, netcdf, etc), and of which the parameters are fitted.
The supercell potential can be used as a "calculator" to get the energies and their derivatives.
There are abstract type for both of them. To add a new potential one should add both of them. And the primitive potential should 
have a fill_supercell.

### Add has_varibles in the abstract class
    * in abstract_primitive_potential 
    * in abstract_potential
    
    TODO: since there are a few of this, and they are universal in the whole program. 
    Would it be better if we define a data structure to save it and use pointers.

### Add a section to a cell
    * The mb_cell_t type has sections (lattice, spin, lwf, ...).

### Adding primitive potential

    * make a derived type from abstract_primitive_potential_t

    * modify the function in the manager  (read_potentials). It usually has 4 steps.
    1. allocate a (abstract_primitive_t) pointer to the primitive potential needed.
    2. set the primitive_cell pointer to the potential. This should be done before the loading from file function, because some of the load function also load information for the primitive cell type.
    3. load the primitive potential from file. 
    4. append the primitive potential to the primitive potential list, which means there will be a pointer pointed to the allocated type.
    
   The finalize function will be called by the primitive potential list when itself is finalized. 

### Adding supercell potential

    * make a derived type from abstract_potential_t 

    * override the methods if necessary

    * There is NO need to do anything in the manager for building the supercell potential. ALL supercell potentials are built by the fill_supercell method of the primitve potetial list automatically.

    * Also there is no need to worry about calling the finalize function: it will be called by the potential_list finalize function. 
    

##  Movers
The movers
### Adding a new type of mover
    * make a derived type from abstract_mover_t
    * override the methods if necessary
    * add a polymorphic class pointer of the mover to the manager, so that any mover derived from this type can be used. It should be null by default, so one can know if it gets initialized.
    * add the initialization to the set_movers method in the manager. 
    * implement a run function to make the mover work. 
    * The finalization function need to be called inside the manager finalize function. One thing has to be noted: the mover might not be initialize. Therefore, before finalize it, always check using the associated(pointer). 

### Adding a new lattice mover
    * make a derived type from lattice_mover_t
    * override (at least) the initialize, finalize, and run_one_step function
    * modify the set_movers function in the manager to activate the mover (if the parameters ask for it.)
    
### Write hist file
    * A hist file 
    NOTE: remember to put the netcdf related code between if have netcdf

## How to use supercell_maker
The supercell maker use the translation symmetry to 
### translation

## 

This directory, tests, contains tests which exercise parts 
of the ABINIT package.

These tests are designed primarily to exercise parts of the code
quickly, NOT necessarily to give physically sensible results.
For greater speed, some tests are not run to full convergence.
Also the quality parameters (especially ecut) are minimal, i.e.
the calculations are underconverged.

Most of them are ran through the runtests.py utility. Type

    $ runtests.py --help

to get help.

There is a small README file in the subdirectories that contain the input and reference files.
The documentation related specifically to one test is appended to the input file of this test,
and can also be accessed through `runtests.py`  (aka `runtests.py -l`).

Also, typing

    $ make help

will give information on the use of the make command in the present directory. 
`make` mostly relies on `runtests.py`,
but also triggers some other actions, in which runtests.py is not used.

The video below gives an overwiew of the command line options of `runtests.py`

[![asciicast](https://asciinema.org/a/40324.png)](https://asciinema.org/a/40324)
# Information about the pseudopotentials in this directory

These JTH v1.0 pseudopotentials have the same characteristics:

* Same pseudopotential generator (Atompaw 4.0.0.12)

* Same pseudopotential format (XML for PAW)

* Same exchange-correlation functional (pspxc 7 , Perdew-Wang 1992)

* Standard accuracy

They have been downloaded on July 21, 2020 from the page http://www.pseudo-dojo.org/, with the settings
PAW (JTH v1.0), LDA, standard, xml



# Information about the pseudopotentials in this directory

These pseudopotentials have the same characteristics:

* Same pseudopotential generator (Troullier-Martins, generated by A. Khein and D.C. Allan)

* Same pseudopotential format (pspcod 1)

* Same exchange-correlation functional (pspxc 1 , Perdew-Wang parameterized by Teter)

They have been downloaded from the page https://www.abinit.org/sites/default/files/PrevAtomicData/psp-links/lda_tm.html
on 21 July 2020.
# Information about the pseudopotentials in this directory

These pseudodojo NC-SR v0.4 pseudopotentials have the same characteristics:

* Same pseudopotential generator (ONCVPSP-3.3.0)

* Same pseudopotential format (pspcod 8)

* Same exchange-correlation functional (pspxc 11 , PBE - Perdew-Burke-Ernzerhof)

* Standard accuracy

Several (As.psp8  Ca.psp8  Ga.psp8  In.psp8  N.psp8  O.psp8)
have been downloaded on July 21, 2020 from the page http://www.pseudo-dojo.org/, with the settings
NC SR (ONCVPSP v0.4), PBE, standard, psp8

Se-d.psp8 and Zn-sp.psp8 have been downloaded on November 1, 2020 from 
https://github.com/abinit/pseudo_dojo/tree/master/pseudo_dojo/pseudos/ONCVPSP-PBE-PDv0.4

H.psp8 has been downloaded on December 30, 2020 from the page http://www.pseudo-dojo.org/, with the settings
NC SR (ONCVPSP v0.4), PBE, standard, psp8
# Information about the pseudopotentials in this directory

These pseudodojo NC-FR v0.4 pseudopotentials have the same characteristics:

* Same pseudopotential generator (ONCVPSP-3.3.0)

* Same pseudopotential format (pspcod 8)

* Same exchange-correlation functional (pspxc 11 , PBE - Perdew-Burke-Ernzerhof)

* Standard accuracy

* Fully relativistic (so, with spin-orbit information)

They have been downloaded on July 21, 2020 from the page http://www.pseudo-dojo.org/, with the settings
NC FR (ONCVPSP v0.4), PBE, standard, psp8



# Information about the pseudopotentials in this directory

These pseudopotentials have the same characteristics:

* Same pseudopotential generator (HGH, see Phys. Rev. B 58, 3641 (1998))

* Same pseudopotential format (pspcod 3)

* Same exchange-correlation functional (pspxc 1 , Perdew-Wang parameterized by Teter)

They can also be found at the page https://www.abinit.org/sites/default/files/PrevAtomicData/psp-links/hgh.html
WARNING : BigDFT try to read an additional line giving rcutoff and rloc, present in some pseudopotentials.
If such a line exist after the value defined by lmax, but does NOT contain proper rcutoff and rloc (e.g. 0.0), BigDFT might fail by hanging
or SEGFAULT, without giving proper error message. This is difficult to debug.

# Information about the pseudopotentials in this directory

These JTH v1.0 pseudopotentials have the same characteristics:

* Same pseudopotential generator (Atompaw 4.0.0.12)

* Same pseudopotential format (XML for PAW)

* Same exchange-correlation functional (pspxc 11 , PBE - Perdew-Burke-Ernzerhof)

* Standard accuracy

They have been downloaded on July 21, 2020 from the page http://www.pseudo-dojo.org/, with the settings
PAW, PBE, standard, xml . 
Most are JTH v1.0, except JTH v1.1 for Fe.xml, Ni.xml, Sn-sp.xml and Sn.xml .

Na.xml and I.xml have been downloaded on February 11, 2021, from the same page with the same settings. They are JTH v1.0 .



# Information about the atomic density files in this directory

These files have been generated with pspxc 8, Perdew-Wang 1992.
They have been downloaded from the page https://www.abinit.org/all_core_electron on 22 July 2020.
# Information about the pseudopotentials in this directory

These pseudodojo NC-SR v0.4 pseudopotentials have the same characteristics:

* Same pseudopotential generator (ONCVPSP-3.3.0)

* Same pseudopotential format (pspcod 8)

* Same exchange-correlation functional (pspxc -1012 , PW - Perdew-Wang LDA)

* Standard accuracy

Most have been downloaded on July 21, 2020 from the page http://www.pseudo-dojo.org/, with the settings
NC SR (ONCVPSP v0.4), LDA, standard, psp8

The O.psp8 has been downloaded on September 26, 2020 from Github 
https://github.com/abinit/pseudo_dojo/blob/master/pseudo_dojo/pseudos/ONCVPSP-PW-PDv0.4/O/O.psp8
At that time, it was not available on the pseudodojo Web site.



# Information about the pseudopotentials in this directory

These JTH v1.0 pseudopotentials have the same characteristics:

* Same pseudopotential generator (Atompaw 4.0.0.12)

* Same pseudopotential format (XML for PAW)

* Same exchange-correlation functional (pspxc 7 , Perdew-Wang 1992)

* Stringent accuracy

They have been downloaded on July 21, 2020 from the page http://www.pseudo-dojo.org/, with the settings
PAW (JTH v1.0), LDA, stringent, xml



# Information about the pseudopotentials in this directory

These pseudodojo NC-SR v0.4 large core pseudopotentials have the same characteristics:

* Same pseudopotential generator (ONCVPSP-3.3.0)

* Same pseudopotential format (pspcod 8)

* Same exchange-correlation functional (pspxc -1012 , PW - Perdew-Wang LDA)

* Standard accuracy

They have been downloaded on September 26, 2020 from the Github directory 
in https://github.com/abinit/pseudo_dojo/tree/master/pseudo_dojo/pseudos .
They were NOT available on the pseudodojo Web site, because of lower accuracy than the standard small core 
pseudopotentials delivered on the pseudodojo Web page.



# Information about the pseudopotentials in this directory

These pseudopotentials have the same characteristics:

* Same pseudopotential generator (GTH, see Phys. Rev. B 54, 1703 (1996))

* Same pseudopotential format (pspcod 2)

* Same exchange-correlation functional (pspxc 1 , Perdew-Wang parameterized by Teter)

They can also be found at the page https://www.abinit.org/sites/default/files/PrevAtomicData/psp-links/hgh.html
WARNING : BigDFT try to read an additional line giving rcutoff and rloc, present in some pseudopotentials.
If such a line exist after the value defined by lmax, but does NOT contain proper rcutoff and rloc (e.g. 0.0), BigDFT might fail by hanging
or SEGFAULT, without giving proper error message. This is difficult to debug.

# Information about the core density files in this directory

These files have been generated with pspxc 8, Perdew-Wang 1992.
They have been downloaded from the page https://www.abinit.org/core_electron on 22 July 2020.

# Git submodules with precomputed ABINIT files

This directory contains git submodules with precomputed netcdf or text files.
These files are used in selected ABINIT tutorials to bypass the preliminary steps (e.g. expensive GS/DFPT calculations).
We use netcdf or text files since these formats are portable across different architectures 
contrarily to Fortran binary files that are not portable due to hardware-endianess.

The advantage of such git-module-based approach is that, in the tutorials, we can focus 
on the most important part of the calculation without having to waste time to generate the required files.
Moreover this improves the stability of the automatic tests as well as user experience.
The disadvantage is that including all these files in the official distribution would 
increase the size of tarball file.
This is the reason, why we decided to host these external files on github repos 
that are included here as submodules. 
These submodules **are not included** in the official tarball file:
the directory modules_with_data is indeed listed in ~abinit/config/specs/junk.conf.

Users interested in running tutorials based on git submodules can easily download the data from the internet
or use git directly to fetch the repository from the internet.
For a quick intro to submodules, please consult the [official documentation](https://git-scm.com/book/en/v2/Git-Tools-Submodules).

Also, note that the ABINIT test suite is submodule-aware.
A test that requires a submodule, must declare it in the files section with the syntax.

```
#%% [files]
#%% use_git_submodule = MgO_eph_zpr
```

In the ABINIT input file, the location of the precomputed file can be passed via strings, e.g.:

```
getden_filepath "MgO_eph_zpr/flow_zpr_mgo/w0/t0/outdata/out_DEN.nc"

structure "abifile:MgO_eph_zpr/flow_zpr_mgo/w0/t0/outdata/out_DEN.nc"
```

At runtime, runtests.py will create inside the working directory of the test a symbolic link 
that points to e.g. ~abinit/tests/modules_with_data/MgO_eph_zpr/

For an example, see tutorespfn/Input/teph4zpr_3.in

    git submodule update --remote --init

[submodule "tests/modules_with_data/MgO_eph_zpr"]
	path = tests/modules_with_data/MgO_eph_zpr
	url = https://github.com/abinit/MgO_eph_zpr.git
	branch = master

To add a new submodule, use:

    git submodule add https://github.com/abinit/MgO_eph_zpr.git

that will add the following section to ~abinit/gitmodules.

[submodule "tests/modules_with_data/MgO_eph_zpr"]
	path = tests/modules_with_data/MgO_eph_zpr
	url = git://github.com/abinit/MgO_eph_zpr.git
	branch = master



This tutorial describes how to produce test-coverage metrics for C/C++ projects.  We start by instrumenting an autotools build to produce `gcov` output with `gcc`; we then generate test coverage artifacts by running our test suite, and finally we explore our results using `lcov`.

### Introduction

We'd like to assess the quality of the existing test suite for each Product Strategy projects.  A measurement of *test coverage* will tell us what part of the project's code is "covered" or exercised by its tests.  50% is a start; 80% would be a good goal for a neglected project; one rarely encounters 100% test coverage, but we'd like to get as close as we can.  Initially we'll use these findings to gain an overview of the test-quality of each project; ultimately these metrics will guide the improvement of our codebase, and enable us to monitor our progress using Jenkins and associated open-source tools.

### A Three-Part Process in Several Steps

We'll enable test-coverage of a particular C/C++ project in a three-part process:

* enabling a special build
* running the tests
* studying the output

Our first step will be to enable a special build.  Ultimately this just means adding a few flags to our `gcc` invocation, but it never seems so straightforward with autotools projects ;) .  The Product Strategy Quality team has made available a set of files to facilitate this build--please check these out now if you'd like to follow along.

    bzr co lp:~allanlesage/coverage-tutorial

Inspecting this archive you'll find 

* __`gcov.m4`__: an `autoconf` macro which will check for relevant tools
* __`Makefile.am.coverage`__, which includes our coverage-enabled `automake` targets
* an old revision of __`dbus-test-runner`__
* a copy of this tutorial

Before we start let's make sure we have `lcov` installed.  `lcov` incoporates the GNU tool `gcov` and produces pretty HTML reports of our coverage results.

    sudo apt-get lcov

(Note that the Debian `lcov` package includes `genhtml`.)  We're ready to begin.


##### 1.  Branch a project of interest

I heard you like testing, so for this example we'll test __dbus-test-runner__ which is a runner which runs tests against the dbus messaging system.  An old revision is included in our coverage-tutorial archive; if you want to start fresh (or on a different project), you would

    bzr branch lp:dbus-test-runner


##### 2.  Install the `gcov.m4` `autoconf` macro and invoke it

Open `gcov.m4`: here's where we're defining the compiler flags and checking for necessary tools.  Let's put this file where `autoconf` can find it.

Your project may already have a directory for `m4` macros:

    $ grep AC_CONFIG_MACRO_DIR *
    configure.ac:AC_CONFIG_MACRO_DIR([m4])

If you find such a directive, simply copy the `gcov.m4` file into the named directory.  If not, create an `m4` directory and copy the `gcov.m4` file there--and don't forget to include a reference to the directory in the body of `configure.ac` (it'll look like the `grep` result above).


##### 3.  Massage `configure.ac` to include our necessary coverage flags.

This is the essential move of our special build.  `gcc` supports coverage reporting with the addition of a few compiler flags.  Here they are in the `gcov.m4` file:

    # Remove all optimization flags from CFLAGS
    changequote({,})
    CFLAGS=`echo "$CFLAGS" | $SED -e 's/-O[0-9]*//g'`
    changequote([,])
    # Add the special gcc flags
    COVERAGE_CFLAGS="-O0 -fprofile-arcs -ftest-coverage"
    COVERAGE_CXXFLAGS="-O0 -fprofile-arcs -ftest-coverage"
    COVERAGE_LDFLAGS="-lgcov"

When the tests are run, `--fprofile-arcs` produces a tally of the execution of each *arc* of the code--one `.gcda` file for each source file.  The `--ftest-coverage` flag produces `.gcno` files, which link an *arc* to a source line so that we can see which lines were touched.  We'll watch these files being produced in a little bit; you can read about the flags at length in the [GNU documentation](http://gcc.gnu.org/onlinedocs/gcc/Debugging-Options.html).  Note that we also remove optimization with the `-O0` flag to get more precise results.

Now here's the step which requires knowledge both of your code and little bit of `autoconf`.  We've included the flags above (in step 3), but we'll now need to edit our `configure.ac` to make the flags available to our build.  For dbus-test-runner the diff looks like this:

    === modified file configure.ac
    --- configure.ac  2010-12-08 02:35:12 +0000
    +++ configure.ac  2011-12-06 21:42:04 +0000
    @@ -45,6 +45,16 @@
     AM_GLIB_GNU_GETTEXT
     
     ###########################
    +# gcov coverage reporting
    +###########################
    +
    +m4_include([m4/gcov.m4])
    +AC_TDD_GCOV
    +AC_SUBST(COVERAGE_CFLAGS)
    +AC_SUBST(COVERAGE_CXXFLAGS)
    +AC_SUBST(COVERAGE_LDFLAGS)
    +
    +###########################
     # Files
     ###########################

And then having added these flags to the build process, we need to actually *actually* add them to the build proper:

    === modified file src/Makefile.am
    --- src/Makefile.am     2009-12-07 21:00:43 +0000
    +++ src/Makefile.am     2011-12-06 21:42:04 +0000
    @@ -3,6 +3,8 @@
     
     dbus_test_runner_SOURCES = dbus-test-runner.c
     dbus_test_runner_CFLAGS  = $(DBUS_TEST_RUNNER_CFLAGS) \
    +                         $(COVERAGE_CFLAGS) \
                              -DDEFAULT_SESSION_CONF="\"$(datadir)/dbus-test-runner/session.conf\"" \
                              -Wall -Werror
     dbus_test_runner_LDADD   = $(DBUS_TEST_RUNNER_LIBS)
    +dbus_test_runner_LDFLAGS = $(COVERAGE_LDFLAGS)

Here we risk running afoul of the autotools "ancient ones".  This is an uncomplicated project--if you're not getting the results you want in the next step, I can recommend [*The Goat Book*](http://sourceware.org/autobook/autobook/autobook_40.html#SEC40) as a decent tutorial on how these macros work.

##### 4.  Verify that `autoconf.sh` `--enable-gcov` generates `.gcno` files

Having enabled our special build with the `gcc` compiler flags, let's verify that `autoconf` is generating the `.gcno` files we've asked for:

    $ ./autogen.sh --enable-gcov
    ...
    $ find -name *.gcno
    ./src/dbus_test_runner-dbus-test-runner.gcno

If all goes well we'll find a `.gcno` for each `.o` file compiled.  If not, then our flags aren't being respected--maybe something's wrong with our `configure.ac` addition in step 3.  It's important to have these checkpoints along the way to divide the autotools process into testable pieces.

##### 5.  Install `Makefile.am.coverage`

So now that we have the instrumentation we need, we'll use a special build to run the tests.  `Copy Makefile.am.coverage` into the top-level directory.

    cp ../Makefile.am.coverage .

Inspection shows that this file defines some extra make targets to generate coverage results using `lcov`.  We'll alert automake to this by simply including the file--here's the diff for dbus-test-runner:

    === modified file Makefile.am
    --- Makefile.am   2009-04-23 21:19:56 +0000
    +++ Makefile.am   2011-12-19 18:00:38 +0000
    @@ -1,1 +1,3 @@
     SUBDIRS = data src tests po
    +
    +include $(top_srcdir)/Makefile.am.coverage


##### 6.  Verify that make check generates `.gcda` files

Now we'll actually run the tests.

    $ make coverage-html
    ...
    $ find -name *.gcda
    ./src/dbus_test_runner-dbus-test-runner.gcda

Again these `.gcda` files represent a tally of 'arcs' through the code.  Note that we can generate these files not just while running tests, but also during normal execution--you can see the kinship with profiling.  `lcov` (via `gcov`) has used these tallies with our `.gcno` files to produce line-by-line results which we'll study in a moment.  Here's what the test coverage looks like for dbus-test-runner:

    Overall coverage rate:
      lines......: 78.8% (215 of 273 lines)
      functions..: 86.4% (19 of 22 functions)
      branches...: 61.2% (60 of 98 branches)

For a small project these figures show that we have a good test-suite to build on.  We're curious about what we're missing, though, so we'll investigate below.  Before you register a comment about "no coverage results", recognize that you may actually have zero code coverage.  But at least you're able to measure it, riiight?

##### 7.  Have a look at the `lcov` pages

`lcov` has produced a coverage-results directory at the top-level of our project; open index.html with a web browser and explore.

![lcov index](./lcov_index.jpg "lcov index")

At a glance our line coverage is 78.8%, and our "test-coverage progress bar" is yellow.  Be aware of the difference between the offered metrics:

* __line coverage__: how many lines have our tests touched
* __function coverage__: how many functions have our tests touched 
* __branch coverage__: for the graph which describes all possible paths of control through this file--especially through conditionals, e.g.--what percentage has our tests touched

In our opinion the killer feature of the `lcov` output is the source-file display, which shows which lines weren't touched by tests.  Drill into the source directory to see the results for a particular file.

![lcov detail](./lcov_detail.jpg "lcov detail")

Here in `dbus-test-runner.c` line 93 we've hit the line that tests the status of `G_IO_STATUS_NORMAL` 61 times, but never taken the path in the line below, for which `G_IO_STATUS_NORMAL` is false.


### Conclusion

We've presented an introduction to test coverage, featuring the `gcc` toolchain and autotools--as demonstrated with a set of utilities we've found useful here in the Quality group.

I hope it's obvious that this tutorial is just the beginning--not just of applying these methods to our code (we and you), but also of understanding what test coverage means to the improvement of Quality.  Our team has witnessed that having an observable measure like the `lcov` green bars encourages developers to increase test coverage.  However while chasing into a particularly narrow corner of their code, our developers have produced some confounding results which have caused us to doubt the accuracy of the available tools--I hope that Thomas Voss will follow with a post on some of his findings in the future.

Regardless, test coverage will be an important metric for our group for the coming cycles, and the bars will continue to grow greener as Spring approaches. . . .  Meanwhile I'll be interested to learn about your autotools/`gcc`/coverage experiences:

* What `lcov` alternatives have you tried?
* Which hallowed autotools rituals have I profaned in preparing this tutorial?
* What's the highest coverage percentage you've witnessed in a GPL project?
# What is this

Neat is the NEw Abinit Testing tool. In this folder are tools for use in Fortran to produce
structured data output. These data are to be used by the python side of the project in
smart testing.

This folder is for lower level tools that handle conversion of data into YAML
document.
ABINIT users scripts
====================

A collection of scripts contributed by various users.
The scripts are organized as follow:

- configure
    To ease configuration/compilation of ABINIT.

- post_processing
    To plot, extract or modify output data.

- pre_processing
    To check, visualize or manipulate input data.

- deprecated
    Scripts that are not documented or suspected to be broken.
    Will eventually be removed. If you think a deprecated script
    should not be kept, contact the developers via the forum.

HOW TO CONTRIBUTE
-----------------

Before commiting a new script to this directory, please make sure
it contains full documentation. If your script is a single file,
include comments at the head of the file. If you want to add a
collection of scripts, you can gather them in a sub-directory and
include a README file.

Proper documentation should contain:

- A one-liner description of the program.
  (to be added to the local README file)

- A literal description of the program.

- The data on which the script operates. 

- Usage (how to feed the inputs to your script.)

- The output data produced.

- Author information (name and email address).


# Installation notes for ABINIT

This page provides an introduction to the installation of the
ABINIT package and the compilation of the executables.
It also discusses how to test whether the compilation was successful by running the internal test suite.
Finally, it gives related complements for the developers.

Any comment or suggestion to improve the procedure or this page is welcome!
Simply contact the ABINIT group on the [discourse forum](https://discourse.abinit.org).

## Overview

For the vast majority of people willing to use ABINIT (simple users -not developers-, with Unix/Linux or MacOS
working in the terminal), the installation/compilation steps are:

  1. **Prerequisite**: you need a Fortran compiler, a C compiler, the Python interpreter (>= 2.7.5),
     some mandatory libraries (**BLAS/LAPACK**, **FFT**, **NetCDF4**, **HDF5** and **LibXC**),
     possibly some recommended libraries (MPI) and other optional libraries such as Wannier90.
     The libraries can be installed with the help of the "fallback" procedure, see below for more info on this step.
     Alternatively, you may want to install everything from source using the procedure detailed in
     this [tutorial](/tutorial/compilation).
  2. Get the [latest version of the ABINIT package](https://www.abinit.org/packages) (abinit-x.y.z.tar.gz)
     from the abinit web site.
     More information are available [here](#how-to-get-a-version-of-abinit).
  3. Prepare a configuration file named "hostname".ac9, that contains the information
     about libraries and compilation options.
     This step is not mandatory as the ABINIT *configure* script will try to detect the installation location
     if options are not provided but the automatic detection procedure can fail.
     See below for more information, as well as [this section](#how-to-write-the-hostnameac9-file).
  4. Issue `./configure` (or, even better, first create a *tmp* directory for the build
     then `cd tmp` and finally run `../configure` within the build directory).
     For further details, consult [this link](https://wiki.abinit.org/doku.php?id=build:configure).
  5. Issue `make` (or `make -jN` for compiling with N processors, e.g. `make -j4`
     to use four processors). This step might take dozen of minutes depending on the compilation options.
     More information are available [here](#how-to-make-the-executables).
  6. Optionally, issue `make install` to install the package (root privileges are needed if the installation direction
     is not specified via `--prefix`).

Note that the details of step 1 and 3 might vary significantly depending on the operating system.
Further details are provided by 
[this tutorial](/tutorial/compilation) that cover the scenario in which you want to build everything from source
and install libraries in your $HOME directory.
There are also other pages focusing on
[macOS](/INSTALL_MacOS), [CentOS](/INSTALL_CentOS) and [Ubuntu](/INSTALL_Ubuntu)
that discuss how to bypass the compilation of the external libraries
using homebrew or MacPorts (for macOS), dnf (for CentOS) or apt (for Ubuntu).

Examples of configuration files to compile Abinit on clusters are available
in the |abiconfig| package on github
(specifically the [directory for ABINITv9](https://github.com/abinit/abiconfig/tree/master/abiconfig/clusters)),
while the configuration files
used for our buildbot testfarm are available in the [autoconf_examples section](/developers/autoconf_examples/).
The [ABINIT Wiki](https://wiki.abinit.org) also has a **build abinit** section, that might be useful.
In particular, the current documentation for the fallback procedure is available
[here](https://wiki.abinit.org/doku.php?id=build:fallbacks) while configuration options are documented
[in this page](https://wiki.abinit.org/doku.php?id=build:configure).

If you succeed to build the executables, and would like to check whether
the executables work properly, please consult the two sections on [Internal tests](#how-to-run-the-internal-tests)
and [Automatic tests](#how-to-make-the-automatic-tests).

If you want to have a much better handling on the ABINIT source code than normal users, or if
you downloaded ABINIT from Gitlab or GitHub anyhow, then consult the section [For developers](#for-developers).

## How to get a version of ABINIT?

We assume that you have a F90 compiler under UNIX/Linux or macOS X and that you want to
**compile the source files**, and, perhaps, **modify and/or add
new files**. This is the typical scenario for most users and system administrators.
In what follows, _x.y.z_ represents the ABINIT version.

In order to get the ABINIT package, you have first to download the file
**abinit-_x.y.z_.tar.gz** from the
[packages webpage](https://www.abinit.org/packages) of the ABINIT Web site, then issue:

```sh
gunzip abinit-x.y.z.tar.gz | tar -xvf -
```

Alternatively, you may use

```sh
wget https://www.abinit.org/sites/default/files/packages/abinit-x.y.z.tar.gz
tar xzf abinit-x.y.z.tar.gz
```

If correctly done, a main directory, denoted ~abinit in the present document
(usually, its real name will be **abinit-_x.y.z_**) and a whole set of
subdirectories should have been created, including:

  * the source files inside the **_src_** directory;
  * a tarball with the fallbacks and a script to create libraries in the directory **_fallbacks_**;
  * the documentation in the **_doc_** directory;
  * the complete set of tests and the pseudopotentials needed for the automatic tests in the directory **_tests_**;
  * all the scripts and configuration options needed to generate *configure* and Makefiles
    in the **_config_** directory.

Obviously, the package does not contain object files and binary executables that will be built by *make*.
Note also that pseudopotentials for production runs **are not shipped** with the package.
The tarball contains several dozen pseudopotentials **for testing purposes** in ~abinit/tests/Psps_for_tests.
The largest set of pseudopotentials and PAW atomic datafiles for production runs can be found
on the [ABINIT Atomic data pseudopotentials and PAW datasets Web page](http://www.abinit.org/psp-tables).
Other pseudopotentials have been generated by many different users, and might be
shared but you might have to contact them.

The web site <https://www.abinit.org> contains many other resources including links to the forum,
the mailing list, the ABINIT events etc and, last but not least, tutorials and documentation pages.

## How to compile the executables?

We now suppose that you have a F90 compiler and that you want to compile the source files.

In most cases, you will have to provide to the *configure* utility some
information: the location of the F90 compiler (and sometimes even the C
compiler), the adequate compiler options, and, if you want to
produce the parallel binaries, the location of the MPI library.

Although the present building tools should be powerful enough to
succeed to build the binaries without you giving such information,
on a significant number of platforms it has been observed that it is still better to specify
options explicitly in order to avoid suboptimal executables or downgraded capabilities.

Supposing that you are in the lucky case in which the build system is able detects all the requirements,
then building ABINIT is rather simple.
Issue:

  * `./configure` (or first create a tmp directory, then `cd tmp`, then `../configure`).
  * `make` (or `make -jN` for compiling with N processors on a SMP machine, e.g. `make -j4` for four processors).
    Might take two minutes.
  * Optionally, `make install`

Well, it might also be that only one additional information is needed, in which case something like:

```sh
configure FC=gfortran
make
```

might work. In both cases, let's explain a bit what is done, and the further possibilities.

The *configure* step produces the set of Makefile files (among other things),
taking into account information about your machine and the "hostname".ac9 file.
It takes three minute or less. The *make* step compiles everything,
according to the Makefile files produced in the previous step.
The executables will be located in the subdirectory ~abinit/src/98_main, if
you have chosen to issue ./configure in the ~abinit directory. If you have
issued ./configure in another directory, it will be placed accordingly.

!!! important

    The time required to build
    everything is highly dependent on the compiler and platform. On a 2.8 GHz
    quad-proc machine (using *make -j4*), the whole compilation is about 10 minutes.
    On other platforms, with only one processor, it might be more than one hour.

The *make* command accepts the name of the target(s) as optional argument.
To compile only one of the executable after the configure step, issue

```sh
make name_of_the_binary
```

where *name_of_the_binary* can be *abinit*, *cut3d*, *anaddb*, etc.

Also,

```sh
make install
```

will install the package in the /usr/local directory.

## How to write the "hostname".ac9 file?

Let's come back to the case where the build system needs some more
information. This information should be stored in a file named "hostname".ac9,
where "hostname" is the result of executing the command `hostname -s`.

!!! tip

    Note that the command `hostname` by default returns the fully qualified domain name (FQDN),
    e.g. abiref.pcpm.ucl.ac.be,
    while only the first word of the returned chain of character is needed, e.g. abiref.
    This is the reasone why we had to use `hostname -s`.

There is a template for such "hostname".ac9 file, located in ~abinit/doc/config. 
Its name is *config-template.ac9*. 
Examples of such files, that are used for testing the package on our testfarm, 
can be found in ~abinit/doc/build/config-examples,
or equivalently in the [autoconf_examples section](/developers/autoconf_examples/).
Additional examples of configuration files for clusters are provided by the *abiconfig* project 
and are available [here](https://github.com/abinit/abiconfig/tree/master/abiconfig/clusters).

Most of the examples provided in the ~abinit/doc/build/config-examples
directory contain five important variables: location of the F90/C compilers, 
F90 and C compilation options, location of MPI library (if enabled).
On the other hand, there are many other additional control flags ("with_XYZ"),
needed for advanced use.

Your hostname.ac9 file might be placed in your home directory inside a new
directory named ~/.abinit/build. 
If you opt for this solution, every time you
install a new version of ABINIT, the configuration file will be automatically used by
the configure script so you do not have to care anymore about this file after the first installation.

On the other hand, if you want to play with several configurations, 
you can place the hostname.ac9 file in the ~abinit directory, where such a
hostname.ac9 file will be also seen by the build system (and preferred over the
one located in ~/.abinit/build) or inside your build directory (like ~abinit/tmp). 
As mentioned above, you might even input the options contained in the hostname.ac9 file
directly on the command line.

Note the order of precedence for the location of the hostname.ac9 file (or
command-line interface), in case more than one possibility is used
(decreasing order of precedence):

  * Command line (overcome all other information)
  * Your build directory (~abinit/tmp)
  * The ABINIT top source directory (~abinit)
  * ~/.abinit/build
  * /etc/abinit/build

When the hostname.ac9 file is ready, you can come back to the configure/make sequence.

## How to run the internal tests

The abinit code has several small internal tests (three basic ones, called *fast*, *v1* and
*v5*, and then one for each of the libraries *bigdft*, *etsf_io*, *libxc*,
*wannier90*), that can be issued automatically, and that check automatically whether the results are correct. 
These tests are available whether you have got the package from the Web or from the ABINIT
archive. Of course, you need to have compiled abinit in order to run the
internal tests. Moreover, the simple implementation procedure assumes that the
executable is located in ~abinit/src/98_main (the standard location after issuing *make*).

You can begin with the *fast* suite. Simply issue the command:

```sh
make test_fast
```

It will run for a few seconds. It should print:

```text
Status file, reporting on built-in test fast

==> The run finished cleanly.
    Moreover, comparison of the total energy, and other (few) relevant quantities with reference values has been successful.
    This does not mean that no problem is present, however.
    Please run the complete set of ABINIT tests to gain a better confidence in your installation.
```

This means that the internal *fast* suite ran successfully. If you do not get
this message, then the executables were not properly generated, or there is a
problem with the makefile that drives the internal test. In this case, after
having tried to solve the problem by yourself, you should contact somebody in
the ABINIT group.

In addition to this small message, you can have access to all generated files,
that are located inside the tests/built-in/Input subdirectory.

Supposing the *fast* tests are OK, then you might issue the command:

```sh
make tests_in
```

The *fast* tests will be executed once more, followed by the other internal tests.
Again, we hope that you will get the positive diagnostics for the other tests.
Of course, the *bigdft*, *etsf_io*, *libxc*, and *wannier90* needs the
appropriate library to be installed in order to work properly.

For further information on these internal tests, see the ~abinit/tests/built-in/README file.

You might now read the [new user's guide](/guide/new_user), in
order to learn how to use the code, and then follow the four basic
tutorials, see the [entry page for the tutorials](/tutorial/).
This is useful if you consider that the installation has been successful. Or
you might continue to read the present Web page, and try to perform the speed
tests, as well as the other tests.

## How to execute the automatic tests

The workhorse script to run the test suite is called *runtests.py*.
It is very flexible.
A reasonable set of tests (those contained in the fast and v"x"
directories), can be executed automatically by issuing inside the ~abinit/tests directory the command:

```sh
./runtests.py
```

or e.g.

```sh
./runtests.py -j4
```

to run the test suite in parallel with 4 cores.

This is the recommended procedure for developers. In order to execute these
tests, you will need a larger disk space than for the simple installation of
the code (the total additional disk space required is on the order of 1GB).

The command line options of *runtests.py* are numerous. Please issue

```sh
./runtests.py --help
```

in order to access the documentation with the examples or consult the video below.

[![asciicast](https://asciinema.org/a/40324.png)](https://asciinema.org/a/40324)

Let us now examine the different subdirectories of the ~abinit/tests directory.

~ABINIT/tests/fast is the simplest, and its content will be described in some
detail below. For tests of the parallel version see the directory tests/paral. For tests
of the response function features of abinit, and for tests of mrgddb and
anaddb, see the subdirectories tests/v2. The other directories tests/v3,
tests/v4, etc present further tests of recently implemented features of ABINIT.

**1) tests/fast** (for the sequential version only)

This subdirectory contains a basic set of tests of the code, aimed at testing
whether the code is coherent in time (successive versions), and exercising
several parts of the code. However, they do not examine its accuracy on
physical problems, mainly because the number of plane waves is too small,
and some tests are not run to self-consistent convergence. 
32 MB of memory should be enough for these tests.

The input files for each of the tests can be found in the
~abinit/tests/fast/Input directory. At the bottom of each input file, 
there is a section with metadata and parameters defining the test. 
Such metadata mentions the executable to be
used, the output files to be analyzed, the admitted tolerances with respect to
reference output files, the author of the test and a brief description of the test.

To run only the tests in this directory, simply issue

```sh
./runtests.py fast
```
in the ~abinit/tests/ directory.

The script will create a directory named *Test_suite*. All the results will be stored in that
directory. The output files are be automatically compared  to a set of reference files (in ~abinit/tests/fast/Refs)
thanks to the GNU 'diff' utility.
The corresponding difference files are prefixed by 'diff.'.

In addition to 'diff', there are two other levels of automatic analysis: one
based on a python diff tool called 'fldiff', producing 'fldiff.report' files,
and another where the output of 'fldiff' is further analyzed to produce a
brief report called 'report'. The latter step is only performed in case all
the tests cases of one directory are performed (including the case where tests
of different directories are performed).

The one-line summaries produced by fldiff (see later) are compared with the
tolerances indicated in the input file (metadata added at the end of the input
file). This procedure produces a file called "report", in which there is a one
line assessment of the behaviour of each test: succeeded (everything is OK),
passed (the test is OK for users in production), passed marginally (the test
is within 1.5 of the usually accepted deviation, which is likely OK for most
applications - still to be improved by the development team, though), failed
(there is a problem, the deviation is usually not accepted). This is by far
the most convenient tool to analyze the automatic tests of abinit.

The vast majority of tests cases succeed or pass on all platforms that are
used by the developer team in Louvain-la-neuve. Some problems are mentioned in
the file ~abinit/KNOWN_PROBLEMS. Additionally, there might be specific
problems for some test case for some platforms, also mentioned in
~abinit/KNOWN_PROBLEMS. So, apart from the known problems, mentioned in this
file, the "report" file should mention, for each test case, only "succeeded" or "passed".

The comparing tool 'fldiff' -for 'floating diff'- performs in a more detailed
way the comparison of floating numbers between the output files and the
reference files than in the case of a 'diff' command. As used presently, for
each run inside one directory, one single file, called 'fldiff.report', will
be produced, and gather the analysis for all tests in that directory.

If for one test case, the two files differ by the number of lines, the
'fldiff.report' file will report that it cannot compare the two files. Usually
this problem will be seen at the level of 'command signs' appearing sometimes
in the first column of the output files, so a typical error message
(announcing something went wrong) will be:

    Case_1
    22
    The diff analysis cannot be pursued: the command sign differ.

By contrast, it will identify the floating numbers and ignore their
differences if they are within some prescribed tolerance, or if the difference
is not relevant. For example, it is able to ignore the differences in timings.
If everything goes fine for a test, fldiff should identify only the differences in:

  * the dates of execution (possibly);
  * the version numbers (possibly);
  * the platform description (possibly);
  * the overall execution time (this is ALWAYS printed, even without differences).

So, a successful execution of one test case may be announced as follows in the fldiff.report file:

    Case_1
    2
    <  Version 8.0.6  of ABINIT
    >  Version 8.0.3  of ABINIT
    5
    <  Starting date: Mon 23 May 2016.
    >  Starting date: Mon  4 Apr 2016
    202
    < +overall time at end (sec): cpu=          7.1  wall=          8.0
    > +Overall time at end (sec): cpu=          7.3  wall=          8.0
    Summary Case_1: no significant difference has been found


The fldiff.report file will have one such section for each test_case that was
run. It begins with the number of the test case, then includes a few blocks of
three lines: the number of the line where something is happening, followed by
the content of the two lines. Finally, there is a one-line summary for each test case.

More information on the fldiff script can be found in the ~abinit/tests/Scripts/fldiff.pl file.

**2) tests/v1**

This directory contains tests built in the same spirit as those in the
test/fast directory, but that exercise other basic features, like the
treatment of metals, the GGA, the new pseudopotentials, the multi-dataset
mode, the cell parameters optimization, and the spatial symmetry groups
database. These were developed during the development time of the version 1 of
ABINIT. Of course, the automatic difference procedure only compares to recent
runs of the ABINIT code. As for the 'fast' test cases, the fldiff.report and
report files are also available. 64 MB of memory should be enough for these tests.

**3) tests/v2**

This directory contains tests built in the same spirit as those in the
tests/fast/ or tests/v1 directory, but that exercise features not present in
version 1 of the ABINIT package, mainly the response function features, and
the use of the mrgddb and anaddb codes. Again, the automatic difference
procedure only compares to recent runs of the ABINIT code. As for the 'fast'
test cases, the fldiff.report and report files are also available. 64 MB of
memory should be enough for these tests.

**4) tests/v3, tests/v4, tests/v5, tests/v6, tests/v67mbpt, tests/v7, tests/v8, tests/v9, tests/bigdft,
    tests/etsf_io, tests/libxc, tests/wannier90**

These directories contain tests built in the same spirit as those in the
tests/fast/, tests/v1, tests/v2 directory, but that exercise features not present in the
version 1 or 2 of the ABINIT package, noticeably the
use of the GW code, the utilities Cut3d, AIM, .., the PAW ... . Or the
interfacing with fallbacks. Again, the automatic difference procedure only
compares to recent runs of the ABINIT code. Like for the 'fast' test cases, the
fldiff.report and report files are also available. 64 MB of memory should be
enough for these tests.

**5) tests/paral/ and tests/mpiio/** (need MPI support)

This directory contains tests built in the same spirit as those in the
test/fast/ directory, but that exercise the parallel version of the ABINIT code.

The script runtests.py considers one of the different input files, and for
this file, it will use the parallel code with one processing node, then
perform different parallel runs with an increasing number of processing nodes,
as specified in the metadata contained in the input file. As for the other
series of test, the diff and the fldiff utilities are used automatically, and
fldiff.report and report files are produced automatically.

## For developers

Note the documentation available in the pages labelled [Developers](/developers/git_and_gitlab/),
as well as the [developer's corner of the Wiki](https://wiki.abinit.org)
The following sections are complements for the installation from gitlab, and the generation of the
ABINIT distribution.

### Git, autotools and makemake

If you want to have a full handle on the package (compilation, modification of files, writing
of scripts, you need additional prerequisites, the
(free) software applications *git*, *automake*, *autoconf*, *libtools*.

More explicitly, you need minimally (version numbers can be upgraded)

 * [GNU Autoconf 2.69](ftp://ftp.gnu.org/gnu/autoconf/autoconf-2.69.tar.gz)
 * [GNU Automake 1.15](ftp://ftp.gnu.org/gnu/automake/automake-1.15.tar.gz)
 * [GNU Libtool 2.2.4](ftp://ftp.gnu.org/gnu/libtool/libtool-2.2.4.tar.gz)
 * [GNU M4 1.4](ftp://ftp.gnu.org/gnu/m4/m4-1.4.4.tar.gz)

If you do not have these tools, please consult your local computer guru, and/or the following pages:

  * [An overview of ABINIT development](https://wiki.abinit.org/doku.php?id=developers:overview)
  * [10 steps to hike ABINIT](https://wiki.abinit.org/doku.php?id=developers:hike)
  * [Buildbot and the test farm](https://wiki.abinit.org/doku.php?id=bb:overview)

If you want to develop on a regular basis, please have a Git(lab) access
created for you by contacting Jean-Michel Beuken, as described in these pages.
If it is only very occasional, you might as well rely on the [ABINIT Github Web site](https://github.com/abinit).

It is strongly advised to subscribe to the [ABINIT discourse forum](https://discourse.abinit.org/)
to receive the latest information concerning new developments.

After having installed git, and obtained a gitlab branch on the ABINIT internal server, 
create an autonomous copy of the source code, on top of which you have to make your development.
This is explained in the ABINIT wiki
[gitlab: ABINIT specificities](https://wiki.abinit.org/doku.php?id=developers:git:specificities_git_abinit)

For your gitlab branches on the internal server, you will have the
permission not only to clone/fetch/pull, but also to commit/push your
modifications. You might alternatively download other branches of the
archives, but you will not be able to commit to these branches. So, do not
start to modify these, you will not be able to include them afterwards in the archive.

Working with `git clone` creates a local archive for your daily work, this
archive being linked to the main ABINIT archive. This very efficient technique
is recommended as it makes you more independent for the management of your
work (you will be able to create new branches). One big advantage of this
approach is that people working with a laptop can develop and commit safely
without a network connection.

Then, make sure you have the ABINIT prerequisite (compilers, libraries etc), and that you
have set up an ac9 file.

At this stage, before being able to compile, cd to the newly created abinit directory, and issue:

```sh
./config/scripts/makemake
```

This command initializes a whole set of files and scripts, needed for the
autotools, as well as for the global work on ABINIT sources. 
This initialization might take up to two minutes.
After this initialisation, you can proceed with the configure/make procedure
as described in section 2.

### How to generate the source package

If you want to produce the source package abinit-_x.y.z_.tar.gz, after having compiled and tested ABINIT, issue:

```sh
make dist
```

inside ~abinit/.
TODO list for the documentation of ABINIT
=========================================

The first version of this TODO was written by Yann.
Several points have been adressed in the mkdocs version but there
are still points worth discussing

* General remarks:

    * In prevision of the Autotools support and packaging, the documentation
    should be reorganized in relevant sections => needs discussion.
    * Copyright and history information might be inserted as comments.
    * To be discussed: which files should exist in one format only, and which
    ones should be multi-format?
    * Dependencies should be stored somewhere, to known what to update and when.

* aim_help.html:

    * Is it up-to-date?

* aimhelp.tex,aimhelp.ps:

    * Do not match aim_help.html => which is the right one?

* anaddb_help.html:

    * Is it up-to-date?

* band2eps_help:

	* Can barely be called documentation (and executable) => restart from scratch.

* check_list:

	* To whom is it truly useful? 
        MG: Xavier.
        @Xavier: Could you move the file somewhere>

* conducti_manual.tex:

	* Is it complete and accurate?
	* Is it up-to-date?


* contributors:

	* Format better the file.
	* Remove copyright notice.

* cut3d_help:

	* Is it complete and accurate?
	* Is it up-to-date?

* cut3d_help.html:

	* Does not match cut3d_help => which is the right one?
	* Remove links at bottom.
	* Remove copyright notice.

* ddbs.upgrade:

	* Name: what about ddbs-upgrade-howto?
	* Reformat for markdown.

* elphon_manual.ps:

	* Violation of the GPL: where is the source? Should be relicensed?

* format_KSS:

	* Is it complete and accurate?
	* Is it up-to-date?
	* Reformat for markdown.

* gwmanual.txt:

	* Is it complete and accurate?
	* Is it up-to-date?
	* Reformat for markdown.

* known_problems.x.y.z:

	* Should be generated automatically from a database.
	* Should be presented in reverse-chronological order.

* make_help, make_dev_help, make_targz_help, makefile_macros_help:

	* Are they up-to-date?
	* Will be changed by the Autotools support.

* mrgddb_help.html:

	* Barely formatted.
	* Prepare text version for markdown.
	* Remove links at bottom.
	* Remove copyright notice.

* new_user_guide.html:

	* Is it complete and accurate?
	* Is it up-to-date?

* other_codes:

	* Probably out-of-date.
	* Set-up a database?

* paral_use:

	* Is it complete and accurate?
	* Is it up-to-date?

* piezoelectric.txt:

	* Only notes for now, will evolve a lot.
	* Reformat for markdown.

* planning:

	* Is it complete and accurate?
	* Is it up-to-date?
	* Reformat for markdown.
	* Remove copyright notice.

* problem_report:

	* Is it up-to-date?
	* Reformat for markdown.
	* Remove copyright notice.

* respfn_help.html:

	* Is it complete and accurate?
	* Is it up-to-date?

* spacegrouphelpfile.html, spgrdefinition.html, spgrdescription.html, spgrhead.html, spgrcopyright.html:

	* Should be rewritten. DONE by MG

* tuning:

	* Is it complete and accurate?
	* Is it up-to-date?

* welcome.html:

	* Is it complete and accurate?
	* Is it up-to-date?
	* Prepare text version for markdown (may take time).
	* Remove links at bottom.
	* Remove copyright notice.

* Features/features*:

	* File extensions vary => choose only one.
	* Barely formatted, almost plain-text.
	* Reformat for markdown.
	* Remove copyright notice.

* Images/*:

	* Only images for the tutorial => nothing to do.

* Installation_notes/install*.htm*:

	* File extensions vary => choose only one.
	* The source is not even valid HTML !!!
	* Improve layout.
	* Prepare text version for markdown (may take time).
	* Remove links at bottom.
	* Remove copyright notice.

* Macroave_Docs/README:

	* Ready for markdown.
	* Remove copyright notice.

* Macroave_Docs/macroave.(tex,ps,pdf):

	* Are they complete and accurate?
	* Are they up-to-date?

* Macroave_Docs/macroave.toc:

	* Temporary file => should be removed.

* Miscellaneous/*:

	* Are they useful?
	* Are they complete and accurate?
	* Are they up-to-date?
	* Reformatting: discuss types and priorities.

* Notes_for_coding/*:

	* Are they complete and accurate?
	* Are they up-to-date?
	* Should be reformatted to allow multi-format presentation.

* Presentation/*.tex:

	* Are they complete and accurate?
	* Are they up-to-date?

* Presentation/presentation.pdf.gz:

	* Small file anyway => decompress it.

* Psp_Infos/*.info:

	* All files should be reformatted in order to produce both HTML
	  and PDF versions of the information.

* Release_notes/release_notes*

	* File extensions vary => choose only one.
	* Plain-text version required by GNU coding standards.
	* Barely formatted, almost plain-text.
	* Reformat for markdown.
	* Remove copyright notice.

* Theory/*:

	* Are they complete and accurate?
	* Are they up-to-date?
	* Find more explicit names.

* Tutorial/*.html:

	* welcome.html: Rename it to something like summary.html?
	* The source is not even valid HTML !!! => fix it.
	* Upgrade to XHTML.
	* Should be part of a PDF manual too.
	* Remove links at top and bottom.
	* Remove copyright notice.


* README_conda with ${CONDA_ENV}
---
authors: DCA, XG, RC
---

# New user help file  

This page gives a beginner's introduction to the ABINIT resources, 
the package, and the main ABINIT applications.  

## Foreword
  
The ABINIT project is a group effort of dozens of people worldwide, 
who develop the main ABINIT application 
which is delivered with many other files (post-processors, tests, documentation, ...) in the ABINIT package. 
The ABINIT project includes also resources
provided on the [ABINIT Web site](https://www.abinit.org) and 
the [github organization](https://github.com/abinit).

Before reading the present page, and get some grasp about the main ABINIT
application, you should get some theoretical background. In case you have
already used another electronic structure code, or a quantum chemistry code,
it might be sufficient to read the introduction of [[cite:Payne1992]].
If you have never used another electronic structure code or a Quantum
Chemistry package, you should complete such reading by going (at your own
pace) through the Chaps. 1 to 13 , and appendices L and M of R.M. Martin's book [[cite:Martin2004]].

After having gone through the present New User's Guide, you should follow the [ABINIT tutorial](/tutorial/).

## Introduction
  
ABINIT is a package whose main program allows to find the total energy, charge
density and electronic structure of systems made of electrons and nuclei
(molecules and periodic solids) within Density Functional Theory, using
pseudopotentials and a planewave basis, or augmented plane waves, or even wavelets. 

Some possibilities of ABINIT go beyond Density Functional Theory,
i.e. the many-body perturbation theory (GW approximation the Bethe-Salpether
equation), Time-Dependent Density Functional Theory, Dynamical Mean-Field
Theory, the Allen-Heine-Cardona theory to find temperature-dependent electronic structure. 

ABINIT also includes options to optimize the geometry
according to the DFT forces and stresses, or to perform molecular dynamics
simulation using these forces, or to generate dynamical (vibrations - phonons)
properties, dielectric properties, mechanical properties, thermodynamical
properties, etc. In addition to the main ABINIT code, different utility
programs are provided.

We suppose that you have downloaded the ABINIT package from the Web site,
unpacked it and installed it. If not, you might nevertheless continue reading
the present Web page, just to get an overview, but it might prove more
fruitful to have first downloaded the ABINIT package and at least unpacked it,
see the [installation notes](../installation).

!!! note

    We will use the name "~abinit" to refer to the directory that contains the
    ABINIT package after download. In practice, a version number is appended to
    this name, to give for example: abinit-8.8.0. The ABINIT package versioning
    scheme is explained later in this file.

~abinit contains different subdirectories. For example, the present file, as
well as other descriptive files, should be found in ~abinit/doc/.
Other subdirectories will be described later.

## The main executable: abinit
  
After compilation, the main code will be present in the package as
~abinit/src/98_main/abinit (or perhaps at another place, depending on your installation).

To run abinit you need three things:

1. Access to the executable, abinit. 
2. An input file. 
3. A pseudopotential input file for each kind of element in the unit cell. 

With these items a job can be run.

The full list of input variables, all of which are provided in the single
input file, is given in the ABINIT [[varset:allvars|list of all variable]].
The detailed description of input variables is given in many "Variable Set" files, including:

  * Basic variables, [[varset:basic]]
  * Ground-state calculation variables, [[varset:gstate]]
  * GW variables, [[varset:gw]]
  * Files handling variables, [[varset:files]]
  * Parallelisation variables, [[varset:paral]]
  * Density Functional Perturbation Theory variables, [[varset:dfpt]]

A set of examples aimed at guiding the beginner is available in the [[tutorial:index|tutorials]].

Other test cases (more than 1000 input files) can be found in the ~abinit/test
subdirectories, e.g. "fast", the "vX" series (v1, v2, ... v67mbpt, v7, v8),
"libxc", "paral", the "tutoX" series ...

Many different sorts of pseudopotentials can be used with ABINIT. 
Most of them can be found on the [atomic data files](https://www.abinit.org/downloads/atomic-data-files) 
page of the ABINIT web site. 
There are official recommended pseudopotentials tables 
(the PAW JTH table, and the norm-conserving table from ONCVPSP), and also some older sets of pseudopotentials. 
Information on pseudopotential files can be found in the [[help:abinit#5|ABINIT help file]],
the [[theory:pseudopotentials|Pseudopotential theory document]], on the [ABINIT wiki](https://wiki.abinit.org/doku.php?id=developers:pseudos),
and in the [[topic:PseudosPAW|PseudosPAW]] topics.

!!! warning

    A subset of existing pseudopotentials are used for test cases, and are located in the 
    ~abinit/tests/Psps_for_tests directory but they **are not recommended** for production. 

## Other programs in the package
  
In addition to abinit, there are utility programs provided in the package.
Some utility programs are written in Fortran (like the main abinit program), and
their sources is also in ~abinit/src/98_main. 
These include:

mrgddb and anaddb 
:   They allow one to post-process responses to atomic
    displacements and/or to homogeneous electric field, and/or to strain
    perturbation, as generated by abinit, to produce full phonon band structures,
    thermodynamical functions, piezoelectric properties, superconducting
    properties, to name a few. `mrgddb` is for "Merge of Derivative DataBases",
    while `anaddb` is for "Analysis of Derivative DataBases".

cut3d 
:   It can be used to post-process the three-dimensional density (or
    potential) files generated by abinit. It allows one to deduce charge density
    in selected planes (for isodensity plots), along selected lines, or at
    selected points. It allows one also to make the Hirshfeld decomposition of the
    charge density in "atomic" contributions.

fold2Bloch
:   It is used for unfolding first-principle electronic band structures

aim
:   It is also a post-processor of the three-dimensional density files
    generated by abinit. It performs the Bader Atom-In-Molecule decomposition of
    the charge density in "atomic" contributions.

conducti
:   It allows one to compute the frequency-dependent optical conductivity.

Some utility programs are not written in Fortran, but in Python. They are
contained in ~abinit/scripts, where post-processing (numerous tools) and pre-
processing scripts are distinguished. Some allows one to visualize ABINIT
outputs, like abinit_eignc_to_bandstructure.py.

## Other resources outside the ABINIT package
  
In addition to the ABINIT package, other resources can be obtained from the
[ABINIT github site](https://github.com/abinit). The sources of the latest
version of the ABINIT package are actually mirrored on this site, but for
other resources (not in the package) this is the only download point.

[AbiPy](https://github.com/abinit/abipy)
:   is an open-source library for analyzing the results produced by
    ABINIT (including visualisation), and for preparing input files and workflows
    to automate calculations (so-called high-throughput calculations).
    It provides interface with [pymatgen](http://pymatgen.org/), 
    developed by the [Materials Project](http://materialsproject.org/).
    Abinit tutorials based on AbiPy are available in the [abitutorials repository](https://github.com/abinit/abitutorials).

[PseudoDojo](http://www.pseudo-dojo.org/)
:   is a Python framework for generating and validating
    pseudopotentials (or PAW atomic data files). Normal ABINIT users benefit a lot
    from this project, since the ABINIT recommended table of norm-conserving
    pseudopotentials has been generated thanks to it. 
    The recommended PAW table is also provided via the pseudo-dojo interface.

[abiconfig](https://github.com/abinit/abiconfig)
:   is a holding area for configuration files used to
    configure/compile Abinit on clusters. You might benefit from it if you are 
    installing Abinit on a cluster.

[abiflows](https://github.com/abinit/abiflows)
:   provides flows for high-throughput calculations with ABINIT.


[abiconda](https://github.com/abinit/abiconda)
:   contains conda recipes to build Abinit-related packages (like AbiPy). 
    You might benefit from it if you install Abipy on your machine.


In addition to the resources that the ABINIT developer provide to the
community through the ABINIT packages, portal and Github, many ABINIT-independent 
commercial or free applications can be used to visualize ABINIT
outputs or interact with ABINIT. We provide a (not very well maintained) list
of links in [the last section of http://www.abinit.org/sponsors](http://www.abinit.org/sponsors).
Of course, you might get more by browsing the Web.

## Input variables to abinit
  
As an overview, the most important input variables, to be provided in the
input file, are listed below:

**Specification of the geometry of the problem, and types of atoms:**

[[natom]]
:       total number of atoms in unit cell

[[ntypat]]
:   number of types of atoms

[[typat]]([[natom]]):
:   sequence of integers, specifying the type of each atom.
    NOTE: the atomic coordinates ([[xcart]] or [[xred]])
    must be specified in the same order

[[rprim]](3,3)
:   unscaled primitive translations of periodic cell;
    each COLUMN of this array is one primitive translation

[[xcart]](3,[[natom]])
:   cartesian coordinates (Bohr) of atoms in unit cell
    NOTE: only used when [[xred]] is absent

[[xred]](3,[[natom]])
:   fractional coordinates for atomic locations;
    NOTE: leave out if [[xcart]] is used

[[znucl]]([[ntypat]])
:   Nuclear charge of each type of element; must agree with
    nuclear charge found in psp file.

**Specification of the planewave basis set, Brillouin zone wavevector sampling, and occupation of the bands:**

[[ecut]]            
:       planewave kinetic energy cutoff in Hartree

[[kptopt]]
:       option for specifying the k-point grid
        if [[kptopt]]=1, automatic generation, using ngkpt and shiftk.

[[ngkpt]](3)
:       dimensions of the three-dimensional grid of k-points

[[occopt]]
:       set the occupation of electronic levels:
        =1 for semiconductors
        =3 ... 7  for metals

**Specification of the type of calculation to be done:**

[[ionmov]]
:       when [[ionmov]] = 0: the ions and cell shape are fixed
                        = 2: search for the equilibrium geometry
                        = 6: molecular dynamics

[[iscf]]
:       either a positive number for defining self-consistent
        algorithm (usual), or -2 for band structure in fixed potential

[[optdriver]]
:       when == 3 and 4: will do GW calculations (many-body perturbation theory)

[[rfelfd]]
:       when /= 0: will do response calculation to electric field

[[rfphon]]
:       when = 1: will do response calculation to atomic displacements

**Specification of the numerical convergency of the calculation:**

[[nstep]]
:    maximal number of self-consistent cycles (on the order of 20)

[[tolvrs]]
:    tolerance on self-consistent convergence

[[ntime]]
:       number of molecular dynamics or relaxation steps

[[tolmxf]]
:       force tolerance for structural relaxation in Hartree/Bohr

## Output files
  
Output from an abinit run shows up in several files and in the standard output. 
Usually one runs the command with a pipe of standard output to a log
file, which can be inspected for warnings or error messages if anything goes
wrong or otherwise can be discarded at the end of a run. The more easily
readable formatted output goes to the standard output file, generated by abinit with default extension .abo .
No error message is reported in the latter file. On the other hand, this is the file
that is usually kept for archival purposes.

In addition, wavefunctions can be input (starting point) or output (result of
the calculation), and possibly, charge density and/or electrostatic potential,
if they have been asked for. These three sets of data are stored in unformatted binary files (native Fortran),
or in NetCDF format.  

The Density Of States (DOS) can also be an output as a formatted (readable) file.
An analysis of geometry can also be provided (GEO file).
The name of these files is constructed from a "root" name, 
that might be different for input files and output files, and that is either
provided by ABINIT or provided by the user,
to which the code will append a descriptor, like WFK for wavefunctions, DEN
for the density, POT for the potential, DOS for the density of states...

There are also different temporary files, also constructed from a "root" name.
Amongst these files, there is a "status" file, summarizing the current status of advancement of the
code, in long jobs. The [[help:abinit|ABINIT help file]] contains more details.

## What does the code do?
  
The simplest sort of job computes an electronic structure for a fixed set of
atomic positions within a periodic unit cell. By electronic structure, we mean
a set of eigenvalues and wavefunctions which achieve the lowest DFT energy
possible for that basis set (that number of planewaves). 

The code takes the description of the unit cell and atomic positions and assembles a crystal
potential from the input atomic pseudopotentials, then uses either an input
wavefunction or simple gaussians to generate the initial charge density and
screening potential, then uses a self-consistent algorithm to iteratively
adjust the planewave coefficients until a sufficient convergence is reached in the energy.

Analytic derivatives of the energy with respect to atomic positions and unit
cell primitive translations yield atomic forces and the stress tensor. The
code can optionally adjust atomic positions to move the forces toward zero and
adjust unit cell parameters to move toward zero stress. It can performs
molecular dynamics. It can also be used to find responses to atomic
displacements and homogeneous electric field, so that the full phonon band
structure can be constructed.

## Versioning logic
  
We finish this "help for new user" with a brief explanation of the logic of ABINIT version releases.

The full name of a version has three digits (for example, 8.8.3). The first
digit is the slowly varying one (in average, it is changed after two or three
years). It indicates the major efforts and trends in that version. At the
level of 1.x.y ABINIT (before 2000 !), the major effort was placed on the
"ground-state" properties (total energy, forces, geometry optimisation,
molecular dynamics ...). With version 2.x.y , response-function features
(phonons, dielectric response, effective charges, interatomic force constants
...) were included. The main additional characteristics of version 3.x.y were
the distribution under the GNU General Public Licence, the set-up of the
documentation and help to the user through the Web site in html format, and
the availability of GW capabilities. The version 4.x.y put a lot of effort in
the speed of ABINIT (e.g. PAW), and its parallelisation. These historical
developments explain why the tests are gathered in directories "v1", "v2",
"v3", etc. Every 4 to 8 months, we release a "production version" of ABINIT in
which the second digit, an even number, is incremented, which usually goes
with additional features. A [release notes document](about/release-notes) is issued, with the list of
additional capabilities, and other information with respect to modifications
with the previous release. The odd second digits are used for internal
management only, so-called "development versions" of ABINIT (for example
8.9.0). Two versions differing by the last (third) digit have the same
capabilities, but the one with the largest last digit is more debugged than
the other: version 8.8.3 is more debugged than 8.8.2, but no new features has
been added (so likely, no additional bug!).

In order to start using ABINIT, please follow [[tutorial:index|this tutorial.]]
To learn how to compile the code from source, please consult the following guide:

<embed src="https://wiki.abinit.org/lib/exe/fetch.php?media=build:installing_abinit.pdf" type="application/pdf" width="100%" height="480px">
## Getting started with mkdocs

!!! warning
    
    The code supports py2.7 and python3.6 but py3k is **strongly** suggested
    especially when building the final version before deploying 
    as py3k has native support for unicode.

Install the python packages required to build the static website with:

    pip install -r requirements.txt

then install the mkdocs plugin with:

    cd abimkdocs_plugin
    python setup.py install

If you use conda, you may want to create a new environment based on python3.6 with:

    conda create -n abinit-abimkdocs-2 python=3.6
    source activate abinit-abimkdocs

and then install the packages with pip (see above commands).

MkDocs comes with a built-in dev-server that lets you preview your documentation as you work on it. 
Make sure you are in `~abinit`, and then start *our customized* server 
by running the `mksite.py` serve command with the `--dirtyreload` option:

```console
$ ./mksite.py serve --dirtyreload
Regenerating database...
Saving database to /Users/gmatteo/git_repos/abidocs/doc/tests/test_suite.cpkl
Initial website generation completed in 9.17 [s]
Generating markdown files with input variables of code: `abinit`...
...
...
INFO    -  Building documentation...
INFO    -  Cleaning site directory
[I 170826 03:37:05 server:283] Serving on http://127.0.0.1:8000
[I 170826 03:37:05 handlers:60] Start watching changes
[I 170826 03:37:05 handlers:62] Start detecting changes
```

Open up `http://127.0.0.1:8000/` in your browser, and you'll see the default home page being displayed.

Use:

    $ ./mksite.py --help

to get the list of commands and:

    $ ./mksite.py COMMAND --help

to get the documentation of `COMMAND`.

## Notes

topics are still in Yaml format
wikilink syntax: <span style="background-color: #E0E0E0;font-size:90%;"> &#91; [names|text] &#93; </span> will become <span style="background-color: #E0E0E0;font-size:90%;"> &#91; [text|namespace] &#93; </span>.

## Troubleshooting

If you get error message about ASCII being used as default encoding on your machine add:

    export LC_ALL=en_US.UTF-8  
    export LANG=en_US.UTF-8

to your ~/.bashrc and source it

## How to add new variables

The yaml database has been replaced by python modules.
The variables are now declared in `~abinit/abimkdocs/variables_CODENAME.py`.

This file consists of a list of dictionaries, each dictionary
contains the declaration of a single variable and the associated documentation in markdown format.
Wikilinks, latex and markdown extensions can be used inside `text`.
This is, for example, the declaration of the `accuracy` variable in python:

```python
Variable(
    abivarname="accuracy",
    varset="basic",
    vartype="integer",
    topics=['Planewaves_basic', 'SCFControl_basic'],
    dimensions="scalar",
    defaultval=0,
    mnemonics="ACCURACY",
    text="""
Allows to tune the accuracy of a calculation by setting automatically the
variables according to the following table:

accuracy         | 1         | 2          | 3            | 4            | 5         | 6
---              |---        |---         |---           |---           |---        |---
[[ecut]]         | E_min     | E_med      | E_med        | E_max        | E_max     | E_max
[[pawecutdg]]    | ecut      | ecut       | 1.2 * ecut   | 1.5 * ecut   | 2 * ecut  | 2 * ecut
[[fband]]        | 0.5       | 0.5        | 0.5          | 0.5          | 0.75      | 0.75
...

For a parallel calculation, [[timopt]] is enforced to be 0.
...
""",
),
```

Adding a new variable is easy. Edit the python module and add a new item at the end of the list. 
A template is provided ...


## How to add a new bibtex entry

Citations must be in bibtex format and provide enough information so that the python code
can generate appropriated links in the website.
For published work with a DOI, we strongly recommend *avoiding* a `cut&paste` from your bibtex files
(there are units tests to enforce the presence of particular entries in the bibtex document and
your bibtex may not fullfill these requirements).

A much better solution is to use BetterBib and the DOI of the article to fetch data 
from Crossref and produce the bibtex entry. 
BetterBib is available from the Python Package Index, so simply type:

    pip install betterbib

and then use doi2bibtex from the command line:

    doi2bibtex 10.1103/PhysRevLett.96.066402

Add the entry to the bibtex file and use the `FirstAuthorYear` convention for the key 
(make sure it's not a duplicated entry).
Note that the bibtex ID must be of the form "FirstauthornameYEAR", e.g. "Amadon2008" 
(start with an uppercase letter, then lower case, then four-digit year). 
Possibly, a letter might be added in case of ambiguity: e.g. there exists also `Amadon2008a`
Then, build the HTML pages using `mksite.py serve`.

Run the tests in `./tests/test_bibtex.py` with pytest (see next section) to validate your changes.

## Running the unit tests

Unit tests are located in the ./tests directory. 
To execute the tests, install `pytest` with:

    pip install pytest

and then:

    pytest -v ./tests/

Use 

    pytest -v ./tests/test_variables.py

to execute a particular module.

## Checking links with linkchecker

Build the website with:

    ./mksite.py build

then use

    linkchecker site/index.html > links.err

!!! important

    For the time being, linkchecker python2.7
---
authors: MG, MS
---

# Bethe-Salpeter equation in ABINIT  

##   

These notes provide a brief introduction to the Bethe-Salpeter (BS) formalism
outlining the most important equations involved in the theory. 
The approach used to compute the BS kernel and the macroscopic dielectric in an
implementation based on planewaves and norm-conserving pseudopotentials is also discussed.

The conventions used in the equations are explained in the [[theory:mbt|Many-body theory notes]].
A much more consistent discussion of the theoretical aspects of the Bethe-Salpeter equation can be found in
[[cite:Onida2002]] and references therein.

## 1 Optical properties and local field effects
  
Before discussing the Bethe-Salpeter problem, it is worth reminding some basic
results concerning the *ab initio* description of optical properties.

In frequency and reciprocal space, the microscopic dielectric function is
related to the irreducible polarizability by the following relation

\begin{equation} 
\ee_{\GG_1\GG_2}(\qq;\ww) = \delta_{\GG_1,\GG_2} - v(\qq, \GG_1) \tchi_{\GG_1\GG_2}(\qq;\ww) 
\end{equation} 

from which the inverse dielectric function is obtained via matrix inversion.

The *macroscopic* dielectric function, $\ee_M^{\text{LF}}(\ww)$, can be directly related
to the inverse of a single element, the first ($\GG_1 = 0, \GG_2 =0$) element
of the inverse of the *microscopic* dielectric matrix by means of:

\begin{equation}\label{eq:abs_LFE} 
\ee_M^{\text{LF}}(\ww) = \lim_{\qq \rightarrow 0} \dfrac{1}{\ee^{-1}_{0 0}(\qq,\ww)} 
\end{equation} 

The microscopic dielectric matrix is the one usually calculated within the RPA in
a GW calculation. The optical absorption spectrum is simply given by the
imaginary part of $\ee_M^{\text{LF}}(\ww)$. 
Sometimes, especially when comparing with
experimental spectra, the real part is simply called $\ee_1$ and the imaginary part $\ee_2$.

Note that above equation differs from

\begin{equation} \label{eq:abs_NLFE} 
\ee_M^{\text{NLF}}(\ww) = \lim_{\qq \rightarrow 0} {\ee_{0 0}(\qq,\ww)} 
\end{equation} 

due to the so called local-field effects introduced by the presence of the crystalline environment. 
The former quantity *with* local fields (LF) is more accurate than the latter one *without* local field (NLF). 

!!! important

    The reason the two equations are different is
    because in the first, the expression in the denominator is the first element
    of the inverse of the *whole* microscopic dielectric matrix. This element
    depends on the *entire* matrix and cannot simply be calculated by taking the
    inverse of the first element of the non-inverted matrix.

In the [[theory:mbt#RPA_Fourier_space|GW_notes]], we have discussed how
to calculate the irreducible polarizability and thus the absorption spectrum
within the random phase approximation (RPA). It turns out, however, that the
RPA dielectric function evaluated with Kohn-Sham orbitals and eigenvalues
yields absorption spectra that are in quantitative, and sometimes even in
qualitative, disagreement with experiments. This is not surprising since the
polarizability of the Kohn-Sham system is not expected to reproduce the
response of the fully interacting system.

Important discrepancies with experiment are found even when the DFT gap is
corrected by replacing the KS eigenvalues by quasiparticle energies calculated
in the GW approximation. This indicates that, as far as optical properties
are concerned, there is some important physics that is not correctly captured
by the GW approximation.

The fundamental reason for the failure of the RPA can be traced back to the
neglect of the electron-hole interaction that, in the many-body language,
should be included through the vertex function. Replacing the vertex function
with a local and instantaneous function is a too crude and unrealistic
approximation for the many-body polarizability. 

In the next section we will discuss how to obtain an improved approximation for the vertex and therefore
an improved approximation for the polarizability that takes into account many-
body effects due to the electron-hole interaction

## 2 The Bethe-Salpeter equation in a nutshell
  
A schematic picture of the different levels of the description of optical
spectra available in ABINIT is given below.

![](bse_assets/schematic_dft_gw_bse.svg)

The Bethe-Salpeter theory is formulated in terms of two-particle propagators.
These are four-point functions describing the motion through the system of two
particles at once. We will label them $L^0$ and $L$, where the subscript zero
indicates that the particles are non-interacting. By restricting the starting
and ending point of the particles, the two-point contraction of $L^0$ gives
the reducible independent-particle polarizability according to

$$ \chi^0(12) = L^0(11, 22) $$ while the two-point contraction of $L$ gives
the reducible many-body polarizability $$ \hat\chi(12) = L(11, 22) $$ that
should not be confused with the *irreducible* polarizability of the many-body
system (denoted with a tilde in the formulas in the [[theory:mbt#RPA_Fourier_space|GW_notes]]).

The utility of working in terms of the reducible quantities is that the macroscopic dielectric function
*with* local field effects is obtained directly from the reducible polarizability using  

\begin{equation} 
\ee_M^{\text{LF}}(\ww) = 1 - \lim_{\qq \rightarrow 0} v(\qq)\,\tchi_{00}(\qq;\ww) 
\end{equation}

Computing the reducible $L$ directly, if possible, thus allows one to avoid
the costly matrix inversion of the dielectric function that should otherwise
be performed for each frequency.

One can show that $L$ satisfies the Dyson-like equation:

\begin{equation} 
L = L^0 + L^0 K L \Longrightarrow L = \bigl [ 1 - L^0 K]^{-1} L^0 
\end{equation} 

that involves the Bethe-Salpeter kernel $K$:

\begin{equation}\label{eq:BSE_kernel_LF} 
K(1234) = \underbrace{\delta(12)\delta(34)\bar v(13)}_{Exchange} -
\underbrace{\delta(13)\delta(24)W(12)}_{Coulomb} 
\end{equation} 

where the bar symbol signifies that the Coulomb interaction has to be taken without its
long-range Fourier component at $\GG =0$: 

\begin{equation} \begin{cases} {\bar
v(\qq)} = v(\qq) \quad {\text{if}}\; \qq \neq 0 \\\ \\\ {\bar v(\qq=0)} = 0
\end{cases} \end{equation}

As discussed in [[cite:Onida2002]], local field effects (LF) are included in the
formalism through the exchange term while the Coulomb term describes the
screened electron-hole interaction that is responsible for the creation of the
excitons. All interaction is in principle already contained in $W$, and the
rewriting of the kernel in this way is mostly an algebraic trick which allows
one to easily separate the contributions and calculate the optical spectrum
both with and without the LF.

The Dyson-like equation for $L$ becomes a matrix problem once a particular
basis set is chosen to expand the four-point functions involved in the
problem. In the standard approach, the polarisabilities and the BS kernel are
expressed in the so-called *transition* space (i.e. as products of single-particle orbitals) using:

\begin{equation} F(1234) = 
\sum_{ \substack{(n_1 n_2) \\\ (n_3 n_4)} }
\psi_{n_1}^\*(1) \psi_{n_2}(2)\, F_{ (n_1 n_2) (n_3 n_4) }\, \psi_{n_3}(3)
\psi_{n_4}^\*(4) 
\end{equation} 

and 

\begin{equation} F_{ (n_1 n_2) (n_3 n_4)
} = \int F(1234) \psi_{n_1}(1) \psi_{n_2}^\*(2) \psi_{n_3}^\*(3)
\psi_{n_4}(4) \dd (1234) \end{equation} 

where $n_i$ is a shorthand notation to denote band, **k**-point and spin index. 
The expansion is exact provided that
the set of single-particle orbitals from a complete basis set in the Hilbert space.

The main advantage of using the transition space for solving the problem is
that the RPA polarizability is diagonal in this representation

\begin{equation} 
\chi^0_{ (n_1 n_2) (n_3 n_4) } (\ww) = \dfrac{ (f_{n_2} -
f_{n_1}) } { (\ee_{n_2} - \ee_{n_1} - \ww)} \delta_{n_1 n_3} \delta_{n_2 n_4}
\end{equation} 

thus leading to a considerable simplification in the mathematical formalism.

After some algebra (see [[cite:Onida2002]] for a more complete derivation) one
finds that, in a system with an energy gap, the Dyson-like equation for $L$
can be expressed in terms of an effective two-particle Hamiltonian, $H$ , according to

\begin{equation} 
L = \bigl [ H-\ww \bigr]^{-1}\,F 
\end{equation} 

The explicit form for $H$ and $F$ in the transition space is given by 

\begin{equation}
H =
\left( 
\begin{array}{c|cc} 
  &   |v'c'\kk'\rangle & |c'v'\kk'\rangle \\ \hline 
\langle vc\kk|  & R  &  C  \\ %\hline 
\langle cv\kk|  &  -C^* & -R^*   \\
\end{array}
\right) 
\end{equation}

\begin{equation}
F =
\left( 
\begin{array}{c|cc} 
  &   |v'c'\kk'\rangle & |c'v'\kk'\rangle     \\ \hline 
\langle vc\kk|  & 1  &  0   \\ %\hline 
\langle cv\kk|  & 0  & -1   \\
\end{array}
\right) 
\end{equation}

where valence states are indicated by the indices $v$ , $v'$ while $c$ , $c'$
are used for conduction bands. The $R$ sub-matrix is Hermitian and is usually
referred to as the resonant block while $C$ (the so-called coupling block) is
a complex symmetric sub-matrix. Note that, due to the presence of the coupling
term, the BS Hamiltonian is non-Hermitian although the operator possesses a real spectrum.

The inclusion of spin in the formalism would require an extensive discussion
on its own. Here, for simplicity, we limit the analysis to the case of spin
unpolarized semiconductors ([[nsppol]]=1). In this particular case, the matrix
elements of the resonant block are given by

\begin{equation} 
R_{vc\kk,v'c'\kk'} = ( \ee_{c\kk} - \ee_{v\kk})\delta_{vv'}\delta_{cc'}\delta_{\kk\kk'} + 2 \bar v_{(vc\kk)(v'c'\kk')} -
W_{(vc\kk)(v'c'\kk')} 
\end{equation} 

with single particle transition energies on the diagonal.

The matrix elements of $v$ and $W$ are defined as:

\begin{equation} 
{\bar v}_{(n_1 n_2) (n_3 n_4)} = \delta_{\sigma_1 \sigma_2}\,
\delta_{\sigma_3 \sigma_4} \iint \psi_{n_1}(\rr) \psi^*_{n_2}(\rr) {\bar
v(\rr-\rr')} \psi_{n_3}^*(\rr') \psi_{n_4}(\rr') \dd \rr \dd \rr'
\end{equation} 

\begin{equation} 
W_{(n_1 n_2) (n_3 n_4)} = \delta_{\sigma_1
\sigma_3}\, \delta_{\sigma_2 \sigma_4} \iint \psi_{n_1}(\rr) \psi^*_{n_3}(\rr)
{W(\rr,\rr',\ww=0)} \psi^*_{n_2}(\rr') \psi_{n_4}(\rr') \dd \rr \dd \rr'
\end{equation} 

Note, in particular, that only the static limit ($\omega = 0$) of $W$ is involved in the last expression.

The coupling matrix elements are usually smaller than the resonant ones. This
is especially true in bulk systems due to the different spatial behavior of
conduction and valence states. In solid state calculations, it is therefore
common practice to ignore the $C$ block (the so-called Tamm-Dancoff approximation). 
Under this assumption the excitonic Hamiltonian is given by a Hermitian operator.

![](bse_assets/TDA.svg)

The variable [[bs_coupling]] defines whether the coupling block should be included in the calculation.

The macroscopic dielectric function is obtained by contracting the many-body
$L$ and then taking the optical limit of the $\GG = \GG' = 0$ component along
a particular direction in $\qq$-space. 
The final result reads:

\begin{equation} 
\ee_M(\ww) = 1 - \lim_{\qq \rightarrow 0} v(\qq)\,\langle
P(\qq)|\bigl[ H - \ww \bigr]^{-1}\,F |P(\qq)\rangle 
\end{equation} 

where

\begin{equation} P(\qq)_{n_1 n_2} = \langle n_2|e^{i\qq\cdot\rr}|n_1 \rangle
\underset{\qq \rightarrow 0}{ \approx} \delta_{n_1 n_2} + i \qq \cdot \langle
n_2|\rr|n_1 \rangle + O(q^2) 
\end{equation} 

is the matrix element of the dipole operator in transition space.

By default the code calculates the macroscopic dielectric function taking the
limit along six different directions in **q** -space (the three basis vectors
of the reciprocal lattice and the three Cartesian axis). It is possible to
change the default directions using the variables [[gw_nqlwl]] and [[gw_qlwl]].

## 3 Solving the Bethe-Salpeter problem
  
At this point it is clear that the evaluation of the macroscopic dielectric
function within the BS formalism is a two-step process:

  1. Construction of the $H$ matrix in the transition space.   

  2. Computation of the macroscopic dielectric function using the two equations reported at the end of the previous section. 

The flowchart of a typical Bethe-Salpeter run is schematically depicted in the diagram below:

![](bse_assets/bse_flowchart.svg)

The WFK file is represented with an ellipsis. The path on the left indicated
with blue arrows represents the RPA calculation ([[optdriver]]=3) that
produces the SCR file (see also the [[tutorial:gw1|GW1 tutorial]]).
Once the WFK and the SCR file are available, we can finally construct the
Hamiltonian and solve the Bethe-Salpeter problem (the rectangle at the bottom of the flowchart).

For BS computations, it is common practice to simulate the self-energy
corrections by employing the scissors operator whose value can be obtained
either from experiments or from ab-initio calculations. 
The scissors operator allows one to avoid a costly GW calculation that should performed for all
the $\kk$-points and bands included in the transition space (the optional
path on the right indicated with yellow arrows that corresponds to [[optdriver]]=4).

The construction of the Hamiltonian matrix represents a significant portion of
the overall CPU time due to the large number of transitions needed for an
accurate description of the frequency-dependence of the polarizability. On the
other hand, also the calculation of the macroscopic dielectric function poses
numerical difficulties since an expensive matrix inversion should be performed for each frequency.

The code implements different methods proposed in the literature to avoid the
matrix inversion for each frequency. The variable [[bs_algorithm]] is used to
select among the different possibilities.

[[bs_algorithm]]=1 employs standard (Sca)LAPACK routines to obtain the
spectral representation of $H$ in terms of eigenvalues and right eigenvectors of $H$:

\begin{equation} 
\begin{cases} H |\lambda\rangle = \ee_\lambda |\lambda\rangle
\\\ \\\ O_{\lambda\lambda'} = \langle\lambda|\lambda'\rangle \\\ \\\ H =
\sum_{\lambda \lambda'} \ee_\lambda |\lambda\rangle O_{\lambda \lambda'}
\langle\lambda'| \end{cases} 
\end{equation} 

Then, as discussed in [[cite:Albrecht1998]], the inverse of $H - \ww$ is obtained according to 

\begin{equation} \bigl[ H -\ww
\bigr]^{-1} = \sum_{\lambda \lambda'} |\lambda\rangle \dfrac{O_{\lambda
\lambda'}^{-1}}{(\ee_\lambda - \ww)} \langle\lambda'| 
\end{equation} 

We do not elaborate this point further since the direct diagonalization is advantageous
only in particular circumstances, as explained in the documentation.

[[bs_algorithm]] = 2 avoids the diagonalization using an iterative algorithm
that constructs a basis set in which $H$ is represented by a real symmetric
tridiagonal matrix [[cite:Haydock1980]].
Without entering into detail, one can schematically
represent the Haydock technique as an algorithmic procedure that transforms a
dense (hermitian) matrix into a sparse (tridiagonal) one:

\begin{equation}
 R = R^\* =
\begin{pmatrix}
*  & *  & * & *  & * \\
*  & *  & * & *  & * \\
*  & *  & * & *  & * \\
*  & *  & * & *  & * \\
*  & *  & * & *  & *
\end{pmatrix}
\Longrightarrow
\begin{pmatrix}
a_1 & b_2 &     &   &  \\
b_2 & a_2 & b_3 &   &  \\
    & b_3 & *   & * &  \\
    &     & *   & * & * \\
    &     &     & * & * 
\end{pmatrix}
\end{equation}

Once the coefficient of the tridiagonal form are know, the
macroscopic dielectric function is evaluated in terms of the continued fraction: 

\begin{equation} 
\ee(\ww) \propto \cfrac{1} {\ww - a_1 - \cfrac{b_2^2}{\ww - a_2 - \cfrac{b_3^2}{\cdots}}} 
\end{equation} 

where a small complex shift, defined by [[zcut]], is used to avoid the singularities along the real axis. 
The number of iteration required to converge is almost
independent on the size of the problem, usually of the order of 100-200
iterations. The algorithm involves simple matrix-vector multiplications that
are efficiently performed with BLAS calls and, most importantly, are easy to parallelize. 
Another distinct advantage is that the Haydock method is less
memory demanding than the direct diagonalization since only three temporary
vectors are required instead of the full set of eigenvectors.

Note that the original formulation of the method presented in [[cite:Haydock1980]]
assumes an Hermitian operator thus it can be used only for TDA calculations.
We refer the reader to [[cite:Gruning2009]] for a generalization of the method to
the case in which the coupling term cannot be neglected. The main drawback of
the method is that it does not give direct access to the excitonic spectrum
hence it cannot be used to calculate binding energies or to plot the excitonic
wavefunctions. Moreover, for the time being, [[bs_algorithm]] = 2 cannot be used
for calculations in which the coupling term is included.

[[bs_algorithm]] = 3 employs a standard conjugate-gradient method to calculate the
lowest eigenstates of the Hamiltonian. 
At present, this method is still under testing and does not support calculations with the coupling term.

## 4 Kernel matrix elements in reciprocal space
  
Our implementation employs planewaves to expand the periodic part of the Bloch
states, $u$ , and the two-point function $W(\rr,\rr')$ that becomes a $\qq$-dependent matrix in reciprocal-space.
The conventions used for the transforms are documented in [this section](mbt.md#notations) of the $GW$ notes.

The matrix elements of the exchange term are evaluated in reciprocal space using:

\begin{equation} 
{\bar v}_{(vc\kk) (v'c'\kk')} = \dfrac{1}{V} \sum_{\GG \neq
0} {\bar v}(\GG) \; \langle c\kk |e^{i\GG\cdot\rr} |v\kk \rangle \langle
v'\kk'|e^{-i\GG\cdot\rr}|c'\kk' \rangle 
\end{equation} 

while the Coulomb term is calculated according to 

\begin{equation} 
W_{(vc\kk) (v'c'\kk')} =
\dfrac{1}{V} \sum_{\GG_1\GG_2} W_{\GG_1\GG_2}(\kk'-\kk,\ww=0) \langle
v'\kk'|e^{i(\qq +\GG_1)\cdot \rr}|v\kk \rangle \langle c \kk
|e^{-i(\qq+\GG_2)\cdot\rr} |c'\kk' \rangle 
\end{equation} 

The number of $\GG$-vectors in $W$ and in the modified Coulomb interaction is specified through
[[ecuteps]] while the wavefuctions entering the oscillator matrix elements are
expanded on a $\GG$-sphere of energy [[ecutwfn]]. 
The computation of the oscillator matrix elements is discussed in 
[[theory:mbt#oscillator_notes|this section]] of the GW Notes.

The input variable [[bs_exchange_term]] can be used to disable the computation
of the exchange term, this option is mainly used for performing optical
calculations without local field effects. The variable [[bs_coulomb_term]]
governs the computation of the Coulomb term, the most CPU-demanding part due
to the presence of the double sum over $\GG$ -vectors.

It is also important to stress that, in the two equations above, the **k**-point 
index runs over the full Brillouin zone hence the size of the
Hamiltonian is defined by the number of point in the full Brillouin zone and
not by the number of points in the irreducible wedge.

## 5 Matrix elements of the dipole operator
  
The accurate calculation of optical properties require the correct treatment
of the optical limit ($\GG = 0, |\qq| \rightarrow 0$) of the oscillator matrix
elements. The computation of these terms deserves some clarification, due to
the presence of the fully nonlocal pseudopotential term in the Kohn-Sham
Hamiltonian.

A linear expansion up to the first order in **q** of the oscillator matrix element results in:

\begin{equation} 
\langle b_1,\kmq|e^{-i\qq\cdot\rr}|b_2,\kk \rangle
\underset{\qq \rightarrow 0}{\approx} -i\,\qq\cdot \langle b_1,\kk|\rr|b_2,\kk
\rangle + \mcO(q^2) 
\end{equation} 

where we have assumed $b_1 \neq b_2$ and the difference between the two wave functions 
at $\kk - \qq$ and $\kk$ has been
neglected because it only introduces terms that are quadratic in $\qq$.

Unfortunately, the above expression cannot be directly used because the matrix
elements of the position operator are ill-defined when wavefunctions obey
Born-von Karman periodic boundary conditions. For this reason, the first order
contribution has to be evaluated using the equivalent expression [[cite:Baroni1986]]

\begin{equation} 
\langle b_1,\kmq|e^{-i\qq\cdot\rr}|b_2,\kk\rangle
\underset{\qq \rightarrow 0}{\approx} \dfrac{ \langle b_1,\kk|-i\qq\cdot\nabla
+ i\qq\cdot \bigl[V_{\text{nl}},\rr\bigr]|b_2,\kk \rangle } {
\varepsilon_{b_2\kk} - \varepsilon_{b_1\kk} } 
\end{equation} 

The term involving the momentum operator is efficiently evaluated in reciprocal space
with linear scaling in [[npwwfn]] while the treatment of the nonlocal part of
the pseudopotential is more involved and much more CPU-demanding.

The role played by this additional term is usually marginal in the case of GW
calculations: the QP corrections are obtained by performing an integration in
$\qq$-space and only the $\qq \rightarrow 0$ component of the inverse dielectric
matrix is affected by the commutator of the non-local part of the pseudopotential.

For this reason it is common practice, especially during the GW convergence
tests, to neglect this contribution by setting [[inclvkb]] = 0. 
Strictly speaking, however, this approximation is justified only in the case of
calculations in bulk crystals provided that the BZ sampling is well converged.
Particular care has to be taken when performing GW calculations in non-periodic systems 
due to the reduced dimensionality of the BZ.

!!! important

    The commutator of the nonlocal part should ALWAYS be included
    when studying optical properties, both at the RPA and at the BS level.
---
authors: MG
---

$$
\newcommand{\aa}{\alpha}
\newcommand{\PS}{{\text{PS}}}
\newcommand{\Ylm}{{Y_m^l}}
\newcommand{\Vloc}{V_{\text{loc}}}
\newcommand{\vv}{\hat v}
\newcommand{\vloc}{\vv_{\text{loc}}}
\newcommand{\Vnl}{V_{\text{nl}}}
\newcommand{\lm}{{lm}}
\newcommand{\KK}{{\bf K}}
\newcommand{\KKp}{{\bf{K'}}}
\newcommand{\KKhat}{\widehat \KK}
\newcommand{\KKphat}{\widehat{\KK}'}
\newcommand{\jl}{j_l}
\newcommand{\rrhat}{{\widehat\rr}}
\newcommand{\dd}{{\,\text{d}}}
$$

## Pseudopotentials in the Kleynman Bylander form

This page reports the basic definitions and equations needed to evaluate the nonlocal part of the Hamiltonian. 
We mainly focus on norm-conserving pseudopotentials.
Providing a consistent introduction to the theory of pseudopotentials is beyond the purpose of this section,
a more complete discussion of the topic can be found in specialized articles available in the literature
[[cite:Hamann1979]], [[cite:Bachelet1982]], [[cite:Kleinman1982]], [[cite:Hamann1989]], [[cite:Troullier1991]]
[[cite:Gonze1991]], [[cite:Fuchs1999]].
For the generalization of norm conservation to multiple projectors (Optimized norm-conserving Vanderbilt pseudopotentials)
we refer the reader to [[cite:Hamann2013]]. 
The generation and validation of ONCVPSP pseudopotentials is described in
the [PseudoDojo](http://www.pseudo-dojo.org/) paper [[cite:Setten2018]].

For our purpose, we can summarize by saying that a pseudopotential is constructed in order to replace
the atomic all-electron potential such that core states are eliminated and valence electrons are 
described by pseudo wavefunctions whose representation in the Fourier space decays rapidly.

Modern norm-conserving pseudopotentials are constructed such that the scattering properties 
of the all-electron atom are reproduced around the reference energy configuration up to first order in energy,
following the procedure described in [[cite:Hamann1979]], [[cite:Hamann1989]], and [[cite:Troullier1991]].
In modern ab initio codes, the fully separable KB form, proposed by Kleynman and Bylander, is usually employed
as it drastically reduces the number of operations required for the application of the nonlocal part of the Hamiltonian
as well as the memory required to store the operator [[cite:Kleinman1982]].
In the KB form, the interaction between a valence electron and an ion of type $\aa$ is described by means of the operator:

\begin{equation}\label{eq:KBsingleatom}
\vv_\PS^\aa = 
\vloc^\aa(r) + \sum_\lm |\chi_\lm^\aa\ra\,E_l^\aa\,\la\chi_\lm^\aa|,
% \vloc_\aa(\rr) + \sum_\lm |\Ylm\,u_{l\aa}^\KB\ra E_{l\aa}^\KB \lau_{l\aa}^\KB\,\Ylm|
\end{equation}

where $\vloc^\aa(r)$ is a purely local potential with a Coulomb tail $\gamma/r$.

!!! note

    For simplicity, the discussion is limited to pseudopotentials with a single projector per angular channel.
    The generalization to multi-projector pseudopotentials requires introducing an additional $n$ index
    in the KB energies i.e. $E_{nl}^\aa$ and in the projectors $\chi_{n\lm}^\aa(\rr)$.

The so-called Kleinman-Bylander energies, $E_l^\aa$,
measure the strength of the nonlocal component with respect to the local part.
The projectors $\chi_\lm^\aa(\rr)$ are 
short-ranged functions, expressed in terms of a complex spherical harmonic $\Ylm(\theta,\phi)$
multiplied by an $l$- and atom-dependent radial function: 

\begin{equation}
\label{eq:KB_projectors}
\chi_\lm^\aa(\rr) = \dfrac{1}{r}\,\chi_l^\aa(r)\,\Ylm(\theta,\phi).
\end{equation}

<!--
where 

\begin{equation}\label{eq:KBfunctionU}
u_{l\aa}^\KB (\rr) = 
\frac{\Delta v_{l\aa}(\rr)\,u_{l\aa}^\PS (\rr)}
     {\norm{u_{l\aa}(\rr)\,\Delta_{l\aa}}^{1/2}}
\end{equation}
are localized functions defined in terms of the short-ranged ...
and the pseudo eigenfuncions of the reference atom.
\begin{equation}\label{eq:KBenergy}
E_{l\aa}^\KB = 
\frac{\la u_{l\aa}^\PS \Delta v_{l\aa} | \Delta v_{l\aa} u_{l\aa}^\PS \ra }
     {\la u_{l\aa}^\PS | \Delta v_{l\aa} | u_{l\aa}^\PS \ra}
\end{equation}
-->

The total nonlocal part of the Hamiltonian is obtained by summing the different atom-centered contributions.
The final expression in real space reads:

\begin{equation}
\Vnl(\rr_1,\rr_2)= 
\sum_{\substack{\RR \\ \aa\tt_\aa}}
\sum_\lm \la\rr_1-\RR-\tt_\aa|\chi_\lm^\aa\ra E_l^\aa\la\chi_\lm^\aa|\rr_2-\RR-\tt_\aa\ra,
\end{equation}

where $\RR$ runs over the sites of the Bravais lattice, and $\tt_\aa$ over the positions of the atoms with the same type $\aa$ located inside the unit cell.
Due to the nonlocality of the operator, its Fourier transform depends on two separate indices, $\kk+\GG_1$ and $\kk+\GG_2$, instead of the simple difference $\GG_1-\GG_2$.
The shorthand notation $\KK = \kk + \GG$ is used in the following to simplify the derivation.

Using the Rayleigh expansion of planewaves in terms of spherical harmonics $\Ylm(\theta,\phi)$ and spherical Bessel functions $\jl(Kr)$:

\begin{equation}\label{eq:PWinYlmPl}
e^{i\KK\cdot\rr} = 
 4\pi\,\sum_\lm i^l \jl(Kr) \Ylm^\*(\KKhat)\,\Ylm(\rrhat),
\end{equation}

the Fourier representation of the projector, defined by \ref{eq:KB_projectors}, can be expressed as:

\begin{equation}
\la\KK|\chi_\lm^\aa\ra = 
 \frac{4\pi}{\Omega^{1/2}}\, (-1)^l\,\Ylm(\KKhat) 
 \int_0^\infty\, r\jl(Kr)\,\chi_l^\aa(r)r\,\dd r =
 \frac{4\pi}{\Omega^{1/2}}\, (-1)^l\,\Ylm(\KKhat) F_l^\aa(K).
\end{equation}

<!--
%where the form factors $F_l^\aa(K)$ related to the atom of type $\aa$ is defined by
%\begin{equation}\label{wq:defformfactors}
% F_l^\aa(K) \df 
% \frac{\int_0^\infty r\,j_l (Kr)\,u_{l\aa} \Delta v_{l\aa}\,\dd r}
%      {\norm{u_{l\aa} \Delta v_{l\aa}}^{1/2} }
%\end{equation}
-->

Finally, the expression for the total nonlocal operator in reciprocal space is:

\begin{equation}
\label{eq:VnlKBmatrixelements}
\Vnl(\KK,\KKp)= \frac{(4\pi)^2}{\Omega} \sum_{\aa \tt_\aa} \sum_\lm
 \,e^{-i(\KK-\KKp)\cdot\tt_\aa}\,
 \Ylm(\KKhat)\Ylm^\*(\KKphat)\, E_l^\aa F_l^\aa(K) F_l^\aa(K').
\end{equation}

!!! note

    Abinit employs iterative eigenvalue solvers to solve the KS equations.
    This means that we only need to compute $\Vnl |\Psi\ra$, i.e., we only
    need to **apply** the Hamiltonian onto a set of trial eigenvectors.
    Therefore the full $\Vnl(\KK,\KKp)$ matrix is never constructed explicitly.
---
authors: G. Zerah
---

## Non colinear magnetism

## Notations and theoretical considerations

We will denote the spinor by $\Psi^{\alpha\beta}$, ${\alpha, \beta}$ being the two spin indexes.
The magnetic properties are well represented by introducing the spin density matrix:

$$ \rho^{\alpha\beta}(\rr) = \sum_n f_n \la \rr|\Psi_n^\alpha\ra \la\Psi_n^\beta|\rr\ra $$

where the sum runs over all states and $f_n$ is the occupation of state $n$.

With $\rho^{\alpha\beta}(\rr)$, we can express the scalar density by

$$ \rho(\rr)=\sum_{\alpha} \rho^{\alpha\alpha}(\rr) $$

and the magnetization density $\vec m(\rr)$ (in units of $\hbar /2$) whose components are:

$$ m_i(\rr) = \sum_{\alpha\beta} \rho^{\alpha\beta}(\rr) \sigma_i^{\alpha\beta}, $$

where the $\sigma_i$ are the Pauli matrices.

In general, $E_{xc}$ is a functional of $\rho^{\alpha\beta}(\rr)$, or equivalently of $\vec m(\rr)$ and $\rho(\rr)$. 
It is therefore denoted as $E_{xc}[n(\rr), \vec m(\rr)]$.

The expression of $V_{xc}$ taking into account the above expression of $E_{xc}$ is:

$$
V_{xc}^{\alpha\beta}(\rr)={\delta E_{xc} \over \delta \rho (\rr)} \delta_{\alpha\beta} +
\sum_{i=1}^3 {\delta E_{xc} \over \delta m_i (\rr) }\sigma_i^{\alpha\beta}
$$

In the LDA approximation, due to its rotational invariance, $E_{xc}$ is indeed a functional of $n(\rr)$ and $|m(\rr)|$ only.
In the GGA approximation, on the contrary, we **assume** that it is a functional of $n(\rr)$ and $|m(\rr)|$ and their gradients.
(This is not the most general functional of $\vec m(\rr)$ dependent upon first order derivatives, and rotationally invariant.)
We therefore use exactly the same functional as in the spin polarized situation, using the local direction
of $\vec m(\rr)$ as polarization direction.

We then have

$$ 
{\delta E_{xc} \over \delta m_i (\rr) }={\delta E_{xc} \over \delta |m_i (\rr)| } \widehat {m(\rr)},
$$

where $\widehat {m(\rr)} = {m(\rr) \over |m(\rr)|}$.
Now, in the LDA-GGA formulations, $n_\uparrow + n_\downarrow =n$ and $|n_\uparrow-n_\downarrow|=|m|$
and therefore, if we set $n_\uparrow = (n+m)/2$ and $n_\downarrow=(n-n_\uparrow)$, we have:

$$
{\delta E_{xc} \over \delta \rho (\rr)} = {1 \over 2} \Bigl(
{\delta E_{xc} \over \delta n_\uparrow(\rr)}+
{\delta E_{xc} \over \delta n_\downarrow(\rr)}
\Bigr )
$$

and

$$
{\delta E_{xc} \over \delta |m (\rr)| }={1 \over 2} \Bigl ( 
{\delta E_{xc} \over \delta n_\uparrow(\rr)} -
{\delta E_{xc} \over \delta n_\downarrow(\rr)}
\Bigr )
$$

This makes the connection with the more usual spin polarized case.

Expression of $V_{xc}$ in LDA-GGA

$$
V_{xc}(\rr) = {\delta E_{xc} \over \delta \rho (\rr)} \delta_{\alpha\beta}+ {\delta E_{xc} \over \delta |m (\rr)| }
 {\widehat m(\rr)}.\sigma
$$

## Implementation

Computation of $\rho^{\alpha\beta}(\rr) = \sum_n f_n \la \rr|\Psi^\alpha\ra \la\Psi^\beta|\rr\ra$

One would like to use the routine *mkrho* which does precisely this
but this routine transforms only real quantities, whereas
$\rho^{\alpha\beta}(\rr)$ is hermitian and can have complex elements.
The *trick* is to use only the real quantities:

\begin{eqnarray*}
\rho^{11}(\rr)& = &\sum_n f_n \la \rr|\Psi^1\ra \la\Psi^1\ra \\
\rho^{22}(\rr)&=&\sum_n f_n \la \rr|\Psi^2\ra \la\Psi^2\ra \\
\rho(\rr)+m_x(\rr)&=&\sum_{n} f_n (\Psi^{1}+\Psi^{2})^*_n (\Psi^{1}+\Psi^{2})_n \\
\rho(\rr)+m_y(\rr)&=&\sum_{n} f_n (\Psi^{1}-i \Psi^{2})^*_n (\Psi^{1}-i \Psi^{2})_n
\end{eqnarray*}

and compute $\rho(\rr)$ and $\vec m(\rr)$ with the help of:

\begin{eqnarray*}
\rho(\rr)&=&\rho^{11}(\rr)+\rho^{22}(\rr) \\
m_z(\rr)&=&\rho^{11}(\rr) - \rho^{22}(\rr)
\end{eqnarray*}


For more information about noncollinear magnetism see [[cite:Hobbs2000]] 
and [[cite:Perdew1992]] for the xc functional.
---
description: Suggested acknowledgments and references
---

# Acknowledgments  

This file provides a description of suggested acknowledgments and references
to be inserted in scientific papers whose results have been obtained thanks to
ABINIT. It discusses also briefly the problem of co-authorship.

## Introduction

In the next section, you will find several references we suggest you to cite in your papers that have benefited
from ABINIT. However, we wish first to clarify the spirit in which the present document has been written.
The users of the code have no formal obligations with respect to the ABINIT group (within the limits of the
GNU General Public License). However, it is common practice in the scientific
literature, to acknowledge the efforts of people that have made the research possible.
Please note the following:

1. The ABINIT project, in order to be viable, should be known as a robust tool, 
that has been tested, and that has allowed good scientific research.
This will be facilitated if the ABINIT project is mentioned properly in research papers. 

2. Some recent ideas and algorithms are coded, and it would be fair to cite these.

3. You might also register on the [ABINIT discourse forum](https://discourse.abinit.org). 
Indirectly, this also helps the ABINIT developer group, because the total number of registered people 
on the forum is often cited as an indicator of the user community size. 
In agreement with the GNU General Public License, there is no request for co-authorship 
of articles whose scientific results have been obtained thanks to ABINIT, by any ABINIT developer. 
This applies even for recently implemented features, as their availability in a public version 
is governed by the GNU GPL license.
If you think your work could benefit from collaboration with ABINIT developers, 
you can contact the ABINIT group for a possible arrangement, in which case co-authorship should be discussed. 
(Of course, the ABINIT developers also have the right to decline giving assistance to users).

## List of suggestions

The 2020 ABINIT article [Gen.1](#g1), that describe the impact, environment and recent developments,
first in the list of suggestions below, should be cited in papers that have benefited from the
ABINIT project, irrespective of their content.
The other 2020 ABINIT article [Gen.2](#g2), an overview of the ABINIT project, with focus on specific developments,
is also strongly suggested for citation. In particular, see the overview of ABINIT capabilities, in Tables I and II.
There are four other ABINIT articles, [Gen.3](#g3), [Gen.4](#g4), [Gen.5](#g5), and [Gen.6](#g6) that might as well be
considered, irrespective of ther content, because these publications are
quite general as well, although they are older (2016, 2009, 2005 and 2002).  

There are also many articles that are more focused: they describe some
specific capability of ABINIT. The following list is actually not complete...
More references will be proposed by ABINIT itself (see the end of the output
file), see the [[varset:allvars|database]] of information on the input variables, 
in the [[topic:index|topics files]]. as well as in the references of the five general papers.

<a id="g1"></a>
- **Gen.1** At least, the most recent article [[cite:Gonze2020]] that describes the ABINIT project
(impact, environment and recent developments)
should be mentioned in the bibliography section of your paper.
A version of this paper, that is not formatted for Computer Phys. Comm. is available
[here](https://www.abinit.org/sites/default/files/ABINIT20.pdf).
The licence allows the authors to put it on the Web.

<a id="g2"></a>
- **Gen.2** The other 2020 ABINIT article [[cite:Romero2020]] that describes the ABINIT project
(overview and specific developments) is also a strong suggestion for citation 
in the bibliography section of your paper.
A version of this paper, that is not formatted for J. Chem. Phys. is available
[here](https://www.abinit.org/sites/default/files/ABINIT20_JPC.pdf).
The licence allows the authors to put it on the Web.

<a id="g3"></a>
- **Gen.3** A version of the 2016 ABINIT article [[cite:Gonze2016]],
that is not formatted for Computer Phys. Comm. is available 
[here](https://www.abinit.org/sites/default/files/ABINIT16.pdf).
The licence allows the authors to put it on the Web. 

<a id="g4"></a>
- **Gen.4**. The 2009 ABINIT article [[cite:Gonze2009]] is especially detailed. A version of this paper, 
that is not formatted for Computer Phys. Comm. is available 
[here](https://www.abinit.org/sites/default/files/about/ABINIT_CPC_v10.pdf). 
The licence allows the authors to put it on the Web. 

<a id="g5"></a>
- **Gen.5**. The 2005 ABINIT article [[cite:Gonze2005]] is quite short. 
The .pdf of the latter paper is available [here](https://www.abinit.org/sites/default/files/zfk_0505-06_558-562.pdf). 
Note that it should not redistributed (Copyright by Oldenburg Wissenshaftverlag, 
the licence allows the authors to put it on the Web).

<a id="g6"></a>
- **Gen.6**. The very first paper on the ABINIT project [[cite:Gonze2002]] might also be considered for citation.

<a id="s1"></a>
- **Spe.1.** If the Projector-Augmented Wave method as implemented in ABINIT is used [[cite:Torrent2008]] should be mentioned.

<a id="s2"></a>
- **Spe.2.** Many ingredients needed for the calculations of responses to atomic displacements 
or homogeneous electric fields (dynamical matrices, effective charges and dielectric constants), 
as well as the Fourier interpolation implemented in the 'anaddb' code are described in [[cite:Gonze1997]] and [[cite:Gonze1997a]]. 

<a id="s3"></a>
- **Spe.3.** The methods used for the calculation of responses to homogeneous strain 
(elastic tensors, piezoelectric tensors, and internal force-response tensors) are described in [[cite:Hamann2005]].

<a id="s4"></a>
- **Spe.4.** If the "static" non-linear capabilities of ABINIT are used (Raman efficiencies, electro-optic coefficients ... ), 
cite [[cite:Veithen2005]] 

<a id="s5"></a>
- **Spe.5.** If the integration over the phonon degrees of freedom is used ([[anaddb:thmflag]]), cite [[cite:Lee1995]]. 

<a id="s6"></a>
- **Spe.6.** If the self-consistent capabilities of ABINIT beyond DFT are used (GW, COHSEX, HF, etc), 
cite [[cite:Bruneval2006]].

<a id="s7"></a>
- **Spe.7.** If the completeness relationship is used to speed up the convergence with respect to the number 
of bands in a GW calculation (input variables [[gwcomp]] and [[gwencomp]]), cite [[cite:Bruneval2008]] 

<a id="s8"></a>
- **Spe.8.** If the massive parallelism of ABINIT (coupled band/FFT or even coupled band/FFT/k-points ) is used, 
cite [[cite:Bottin2008]] ([available on Arxiv.org](https://arxiv.org/abs/0707.3405)). 

<a id="s9"></a>
- **Spe.9.** If the DFT+U method as implemented in ABINIT is used, cite [[cite:Amadon2008]]. 

<a id="s10"></a>
- **Spe.10.** If the ONCVPSP pseudopotentials are used, cite [[cite:Hamann2013]]. 

<a id="s11"></a>
- **Spe.11.** If the Van Der Waals DFT-D (Grimme) functionals are used, cite [[cite:Vantroeye2016]]. 

<a id="s12"></a>
- **Spe.12.** If the temperature dependence of the electronic structure or the zero-point motion effect 
on the electronic structure are computed, cite [[cite:Ponce2014]] and [[cite:Ponce2015]].

<a id="s13"></a>
- **Spe.13.** If the direct (DFPT) computation of effective masses is used, see [[topic:EffectiveMass]], cite [[cite:Laflamme2016]].

<a id="s14"></a>
- **Spe.14.** If flexoelectricity or dynamical quadrupoles are computed, cite [[cite:Royo2019]].

<a id="s15"></a>
- **Spe.15.** If the new procedure for electron-phonon calculations [[optdriver]]=7 is used, cite [[cite:Brunin2020]] and/or [[cite:Brunin2020b]].


---
authors: M. Giantomassi, X. Gonze, Y. Suzukawa, M. Mikami
---

# Geometric considerations

## Real space

The three primitive translation vectors are $\RR_1$, $\RR_2$, $\RR_3$.
Their representation in Cartesian coordinates (atomic units) is:

$$ \RR_1 \rightarrow {rprimd(:, 1)} \,, $$

$$ \RR_2 \rightarrow {rprimd(:, 2)} \,, $$

$$ \RR_3 \rightarrow {rprimd(:, 3)} \,. $$

Related input variables: [[acell]], [[rprim]], [[angdeg]].

The atomic positions are specified by the coordinates ${\bf x}_{\tau}$
for $\tau=1 \dots N_{atom}$ where $N_{atom}$ is the number of atoms [[natom]].

Representation in reduced coordinates:

\begin{eqnarray*}
{\bf x}_{\tau} &=& x^{red}_{1\tau} \cdot {\bf R}_{1}
 + x^{red}_{2\tau} \cdot {\bf R}_{2}
 + x^{red}_{3\tau} \cdot {\bf R}_{3}  \,, \\ 
\text{where} \,  
  \tau &\rightarrow& {iatom} \,, \\
 N_{atom} &\rightarrow& {natom} \,, \\
 x^{red}_{1\tau} &\rightarrow& {xred(1,iatom)} \,, \\
 x^{red}_{2\tau} &\rightarrow& {xred(2,iatom)} \,, \\
 x^{red}_{3\tau} &\rightarrow& {xred(3,iatom)} \,.
\end{eqnarray*}

Related input variables: [[xcart]], [[xred]].

The volume of the primitive unit cell (called *ucvol* in the code) is

\begin{eqnarray*}
\Omega &=& {\bf R}_1 \cdot ({\bf R}_2 \times {\bf R}_3) \,.
\end{eqnarray*}

The scalar products in the reduced representation are valuated thanks to

$$ 
{\bf r} \cdot {\bf r'} =\left(
\begin{array}{ccc}
r^{red}_{1} & r^{red}_{2} & r^{red}_{1}
\end{array}
\right)
\left(
\begin{array}{ccc}
{\bf R}_{1} \cdot {\bf R}_{1} & {\bf R}_{1} \cdot {\bf R}_{2} &
{\bf R}_{1} \cdot {\bf R}_{3} \\
{\bf R}_{2} \cdot {\bf R}_{1} & {\bf R}_{2} \cdot {\bf R}_{2} &
{\bf R}_{2} \cdot {\bf R}_{3} \\
{\bf R}_{3} \cdot {\bf R}_{1} & {\bf R}_{3} \cdot {\bf R}_{2} &
{\bf R}_{3} \cdot {\bf R}_{3}
\end{array}
\right)
\left(
\begin{array}{c}
r^{red \prime}_{1} \\
r^{red \prime}_{2} \\
r^{red \prime}_{3}
\end{array}
\right) \,,
$$

that is

$$ {\bf r} \cdot {\bf r'} = \sum_{ij} r^{red}_{i} {\bf R}^{met}_{ij} r^{red \prime}_{j} \,, $$

where ${\bf R}^{met}_{ij}$ is the metric tensor in real space stored in `rmet` array:

$$ {\bf R}^{met}_{ij} \rightarrow {rmet(i,j)} \,. $$

## Reciprocal space

The three primitive translation vectors in reciprocal space are
$\GG_1$, $\GG_2$,$\GG_3$

\begin{eqnarray*}
{\bf G}_{1}&=&\frac{2\pi}{\Omega}({\bf R}_{2}\times{\bf R}_{3}) \rightarrow {2\pi\, gprimd(:,1)} \,, \\
{\bf G}_{2}&=&\frac{2\pi}{\Omega}({\bf R}_{3}\times{\bf R}_{1}) \rightarrow {2\pi\, gprimd(:,2)} \,, \\
{\bf G}_{3}&=&\frac{2\pi}{\Omega}({\bf R}_{1}\times{\bf R}_{2}) \rightarrow {2\pi\, gprimd(:,3)} \,.
\end{eqnarray*}

This definition is such that $\GG_i \cdot \RR_j = 2\pi\delta_{ij}$ .

!!! warning
    For historical reasons, the internal implementation uses the convention 
    $\GG_i \cdot \RR_j = \delta_{ij}$. This means that a factor $2\pi$ must be taken into account 
    in the Fortran code.
    We don't use this convention in the theory notes to keep the equations as simple as possible.

Reduced representation of vectors (K) in reciprocal space

$$
{\bf K}=K^{red}_{1}{\bf G}_{1}+K^{red}_{2}{\bf G}_{2}
+K^{red}_{3}{\bf G}^{red}_{3} \rightarrow
(K^{red}_{1},K^{red}_{2},K^{red}_{3}) 
$$

e.g. the reduced representation of ${\bf G}_{1}$ is (1, 0, 0).

!!! important

    The reduced representation of the vectors of the reciprocal space
    lattice is made of triplets of integers.

The scalar products in the reduced representation are evaluated thanks to

$$ 
{\bf K} \cdot {\bf K'}=\left(
\begin{array}{ccc}
K^{red}_{1} & K^{red}_{2} & K^{red}_{1}
\end{array}
\right)
\left(
\begin{array}{ccc}
{\bf G}_{1} \cdot {\bf G}_{1} & {\bf G}_{1} \cdot {\bf G}_{2}
& {\bf G}_{1} \cdot {\bf G}_{3} \\
{\bf G}_{2} \cdot {\bf G}_{1} & {\bf G}_{2} \cdot {\bf G}_{2}
& {\bf G}_{2} \cdot {\bf G}_{3} \\
{\bf G}_{3} \cdot {\bf G}_{1} & {\bf G}_{3} \cdot {\bf G}_{2}
& {\bf G}_{3} \cdot {\bf G}_{3}
\end{array}
\right)
\left(
\begin{array}{c}
K^{red \prime}_{1} \\
K^{red \prime}_{2} \\
K^{red \prime}_{3}
\end{array}
\right)   \,,
$$

that is 

$$ {\bf K} \cdot {\bf K'} = \sum_{ij} K^{red}_{i}{\bf G}^{met}_{ij}K^{red \prime}_{j}  \,, $$

where ${\bf G}^{met}_{ij}$ is the metric tensor in reciprocal space called `gmet` inside the code.
Taking into account the internal conventions used by the code, we have the correspondence:

$$ {\bf G}^{met}_{ij} \rightarrow {2\pi\,gmet(i,j)} \,. $$

## Fourier series for periodic lattice quantities

Any function with the periodicity of the lattice i.e. any function fullfilling the property

$$ u(\rr + \RR) = u(\rr) $$

can be represented with the discrete Fourier series:

$$  u(\rr)= \sum_\GG u(\GG)e^{i\GG\cdot\rr} \,, $$ 

where the Fourier coefficient, $u(\GG)$, is given by:

$$ u(\GG) = \frac{1}{\Omega} \int_\Omega u(\rr)e^{-i\GG\cdot\rr}\dd\rr \,. $$

<!--
This appendix reports the conventions used in this work for the Fourier 
representation in frequency- and momentum-space. 
The volume of the unit cell is denoted with $\Omega$, while $V$ is the
total volume of the crystal simulated employing Born-von K\'arman periodic boundary condition~\cite{Ashcroft1976}.

\begin{equation}\label{eq:IFT_2points_convention}
 f(\rr_1,\rr_2)= \frac{1}{V} \sum_{\substack{\qq \\ \GG_1 \GG_2}}  
 e^{i (\qq +\GG_1) \cdot \rr_1}\,f_{\GG_1 \GG_2}(\qq)\,e^{-i (\qq+\GG_2) \cdot \rr_2} 
\end{equation}

\begin{equation}\label{eq:FT_2points_convention}
 f_{\GG_1\GG_2}(\qq) = \frac{1}{V} \iint_V 
 e^{-i(\qq+\GG_1) \cdot \rr_1}\,f(\rr_1, \rr_2)\,e^{i (\qq+\GG_2) \cdot \rr_2}\dd\rr_1\dd\rr_2 
\end{equation}
-->
---
authors: X. Gonze, Y. Suzukawa, M. Mikami, MG
---

$$
\newcommand{\mcR}{{\mathcal{R}}}
\newcommand{\omcR}{{\hat\mcR}}
\newcommand{\Atm}{{\bf A}}
\newcommand{\AFMomcR}{{\underline{\omcR}}}
\newcommand{\AFMmcR}{{\underline{\mcR}}}
\newcommand{\HH}{{\hat H}}
\newcommand{\nk}{{n\kk}}
\newcommand{\RRm}{\mcR^{-1}} 
\newcommand{\Rit}{\mcR^{-1T}}
\newcommand{\rmt}{{\rr-\tt}}
\newcommand{\aa}{\alpha}
\newcommand{\bb}{\beta}
\newcommand{\ss}{\sigma}
\newcommand{\mcJ}{{\mathcal{J}}}
\newcommand{\omcA}{{\hat\mcA}}
\newcommand{\thalf}{{\tfrac{1}{2}}}
\newcommand{\mcE}{{\mathcal{E}}}
\newcommand{\mcA}{{\mathcal{A}}}
$$

Representation and convension of one wavefunction
ABINIT data structures and their theoretical justifications.

## Notations and theoretical considerations

A Bloch wavefunction characterized by a wavevector $\kk$ is such that

$$ \psi_{\bf k}({\bf r}) = e^{i{\bf k}\cdot{\bf r}} u_{\bf k}({\bf r}) \,, $$ 

where $u_{\bf k}({\bf r})$ is periodic, that is

$$ u_{\bf k}({\bf r}+{\bf R})=u_{\bf k}({\bf r}) \,, $$ 

where ${\bf R}$ is a vector of the real space lattice.

Its representation by plane waves reads:

\begin{eqnarray*}
u_{\bf k}({\bf r})&=&\sum_{\bf G}u_{\bf k}({\bf G})e^{i{\bf G}\cdot{\bf r}}  \,, \\
\psi_{\bf k}({\bf r})&=&\sum_{\bf G}u_{\bf k}({\bf G})
e^{i ({\bf k}+{\bf G})\cdot{\bf r}} \,.
\end{eqnarray*} 

Normalization of ${u_k}$ reads:

$$ \sum_{\bf G}|u_{\bf k}({\bf G})|^2 = 1 \,.$$

For a **spinor** wavefunction, there is an additional variable, 
the spin $\sigma$ that can take two values, that is $\sigma=\uparrow$ (spin up) 
or $\sigma=\downarrow$ (spin down).
The following relations hold:

$$
u_{\bf k}({\bf r},\sigma) = \sum_{\bf G}u_{\bf k}({\bf G},\sigma) e^{i{\bf G} \cdot {\bf r}} \,,
$$

$$
\psi_{\bf k}({\bf r},\sigma) = \sum_{\bf G}u_{\bf k}({\bf G},\sigma) e^{i({\bf k}+{\bf G})\cdot{\bf r}} \,,
$$

$$
\sum_{\sigma}\sum_{\bf G}|u_{\bf k}({\bf G},\sigma)|^2 = 1 \,.
$$

## Properties of the wavefunctions (scalar case)

For ground-state wavefunctions, we have to solve the Schrödinger equation

$$ H |\psi_{n{\bf k}}\ra = \varepsilon_{n{\bf k}}|\psi_{n{\bf k}}\ra \,, $$

where $H$ is the Hamiltonian operator, $n$ labels the state (or the band), $\varepsilon_{n{\bf k}}$ is the eigenvalue.

As the wavevector labelling of an eigenstate comes from the property

$$ \psi_{\bf k}({\bf r}+{\bf R}) = e^{i{\bf k}{\bf R}} \psi_{\bf k}({\bf r}) \,, $$

in which $\kk$ can be replaced by $\kk + \GG$ where $\GG$ is any reciprocal space lattice vector, we can
*choose* the wavefunctions at $\kk$ and $\kk + \GG$
to be equal, or to make a linear combination of wavefunctions with the same energy. 
This is a choice of **gauge** that does not affect the value of physical observables.
In what follows we prefer to work with the gauge 

$$ \psi_{\kk + \GG}(\rr) = \psi_\kk(\rr) $$

to keep the notation as simple as possible,

The time-reversal symmetry (non-magnetic case) of the Hamiltonian gives the following relation:

$$
\psi_{n{\bf k}}({\bf r}) = \psi^{*}_{n-\bf k}({\bf r}) \,.
$$

For the ${\bf k}$ wavevectors that are half a reciprocal lattice vector
$(2{\bf k}={\bf G}_{0})$, there is a special relationship between 
the Fourier coefficients of the wavefunction:

$$
u_{n{\bf k}}({\bf G}) =
u_{n{\bf k}-{\bf G}_{0}}({\bf G}+{\bf G}_{0}) = 
u_{n-{\bf k}}({\bf G}+{\bf G}_{0}) =
u^{*}_{n{\bf k}}(-{\bf G}-{\bf G}_{0}) \,.
$$

That is, coefficients at $\GG$ and $-{\bf G}-{\bf G}_{0}$ are related.
This will allow to decrease by a factor of 2 the storage space for these
specific ${\bf k}$ points and accelerate CPU-intensive parts such as the 
application of the non-local part and the Fourier transform.

## Properties of the wavefunctions (spinor case)

One must distinguish two classes of Hamiltonians:

1. the Hamiltonian is spin-diagonal,
2. the Hamiltonian mixes the spin components.

In the first class, one finds usual non-spin-polarized, non-spin-orbit
Hamiltonians, in which case the spin up-spin up and spin down-spin down parts of the Hamiltonian
are equal, as well as spin-polarized
Hamiltonian when the magnetic field varies in strength but *not* in direction.
In the second class, one finds Hamiltonians that include the
spin-orbit splitting as well as non-collinear spin systems.

In the first class, the wavefunctions can be made entirely of *either* spin-up components *or* spin-down
components, and treated independently of those made of opposite spin.
This corresponds to [[nsppol]] = 2.
In the second class, one must stay with spinor wavefunctions.
This corresponds to [[nspinor ]] = 2.

These two classes are mutually exclusive. The possibilities are thus:

 nsppol      | nspinor     | wavefunction type
------------ | ----------- | -----------------
   1         |     1       |   scalar wavefunctions
   2         |     1       |   spin-polarized wavefunctions
   1         |     2       |   spinor wavefunctions


The inclusion of spin-orbit coupling in the Hamiltonian requires [[nspinor]] = 2.

## Plane wave basis set sphere

In order to avoid dealing with an infinite number of plane waves to represent Bloch wavefunctions,
one selects those with a kinetic energy lower than some cutoff energy $E_{\rm cut}$.
The set of allowed ${\bf G}$ vectors will be denoted by $\mcS_{\kk}(E_{\rm cut})$ such that

$$
\GG\,\in \mcS_{\kk}(E_{\rm cut}) \;\mbox{if}\; \dfrac{\lvert{\bf k + G}\rvert^{2}}{2} \leq E_{\rm cut} \,.
$$

The kinetic energy cutoff is computed from the input variables [[ecut]] and [[dilatmx]]
to give the *effective* value

$$ {\text{ecut_eff}} = {\text ecut} * ({\text dilatmx})^2 $$

For special $\kk$-points satisfying the condition $2 \kk = \GG_0$, not all coefficients must be stored. 
A specific storage mode, governed by the input variable [[istwfk]] has been
introduced for the following $\kk$ points:

$$
\Bigl(0, 0, 0\Bigr), 
\left(0, 0, \frac{1}{2}\right),
\left(0, \frac{1}{2}, 0\right),
\left(0, \frac{1}{2}, \frac{1}{2}\right),
\left(\frac{1}{2}, 0, 0,\right),
\left(\frac{1}{2}, 0, \frac{1}{2}\right),
\left(\frac{1}{2}, \frac{1}{2}, 0\right),
\left(\frac{1}{2}, \frac{1}{2}, \frac{1}{2}\right)
$$

For these points, the number of $\GG$ vectors to be taken into account, is decreased by about a factor of 2.
For the $\GG$'s that are not treated, the coefficients
$u_{n{\bf k}}({\bf G})$ can be recovered from those that are treated, thanks to

$$ u_{n{\bf k}}({\bf G}) = u^{*}_{n{\bf k}}(-{\bf G}-{\bf G}_0) $$

The value of [[istwfk]] is automatically computed by the code
on the basis of the k-point coordinates and the treatment of time-reversal symmetry as specified by [[kptopt]].
One can disable the time-reversal trick in the input file by setting explicitly the value of [[istwfk]]
with the syntax:

```
istwfk *1
```

<!--
The number of plane waves is {\text npw}
For ${\text ipw}=1\cdots {\text npw}$, the reduced coordinates of ${\bf G}$
are contained in the array {\text kg}:

$$
\mbox{these are integer numbers}
\cases{
  {\bf G}^{red}_{1}=& {\text kg}(1,{\text ipw}) \cr
  {\bf G}^{red}_{2}=& {\text kg}(2,{\text ipw}) \cr
  {\bf G}^{red}_{3}=& {\text kg}(3,{\text ipw}) \cr
}
$$
This list of $\GG$ vectors is computed in the routine {\text kpgsph.f}.
-->

The maximum number of $\GG$-vectors over all k-points is called [[mpw]] inside the code. 

## FFT grid and FFT box

For the generation of the density from wavefunctions, as well as for
the application of the local part of the potential, one needs to be
able to compute $u_{n\kk}(\rr)$
for a 3D-mesh of $\rr$-points, extremely fast, from the values $u_{n\kk}(\GG)$.

!!! note

    spin up and spin down parts can be treated separately in this
    operation, so they do not need to be specified otherwise in this section.

The FFT algorithm starts from values of a function

$$
\begin{aligned}
  z (j_{1},j_{2},j_{3})  \, \mbox{for} \, &j_{1}=0\cdots(N_{1}-1) \,, \\
               &j_{2}=0\cdots(N_{2}-1) \,,
               &j_{3}=0\cdots(N_{3}-1)
\end{aligned}
$$

and compute fast the transformed

$$
\begin{aligned}
\tilde{z}(l_{1},l_{2},l_{3}) \, \mbox{for} \, 
                                &l_{1}=0\cdots(N_{1}-1)\,, \\
				&l_{2}=0\cdots(N_{2}-1)\,, \\
				&l_{3}=0\cdots(N_{3}-1)  \\
\end{aligned}
$$

with

$$
\tilde{z}(l_{1},l_{2},l_{3})=\sum_{j_{1},j_{2},j_{3}} z(j_{1},j_{2},j_{3})
e^{i2\pi\left(\frac{j_{1}l_{1}}{N_{1}}+\frac{j_{2}l_{2}}{N_{2}}+\frac{j_{3}l_{3}}{N_{3}}\right)} \,.
$$

We want the values of $u_{\bf k}({\bf r})$ on a FFT grid with: 

\begin{eqnarray*}
r^{red}_{1}&=&\frac{0}{N_{1}},\frac{1}{N_{1}},\cdots
\frac{N_{1}-1}{N_{1}}\left(=\frac{l_{1}}{N_{1}}\right) \,, \\
r^{red}_{2}&=&\frac{0}{N_{2}},\frac{1}{N_{1}},\cdots
\frac{N_{2}-1}{N_{2}}\left(=\frac{l_{2}}{N_{2}}\right) \,, \\
r^{red}_{3}&=&\frac{0}{N_{3}},\frac{1}{N_{3}},\cdots
\frac{N_{3}-1}{N_{3}}\left(=\frac{l_{3}}{N_{3}}\right) \,. 
\end{eqnarray*}

The FFT algorithm has a computational cost that scales as $N\log(N)$ where $N$ is the 
total number of points in the mesh.
Note, however, that we cannot obtain $u_\kk(\rr)$ *everywhere* but only
on the discrete set of FFT points given in the above equations.
The choice of $N_{1},N_{2},N_{3}$ is not discussed here.

<!--
The effect of $G^{red}_{1}$ or $G^{red}_{1}+N_{1}$ (or any value of $G^{red}_{1}$ modulo $N$) will be similar.

\begin{eqnarray*}
u_{{\bf k}}({\bf r})&=&\sum_{\bf G} u_{\bf k}({\bf G})
e^{i2\pi {\bf G} \cdot {\bf r}} \\
 &=&\sum_{\bf G} u_{\bf k}({\bf G}) e^{i2\pi(G^{red}_{1}r^{red}_{1} +
G^{red}_{2}r^{red}_{2} + G^{red}_{3}r^{red}_{3})}
\end{eqnarray*}

Let us represent $u_{\kk}(\rr)$ by the segment

```fortran
wf_real (1:2,1:N_{1},1:N_{2},1:N_{3})
```

where the first index refer to the real or imaginary part and the
three others to the integer values $l_{1}+1,l_{2}+1,l_{3}+1$

Let us map the $c_{\bf k}({\bf G})$ coefficients on a similar segment

```fortran
wf_reciprocal (1:2,1:N_{1},1:N_{2},1:N_{3})
```

with a similar meaning of {\tt wf\_reciprocal}$(1:2,j_{1}+1,j_{2}+1,j_{3}+1)$:

\begin{eqnarray*}
j_{1}&=&{\tt mod}({\bf G}^{red}_{1},N_{1}) [\Rightarrow j_{1}\in[0,N_{1}-1]]\\
j_{2}&=&{\tt mod}({\bf G}^{red}_{2},N_{2}) \\
j_{3}&=&{\tt mod}({\bf G}^{red}_{3},N_{3})
\end{eqnarray*}

Then:

\begin{eqnarray*}
\lefteqn{
{\tt wf\_real}(\cdot ,l_{1}+1,l_{2}+1,l_{3}+1)}  \\
&=& \sum^{N_{1}-1}_{j_{1}=0}
\sum^{N_{2}-1}_{j_{2}=0} \sum^{N_{3}-1}_{j_{3}=0} {\tt wf\_reciprocal}
(\cdot ,j_{1}+1,j_{2}+1,j_{3}+1) \times
e^{i2\pi(\frac{j_{1}l_{1}}{N_{1}}+\frac{j_{2}l_{2}}{N_{2}}+\frac{j_{3}l_{3}}{N_{3}})}
\end{eqnarray*}

This is, up to the array indexing convention, precisely the operation done by the FFT algorithm.
-->

For FFT efficiency (minimisation of cache conflicts), the arrays
are not dimensioned with $(N_1, N_2, N_3)$ but with $(N_4, N_5, N_6)$ where:

\begin{eqnarray*}
N_4 = N_1 + 1\;{\text{if}}\; N_1\; {\text{is even else}}\; N_1, \\
N_5 = N_2 + 1\;{\text{if}}\; N_2\; {\text{is even else}}\; N_2, \\
N_6 = N_3 + 1\;{\text{if}}\; N_3\; {\text{is even else}}\; N_3.
\end{eqnarray*}

The FFT mesh is given by [[ngfft]]. 
PAW requires an additional **dense** FFT mesh for densities and potentials called [[ngfftdg]].

<!--
## Wavefunctions and spatial symmetries

If some spatial symmetry operation commutes with the Hamiltonian:

$$ [H,S_{\bf t}]=0 $$

then

\begin{eqnarray*}
H|\psi\ra = \varepsilon|\psi>&\Rightarrow&
S_{\bf t}H|\psi\ra = \varepsilon S_{t}|\psi\ra \\
 &\Rightarrow& H[S_{\bf t}|\psi\ra] = \varepsilon[S_{\bf t}|\psi]
\end{eqnarray*}

$S_{\bf t}|\psi\ra$ is also an eigenvector, with the same eigenvalue as $|\psi\ra$.

However its wavevector is different:

\begin{eqnarray*}
\psi_{n{\bf k}}({\bf r}+{\bf R}) &=&
 e^{i2\pi {\bf k} {\bf R}} \psi_{n{\bf k}}({\bf r}) \\
\Rightarrow (S_{\bf t} \psi_{n{\bf k}})({\bf r}+{\bf R})&=&
\psi_{n{\bf k}}((S_{\bf t})^{-1}({\bf r}+{\bf R})) \\
 &=&\psi_{n{\bf k}}(\sum_{\beta}S^{-1}_{\alpha\beta}(r_{\beta}+R_{\beta}-t_{\beta})) \\
 &=&\psi_{n{\bf k}}(\sum_{\beta}S^{-1}_{\alpha\beta}(r_{\beta}-t_{\beta})+\sum_{\beta}S^{-1}_{\alpha\beta}R_{\beta}) \\
 &=&\psi_{n{\bf k}}((S_{t})^{-1}({\bf r})+\sum_{\beta}S^{-1}_{\alpha\beta}R_{\beta}) \\
\noalign{\hbox{($S^{-1}_{\alpha\beta} R_{\beta}$ must be a vector of the real space lattice if $S_{t}$ leaves the lattice invariant)}}
 &=&e^{i2\pi \sum_{\alpha\beta} k_{\alpha}
S^{-1}_{\alpha\beta} R_{\beta}} \psi_{n{\bf k}}((S_{t})^{-1}({\bf r})) \\
 &=&e^{i2\pi {\bf k}'\cdot{\bf R}}(S_{\bf t}\psi_{n{\bf k}})({\bf r})
\end{eqnarray*}

where $({\bf k}')_{\alpha} = \sum_{\beta} S^{-1}_{\beta\alpha} k_{\beta}$

For a vector in the reciprocal space

$$ ({\bf k}')_{\beta} = (S_{\bf t}({\bf k}))_{\beta} = \sum_{\beta} S^{-1}_{\beta\alpha} k_{\beta} $$

i.e. the inverse transpose of $S_{\alpha\beta}$ is used.

The preceeding result means

\begin{eqnarray*}
\psi_{n(S^{-1,t}{\bf k})}
 &\stackrel{\rm L.C.}{=}& (S_{t}\psi_{n{\bf k}})({\bf r}) \\
 &\stackrel{\rm L.C.}{=}& \psi_{n{\bf k}} (\sum_{\beta}
 S^{-1}_{\alpha\beta}(r_{\beta}-t_{\beta}))
\end{eqnarray*}

\begin{eqnarray*}
&\Longrightarrow& u_{n(S^{-1,t} k)}({\bf r}) e^{i2\pi \sum_{\alpha\beta}
S^{-1,t}_{\alpha\beta} k_{\beta} r_{\alpha}} \stackrel{\rm L.C.}{=}
e^{i2\pi \sum_{\alpha\beta} k_{\alpha}
S^{-1}_{\alpha\beta}(r_{\beta}-t_{\beta})} \times u_{n{\bf k}}(\sum_{\beta}
S^{-1}_{\alpha\beta}(r_{\beta}-t_{\beta})) \\
&\Longrightarrow& u_{n(S^{-1,t} k)}({\bf r}) \stackrel{\rm L.C.}{=}
e^{-i2\pi \sum_{\alpha\beta} k_{\alpha} S^{-1}_{\alpha\beta}
t_{\beta}} u_{nk}(\sum_{\beta}
S^{-1}_{\alpha\beta}(r_{\beta}-t_{\beta})) \\
&\Longrightarrow& \sum_{{\bf G}} c_{n(S^{-1,t}k )}({\bf G})
 e^{i2\pi{\bf G}\cdot{\bf r}}
\stackrel{\rm L.C.}{=} e^{-i2\pi \sum_{\alpha\beta} k_{\alpha}
S^{-1}_{\alpha\beta} t_{\beta}} \sum_{{\bf G}'} c_{n{\bf k}}({\bf G}')
 e^{i2\pi \sum_{\alpha\beta} G'_{\alpha}S^{-1}_{\alpha\beta}
 (r_{\beta}-t_{\beta})} \\
&\Longrightarrow& c_{n(S^{-1,t} k)}(\sum_{\alpha} G'_{\alpha}
S^{-1}_{\alpha\beta}) \stackrel{\rm L.C.}{=} e^{-i2\pi
\sum_{\alpha\beta}(k_{\alpha}+G'_{\alpha}) S^{-1}_{\alpha\beta}
t_{\beta}} c_{n{\bf k}}({\bf G}') \\
\end{eqnarray*}

This formula allows to derive coefficients $c_{n}$ at one ${\bf k}$ point
from these at a symmetric ${\bf k}$ point.
-->

## Symmetry Properties

### Effect of space group symmetries on electron energies and wavefunctions

All the symmetry operations which leave the crystal unchanged constitute a space group.
Besides the translation symmetry operations, a space group contains proper or 
improper rotations followed by an appropriate fractional displacement.
There are 230 in total [[cite:Bassani1975]]. 

A generic element of the space group will be denoted in the following with $\hat\mcR_\tt$
where $\mcR$ is a real orthonormal matrix associated to
a proper or improper rotation while $\tt$ is the corresponding fractional translation.
If all the fractional translations are zero the space group is said to be *symmorphic*.
In Abinit, the rotations in real space are called [[symrel]], while the corresponding 
fractional translation are stored in the [[tnons]] array.
Note that both *symrel* and *tnons* are given in reduced coordinates. 

The application of a symmetry operation $\hat\mcR_\tt$ to a vector $\Atm_i$ 
defining the position of an atom in the unit cell gives:

\begin{equation}
\omcR_\tt\, \Atm_i \equiv \mcR^{-1} (\Atm_i -\tt) = \Atm_j + \RR,
\end{equation}

where $\Atm_j$ indicates an atom in the same unit cell of the same type as 
$\Atm_i$ which may be coincident with $\Atm_i$, and $\RR$ is a suitable lattice translation (possibly zero).
The application of the symmetry operation $\hat \mcR_\tt$ on a generic function $F(\rr)$ 
of the three spatial coordinates is conventionally defined by:

\begin{equation}
\omcR_\tt\, F (\rr) \equiv F(\Ri (\rr-\tt)).  
\end{equation}

Since $\omcR_\tt$ commutes with the Hamiltonian $\HH$ of the crystal, it readily
follows that, given $\Psi_{n\kk}(\rr)$ being eigenstate of $\HH$,
 $\omcR_\tt\, \Psi_{n\kk}(\rr)$ is also eigenstate of the Schrödinger problem with the same eigenvalue:

\begin{equation}
 \begin{cases}
  \bigl[
   \hat\mcR_\tt, \HH \bigr]  =0 &  \\
   \\
   \HH \Psi_{n\kk}(\rr) = \ee_{n\kk}\Psi_{n\kk}(\rr) &
 \end{cases}
 \Longrightarrow \HH\hat\mcR_\tt\, \Psi_{n\kk}= 
 \omcR_\tt\HH\,\Psi_{n\kk} = \ee_{n\kk}\,\omcR_\tt\Psi_{n\kk}.
\end{equation}

Although $\omcR_\tt\,\Psi_{n\kk}(\rr)$ has the same eigenenergy as $\Psi_{n\kk}(\rr)$, its 
crystalline momentum is different.
The operation $\hat\mcR_\tt$ of the space group transforms a Bloch function with vector $\kk$
into a new Bloch wave function of crystalline momentum $\mcR \kk$. 
This important property can be seen as follows:

\begin{equation}
\label{eq:Rotation_of_psi}
 \begin{aligned}
 \Bigl[ \omcR_\tt\Psi_\nk \Bigr] (\rr+\RR) =  \quad 
   & \Psi_\nk \bigl( \Ri (\rr+\RR-\tt) \bigr) \\ 
 = & e^{i \kk\cdot\Ri (\rr+\RR-\tt)}\, u_\nk \bigl( \Ri(\rr-\tt) \bigr) \\
 = & \quad e^{i\Rit \kk\cdot\rr}\, \Psi_\nk \bigl( \Ri (\rr-\tt)\bigr ) \\
 = & e^{i\,\mcR \kk \cdot \rr}\, \omcR_\tt \Psi_\nk(\rr),
 \end{aligned}
\end{equation}

where $\RR$ is an arbitrary vector of the direct Bravais lattice and the invariance of the periodic part of 
the Bloch wave function has been exploited.

!!! note 

    The last equality in follows from the orthogonality
    of the $\mcR$ matrix when referred to in a Cartesian frame of reference. 
    The orthogonality of the rotation matrix does not hold anymore if, as usually done in the ABINIT code,
    the symmetry operations are expressed in reduced coordinates.
    In this case, the correct matrix to use for operations in reciprocal space is given by the transpose of $\Ri$.
    (called *symrec* in the Fortran code)

For a nondegenerate state, one obtains:

\begin{equation}
\label{eq:rotation_wfn}
\Psi_{\mcR\kk}(\rr) = \Psi_{\kk} (\Ri(\rr-\tt)).
%u_{\mcR^{-1}\kk}(\GG)    & = u_\kk(\mcR^{-1}\GG)
\end{equation}

The above equation can be used to reduce the number of $\kk$-points and 
matrix elements that have to be evaluated and stored since the information in
the full Brillouin zone can be reconstructed by symmetry from an appropriate irreducible wedge.
The set of equations below summarizes the most useful relationships commonly employed:

\begin{eqnarray}
\label{eq:space_group_symmetry}
\begin{cases}
\ee_{\mcR\kk}    & =  \quad \ee_{\kk} 
\\
u_{\mcR\kk}(\rr) & =  \quad e^{-i \mcR\kk     \cdot\tt}\, u_{\kk}\bigl(\RRm(\rmt)\bigr) 
\\
u_{\mcR\kk}(\GG) & =  \quad e^{-i(\mcR\kk+\GG)\cdot\tt}\, u_{\kk}(\RRm\GG).
\end{cases}
\end{eqnarray}

The time invariance of the Hamiltonian, $\HH^\* = \HH$, might introduce additional constraints:

\begin{eqnarray}
\label{eq:time_reversal_symmetry}
\begin{cases}
\ee_\nk   & =  \quad \ee_{-\kk}, 
\\
u_\nk(\rr)& =  \quad u^\*_{n-\kk}(\rr), 
\\
u_\nk(\GG)& =  \quad u_{n-\kk}^\*(-\GG).
\end{cases}
\end{eqnarray}

It is important to stress that the set of equations in \ref{eq:space_group_symmetry}
hold only in case of nondegenerate states.
In the presence of degeneracy, the application of a symmetry operation on a Bloch 
wave function with momentum $\kk$ belonging to the set of degenerate states $\mcC_{\nk}$, 
produces a new eigenstate with same energy and crystalline momentum $\mcR \kk$. 
The new eigenstate is given by an appropriate linear combination of the degenerate states with wave vector $\mcR\kk$. 
More explicitly:

\begin{equation}
\label{eq:symmetry_for_degenerate_states}
\omcR_\tt\Psi_\nk (\rr)  = \Psi_\nk(\mcR^{-1}(\rr-\tt))
= \sum_{\aa \in\,\mcC_\nk} D_{\aa n}(\mcR)\,\Psi_{\aa\mcR\kk}(\rr),
%\psi_{n\mcR\kk}(\rr)% u_{\mcR^{-1}\kk}(\GG)    & = u_\kk(\mcR^{-1}\GG)
\end{equation}

where $D_{\aa\bb}(\mcR)$ is the unitary transformation associated with $\mcR$ [[cite:Bassani1975]]. 
Equation \ref{eq:symmetry_for_degenerate_states} reduces to \ref{eq:rotation_wfn}
if the state $\Psi_\nk(\rr)$ is nondegenerate since, in this particular case, $D_{\aa\bb}(\mcR) = \delta_{\aa\bb}$.

### Magnetic Space Groups

An extension of the concept of symmetry is needed in order to explain the magnetic properties 
of crystals [[cite:Landau1984]].
In addition to the spatial arrangement of atoms, the orientation of the atomic magnetic moment 
also becomes important in such systems.
It may turn out that the usual spatial operation, while leaving the crystal unchanged in 
regard to its geometrical structure, will reverse the orientation of spins.

Let the symbol $\mcJ$  indicate the operation of reversing all spins, and 
let $\mcE$ denote the identity operator. 
A combined operation consisting of an ordinary symmetry operation followed by $\mcJ$
is a new type of symmetry operation called a complementary operation. 
The rules for operator composition in the standard nonmagnetic group can be easily extended to include $\mcJ$:

\begin{equation}
\text{If}\;\; \omcA_1 \cdot \omcA_2 = \omcA_3 \;\;\text{then}
\begin{cases}
(\mcJ \omcA_1) \cdot (\mcJ\omcA_2) = \omcA_3
\\\\
(\mcJ \omcA_1) \cdot \omcA_2 = \mcJ\omcA_3
\\\\
\omcA_1 \cdot (\mcJ\omcA_2) = \mcJ\omcA_3.
\end{cases}
\end{equation}

Magnetic groups are obtained by replacing some of the symmetry elements of the initial non-magnetic 
space group by their complementary operations so that the resulting ensemble forms a group 
with respect to the new algebra.
Magnetic space groups are sometimes referred to as Shubnikov groups, and can be 
classified according to three different categories [[cite:Bhagavantam1964]].

Shubnikov type IV 
:   Groups which include $\mcJ$ explicitly, so-called grey groups or Shubnikov type IV (230 in number).
    Each group can be obtained by taking the direct product of each of the conventional space 
    groups with the group $(\mcE,\mcJ)$

Shubnikov type III
:   Groups which do not include $\mcJ$ explicitly, but which contain complementary symmetry 
    operations. Also called mixed groups or Shubnikov type III. There are 1191 in total.

Colorless 
:   Groups which do not include $\mcJ$ either explicitly or in conjunction with a conventional 
    symmetry operation, also named colorless groups. There are 230 and they are indistinguishable from 
    the conventional space groups

Henceforth, in order to keep the notation as simple as possible, a symmetry operation containing $\mcJ$ 
will be denoted with an underlined symbol $\AFMomcR_\tt$, and will be said to have anti-ferromagnetic character.
On the contrary, symmetry operations which preserve the orientation of the spin projection will be denoted using the 
standard notation $\omcR_\tt$, and will be said to have ferromagnetic character.
The action of a magnetic symmetry on a nondegenerate Bloch state has to be generalized according to:

\begin{eqnarray}
\label{eq:AFM_symmetries_wfs}
\omcR_\tt\,    \Psi^\ss_\kk(\rr)  \equiv & \Psi^\ss_\kk(\Ri(\rr-\tt))    & = \Psi_{\mcR\kk}^\ss(\rr),
\\
\AFMomcR_\tt\, \Psi^\ss_\kk(\rr)  \equiv & \Psi^{-\ss}_\kk(\Ri(\rr-\tt)) & = \Psi_{\mcR\kk}^{-\ss}(\rr),
\end{eqnarray}

where $\ss = \pm \thalf$. 
Using the above symmetry relations, 
one can verify that the two spin components of the electron density transform according to:

\begin{eqnarray}
\label{eq:rho_vxc_AFM_symmetry}
\omcR_\tt\,    n^\ss(\rr) =  & n^\ss \bigl(\mcR^{-1}(\rr-\tt)\bigr) & =  n^\ss   (\rr),
\\
\AFMomcR_\tt\, n^\ss(\rr) =  & n^\ss \bigl(\mcR^{-1}(\rr-\tt)\bigr) & =  n^{-\ss}(\rr),
\end{eqnarray}

from which it follows that the total charge and, consequently, the Hartree potential are invariant
under the application of any operation of the magnetic group.
Similar symmetry relationships hold for the two components of the exchange-correlation potential.

The magnetic character of the symmetry operation is stored in the [[symafm]] array.
---
authors: MG, MS
---

# Many-Body Theory in ABINIT  

The aim of this section is to introduce the Green's function formalism, the
concept of self-energy and the set of coupled equations proposed by Hedin. 
The self-consistent solution of these Hedin equations, in principle, gives the exact Green's function
of the interacting system.

We mainly focus on the aspects of the theory that
are important for understanding the different steps of the calculation and the
role played by the input variables used to control the run. A much more
consistent and rigorous introduction to the physical concept of Green's
function and self-energy can be found in any standard textbook on Many-Body
theory, see for example [[cite:Abrikosov1975]], [[cite:Fetter1971]], [[cite:Mattuck1976]].

## Green's function and self-energy
  
The time-ordered Green's function $G(12)$, also called the propagator, defines
the probability amplitude for the propagation of an added or removed electron
in a many-body system. 
Since the probability amplitude is simply given by the overlap between the final and the initial state, $G(12)$ can be expressed as

\begin{equation} 
G(1,2) = -i\left\langle \Theta_0^N \left| \hat{T} \left[
\hat{\psi}(1)\hat{\psi}^{\dagger}(2) \right] \right| \Theta_0^N \right\rangle,
\end{equation} 

where the matrix element is taken in the Heisenberg representation. 
$\hat{T}$ is the time-ordering operator and the creation and annihilation field operators act on the ground state of the $N$-electron many-body Hamiltonian. 
The conventions used in the equations are explained in the section on [notation](#notations). 
The Green's function contains only a part of the full information carried by the many-body wave function, but it includes the
relevant part for the study of charged excitations. 
In addition, any single-particle operator acting on the system can be evaluated once the Green's function is known.

Useful physical information about the charged excitation energies of the many-body system can be obtained by expressing the propagator in the so-called
Lehmann representation. [[cite:Abrikosov1975]], [[cite:Fetter1971]], [[cite:Mattuck1976]]. 
To this purpose it is useful to introduce the following notation to denote the charged excitation energies of the $N$-electron system [[cite:Onida2002]]:

\begin{equation} 
\varepsilon_i = 
\begin{cases} 
E_i^{(N+1)} - E_0^N & \text{if}\,\varepsilon_i > \mu \\\ E_0^N-E_i^{(N-1)} & \text{if}\,\varepsilon_i < \mu, 
\end{cases} 
\end{equation} 

where $E_0^N$  is the ground state energy of
the electron system with $N$ electrons, and $i$ is the set of quantum numbers
labeling the excited states with $N \pm 1$ electrons. Finally, $\mu$ is the
chemical potential of the system. 
Other important quantities that will be used in the following are the so-called Lehmann amplitudes. 
In the Schrodinger representation, they are defined by 

\begin{equation} \label{eq:Lehmann_amplitudes}
\Psi_i (\rr) \equiv \begin{cases} \langle\Theta_0^N|\hat\psi(\rr)
|\Theta_i^{N+1}\rangle \quad\qquad \ee_i > \mu \\\ \\\ \langle\Theta_i^{N-1}|
\hat\psi(\rr) |\Theta_0^N\rangle \quad\qquad \ee_i < \mu \end{cases}
\end{equation} 

The Lehmann representation of the Green's function

\begin{equation} \label{eq:Lehmann_representation} 
G (\rr_1,\rr_2;\ww) = \sum_i \frac{\Psi_i(\rr_1)\Psi_i^\*(\rr_2)} {\ww-\ee_i
+i\eta\,\sign(\ee_i-\mu)} \qquad\eta \rightarrow 0^+, 
\end{equation} 

makes clear that, in the frequency domain, the time-ordered Green's function
contains the complete excitation spectrum corresponding to excitations of an 
$(N-1)$-particle and an $(N+1)$-particle system. Hence, locating the poles
of the Green's function in the complex plane provides the information needed
to interpret those processes measured in experiments in which a single
electron is inserted to or removed from the system. The figure below gives a
schematic representation of the location of the poles of the time-ordered Green's function.

![](mbt_assets/poles_G.svg)

The ionization potential is the energy required to remove an electron
from the system, the electron affinity to add an electron, and the chemical
potential is typically taken to be in the middle of the gap. For a metallic
system these energies are all equal to each other, and there is no gap.

The Dyson equation

\begin{equation} \label{eq:G_eq_G0SG} 
G(12) = \Go(12) + \int \Go(13)\,\Sigma(34)\,G(42)\dd34. 
\end{equation} 

establishes the connection between the fully interacting $G$ and $\Go$, the Green's  of an approximate
non-interacting system through a (non-local, non-Hermitian and time dependent)
potential called the self-energy, $\Sigma$. Since $\Go$ is supposed to be known
exactly, the problem of calculating $G (12)$ has now been reduced to the calculation of the self-energy.


The self-energy is not a mere mathematical device used in a roundabout way to
obtain $G$, but it has a direct physical meaning. The knowledge of the self-energy operator, 
allows one to describe the quantum-mechanical state of a
renormalized electron in the many-body system by solving the quasiparticle
(QP) equation [[cite:Onida2002]]:

\begin{equation} \label{eq:QP_equation} 
\Bigl[ {\hat h}_0(\rr_1) + v_H(\rr_1)
\Bigr] \Psi(\rr_1) + \int\,\Sigma(\rr_1,\rr_2;\ee^\QP)\Psi(\rr_2)\dd\rr_2 = \ee^\QP \Psi(\rr_1) 
\end{equation}  

The QP eigenstates obtained in this way can be used
to construct $G$ according to the Lehmann representation. Note that the QP
equation differs from the Kohn Sham equation since the QP eigenvectors and
eigenvalues do have a direct physical meaning: they can be used to obtain both
the charge density of the interacting system and to describe the properties of charged excitations.

## Hedin's equations
  
In 1965 Hedin [[cite:Hedin1965]] showed how to derive a set of coupled integro-
differential equations whose self-consistent solution, in principle, gives the
exact self-energy of the system and therefore the exact $G$.

The fundamental building blocks employed in the formalism are the irreducible polarizability:

\begin{equation}\label{eq:chi_tilde_def} 
\tchi(12) \equiv \dfrac{\delta n (1)}{\delta \Ueff (2)} = -i\frac{\delta G(11^+)}{\delta \Ueff(2)}, 
\end{equation}

which describes the linear response of the density to changes in the total
effective potential (the superposition of the external potential plus the
internal classical Hartree potential) and the dynamically screened
interaction, $W$ , that is related to the bare Coulomb interaction, $v$ , and
to the inverse dielectric function through: 

\begin{equation} \label{W_def}
W(12) \equiv \int \ee^{-1}(13)\,v(32)\dd3. 
\end{equation} 

The dielectric matrix $\ee(12)$ is related to the irreducible polarizability $\tchi(12)$ by the
following relation: 

\begin{equation} 
\ee(1, 2) = \delta(1, 2) - \int v(1, 3)\tchi(3, 2)\dd 3 
\end{equation}

The pentagon sketched in the figure below shows how the various physical quantities are interrelated:

![](mbt_assets/hedin_pentagon.svg)

The polarization function renormalizes the bare interaction resulting in the
screened interaction $W (12)$. The screened interaction, $W (12)$, the many-
body propagator $G (12)$, and the vertex function, $\Gamma(12;3)$, which describe the
interactions between virtual hole and electron excitations [[cite:Onida2002]], are
the essential ingredients for the determination of $\Sigma(12)$.

The iteration starts by setting $G = \Go$. Then the set of equations should
in principle be iterated until self-consistency in all terms is reached.

## The GW approximation
  
The practical solution of Hedin's equations is extremely complicated as they
are not just numerical relations but contain a functional derivative in the
equation for the vertex. The direct evaluation of the vertex function is very
challenging. The set of equations can, however, be iterated assuming that only
a few iterations are actually needed to obtain physically meaningful results.

A widely used approach to the approximate solution of Hedin's equations is the
so-called $GW$ approximation [[cite:Hedin1965]], which consists in approximating
the vertex function with a local and instantaneous function:

\begin{equation} 
\Gamma(12;3) \approx \delta(1,2)\delta(1,3) \equiv \Gamma^{GW}(12;3). 
\end{equation} 

This approximated vertex, once inserted in
the full set of Hedin's equations, leads to a considerable simplification in the set of equations:

![](mbt_assets/gw_pentagon.svg)

Thanks to the neglect of vertex corrections, the irreducible polarizability $\tchi (12)$ is now given by

\begin{equation}\label{eq:RPA_with_G} 
\tchi = -i\,G(12)G(21^+). 
\end{equation}

which, once rewritten in terms of orbitals and energies, reduces to the RPA
expression proposed by Adler [[cite:Adler1962]] and Wiser [[cite:Wiser1963]].

In real space, the self-energy reduces to a simple direct product of the
dressed electron propagator, $G (12)$, and the dynamically screened interaction, $W (12)$:

\begin{equation} 
\Sigma(12) = i G(12)\,W(1^+2). 
\end{equation}

The self-energy, a simple product in the space-time domain, becomes a convolution when
expressed in frequency-space: 

\begin{equation} 
\Sigma(\rr_1,\rr_2;\ww) = \frac{i}{2\pi} \int G(\rr_1,\rr_2;\ww+\ww') W(\rr_1,\rr_2;\ww')e^{i\ww'\delta^+}\dd\ww'. 
\end{equation}

Ideally, the set of GW equations should still be iterated until self-
consistency in all terms is reached; this is the fully self-consistent GW method (SCGW).  
However SCGW calculations for real systems are still very
challenging, and very few have been reported in the literature  Moreover, the
utility of fully SCGW results are still under debate within the scientific community.

The problem is that self-consistency typically improves total energies, but
worsens spectral properties (such as band gaps and optical spectra). Since
obtaining the spectral information is often the main reason for doing such
difficult calculations in the first place, many authors agree that a useful
self-consistent approach would need the inclusion of some kind of vertex
correction during the solution of the equations.

For this reason, the most common approach employed in the _ab initio_
community consists of using the best available approximation for $G$ and $W$
as a starting point and performing only a single-iteration of the
parallelogram (the so-called one-shot $GW$ method, or $\Go\Wo$). In this case
the self-energy is simply given by:

\begin{equation} 
\Sigma(12) = i\Go^\KS(12)\Wo(1^+2) 
\end{equation} 

where $\Go^\KS(12)$ is the independent-particle propagator of the Kohn-Sham (KS)
Hamiltonian, and the screened interaction is approximated with the RPA
calculated with KS energies and wave functions: 

\begin{equation} 
\chi^0(12) = -i \Go^\KS(12)\Go^\KS(12). 
\end{equation}

## Perturbative approach
  
Despite all the fundamental differences between many-body theory and DFT, the
Kohn-Sham exchange-correlation potential can be seen as a static, local and
Hermitian approximation to the self-energy. Indeed, in many cases the Kohn-
Sham energies already provide a reasonable estimate of the band structure and
are usually in qualitative agreement with experiment.

This observation suggests that a simple, albeit accurate, solution for the QP
energies can be obtained using first-order perturbation theory, treating the
exchange and correlation potential, $V_{xc}$, as a zeroth-order approximation to
the non-local and energy dependent self-energy [[cite:Hybertsen1985]], [[cite:Hybertsen1986]]

Under the assumption that the QP wavefunctions equal the KS orbitals, we can
expand the self-energy operator around $\ee^\KS$ obtaining a closed expression for $\ee^\QP$:

\begin{equation} \label{eq:implicit_QP_energy} 
\ee^\QP = \ee^\KS + Z \langle\Psi^\KS|\Sigma(\ee^\KS)-\vxc|\Psi^\KS\rangle. 
\end{equation} 

where

\begin{equation} \label{eq:Z_factor} Z \equiv \left[ 1- \langle \Psi^\KS|
\PDER{\Sigma(\ee)}{\ee^\KS}|\Psi^\KS\rangle \right]^{-1} 
\end{equation} 

is the so-called renormalization factor. 
This corresponds to making a Taylor
expansion of the self-energy matrix element around the KS energy, as depicted below.

![](mbt_assets/self_energy_taylor.png)

<a id="RPA_Fourier_space"></a>
## The RPA polarizability in Fourier space
  
In the reciprocal space and frequency domain (implying a Fourier transform
(FT) of the real space coordinates and time variables), the independent-
particle polarizability assumes the form:

![](mbt_assets/chi0_sc_tr.svg)

where only the transitions between valence ($v$) and conduction states ($c$) contribute 
(for simplicity we have assumed a semiconductor with time-reversal invariance,
the conventions used for the Fourier transform are
discussed in the [notation](#notations) section).

The number of bands used to compute the polarizability is specified by
[[nband]], while [[zcut]] gives the small complex shift used to avoid the
divergences in the denominators. The frequency mesh is defined by the set of
variables [[nfreqre]], [[nfreqim]], [[freqremax]], and [[freqremin]] (a number
of more exotic grid choices are available through input variables beginning
with `gw_...` or `cd_...`, e.g. [[gw_frqim_inzgrid]]).

$M$ is a shorthand notation to denote the matrix element of a plane wave
sandwiched between two wavefunctions (i.e. oscillator matrix elements). 
The number of planewaves (PW) used to describe the wavefunctions is determined by
[[ecutwfn]] while the number of $\GG$-vectors used to describe the
polarizability (i.e. the number of _G_ vectors in the oscillator matrix
elements) is determined by [[ecuteps]]. 

The oscillators are ubiquitous in the Many-Body part of ABINIT and their calculation 
represents one of the most CPU intensive part of the execution. For this reason we 
devote a separate [section](#notes-on-the-calculation-of-the-oscillator-matrix-elements)
 to the discussion of some important technical details concerning their computation.

In principle, the set of $\qq$-points in the screening matrix is given by all
the possible differences between two crystalline momenta of the wavefunctions
stored in the KSS file, so it is controlled by the chosen $\kk$-point grid.
The code, however, exploits the invariance of the two-point function under the
action of any symmetry operation of the crystalline space group:

\begin{equation} 
\chi^0(\rr_1,\rr_2) = \chi^0
\bigl(\Ri(\rr_1-\tt),\Ri(\rr_2-\tt)\bigr) 
\end{equation} 

so that only the $\qq$-points in the irreducible Brillouin zone (IBZ) have to be calculated explicitly.

In frequency and reciprocal space, the microscopic dielectric function is
related to the irreducible polarizability by the following relation

\begin{equation} 
\ee_{\GG_1\GG_2}(\qq;\ww) = \delta_{\GG_1,\GG_2} - v(\qq,
\GG_1) \tchi_{\GG_1\GG_2}(\qq;\ww) 
\end{equation} 

from which the inverse dielectric function is obtained via matrix inversion. Following Adler
[[cite:Adler1962]], the macroscopic dielectric function, $\ee_M^{\text{LF}}(\ww)$, can be directly
related to the inverse of the microscopic dielectric matrix by means of:

\begin{equation} \label{eq:abs_LFE} 
\ee_M^{\text{LF}}(\ww) = \lim_{\qq \rightarrow 0} \dfrac{1}{\ee^{-1}_{0 0}(\qq,\ww)} 
\end{equation} 

The optical absorption spectrum -- the quantity one can compare with experiments -- is
given by the imaginary part.

Note that the equation above differs from

\begin{equation} \label{eq:abs_NLFE} 
\ee_M^{\text{NLF}}(\ww) = \lim_{\qq \rightarrow 0} {\ee_{0 0}(\qq,\ww)} 
\end{equation} 

due to the so called local-field effects introduced by the presence of the crystalline environment. 
These spectra, if calculated, are typically output as ... **_LF** and ... **_NLF**
files during the course of a calculation.

## Notes on the calculation of the oscillator matrix elements
  
Many body calculations require the evaluation of integrals involving the
oscillator matrix elements

![](mbt_assets/oscillator_longv.svg)

where the **k** -point belongs to the full Brillouin zone.

These terms are evaluated by performing a Fast Fourier Transform (FFT) of the
real space product of the two wavefunctions (the second expression in the
equation above). Thanks to the FFT algorithm, the CPU-time requirement scales
almost linearly with the number of points in the FFT box, moreover the code
implements refined algorithms (for instance zero-padded FFTs, FFTW3 interface)
to optimize the computation.

There can be a significant speed-up in this component depending on the
numerical FFT library used. If possible, it should always be advantageous to
link and use the FFTW3 library in GW calculations (controlled by setting
[[fftalg]] 312). The performance of the various FFT libraries for a given type
of calculation can be benchmarked with the **fftprof** utility.

For a given set of indices ( $b_1$, $b_2$, $\kk$, $\qq$ ), the calculation of
the oscillator is done in four different steps:

  1. The two wavefunctions in the irreducible wedge are FFT transformed from the $\GG$-space to the real space representation, 
  2. The orbitals are rotated in real space on the FFT mesh to obtain the points $\kk$ and $\kk-\qq$ in the full Brillouin zone. 
  3. Computation of the wavefunction product. 
  4. FFT transform of the product to obtain $M$

Each oscillator thus requires three different FFTs (two transforms to
construct the product, one FFT to get M). The number of FFTs can be
significantly reduced by pre-computing and storing in memory the real space
representation of the orbitals at the price of a reasonable increase of the
memory allocated. However, for very memory demanding calculations, the real
space orbitals can be calculated on the fly with an increase in computational
time instead. This option is controlled by the second digit of the input variable [[gwmem]].

The third term in the equation defining the oscillators makes it clear that
the product of the periodic part of the orbitals has non-zero Fourier
components in a sphere whose radius is $2 × R_{wfn}$ where $R_{wfn}$ is the radius
of the $\GG$-sphere used for the wavefunctions (set by [[ecutwfn]]). To avoid
aliasing errors in the FFT one should therefore use an FFT box that encloses
the sphere with radius $2 × R_{wfn}$, but this leads to a significant increase in
the computing effort as well as in the memory requirements. The input variable
[[fftgw]] specifies how to setup the FFT box for the oscillators and should be
used to test how the aliasing errors affect the final results. The default
setting of **fftgw 21** is safe, a setting of **fftgw 11** is fast but can be
inaccurate, and a setting of **fftgw 31** gives the maximum possible accuracy
at a significant computational cost.

<a id="hilbert_transform"></a>
## Hilbert transform method
  
The computational effort for the evaluation of the RPA polarizability with the
Adler-Wiser expression scales linearly with the number of frequencies computed
([[nfreqre]] and [[nfreqim]]), albeit with a large prefactor which increases
with the fourth power of the number of atoms. The main reason for the linear
scaling is that the frequency dependence cannot be factorized out of the sum
over transitions, hence a distinct and expensive summation over transitions
has to be performed separately for each frequency.

This linear scaling represents a serious problem, especially when many
frequencies are wanted, for example when computing QP corrections within the
contour deformation technique described in the [[tutorial:gw2|GW2 tutorial]].

This computational bottleneck can be removed, under certain circumstances, by
employing an efficient algorithm proposed in [[cite:Miyake2000]] and subsequently
revisited in [[cite:Shishkin2006]], in which only the spectral function

![](mbt_assets/sf_chi0.svg)

has to be evaluated in terms of electronic transitions between valence and
conduction states. The Dirac delta function can be approximated either by
means of a triangular function centered at the energy transition following
[[cite:Shishkin2006]] or a gaussian approximant following [[cite:Miyake2000]] (see the
related input variables [[spmeth]], and [[spbroad]]). The spectral function is
evaluated on a linear frequency mesh which covers the entire set of transition
energies included in the calculation. The number of points in the mesh is given by [[nomegasf]].

The evaluation of the spectral function is rather efficient thanks to the
presence of the delta-function in the expression above. For example, when
[[spmeth]]=1, the CPU time required to compute the spectral function on an
arbitrarily dense frequency mesh is just twice that required by a single
static computation based on the standard Adler-Wiser expression.

The full polarizability is then efficiently retrieved by means of a less
expensive frequency integration (a Hilbert transform):

\begin{equation} \label{eq:chi0_Hilbert_transform} 
{\chi^0}_{\GG_1\GG_2}
(\qq,\ww) = \int_0^{+\infty} \tchi^\mcS_{\GG_1\GG_2} (\qq,\ww') \times
\biggl(\frac{1}{\ww-\ww'+i\delta}-\frac{1}{\ww+\ww'-i\delta}\biggr)\dd\ww'
\end{equation}

The price to be paid, however, is that a large table for the spectral function
has to be stored in memory and a Hilbert transform has to be performed for
each pair ( $\GG_1, \GG_2$). Since the computing time required for the
transform scales quadratically with the number of vectors in the
polarizability (governed by [[ecuteps]]), the CPU time spent in this part will
overcome the computing time of the standard Adler-Wiser formalism for large
[[ecuteps]]. A theoretical estimate of the crossover point is hard to give
because it depends on many factors. However, if many frequencies are needed,
such as for the evaluation of optical spectra, or accurate contour deformation
integrations, or even mapping full grids in the complex plane, the Hilbert
transform method can be significantly faster, and its use is well worth considering.

<a if="evaluation_gw_sigma"></a>
## Evaluation of the GW self-energy
  
Following the standard approach, we separate the screened interaction into the
static bare Coulomb term and a frequency-dependent contribution according to:

\begin{equation} 
W = v + (\ee^{-1} - 1) v 
\end{equation} 

where matrix notation is used.

This particular decomposition of $W$ , once inserted in the convolution
defining $\Sigma$, leads to the split of the self-energy into two different
contributions (exchange and correlation):

\begin{equation} 
\Sigma(\rr_1,\rr_2;\ww) \equiv \Sigma_x(\rr_1,\rr_2) + \Sigma_c(\rr_1,\rr_2;\ww), 
\end{equation}

The exchange part is static and turns out to have the same mathematical
structure as the Fock operator in Hartree-Fock theory, albeit constructed with
quasiparticle amplitudes

\begin{equation}\label{eq:Sigma_x} 
\Sigma_x(\rr_1,\rr_2)= -\sum_\kk^\BZ
\sum_\nu^\text{occ} \Psi_{n\kk}(\rr_1){\Psi^\*_{n\kk}}(\rr_2)\,v(\rr_1,\rr_2)
\end{equation} 

while the dynamic part $\Sigma_c(\ww)$ accounts for correlation effects beyond $\Sigma_x$.

It is important to stress that, for computational efficiency, the code does
not compute the full self-energy operator by default. Only its matrix elements
for the states specified by [[kptgw]] and [[bdgw]] are computed and used to
obtain the QP corrections.

When expressed in reciprocal space, the diagonal matrix elements of the
exchange part are given by:

![](mbt_assets/self_x_mel.svg)

The evaluation of these terms represents a minor fraction of the overall CPU
time since only occupied states are involved. However we should always keep in
mind that, due to the long range of the bare Coulomb interaction, the
convergence with respect to the number of plane waves used in the oscillators
_M_ ([[ecutsigx]]) is usually slow, much slower than the convergence of the
correlation part, which is short-ranged. This plane wave cutoff can be
converged independently of others if necessary, and given a much larger value
in comparison to [[ecut]], [[ecutwfn]] and [[ecuteps]].

Another point worth noting is the presence in the expression of the Coulomb
singularity for $| \qq + \GG | \rightarrow 0$ . 
From a mathematical point of view, the integral is well-defined since the singularity is integrable in three-
dimensional space once the thermodynamic limit, $N_\qq \rightarrow \infty$, is reached.

On the other hand, only a finite number of $\qq$-points can be used for
practical applications, and a careful numerical treatment is needed to avoid
an exceedingly slow convergence with respect to the BZ sampling. To accelerate
the convergence in the number of $\qq$-points, the code implements several
techniques proposed in the literature. We refer to the documentation of
[[icutcoul]] for a more extensive discussion.

The expression for the matrix elements of the correlation part is instead given by:

![](mbt_assets/self_c_mel.svg)

where all dynamical effects are now contained in the frequency convolution integral _J_.

The explicit expression for _J_ depends on the method used to treat the
screened interaction. The code implements four different plasmon-pole
techniques to model the frequency dependence of **W** in an efficient but
approximate way, alternatively, it is possible to use the more sophisticated
frequency integration of the contour deformation method [[cite:Lebegue2003]] for
accurate QP calculations (see the related variables [[ppmodel]] and [[gwcalctyp]]).

The double sum over $\GG$-vectors is performed for all the plane waves
contained within a sphere of energy [[ecuteps]] (it cannot be larger than the
value used to generate the SCR file). For each state, the correlated matrix
elements are evaluated on a linear frequency mesh centered around the initial
KS energy and the derivative needed for the renormalization factor is obtained
numerically (see [[nomegasrd]] and [[omegasrdmax]]).

Note that here, in contrast to the exchange term, the sum over the band index
$n$ should extend up to infinity although in practice only a finite number of
states can be used (specified by [[nband]]). 

!!! important

    It is also advised to take special care of the convergence with respect to [[nband]] and [[ecuteps]]. 
    In a GW calculation these two parameters tend not to be independent. 
    Converging one at a low value of the other and vice versa can easily lead to under converged results [[cite:Setten2017]].

## Plasmon-pole models
  
One of the major computational efforts in self-energy calculations is
represented by the calculation of the frequency dependence of the screened
interaction, which is needed for the evaluation of the convolution. Due to the
ragged behavior of $G(\ww)$ and $W (\ww)$ along the real axis, numerous real
frequencies are in principle required to converge the results (note that the
maximum frequencies needed are now reported if [[prtvol]] > 9). On the other
hand, since the fine details of $W(\ww)$ are integrated over, it is reasonable
to expect that approximate models, able to capture the main physical features
of the screened interaction, should give sufficiently accurate results with a
considerably reduction of computational effort. This is the basic idea behind
the so-called plasmon-pole models in which the frequency dependence of $W(\ww)$
is modelled in terms of analytic expressions depending on coefficients that
are derived from first principles, i.e. without any adjustable external parameters.

Four different plasmon-pole techniques are available in ABINIT and the input
variable [[ppmodel]] selects the method to be used.

When [[ppmodel]]=1,2 the frequency dependence of the inverse dielectric
function is modeled according to

\begin{equation} \label{eq:ppmodel_imag}
\text{Im}\,\ee^{-1}_{\GG_1\GG_2}(\qq,\ww) = A_{\GG_1\GG_2}(\qq)\, \bigl[
\delta(\ww-\ww_{\GG_1\GG_2}(\qq)) - \delta(\ww+\ww_{\GG_1\GG_2}(\qq)) \bigr]
\end{equation} 

\begin{equation} \label{eq:ppmodel_real}
\text{Re}\,\ee^{-1}_{\GG_1 \GG_2} (\qq,\ww) = \delta_{\GG_1 \GG_2} +
\dfrac{\Omega_{\GG_1\GG_2}^2(\qq)}{\ww^2-\tww^2_{\GG_1\GG_2}(\qq)}
\end{equation} 

The two models differ in the approach used to compute the
parameters. [[ppmodel]]=1 derives the parameters such that the inverse
dielectric matrix is correctly reproduced at two different explicitly
calculated frequencies: the static limit ($\ww = 0$) and an additional imaginary
point located at [[ppmfrq]]. Unless the user overrides this, the default value
is calculated from the average electronic density of the system. The plasmon-
pole parameters of [[ppmodel]]=2 are calculated so as to reproduce the static
limit exactly and to fulfill a generalized frequency sum rule relating the
imaginary part of the many-body inverse dielectric matrix to the plasma
frequency and the charge density [[cite:Hybertsen1986]]

For a discussion of the models corresponding to [[ppmodel]]=3,4 we refer the
reader to the original papers cited in the documentation of the variable.

## Contour deformation technique
  
The contour deformation method was proposed in order to avoid having to deal
with quantities close to the real axis as much as possible [[cite:Lebegue2003]].
The integral over the real frequency axis can be transformed into an integral
over the contour depicted in red in the figure below. The integral over real
frequency is traded with an integration along the imaginary axis plus
contributions coming from the poles of the integrand lying inside the contour:

![](mbt_assets/contour.png)

\begin{equation}
\label{eq:GW_CD}
\Sigma_c(\ww) = \dfrac{i}{2\pi} \Bigl\{ 
 2\pi\,i \sum_{z_p}^\mcC \lim_{z\rightarrow z_p} G(z)\,{W}^{\text{c}}(z)\,(z-z_p) 
 -\int_{-\infty}^{+\infty} G(\ww+i\ww')\,{W}^{\text{c}}(i\ww') \dd(i\ww')
 \Bigr\}.
\end{equation}


In the above equation, the first sum is restricted to the poles lying inside the path $\mcC$. 
$W^c(z)$ represents the frequency dependent part of the screened
interaction, whose expression in reciprocal space is given by:

\begin{equation} 
W^{\text{c}}(\qq,\ww)_{\GG_1\GG_2} \equiv
\bigl(\tee^{-1}_{\GG_1\GG_2}(\qq,\ww) -\delta_{\GG_1,\GG_2}\bigr) \,{\tilde
v}_{\GG_1\GG_2}(\qq) 
\end{equation} 

The integration along the imaginary axis
is expected to converge quickly with respect to the number of sampled
frequencies since the integrand is typically very smooth. Only the residues of
the integrand have to be evaluated at the complex poles contributed by the
Green's function whose frequency dependence is known.

## Notations
  
The following shorthand notations are employed: 

$$ (1) \equiv (\rr_1,t_1) $$

$$ \delta(12) = \delta(\rr_1-\rr_2)\,\delta(t_1-t_2) $$ 

$$ \int \dd1 = \int
\dd\rr_1 \int_{-\infty}^{+\infty} \dd t_1 $$ 

$$ v(12)= v(\rr_1,\rr_2)\,\delta(t_1-t_2) $$ 

$$ 1^+ = (\rr_1,t_1 + \eta)_{\eta \rightarrow 0^+} $$ 

where $v(\rr_1, \rr_2)$ represents the bare Coulomb interaction, and $\eta$ is a positive infinitesimal.

The Fourier transforms for periodic lattice quantities are defined as

\begin{alignat}{2}\label{eq:FT_1point_convention} 
u(\rr)= \sum_\GG u(\GG)e^{i\GG\cdot\rr}, &\quad u(\GG) = \frac{1}{\Omega} \int_\Omega
u(\rr)e^{-i\GG\cdot\rr}\dd\rr 
\end{alignat}

\begin{equation}\label{eq:IFT_2points_convention} 
f(\rr_1,\rr_2)= \frac{1}{V}
\sum_{\substack{\qq \\\ \GG_1 \GG_2}} e^{i (\qq +\GG_1) \cdot \rr_1}\,f_{\GG_1
\GG_2}(\qq)\,e^{-i (\qq+\GG_2) \cdot \rr_2} 
\end{equation}

\begin{equation}\label{eq:FT_2points_convention} 
f_{\GG_1\GG_2}(\qq) =
\frac{1}{V} \iint_V e^{-i(\qq+\GG_1) \cdot \rr_1}\,f(\rr_1, \rr_2)\,e^{i
(\qq+\GG_2) \cdot \rr_2}\dd\rr_1\dd\rr_2
\end{equation} 

The volume of the unit cell is denoted with $\Omega$, while $V$ is the total volume of the crystal simulated
employing Born-von Karman periodic boundary condition. Unless otherwise
specified, Hartree atomic units will be used throughout.
---
authors: MG
---

$$
\newcommand{\lm}{{lm}}
\newcommand{\Ylm}{{Y_m^l}}
\newcommand{\rY}{{\mathcal Y}}
\newcommand{\rYlm}{{\mathcal Y}_m^l}
\newcommand{\rYLM}{{\mathcal Y}_M^L}
\newcommand{\lp}{{l'}}
\newcommand{\rrhat}{{\widehat\rr}}
\newcommand{\rrphat}{{{\widehat\rr}'}}
\newcommand{\mcP}{{\mathcal{P}}}
\newcommand{\df}{\equiv} %this required mathtools :=  
\newcommand{\li}{{l_i}}
\newcommand{\mi}{{m_i}}
\newcommand{\nj}{{n_j}}
\newcommand{\lj}{{l_j}}
\newcommand{\mj}{{m_j}}
\renewcommand{\mp}{{m'}}
\newcommand{\mcG}{{\mathcal G}}
\newcommand{\omcR}{{\hat\mcR}}
\newcommand{\mcR}{{\mathcal{R}}}
\newcommand{\ddO}{{\dd\Omega}}
\newcommand{\LM}{{LM}}
\newcommand{\mcRm}{\mcR^{-1}} 
\newcommand{\kpGhat}{\widehat{\kpG}}
\newcommand{\rrp}{{\bf r'}}
\newcommand{\Gaunt}{{\mathcal G}}
$$ 

# Complex and Real Spherical Harmonics

## Complex Spherical Harmonics

Complex spherical harmonics, $\Ylm$, are defined as the eigenfunctions of the orbital 
angular momentum operators, $\hat L^2$ and $\hat L_z$. 
They can be labeled by two quantum numbers, $l$ and $m$, related to the eigenvalues of 
$\hat L^2$ and $\hat L_z$: 

\begin{equation}
\hat L^2 \Ylm(\theta,\phi) = l(l+1) \Ylm(\theta,\phi),
\end{equation}

and 

\begin{equation}
\hat L_z \Ylm(\theta,\phi) = m \Ylm(\theta,\phi).
\end{equation}

Their explicit expression is:

\begin{equation}
\Ylm(\theta,\phi) = (-1)^m N_\lm P_m^l(\cos\theta) e^{im\phi},
\end{equation}

where $l$ takes non-negative integer values and the possible values for $m$ are 
integers from $-l$ to $l$. 
The two symbols $\theta$ and $\phi$ are used to denote angular spherical coordinates.
$N_\lm$ is the normalization factor:

\begin{equation}
N_\lm = \sqrt{\frac{2l+1}{4\pi} \dfrac{(l-m)!}{(l+m)!}}
\end{equation}

with $P_m^l$ being the associated Legendre polynomials.
Complex spherical harmonics are orthogonal functions on the sphere, i.e.:

\begin{equation}
\la\Ylm|Y_\mp^\lp\ra_\Omega = \delta_{l\lp}\delta_{m\mp},
\end{equation}

where $\la.|.\ra_\Omega$ denotes the integration over the unit sphere.
Furthermore, they form a complete basis set since any square-integrable 
function $f(\theta,\phi)$ can be expanded in series according to:

\begin{equation}
f(\theta,\phi) = \sum_{l=0}^\infty \sum_{m=-l}^l a_{\lm}\Ylm(\theta,\phi).
\end{equation}

Complex spherical harmonics satisfy two useful properties commonly employed in practical applications.
The parity properties:

\begin{equation}\label{eq:parity_property_Ylm}
{Y_m^l}^\dagger(\rrhat) = (-1)^m Y_{-m}^l (\rrhat),
\end{equation}

\begin{equation}
\Ylm(-\rrhat) = (-1)^l \Ylm(\rrhat),
\end{equation}

and the addition theorem:

\begin{equation}\label{eq:addition_theorem_CSH}
\sum_m \Ylm(\rrhat)\, \Ylm^\*(\rrphat) = 
\dfrac{2l+1}{4\pi}\,\mcP_l (\rrhat\cdot\rrphat),
\end{equation}

where $\mcP_l(q)$ are Legendre polynomials.
Equation \ref{eq:addition_theorem_CSH} is generally employed in the 
application of the nonlocal part of the Hamiltonian in the case of 
norm-conserving pseudopotentials when [[useylm]] = 0. 

## Real spherical harmonics <a id="RSH"><a>

Real spherical harmonics (RSH) are obtained by combining complex conjugate 
functions associated to opposite values of $m$. 
RSH are the most adequate basis functions for calculations in which atomic 
symmetry is important since they can be directly related to the irreducible
representations of the subgroups of $D_3$ [[cite:Blanco1997]].
Moreover, being real, they have half the memory requirement of complex spherical harmonics. 
This is clearly an advantage if high angular momenta are needed or several RHS values have to be stored in memory.
A possible definition for RHS is [[cite:Blanco1997]]:

\begin{equation}\label{eq:Definition_real_harmonics}
 \rYlm \df 
 \begin{cases}
  \quad \dfrac{(-1)^m}{\sqrt{2}}\,(\Ylm + \Ylm^\*) \quad  m > 0
  \\\\ 
  \quad Y_0^l  \quad m = 0 
  \\\\
  \quad \dfrac{(-1)^m}{i\sqrt{2}}\,(Y_{\lvert{m}\rvert}^l - {Y_{\lvert{m}\rvert}^l}^\*)  \quad  m < 0.
 \end{cases}
\end{equation}

Equation \ref{eq:Definition_real_harmonics} can be rewritten in matrix notation as

\begin{equation}\label{eq:Unitaty_transformation_Y_Re2Cmplx}
\underline\rY^l = U^l \underline{Y}^l,
\end{equation}

where $\underline\rY^l$ and $\underline{Y}^l$ are column vectors containing real and 
spherical harmonics ordered by increasing $m$ values.
$U^l$ is a unitary matrix whose explicit expression can be found in [[cite:Blanco1997]].

The basic properties of RSH can be easily derived from 
the properties of complex spherical harmonics by means of \ref{eq:Definition_real_harmonics}.

Reality
:
    \begin{equation}
    {\rYlm}^\*(\rrhat) = \rYlm(\rrhat),
    \end{equation}

Parity property
:
    \begin{equation}
    \rYlm(-\rrhat) = (-1)^l \rYlm (\rrhat),
    \end{equation}

Orthonormality
:
    \begin{equation}
    \la\rYlm|\rY_{\mp}^\lp\ra_\Omega = \delta_{l\lp} \delta_{m\mp}.
    \end{equation}

Certain on-site matrix elements include the integration of three real spherical harmonics, which 
can be evaluated exactly:

\begin{equation}\label{eq:Gaunt_def}
\Gaunt_{\li\mi\lj\mj}^{\LM} \df \int \rY_\mi^\li\,\rY_M^L\,\rY_\mj^\lj\ddO.
\end{equation}

The above integral is known in the literature under the name of Gaunt coefficient.
Due to selection rules, the integral in equation \ref{eq:Gaunt_def} is nonzero only if $\mi+\mj = M$
and $\lvert{\li+\lj}\rvert \ge (L,L+2,L+4, \dots) \ge \lvert{\li-\lj}\rvert$.

!!! note

    The expression for $\mcG$ follows the internal conventions 
    used inside the ABINIT code and slightly differs from the one reported in standard text-books.

## Symmetry transformation of RSH

The use of symmetry properties is of fundamental importance to simplify the
determination of electronic structure as it greatly 
reduces the number of non-equivalent integrals that have to be  computed,
as well as the size of any tables stored in memory.

Frequently one has to evaluate the value of a RSH at a rotated point after 
the application of a symmetry operation belonging to the space group of the crystal.
This usually occurs when $\kk$-dependent matrix elements have to be symmetrized in the full 
Brillouin zone starting from the knowledge of their symmetrical image in the irreducible wedge. 
The effect of a proper or improper rotation on a RSH can be deduced from:

\begin{equation}
\label{eq:RSH_rotation}
\omcR\,\rYlm(\rrhat) = \rYlm(\mcRm \rrhat) = 
\sum_\alpha D^l_{\alpha m}(\mcR)\,\rYlm(\rrhat).
\end{equation}

That is, spherical harmonics of given $l$ are transformed into a linear combination
of RHS of same $l$ where the coefficients are given by:

\begin{equation}
D^l_{\alpha m}(\mcR) \df \la\rY^l_\alpha|\hat\mcR|\rYlm\ra.
\end{equation}

## Useful expansions in terms of RSH

By means of the unitary transformation given in \ref{eq:Definition_real_harmonics}, 
it is possible to rewrite the Rayleigh expansion of a plane wave in terms of RSH as

\begin{equation}
\label{eq:Rayleigh_expansion_real_Ylm}
e^{i(\kpG)\cdot\rr} = 
4\pi \sum_\lm i^l j_l(\lvert{\kpG}\rvert r)\,\rYlm(\kpGhat)\,\rYlm(\rrhat)
\end{equation}

with $j_l(x)$ the spherical Bessel function of order $l$. 
In a similar way, the electrostatic potentials can be expanded in a real spherical harmonics basis set.
The final expression is very similar to the one obtained in the case of complex spherical harmonics:

\begin{equation}\label{eq:Expansion_Coulombian_RSH}
\dfrac{1}{\lvert{\rr-\rrp}\rvert} = \sum_{l=0}^\infty 
\dfrac{4\pi}{2l+1} 
\sum_{m=-l}^{m=l}
\dfrac{r_<^l}{r_>^{l+1}} \rYlm(\rrhat)\rYlm(\rrphat),
\end{equation}

with $r_< \df \min(r,r')$ and $r_> \df \max(r,r')$.
---
authors: RC
---

# List of 230 3D symmetry space groups

This page presents the list of the 230 3D symmetry space groups with
characteristics, and also, if applicable, derivatives of the space group. 

This list of symmetry groups is part of the on-line help of the ABINIT code.
Besides *classical* crystallographic information, it provides the values of
certain symmetry-related variables used in the input file of the ABINIT code.

The table entries are as follows:

SPGROUP
:   *SPace GROUP number* as found in the International Tables of Crystallography, 1983 edition [[cite:Hahn1983]].

SPGAXOR
:   *SPace Group AXis ORientation*, the orientation of the unit cell axis. The allowed values are: 

    * Trigonal groups:
        * 1 represents the hexagonal axes
        * 2 represents the rhombohedral axes
    * Orthorhombic groups:
        * 1 abc -> abc
        * 2 abc -> cab
        * 3 abc -> bca
        * 4 abc -> acb
        * 5 abc -> bac
        * 6 abc -> cba
    * Monoclinic: 3 or 9 possibilities depending on the space group

SPGORIG
:   *SPace Group ORIGin*, the position of the origin in the unit cell. 
    The allowed values are 1 and 2. they correspond to the actual choices in the 
    International Tables of Crystallography, 1986 edition. 

BRVLTT
:   *BRaVais LaTTice*, the type of Bravais lattice. The allowed values are:

    * 1 = Primitive with no associated translations
    * 2 = Inner centered with (a/2 + b/2 + c/2) associated translation
    * 3 = Face centered with (a/2 + b/2; b/2 + c/2; c/2 + a/2) associated translations
    * 4 = C - centered with (a/2 + b/2) associated translation
    * 5 = A - centered with (b/2 + c/2) associated translation
    * 6 = B - centered with (c/2 + a/2) associated translation
    * 7 = Rhombohedral lattice

INTERNATIONAL
: the INTERNATIONAL notation of the space group

SCHOENFLIES
: the equivalent SCHOENFLIES notation of the space group

MULTIPLICITY (x y z)
: the maximum multiplicity, after applying Bravais lattice translations,  corresponding to a general position (x y z)


## List of 230 3D symmetry space groups

This section presents the list of the 230 3D symmetry space groups with
characteristics, and also, if applicable, derivatives of the space group. 

<TABLE BORDER = "2" cellspacing = "2" cellpadding = "5">
<tr>
<td width = 10%>SPGROUP </td>
<td width = 10%> SPGAXOR </td>
<td width = 10%> SPGORIG </td>
<td width = 10%> BRVLTT </td>
<td width = 22%> INTERNATIONAL </td>
<td width = 20%> SCHOENFLIES </td>
<td width = 10%> ORDER </td>
</tr>

<tr><td>  1  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P1  </td><td>  C1<SUP>1</SUP>  </td><td>  1  </td></tr>
<tr><td>  2  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P-1  </td><td>  Ci<SUP>1</SUP>  </td><td>  2  </td></tr>
<tr><td>  3  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P2_b = P121  </td><td>  C2<SUP>1</SUP>  </td><td>  2  </td></tr>
<tr><td>  3  </td><td>  3  </td><td>  1  </td><td>  1  </td><td>  P2_c = P112  </td><td>  C2<SUP>1</SUP>  </td><td>  2  </td></tr>
<tr><td>  3  </td><td>  2  </td><td>  1  </td><td>  1  </td><td>  P2_a = P211  </td><td>  C2<SUP>1</SUP>  </td><td>  2  </td></tr>
<tr><td>  4  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P21_b = P1211  </td><td>  C2<SUP>2</SUP>  </td><td>  2  </td></tr>
<tr><td>  4  </td><td>  3  </td><td>  1  </td><td>  1  </td><td>  P21_c = P1121  </td><td>  C2<SUP>2</SUP>  </td><td>  2  </td></tr>
<tr><td>  4  </td><td>  2  </td><td>  1  </td><td>  1  </td><td>  P21_a = P2111  </td><td>  C2<SUP>2</SUP>  </td><td>  2  </td></tr>
<tr><td>  5  </td><td>  1  </td><td>  1  </td><td>  4  </td><td>  C2_b1 = C121  </td><td>  C2<SUP>3</SUP>  </td><td>  4  </td></tr>
<tr><td>  5  </td><td>  5  </td><td>  1  </td><td>  5  </td><td>  C2_b2 = A121  </td><td>  C2<SUP>3</SUP>  </td><td>  4  </td></tr>
<tr><td>  5  </td><td>  6  </td><td>  1  </td><td>  2  </td><td>  C2_b3 = I121  </td><td>  C2<SUP>3</SUP>  </td><td>  4  </td></tr>
<tr><td>  5  </td><td>  7  </td><td>  1  </td><td>  5  </td><td>  C2_c1 = A112  </td><td>  C2<SUP>3</SUP>  </td><td>  4  </td></tr>
<tr><td>  5  </td><td>  8  </td><td>  1  </td><td>  6  </td><td>  C2_c2 = B112 = B2  </td><td>  C2<SUP>3</SUP>  </td><td>  4  </td></tr>
<tr><td>  5  </td><td>  9  </td><td>  1  </td><td>  2  </td><td>  C2_c3 = I112  </td><td>  C2<SUP>3</SUP>  </td><td>  4  </td></tr>
<tr><td>  5  </td><td>  2  </td><td>  1  </td><td>  6  </td><td>  C2_a1 = B211  </td><td>  C2<SUP>3</SUP>  </td><td>  4  </td></tr>
<tr><td>  5  </td><td>  3  </td><td>  1  </td><td>  4  </td><td>  C2_a2 = C211  </td><td>  C2<SUP>3</SUP>  </td><td>  4  </td></tr>
<tr><td>  5  </td><td>  4  </td><td>  1  </td><td>  2  </td><td>  C2_a3 = I211  </td><td>  C2<SUP>3</SUP>  </td><td>  4  </td></tr>
<tr><td>  6  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  Pm_b = P1m1  </td><td>  Cs<SUP>1</SUP>  </td><td>  2  </td></tr>
<tr><td>  6  </td><td>  3  </td><td>  1  </td><td>  1  </td><td>  Pm_c = P11m  </td><td>  Cs<SUP>1</SUP>  </td><td>  2  </td></tr>
<tr><td>  6  </td><td>  2  </td><td>  1  </td><td>  1  </td><td>  Pm_a = Pm11  </td><td>  Cs<SUP>1</SUP>  </td><td>  2  </td></tr>
<tr><td>  7  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  Pc_b1 = P1c1  </td><td>  Cs<SUP>2</SUP>  </td><td>  2  </td></tr>
<tr><td>  7  </td><td>  5  </td><td>  1  </td><td>  1  </td><td>  Pc_b2 = P1n1  </td><td>  Cs<SUP>2</SUP>  </td><td>  2  </td></tr>
<tr><td>  7  </td><td>  6  </td><td>  1  </td><td>  1  </td><td>  Pc_b3 = P1a1  </td><td>  Cs<SUP>2</SUP>  </td><td>  2  </td></tr>
<tr><td>  7  </td><td>  7  </td><td>  1  </td><td>  1  </td><td>  Pc_c1 = P11a  </td><td>  Cs<SUP>2</SUP>  </td><td>  2  </td></tr>
<tr><td>  7  </td><td>  8  </td><td>  1  </td><td>  1  </td><td>  Pc_c2 = P11n  </td><td>  Cs<SUP>2</SUP>  </td><td>  2  </td></tr>
<tr><td>  7  </td><td>  9  </td><td>  1  </td><td>  1  </td><td>  Pc_c3 = P11b = Pb  </td><td>  Cs<SUP>2</SUP>  </td><td>  2  </td></tr>
<tr><td>  7  </td><td>  2  </td><td>  1  </td><td>  1  </td><td>  Pc_a1 = Pb11  </td><td>  Cs<SUP>2</SUP>  </td><td>  2  </td></tr>
<tr><td>  7  </td><td>  3  </td><td>  1  </td><td>  1  </td><td>  Pc_a2 = Pn11  </td><td>  Cs<SUP>2</SUP>  </td><td>  2  </td></tr>
<tr><td>  7  </td><td>  4  </td><td>  1  </td><td>  1  </td><td>  Pc_a3 = Pc11  </td><td>  Cs<SUP>2</SUP>  </td><td>  2  </td></tr>
<tr><td>  8  </td><td>  1  </td><td>  1  </td><td>  4  </td><td>  Cm_b1 = C1m1  </td><td>  Cs<SUP>3</SUP>  </td><td>  4  </td></tr>
<tr><td>  8  </td><td>  5  </td><td>  1  </td><td>  5  </td><td>  Cm_b2 = A1m1  </td><td>  Cs<SUP>3</SUP>  </td><td>  4  </td></tr>
<tr><td>  8  </td><td>  6  </td><td>  1  </td><td>  2  </td><td>  Cm_b3 = I1m1  </td><td>  Cs<SUP>3</SUP>  </td><td>  4  </td></tr>
<tr><td>  8  </td><td>  7  </td><td>  1  </td><td>  5  </td><td>  Cm_c1 = A11m  </td><td>  Cs<SUP>3</SUP>  </td><td>  4  </td></tr>
<tr><td>  8  </td><td>  8  </td><td>  1  </td><td>  6  </td><td>  Cm_c2 = B11m = Bm  </td><td>  Cs<SUP>3</SUP>  </td><td>  4  </td></tr>
<tr><td>  8  </td><td>  9  </td><td>  1  </td><td>  2  </td><td>  Cm_c3 = I11m  </td><td>  Cs<SUP>3</SUP>  </td><td>  4  </td></tr>
<tr><td>  8  </td><td>  2  </td><td>  1  </td><td>  6  </td><td>  Cm_a1 = Bm11  </td><td>  Cs<SUP>3</SUP>  </td><td>  4  </td></tr>
<tr><td>  8  </td><td>  3  </td><td>  1  </td><td>  4  </td><td>  Cm_a2 = Cm11  </td><td>  Cs<SUP>3</SUP>  </td><td>  4  </td></tr>
<tr><td>  8  </td><td>  4  </td><td>  1  </td><td>  2  </td><td>  Cm_a3 = Im11  </td><td>  Cs<SUP>3</SUP>  </td><td>  4  </td></tr>
<tr><td>  9  </td><td>  1  </td><td>  1  </td><td>  4  </td><td>  Cc_b1 = C1c1  </td><td>  Cs<SUP>4</SUP>  </td><td>  4  </td></tr>
<tr><td>  9  </td><td>  5  </td><td>  1  </td><td>  5  </td><td>  Cc_b2 = A1n1  </td><td>  Cs<SUP>4</SUP>  </td><td>  4  </td></tr>
<tr><td>  9  </td><td>  6  </td><td>  1  </td><td>  2  </td><td>  Cc_b3 = I1a1  </td><td>  Cs<SUP>4</SUP>  </td><td>  4  </td></tr>
<tr><td>  9  </td><td>  7  </td><td>  1  </td><td>  5  </td><td>  Cc_c1 = A11a  </td><td>  Cs<SUP>4</SUP>  </td><td>  4  </td></tr>
<tr><td>  9  </td><td>  8  </td><td>  1  </td><td>  6  </td><td>  Cc_c2 = B11n  </td><td>  Cs<SUP>4</SUP>  </td><td>  4  </td></tr>
<tr><td>  9  </td><td>  9  </td><td>  1  </td><td>  2  </td><td>  Cc_c3 = I11b  </td><td>  Cs<SUP>4</SUP>  </td><td>  4  </td></tr>
<tr><td>  9  </td><td>  2  </td><td>  1  </td><td>  6  </td><td>  Cc_a1 = Bb11  </td><td>  Cs<SUP>4</SUP>  </td><td>  4  </td></tr>
<tr><td>  9  </td><td>  3  </td><td>  1  </td><td>  4  </td><td>  Cc_a2 = Cn11  </td><td>  Cs<SUP>4</SUP>  </td><td>  4  </td></tr>
<tr><td>  9  </td><td>  4  </td><td>  1  </td><td>  2  </td><td>  Cc_a3 = Ic11  </td><td>  Cs<SUP>4</SUP>  </td><td>  4  </td></tr>
<tr><td>  10  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P2/m_b = P12/m1  </td><td>  C2h<SUP>1</SUP>  </td><td>  4  </td></tr>
<tr><td>  10  </td><td>  3  </td><td>  1  </td><td>  1  </td><td>  P2/m_c = P112/m  </td><td>  C2h<SUP>1</SUP>  </td><td>  4  </td></tr>
<tr><td>  10  </td><td>  2  </td><td>  1  </td><td>  1  </td><td>  P2/m_a = P2/m11  </td><td>  C2h<SUP>1</SUP>  </td><td>  4  </td></tr>
<tr><td>  11  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P2<SUB>1</SUB>/m_b = P12<SUB>1</SUB>/m1  </td><td>  C2h<SUP>2</SUP>  </td><td>  4  </td></tr>
<tr><td>  11  </td><td>  3  </td><td>  1  </td><td>  1  </td><td>  P2<SUB>1</SUB>/m_c = P112<SUB>1</SUB>/m  </td><td>  C2h<SUP>2</SUP>  </td><td>  4  </td></tr>
<tr><td>  11  </td><td>  2  </td><td>  1  </td><td>  1  </td><td>  P2<SUB>1</SUB>/m_a = P2<SUB>1</SUB>/m11  </td><td>  C2h<SUP>2</SUP>  </td><td>  4  </td></tr>
<tr><td>  12  </td><td>  1  </td><td>  1  </td><td>  4  </td><td>  C2/m_b1 = C12/m1  </td><td>  C2h<SUP>3</SUP>  </td><td>  8  </td></tr>
<tr><td>  12  </td><td>  5  </td><td>  1  </td><td>  5  </td><td>  C2/m_b2 = A12/m1  </td><td>  C2h<SUP>3</SUP>  </td><td>  8  </td></tr>
<tr><td>  12  </td><td>  6  </td><td>  1  </td><td>  2  </td><td>  C2/m_b3 = I12/m1  </td><td>  C2h<SUP>3</SUP>  </td><td>  8  </td></tr>
<tr><td>  12  </td><td>  7  </td><td>  1  </td><td>  5  </td><td>  C2/m_c1 = A112/m  </td><td>  C2h<SUP>3</SUP>  </td><td>  8  </td></tr>
<tr><td>  12  </td><td>  8  </td><td>  1  </td><td>  6  </td><td>  C2/m_c2 = B112/m = B2/m  </td><td>  C2h<SUP>3</SUP>  </td><td>  8  </td></tr>
<tr><td>  12  </td><td>  9  </td><td>  1  </td><td>  2  </td><td>  C2/m_c3 = I112/m  </td><td>  C2h<SUP>3</SUP>  </td><td>  8  </td></tr>
<tr><td>  12  </td><td>  2  </td><td>  1  </td><td>  6  </td><td>  C2/m_a1 = B2/m11  </td><td>  C2h<SUP>3</SUP>  </td><td>  8  </td></tr>
<tr><td>  12  </td><td>  3  </td><td>  1  </td><td>  4  </td><td>  C2/m_a2 = C2/m11  </td><td>  C2h<SUP>3</SUP>  </td><td>  8  </td></tr>
<tr><td>  12  </td><td>  4  </td><td>  1  </td><td>  2  </td><td>  C2/m_a3 = I2/m11  </td><td>  C2h<SUP>3</SUP>  </td><td>  8  </td></tr>
<tr><td>  13  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P2/c_b1 = P12/c1  </td><td>  C2h<SUP>4</SUP>  </td><td>  4  </td></tr>
<tr><td>  13  </td><td>  5  </td><td>  1  </td><td>  1  </td><td>  P2/c_b2 = P12/n1  </td><td>  C2h<SUP>4</SUP>  </td><td>  4  </td></tr>
<tr><td>  13  </td><td>  6  </td><td>  1  </td><td>  1  </td><td>  P2/c_b3 = P12/a1  </td><td>  C2h<SUP>4</SUP>  </td><td>  4  </td></tr>
<tr><td>  13  </td><td>  7  </td><td>  1  </td><td>  1  </td><td>  P2/c_c1 = P112/a  </td><td>  C2h<SUP>4</SUP>  </td><td>  4  </td></tr>
<tr><td>  13  </td><td>  8  </td><td>  1  </td><td>  1  </td><td>  P2/c_c2 = P112/n  </td><td>  C2h<SUP>4</SUP>  </td><td>  4  </td></tr>
<tr><td>  13  </td><td>  9  </td><td>  1  </td><td>  1  </td><td>  P2/c_c3 = P112/b = P2/b  </td><td>  C2h<SUP>4</SUP>  </td><td>  4  </td></tr>
<tr><td>  13  </td><td>  2  </td><td>  1  </td><td>  1  </td><td>  P2/c_a1 = P2/b11  </td><td>  C2h<SUP>4</SUP>  </td><td>  4  </td></tr>
<tr><td>  13  </td><td>  3  </td><td>  1  </td><td>  1  </td><td>  P2/c_a2 = P2/n11  </td><td>  C2h<SUP>4</SUP>  </td><td>  4  </td></tr>
<tr><td>  13  </td><td>  4  </td><td>  1  </td><td>  1  </td><td>  P2/c_a3 = P2/c11  </td><td>  C2h<SUP>4</SUP>  </td><td>  4  </td></tr>
<tr><td>  14  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P2<SUB>1</SUB>/c_b1 = P12<SUB>1</SUB>/c1  </td><td>  C2h<SUP>5</SUP>  </td><td>  4  </td></tr>
<tr><td>  14  </td><td>  5  </td><td>  1  </td><td>  1  </td><td>  P2<SUB>1</SUB>/c_b2 = P12<SUB>1</SUB>/n1  </td><td>  C2h<SUP>5</SUP>  </td><td>  4  </td></tr>
<tr><td>  14  </td><td>  6  </td><td>  1  </td><td>  1  </td><td>  P2<SUB>1</SUB>/c_b3 = P12<SUB>1</SUB>/a1  </td><td>  C2h<SUP>5</SUP>  </td><td>  4  </td></tr>
<tr><td>  14  </td><td>  7  </td><td>  1  </td><td>  1  </td><td>  P2<SUB>1</SUB>/c_c1 = P112<SUB>1</SUB>/a  </td><td>  C2h<SUP>5</SUP>  </td><td>  4  </td></tr>
<tr><td>  14  </td><td>  8  </td><td>  1  </td><td>  1  </td><td>  P2<SUB>1</SUB>/c_c2 = P112<SUB>1</SUB>/n  </td><td>  C2h<SUP>5</SUP>  </td><td>  4  </td></tr>
<tr><td>  14  </td><td>  9  </td><td>  1  </td><td>  1  </td><td>  P2<SUB>1</SUB>/c_c3 = P112<SUB>1</SUB>/b = P2<SUB>1</SUB>/b  </td><td>  C2h<SUP>5</SUP>  </td><td>  4  </td></tr>
<tr><td>  14  </td><td>  2  </td><td>  1  </td><td>  1  </td><td>  P2<SUB>1</SUB>/c_a1 = P2<SUB>1</SUB>/b11  </td><td>  C2h<SUP>5</SUP>  </td><td>  4  </td></tr>
<tr><td>  14  </td><td>  3  </td><td>  1  </td><td>  1  </td><td>  P2<SUB>1</SUB>/c_a2 = P2<SUB>1</SUB>/n11  </td><td>  C2h<SUP>5</SUP>  </td><td>  4  </td></tr>
<tr><td>  14  </td><td>  4  </td><td>  1  </td><td>  1  </td><td>  P2<SUB>1</SUB>/c_a3 = P2<SUB>1</SUB>/c11  </td><td>  C2h<SUP>5</SUP>  </td><td>  4  </td></tr>
<tr><td>  15  </td><td>  1  </td><td>  1  </td><td>  4  </td><td>  C2/c_b1 = C12/c1  </td><td>  C2h<SUP>6</SUP>  </td><td>  8  </td></tr>
<tr><td>  15  </td><td>  5  </td><td>  1  </td><td>  5  </td><td>  C2/c_b2 = A12/n1  </td><td>  C2h<SUP>6</SUP>  </td><td>  8  </td></tr>
<tr><td>  15  </td><td>  6  </td><td>  1  </td><td>  2  </td><td>  C2/c_b3 = I12/a1  </td><td>  C2h<SUP>6</SUP>  </td><td>  8  </td></tr>
<tr><td>  15  </td><td>  7  </td><td>  1  </td><td>  4  </td><td>  C2/c_c1 = A112/a  </td><td>  C2h<SUP>6</SUP>  </td><td>  8  </td></tr>
<tr><td>  15  </td><td>  8  </td><td>  1  </td><td>  6  </td><td>  C2/c_c2 = B112/n  </td><td>  C2h<SUP>6</SUP>  </td><td>  8  </td></tr>
<tr><td>  15  </td><td>  9  </td><td>  1  </td><td>  2  </td><td>  C2/c_c3 = I112/b  </td><td>  C2h<SUP>6</SUP>  </td><td>  8  </td></tr>
<tr><td>  15  </td><td>  2  </td><td>  1  </td><td>  6  </td><td>  C2/c_a1 = B2/b11  </td><td>  C2h<SUP>6</SUP>  </td><td>  8  </td></tr>
<tr><td>  15  </td><td>  3  </td><td>  1  </td><td>  4  </td><td>  C2/c_a2 = C2/n11  </td><td>  C2h<SUP>6</SUP>  </td><td>  8  </td></tr>
<tr><td>  15  </td><td>  4  </td><td>  1  </td><td>  2  </td><td>  C2/c_a3 = I2/c11  </td><td>  C2h<SUP>6</SUP>  </td><td>  8  </td></tr>
<tr><td>  16  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P222  </td><td>  D2<SUP>1</SUP>  </td><td>  4  </td></tr>
<tr><td>  17  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P2221  </td><td>  D2<SUP>2</SUP>  </td><td>  4  </td></tr>
<tr><td>  17  </td><td>  2  </td><td>  1  </td><td>  1  </td><td>  P2<SUB>1</SUB>22  </td><td>  D2<SUP>2</SUP>  </td><td>  4  </td></tr>
<tr><td>  17  </td><td>  3  </td><td>  1  </td><td>  1  </td><td>  P22<SUB>1</SUB>2  </td><td>  D2<SUP>2</SUP>  </td><td>  4  </td></tr>
<tr><td>  18  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P2<SUB>1</SUB>2<SUB>1</SUB>2  </td><td>  D2<SUP>3</SUP>  </td><td>  4  </td></tr>
<tr><td>  18  </td><td>  2  </td><td>  1  </td><td>  1  </td><td>  P22<SUB>1</SUB>2<SUB>1</SUB>  </td><td>  D2<SUP>3</SUP>  </td><td>  4  </td></tr>
<tr><td>  18  </td><td>  3  </td><td>  1  </td><td>  1  </td><td>  P2<SUB>1</SUB>22<SUB>1</SUB>  </td><td>  D2<SUP>3</SUP>  </td><td>  4  </td></tr>
<tr><td>  19  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P2<SUB>1</SUB>2<SUB>1</SUB>2<SUB>1</SUB>  </td><td>  D2<SUP>4</SUP>  </td><td>  4  </td></tr>
<tr><td>  20  </td><td>  1  </td><td>  1  </td><td>  4  </td><td>  C222<SUB>1</SUB>  </td><td>  D2<SUP>5</SUP>  </td><td>  8  </td></tr>
<tr><td>  20  </td><td>  2  </td><td>  1  </td><td>  5  </td><td>  A2<SUB>1</SUB>22  </td><td>  D2<SUP>5</SUP>  </td><td>  8  </td></tr>
<tr><td>  20  </td><td>  3  </td><td>  1  </td><td>  6  </td><td>  B22<SUB>1</SUB>2  </td><td>  D2<SUP>5</SUP>  </td><td>  8  </td></tr>
<tr><td>  21  </td><td>  1  </td><td>  1  </td><td>  4  </td><td>  C222  </td><td>  D2<SUP>6</SUP>  </td><td>  8  </td></tr>
<tr><td>  21  </td><td>  2  </td><td>  1  </td><td>  5  </td><td>  A222  </td><td>  D2<SUP>6</SUP>  </td><td>  8  </td></tr>
<tr><td>  21  </td><td>  3  </td><td>  1  </td><td>  6  </td><td>  B222  </td><td>  D2<SUP>6</SUP>  </td><td>  8  </td></tr>
<tr><td>  22  </td><td>  1  </td><td>  1  </td><td>  3  </td><td>  F222  </td><td>  D2<SUP>7</SUP>  </td><td>  16  </td></tr>
<tr><td>  23  </td><td>  1  </td><td>  1  </td><td>  2  </td><td>  I222  </td><td>  D2<SUP>8</SUP>  </td><td>  8  </td></tr>
<tr><td>  24  </td><td>  1  </td><td>  1  </td><td>  2  </td><td>  I2<SUB>1</SUB>2<SUB>1</SUB>2<SUB>1</SUB>  </td><td>  D2<SUP>9</SUP>  </td><td>  8  </td></tr>
<tr><td>  25  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  Pmm2  </td><td>  C2v<SUP>1</SUP>  </td><td>  4  </td></tr>
<tr><td>  25  </td><td>  2  </td><td>  1  </td><td>  1  </td><td>  P2mm  </td><td>  C2v<SUP>1</SUP>  </td><td>  4  </td></tr>
<tr><td>  25  </td><td>  3  </td><td>  1  </td><td>  1  </td><td>  Pm2m  </td><td>  C2v<SUP>1</SUP>  </td><td>  4  </td></tr>
<tr><td>  26  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  Pmc2<SUB>1</SUB>  </td><td>  C2v<SUP>2</SUP>  </td><td>  4  </td></tr>
<tr><td>  26  </td><td>  5  </td><td>  1  </td><td>  1  </td><td>  Pcm2<SUB>1</SUB>  </td><td>  C2v<SUP>2</SUP>  </td><td>  4  </td></tr>
<tr><td>  26  </td><td>  2  </td><td>  1  </td><td>  1  </td><td>  P2<SUB>1</SUB>ma  </td><td>  C2v<SUP>2</SUP>  </td><td>  4  </td></tr>
<tr><td>  26  </td><td>  6  </td><td>  1  </td><td>  1  </td><td>  P2<SUB>1</SUB>am  </td><td>  C2v<SUP>2</SUP>  </td><td>  4  </td></tr>
<tr><td>  26  </td><td>  3  </td><td>  1  </td><td>  1  </td><td>  Pb2<SUB>1</SUB>m  </td><td>  C2v<SUP>2</SUP>  </td><td>  4  </td></tr>
<tr><td>  26  </td><td>  4  </td><td>  1  </td><td>  1  </td><td>  Pm2<SUB>1</SUB>b  </td><td>  C2v<SUP>2</SUP>  </td><td>  4  </td></tr>
<tr><td>  27  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  Pcc2  </td><td>  C2v<SUP>3</SUP>  </td><td>  4  </td></tr>
<tr><td>  27  </td><td>  2  </td><td>  1  </td><td>  1  </td><td>  P2aa  </td><td>  C2v<SUP>3</SUP>  </td><td>  4  </td></tr>
<tr><td>  27  </td><td>  3  </td><td>  1  </td><td>  1  </td><td>  Pb2b  </td><td>  C2v<SUP>3</SUP>  </td><td>  4  </td></tr>
<tr><td>  28  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  Pma2  </td><td>  C2v<SUP>4</SUP>  </td><td>  4  </td></tr>
<tr><td>  28  </td><td>  5  </td><td>  1  </td><td>  1  </td><td>  Pbm2  </td><td>  C2v<SUP>4</SUP>  </td><td>  4  </td></tr>
<tr><td>  28  </td><td>  2  </td><td>  1  </td><td>  1  </td><td>  P2mb  </td><td>  C2v<SUP>4</SUP>  </td><td>  4  </td></tr>
<tr><td>  28  </td><td>  6  </td><td>  1  </td><td>  1  </td><td>  P2cm  </td><td>  C2v<SUP>4</SUP>  </td><td>  4  </td></tr>
<tr><td>  28  </td><td>  3  </td><td>  1  </td><td>  1  </td><td>  Pc2m  </td><td>  C2v<SUP>4</SUP>  </td><td>  4  </td></tr>
<tr><td>  28  </td><td>  4  </td><td>  1  </td><td>  1  </td><td>  Pm2a  </td><td>  C2v<SUP>4</SUP>  </td><td>  4  </td></tr>
<tr><td>  29  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  Pca2<SUB>1</SUB>  </td><td>  C2v<SUP>5</SUP>  </td><td>  4  </td></tr>
<tr><td>  29  </td><td>  5  </td><td>  1  </td><td>  1  </td><td>  Pbc2<SUB>1</SUB>  </td><td>  C2v<SUP>5</SUP>  </td><td>  4  </td></tr>
<tr><td>  29  </td><td>  2  </td><td>  1  </td><td>  1  </td><td>  P2<SUB>1</SUB>ab  </td><td>  C2v<SUP>5</SUP>  </td><td>  4  </td></tr>
<tr><td>  29  </td><td>  6  </td><td>  1  </td><td>  1  </td><td>  P2<SUB>1</SUB>ca  </td><td>  C2v<SUP>5</SUP>  </td><td>  4  </td></tr>
<tr><td>  29  </td><td>  3  </td><td>  1  </td><td>  1  </td><td>  Pc2<SUB>1</SUB>b  </td><td>  C2v<SUP>5</SUP>  </td><td>  4  </td></tr>
<tr><td>  29  </td><td>  4  </td><td>  1  </td><td>  1  </td><td>  Pb2<SUB>1</SUB>a  </td><td>  C2v<SUP>5</SUP>  </td><td>  4  </td></tr>
<tr><td>  30  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  Pnc2  </td><td>  C2v<SUP>6</SUP>  </td><td>  4  </td></tr>
<tr><td>  30  </td><td>  5  </td><td>  1  </td><td>  1  </td><td>  Pcn2  </td><td>  C2v<SUP>6</SUP>  </td><td>  4  </td></tr>
<tr><td>  30  </td><td>  2  </td><td>  1  </td><td>  1  </td><td>  P2na  </td><td>  C2v<SUP>6</SUP>  </td><td>  4  </td></tr>
<tr><td>  30  </td><td>  6  </td><td>  1  </td><td>  1  </td><td>  P2an  </td><td>  C2v<SUP>6</SUP>  </td><td>  4  </td></tr>
<tr><td>  30  </td><td>  3  </td><td>  1  </td><td>  1  </td><td>  Pb2n  </td><td>  C2v<SUP>6</SUP>  </td><td>  4  </td></tr>
<tr><td>  30  </td><td>  4  </td><td>  1  </td><td>  1  </td><td>  Pn2b  </td><td>  C2v<SUP>6</SUP>  </td><td>  4  </td></tr>
<tr><td>  31  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  Pmn2<SUB>1</SUB>  </td><td>  C2v<SUP>7</SUP>  </td><td>  4  </td></tr>
<tr><td>  31  </td><td>  5  </td><td>  1  </td><td>  1  </td><td>  Pnm2<SUB>1</SUB>  </td><td>  C2v<SUP>7</SUP>  </td><td>  4  </td></tr>
<tr><td>  31  </td><td>  2  </td><td>  1  </td><td>  1  </td><td>  P2<SUB>1</SUB>mn  </td><td>  C2v<SUP>7</SUP>  </td><td>  4  </td></tr>
<tr><td>  31  </td><td>  6  </td><td>  1  </td><td>  1  </td><td>  P2<SUB>1</SUB>nm  </td><td>  C2v<SUP>7</SUP>  </td><td>  4  </td></tr>
<tr><td>  31  </td><td>  3  </td><td>  1  </td><td>  1  </td><td>  Pn2<SUB>1</SUB>m  </td><td>  C2v<SUP>7</SUP>  </td><td>  4  </td></tr>
<tr><td>  31  </td><td>  4  </td><td>  1  </td><td>  1  </td><td>  Pm2<SUB>1</SUB>n  </td><td>  C2v<SUP>7</SUP>  </td><td>  4  </td></tr>
<tr><td>  32  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  Pba2  </td><td>  C2v<SUP>8</SUP>  </td><td>  4  </td></tr>
<tr><td>  32  </td><td>  2  </td><td>  1  </td><td>  1  </td><td>  P2cb  </td><td>  C2v<SUP>8</SUP>  </td><td>  4  </td></tr>
<tr><td>  32  </td><td>  3  </td><td>  1  </td><td>  1  </td><td>  Pc2a  </td><td>  C2v<SUP>8</SUP>  </td><td>  4  </td></tr>
<tr><td>  33  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  Pna2<SUB>1</SUB>  </td><td>  C2v<SUP>9</SUP>  </td><td>  4  </td></tr>
<tr><td>  33  </td><td>  5  </td><td>  1  </td><td>  1  </td><td>  Pbn2<SUB>1</SUB>  </td><td>  C2v<SUP>9</SUP>  </td><td>  4  </td></tr>
<tr><td>  33  </td><td>  2  </td><td>  1  </td><td>  1  </td><td>  P2<SUB>1</SUB>nb  </td><td>  C2v<SUP>9</SUP>  </td><td>  4  </td></tr>
<tr><td>  33  </td><td>  6  </td><td>  1  </td><td>  1  </td><td>  P2<SUB>1</SUB>cn  </td><td>  C2v<SUP>9</SUP>  </td><td>  4  </td></tr>
<tr><td>  33  </td><td>  3  </td><td>  1  </td><td>  1  </td><td>  Pc2<SUB>1</SUB>n  </td><td>  C2v<SUP>9</SUP>  </td><td>  4  </td></tr>
<tr><td>  33  </td><td>  4  </td><td>  1  </td><td>  1  </td><td>  Pn2<SUB>1</SUB>a  </td><td>  C2v<SUP>9</SUP>  </td><td>  4  </td></tr>
<tr><td>  34  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  Pnn2  </td><td>  C2v<SUP>10</SUP>  </td><td>  4  </td></tr>
<tr><td>  34  </td><td>  2  </td><td>  1  </td><td>  1  </td><td>  P2nn  </td><td>  C2v<SUP>10</SUP>  </td><td>  4  </td></tr>
<tr><td>  34  </td><td>  3  </td><td>  1  </td><td>  1  </td><td>  Pn2n  </td><td>  C2v<SUP>10</SUP>  </td><td>  4  </td></tr>
<tr><td>  35  </td><td>  1  </td><td>  1  </td><td>  4  </td><td>  Cmm2  </td><td>  C2v<SUP>11</SUP>  </td><td>  8  </td></tr>
<tr><td>  35  </td><td>  2  </td><td>  1  </td><td>  5  </td><td>  A2mm  </td><td>  C2v<SUP>11</SUP>  </td><td>  8  </td></tr>
<tr><td>  35  </td><td>  3  </td><td>  1  </td><td>  6  </td><td>  Bm2m  </td><td>  C2v<SUP>11</SUP>  </td><td>  8  </td></tr>
<tr><td>  36  </td><td>  1  </td><td>  1  </td><td>  4  </td><td>  Cmc2<SUB>1</SUB>  </td><td>  C2v<SUP>12</SUP>  </td><td>  8  </td></tr>
<tr><td>  36  </td><td>  5  </td><td>  1  </td><td>  4  </td><td>  Ccm2<SUB>1</SUB>  </td><td>  C2v<SUP>12</SUP>  </td><td>  8  </td></tr>
<tr><td>  36  </td><td>  2  </td><td>  1  </td><td>  5  </td><td>  A2<SUB>1</SUB>ma  </td><td>  C2v<SUP>12</SUP>  </td><td>  8  </td></tr>
<tr><td>  36  </td><td>  6  </td><td>  1  </td><td>  5  </td><td>  A2<SUB>1</SUB>am  </td><td>  C2v<SUP>12</SUP>  </td><td>  8  </td></tr>
<tr><td>  36  </td><td>  3  </td><td>  1  </td><td>  6  </td><td>  Bb2<SUB>1</SUB>m  </td><td>  C2v<SUP>12</SUP>  </td><td>  8  </td></tr>
<tr><td>  36  </td><td>  4  </td><td>  1  </td><td>  6  </td><td>  Bm2<SUB>1</SUB>b  </td><td>  C2v<SUP>12</SUP>  </td><td>  8  </td></tr>
<tr><td>  37  </td><td>  1  </td><td>  1  </td><td>  4  </td><td>  Ccc2  </td><td>  C2v<SUP>13</SUP>  </td><td>  8  </td></tr>
<tr><td>  37  </td><td>  2  </td><td>  1  </td><td>  5  </td><td>  A2aa  </td><td>  C2v<SUP>13</SUP>  </td><td>  8  </td></tr>
<tr><td>  37  </td><td>  3  </td><td>  1  </td><td>  6  </td><td>  Bb2b  </td><td>  C2v<SUP>13</SUP>  </td><td>  8  </td></tr>
<tr><td>  38  </td><td>  1  </td><td>  1  </td><td>  5  </td><td>  Amm2  </td><td>  C2v<SUP>14</SUP>  </td><td>  8  </td></tr>
<tr><td>  38  </td><td>  5  </td><td>  1  </td><td>  6  </td><td>  Bmm2  </td><td>  C2v<SUP>14</SUP>  </td><td>  8  </td></tr>
<tr><td>  38  </td><td>  2  </td><td>  1  </td><td>  6  </td><td>  B2mm  </td><td>  C2v<SUP>14</SUP>  </td><td>  8  </td></tr>
<tr><td>  38  </td><td>  6  </td><td>  1  </td><td>  4  </td><td>  C2mm  </td><td>  C2v<SUP>14</SUP>  </td><td>  8  </td></tr>
<tr><td>  38  </td><td>  3  </td><td>  1  </td><td>  4  </td><td>  Cm2m  </td><td>  C2v<SUP>14</SUP>  </td><td>  8  </td></tr>
<tr><td>  38  </td><td>  4  </td><td>  1  </td><td>  5  </td><td>  Am2m  </td><td>  C2v<SUP>14</SUP>  </td><td>  8  </td></tr>
<tr><td>  39  </td><td>  1  </td><td>  1  </td><td>  5  </td><td>  Abm2  </td><td>  C2v<SUP>15</SUP>  </td><td>  8  </td></tr>
<tr><td>  39  </td><td>  5  </td><td>  1  </td><td>  6  </td><td>  Bma2  </td><td>  C2v<SUP>15</SUP>  </td><td>  8  </td></tr>
<tr><td>  39  </td><td>  2  </td><td>  1  </td><td>  6  </td><td>  B2cm  </td><td>  C2v<SUP>15</SUP>  </td><td>  8  </td></tr>
<tr><td>  39  </td><td>  6  </td><td>  1  </td><td>  4  </td><td>  C2mb  </td><td>  C2v<SUP>15</SUP>  </td><td>  8  </td></tr>
<tr><td>  39  </td><td>  3  </td><td>  1  </td><td>  4  </td><td>  Cm2a  </td><td>  C2v<SUP>15</SUP>  </td><td>  8  </td></tr>
<tr><td>  39  </td><td>  4  </td><td>  1  </td><td>  5  </td><td>  Ac2m  </td><td>  C2v<SUP>15</SUP>  </td><td>  8  </td></tr>
<tr><td>  40  </td><td>  1  </td><td>  1  </td><td>  5  </td><td>  Ama2  </td><td>  C2v<SUP>16</SUP>  </td><td>  8  </td></tr>
<tr><td>  40  </td><td>  5  </td><td>  1  </td><td>  6  </td><td>  Bbm2  </td><td>  C2v<SUP>16</SUP>  </td><td>  8  </td></tr>
<tr><td>  40  </td><td>  2  </td><td>  1  </td><td>  6  </td><td>  B2mb  </td><td>  C2v<SUP>16</SUP>  </td><td>  8  </td></tr>
<tr><td>  40  </td><td>  6  </td><td>  1  </td><td>  4  </td><td>  C2cm  </td><td>  C2v<SUP>16</SUP>  </td><td>  8  </td></tr>
<tr><td>  40  </td><td>  3  </td><td>  1  </td><td>  4  </td><td>  Cc2m  </td><td>  C2v<SUP>16</SUP>  </td><td>  8  </td></tr>
<tr><td>  40  </td><td>  4  </td><td>  1  </td><td>  5  </td><td>  Am2a  </td><td>  C2v<SUP>16</SUP>  </td><td>  8  </td></tr>
<tr><td>  41  </td><td>  1  </td><td>  1  </td><td>  5  </td><td>  Aba2  </td><td>  C2v<SUP>17</SUP>  </td><td>  8  </td></tr>
<tr><td>  41  </td><td>  5  </td><td>  1  </td><td>  6  </td><td>  Bba2  </td><td>  C2v<SUP>17</SUP>  </td><td>  8  </td></tr>
<tr><td>  41  </td><td>  2  </td><td>  1  </td><td>  6  </td><td>  B2cb  </td><td>  C2v<SUP>17</SUP>  </td><td>  8  </td></tr>
<tr><td>  41  </td><td>  6  </td><td>  1  </td><td>  4  </td><td>  C2cb  </td><td>  C2v<SUP>17</SUP>  </td><td>  8  </td></tr>
<tr><td>  41  </td><td>  3  </td><td>  1  </td><td>  4  </td><td>  Cc2a  </td><td>  C2v<SUP>17</SUP>  </td><td>  8  </td></tr>
<tr><td>  41  </td><td>  4  </td><td>  1  </td><td>  5  </td><td>  Ac2a  </td><td>  C2v<SUP>17</SUP>  </td><td>  8  </td></tr>
<tr><td>  42  </td><td>  1  </td><td>  1  </td><td>  3  </td><td>  Fmm2  </td><td>  C2v<SUP>18</SUP>  </td><td>  16  </td></tr>
<tr><td>  42  </td><td>  2  </td><td>  1  </td><td>  3  </td><td>  F2mm  </td><td>  C2v<SUP>18</SUP>  </td><td>  16  </td></tr>
<tr><td>  42  </td><td>  3  </td><td>  1  </td><td>  3  </td><td>  Fm2m  </td><td>  C2v<SUP>18</SUP>  </td><td>  16  </td></tr>
<tr><td>  43  </td><td>  1  </td><td>  1  </td><td>  3  </td><td>  Fdd2  </td><td>  C2v<SUP>19</SUP>  </td><td>  16  </td></tr>
<tr><td>  43  </td><td>  2  </td><td>  1  </td><td>  3  </td><td>  F2dd  </td><td>  C2v<SUP>19</SUP>  </td><td>  16  </td></tr>
<tr><td>  43  </td><td>  3  </td><td>  1  </td><td>  3  </td><td>  Fd2d  </td><td>  C2v<SUP>19</SUP>  </td><td>  16  </td></tr>
<tr><td>  44  </td><td>  1  </td><td>  1  </td><td>  2  </td><td>  Imm2  </td><td>  C2v<SUP>20</SUP>  </td><td>  8  </td></tr>
<tr><td>  44  </td><td>  2  </td><td>  1  </td><td>  2  </td><td>  I2mm  </td><td>  C2v<SUP>20</SUP>  </td><td>  8  </td></tr>
<tr><td>  44  </td><td>  3  </td><td>  1  </td><td>  2  </td><td>  Im2m  </td><td>  C2v<SUP>20</SUP>  </td><td>  8  </td></tr>
<tr><td>  45  </td><td>  1  </td><td>  1  </td><td>  2  </td><td>  Iba2  </td><td>  C2v<SUP>21</SUP>  </td><td>  8  </td></tr>
<tr><td>  45  </td><td>  2  </td><td>  1  </td><td>  2  </td><td>  I2cb  </td><td>  C2v<SUP>21</SUP>  </td><td>  8  </td></tr>
<tr><td>  45  </td><td>  3  </td><td>  1  </td><td>  2  </td><td>  Ic2a  </td><td>  C2v<SUP>21</SUP>  </td><td>  8  </td></tr>
<tr><td>  46  </td><td>  1  </td><td>  1  </td><td>  2  </td><td>  Ima2  </td><td>  C2v<SUP>22</SUP>  </td><td>  8  </td></tr>
<tr><td>  46  </td><td>  5  </td><td>  1  </td><td>  2  </td><td>  Ibm2  </td><td>  C2v<SUP>22</SUP>  </td><td>  8  </td></tr>
<tr><td>  46  </td><td>  2  </td><td>  1  </td><td>  2  </td><td>  I2mb  </td><td>  C2v<SUP>22</SUP>  </td><td>  8  </td></tr>
<tr><td>  46  </td><td>  6  </td><td>  1  </td><td>  2  </td><td>  I2cm  </td><td>  C2v<SUP>22</SUP>  </td><td>  8  </td></tr>
<tr><td>  46  </td><td>  3  </td><td>  1  </td><td>  2  </td><td>  Ic2m  </td><td>  C2v<SUP>22</SUP>  </td><td>  8  </td></tr>
<tr><td>  46  </td><td>  4  </td><td>  1  </td><td>  2  </td><td>  Im2a  </td><td>  C2v<SUP>22</SUP>  </td><td>  8  </td></tr>
<tr><td>  47  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  Pmmm  </td><td>  D2h<SUP>1</SUP>  </td><td>  8  </td></tr>
<tr><td>  48  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  Pnnn_1  </td><td>  D2h<SUP>2</SUP>  </td><td>  8  </td></tr>
<tr><td>  48  </td><td>  1  </td><td>  2  </td><td>  1  </td><td>  Pnnn_2  </td><td>  D2h<SUP>2</SUP>  </td><td>  8  </td></tr>
<tr><td>  49  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  Pccm  </td><td>  D2h<SUP>3</SUP>  </td><td>  8  </td></tr>
<tr><td>  49  </td><td>  2  </td><td>  1  </td><td>  1  </td><td>  Pmaa  </td><td>  D2h<SUP>3</SUP>  </td><td>  8  </td></tr>
<tr><td>  49  </td><td>  3  </td><td>  1  </td><td>  1  </td><td>  Pbmb  </td><td>  D2h<SUP>3</SUP>  </td><td>  8  </td></tr>
<tr><td>  50  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  Pban_1  </td><td>  D2h<SUP>4</SUP>  </td><td>  8  </td></tr>
<tr><td>  50  </td><td>  5  </td><td>  2  </td><td>  1  </td><td>  Pban_2  </td><td>  D2h<SUP>4</SUP>  </td><td>  8  </td></tr>
<tr><td>  50  </td><td>  2  </td><td>  1  </td><td>  1  </td><td>  Pncb_1  </td><td>  D2h<SUP>4</SUP>  </td><td>  8  </td></tr>
<tr><td>  50  </td><td>  6  </td><td>  2  </td><td>  1  </td><td>  Pncb_2  </td><td>  D2h<SUP>4</SUP>  </td><td>  8  </td></tr>
<tr><td>  50  </td><td>  3  </td><td>  1  </td><td>  1  </td><td>  Pcna_1  </td><td>  D2h<SUP>4</SUP>  </td><td>  8  </td></tr>
<tr><td>  50  </td><td>  4  </td><td>  2  </td><td>  1  </td><td>  Pcna_2  </td><td>  D2h<SUP>4</SUP>  </td><td>  8  </td></tr>
<tr><td>  51  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  Pmma  </td><td>  D2h<SUP>5</SUP>  </td><td>  8  </td></tr>
<tr><td>  51  </td><td>  5  </td><td>  1  </td><td>  1  </td><td>  Pmmb  </td><td>  D2h<SUP>5</SUP>  </td><td>  8  </td></tr>
<tr><td>  51  </td><td>  2  </td><td>  1  </td><td>  1  </td><td>  Pbmm  </td><td>  D2h<SUP>5</SUP>  </td><td>  8  </td></tr>
<tr><td>  51  </td><td>  6  </td><td>  1  </td><td>  1  </td><td>  Pcmm  </td><td>  D2h<SUP>5</SUP>  </td><td>  8  </td></tr>
<tr><td>  51  </td><td>  3  </td><td>  1  </td><td>  1  </td><td>  Pmcm  </td><td>  D2h<SUP>5</SUP>  </td><td>  8  </td></tr>
<tr><td>  51  </td><td>  4  </td><td>  1  </td><td>  1  </td><td>  Pmam  </td><td>  D2h<SUP>5</SUP>  </td><td>  8  </td></tr>
<tr><td>  52  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  Pnna  </td><td>  D2h<SUP>6</SUP>  </td><td>  8  </td></tr>
<tr><td>  52  </td><td>  5  </td><td>  1  </td><td>  1  </td><td>  Pnnb  </td><td>  D2h<SUP>6</SUP>  </td><td>  8  </td></tr>
<tr><td>  52  </td><td>  2  </td><td>  1  </td><td>  1  </td><td>  Pbnn  </td><td>  D2h<SUP>6</SUP>  </td><td>  8  </td></tr>
<tr><td>  52  </td><td>  6  </td><td>  1  </td><td>  1  </td><td>  Pcnn  </td><td>  D2h<SUP>6</SUP>  </td><td>  8  </td></tr>
<tr><td>  52  </td><td>  3  </td><td>  1  </td><td>  1  </td><td>  Pncn  </td><td>  D2h<SUP>6</SUP>  </td><td>  8  </td></tr>
<tr><td>  52  </td><td>  4  </td><td>  1  </td><td>  1  </td><td>  Pnan  </td><td>  D2h<SUP>6</SUP>  </td><td>  8  </td></tr>
<tr><td>  53  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  Pmna  </td><td>  D2h<SUP>7</SUP>  </td><td>  8  </td></tr>
<tr><td>  53  </td><td>  5  </td><td>  1  </td><td>  1  </td><td>  Pnmb  </td><td>  D2h<SUP>7</SUP>  </td><td>  8  </td></tr>
<tr><td>  53  </td><td>  2  </td><td>  1  </td><td>  1  </td><td>  Pbmn  </td><td>  D2h<SUP>7</SUP>  </td><td>  8  </td></tr>
<tr><td>  53  </td><td>  6  </td><td>  1  </td><td>  1  </td><td>  Pcnm  </td><td>  D2h<SUP>7</SUP>  </td><td>  8  </td></tr>
<tr><td>  53  </td><td>  3  </td><td>  1  </td><td>  1  </td><td>  Pncm  </td><td>  D2h<SUP>7</SUP>  </td><td>  8  </td></tr>
<tr><td>  53  </td><td>  4  </td><td>  1  </td><td>  1  </td><td>  Pman  </td><td>  D2h<SUP>7</SUP>  </td><td>  8  </td></tr>
<tr><td>  54  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  Pcca  </td><td>  D2h<SUP>8</SUP>  </td><td>  8  </td></tr>
<tr><td>  54  </td><td>  5  </td><td>  1  </td><td>  1  </td><td>  Pccb  </td><td>  D2h<SUP>8</SUP>  </td><td>  8  </td></tr>
<tr><td>  54  </td><td>  2  </td><td>  1  </td><td>  1  </td><td>  Pbaa  </td><td>  D2h<SUP>8</SUP>  </td><td>  8  </td></tr>
<tr><td>  54  </td><td>  6  </td><td>  1  </td><td>  1  </td><td>  Pcaa  </td><td>  D2h<SUP>8</SUP>  </td><td>  8  </td></tr>
<tr><td>  54  </td><td>  3  </td><td>  1  </td><td>  1  </td><td>  Pbcb  </td><td>  D2h<SUP>8</SUP>  </td><td>  8  </td></tr>
<tr><td>  54  </td><td>  4  </td><td>  1  </td><td>  1  </td><td>  Pbab  </td><td>  D2h<SUP>8</SUP>  </td><td>  8  </td></tr>
<tr><td>  55  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  Pbam  </td><td>  D2h<SUP>9</SUP>  </td><td>  8  </td></tr>
<tr><td>  55  </td><td>  2  </td><td>  1  </td><td>  1  </td><td>  Pmcb  </td><td>  D2h<SUP>9</SUP>  </td><td>  8  </td></tr>
<tr><td>  55  </td><td>  3  </td><td>  1  </td><td>  1  </td><td>  Pcma  </td><td>  D2h<SUP>9</SUP>  </td><td>  8  </td></tr>
<tr><td>  56  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  Pccn  </td><td>  D2h<SUP>10</SUP>  </td><td>  8  </td></tr>
<tr><td>  56  </td><td>  2  </td><td>  1  </td><td>  1  </td><td>  Pnaa  </td><td>  D2h<SUP>10</SUP>  </td><td>  8  </td></tr>
<tr><td>  56  </td><td>  3  </td><td>  1  </td><td>  1  </td><td>  Pbnb  </td><td>  D2h<SUP>10</SUP>  </td><td>  8  </td></tr>
<tr><td>  57  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  Pbcm  </td><td>  D2h<SUP>11</SUP>  </td><td>  8  </td></tr>
<tr><td>  57  </td><td>  5  </td><td>  1  </td><td>  1  </td><td>  Pcam  </td><td>  D2h<SUP>11</SUP>  </td><td>  8  </td></tr>
<tr><td>  57  </td><td>  2  </td><td>  1  </td><td>  1  </td><td>  Pmca  </td><td>  D2h<SUP>11</SUP>  </td><td>  8  </td></tr>
<tr><td>  57  </td><td>  6  </td><td>  1  </td><td>  1  </td><td>  Pmab  </td><td>  D2h<SUP>11</SUP>  </td><td>  8  </td></tr>
<tr><td>  57  </td><td>  3  </td><td>  1  </td><td>  1  </td><td>  Pbma  </td><td>  D2h<SUP>11</SUP>  </td><td>  8  </td></tr>
<tr><td>  57  </td><td>  4  </td><td>  1  </td><td>  1  </td><td>  Pcmb  </td><td>  D2h<SUP>11</SUP>  </td><td>  8  </td></tr>
<tr><td>  58  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  Pnnm  </td><td>  D2h<SUP>12</SUP>  </td><td>  8  </td></tr>
<tr><td>  58  </td><td>  2  </td><td>  1  </td><td>  1  </td><td>  Pmnn  </td><td>  D2h<SUP>12</SUP>  </td><td>  8  </td></tr>
<tr><td>  58  </td><td>  3  </td><td>  1  </td><td>  1  </td><td>  Pnmn  </td><td>  D2h<SUP>12</SUP>  </td><td>  8  </td></tr>
<tr><td>  59  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  Pmmn_1  </td><td>  D2h<SUP>13</SUP>  </td><td>  8  </td></tr>
<tr><td>  59  </td><td>  5  </td><td>  2  </td><td>  1  </td><td>  Pmmn_2  </td><td>  D2h<SUP>13</SUP>  </td><td>  8  </td></tr>
<tr><td>  59  </td><td>  2  </td><td>  1  </td><td>  1  </td><td>  Pnmm_1  </td><td>  D2h<SUP>13</SUP>  </td><td>  8  </td></tr>
<tr><td>  59  </td><td>  6  </td><td>  2  </td><td>  1  </td><td>  Pnmm_2  </td><td>  D2h<SUP>13</SUP>  </td><td>  8  </td></tr>
<tr><td>  59  </td><td>  3  </td><td>  1  </td><td>  1  </td><td>  Pmnm_1  </td><td>  D2h<SUP>13</SUP>  </td><td>  8  </td></tr>
<tr><td>  59  </td><td>  4  </td><td>  2  </td><td>  1  </td><td>  Pmnm_2  </td><td>  D2h<SUP>13</SUP>  </td><td>  8  </td></tr>
<tr><td>  60  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  Pbcn  </td><td>  D2h<SUP>14</SUP>  </td><td>  8  </td></tr>
<tr><td>  60  </td><td>  5  </td><td>  1  </td><td>  1  </td><td>  Pcan  </td><td>  D2h<SUP>14</SUP>  </td><td>  8  </td></tr>
<tr><td>  60  </td><td>  2  </td><td>  1  </td><td>  1  </td><td>  Pnca  </td><td>  D2h<SUP>14</SUP>  </td><td>  8  </td></tr>
<tr><td>  60  </td><td>  6  </td><td>  1  </td><td>  1  </td><td>  Pnab  </td><td>  D2h<SUP>14</SUP>  </td><td>  8  </td></tr>
<tr><td>  60  </td><td>  3  </td><td>  1  </td><td>  1  </td><td>  Pbna  </td><td>  D2h<SUP>14</SUP>  </td><td>  8  </td></tr>
<tr><td>  60  </td><td>  4  </td><td>  1  </td><td>  1  </td><td>  Pcnb  </td><td>  D2h<SUP>14</SUP>  </td><td>  8  </td></tr>
<tr><td>  61  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  Pbca  </td><td>  D2h<SUP>15</SUP>  </td><td>  8  </td></tr>
<tr><td>  61  </td><td>  2  </td><td>  1  </td><td>  1  </td><td>  Pcab  </td><td>  D2h<SUP>15</SUP>  </td><td>  8  </td></tr>
<tr><td>  62  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  Pnma  </td><td>  D2h<SUP>16</SUP>  </td><td>  8  </td></tr>
<tr><td>  62  </td><td>  5  </td><td>  1  </td><td>  1  </td><td>  Pmnb  </td><td>  D2h<SUP>16</SUP>  </td><td>  8  </td></tr>
<tr><td>  62  </td><td>  2  </td><td>  1  </td><td>  1  </td><td>  Pbnm  </td><td>  D2h<SUP>16</SUP>  </td><td>  8  </td></tr>
<tr><td>  62  </td><td>  6  </td><td>  1  </td><td>  1  </td><td>  Pcmn  </td><td>  D2h<SUP>16</SUP>  </td><td>  8  </td></tr>
<tr><td>  62  </td><td>  3  </td><td>  1  </td><td>  1  </td><td>  Pmcn  </td><td>  D2h<SUP>16</SUP>  </td><td>  8  </td></tr>
<tr><td>  62  </td><td>  4  </td><td>  1  </td><td>  1  </td><td>  Pnam  </td><td>  D2h<SUP>16</SUP>  </td><td>  8  </td></tr>
<tr><td>  63  </td><td>  1  </td><td>  1  </td><td>  4  </td><td>  Cmcm  </td><td>  D2h<SUP>17</SUP>  </td><td>  16  </td></tr>
<tr><td>  63  </td><td>  5  </td><td>  1  </td><td>  4  </td><td>  Ccmm  </td><td>  D2h<SUP>17</SUP>  </td><td>  16  </td></tr>
<tr><td>  63  </td><td>  2  </td><td>  1  </td><td>  5  </td><td>  Amma  </td><td>  D2h<SUP>17</SUP>  </td><td>  16  </td></tr>
<tr><td>  63  </td><td>  6  </td><td>  1  </td><td>  5  </td><td>  Amam  </td><td>  D2h<SUP>17</SUP>  </td><td>  16  </td></tr>
<tr><td>  63  </td><td>  3  </td><td>  1  </td><td>  6  </td><td>  Bbmm  </td><td>  D2h<SUP>17</SUP>  </td><td>  16  </td></tr>
<tr><td>  63  </td><td>  4  </td><td>  1  </td><td>  6  </td><td>  Bmmb  </td><td>  D2h<SUP>17</SUP>  </td><td>  16  </td></tr>
<tr><td>  64  </td><td>  1  </td><td>  1  </td><td>  4  </td><td>  Cmca  </td><td>  D2h<SUP>18</SUP>  </td><td>  16  </td></tr>
<tr><td>  64  </td><td>  5  </td><td>  1  </td><td>  4  </td><td>  Ccmb  </td><td>  D2h<SUP>18</SUP>  </td><td>  16  </td></tr>
<tr><td>  64  </td><td>  2  </td><td>  1  </td><td>  5  </td><td>  Abma  </td><td>  D2h<SUP>18</SUP>  </td><td>  16  </td></tr>
<tr><td>  64  </td><td>  6  </td><td>  1  </td><td>  5  </td><td>  Acam  </td><td>  D2h<SUP>18</SUP>  </td><td>  16  </td></tr>
<tr><td>  64  </td><td>  3  </td><td>  1  </td><td>  6  </td><td>  Bbcm  </td><td>  D2h<SUP>18</SUP>  </td><td>  16  </td></tr>
<tr><td>  64  </td><td>  4  </td><td>  1  </td><td>  6  </td><td>  Bmab  </td><td>  D2h<SUP>18</SUP>  </td><td>  16  </td></tr>
<tr><td>  65  </td><td>  1  </td><td>  1  </td><td>  4  </td><td>  Cmmm  </td><td>  D2h<SUP>19</SUP>  </td><td>  16  </td></tr>
<tr><td>  65  </td><td>  2  </td><td>  1  </td><td>  5  </td><td>  Ammm  </td><td>  D2h<SUP>19</SUP>  </td><td>  16  </td></tr>
<tr><td>  65  </td><td>  3  </td><td>  1  </td><td>  6  </td><td>  Bmmm  </td><td>  D2h<SUP>19</SUP>  </td><td>  16  </td></tr>
<tr><td>  66  </td><td>  1  </td><td>  1  </td><td>  6  </td><td>  Cccm  </td><td>  D2h<SUP>20</SUP>  </td><td>  16  </td></tr>
<tr><td>  66  </td><td>  2  </td><td>  1  </td><td>  5  </td><td>  Amaa  </td><td>  D2h<SUP>20</SUP>  </td><td>  16  </td></tr>
<tr><td>  66  </td><td>  3  </td><td>  1  </td><td>  6  </td><td>  Bbmb  </td><td>  D2h<SUP>20</SUP>  </td><td>  16  </td></tr>
<tr><td>  67  </td><td>  1  </td><td>  1  </td><td>  4  </td><td>  Cmma  </td><td>  D2h<SUP>21</SUP>  </td><td>  16  </td></tr>
<tr><td>  67  </td><td>  5  </td><td>  1  </td><td>  4  </td><td>  Cmmb  </td><td>  D2h<SUP>21</SUP>  </td><td>  16  </td></tr>
<tr><td>  67  </td><td>  2  </td><td>  1  </td><td>  5  </td><td>  Abmm  </td><td>  D2h<SUP>21</SUP>  </td><td>  16  </td></tr>
<tr><td>  67  </td><td>  6  </td><td>  1  </td><td>  5  </td><td>  Acmm  </td><td>  D2h<SUP>21</SUP>  </td><td>  16  </td></tr>
<tr><td>  67  </td><td>  3  </td><td>  1  </td><td>  6  </td><td>  Bmcm  </td><td>  D2h<SUP>21</SUP>  </td><td>  16  </td></tr>
<tr><td>  67  </td><td>  4  </td><td>  1  </td><td>  6  </td><td>  Bmam  </td><td>  D2h<SUP>21</SUP>  </td><td>  16  </td></tr>
<tr><td>  68  </td><td>  1  </td><td>  1  </td><td>  4  </td><td>  Ccca_1  </td><td>  D2h<SUP>22</SUP>  </td><td>  16  </td></tr>
<tr><td>  68  </td><td>  1  </td><td>  2  </td><td>  4  </td><td>  Ccca_2  </td><td>  D2h<SUP>22</SUP>  </td><td>  16  </td></tr>
<tr><td>  68  </td><td>  5  </td><td>  1  </td><td>  4  </td><td>  Cccb_1  </td><td>  D2h<SUP>22</SUP>  </td><td>  16  </td></tr>
<tr><td>  68  </td><td>  5  </td><td>  2  </td><td>  4  </td><td>  Cccb_2  </td><td>  D2h<SUP>22</SUP>  </td><td>  16  </td></tr>
<tr><td>  68  </td><td>  2  </td><td>  1  </td><td>  5  </td><td>  Abaa_1  </td><td>  D2h<SUP>22</SUP>  </td><td>  16  </td></tr>
<tr><td>  68  </td><td>  2  </td><td>  2  </td><td>  5  </td><td>  Abaa_2  </td><td>  D2h<SUP>22</SUP>  </td><td>  16  </td></tr>
<tr><td>  68  </td><td>  6  </td><td>  1  </td><td>  5  </td><td>  Acaa_1  </td><td>  D2h<SUP>22</SUP>  </td><td>  16  </td></tr>
<tr><td>  68  </td><td>  6  </td><td>  2  </td><td>  5  </td><td>  Acaa_2  </td><td>  D2h<SUP>22</SUP>  </td><td>  16  </td></tr>
<tr><td>  68  </td><td>  3  </td><td>  1  </td><td>  6  </td><td>  Bbcb_1  </td><td>  D2h<SUP>22</SUP>  </td><td>  16  </td></tr>
<tr><td>  68  </td><td>  3  </td><td>  2  </td><td>  6  </td><td>  Bbcb_2  </td><td>  D2h<SUP>22</SUP>  </td><td>  16  </td></tr>
<tr><td>  68  </td><td>  4  </td><td>  1  </td><td>  6  </td><td>  Bbab_1  </td><td>  D2h<SUP>22</SUP>  </td><td>  16  </td></tr>
<tr><td>  68  </td><td>  4  </td><td>  2  </td><td>  6  </td><td>  Bbab_2  </td><td>  D2h<SUP>22</SUP>  </td><td>  16  </td></tr>
<tr><td>  69  </td><td>  1  </td><td>  1  </td><td>  3  </td><td>  Fmmm  </td><td>  D2h<SUP>23</SUP>  </td><td>  32  </td></tr>
<tr><td>  70  </td><td>  1  </td><td>  1  </td><td>  3  </td><td>  Fddd_1  </td><td>  D2h<SUP>24</SUP>  </td><td>  32  </td></tr>
<tr><td>  70  </td><td>  1  </td><td>  2  </td><td>  3  </td><td>  Fddd_2  </td><td>  D2h<SUP>24</SUP>  </td><td>  32  </td></tr>
<tr><td>  71  </td><td>  1  </td><td>  1  </td><td>  2  </td><td>  Immm  </td><td>  D2h<SUP>25</SUP>  </td><td>  16  </td></tr>
<tr><td>  72  </td><td>  1  </td><td>  1  </td><td>  2  </td><td>  Ibam  </td><td>  D2h<SUP>26</SUP>  </td><td>  16  </td></tr>
<tr><td>  72  </td><td>  2  </td><td>  1  </td><td>  2  </td><td>  Imcb  </td><td>  D2h<SUP>26</SUP>  </td><td>  16  </td></tr>
<tr><td>  72  </td><td>  3  </td><td>  1  </td><td>  2  </td><td>  Icma  </td><td>  D2h<SUP>26</SUP>  </td><td>  16  </td></tr>
<tr><td>  73  </td><td>  1  </td><td>  1  </td><td>  2  </td><td>  Ibca  </td><td>  D2h<SUP>27</SUP>  </td><td>  16  </td></tr>
<tr><td>  73  </td><td>  2  </td><td>  1  </td><td>  2  </td><td>  Icab  </td><td>  D2h<SUP>27</SUP>  </td><td>  16  </td></tr>
<tr><td>  74  </td><td>  1  </td><td>  1  </td><td>  2  </td><td>  Imma  </td><td>  D2h<SUP>28</SUP>  </td><td>  16  </td></tr>
<tr><td>  74  </td><td>  5  </td><td>  1  </td><td>  2  </td><td>  Immb  </td><td>  D2h<SUP>28</SUP>  </td><td>  16  </td></tr>
<tr><td>  74  </td><td>  2  </td><td>  1  </td><td>  2  </td><td>  Ibmm  </td><td>  D2h<SUP>28</SUP>  </td><td>  16  </td></tr>
<tr><td>  74  </td><td>  6  </td><td>  1  </td><td>  2  </td><td>  Icmm  </td><td>  D2h<SUP>28</SUP>  </td><td>  16  </td></tr>
<tr><td>  74  </td><td>  3  </td><td>  1  </td><td>  2  </td><td>  Imcm  </td><td>  D2h<SUP>28</SUP>  </td><td>  16  </td></tr>
<tr><td>  74  </td><td>  4  </td><td>  1  </td><td>  2  </td><td>  Imam  </td><td>  D2h<SUP>28</SUP>  </td><td>  16  </td></tr>
<tr><td>  75  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P4  </td><td>  C4<SUP>1</SUP>  </td><td>  4  </td></tr>
<tr><td>  76  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P4<SUB>1</SUB>  </td><td>  C4<SUP>2</SUP>  </td><td>  4  </td></tr>
<tr><td>  77  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P4<SUB>2</SUB>  </td><td>  C4<SUP>3</SUP>  </td><td>  4  </td></tr>
<tr><td>  78  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P4<SUB>3</SUB>  </td><td>  C4<SUP>4</SUP>  </td><td>  4  </td></tr>
<tr><td>  79  </td><td>  1  </td><td>  1  </td><td>  2  </td><td>  I4  </td><td>  C4<SUP>5</SUP>  </td><td>  8  </td></tr>
<tr><td>  80  </td><td>  1  </td><td>  1  </td><td>  2  </td><td>  I4<SUB>1</SUB>  </td><td>  C4<SUP>6</SUP>  </td><td>  8  </td></tr>
<tr><td>  81  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P-4  </td><td>  S4<SUP>1</SUP>  </td><td>  4  </td></tr>
<tr><td>  82  </td><td>  1  </td><td>  1  </td><td>  2  </td><td>  I-4  </td><td>  S4<SUP>2</SUP>  </td><td>  8  </td></tr>
<tr><td>  83  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P4/m  </td><td>  C4h<SUP>1</SUP>  </td><td>  8  </td></tr>
<tr><td>  84  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P4<SUB>2</SUB>/m  </td><td>  C4h<SUP>2</SUP>  </td><td>  8  </td></tr>
<tr><td>  85  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P4/n_1  </td><td>  C4h<SUP>3</SUP>  </td><td>  8  </td></tr>
<tr><td>  85  </td><td>  1  </td><td>  2  </td><td>  1  </td><td>  P4/n_2  </td><td>  C4h<SUP>3</SUP>  </td><td>  8  </td></tr>
<tr><td>  86  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P4<SUB>2</SUB>/n_1  </td><td>  C4h<SUP>4</SUP>  </td><td>  8  </td></tr>
<tr><td>  86  </td><td>  1  </td><td>  2  </td><td>  1  </td><td>  P4<SUB>2</SUB>/n_2  </td><td>  C4h<SUP>4</SUP>  </td><td>  8  </td></tr>
<tr><td>  87  </td><td>  1  </td><td>  1  </td><td>  2  </td><td>  I4/m  </td><td>  C4h<SUP>5</SUP>  </td><td>  16  </td></tr>
<tr><td>  88  </td><td>  1  </td><td>  1  </td><td>  2  </td><td>  I4<SUB>1</SUB>/a_1  </td><td>  C4h<SUP>6</SUP>  </td><td>  16  </td></tr>
<tr><td>  88  </td><td>  1  </td><td>  2  </td><td>  2  </td><td>  I4<SUB>1</SUB>/a_2  </td><td>  C4h<SUP>6</SUP>  </td><td>  16  </td></tr>
<tr><td>  89  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P4<SUB>2</SUB>2  </td><td>  D4<SUP>1</SUP>  </td><td>  8  </td></tr>
<tr><td>  90  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P42<SUB>1</SUB>2  </td><td>  D4<SUP>2</SUP>  </td><td>  8  </td></tr>
<tr><td>  91  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P4<SUB>1</SUB>22  </td><td>  D4<SUP>3</SUP>  </td><td>  8  </td></tr>
<tr><td>  92  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P4<SUB>1</SUB>2<SUB>1</SUB>2  </td><td>  D4<SUP>4</SUP>  </td><td>  8  </td></tr>
<tr><td>  93  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P4222  </td><td>  D4<SUP>5</SUP>  </td><td>  8  </td></tr>
<tr><td>  94  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P422<SUB>1</SUB>2  </td><td>  D4<SUP>6</SUP>  </td><td>  8  </td></tr>
<tr><td>  95  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P4<SUB>3</SUB>22  </td><td>  D4<SUP>7</SUP>  </td><td>  8  </td></tr>
<tr><td>  96  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P4<SUB>3</SUB>2<SUB>1</SUB>2  </td><td>  D4<SUP>8</SUP>  </td><td>  8  </td></tr>
<tr><td>  97  </td><td>  1  </td><td>  1  </td><td>  2  </td><td>  I422  </td><td>  D4<SUP>9</SUP>  </td><td>  16  </td></tr>
<tr><td>  98  </td><td>  1  </td><td>  1  </td><td>  2  </td><td>  I4<SUB>1</SUB>22  </td><td>  D4<SUP>10</SUP>  </td><td>  16  </td></tr>
<tr><td>  99  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P4mm  </td><td>  C4v<SUP>1</SUP>  </td><td>  8  </td></tr>
<tr><td>  100  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P4bm  </td><td>  C4v<SUP>2</SUP>  </td><td>  8  </td></tr>
<tr><td>  101  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P42cm  </td><td>  C4v<SUP>3</SUP>  </td><td>  8  </td></tr>
<tr><td>  102  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P42nm  </td><td>  C4v<SUP>4</SUP>  </td><td>  8  </td></tr>
<tr><td>  103  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P4cc  </td><td>  C4v<SUP>5</SUP>  </td><td>  8  </td></tr>
<tr><td>  104  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P4nc  </td><td>  C4v<SUP>6</SUP>  </td><td>  8  </td></tr>
<tr><td>  105  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P4<SUB>2</SUB>mc  </td><td>  C4v<SUP>7</SUP>  </td><td>  8  </td></tr>
<tr><td>  106  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P4<SUB>2</SUB>bc  </td><td>  C4v<SUP>8</SUP>  </td><td>  8  </td></tr>
<tr><td>  107  </td><td>  1  </td><td>  1  </td><td>  2  </td><td>  I4mm  </td><td>  C4v<SUP>9</SUP>  </td><td>  16  </td></tr>
<tr><td>  108  </td><td>  1  </td><td>  1  </td><td>  2  </td><td>  I4cm  </td><td>  C4v<SUP>10</SUP>  </td><td>  16  </td></tr>
<tr><td>  109  </td><td>  1  </td><td>  1  </td><td>  2  </td><td>  I4<SUB>1</SUB>md  </td><td>  C4v<SUP>11</SUP>  </td><td>  16  </td></tr>
<tr><td>  110  </td><td>  1  </td><td>  1  </td><td>  2  </td><td>  I4<SUB>1</SUB>cd  </td><td>  C4v<SUP>12</SUP>  </td><td>  16  </td></tr>
<tr><td>  111  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P-42m  </td><td>  D2d<SUP>1</SUP>  </td><td>  8  </td></tr>
<tr><td>  112  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P-42c  </td><td>  D2d<SUP>2</SUP>  </td><td>  8  </td></tr>
<tr><td>  113  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P-42<SUB>1</SUB>m  </td><td>  D2d<SUP>3</SUP>  </td><td>  8  </td></tr>
<tr><td>  114  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P-42<SUB>1</SUB>c  </td><td>  D2d<SUP>4</SUP>  </td><td>  8  </td></tr>
<tr><td>  115  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P-4m2  </td><td>  D2d<SUP>5</SUP>  </td><td>  8  </td></tr>
<tr><td>  116  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P-4c2  </td><td>  D2d<SUP>6</SUP>  </td><td>  8  </td></tr>
<tr><td>  117  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P-4b2  </td><td>  D2d<SUP>7</SUP>  </td><td>  8  </td></tr>
<tr><td>  118  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P-4n2  </td><td>  D2d<SUP>8</SUP>  </td><td>  8  </td></tr>
<tr><td>  119  </td><td>  1  </td><td>  1  </td><td>  2  </td><td>  I-4m2  </td><td>  D2d<SUP>9</SUP>  </td><td>  16  </td></tr>
<tr><td>  120  </td><td>  1  </td><td>  1  </td><td>  2  </td><td>  I-4c2  </td><td>  D2d<SUP>10</SUP>  </td><td>  16  </td></tr>
<tr><td>  121  </td><td>  1  </td><td>  1  </td><td>  2  </td><td>  I-42m  </td><td>  D2d<SUP>11</SUP>  </td><td>  16  </td></tr>
<tr><td>  122  </td><td>  1  </td><td>  1  </td><td>  2  </td><td>  I-42d  </td><td>  D2d<SUP>12</SUP>  </td><td>  16  </td></tr>
<tr><td>  123  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P4/mmm  </td><td>  D4h<SUP>1</SUP>  </td><td>  16  </td></tr>
<tr><td>  124  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P4/mcc  </td><td>  D4h<SUP>2</SUP>  </td><td>  16  </td></tr>
<tr><td>  125  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P4/nbm_1  </td><td>  D4h<SUP>3</SUP>  </td><td>  16  </td></tr>
<tr><td>  125  </td><td>  1  </td><td>  2  </td><td>  1  </td><td>  P4/nbm_2  </td><td>  D4h<SUP>3</SUP>  </td><td>  16  </td></tr>
<tr><td>  126  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P4/nnc_1  </td><td>  D4h<SUP>4</SUP>  </td><td>  16  </td></tr>
<tr><td>  126  </td><td>  1  </td><td>  2  </td><td>  1  </td><td>  P4/nnc_2  </td><td>  D4h<SUP>4</SUP>  </td><td>  16  </td></tr>
<tr><td>  127  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P4/mbm  </td><td>  D4h<SUP>5</SUP>  </td><td>  16  </td></tr>
<tr><td>  128  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P4/mnc  </td><td>  D4h<SUP>6</SUP>  </td><td>  16  </td></tr>
<tr><td>  129  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P4/nmm_1  </td><td>  D4h<SUP>7</SUP>  </td><td>  16  </td></tr>
<tr><td>  129  </td><td>  1  </td><td>  2  </td><td>  1  </td><td>  P4/nmm_2  </td><td>  D4h<SUP>7</SUP>  </td><td>  16  </td></tr>
<tr><td>  130  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P4/ncc_1  </td><td>  D4h<SUP>8</SUP>  </td><td>  16  </td></tr>
<tr><td>  130  </td><td>  1  </td><td>  2  </td><td>  1  </td><td>  P4/ncc_2  </td><td>  D4h<SUP>8</SUP>  </td><td>  16  </td></tr>
<tr><td>  131  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P4<SUB>2</SUB>/mmc  </td><td>  D4h<SUP>9</SUP>  </td><td>  16  </td></tr>
<tr><td>  132  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P4<SUB>2</SUB>/mcm  </td><td>  D4h<SUP>10</SUP>  </td><td>  16  </td></tr>
<tr><td>  133  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P4<SUB>2</SUB>/nbc_1  </td><td>  D4h<SUP>11</SUP>  </td><td>  16  </td></tr>
<tr><td>  133  </td><td>  1  </td><td>  2  </td><td>  1  </td><td>  P4<SUB>2</SUB>/nbc_2  </td><td>  D4h<SUP>11</SUP>  </td><td>  16  </td></tr>
<tr><td>  134  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P4<SUB>2</SUB>/nnm_1  </td><td>  D4h<SUP>12</SUP>  </td><td>  16  </td></tr>
<tr><td>  134  </td><td>  1  </td><td>  2  </td><td>  1  </td><td>  P4<SUB>2</SUB>/nnm_2  </td><td>  D4h<SUP>12</SUP>  </td><td>  16  </td></tr>
<tr><td>  135  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P4<SUB>2</SUB>/mbc  </td><td>  D4h<SUP>13</SUP>  </td><td>  16  </td></tr>
<tr><td>  136  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P4<SUB>2</SUB>/mnm  </td><td>  D4h<SUP>14</SUP>  </td><td>  16  </td></tr>
<tr><td>  137  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P4<SUB>2</SUB>/nmc_1  </td><td>  D4h<SUP>15</SUP>  </td><td>  16  </td></tr>
<tr><td>  137  </td><td>  1  </td><td>  2  </td><td>  1  </td><td>  P4<SUB>2</SUB>/nmc_2  </td><td>  D4h<SUP>15</SUP>  </td><td>  16  </td></tr>
<tr><td>  138  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P4<SUB>2</SUB>/ncm_1  </td><td>  D4h<SUP>16</SUP>  </td><td>  16  </td></tr>
<tr><td>  138  </td><td>  1  </td><td>  2  </td><td>  1  </td><td>  P4<SUB>2</SUB>/ncm_2  </td><td>  D4h<SUP>16</SUP>  </td><td>  16  </td></tr>
<tr><td>  139  </td><td>  1  </td><td>  1  </td><td>  2  </td><td>  I4/mmm  </td><td>  D4h<SUP>17</SUP>  </td><td>  32  </td></tr>
<tr><td>  140  </td><td>  1  </td><td>  1  </td><td>  2  </td><td>  I4/mcm  </td><td>  D4h<SUP>18</SUP>  </td><td>  32  </td></tr>
<tr><td>  141  </td><td>  1  </td><td>  1  </td><td>  2  </td><td>  I4<SUB>1</SUB>/amd_1  </td><td>  D4h<SUP>19</SUP>  </td><td>  32  </td></tr>
<tr><td>  141  </td><td>  1  </td><td>  2  </td><td>  2  </td><td>  I4<SUB>1</SUB>/amd_2  </td><td>  D4h<SUP>19</SUP>  </td><td>  32  </td></tr>
<tr><td>  142  </td><td>  1  </td><td>  1  </td><td>  2  </td><td>  I4<SUB>1</SUB>/acd_1  </td><td>  D4h<SUP>20</SUP>  </td><td>  32  </td></tr>
<tr><td>  142  </td><td>  1  </td><td>  2  </td><td>  2  </td><td>  I4<SUB>1</SUB>/acd_2  </td><td>  D4h<SUP>20</SUP>  </td><td>  32  </td></tr>
<tr><td>  143  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P3  </td><td>  C3<SUP>1</SUP>  </td><td>  3  </td></tr>
<tr><td>  144  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P3<SUB>1</SUB>  </td><td>  C3<SUP>2</SUP>  </td><td>  3  </td></tr>
<tr><td>  145  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P3<SUB>2</SUB>  </td><td>  C3<SUP>3</SUP>  </td><td>  3  </td></tr>
<tr><td>  146  </td><td>  1  </td><td>  1  </td><td>  7  </td><td>  R3_H  </td><td>  C3<SUP>4</SUP>  </td><td>  9  </td></tr>
<tr><td>  146  </td><td>  2  </td><td>  1  </td><td>  7  </td><td>  R3_R  </td><td>  C3<SUP>4</SUP>  </td><td>  3  </td></tr>
<tr><td>  147  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P-3  </td><td>  C3i<SUP>1</SUP>  </td><td>  6  </td></tr>
<tr><td>  148  </td><td>  1  </td><td>  1  </td><td>  7  </td><td>  R-3_H  </td><td>  C3i<SUP>2</SUP>  </td><td> 18  </td></tr>
<tr><td>  148  </td><td>  2  </td><td>  1  </td><td>  7  </td><td>  R-3_R  </td><td>  C3i<SUP>2</SUP>  </td><td>  6  </td></tr>
<tr><td>  149  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P312  </td><td>  D3<SUP>1</SUP>  </td><td>  6  </td></tr>
<tr><td>  150  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P321  </td><td>  D3<SUP>2</SUP>  </td><td>  6  </td></tr>
<tr><td>  151  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P3<SUB>1</SUB>12  </td><td>  D3<SUP>3</SUP>  </td><td>  6  </td></tr>
<tr><td>  152  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P3<SUB>1</SUB>21  </td><td>  D3<SUP>4</SUP>  </td><td>  6  </td></tr>
<tr><td>  153  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P3<SUB>2</SUB>12  </td><td>  D3<SUP>5</SUP>  </td><td>  6  </td></tr>
<tr><td>  154  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P3<SUB>2</SUB>21  </td><td>  D3<SUP>6</SUP>  </td><td>  6  </td></tr>
<tr><td>  155  </td><td>  1  </td><td>  1  </td><td>  7  </td><td>  R32_H  </td><td>  D3<SUP>7</SUP>  </td><td>  18  </td></tr>
<tr><td>  155  </td><td>  2  </td><td>  1  </td><td>  7  </td><td>  R32_R  </td><td>  D3<SUP>7</SUP>  </td><td>  6  </td></tr>
<tr><td>  156  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P3m1  </td><td>  C3v<SUP>1</SUP>  </td><td>  6  </td></tr>
<tr><td>  157  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P31m  </td><td>  C3v<SUP>2</SUP>  </td><td>  6  </td></tr>
<tr><td>  158  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P3c1  </td><td>  C3v<SUP>3</SUP>  </td><td>  6  </td></tr>
<tr><td>  159  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P31c  </td><td>  C3v<SUP>4</SUP>  </td><td>  6  </td></tr>
<tr><td>  160  </td><td>  1  </td><td>  1  </td><td>  7  </td><td>  R3m_H  </td><td>  C3v<SUP>5</SUP>  </td><td>  18  </td></tr>
<tr><td>  160  </td><td>  2  </td><td>  1  </td><td>  7  </td><td>  R3m_R  </td><td>  C3v<SUP>5</SUP>  </td><td>  6  </td></tr>
<tr><td>  161  </td><td>  1  </td><td>  1  </td><td>  7  </td><td>  R3c_H  </td><td>  C3v<SUP>6</SUP>  </td><td>  18  </td></tr>
<tr><td>  161  </td><td>  2  </td><td>  1  </td><td>  7  </td><td>  R3c_R  </td><td>  C3v<SUP>6</SUP>  </td><td>  6  </td></tr>
<tr><td>  162  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P-31m  </td><td>  D3d<SUP>1</SUP>  </td><td>  12  </td></tr>
<tr><td>  163  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P-31c  </td><td>  D3d<SUP>2</SUP>  </td><td>  12  </td></tr>
<tr><td>  164  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P-3m1  </td><td>  D3d<SUP>3</SUP>  </td><td>  12  </td></tr>
<tr><td>  165  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P-3c1  </td><td>  D3d<SUP>4</SUP>  </td><td>  12  </td></tr>
<tr><td>  166  </td><td>  1  </td><td>  1  </td><td>  7  </td><td>  R-3m_H  </td><td>  D3d<SUP>5</SUP>  </td><td>  36  </td></tr>
<tr><td>  166  </td><td>  2  </td><td>  1  </td><td>  7  </td><td>  R-3m_R  </td><td>  D3d<SUP>5</SUP>  </td><td>  12  </td></tr>
<tr><td>  167  </td><td>  1  </td><td>  1  </td><td>  7  </td><td>  R-3c_H  </td><td>  D3d<SUP>6</SUP>  </td><td>  36  </td></tr>
<tr><td>  167  </td><td>  2  </td><td>  1  </td><td>  7  </td><td>  R-3c_R  </td><td>  D3d<SUP>6</SUP>  </td><td>  12  </td></tr>
<tr><td>  168  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P6  </td><td>  C6<SUP>1</SUP>  </td><td>  6  </td></tr>
<tr><td>  169  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P6<SUB>1</SUB>  </td><td>  C6<SUP>2</SUP>  </td><td>  6  </td></tr>
<tr><td>  170  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P6<SUB>5</SUB>  </td><td>  C6<SUP>3</SUP>  </td><td>  6  </td></tr>
<tr><td>  171  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P6<SUB>2</SUB>  </td><td>  C6<SUP>4</SUP>  </td><td>  6  </td></tr>
<tr><td>  172  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P6<SUB>4</SUB>  </td><td>  C6<SUP>5</SUP>  </td><td>  6  </td></tr>
<tr><td>  173  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P6<SUB>3</SUB>  </td><td>  C6<SUP>6</SUP>  </td><td>  6  </td></tr>
<tr><td>  174  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P-6  </td><td>  C3h<SUP>1</SUP>  </td><td>  6  </td></tr>
<tr><td>  175  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P6/m  </td><td>  C6h<SUP>1</SUP>  </td><td>  12  </td></tr>
<tr><td>  176  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P6<SUB>3</SUB>/m  </td><td>  C6h<SUP>2</SUP>  </td><td>  12  </td></tr>
<tr><td>  177  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P622  </td><td>  D6<SUP>1</SUP>  </td><td>  12  </td></tr>
<tr><td>  178  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P6<SUB>1</SUB>22  </td><td>  D6<SUP>2</SUP>  </td><td>  12  </td></tr>
<tr><td>  179  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P6<SUB>5</SUB>22  </td><td>  D6<SUP>3</SUP>  </td><td>  12  </td></tr>
<tr><td>  180  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P6<SUB>2</SUB>22  </td><td>  D6<SUP>4</SUP>  </td><td>  12  </td></tr>
<tr><td>  181  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P6<SUB>4</SUB>22  </td><td>  D6<SUP>5</SUP>  </td><td>  12  </td></tr>
<tr><td>  182  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P6<SUB>3</SUB>22  </td><td>  D6<SUP>6</SUP>  </td><td>  12  </td></tr>
<tr><td>  183  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P6mm  </td><td>  C6v<SUP>1</SUP>  </td><td>  12  </td></tr>
<tr><td>  184  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P6cc  </td><td>  C6v<SUP>2</SUP>  </td><td>  12  </td></tr>
<tr><td>  185  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P6<SUB>3</SUB>cm  </td><td>  C6v<SUP>3</SUP>  </td><td>  12  </td></tr>
<tr><td>  186  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P6<SUB>3</SUB>mc  </td><td>  C6v<SUP>4</SUP>  </td><td>  12  </td></tr>
<tr><td>  187  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P-6m2  </td><td>  D3h<SUP>1</SUP>  </td><td>  12  </td></tr>
<tr><td>  188  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P-6c2  </td><td>  D3h<SUP>2</SUP>  </td><td>  12  </td></tr>
<tr><td>  189  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P-62m  </td><td>  D3h<SUP>3</SUP>  </td><td>  12  </td></tr>
<tr><td>  190  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P-62c  </td><td>  D3h<SUP>4</SUP>  </td><td>  12  </td></tr>
<tr><td>  191  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P6/mmm  </td><td>  D6h<SUP>1</SUP>  </td><td>  24  </td></tr>
<tr><td>  192  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P6/mcc  </td><td>  D6h<SUP>2</SUP>  </td><td>  24  </td></tr>
<tr><td>  193  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P6<SUB>3</SUB>/mcm  </td><td>  D6h<SUP>3</SUP>  </td><td>  24  </td></tr>
<tr><td>  194  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P6<SUB>3</SUB>/mmc  </td><td>  D6h<SUP>4</SUP>  </td><td>  24  </td></tr>
<tr><td>  195  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P23  </td><td>  T<SUP>1</SUP>  </td><td>  12  </td></tr>
<tr><td>  196  </td><td>  1  </td><td>  1  </td><td>  3  </td><td>  F23  </td><td>  T<SUP>2</SUP>  </td><td>  48  </td></tr>
<tr><td>  197  </td><td>  1  </td><td>  1  </td><td>  2  </td><td>  I23  </td><td>  T<SUP>3</SUP>  </td><td>  24  </td></tr>
<tr><td>  198  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P2<SUB>1</SUB>3  </td><td>  T<SUP>4</SUP>  </td><td>  12  </td></tr>
<tr><td>  199  </td><td>  1  </td><td>  1  </td><td>  2  </td><td>  I2<SUB>1</SUB>3  </td><td>  T<SUP>5</SUP>  </td><td>  24  </td></tr>
<tr><td>  200  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  Pm-3  </td><td>  Th<SUP>1</SUP>  </td><td>  24  </td></tr>
<tr><td>  201  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  Pn-3_1  </td><td>  Th<SUP>2</SUP>  </td><td>  24  </td></tr>
<tr><td>  201  </td><td>  1  </td><td>  2  </td><td>  1  </td><td>  Pn-3_2  </td><td>  Th<SUP>2</SUP>  </td><td>  24  </td></tr>
<tr><td>  202  </td><td>  1  </td><td>  1  </td><td>  3  </td><td>  Fm-3  </td><td>  Th<SUP>3</SUP>  </td><td>  96  </td></tr>
<tr><td>  203  </td><td>  1  </td><td>  1  </td><td>  3  </td><td>  Fd-3_1  </td><td>  Th<SUP>4</SUP>  </td><td>  96  </td></tr>
<tr><td>  203  </td><td>  1  </td><td>  2  </td><td>  3  </td><td>  Fd-3_2  </td><td>  Th<SUP>4</SUP>  </td><td>  96  </td></tr>
<tr><td>  204  </td><td>  1  </td><td>  1  </td><td>  2  </td><td>  Im-3  </td><td>  Th<SUP>5</SUP>  </td><td>  48  </td></tr>
<tr><td>  205  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  Pa-3  </td><td>  Th<SUP>6</SUP>  </td><td>  24  </td></tr>
<tr><td>  206  </td><td>  1  </td><td>  1  </td><td>  2  </td><td>  Ia-3  </td><td>  Th<SUP>7</SUP>  </td><td>  48  </td></tr>
<tr><td>  207  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P432  </td><td>  O<SUP>1</SUP>  </td><td>  24  </td></tr>
<tr><td>  208  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P4<SUB>2</SUB>32  </td><td>  O<SUP>2</SUP>  </td><td>  24  </td></tr>
<tr><td>  209  </td><td>  1  </td><td>  1  </td><td>  3  </td><td>  F432  </td><td>  O<SUP>3</SUP>  </td><td>  96  </td></tr>
<tr><td>  210  </td><td>  1  </td><td>  1  </td><td>  3  </td><td>  F4<SUB>1</SUB>32  </td><td>  O<SUP>4</SUP>  </td><td>  96  </td></tr>
<tr><td>  211  </td><td>  1  </td><td>  1  </td><td>  2  </td><td>  I432  </td><td>  O<SUP>5</SUP>  </td><td>  48  </td></tr>
<tr><td>  212  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P4<SUB>3</SUB>32  </td><td>  O<SUP>6</SUP>  </td><td>  24  </td></tr>
<tr><td>  213  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P4<SUB>1</SUB>32  </td><td>  O<SUP>7</SUP>  </td><td>  24  </td></tr>
<tr><td>  214  </td><td>  1  </td><td>  1  </td><td>  2  </td><td>  I4<SUB>1</SUB>32  </td><td>  O<SUP>8</SUP>  </td><td>  48  </td></tr>
<tr><td>  215  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P-43m  </td><td>  Td<SUP>1</SUP>  </td><td>  24  </td></tr>
<tr><td>  216  </td><td>  1  </td><td>  1  </td><td>  3  </td><td>  F-43m  </td><td>  Td<SUP>2</SUP>  </td><td>  96  </td></tr>
<tr><td>  217  </td><td>  1  </td><td>  1  </td><td>  2  </td><td>  I-43m  </td><td>  Td<SUP>3</SUP>  </td><td>  48  </td></tr>
<tr><td>  218  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  P-43n  </td><td>  Td<SUP>4</SUP>  </td><td>  24  </td></tr>
<tr><td>  219  </td><td>  1  </td><td>  1  </td><td>  3  </td><td>  F-43c  </td><td>  Td<SUP>5</SUP>  </td><td>  96  </td></tr>
<tr><td>  220  </td><td>  1  </td><td>  1  </td><td>  2  </td><td>  I-43d  </td><td>  Td<SUP>6</SUP>  </td><td>  48  </td></tr>
<tr><td>  221  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  Pm-3m  </td><td>  Oh<SUP>1</SUP>  </td><td>  48  </td></tr>
<tr><td>  222  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  Pn-3n_1  </td><td>  Oh<SUP>2</SUP>  </td><td>  48  </td></tr>
<tr><td>  222  </td><td>  1  </td><td>  2  </td><td>  1  </td><td>  Pn-3n_2  </td><td>  Oh<SUP>2</SUP>  </td><td>  48  </td></tr>
<tr><td>  223  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  Pm-3n  </td><td>  Oh<SUP>3</SUP>  </td><td>  48  </td></tr>
<tr><td>  224  </td><td>  1  </td><td>  1  </td><td>  1  </td><td>  Pn-3m_1  </td><td>  Oh<SUP>4</SUP>  </td><td>  48  </td></tr>
<tr><td>  224  </td><td>  1  </td><td>  2  </td><td>  1  </td><td>  Pn-3m_2  </td><td>  Oh<SUP>4</SUP>  </td><td>  48  </td></tr>
<tr><td>  225  </td><td>  1  </td><td>  1  </td><td>  3  </td><td>  Fm-3m  </td><td>  Oh<SUP>5</SUP>  </td><td>  192  </td></tr>
<tr><td>  226  </td><td>  1  </td><td>  1  </td><td>  3  </td><td>  Fm-3c  </td><td>  Oh<SUP>6</SUP>  </td><td>  192  </td></tr>
<tr><td>  227  </td><td>  1  </td><td>  1  </td><td>  3  </td><td>  Fd-3m_1  </td><td>  Oh<SUP>7</SUP>  </td><td>  192  </td></tr>
<tr><td>  227  </td><td>  1  </td><td>  2  </td><td>  3  </td><td>  Fd-3m_2  </td><td>  Oh<SUP>7</SUP>  </td><td>  192  </td></tr>
<tr><td>  228  </td><td>  1  </td><td>  1  </td><td>  3  </td><td>  Fd-3c_1  </td><td>  Oh<SUP>8</SUP>  </td><td>  192  </td></tr>
<tr><td>  228  </td><td>  1  </td><td>  2  </td><td>  3  </td><td>  Fd-3c_2  </td><td>  Oh<SUP>8</SUP>  </td><td>  192  </td></tr>
<tr><td>  229  </td><td>  1  </td><td>  1  </td><td>  2  </td><td>  Im-3m  </td><td>  Oh<SUP>9</SUP>  </td><td>  96  </td></tr>
<tr><td>  230  </td><td>  1  </td><td>  1  </td><td>  2  </td><td>  Ia-3d  </td><td>  Oh<SUP>10</SUP>  </td><td>  96  </td></tr>
</TABLE>
</body></html>
---
authors: XG, DCA
---

# The DFPT (respfn) code

This page complements the main [[help:abinit]], for matters related
to responses to perturbations computed with DFPT.
It will be easier to discover the present file with the help of the [[tutorial:rf1|DFPT1 tutorial]].  

<a id="intro"></a> 
## 0 Introducing the computation of responses
  
ABINIT can compute the response to different perturbations, and provide access
to quantities that are second derivatives of total energy (2DTE) with respect
to these perturbations. 
Presently, they can be of four types: 

1. phonons 
2. static homogeneous electric field
3. strain  
4. magnetic field (coupling to the spin, not the orbital motion)

The physical properties connected to 2DTE with respect to perturbations (1) and (2) are the phonon
dynamical matrices, the dielectric tensor, and the Born effective charges,
while the additional strain perturbation (3), mixed with phonon and electric
field leads to elastic constant, internal strain, and piezoelectricity.
The magnetic field perturbation is a recent addition to ABINIT, and will not be detailed at present.


More functionalities of the computation of responses should be implemented
sooner or later. Some third derivatives of the
total energy (3DTE) are also implemented. The 3DTE might give phonon-phonon
coupling, non-linear electric response, anharmonic elastic constants, Gruneisen parameters,...

The basic quantities that ABINIT will compute are the **first-order** derivatives
of the wavefunctions (1WF) with respect to these perturbations. The later
calculation of the 2DTE and 3DTE from these 1WF is an easy computational task:
the construction of the 2DTE with respect to perturbations j1 and j2
involves mainly evaluating matrix elements between the 1WF of j1 and/or the 1WF of j2. 
More details on this technique can be found in [[cite:Gonze1997]] and [[cite:Gonze1997a]].

The calculation of the 1WF for a particular perturbation is done using a
variational principle and an algorithm rather similar to the one used to find
the unperturbed ground-state wavefunctions. Thus, a lot of technical details
and parameters are the same for both ground-state and response-function
calculations. This justifies the development of one unique code for these two
classes of properties: many of the routines of abinit are common in these
calculations, or had to be duplicated, but with relatively small modifications.

The ABINIT code performs a rather primitive analysis of the calculated 2DTEs.
For example, it gives the phonon frequencies, electronic dielectric tensor and
effective charges. But the main output of the code is the Derivative DataBase
(DDB): a file that contains the set of all 2DTEs and 3DTEs calculated by the
code. This DDB can be manipulated by the MRGDDB code, and fully analyzed by
the Anaddb code. See the corresponding [[help:mrgddb]] and [[help:anaddb]].

<a id="1"></a>
## 1 Description of perturbations
  
The perturbation of the **phonon** type is the displacement of one atom along
one of the axis of the unit cell, by a unit of length (in reduced coordinates).
It is characterized by two integer numbers and one wavevector.
The two integer numbers are the number of the moved atom, which will be noted
**ipert**, and the number of the axis of the unit cell, which will be noted **idir**. 

!!! important

    *ipert* for phonon perturbation can have values between 1 and [[natom]],
    *idir* can have values between 1 and 3.

The set of all possible phonon perturbations for one wavevector has thus 3 * [[natom]] elements. 
From this basis set, all phonons can be constructed by linear combination. 
The set of atoms to be moved in one dataset of ABINIT is determined by the input
variable [[rfatpol]]. The set of directions to be considered in one dataset of
ABINIT is determined by the input variable [[rfdir]]. The wavevector to be
considered in one dataset of ABINIT is determined by the input variables
[[nqpt]], [[qpt]], and [[qptnrm]].

The perturbations of the **electric field** type are

  * the application of the homogeneous electric field along the axes of the reciprocal lattice 
  * the derivative of the Hamiltonian with respect to the electronic wavevector along 
    the axes of the reciprocal lattice (which allows to compute derivatives of wavefunctions with respect to their wavevector), 
    an **auxiliary** quantity needed **before** the application of the homogeneous electric field. 
    The perturbation is the change of wavevector by dk in the Hamiltonian, hence the perturbation 
    is referred to as the derivative dk perturbation ("ddk" perturbation). 

Note 1
:   the ddk perturbation is defined as the derivative with respect to k
    in reduced coordinates; this is equivalent to applying a linear perturbation
    of strength $2\pi$ along the conjugate direction in real space. This statement
    comes from the derivation of the phase factor $\exp(i2\pi kr)$ with respect to
    k in reduced coordinates.

Note 2
:   in case the electric field type perturbation is computed inside a
    finite electric field, the derivative of the Hamiltonian with respect to the
    electronic wavevector is not computed: everything is done by a finite-
    difference technique. Also, the definition of the homogeneous electric field
    perturbation is not along the axes of the reciprocal lattice, but in cartesian
    coordinates. Sorry for the possible confusion ...

These electric-type perturbations are also characterized by two numbers:
*ipert* being natom + 1 for the ddk perturbation and natom + 2 for the electric
field, and *idir* being 1, 2 or 3, as for phonon perturbations. Although the
possibility of electric field characterized by a non-zero wavevector is
envisioned for a future version of the code, at present only homogeneous
fields are considered. So the wavevector of the electric field type
perturbations is $\Gamma$ (q=0).

The perturbations of the **strain** type are either an uniaxial strain or a
shear strain. The strain perturbations are considered in cartesian coordinates
(x,y,z). They are characterized by two numbers, with *ipert* being natom + 3 for
the uniaxial strains, and natom + 4 for the shear strains, and *idir* describes
the particular component. 

Explicitly, for uniaxial strains:

* idir = 1 gives the xx strain perturbation, 
* idir = 2 gives the yy strain perturbation, 
* idir = 3 gives the zz strain perturbation, 

while for shear strains:

* idir=1 gives the yz strain perturbation, 
* idir=2 gives the xz perturbation, 
* idir=3 gives the xy perturbation.  

Note that the "rigid-atom" elastic constants, as output of ABINIT, are those
obtained with **frozen** internal coordinates. The internal coordinate
relaxation, needed to give "physical" elastic constants can be handled through
the knowledge of the internal strain and dynamical matrix at $\Gamma$, in ANADDB.
Of course, if all the internal coordinate are fixed by symmetry, all the
internal strains vanish, and the "rigid-atom" and "physical" elastic constants are equal.  
Limitations of the present implementation (as of v5.7):

  * Symmetry is presently used to skip redundant k points in the BZ sum, 
    but not to skip redundant strain perturbations.

We also define the index of the perturbation, called *pertcase*, equal to idir + 3 * (ipert - 1). 
Accordingly, pertcase runs from 1 to 3 * (natom + 4), and will be
needed to identify output and input files, see section 6.

!!! summary

    To summarize, the perturbations are characterized by two numbers, **ipert** from
    1 to natom + 4, and **idir**, from 1 to 3, as well as one wavevector (that is
    gamma when a non-phonon perturbation is considered). A number called
    **pertcase** combines *ipert* and *idir*, and runs from 1 to 3 * (natom + 4).

The 2DTE, being derivative of the total energy with respect to two
perturbations, will be characterized by two sets of (idir,ipert), or by two
pertcase numbers, while 3DTE will need three such sets or pertcase numbers.
In addition they will depend on one wavevector (for 2DTE) or two wavevectors (for 3DTE).

In the non-stationary implementation of the 2DTE, used for off-diagonal elements in ABINIT, the first pertcase
corresponds to the perturbation that gives the derivative of the potential,
and the second pertcase corresponds to the perturbation that gives the
derivative of the wavefunctions.

<a id="2"></a>
## 2 Filenames and input of ground-state wavefunctions
  
The **same** 'files' file as for GS calculations is used for RF calculations.
Actually, in the multi-dataset mode, one will be able to make in one ABINIT
run, ground-state computations as well as response-function computations, so
that the 'files' file must be the same.... The 'input' file will have many
common input variables for these different cases, but also some separate ones.

Two ground-state wavefunction files might be needed:

  * the file of ground-state wavefunctions at a set of wavevectors, named k-points
  * the file of ground-state wavefunctions at the corresponding k+q, where q is the wavevector of the perturbation

These files also contain the corresponding eigenvalues.

Note that the second file (k+q) will be identical to the first one (k), in the
case of zero-wavevector perturbation. Also, if the q-wavevector maps the
k-point grid onto itself (q being the difference between points that belong to
the grid), all the information on the k+q grid is contained in the k grid, a
fact that ABINIT is able to exploit.

One should have a look at the input variables [[irdwfk]], and [[irdwfq]], for
a description of the ground-state wavefunction file names generated from the
root names provided in the 'files' file. In the multi-dataset mode, the
following input variables will be relevant: [[getwfk]], and [[getwfq]]. The
file names of the ground-state wavefunction file follow the same convention as
for the ground-state case. Thus, read the 
[[help:abinit#files-file|corresponding section]] of the abinit help file, if needed.

In the case of an electric field perturbation, the output 1WF of the
corresponding ddk perturbation is needed as input. If the option [[rfelfd]]=1
is used, then the code will take care of doing first the derivative dk
perturbation calculation, then write the 1WF at the correct place, as an
output file, then begin the homogeneous field perturbation calculation.
Usually, the use of [[rfelfd]]=1 is not recommended, as the ddk computation is
the most often done with different parameters as the electric field perturbation,
a dataset with [[rfelfd]]=2 being followed with a dataset with [[rfelfd]]=3.

The nomenclature for first-order wavefunction files is also given in the
[[help:abinit#files-file|abinit help]] file, but it is worth to specify it in more
detail here. The root name is formed from the string of character in the third
line of the 'files' file (for an input file) or the fourth line of the 'files'
file (for an output file), that is complemented, in the multi-dataset mode, by
' **_DS** ' and the dataset number, and then,
the string ' **_1WF** ' is added, followed by pertcase (the index of the perturbation). This gives, e.g., for
a 'files' file whose fourth line is '/tmp/o', for the dataset number 3, and a
perturbation corresponding to the displacement of the second atom in the x
direction (pertcase=4), the following name of the corresponding 1st-order
wavefunction output file:
    
     /tmp/o_DS3_1WF4

Such a file might be used as input of another computation, or of another dataset.

The relevant input variables are: [[ird1wf]], and [[irdddk]], as well as
[[get1wf]], and [[getddk]], in the multi-dataset mode.

<a id="symmetries"></a>
## 3 The use of symmetries
  
In order to understand correctly the behaviour of response-function runs, 
some information on the use of symmetries must be given.

Some perturbations (including their wavevector) may be invariant for some
symmetries. The code is able to use symmetries to skip perturbations of which a
symmetric has already been calculated (except in the case of strain
perturbations). ABINIT is also able to use the symmetries that keeps the
perturbations invariant, to reduce the number of k points needed for the
sampling of electronic wavefunctions in the Brillouin zone (although this
feature is not optimal yet). There is one exception to this, the ddk
perturbation, for which the spatial symmetries cannot be used yet.

In any case, unlike for the ground-state, the input k-point set for response
function should NOT have been decreased by using spatial symmetries, prior to
the loop over perturbations (see section 4). Only the time-reversal symmetry,
retained by calculations at q=0, ought to be used to decrease this input
k-point set. Considering each perturbation in turn, ABINIT will be able to
select from this non-reduced set of k-points, the proper k-point set,
automatically, by using the symmetries that leave each perturbation invariant.

Accordingly, the preferred way to generate the k-point grid is of course to
use the [[ngkpt]] or [[kptrlatt]] input variables, with different values of [[kptopt]]:

  * kptopt = 1 for the ground state
  * kptopt = 2 for response functions at q=0 
  * kptopt = 3 for response functions at non-zero q 

<a id="4"></a>
## 4 Organisation of response-function computations
  
In agreement with the information provided in the previous sections, different
cases can be distinguished.

When one considers the response to an atomic displacement with q=0, the
following procedure is suggested:

  * first, a self-consistent ground-state computation with the restricted set of k-points 
    in the Irreducible Brillouin Zone (with [[kptopt]]=1)

  * second, a self-consistent response-function computation with the atomic displacement perturbation, 
    with the half set of k-points (with [[kptopt]]=2)

When one considers the response to an electric field (with q=0), the following
procedure is suggested:

  * first, a self-consistent ground-state computation with the restricted set of k-points 
    in the Irreducible Brillouin Zone (with [[kptopt]]=1)

  * second, a non-self-consistent response-function computation of the d/dk perturbation, 
    with the half set of k-points (with [[kptopt]]=2, and [[iscf]]=-3)

  * third, a self-consistent response-function computation of the electric field perturbation, 
    with the half set of k-points (with [[kptopt]]=2)

When one considers the response to an atomic displacement in the special case
where q connects k-points that both belong to the special k-point grid, the
following procedure is suggested:

  * first, a self-consistent ground-state computation with the restricted set of k-points 
    in the Irreducible Brillouin Zone (with [[kptopt]]=1)

  * second, a self-consistent response-function computation of the atomic displacement perturbation, 
    with the full set of k-points (with [[kptopt]]=3)

When one considers the response to an atomic displacement for a general q
point, the following procedure is suggested:

  * first, a self-consistent ground-state computation with the restricted set of k-points 
    in the Irreducible Brillouin Zone (with [[kptopt]]=1)

  * second, a non-self-consistent ground-state run with the set of k+q points, that might be 
    reduced thanks to symmetries (with [[kptopt]]=1)

  * third, a self-consistent response-function computation of the atomic displacement perturbation, 
    with the full set of k-points (with [[kptopt]]=3)

Of course, these different steps can be combined when a set of responses is
looked for. In particular, the computations of responses at gamma, in the case
where the full dynamical matrix as well as the dielectric tensor and the Born
effective charges are needed, can be combined as follows:

  * first, a self-consistent ground-state computation with the restricted set of k-points 
    in the Irreducible Brillouin Zone (with [[kptopt]]=1)

  * second, the three non-self-consistent response-function computations (one for each direction) 
    of the d/dk perturbation, with the half set of k-points (with [[kptopt]]=2, and [[iscf]]=-3)

  * third, all the self-consistent response-function computations of the electric field perturbations 
    and of the atomic displacements, with the half set of k-points (with [[kptopt]]=2)

Still, computations of perturbations at different q wavevectors cannot be
mixed. But they can follow the other computations. Supposing that
perturbations at q=0 and a general q point are to be performed, they will be combined as follows:

  * first, a self-consistent ground-state computation with the restricted set of k-points 
    in the Irreducible Brillouin Zone (with [[kptopt]]=1)

  * second, the three non-self-consistent response-function computations (one for each direction) 
    of the d/dk perturbation, with the half set of k-points (with [[kptopt]]=2, and [[iscf]]=-3)

  * third, all q=0 self-consistent response-function computations of the electric field perturbations 
    and of the atomic displacements, with the half set of k-points (with [[kptopt]]=2)

  * fourth, a non-self-consistent ground-state computation with the set of k+q points, 
    that might be reduced thanks to symmetries (with [[kptopt]]=1)

  * fifth, the self-consistent response-function computations of the atomic displacement perturbations 
    with a q wavevector, with the full set of k-points (with [[kptopt]]=3)

Note that the error in the 2DTE is **linear** in the **ground-state**
wavefunction error (unlike the error due to the 1WFs). Moreover, a large
prefactor is associated with this source of error (it can even cause cause the
instability of the SCF procedure). As a consequence, the convergence of the
ground-state wavefunction should be very good. The same is true at the level
of the ddk wavefunctions.

As mentioned in the introduction, inside the response-function part of the
code, the calculation proceeds in two steps: first the calculation of the
first-order derivative of the wavefunctions (1WF), then the combinations of
these 1WF to build the 2DTE and 3DTE.

In an initialisation part, the input file and all the ground-state files are
read, and a few basic quantities are constructed.

In the first part, every perturbation is examined, one at a time, separately:

  * A file containing previous RF wavefunctions is eventually read.

  * The minimisation of the variational expression is performed, and this procedure generates 
    the 1WF as well as the first-order density and potential.

  * It is possible, knowing these quantities for the perturbation ipert1, to construct all the 2DTE 
    with respect to this perturbation and any ipert2, except for ipert2 being an homogeneous electric field, 
    in which case the derivative of the ground-state wavefunctions with respect to their wavevector (ddk) is also needed. 
    This feature has been implemented for ipert2 being any phonon (of the same wavevector than ipert1), 
    or an homogeneous electric field.

  * The first-order wavefunctions (1WF) are written as well as the first-order density or potential (if requested).

<a id="5"></a>
## 5 List of relevant input variables
  
A subset of the ABINIT input variables have a modified meaning or a modified
behaviour in case of RF calculations. Here is the list of these input
variables, together with the variables that applies only to RF computations.
Note that the code will do a RF calculation ([[optdriver]]=1) when one of
[[rfphon]] or [[rfelfd]] is non-zero.

  * [[amu]]
  * [[getwfk]], [[getwfq]], [[get1wf]], [[getddk]] 
  * [[irdwfk]], [[irdwfq]], [[ird1wf]], [[irdddk]] 
  * [[iscf]]
  * [[nkpt]]
  * [[nqpt]], [[qpt]], [[qptnrm]] 
  * [[nsym]]
  * [[rfasr]]
  * [[rfatpol]]
  * [[rfdir]]
  * [[rfelfd]]
  * [[rfphon]]
  * [[dfpt_sciss]]
  * [[tolwfr]], [[toldfe]], [[tolvrs]]

<a id="output"></a>
## 6 The different output files
  
Output from the code goes to several places listed below.

**6.1. The log file**

This file is the same as the log file of the abinit code when computing ground
state (GS) results. Possibly, the output of datasets related to response
functions will be intertwined with those concerned with ground-state case. The
purpose of this file is the same as in the GS case, and the use of error messages is unchanged.

<a id="6.2"></a>
**6.2. The main output file**

This file is the same as the main output file of the abinit code when
computing ground state (GS) results. Possibly, the output of datasets
related to response functions will be intertwined with those concerned with
ground-state case. We explain here the parts related to the RF computation.

The initialisation part is the same as for the GS. So, the reader is advised
to read the [[help:abinit#outputfile|section 6.2]] of the abinit help file,
as well as the first paragraph of the section [[help:abinit#6.3|6.3]] of
this file. Afterwards, the content of the main output file differs a bit...

The main output file reports on the initialisation of the ground-state
wavefunctions at k+q, then the loop on the perturbations begins. For each
perturbation, there is:

  * a short description of the perturbation
  * the timing information
  * the report on the initialisation of the 1WF
  * then the iterations for the minimisation begin, and the output file describes 
    the number of the iteration, the second derivative of the total energy obtained (2DTEnergy in Ha), 
    the change in 2DTEnergy since last iteration, the maximum residual over all bands and k points, 
    and the square of the potential residual.
  * after the iterations are completed, the residuals are reported
  * in case of the derivative dk perturbation, ek2 (to be explained) and the f-sum rule ratio value are printed. 
    The f-sum rule ratio should be close to 1 (not when ecutsm/=0, however).
  * then the components of the 2DTEnergy, broken in at most 14 pieces, depending on the perturbation
  * then the 2DTEnergy in Hartree and in eV
  * then the relaxation contribution (caused by changes in wavefunctions), and the non-relaxation contribution 
    (Ewald and frozen-wavefunction contribution)
  * then the 2DTEnergy evaluated using a non-variational expression.

After the computation of each perturbation, the output file reports on the
2DTE matrix elements. This part is not executed if the only perturbation is
the derivative dk perturbation. It will give the following information:

  * if [[prtvol]]=1 or bigger, the full detail of every contribution to the 2DTE in reduced coordinates.
  * the 2DTE in reduced coordinates.
  * then it will use the 2DTE to perform already some analysis of the data without use the Mrgddb and Anaddb codes, namely: 
    the full dynamical matrix (cartesian coordinates, masses included) the effective charges, and the dielectric tensor, 
    the phonon frequencies (including the analysis of the non- analyticity if we are at $\Gamma$). 
    Note that phonon frequencies lower than 1.0d-8Ha (absolute value) are automatically set to zero, 
    while imaginary phonon frequencies (square roots of negative eigenvalues - indicating an instability) 
    are printed as negative (this facilitates the post-processing).

For this last analysis, the code assumes that the whole set of perturbations
in a class has been calculated, either all the phonon perturbations or the
homogeneous electric field perturbation, or both. If this is not true, the
code will give results that may be wrong in the case that the reduced system
of coordinates is not cartesian (for the dynamical matrix, the effective
charge tensor of the dielectric matrix), or in all case wrong (the phonon
frequencies); also the code will put zero in the matrix elements that have not been calculated. 
A Warning message is issued if the above information cannot be trusted.

Finally, the code provides the timing information.

<a id="6.3"></a>
**6.3. The first-order wavefunction (1WF) files**

These are unformatted data files containing the planewaves coefficients of all
the wavefunctions, written in the following format. First, the header (see
[[help:abinit#header|section 6.4]] of the abinit help file), followed by
    
```fortran
     bantot=0                                    <-- counts over all bands
     index=0                                     <-- index for the wavefunctions
     do isppol=1,nsppol
      do ikpt=1,nkpt
       write(unit) npw1,nspinor,nband                    <-- for each k point
       write(unit) kg(1:3,1:npw1)                        <-- plane wave reduced coordinates
       do iband=1,nband1
        write(unit) (eigen1(jband+(iband-1)*nband+bantot),jband=1,2*nband)  <-- column of eigenvalue matrix
        write(unit) (cg(ii+index),ii=1,2*npw1*nspinor)   <-- wavefunction coefficients
       enddo                                            for a single band and k point
       bantot=bantot+nband
       index=index+2*npw1*nspinor*nband1
      enddo
     enddo
```
  
In this code section, npw1(ikpt) is the number of planewaves in the basis at
the k+q point, nband1(ikpt) is likewise the number of bands at the k point
(which may vary among k points depending on occopt), and the factor of 2 in
writing the wavefunction results from the fact that it is complex.  
eigen1 is the array that contains the matrix element of the first-order
Hamiltonian between the different ground-state wavefunctions. It could be used
to build the electron-phonon coupling and deformation potentials.  
Note that the format for first-order WF file differs from the format used for
the ground-state WF file by the fact that eigen1 is now an array, and no more
a vector, and is written with the corresponding wf, and no more before the
writing of all wf for one k point.

**6.4. The first-order density files**

They consist of the header lines, followed by
    
```fortran
    write(unit) (rhor1(ir),ir=1,cplex*ngfft(1)*ngfft(2)*ngfft(3))
```

Here, rhor1 is the change of electron density in electrons/Bohr^3. The
parameter cplex is 1 when q=0 and 2 when q/=0 . Indeed, for q=0, the density
change is a real quantity, while it is complex in general when q/=0 .

<a id="ddb"></a>
**6.5. The derivative database (DDB)**

It is made of two parts. The first should allow one to unambiguously identify
the run that has generated the DDB, while the second part contains the 2DTE,
grouped by blocks of data.

Note that the DDB output of the ABINIT code can be merged with other DDBs as
described in the [[help:mrgddb|Mrgddb help file]].

The first part contains:

  * the DDB version number (that defines the structure of the DDB)
  * seven parameters needed for the dimensionning of the DDB file 
    ([[natom]], [[nkpt]], [[nsppol]], [[nsym]], [[ntypat]], [[occopt]], and [[nband]] - 
    or the array [[nband]] ([[nkpt]]* [[nsppol]]) if [[occopt]]=2)
  * different information on the run that generated the 2DTE 
    ([[acell]],[[amu]],[[ecut]],[[iscf]],[[ixc]],[[kpt]],[[kptnrm]], 
     [[ngfft]],[[occ]],[[rprim]],[[dfpt_sciss]],[[symrel]],[[xred]],[[tnons]],[[typat]],[[tolwfr]],[[wtk]],[[ziontypat]], 
    as well as information on the pseudopotentials by means of their Kleinman-Bylander energies). 
    These values are simply a transcription of the input data, or other simple internal parameters.

Note: the format and content of this first part of the DDBs have to be updated in the future ...

The second part contains:

  * the number of data blocks
  * for each data block, the type of the block, its number of elements, and the list of elements.

The elements of a 2DTE block are described by 4 integers and 2 real numbers.
The 2 first integers define a first perturbation in the form (idir1,ipert1),
the two next define a second perturbation in the form (idir2,ipert2). The
matrix element corresponds to the derivative of the total energy with respect
to the parameters corresponding to these perturbations. The real numbers are
the real and imaginary parts of the 2DTE. Sometimes, the code uses spatial
symmetries, the time-reversal symmetry, or even the permutation of first and
second perturbations to deduce the value of non-computed matrix elements. This
behaviour might be improved, as it is sometimes confusing ...

<a id="numerical-quality"></a>
## 7 Numerical quality of the calculations
  
It is possible to get from the RF calculations essentially EXACT derivatives
of the total energy with respect to perturbations. There is a published
account of this fact in [[cite:Gonze1995]]. An agreement of 8 digits
or more was obtained in the comparison with finite-difference derivatives of GS data.

The accuracy of these calculations are thus entirely determined by the input
parameters the user choose for the RF run, and the preparatory GS runs.

We will now review the convergence parameters, usually the same as for the GS
calculations, indicating only the specific features related to RF calculations.

Input parameters that could influence the accuracy of the calculation are:

  * [[ecut]] (the energy cut-off, that depends strongly on the pseudopotential)
  * [[ixc]] (describing the exchange-correlation functional)
  * [[nkpt]](or, more accurately, the Brillouin zone sampling, that can be determined 
     alternatively by the inputs variables [[ngkpt]] or [[kptrlatt]])
  * one of the self-consistent convergence tolerance parameters, [[toldfe]], [[tolvrs]], or [[tolwfr]].

The input parameters [[ecut]], [[ecutsm]], [[ixc]], [[intxc]], the set of
k-wavevectors, as well as the related variables have to be the SAME in the
ground-state calculations that go before a RF run and this RF run.

Namely: do not try to use ground-state wavefunction files produced with
[[ecut]]=10Ha for a RF run with [[ecut]]=20Ha. In some cases, the code will complain
and stop, but in other cases, it might simply produce garbage !

If the value of [[ngfft]] is input by hand, its value must also be equal in
the GS and RF cases. ALWAYS use [[ngfft]] large enough to have boxcut=2 or
larger, in order to avoid any FFT filter error. In the GS case, boxcut as
small as 1.5 could be allowed in some cases. It is not allowed with RF
calculations, because they are more sensitive to that error.

The convergence tests with respect to [[ecut]], and the k-point grid should be
done carefully. This was already emphasized in the [[help:abinit]] and is re-
emphasized here. The user should test the convergence DIRECTLY on the property
he or she is interested in. For example, if the user wants a phonon frequency
accurate to 10 cm^-1, he/she could be lead to do a full calculation (GS+RF) of
phonons at 30Ha, then another full calculation at 35Ha, then another at
40Ha... It is an error to rely on tolerance on the total energy (for example
1mHa/atom) or geometry (accuracy of one part per thousand on the bond lengths)
to draw 'a priori' conclusions on the convergence of other quantities, and not
monitor the convergence of these directly. To be clear: if phonon frequencies
are needed, check the convergence of phonon frequencies !

The user should note that for bands with very small occupancy in the metallic
case as well as unoccupied bands for insulators, the ground state run
preceeding response function runs will not necessarily converge these
wavefunctions using usual ground-state tests such as [[toldfe]] or (better)
[[tolvrs]]. To be sure that inaccuracies are not introduced into the response
function calculation by poorly converged unoccupied bands, a separate run
starting from a saved charge density ([[prtden]]=1 in the self-consistent run)
and using [[iscf]]=-2 and [[tolwfr]] may be needed.
---
authors: RShaltaf
---

# the Mrgscr utility  

This file explains the i/o parameters needed for the screening files (SCR) merging code (MRGSCR).  

The user is advised to be familiar with the main [[help:abinit]] especially
the GW part before reading the present file.

## 1 Introduction
  
The mrgscr is a utility that comes with the ABINIT code. It is used to merge
partial screening files. These files contain the screening calculated on some
selected q points generated using the input variables 'nqptdm' and 'qptdm'.
The mrgscr utility merge these files into a single file that contain the
screening on the full list of q points.

Like other utilities within ABINIT (e.g. mrgddb, mrgkk), the input is very
simple, and could be given directly at the screen, or more conveniently, piped from a file.

The user should give the number of SCRs that will be merged in the first line,
then the name of the output file in the second line, after what he/she shall
write the whole set of filenames for the SCRs to be merged, one on each line.

## 2 Why such utility can be useful?
  
The GW part of ABINIT code in its present version (5.x) is not yet
parallelized. The input variables 'nqptdm' and 'qptdm' are meant to be used as
a kind of manual parallelization by splitting the whole screening calculation
into several smaller jobs which can be submitted into several machines
simultaneously. mrgscr later can be used to merge the several output SCRs into a single file.

## 3 How does the mrgscr code work ?
  
First the code reads the header of the first partial file, then it calculates
the full list of q points that should exist in the full screening file. Then
it checks the consistency of the other files and the existing q points in each
file. Last it merges the files into a single file, but it worth to say here
that merging WILL NOT be successful and no output file will be generated if:

1) the files are not consistent, i.e, different PPs, different DM size etc.;
usually the files MUST be generated under same conditions for every single
input variable, except of course for 'nqptdm' and =20 'qptdm'.

2) the various input partial screening files can not form a complete file via
merging. This often happen if the user still have one or =20 more q points on
which screening still need to be calculated.

## 4 What if there is a repetition of one or more q points through different input partial screening files?
  
No problem, the code will include only one q point from every two repeated q
points, and will report it in the log file.

## 5 How to check the status of the resulting screening file, or partial files, what is there, what is needed, etc.?
  
mrgscr can do it, the user only need to use a two-line input file. The first
line of the input file should be 0, then the second line should contain the
name of the file to be checked. The result of the checking is reported through the log file.

Examples
    
    merging case
    3                    <== the number of files to be merged
    out                  <== name of out put file
    1_SCR                <== start:name of the files, to be merged
    2_SCR
    3_SCR
    1                    <== to merge q-points (2 = to merge frequencies) 
    
    
    checking case
    
    0                    <== just write zero
    1_SCR                <== the name of the file to be checked
---
authors: XG, DCA
---

# The anaddb utility  

This file explains the use and i/o parameters needed for the "Analysis of Derivative DataBase" code.

This code is able to compute interatomic force constants, but also, more
generally, many different physical properties from databases containing
derivatives of the total energy (Derivative DataBases - DDB).  
The user is not supposed to know how the Derivative DataBase (DBB) has been
generated. He/she should simply know what material is described by the DDB he/she wants to use.  

If he/she is interested in the generation of DDB, and wants to know more about
this topic, he/she will read different help files of the ABINIT package,
related to the [[help:abinit|main ABINIT executable]], to the
[[help:respfn|DFPT features of ABINIT]], and to the [[help:mrgddb|DDB merge tool]].

It will be easier to discover the present file with the help of the [tutorials](/tutorial),
especially the tutorials on [DFPT1](/tutorial/rf1) and [DFPT2](/tutorial/rf2).  

## 1 Introduction
  
In short, a Derivative DataBase contains a list of derivatives of the total
energy with respect to three kind of perturbations: phonons, electric field
and stresses. The present code analyses the DDB, and directly gives properties
of the material under investigation, like phonon spectrum, frequency-dependent
dielectric tensor, thermal properties.

Given an input file (parameters described below), the user must create a
"files" file which lists names for the files the job will require, including
the main input file, the main output file, the name of the DDB, and some other
file names optionally used for selected capabilities of the code.

The files file (called for example ab.files) could look like:
    
      anaddb.in  
      anaddb.out  
      ddb  
      band_eps  
      gkk  
      anaddb.ep  
      ddk  
     
In this example:  

  * the main input file is called "anaddb.in",   
  * the main output will be put into the file called "anaddb.out",   
  * the input DDB file is called "ddb",   
  * information to draw phonon band structures will go to band_eps  
  * the input GKK file is called "gkk" (used only for electron-phonon interactions)  
  * the base filename for electron-phonon output "anaddb.ep" (used only for electron-phonon interactions)  
  * the file name for ddk reference files: these are the GKK files generated in k-point derivative runs, 
    using the [[prtgkk]] abinit input variable (used only for electron-phonon transport calculations)

Other examples are given in the ~abinit/test/v2 directory. The latter three
filename information is often not used by anaddb. The maximal length of names
for the main input or output files is presently 264 characters.

The main executable file is called anaddb. Supposing that the "files" file is
called anaddb.files, and that the executable is placed in your working
directory, anaddb is run interactively (in Unix) with the command:

    anaddb < anaddb.files >& log
  
or, in the background, with the command

    anaddb < anaddb.files >& log &

where standard out and standard error are piped to the log file called "log"

!!! tip
    Piping the standard error, thanks to the '&' sign placed after '>' is
    **really important** for the analysis of eventual failures, when not due to
    ABINIT, but to other sources, like disk full problem.

The user can specify any names he/she wishes for any of these files. Variations of the
above commands could be needed, depending on the flavor of UNIX that is used
on the platform that is considered for running the code.

The syntax of the input file is strictly similar to the syntax of the main
abinit input files: the file is parsed, keywords are identified, comments are
also identified. However, the multidataset mode is not available.

## 2 Input variables
  
This ANADDB utility is able to perform many different tasks, each governed by
a selected set of input variables, with also some input variables common to
many of the different tasks. The 'flag' variables activates the different tasks 
e.g. [[dieflag@anaddb]], [[thmflag@anaddb]], [[elphflag@anaddb]]

The list of input variables for the anaddb input file are presented in the
[[varset:anaddb]] variable set. In order to discover them, it is easier to use
the different tutorials: start with the [second DFPT tutorial](/tutorial/rf2), then follow 
the [tutorial on elasticity and
piezoelectricity](/tutorial/elastic), the [tutorial on electron-phonon
interaction](/tutorial/eph), and the [tutorial on non-linear properties](/tutorial/nlo). 
---
authors: DCA, XG, RC
---

# New user help file  

This page gives a beginner's introduction to the ABINIT resources, 
the package, and the main ABINIT applications.  

## Foreword
  
The ABINIT project is a group effort of dozens of people worldwide, 
who develop the main ABINIT application 
which is delivered with many other files (post-processors, tests, documentation, ...) in the ABINIT package. 
The ABINIT project includes also resources
provided on the [ABINIT Web site](https://www.abinit.org) and 
the [github organization](https://github.com/abinit).

Before reading the present page, and get some grasp about the main ABINIT
application, you should get some theoretical background. In case you have
already used another electronic structure code, or a quantum chemistry code,
it might be sufficient to read the introduction of [[cite:Payne1992]].
If you have never used another electronic structure code or a Quantum
Chemistry package, you should complete such reading by going (at your own
pace) through the Chaps. 1 to 13 , and appendices L and M of R.M. Martin's book [[cite:Martin2004]].

After having gone through the present New User's Guide, you should follow the
[[tutorial:index|ABINIT tutorial]].

## Introduction
  
ABINIT is a package whose main program allows to find the total energy, charge
density and electronic structure of systems made of electrons and nuclei
(molecules and periodic solids) within Density Functional Theory, using
pseudopotentials and a planewave basis, or augmented plane waves, or even wavelets. 

Some possibilities of ABINIT go beyond Density Functional Theory,
i.e. the many-body perturbation theory (GW approximation the Bethe-Salpether
equation), Time-Dependent Density Functional Theory, Dynamical Mean-Field
Theory, the Allen-Heine-Cardona theory to find temperature-dependent electronic structure. 

ABINIT also includes options to optimize the geometry
according to the DFT forces and stresses, or to perform molecular dynamics
simulation using these forces, or to generate dynamical (vibrations - phonons)
properties, dielectric properties, mechanical properties, thermodynamical
properties, etc. In addition to the main ABINIT code, different utility
programs are provided.

We suppose that you have downloaded the ABINIT package from the Web site,
unpacked it and installed it. If not, you might nevertheless continue reading
the present Web page, just to get an overview, but it might prove more
fruitful to have first downloaded the ABINIT package and at least unpacked it,
see the [installation notes](/installation).

!!! note

    We will use the name "~abinit" to refer to the directory that contains the
    ABINIT package after download. In practice, a version number is appended to
    this name, to give for example: abinit-8.8.0. The ABINIT package versioning
    scheme is explained later in this file.

~abinit contains different subdirectories. For example, the present file, as
well as other descriptive files, should be found in ~abinit/doc/.
Other subdirectories will be described later.

## The main executable: abinit
  
After compilation, the main code will be present in the package as
~abinit/src/98_main/abinit (or perhaps at another place, depending on your installation).

To run abinit you need three things:

1. Access to the executable, abinit. 
2. An input file. 
3. A pseudopotential input file for each kind of element in the unit cell. 

With these items a job can be run.

The full list of input variables, all of which are provided in the single
input file, is given in the ABINIT [[varset:allvars|list of all variable]].
The detailed description of input variables is given in many "Variable Set" files, including:

  * Basic variables, [[varset:basic]]
  * Ground-state calculation variables, [[varset:gstate]]
  * GW variables, [[varset:gw]]
  * Files handling variables, [[varset:files]]
  * Parallelisation variables, [[varset:paral]]
  * Density Functional Perturbation Theory variables, [[varset:dfpt]]

A set of examples aimed at guiding the beginner is available in the [[tutorial:index|tutorials]].

Other test cases (more than 1000 input files) can be found in the ~abinit/test
subdirectories, e.g. "fast", the "vX" series (v1, v2, ... v67mbpt, v7, v8),
"libxc", "paral", the "tutoX" series ...

Many different sorts of pseudopotentials can be used with ABINIT. 
Most of them can be found on the [atomic data files](https://www.abinit.org/downloads/atomic-data-files) 
page of the ABINIT web site. 
There are official recommended pseudopotentials tables 
(the PAW JTH table, and the norm-conserving table from ONCVPSP), and also some older sets of pseudopotentials. 
Information on pseudopotential files can be found in the [[help:abinit#5|ABINIT help file]],
the [[theory:pseudopotentials|Pseudopotential theory document]], on the [ABINIT wiki](https://wiki.abinit.org/doku.php?id=developers:pseudos),
and in the [[topic:PseudosPAW|PseudosPAW]] topics.

!!! warning

    A subset of existing pseudopotentials are used for test cases, and are located in the 
    ~abinit/tests/Psps_for_tests directory but they **are not recommended** for production. 

## Other programs in the package
  
In addition to abinit, there are utility programs provided in the package.
Some utility programs are written in Fortran (like the main abinit program), and
their sources is also in ~abinit/src/98_main. 
These include:

mrgddb and anaddb 
:   They allow one to post-process responses to atomic
    displacements and/or to homogeneous electric field, and/or to strain
    perturbation, as generated by abinit, to produce full phonon band structures,
    thermodynamical functions, piezoelectric properties, superconducting
    properties, to name a few. `mrgddb` is for "Merge of Derivative DataBases",
    while `anaddb` is for "Analysis of Derivative DataBases".

cut3d 
:   It can be used to post-process the three-dimensional density (or
    potential) files generated by abinit. It allows one to deduce charge density
    in selected planes (for isodensity plots), along selected lines, or at
    selected points. It allows one also to make the Hirshfeld decomposition of the
    charge density in "atomic" contributions.

fold2Bloch
:   It is used for unfolding first-principle electronic band structures

aim
:   It is also a post-processor of the three-dimensional density files
    generated by abinit. It performs the Bader Atom-In-Molecule decomposition of
    the charge density in "atomic" contributions.

conducti
:   It allows one to compute the frequency-dependent optical conductivity.

Some utility programs are not written in Fortran, but in Python. They are
contained in ~abinit/scripts, where post-processing (numerous tools) and pre-
processing scripts are distinguished. Some allows one to visualize ABINIT
outputs, like abinit_eignc_to_bandstructure.py.

## Other resources outside the ABINIT package
  
In addition to the ABINIT package, other resources can be obtained from the
[ABINIT github site](https://github.com/abinit). The sources of the latest
version of the ABINIT package are actually mirrored on this site, but for
other resources (not in the package) this is the only download point.

[AbiPy](https://github.com/abinit/abipy)
:   is an open-source library for analyzing the results produced by
    ABINIT (including visualisation), and for preparing input files and workflows
    to automate calculations (so-called high-throughput calculations).
    It provides interface with [pymatgen](http://pymatgen.org/), 
    developed by the [Materials Project](http://materialsproject.org/).
    Abinit tutorials based on AbiPy are available in the [abitutorials repository](https://github.com/abinit/abitutorials).

[PseudoDojo](http://www.pseudo-dojo.org/)
:   is a Python framework for generating and validating
    pseudopotentials (or PAW atomic data files). Normal ABINIT users benefit a lot
    from this project, since the ABINIT recommended table of norm-conserving
    pseudopotentials has been generated thanks to it. 
    The recommended PAW table is also provided via the pseudo-dojo interface.

[abiconfig](https://github.com/abinit/abiconfig)
:   is a holding area for configuration files used to
    configure/compile Abinit on clusters. You might benefit from it if you are 
    installing Abinit on a cluster.

[abiflows](https://github.com/abinit/abiflows)
:   provides flows for high-throughput calculations with ABINIT.


[abiconda](https://github.com/abinit/abiconda)
:   contains conda recipes to build Abinit-related packages (like AbiPy). 
    You might benefit from it if you install Abipy on your machine.


In addition to the resources that the ABINIT developer provide to the
community through the ABINIT packages, portal and Github, many ABINIT-independent 
commercial or free applications can be used to visualize ABINIT
outputs or interact with ABINIT. We provide a (not very well maintained) list
of links in [the last section of http://www.abinit.org/sponsors](http://www.abinit.org/sponsors).
Of course, you might get more by browsing the Web.

## Input variables to abinit
  
As an overview, the most important input variables, to be provided in the
input file, are listed below:

**Specification of the geometry of the problem, and types of atoms:**

[[natom]]
:       total number of atoms in unit cell

[[ntypat]]
:   number of types of atoms

[[typat]]([[natom]]):
:   sequence of integers, specifying the type of each atom.
    NOTE: the atomic coordinates ([[xcart]] or [[xred]])
    must be specified in the same order

[[rprim]](3,3)
:   unscaled primitive translations of periodic cell;
    each COLUMN of this array is one primitive translation

[[xcart]](3,[[natom]])
:   cartesian coordinates (Bohr) of atoms in unit cell
    NOTE: only used when [[xred]] is absent

[[xred]](3,[[natom]])
:   fractional coordinates for atomic locations;
    NOTE: leave out if [[xcart]] is used

[[znucl]]([[ntypat]])
:   Nuclear charge of each type of element; must agree with
    nuclear charge found in psp file.

**Specification of the planewave basis set, Brillouin zone wavevector sampling, and occupation of the bands:**

[[ecut]]            
:       planewave kinetic energy cutoff in Hartree

[[kptopt]]
:       option for specifying the k-point grid
        if [[kptopt]]=1, automatic generation, using ngkpt and shiftk.

[[ngkpt]](3)
:       dimensions of the three-dimensional grid of k-points

[[occopt]]
:       set the occupation of electronic levels:
        =1 for semiconductors
        =3 ... 7  for metals

**Specification of the type of calculation to be done:**

[[ionmov]]
:       when [[ionmov]] = 0: the ions and cell shape are fixed
                        = 2: search for the equilibrium geometry
                        = 6: molecular dynamics

[[iscf]]
:       either a positive number for defining self-consistent
        algorithm (usual), or -2 for band structure in fixed potential

[[optdriver]]
:       when == 3 and 4: will do GW calculations (many-body perturbation theory)

[[rfelfd]]
:       when /= 0: will do response calculation to electric field

[[rfphon]]
:       when = 1: will do response calculation to atomic displacements

**Specification of the numerical convergency of the calculation:**

[[nstep]]
:    maximal number of self-consistent cycles (on the order of 20)

[[tolvrs]]
:    tolerance on self-consistent convergence

[[ntime]]
:       number of molecular dynamics or relaxation steps

[[tolmxf]]
:       force tolerance for structural relaxation in Hartree/Bohr

## Output files
  
Output from an abinit run shows up in several files and in the standard output. 
Usually one runs the command with a pipe of standard output to a log
file, which can be inspected for warnings or error messages if anything goes
wrong or otherwise can be discarded at the end of a run. The more easily
readable formatted output goes to the standard output file, generated by abinit with default extension .abo .
No error message is reported in the latter file. On the other hand, this is the file
that is usually kept for archival purposes.

In addition, wavefunctions can be input (starting point) or output (result of
the calculation), and possibly, charge density and/or electrostatic potential,
if they have been asked for. These three sets of data are stored in unformatted binary files (native Fortran),
or in NetCDF format.  

The Density Of States (DOS) can also be an output as a formatted (readable) file.
An analysis of geometry can also be provided (GEO file).
The name of these files is constructed from a "root" name, 
that might be different for input files and output files, and that is either
provided by ABINIT or provided by the user,
to which the code will append a descriptor, like WFK for wavefunctions, DEN
for the density, POT for the potential, DOS for the density of states...

There are also different temporary files, also constructed from a "root" name.
Amongst these files, there is a "status" file, summarizing the current status of advancement of the
code, in long jobs. The [[help:abinit|ABINIT help file]] contains more details.

## What does the code do?
  
The simplest sort of job computes an electronic structure for a fixed set of
atomic positions within a periodic unit cell. By electronic structure, we mean
a set of eigenvalues and wavefunctions which achieve the lowest DFT energy
possible for that basis set (that number of planewaves). 

The code takes the description of the unit cell and atomic positions and assembles a crystal
potential from the input atomic pseudopotentials, then uses either an input
wavefunction or simple gaussians to generate the initial charge density and
screening potential, then uses a self-consistent algorithm to iteratively
adjust the planewave coefficients until a sufficient convergence is reached in the energy.

Analytic derivatives of the energy with respect to atomic positions and unit
cell primitive translations yield atomic forces and the stress tensor. The
code can optionally adjust atomic positions to move the forces toward zero and
adjust unit cell parameters to move toward zero stress. It can performs
molecular dynamics. It can also be used to find responses to atomic
displacements and homogeneous electric field, so that the full phonon band
structure can be constructed.

## Versioning logic
  
We finish this "help for new user" with a brief explanation of the logic of ABINIT version releases.

The full name of a version has three digits (for example, 8.8.3). The first
digit is the slowly varying one (in average, it is changed after two or three
years). It indicates the major efforts and trends in that version. At the
level of 1.x.y ABINIT (before 2000 !), the major effort was placed on the
"ground-state" properties (total energy, forces, geometry optimisation,
molecular dynamics ...). With version 2.x.y , response-function features
(phonons, dielectric response, effective charges, interatomic force constants
...) were included. The main additional characteristics of version 3.x.y were
the distribution under the GNU General Public Licence, the set-up of the
documentation and help to the user through the Web site in html format, and
the availability of GW capabilities. The version 4.x.y put a lot of effort in
the speed of ABINIT (e.g. PAW), and its parallelisation. These historical
developments explain why the tests are gathered in directories "v1", "v2",
"v3", etc. Every 4 to 8 months, we release a "production version" of ABINIT in
which the second digit, an even number, is incremented, which usually goes
with additional features. A [release notes document](/about/release-notes) is issued, with the list of
additional capabilities, and other information with respect to modifications
with the previous release. The odd second digits are used for internal
management only, so-called "development versions" of ABINIT (for example
8.9.0). Two versions differing by the last (third) digit have the same
capabilities, but the one with the largest last digit is more debugged than
the other: version 8.8.3 is more debugged than 8.8.2, but no new features has
been added (so likely, no additional bug!).

In order to start using ABINIT, please follow [[tutorial:index|this tutorial.]]
To learn how to compile the code from source, please consult the following guide:

<embed src="https://wiki.abinit.org/lib/exe/fetch.php?media=build:installing_abinit.pdf" type="application/pdf" width="100%" height="480px">
# The a-TDEP utility  

The Temperature Dependent Effective Potential (TDEP) method
has been developped by O. Hellman *et al.* [[cite:Hellman2011]],
[[cite:Hellman2013]], [[cite:Hellman2013a]] in 2011 and the |a-TDEP| implementation
in ABINIT has been performed and used for the first time in 2015 by
J. Bouchet and F. Bottin [[cite:Bouchet2015]], [[cite:Bouchet2017]].

This manual can be found as a pdf file: [[pdf:a-TDEP_Guide| a-TDEP guide]]

## Prerequisite and theory

The approach used in this code is detailed in a publication dedicated to the development
of all formula (see [[pdf:a-TDEP_Paper|a-TDEP paper]]). We strongly encourage all the users to carefully read
this paper before beginning. All the vibrational, elastic and thermodynamic
quantities computed by |a-TDEP| are
presented with the same writing conventions as the ones used in the output files of |a-TDEP|.
In the same manner, a comprehensive understanding of some ABINIT basic variables is also required
in order to fill the input file and read the output file of |a-TDEP|.

In addition, this paper is also useful to understand
the limitations and convergences which are inherent to the present method.
These particular points are sometimes discussed in the
article, with some references and illustrating examples.

## The ABINIT computation

To run |a-TDEP|, a preliminary
ABINIT simulation is needed. This one could be a molecular dynamic trajectory
or a set of "ground state" calculations on specific configurations (representative of a given thermodynamic state).
After that, all the configurations have to be merged:
(i) in a single *NetCDF* file `HIST.nc` or (ii) in three separated *ASCII* files `fcart.dat`, `xred.dat` and
`etot.dat` (forces in cartesian coordinates, positions in reduced coordinates, total energies in Ha),
as they are written in the output file of ABINIT. In the later case, the 3 files can be built easily
by concatenating in each one all the time steps or configurations (using `agrep` shell instruction, for example).

## The |a-TDEP| computation

In a same manner as performed for ABINIT, the use of |a-TDEP| is quite simple. 
One has just to execute `atdep` as follows:

```sh
    atdep < input.files > log
```

with the `input.files` file containing 3 lines. The first one defines the input
file, the second one is the *NetCDF* file (if present, see above) and the third one
defines the root of all the output files:

    input.in
    HIST.nc
    output

The detection of the `HIST.nc` file is performed at the beginning; so, if this
one is absent, the code will automatically search the 3 `ASCII.dat` files.

## The input files

An example of a |a-TDEP|  calculation (in the special case where the *NetCDF* file `HIST.nc` is employed)
can be found in [[test:v8_37]]. The 2 input files are
given in the `tests/v8/Input` directory.  
Let us describe briefly this [[test:v8_37]] file:

{% dialog tests/v8/Input/t37.abi %}

The input file format is fixed. So:

1. This file begins with a `NormalMode` or `DebugMode` keyword and finishes with `TheEnd` (all the lines after are not read).
2. All the lines between `# Unit cell definition` and `# Optional inputs` are fixed.
3. Between `# Optional inputs` and `TheEnd`, the format is free.

More details:

* The section `# Unit cell definition` defines the bravais lattice [[brav@atdep|brav]]
  (here, a simple cubic), the number of atoms in the unit cell [[natom_unitcell@atdep|natom_unitcell]]
  (here, 5 atoms), their reduced coordinates in the unit cell [[xred_unitcell@atdep|xred_unitcell]]
  (here, a perovskite) and the type of atoms in the unit cell [[typat_unitcell@atdep|typat_unitcell]]
  (here, one atom A, one atom B and 3 atoms C).
* The section `# Supercell definition` defines the multiplicity of the
  supercell with respect to the unit cell multiplicity (here, it is a simple
  2x2x2 multiplication of the unit cell) and the temperature of the system
  temperature(here, 495.05 K).
* The section `# Computation details` defines the range [[nstep_max@atdep|nstep_max]]...[[nstep_min@atdep|nstep_min]]
  of time steps or configurations (here, 100 time steps) and the
  cutoff radius for the pair interactions [[rcut@atdep|Rcut]] (here, all the interaction pairs
  with a bond length larger than 7.426 bohr will not be considered).
* The section `# Optional inputs` can define a large number of optional
  keywords (here [[ngqpt2@atdep|ngqpt2]] defining the q-point grid for the vDOS integration
  is set to 2 2 2 in order to have a test sufficiently fast, which means that
  all the thermodynamic quantities have no sense.)
All the input variables are defined in the `a-TDEP` section of the input variables description.
Note that some input variables, not defined in the `input.in` file, are obtained
from the `HIST.nc` file. In particular, the features of the supercell.

<sub><sup>TODO: Explain the extra input variables when the 3 ASCII files are employed.</sup></sub>

## The output files

A large number of output files are obtained after an execution of |a-TDEP|.

{% dialog tests/v8/Refs/t37.abo %}
{% dialog tests/v8/Refs/t37omega.dat %}
{% dialog tests/v8/Refs/t37thermo.dat %}

1. `*.abo` is the main output file. It includes an echo of the input variables,
   some intermediary results, the definition of the various shells of interaction,
   the second order IFCs for all the atoms in each shell, the elastic constants and moduli,
   the energy of the model...
2. `*omega.dat` contains the dispersion of phonon frequencies (in meV) along a path in the Brillouin Zone.
3. `*thermo.dat` lists all the thermodynamic quantities obtained by considering
   the system as a quantum harmonic crystal: specific heat, vibrational
   energy, entropy and free energy. It also gives all these contributions as a
   function of temperature in the harmonic approximation.
4. `sym.dat` details all the symmetry operations of the bravais lattice,
5. `qpt.dat` defines the q-point grid used to compute the phonon frequencies
   contained in the `omega.dat` file.
6. `xredaverage.xyz` includes the ideal and average positions in the supercell.
7. `Indym*.dat` contain all the symmetry relations between one or two atoms
   in the unit cell or the supercell.
8. `vdos.dat` displays the vibrational density of states (in meV).
9. `dij.dat` lists the dynamical matrices for a particular set of q-points.
10. `etotMDvsTDEP2.dat` compares the MD trajectory with the one computed
    using the second order IFCs (these ones must be superimposed, as much
    as possible).
11. `fcartMDvsTDEP2.dat` plots the MD forces wrt the forces computed using
    the second order IFCs (the cloud of points must be closer to the first bisector).
12. `eigenvectors.dat` lists all the eigenvectors for a particular set of q-points.
13. `nbcoeff-phij.dat` shows how the number of IFC coefficients are reduced (for each shell and each symmetry).
14. ...
---
authors: XG, DCA
---

# The mrgddb utility  

The user is advised to be familiar with the main [[help:abinit]] help file
before reading the present file. It is important to read also the [[help:anaddb]]
to complement the present reading.

## 1 Introduction
  
The mrgddb code has the purpose to merge "transfer" DDBs (that were generated
from the ABINIT code) to make a complete DDB that can be exploited by the Anaddb code.

The input is very simple, and could be given directly at the screen, or more
conveniently, piped from a file. The user should provide first the name of the
new (output) DDB. He/she should then give a short description (one line) of
this new DDB that will be created. This line will be printed at the beginning of the DDB.

The user should then give the number of DDBs that will be merged, then the
whole set of filenames for the DDBs to be merged, one on each line.

## 2 What is the usefulness of a merging code ?
  
The ABINIT code in its present version is only able to produce results for one
q wavevector for each dataset. A database for more than one q point can thus
be created using MRGDDB. Also, it is useful to be able to merge different DDBs
if they are produced independently on different machines.

## 3 How does the merging code work ?
  
The DDBs are made of two parts. The first part is a list of the parameters
that were used to make the DDB, and the second part lists the 2DTEs and 3DTEs.
The merging code will check if the following variables are exactly the same in
the different input DDB: natom, [[ntypat]], [[nband]], [[acell]], [[amu]],
[[ecut]], [[ixc]], lloc, [[ngfft]], [[occ]], [[rprim]], [[typat]], [[xred]],
zion. For [[nband]] and [[occ]], the value of [[occopt]] is taken into account (see abinit help file). 
If possible, MRGDDB will produce a DDB with occopt=0.

In case of two different data sets, the code will print an error message and
stop. The code cannot merge two DDB that have been generated using two
different geometries or convergence ([[ecut]], ...) parameters. The only
exception is connected to the possibility to use Time-reversal symmetry to
decrease the number of special k points when the wavevector of the
perturbation is Gamma. In that case, the code will merge the DDBs and put the
largest set of k-points inside the new DDB. MRGDDB will copy the latest date
of the transfer or current DDB and copy it in the new DDB. It will also take
the less accurate tolwfr and copy it in the new DDB. This ends the first part
of the action of MRGDDB, namely to compare the information of the two
different DDB. 

When the checking is done, MRGDDB will check the content of the
different data blocks and constitute the new DDB by copying sequentially the
non-identical blocks and merging the identical blocks. In case two elements
are identical, MRGDDB copies the value of the transfer DDB. (This latter
property makes it easy to get rid of old, erroneous data and put new, correct
data in its place) Finally, the summary of the block content of the DDB is
provided at the end of the DDB file.
---
authors: XG, RC, GMR, JFB, MCote
---

# the Cut3D utility  

This file explains the use and i/o parameters needed for the
"Cut 3-Dimensional files" post-processor of the ABINIT package.

This code is able to analyse the files produced by ABINIT, that contain
3-Dimensional real space data, like all types of potential files, density
files. Wavefunction files data can also be analysed: first, a k-point number,
and the band number must be given, then, the corresponding wavefunction is transformed to real space.

In all these cases, thanks to Cut3D, one can obtain 2-Dimensional data
corresponding to a cut by a plane, or 1-Dimensional data along a line. One can
also translate the original formatting into many different ones.

Finally, one can also perform angular momenta analysis of wavefunctions with
respect to any given atom, computation of the Hirshfeld atomic charge (starting from a density file).

## 1 How to run cut3d
  
To run cut3d, simply type:
    
    cut3d
  
then, provide answer to the questions. You will have to give first the name of the unformatted file.
For example, t1xo_DEN.

## 2 Analyze most types of files, excluding wavefunction files
  
Supposing that you are not treating a wavefunction file,
you will have to choose between different possibilities:

1. computation of data for a point, to be specified
2. computation of data along a line, to be specified
3. computation of data on a 2D grid, to be specified
4. computation of data on a 3D grid, to be specified
5. conversion to formatted file
6. conversion to indexed formatted
7. conversion to Molekel format
8. conversion to 3D data with coordinates (tecplot ASCII format)
9. output .xsf file for XCrysDen
11. compute atomic charge using the Hirshfeld method

For option 1) you will have the possibility to specify a point in reduced or cartesian coordinates.
For option 2) you will have the possibility to specify a line by its two 
end-points in reduced or cartesian coordinates, or by it being perpendicular to some plane.
For option 3) and 4) many possibilities are offered, including specifications
thanks to points defined in reduced coordinates, cartesian coordinates, or atomic positions.  
To continue the analysis, simply answer the questions of the code, that should
be sufficiently self-explanatory.

## 3 Analyze wavefunction files
  
Instead, supposing that you are treating a wavefunction file, you will be able
to perform the analysis of one wave function. You will have to define the k
point, the number of the band, and possibly the spin-polarization or the spinor component.
Then, you will be asked whether you want to perform the angular component
analysis. You will have to provide the radius of the sphere(s) around each
atom, for which the angular analysis will be performed.
Finally, you will be given the choice between different formatting of the
wavefunction real-space data, including bare files, or XCrysDen formatted file.
---
authors: AB,  OR
---

# the Fold2Bloch utility  

This file explains the use and i/o parameters needed for the "fold2Bloch" post-processor of the ABINIT package.  

This program generates an unfolded spectrum in a small cell, from a folded
spectrum of eigenvalues in a supercell. The new k-wavevectors and the weights
for the range of energies (standard postscript file **_fold2Bloch.out**_ ) are
based on the user's input of number and direction of folds as one of the
execution arguments (x:y:z). For the calculations program uses coefficients
and their vectors, K values, and eigenvalues found in the **__WFK_** file.
Fold2Bloch uses **_wfk_open_read, wfk_read_band_block and hdr_io_** routines
from the ABINIT package to read the _WFK file header and information about the
element structure which is used for our calculations.

It will be easier to discover the present file with the help of the tutorial ([[tutorial:fold2bloch]]).  

## 1 Compiling instructions
  
Compiled as part of abinit package. Fold2Bloch does not differentiate between
parallel and serial configuration so it always compiles for serial operation.

## 2 Execution
  
The program fold2Bloch is executed by invoking the following command _from the directory of the case file_ :

**fold2Bloch case_WFK x:y:z**

Where:

**case** is the name of the file that comes before _WFK identification  
 **x:y:z** are integer values( >0) that represent a multiplicity in the
corresponding direction used when constructing the super-cell; separated by a
colon (:).

Ex.

**fold2Bloch Ga8As7Bi1_WFK 2:2:2**

NOTE:  
If cell structure is spin polarized then there are two output files:  
**fold2Block_up.out** and **fold2Bloch_down.out**

## 3 Output sample
  
_**Output Sample**_

Below are three randomly selected parts from an output file. 
The data is presented as follows (from left to right columns):

New K Values (x, y, z) Eigenvalue(Ht) Weight

0.000000 0.000000 0.000000 -0.884864 1.000000

0.000000 0.000000 1.000000 -0.884864 0.000000

0.000000 0.000000 -1.000000 -0.884864 0.000000

0.000000 1.000000 0.000000 -0.884864 0.000000

0.000000 1.000000 1.000000 -0.884864 0.000000

0.000000 1.000000 -1.000000 -0.884864 0.000000

0.000000 0.000000 0.000000 -0.432062 0.000000

0.000000 0.000000 1.000000 -0.432062 0.500000

0.000000 0.000000 -1.000000 -0.432062 0.500000

0.000000 1.000000 0.000000 -0.432062 0.000000

0.000000 1.000000 1.000000 -0.432062 0.000000

0.000000 1.000000 -1.000000 -0.432062 0.000000

0.000000 0.000000 0.000000 -0.432062 0.000000

0.000000 0.000000 1.000000 -0.432062 0.500000

0.000000 0.000000 -1.000000 -0.432062 0.500000

0.000000 1.000000 0.000000 -0.432062 0.000000

0.000000 1.000000 1.000000 -0.432062 0.000000

0.000000 1.000000 -1.000000 -0.432062 0.000000

  

-0.333333 0.666667 0.500000 1.574788 0.499999

-0.333333 0.666667 1.500000 1.574788 0.000000

-0.333333 0.666667 -0.500000 1.574788 0.500001

-0.333333 -0.333333 0.500000 1.574788 0.000000

-0.333333 -0.333333 1.500000 1.574788 0.000000

-0.333333 -0.333333 -0.500000 1.574788 0.000000

-0.333333 0.666667 0.500000 1.574788 0.500001

-0.333333 0.666667 1.500000 1.574788 0.000000

-0.333333 0.666667 -0.500000 1.574788 0.499999

-0.333333 -0.333333 0.500000 1.574788 0.000000

-0.333333 -0.333333 1.500000 1.574788 0.000000

-0.333333 -0.333333 -0.500000 1.574788 0.000000

  

-0.333333 -0.166667 0.000000 -0.403296 1.000000

-0.333333 -0.166667 1.000000 -0.403296 0.000000

-0.333333 -0.166667 -1.000000 -0.403296 0.000000

-0.333333 0.833333 0.000000 0.040617 0.000000

-0.333333 0.833333 1.000000 0.040617 0.000000

-0.333333 0.833333 -1.000000 0.040617 0.000000

-0.333333 -0.166667 0.000000 0.040617 0.000000

-0.333333 -0.166667 1.000000 0.040617 0.500003

-0.333333 -0.166667 -1.000000 0.040617 0.499997

-0.333333 0.833333 0.000000 0.040617 0.000000

-0.333333 0.833333 1.000000 0.040617 0.000000

-0.333333 0.833333 -1.000000 0.040617 0.000000

-0.333333 -0.166667 0.000000 0.040617 0.000000

-0.333333 -0.166667 1.000000 0.040617 0.499997

-0.333333 -0.166667 -1.000000 0.040617 0.500003

## 4 Subroutines and functions

  
_**Subroutines and Functions**_

**Subroutine Getargs (getargs.F90)**

This routine read in the arguments from the command line, checks them for
correct format, and checks whether the input file exists. All the values are
assigned to their appropriate variables and are passed back to **fold2Bloch**.

**Parameters**

_Input:_

folds, fname

folds : empty array(1,3)  
fname : empty string

_Output:_

folds, fname

folds : array containing number of folds in x, y, and z directions. from
command argument  
fname : string containing the name of the input file. from the command
arguments

_Local:_

num_args, argcount, ii, ios, args, argfolds, dir

num_args : integer containing the number of command arguments sent  
argcount : integer counter used to iterate through all arguments  
ii : index used to indicate ':' position in the folds argument  
ios : integer used with iostat to check if folds are digits  
args : array containing all the arguments  
argfolds : temp string containing folds argument  
dir : boolean use to check if input file exists

**Subroutine Progress (Progress.F90)**

Progress compares K point currently being process with the total number of K
points and calculates percent complete. It then outputs the results and the
current K point that is being processed to the screen for the user to know.

**Parameters**

_Input:_

ikpt, nkpt, kpt

ikpt : current K point number  
nkpt : total number of K points  
kpt : current K point being processed

_Output:_

NONE

_Local:_

NONE

**Subroutine NewK (NewK.F90)**

This subroutine finds new K values for the unfolded spectrum. The K values are
determined based on how many times and in what direction the function needs to
be unfolded, which gives us the shape and size of the unfolded Brillouin zone.

**Parameters**

_Input:_

XX, YY, ZZ, FX, FY, FZ

XX : Original K point coordinate in the x-plane  
YY : Original K point coordinate in the y-plane  
ZZ : Original K point coordinate in the z-plane  
FX : Number of folds in the X direction  
FY : Number of folds in the Y direction  
FZ : Number of folds in the Z direction

_Output:_

NKVal

NKVal : 2D array containing x-y-z coordinates of new K points.

_Local:_

field_x, field_y, field_z, TX, TY, TZ, loop, ii, jj, kk, size

field_x : size of unfolded Brillouin zone in X direction  
field_y : size of unfolded Brillouin zone in Y direction  
field_z : size of unfolded Brillouin zone in Z direction  
TX : temporary holds the X coordinate of a new K point  
TY : temporary holds the Y coordinate of a new K point  
TZ : temporary holds the Z coordinate of a new K point  
loop : points to a place in NKVal to store a new K point  
I, j, k : counters for x, y, and z directions respectively  
size : number of new K points, used for NKVal allocation

**Subroutine SortC (SortC.F90)**

This subroutine sorts energy coefficients into appropriate groups and finds
each group's weight on the unfolded spectrum. Number of groups corresponds to
the number of new K points found in **NewK**. The coefficients are sorted
based on their relative position to the Gamma point.

**Parameters**

_Input:_

FX, FY, FZ, Vector, CoefC, NV

FX, FY, FZ : number of folds in each directions (see **NewK** )  
Vector : array containing x-y-z coordinates of each energy coefficient  
Coef : array containing energy coefficients for a specific Eigenvalue  
NV : number of vectors, and coefficients (one vector per coefficient)

_Output_

Weights

Weights : an array containing weights of each group

_Local_

TGroupc, Sums, counter, sumtot, remainder_x, remainder_y, remainder_z, jj, kk, ll, pp, el

TGroupC : an array containing groups of sorted coefficients  
Sums : an array containing sums of squared coefficients in each group  
counter : an array that keeps track of number of coefficients in each group  
sumtot : total sum of sums of each group  
reminader_x : relative position of each coefficient in x-axis  
reminader_y : relative position of each coefficient in y-axis  
reminader_z : relative position of each coefficient in z-axis  
jj, kk, ll : counters cycling through each group  
p : counter for elements in each group  
el : points to a location in Sums to store the next value

**MAIN (fol2Bloch.F90)**

**Parameters**

_Input_

fname, folds, nkval, nkpt, kpts, cg, kg, eig, weight, nsppol, nspinor, npwarr,
nband.

fname : case name inputted as part of the initial arguments by user, from
getargs()  
folds : initial argument (x:y:z), indicating number and direction of folds,
from getargs()  
nkval : array of new k points after the unfolding, from newk()  
nkpt : total number of K points, from hdr%nkpt  
kpts : K points coordinates, from hdr%kptns  
cg :  
kg : array of vectors per K point, from wfk_read_band_bloch() **  
**eig : array of eigenvalues per K point, from wfk_read_band_bloch() **  
**weight : weights calculate by sortc()  
nsppol : integer indicating up/down polarization (1 if none, 2 if polarized)  
nspinor : number of local orbitals, from hdr%nspinor  
npwarr : number of vectors in each kpoint, from hdr%npwarr  
nband : number of bands in K point, from hdr%nband  

_Output_

nkval, eig, weights  
(written to fold2Bloch.out or fold2Bloch_up.out and fold2Bloch_down.out if spin polarized)

nkval : new K points after unfolding  
eigval : original eigenvalues  
weights : weight of each new K point for a certain eigenvalue

_Local_

ikpt, iband, csppol, cg_b, count, infile, outfile, coefc, mband, mcg, outname

ikpt : integer number of current K point being processed  
iband : integer current band being processed  
csppol : integer current spin polarization being processed  
infile : integer input file number  
outfile : integer output file number  
coefc : array containing coefficient for the band being processed  
mband : integer maximum number of bands  
mcg : integer maximum possible number of coefficients to allocate them all  
 **  
**  
---
authors: PCasek, FF, XG
---

# the AIM utility  

This file explains the use and i/o parameters needed for the Atom-In-Molecule 
(AIM - Bader analysis) utility of the ABINIT package.  
The AIM utility allows to analyse charge densities produced by the ABINIT
code. The AIM analysis (Atom-In-Molecule) has been proposed by Bader. Thanks
to topological properties of the charge density, the space is partitioned in
non-overlapping regions, each containing a nucleus. The charge density of each
region is attributed to the corresponding nucleus, hence the concept of Atom- In-Molecule.

## 1 Introduction
  
The Bader technique allows to partition the space in attraction regions. 
Each of these regions is centered on one atom. The only input for this technique is
the total charge density: the density gradient line starting from one point
in space leads to one unique attracting atom. 
(References to the relevant literature are to be provided).

Around each atom, the basin of attraction forms a irregular, curved
polyhedron. Different polyhedra might have faces, vertices of apices in
common. Altogether, these polyhedra span the whole space.

The points where the density gradient vanishes are called "critical points" (CP). 
They all belong to the surface of some Bader polyhedra. According to the
number of positive eigenvalues of the Hessian of the density, one will distinguish:

  * "Bonding CPs" (a minimum along one eigenvector of the Hessian, but a maximum along the two other eigenvectors of the Hessian), 
     that belong to a face of the Bader volume (one BCP per face)

  * "Ring CPs" (a minimum along two eigenvectors of the Hessian, but a maximum along the remaining eigenvector of the Hessian), 
     that belong to a vertex of the Bader volume (one RCP per vertex)

  * "Cage CPs" (a global minimum, defining an apex of the Bader volume).

The Euler relation must be fulfilled: the number of BCPs minus the number of
RCPs plus the number of CCPs must equal 2. 
The Bader polyhedra might be convex (this is the usual case), but might as well not be convex.

In the present implementation, the user is required to specify one atom for
which he/she wants to compute the Bader volume, surfaces or critical points.
Different runs are needed for different atoms.

In case of the search for critical points, one start from the middle of the
segment with a neighbouring atom (all neighbouring atoms are examined in
turn), and evolves towards a nearby point with zero gradient. 

Then, in case [[aim:crit]] equals 2, one checks that the CP that has been found belongs to
the attraction region of the desired atom. This last step is by no means
trivial. In the present implementation, the check is done by means of the
straight line (radius) connecting the point with the atom. In case the Bader
volume is not convex, it might be that a correctly identified CP of the Bader
volume defines a radius that goes through a region that does not belong to the
Bader volume: the CP is "hidden" from the atom defining the attraction
region. In this case, the CP is considered as NOT being part of the Bader
volume, unfortunately. The reader is advised to look at the automatic test of
the MgO molecule to see such a pathology : no cage critical point is found for
the Mg atom. By chance, this problem is not a severe one, when the user is
interested by other aspects of the Bader analysis, as described below.

In case of the search for the Bader surface, or the integral of the charge
within the Bader surface, the user should define a series of radii of
integration, equally spread over the theta and phi angular variables.
Symmetries can be used to decrease the angular region to be sampled. Along
each of these radii, the algorithm will determine at which distance the radius
crosses the Bader surface. The integral of the charge will be done up to this
distance. For this search of the Bader surface, the information needed from
the critical points analysis is rather restricted : only an estimation of the
minimal and maximal radii of the Bader surface. This is why the fact that not
all CP have been determined is rather unimportant. On the other hand, the fact
that some part of the Bader surface might not be "seen" by the defining atom
must be known by the user. There might be a small amount of "hidden" charge as
well. Numerical tests show that the amount of hidden charge is quite small,
likely less than 0.01 electron.

The determination of density, gradient of density and hessian of the density
is made thanks to an interpolation scheme, within each (small) parallelepiped
of the FFT grid (there are n1*n2*n3 such parallelepiped). Numerical subtleties
associated with such a finite element scheme are more delicate than for the
usual treatment of the density within the main ABINIT code ! There are many
more parameters to be adjusted, defining the criteria for stopping the search
for a CP, or the distance beyond which CPs are considered different. The user
is strongly advised to experiment with the different parameters, in order to
have some feeling about the robustness of his/her calculations against
variation of these. Examples from the automatic tests should help him/her, as
well as the associated comments in the corresponding README files.

Note that the AIM program can also determine the Bader distance for one given
angular direction, or determine the density and laplacian at several, given,
points in space, according to the user will.

## 2 Input and output files
  
To run the program one needs to prepare two files:

  * [files-file] file which contains the name of the input file, the root for the names 
    of the different output files, and the name of other data files. 

  * [input-file] file which gives the values of the input variables 

Except these files you need the valence density in real space (*_DEN file,
output of ABINIT) and the core density files (*.fc file, output of the FHI
pseudopotential generator code, actually available from the ABINIT Web page)

The files file (called for example aim.files) could look like:
    
      aim.in    # input-file
      abo_DEN   # valence density (output of ABINIT)
      aim       # the root of the different output files
      at1.fc    # core density files (in the same order as
      at2.fc    # in the ABINIT files-file )
      ...
    
About the _DEN file:  
Usually, the grid in the real space for the valence density should be finer
than the one proposed by ABINIT. (For example for the lattice parameter
7-8~a.u. , ngfft at least 64 gives the precision of the Bader charge estimated
to be better than 0.003~electrons).

About the core density files:  
LDA core density files have been generated for the whole periodic table, and
are available on the ABINIT web site. Since the densities are weakly dependent
on the choice of the XC functional, and moreover, the charge density analysis
is mostly a qualitative tool, these files can be used for other functionals.
Still, if really accurate values for the Bader charge analysis are needed, one
should generate core density files with the same XC functional as for the valence density.

The main executable file is called aim. Supposing that the "files" file is
called aim.files, and that the executable is placed in your working directory,
aim is run interactively (in Unix) with the command

    aim < aim.files >& log

or, in the background, with the command

    aim < aim.files >& log &

where standard out and standard error are piped to the log file called "log"
(piping the standard error, thanks to the '&' sign placed after '>' is
**really important** for the analysis of eventual failures, when not due to
AIM, but to other sources, like disk full problem ...). 

The user can specify any names he/she wishes for any of these files. 
Variations of the above commands could be needed, depending on the flavor of UNIX that is used on the
platform that is considered for running the code.

The syntax of the input file is quite similar to the syntax of the main abinit
input files: the file is parsed, keywords are identified, comments are also
identified. However, the multidataset mode is not available.

Note that you have to specify what you want to calculate (default = nothing).
An example of the simple input file for Oxygen in bulk MgO is given in
~abinit/test/v3/Input/t57.in. 
There are also corresponding output files in this directory.

Before giving the description of the input variables for the aim input file,
we give some explanation concerning the output file.

The atomic units and cartesian coordinates are used for all output parameters.
The names of the output files are of the form root+suffix. There are different output files:

* [*.out] - the central output file - there are many informations which are clearly described. 
  Here is some additional information on the integration - there is separated integrations of the core and the valence density. 
  In both cases the radial integration is performed using cubic splines and the angular ones by Gauss quadrature. 
  However the principal part of the core density **of the considered atom** is integrated in the sphere 
  of minimal Bader radius using spherical symmetry. 
  The rest of the core density (of this atom, out of this sphere) together with all the core contributions 
  of the neighbors are added to the valence density integration. In the output file, there is the result 
  of the **complete** integration of the core density of the atom, then the two contributions 
  (spherical integration vs. the others) and then the total Bader charge. 

* [*.crit] - the file with the critical points (CPs) - they are listed in the order 
  Bond CPs (BCPs), Ring CPs (RCPs) and Cage CPs (CCPs). 
  The line of the output contains these informations: 
  $$ position \quad \frac{eigenvalues}{of\ Hessian} \quad \frac{index\ of}{bonded\ atom} \quad \Delta \rho_c \quad \rho_c $$ 
  (note: position is evaluated with respect to the considered atom, while the index of bonded atom is listed only for BCPs). 

* [*.surf] - the file with the Bader surface - there is a head of the form (in latex): 
  
```latex
      \begin{tabular}{ccc}
          index of the atom & \multicolumn{2}{c}{position} \\
          ntheta & thetamin & thetamax \\
          nphi & phimin & phimax \\
      \end{tabular}
```

and the list of the Bader surface radii: $$ \theta \quad \phi \quad
r(\theta,\phi) \quad W(\theta,\phi)$$ (note : $W(\theta,\phi)$ is the weight
for the Gauss quadrature). The minimal and maximal radii are given at the last line.

  * [*.log] - the log file - a lot of informations but it is not very nice actually. 
  * [*.gp] - gnuplot script showing the calculated part of the Bader surface with lines. 

The gnuplot scripts are made in the manner that one needs type only
    
    load  'file'

(quotes are necessary). Note, that this isn't considered as the visualization (it is only for working purpose)!

## 3 List of input variables
  
The list of input variables for the aim input file is presented in the [[varset:aim|aim set of input variables]].
---
authors: DCA,  XG
---

# ABINIT, the main code  

This document explains the IO parameters and format needed for the main code (abinit) in the ABINIT package.  

The new user is advised to read first the [new user's guide](../new_user), before reading the present file. 
It will be easier to discover the present file with the help of the [[tutorial:index|tutorial]].
Many user guides are also present on the Web.
As an example, for calculating response properties using abinit, the complementary [[help:respfn]] is needed.
Some additional specialized documentation is not available on the Web, but inside the package, but this should be for
advanced users only.

<a id="intro"></a>
## 1 How to run the code?

The main executable file is called abinit. Supposing that the input file is
called **run.abi**, and that the executable is placed in your working directory,
abinit is run interactively (in Unix) with the command

    abinit run.abi >& log
  
or, in the background, with the command

    abinit run.abi >& log &

where standard out and standard error are piped to the log file called "log"
(piping the standard error, thanks to the '&' sign placed after '>' is
**really important** for the analysis of eventual failures, when not due to
ABINIT, but to other sources, like disk quota issues). It is also possible 
to separate the standard error file from the standard output file with the command

    abinit run.abi > log 2> err &

or even more explicitly 

    abinit run.abi > run.log 2> run.err &

Actually, the user can specify any names he/she wishes for any of these files. 
The suffix .abi is the most usual for the abinit input file, likewise the suffix .abo for the main output file.
Nevertheless, the names of all input/output/temporary abinit files can also be tuned. 
See e.g. [[output_file]] or [[topic:Control]]. 

Variations of the above commands could be needed, depending on the flavor of Unix that is used
on the platform that is considered for running the code.  
If you do not underderstand the standard Unix syntax above, please get familiarized with Unix before continuing.

<a id="2"></a>
## 2 The underlying theoretical framework and algorithms

The methods employed in this computer code to solve the electronic structure
problem are described in part in different review papers as well as research
papers. The code is an implementation of the Local Density Approximation to
the Density Functional Theory, based upon a plane wave basis set and separable
pseudopotentials. The iterative minimization algorithm is a combination of
fixed potential preconditioned conjugate gradient optimization of wavefunction
and a choice of different algorithms for the update of the potential, one of
which is a potential-based conjugate gradient algorithm.

The representation of potential, density and wavefunctions in real space will
be done on a regular 3D grid of points. Its spacing will be determined by the
cut-off energy [[ecut]] of the planewave basis in
reciprocal space. This grid of points will also be the starting point of Fast
Fourier Transforms between real and reciprocal space. The number of such
points, called [[ngfft]], should be sufficiently large for adequate
representation of the functions, but not too large, for reasons of
computational efficiency. The trade-off between accuracy and computational
efficiency is present in many places of the code, and addressed briefly at the
end of the present help file.

We recommend a good introduction to many different concepts valid for this
code, available in this [[cite:Payne1992|Reviews of Modern Physics article]].
Note that this paper does NOT reflect the present status of the code. 
ABINIT is closer in spirit to the [[cite:Kresse1996|paper]] of Kresse and Furthmuller.
If you have never used another electronic structure code or a Quantum
Chemistry package, you should browse through the Chaps. 1 to 13, and
appendices L and M of [[cite:Martin2004|this book]] by R. M. Martin

<a id="input"></a>
## 3 The input file

### 3.1 Format of the input file
  
Note that this input file was called ab_in in the example of the [introduction](#intro).
We first explain the content of the input file without use of the 
**multi-dataset** possibility that will be explained in section [3.3](#multidatasets).

The parameters are input to the code from a single input file. 
Each parameter is provided by giving the name of the input variable and then placing
the numerical value(s) after the name, separated by one or more spaces, or
even an equal sign (equal signs are replaced by blanks by the parser).
Depending on the input variable, the numerical value may be an integer or a
real number (internal representation as double precision number), or 
a character string (delimited with **double quotes**) and may
actually represent an array of values. If it represents an array, the next set
of numbers separated by spaces are taken as the values for the array.

  * Do NOT separate a minus sign from the number to which it applies. 
  * Do NOT use tabs. 
  * NOTE THAT NO LINE OF THE INPUT FILE MAY EXCEED 132 CHARACTERS. 
    That is, only the first 132 characters of each line of the input file 
    will be read and parsed for input variables and their values. 

The names of all the parameters can be found in the [[varset:allvars|input variable database]]. 
The list of input variables present in the latter file links them to their definitions, contained 
in different "variable set" files, some of which are listed here:

  * Basic variables, [[varset:basic]]
  * Files handling variables, [[varset:files]]
  * Ground-state calculation variables, [[varset:gstate]]
  * GW variables, [[varset:gw]]
  * Parallelisation variables, [[varset:paral]]
  * Density Functional Perturbation Theory variables, [[varset:dfpt]]

In the actual input file, these parameters may be given in any desired order,
and more than one may be given per line. 
Spaces are used to separate values and additional spaces are ignored.  
An as example of input, the parameter for length scales is called [[acell]]
and is an array [[acell]](3) for the lengths of the primitive translations in
Bohr atomic units. 
To input a typical Si diamond lattice one would have the line

    acell 10.25311 10.25311 10.25311

in the input file. This may equivalently be written

    acell 3*10.25311

and will still be parsed correctly: it is equivalent to the above line. Even

    acell *10.25311

will work. In the latter case the '*' sign means that the parser should use
the given value to fill the array, by repeating it as **many time as needed**.  
Multiple spaces are ignored, as is any text which does not contain the
character strings which correspond to some input parameters. In case of
arrays, only the needed numbers will be considered, and the eventual numbers
after those needed will also be ignored. For example,

    natom 3           # This gives the number of atoms  
    typat 1 1 2 2 3   # typat(1:natom) gives the type of each atom: only  
                      # the first three data are read, since [[natom]]=3 `  

A given variable is identified by the parser by having at least one blank
before it and after it (again, multiple blanks are irrelevant).  
ABINIT has also some (very limited) interpretor capabilities:

  * It can identify one slash sign (/) being placed between two numbers 
    (without a separating blank) as being the definition of a fraction 
    (e.g. 1/3 will be interpreted as 0.33333333333333d0); 

  * It can identify sqrt(...) or -sqrt(...) as being the definition of a square root, 
    when applied to one valid number - also without a separating blank - 
    (e.g. -sqrt(0.75) will be interpreted as -0.8660254038d0); 

  * Note, however, that these capabilities are NOT recursive. 
    At most, a sqrt identifier can contain an expression that uses a fraction (e.g. sqrt(3/4) is OK), 
    but two fractions (or two sqrt) cannot be used in one expression, and a sqrt cannot be present 
    in the numerator or denominator of a fraction. 

  * A string might be formed by concatenating two strings with the // operator (even blanks before or after // are accepted)

  * Environment variables are accepted, in the form `$VAR` where VAR is the name of the environment variable.
    The end of the name of the environment variable, VAR, is determined by finding the closest separator 
    among a blank, a slash, or a double quote. 
    The parser will automatically substitute the value of the environment variable to the `$VAR` string.
    As an example, "$PSPDIR/PseudosHGH_pwteter" with PSPDIR being Psps_for_tests will give
    "Psps_for_tests/PseudosHGH_pwteter".

Comments should be placed to the right of the comment characters # or !;
anything to the right of a "#" or a "!" on any line is **simply ignored** by the
parser. Additional text, not preceded by a "#" or a "!" would not otherwise
cause trouble unless the text inadvertently contained character strings which
were the same as variable names (e.g. [[acell]]). The characters "#" or "!"
can also be used to "store" old values of variables or place anything else of
convenience into the file in such a way as to be ignored by the parser when
the data is read.  

**Case is irrelevant** as the entire input string is mapped to upper case before
parsing, to remove case sensitivity.  
More than one parameter per line may be given. If a given parameter name is
given more than once in the input file, an error message is printed, and the code stops.

External input files can be included with the syntax:

    include "geometry.inc"

where geometry.inc gives the crystalline structure in the Abinit format:

    cat geometry.in

    # Si in diamond structure
    acell 3*10.25
    rprim   
      0.0 0.5 0.5  
      0.5 0.0 0.5  
      0.5 0.5 0.0
    natom  2
    ntypat 1
    typat  2*1
    xred   0.00  0.00  0.00
           0.25  0.25  0.25
    znucl 14.0

Alternatively, one can use the [[structure]] variable to read the structure from POSCAR files, netcdf files
produced by Abinit or text files with a reduced set of ABINIT variables.


<a id="files-file"></a>
### 3.2 File names in ABINIT

File names in ABINIT are either given automatically by ABINIT, or build from
different input variables, like [[output_file]], [[pseudos]], [[indata_prefix]], [[outdata_prefix]]
or [[tmpdata_prefix]].

**[[output_file]]**

Filename of the main file in which formatted output will be placed (the main
output file). Error messages and other diagnostics will NOT be placed in this
file, but sent to unit 06 (terminal or log file); the unit 06 output can be
ignored unless something goes wrong. The code repeats a lot of information to
both unit 06 and to the main output file. The unit 06 output is intended to be
discarded if the run completes successfully, with the main output file keeping
the record of the run in a nicer looking format.

**[[pseudos]]**

Give filenames of the different pseudopotential input files. The pseudopotential data files
are formatted. There must be as many filenames provided sequentially here as
there are types of atoms in the system, and the order in which the names are
given establishes the identity of the atoms in the unit cell, as listed in [[typat]].

**abi or [[indata_prefix]]**

The other files READ by the code will have a name that is constructed from the
root "abi" or another root defined by [[indata_prefix]]. This apply to optionally read wavefunction, density or potential
files. In the multi-dataset mode, this root will be complemented by **_DS**
and the dataset index. The list of possible input files, with their name
created from the root 'abi', is the following (a similar list exist when
**_DS** and the dataset index are appended to 'abi'):

  * **abi_WFK**
filename of file containing input wavefunction coefficients created from an
earlier run (with [[nqpt]]=0). Will be opened and read if [[irdwfk]] is 1.
The wavefunction file is unformatted and can be very large. **Warning**: in
the multi dataset mode, if getwfk is non-zero, a wavefunction file build from
**abo** will be read.

  * **abi_WFQ**
filename of file containing input wavefunction coefficients created from an
earlier run (with [[nqpt]]=1), as needed for response function calculations.
The wavefunction file is unformatted and can be very large. **Warning**: in
the multi dataset mode, if getwfk is non-zero, a wavefunction file build from
**abo** will be read.

  * **abi_1WFxx**
filename of file containing input first-order wavefunctions created from an
earlier RF run. xx is the index of the perturbation

  * **abi_DEN**
filename of file containing density created from an earlier run. See
explanations related to negative values of [[iscf]]. This file is also
unformatted. **Warning**: in the multi dataset mode, if getwfk is non-zero, a
density file build from **abo** will be read.

  * **abi_HES**
filename of file containing an approximate hessian, for eventual
(re)initialisation of Broyden minimisation. See brdmin.F90 routine. The use of
[[restartxf]] is preferred.

**abo or [[outdata_prefix]]**

Except [[output_file]] and "log", the other files WRITTEN by the code will have a
name that is constructed from the root "abo" or [[outdata_prefix]]. 
This applies to optionally written
wavefunction, density, potential, or density of states files. In the multi-
dataset mode, this root will be complemented by **_DS** and the dataset
index. Also in the multi-dataset mode, the root "abo" can be used to build the
name of **input** files, thanks to the 'get' variables. The list of possible
output files, with their name created from the root 'abo' is the following (a
similar list exists when **_DS** and the dataset index are appended to 'abo'):

  * **abo_WFK**
Filename of file containing output wavefunction coefficients, if [[nqpt]]=0.
The wavefunction file is unformatted and can be very large.

  * **abo_WFQ**
Same as **abo_WFK**, but for the case [[nqpt]]=1. The wavefunctions are
always output, either with the name **abo_WFK**, or with the name
**abo_WFQ**.

  * **abo_1WFxx**
Same as **abo_WFK**, but for first-order wavefunctions, xx is the index of
the perturbation, see the section [[help:respfn#6.3|section 6.3]] of the [[help:respfn]].

  * **abo_DDB**
The derivative database, produced by a response-function dataset, see [[help:respfn#ddb|this section]]
of the respfn help file.

  * **abo_DEN**
filename of file containing density, in the case [[ionmov]]=0. See the keyword
[[prtden]]. This file is unformatted, but can be read by cut3d.

  * **abo_TIMx_DEN**
filenames of files containing density, in the case [[ionmov]]/=0. The value of
"x" after " **TIM** " is described hereafter. See the keyword [[prtden]]. This
file is unformatted, but can be read by cut3d.

  * **abo_POT**
filename of file containing Kohn-Sham potential See the keyword [[prtpot]].
This file is unformatted, but can be read by cut3d.

  * **abo_TIMx_POT**
filenames of files containing Kohn-Sham potential in the case [[ionmov]]/=0.
The value of "x" after "TIM" is described hereafter. See the keyword
[[prtpot]]. This file is unformatted, but can be read by cut3d.

  * **abo_DOS**
filename of file containing density of states. See the keyword [[prtdos]].
This file is formatted.

  * **abo_TIMx_DOS**
filenames of files containing the density of states in the case [[prtdos]]=2
and [[ionmov]]=1 or 2. The value of "x" after "TIM" is described hereafter.
See also the keyword [[prtdos]]. This file is formatted.

  * **abo_GEO**
filename of file containing the geometrical analysis (bond lengths and bond
angles) in the case [[ionmov]]=0. See the keyword [[prtgeo]]. This file is
formatted.

  * **abo_TIMx_GEO**
filenames of files containing the geometrical analysis (bond lengths and bond
angles) in the case [[ionmov]]=1 or 2. The value of "x" after "TIM" is
described hereafter. See also the keyword [[prtgeo]]. This file is formatted.

  * **abo_KSS**
filename of file containing output wavefunction coefficients, if
[[nbandkss]]/=0. This wavefunction file is unformatted and can be very large.
Its purpose is to start a GW calculation using M.Torrent's code. A different
format than for **abo_WFK** is used, see the file
~abinit/doc/developers/format_KSS.txt.

  * **abo_EIG**
A file containing the electronic eigenvalues, for subsequent plotting of band
structure.

When [[ionmov]]/=0, the **POT**, **DEN**, or **GEO** files are output each
time that a SCF cycle is finished. The " **x** " of **TIMx** aims at giving
each of these files a different name. It is attributed as follows:
\- case ionmov==1: there is an initialization phase, that takes 4 calls to
the SCF calculation. The value of x will be A, B, C, and D. Then, x will be 1,
2, 3 ..., actually in agreement with the value of itime (see the keyword
[[ntime]])
\- other ionmov cases: the initialisation phase take only one SCF call. The
value of x will be 0 for that call. Then, the value of x is 1, 2, 3... in
agreement with the value of itime (see the keyword [[ntime]])

**tmp or [[tmpdata_prefix]]**

The temporary files created by the codes will have a name that is constructed
from the root " **tmp** " or [[tmpdata_prefix]]. tmp should usually be chosen such as to give access
to a disk of the machine that is running the job, not a remote (NFS) disk.
Under Unix, the name might be something like `/tmp/user_name/temp`. As an
example, **tmp_STATUS**
gives the status of advancement of the calculation, and is updated very
frequently


<a id="parameters"></a>
### 3.3 More about ABINIT input variables
  
In each section of the ABINIT input variables files, a generic information on
the input variable is given: a **mnemonics**, possibly some
**characteristics**, the **variable type** (integer, real, string), and the
**default value**. Then, follows the description of the variable.

The **characteristics** can be one of the following: 

- **DEVELOP**,
- **NO_MULTI**
- **INTERNAL_ONLY**
- **INPUT_ONLY**
- **EVOLVING**
- **ENERGY**
- **LENGTH**
- **MAGNETIC FIELD**

#### **Physical information**

The **ENERGY**, **LENGTH** and **MAGNETIC FIELD** characteristics indicate
that the physical meaning of the variable is known by ABINIT, so that ABINIT
can treat its physical dimensions with some units.

The use of the atomic unit system (e.g. the Hartree for energy, about 27.211
eV, and the Bohr for lengths about 0.529 Angstroms) is strictly enforced
within the code. However, the dimension of some input variables can be
specified and read correctly. 

At present, this applies to three types of
variables: those that have the dimension of an energy, those that have a
dimension of length, and those that have a dimension of magnetic field. The
first class of variables have the characteristics **ENERGY**, and can be
specified in atomic units (Hartree), or electron-volts, or Rydbergs, or even Kelvin. 

The second class of variables have the characteristics **LENGTH**,
and can be specified in atomic units (Bohr), nm (nanometer) and angstrom. 
The third class of
variables have the characteristics **MAGNETIC FIELD**, and can be
specified in atomic units and Tesla. The abinit parser recognize a dimension
if it is specified after the list of numbers following the input variable
keyword, in the input file. The specification can be upper or lower case, or a
mix thereof. Here is the list of recognized chains of characters:

  * Ry or Rydberg or Rydbergs --> Rydberg (for energies) 
  * eV --> electron-volts (for energies) 
  * K  --> Kelvin (for energies) 
  * Angstr --> Angstrom (for lengths) 
  * nm --> nanometer (for lengths) 
  * T or Tesla --> Tesla (for magnetic fields) 
  * S or Sec or Second --> second (for time) 

Other character chains, like "au" (for atomic units) or "Hartree", or "Bohr" are not recognized, 
but make the parser choose (by default) atomic units, which is the correct behaviour. Example:
    
        acell 8 8 8 angstrom
        ecut 8 Ry
        tsmear 1000 K

or
    
         acell 3*10 Bohr  ecut 270 eV  tsmear 0.01

The use of the atomic units is **mandatory** for other dimensioned input
variables, like the tolerance on forces ([[toldff]]), parameters that define
an 'object' ([[objaax]], [[objaax]], [[objbax]], [[objatr]], [[objbtr]]), and
the initial velocity of atoms ([[vel]] if needed).

The initial atomic positions can be input in Bohr or Angstrom through
[[xcart]] (possibly use the Angstrom unit), or even in reduced coordinates, through [[xred]].

#### **Flow information**

Most of the variables can be used in the multi-dataset mode (see section [3.3](#multidatasets)),
but those that must have a unique value throughout all the datasets are
signaled with the indication **NO_MULTI**.

Some of the input variables, with characteristics **INPUT_ONLY** are only
used by the parser, to initialize other input variables, but are not
transmitted inside the code, beyond the parser. In particular, they are not
echoed in the output file.

At variance, some internal variables, with characteristics **INTERNAL_ONLY** 
are documented in the help files, but are not accessible as input variables.
The documentation is provided because such variables are sometimes mentioned
in the output file.

Most of the input variables do not change while a run is performed. Some of
them, by contrast, may evolve, like the atomic positions, the atomic
velocities, the cell shape, and the occupation numbers. Their echo, after the
run has proceeded, will of course differ from their input value. They are
signaled by the indication **EVOLVING**.

#### **Other information**

**DEVELOP** refers to input variables that are not used in production
runs, but have been introduced during development time, of a feature that is
likely not finalized. For non ABINIT developers, it is strongly advised to skip them.

In addition to giving the input variables, the input file can be useful for
another purpose: placing the word " **exit** " on the top line will cause the
job to end smoothly on the very next iteration, if the [[chkexit]] input
variable is non-zero. This functions because the program closes and reopens
the input file on every iteration and checks the top line for the keyword
"exit". THE WORD MUST BE PLACED WITH SPACES (BLANKS) ON BOTH SIDES. Thus
placing exit on the top line of the input file WHILE THE JOB IS ALREADY
RUNNING will force the job to end smoothly on the very next iteration. On some
machines, this does not work always (we do not know why...). Another
possibility is offered: one can create a file named "abinit.exit" in the
directory where the job was started. The code should also smoothly end. In
both cases, the stop is not immediate. It can take a significant fraction
(about 20% at most) of one SCF step to execute properly the instruction still needed.



<a id="multidatasets"></a>
### 3.4 The multi-dataset mode
  
Until now, we have assumed that the user wants to make computations
corresponding to one set of data: for example, determination of the total
energy for some geometry, with some set of plane waves and some set of k-points.

It is often needed to redo the calculations for different values of some
parameter, letting all the other things equal. As typical examples, we have
convergence studies needed to determine which cut-off energy gives the needed
accuracy. In other cases, one makes chains of calculations in order to compute
the band structure: first a self-consistent calculation of the density and
potential, then the eigenenergy computation along different lines.

For that purpose, the **multi-dataset mode** has been implemented.

It allows the code to treat, in one run, different sets of data, and to chain
them. The number of datasets to be treated is specified by the variable
[[ndtset]], while the indices of the datasets (by default 1, 2, 3, and so on)
can be eventually provided by the array [[jdtset]].

For each dataset to be treated, characterized by some index, each input
variable will determined by the following **rules** (actually, it is easier to
understand when one looks at examples, see below):

  * (1) ABINIT looks whether the variable name (e.g. [[ecut]] ), appended with the index 
    of the dataset (e.g. [[jdtset]]=2), exists (e.g. "ecut2" ). It will take the data that follows this keyword, if it exists.

  * (2) If this modified variable name does not exist, it will look whether a metacharacter, 
    a series or a double-loop data set has been defined, see sections [3.4](#series) or [3.5](#loop).

  * (3) If the variable name appended with the index of the dataset does not exist, and 
    if there is no series nor double-loop dataset for this keyword, it looks for an occurrence 
    of the variable name without any index appended, and take the corresponding data. (This corresponds to the single dataset mode)

  * (4) If such occurrences do not exist, it takes the default value. (Also, similar to the single dataset mode)
    
        ---------------
    
        1st example.
    
        ndtset   2
        acell   8 8 8
        ecut1  10
        ecut2  15

means that there are 2 datasets: a first in which
    
         acell 8 8 8  ecut 10 

has to be used, and a second in which
    
         acell 8 8 8  ecut 15

has to be used.
    
        ------------------
    
        2nd example
    
        ndtset 2     jdtset 4 5
    
        acell   8 8 8
        acell5 10 10 10
        ecut1  10
        ecut2  15
        ecut3  20
        ecut4  25
        ecut5  30
    
this means that there are still two datasets, but now characterized by the
indices 4 and 5, so that the first run will use the generic "acell", and "ecut4":
    
         acell 8 8 8 ecut 25

and the second run will use "acell5" and "ecut5":
    
         acell 10 10 10 ecut 30 

Note that ecut1, ecut2 and ecut3 are not used.



<a id="series"></a>
### 3.5 Defining a series
  
Rule (2) is split in three parts: (2a), (2b) and (2c). Series relate with (2b):

(2b) If the variable name appended with the index of the dataset does not
exist, the code looks whether a series has been defined for this keyword.

There are two kinds of series:

  * arithmetic series (constant _increment_ between terms of the series) 

  * geometric series (constant _ratio_ between terms of the series) 

The first term of the series is defined by the keyword appended with a colon
(e.g. **ecut:** ), while the increment of an arithmetic series is defined by
the keyword appended with a plus (e.g. **ecut+** ), and the factor of a
geometric series is defined by the keyword appended with a times (e.g. **ecut*** ).

If the index of the dataset is 1, the first term of the series is used, while
for index N, the appropriate input data is obtained by considering the Nth
term of the series.
    
     ------------------
    
     3rd example
    
       ndtset 6
       ecut1 10
       ecut2 15
       ecut3 20
       ecut4 25
       ecut5 30
       ecut6 35

is equivalent to
    
        ndtset 6 ecut: 10 ecut+ 5

In both cases, there are six datasets, with increasing values of [[ecut]].



<a id="loop"></a>
### 3.6 Defining a double loop dataset
  
To define a double loop dataset, one has first to define the upper limit of
two loop counters, thanks to the variable [[udtset]]. The inner loop will
execute from 1 to [[udtset]](2), and the outer loop will execute from 1 to
[[udtset]](1). Note that the largest value for [[udtset]](1) is presently 999,
while it is 9 for [[udtset]](2) (so, only the last digit for the inner loop).

The value of [[ndtset]] must be coherent with [[udtset]] (it must equal the
product `udtset(1) * udtset(2)`).

A dataset index is created by the concatenation of the outer loop index and
the inner loop index.  
For example, if [[udtset]](1) is 2 and [[udtset]](2) is 4, the index will
assume the following values: `11, 12, 13, 14, 21, 22, 23, and 24`.

Independently of the use of [[udtset]], rules (2a) and (2c) will be used to
define the value of an input variable:

(2a) The question mark " **?** " can be used as a metacharacter, replacing any
digit from 1 to 9, to define an index of a dataset.  
For example, **ecut?** 1 means that the input value that follows it can be
used for [[ecut]] for the datasets `01, 11, 21, 31, 41, 51, 61, 71, 81, and 91`.

(2c) If the variable name appended with the index of the dataset does not
exist, the code looks whether a double-loop series has been defined for this
keyword. Series can be defined for the inner loop index or the outer loop
index. Two signs will be appended to the variable name (instead of one in the
simple series case). One of these signs must be a question mark " **?** ",
again used as a metacharacter able to assume the values 1 to 9.  
If it is found in the first of the two positions, it means that the series
does not care about the outer loop index (so the values generated are equal
for all outer loop index values). If it is found in the second of the two
positions, the series does not care about the inner loop index. The other sign
can be a colon, a plus or a times, as in the case of the series defined in
(2a), with the same meaning.

Rule (1) has precedence over them, they have precedence over rules (3) or (4),
rule (2a) has precedence over rules (2b) or (2c) and the two latter cannot be
used simultaneously for the same variable.
    
        ------------------
    
        4th example
        ndtset 6    udtset 2 3
        acell1?  10 10 10
        acell2?  15 15 15
        ecut?: 5    ecut?+ 1
     
is equivalent to
    
        ndtset 6     jdtset 11 12 13  21 22 23
        acell11  10 10 10     ecut11 5
        acell12  10 10 10     ecut12 6
        acell13  10 10 10     ecut13 7
        acell21  15 15 15     ecut21 5
        acell22  15 15 15     ecut22 6
        acell23  15 15 15     ecut23 7
     

!!! tip

    More examples can be found in the directory ~abinit/tests/v1, cases 59 and later.



<a id="filenames-multidataset"></a>
### 3.7 File names in the multi-dataset mode
  
The root names for input and output files (potential, density, wavefunctions
and so on) will receive an appendix: **_DS** followed by the index of the
dataset. See section 4.

The  **get** variables can be used to chain the calculations.

Let us mention a few of them: [[getwfk]], [[getwfq]], [[getddk]], [[get1wf]],
[[getden]], [[getcell]], [[getxred]] and [[getxcart]].

  * [[getwfk]] allows to take the output wavefunctions of a previous dataset and use them as input wavefunctions 
  * [[getwfq]], [[getddk]] and [[get1wf]] do similar things for response function calculations 
  * [[getden]] does the same for the density; [[getcell]] does the same for [[acell]] and [[rprim]] 
  * [[getxred]] and [[getxcart]] do the same for the atomic positions, either in reduced coordinates, or in cartesian coordinates. 

The different variables corresponding to each dataset are echoed using the
same indexing convention as for the input step. For the last echo of the code
variables, some output variables are also summarized, using the same conventions:

  * **etotal** (total energy) 
  * **fcart** (cartesian forces) 
  * **strten** (the stress tensor). 



<a id="pseudopotential-files"></a>
## 4 The pseudopotential files and PAW atomic data files
  
Actually, no real understanding of these files is needed to run the code. The
recommended pseudopotentials can be downloaded from the ABINIT Web site at
[[https://www.abinit.org/psp-tables]]. Documentation is
provided there as well as in the dedicated [[topic:PseudosPAW]]. Note that it
is not possible to mix norm-conserving pseudopotentials and PAW atomic data
sets in the same run. Also, every such file has been generated for a
particular choice of the exchange-correlation functional [[ixc]]. It is in
principle incorrect to use a pseudopotential (or PAW data) with another
exchange-correlation functional than the one it has been generated for, but
ABINIT will only send a warning.

For different other reasons, it might nevertheless be useful to be able to
grasp some information from the file. For norm-conserving pseudopotentials
different format are possible (labelled 1 to 8 presently). The associated
internal variable is called pspcod. Information on the header of these
pseudopotential files can be found in the abinit wiki at
[[https://wiki.abinit.org/doku.php?id=developers:pseudos]], that you should
read now (do not pursue with the description of each format, though).



## 5 The different output files
  
Explanation of the output from the code

Output from the code goes to several places listed below.

<a id="logfile"></a>
### 5.1 The log file
  
The "log" file (this is the standard UNIX output file, and corresponds to
Fortran unit number 06): a file which echoes the values of the input
parameters and describes various steps of the calculation, typically in much
more detail than is desired as a permanent record of the run. This log file is
intended to be informative in case of an error or for a fuller description of
the run. For a successful run the user will generally delete the log file
afterwards. There are four types of exception messages: **ERROR**, **BUG**,
**WARNING** and **COMMENT** messages.

**ERROR** and **BUG** messages cause the code to stop, immediately, or after a
very small delay. An **ERROR** is attributed to the user, while a **BUG** is
attributed to the developer.

A **WARNING** message indicates that something happened that is not as
expected, but this something is not so important as to make the code stop. A
**COMMENT** message gives some information to the user, concerning something
unusual. None of them should appear when the run is completely normal.

After a run is completed, always have a look at the end of the log file, to
see whether an **ERROR** or a **BUG** occurred.  

Also, the code gives the number of **WARNING** or **COMMENT** it issued. It is
advised to read at least the **WARNING** messages, during the first month of
ABINIT use.

<a id="outputfile"></a>
### 5.2 The main output file
  
The **main output file** is a formatted output file to be kept as the permanent record of the run.

Note that it is expected **not** to exist at the beginning of the run:  
If a file with the same name as the one specified by the input variable [[output_file]] 
(see also the default output file name description) already exists, the code
will generate, from the given one, another name, appended with **.A**. If
this new name already exists, it will try to append **.B**, and so on, until **.Z**.  
Then, the code stops, and asks you to clean the directory.

The **main output file** starts with a heading:

  * version number and specified platform 
  * copyright notice and distribution licence 
  * date 
  * echo of "files" file (except pseudopotential name) 

Then, for each dataset, it reports the point symmetry group and Bravais
lattice, and the expected memory needs. It echoes the input data, and report
on checks of data consistency for each dataset.

<a id="main-output-file"></a>
### 5.3 More on the main output file
  
Then, for each dataset, the real computation is done, and the code will report
on some initialisations, the SCF convergence, and the final analysis of
results for this dataset. Each of these phases is now described in more
details.

The code reports:

  * The real and reciprocal space translation vectors ( _Note_: the definition of the reciprocal vector is such that $R_i.G_j= \delta_{ij}$).
  * The volume of the unit cell.
  * The ratio between linear dimension of the FFT box and the sphere of plane waves, called boxcut.
    It must be above 2 for exact treatment of convolutions by FFT. 
    [[ngfft]] has been automatically chosen to give a boxcut value larger than 2, but not much
    larger, since more CPU time is needed for larger FFT grids.
  * The code also mention that for the same FFT grid you might treat (slightly) larger [[ecut]] 
    (so, with a rather small increase of CPU time). 
  * The heading for each pseudopotential which has been input.
  * From the inwffil subroutine, a description of the wavefunction initialization 
    (random number initialization or input from a disk file), that is, a report 
    of the number of planewaves (npw) in the basis at each k point
  * From the setup2 subroutine, the average number of planewaves over all k points is reported in two forms, 
  arithmetic average and geometric average. 

Until here, the output of a ground-state computation is identical to the one
of a response-function calculation. See the [[help:respfn]] for the latter,
especially [[help:respfn#6.2|section 6.2]].

Next the code reports information for each SCF iteration:

  * The iteration number.
  * The (pseudo) total energy (Etot) in Hartree [This is not the total energy of the system, 
    since the pseudopotential approximation has been made: a constant energy (in the frozen-core approximation) 
    should be added to the present pseudo total energy in order to obtain a total energy, 
    that includes the contributions from the core electrons. Since only differences of total energy matter 
    (except is extremely rare cases), one can work with this pseudo energy like if it were the true total energy, 
    except that the missing constant depends on the pseudopotential that has been used. 
    Thus one has to perform differences of pseudo energies between simulations that use the same pseudopotentials]. 
  * The change in Etot since last iteration (deltaE).
  * The maximum squared residual residm over all bands and k points 
    (residm - the residual measures the quality of the wavefunction convergence) 
  * The squared residual of the potential in the SCF procedure (vres2).
  * The maximum change in the gradients of Etot with respect to fractional coordinates (diffor, in Hartree).
  * The rms value of the gradients of Etot with respect to fractional coordinates (maxfor, in Hartree).  
    The latter two are directly related to forces on each atom.
  * Then comes an assessment of the SCF convergence: the criterion for fulfillment of the SCF criterion 
    (defined by [[toldfe]], [[toldff]], [[tolwfr]] or [[tolvrs]]) might be satisfied or not... 
  * Then the stresses are reported. 

This ends the content of a fixed atomic position calculation.

Many such blocks can follow.

When the atomic positions have been eventually relaxed, according to the value
of [[ntime]], the code output more information:

  * The squared residuals for each band are reported, k point by k point. 
  * Then the fractional or reduced coordinates are given, 
  * followed by the energy gradients, 
  * followed by the cartesian coordinates in Angstroms, 
  * followed by the cartesian forces in Hartree/Bohr and eV/Angstrom. 
  * Also are given the rms force ( **frms** ) and the maximum absolute value of any force component ( **max** ). 
  * Next are the length scales of the unit cell in Bohr and in Angstroms. 
  * Next are the eigenvalues of each band for each k point, in eV or Hartree or both depending on the choice of [[enunit]].   

<a id="averagepot"></a>
NOTE that the average electrostatic potential of a periodically repeated cell is UNDEFINED.  
In the present implementation, the average Hartree potential and local
potential are imposed to be zero, but not the average exchange-correlation
potential. This definition gives some meaning to the absolute values of
eigenenergies, thanks to Janak's theorem: they are derivatives of the total
energy with respect to occupation number. Indeed, the G=0 contributions of the
Hartree, local potential and ion-ion to the total energy is independent of the
occupation number in the present implementation. With this noticeable
exception, one should always work with **differences** in eigenenergies, as
well as **differences** between eigenenergies and the potential. For example,
the absolute eigenenergies of a bulk cell should not be used to try to predict
a work function. The latter quantity should be obtained in a supercell
geometry, by comparing the Fermi energy in a slab and the potential in the
vacuum in the same supercell.

  * Next are the minimum and maximum values for charge density, and next smaller or larger values (in order to see degeneracies). 

  * Next are the total energy (Ha and eV) and its components: 
    * kinetic, 
    * Hartree, 
    * exchange and correlation (xc), 
    * Ewald (ion-ion energy), 
    * *core correction* to the local pseudopotential, 
    * local pseudopotential, and 
    * nonlocal pseudopotential.  The sum of the Kohn-Sham energies (termed "band energy") is also given. 

  * Next is the stress tensor, (1/ucvol) d(Etot)/d(strain(a,b))
    for Etot=total energy per unit cell and **a**, **b** are **x**, **y**, or **z** components of strain.
    The stress tensor is given in cartesian coordinates in Hartree/Bohr 3 and GPa.
    The basics of the stress tensor are described in [[cite:Nielsen1985]] and [[cite:Nielsen1985a]]. 

Having finished all the calculations for the different datasets, the code
echoes the parameters listed in the input file, using the latest values e.g.
for [[xred]], [[vel]], and [[xcart]], and supplement them with the values
obtained for the total energy, the forces and stresses, as well as occupation numbers.
The latter echoes are very convenient for a quick look at the result of calculation!

This is followed finally by the timing output: both "cpu" time and "wall
clock" time as provided by calls within the code.
The total cpu and wall clock times are reported first, in seconds, minutes,
and hours for convenient checking at a glance.
Next are the cpu and wall times for the principal time-consuming subroutine
calls, each of which is independent of the others. The sum of these times
usually accounts for about 90% of the run time.

The main subroutines, for BIG jobs, are

1. fourwf: the subroutine which performs the fast Fourier transform for the wavefunctions: 
2. fourdp: the subroutine which performs the fast Fourier transform related to density and potential 
3. rhohxc: computes the Hartree and exchange-correlation energy and potential and sometimes 
   derivative of potential; only the XC timing is reported, excluding time connected to the FFTs: `xc:pot/=fourdp.`
4. nonlop: computes the matrix elements of the nonlocal pseudopotential: $\langle G|V_{non-local}|C \rangle$
5. projbd: Gram-Schmidt orthogonalisation 

In case of small jobs, other (initialisation) routines may take a larger
share, and the sum of the times for the principal time-consuming subroutine
calls will not make 90% of the run time.

If the long printing option has been selected ([[prtvol]]=1), the code gives
much more information in the whole output file. These should be rather 
self-explanatory, usually. Some need more explanation.  
In particular the cpu and wall times for major subroutines which are NOT
independent of each other; for example vtorho conducts the loop over k points
and calls practically everything else. In case of a ground state calculation,
at fixed atomic positions, these subroutines are:

1. **abinit**: the main routine 
2. **driver**: select ground state or response calculations 
3. **gstate**: the driver of the ground state calculations 
4. **scfcv_core**: the SCF cycle driver 
5. **vtorho**: compute the density from the potential (it includes a loop over spins and k-points) 
6. **vtowfk**: compute the wavefunctions at one particular k-point (includes a non self consistent loop, and a loop over bands) 
7. **cgwf**: optimize one wavefunction in a fixed potential 
8. **getghc**: computes $\langle G|H|C\rangle$, that is, applies the Hamiltonian operator to an input vector. 

<a id="header"></a>
### 5.4 The header
  
The **wavefunction files**, **density files**, and **potential files** are not plain text files (as they are quite big,
and need some kind of compressed storage mode), but either binary files (from FORTRAN write statements), or netcdf files. 
See the input variable [[iomode]].
FORTRAN binary format is the default, and will be the focus of the following sections. 
Reading a netcdf file is actually much easier than reading a FORTRAN binary file, as netcdf
files can be addressed by content.

In order to provide the relevant metadata about the run that generated such numerical data,
the **wavefunction files**, **density files**, and **potential files** (irrespective of the format) 
contain standardized basic information. In case of FORTRAN,  
such files begin with the same records, called the "header".
This header is treated using the hdr_type Fortran data structure inside ABINIT. 
There are dedicated routines inside ABINIT for initializing a header, updating it,
reading the header of an unformatted disk file, writing a header to an unformatted disk file,
echoing a header to a formatted disk file, cleaning a header data structure.

The header is made of 4 + [[ntypat]] unformatted records, obtained by the
following Fortran90 instructions (format 9.0):

```fortran
     write(unit=header) codvsn,headform,fform

     write(unit=header) bantot,date,intxc,ixc,natom,ngfft(1:3),&
       nkpt,nspden,nspinor,nsppol,nsym,npsp,ntypat,occopt,pertcase,usepaw,&
       ecut,ecutdg,ecutsm,ecut_eff,qptn(1:3),rprimd(1:3,1:3),stmbias,&
       tphysel,tsmear,usewvl,nshiftk_orig,nshiftk,mband

     write(unit=header) istwfk(1:nkpt),nband(1:nkpt*nsppol),&
       npwarr(1:nkpt),so_psp(1:npsp),symafm(1:nsym),symrel(1:3,1:3,1:nsym),&
       typat(1:natom),kpt(1:3,1:nkpt),occ(1:bantot),tnons(1:3,1:nsym),&
       znucltypat(1:ntypat),wtk(1:nkpt)

     write(unit=unit) residm,xred(1:3,1:natom),etotal,fermie,amu(1:ntypat)

     write(unit=unit) kptopt,pawcpxocc,nelect,cellcharge,icoulomb,&
       kptrlatt(3,3),kptrlatt_orig(3,3),shiftk_orig(3),shiftk(3)

     do ipsp=1,npsp
    ! (npsp lines, 1 for each pseudo; npsp=ntypat, except if alchemical pseudo-atoms)
      write(unit=unit) title,znuclpsp,zionpsp,pspso,&
       pspdat,pspcod,pspxc,lmn_size,md5_pseudos
     enddo

    !(in case of usepaw==1, there are some additional records)
     if (usepaw==1)then
      write(unit=unit)( pawrhoij(iatom)%nrhoijsel(1:nspden),iatom=1,natom), cplex, nspden
      write(unit=unit)((pawrhoij(iatom)%rhoijselect(1:      nrhoijsel(ispden),ispden),ispden=1,nspden),iatom=1,natom),&
                      ((pawrhoij(iatom)%rhoijp     (1:cplex*nrhoijsel(ispden),ispden),ispden=1,nspden),iatom=1,natom)
     endif
```

where the type of the different variables is:
    
```fortran
character*8 :: codvsn
integer :: headform,fform
integer :: bantot,cplex,date,icoulomb,intxc,ixc,kptopt,mband,natom,ngfft(3),&
  nkpt,npsp,nshiftk_orig,nshiftk,nspden,nspinor,nsppol,nsym,ntypat,occopt,&
  pawcpxocc,pertcase,usepaw,usewvl
double precision :: acell(3),cellcharge,ecut,ecutdg,ecutsm,ecut_eff,etotal,&
  fermie,nelect,qptn(3),residm,rprimd(3,3),shiftk(3),shiftk_orig(3),&
  stmbias,tphysel,tsmear
integer :: istwfk(nkpt),kptrlatt(3,3),kptrlatt_orig(3,3),nband(nkpt*nsppol),&
  npwarr(nkpt),so_psp(npsp),symafm(nsym),symrel(3,3,nsym),typat(natom),&
  nrhoijsel(nspden),rhoijselect(*,nspden)
double precision :: amu(ntypat),kpt(3,nkpt),occ(bantot),tnons(3,nsym),&
  znucltypat(ntypat),wtk(nkpt),xred(3,natom)
character*132 :: title
character*32 :: md5_pseudos
double precision :: znuclpsp,zionpsp
integer :: pspso,pspdat,pspcod,pspxc,lmn_size
double precision :: rhoij(*,nspden)
```

NOTE: _etotal is set to its true value only for density and potential files.
For other files, it is set to 1.0d20_  
NOTE: _ecut_eff= [[ecut]]*([[dilatmx]])$^2$_  
NOTE: _For all cases where occupation numbers are defined (that is, positive
iscf, and iscf=-3), and for non-metallic occupation numbers, the Fermi energy
is set to the highest occupied eigenenergy. This might not correspond to the
expected Fermi energy for a later non-self-consistent calculation (e.g. the band structure)_

The header might differ for different versions of ABINIT. One pre-v5.3 format
is described below. Note however, that the current version of ABINIT should be
able to read all the previous formats (not to write them), with the exception
of wavefunction files for which the [[ecutsm]] value was non-zero (there has
been a change of definition of the smearing function in v4.4).

The format for ABINIT versions 8.0 to 8.11 was:

```fortran
     write(unit=header) codvsn,headform,fform
     write(unit=header) bantot,date,intxc,ixc,natom,ngfft(1:3),&
    & nkpt,nspden,nspinor,nsppol,nsym,npsp,ntypat,occopt,pertcase,usepaw,&
    & ecut,ecutdg,ecutsm,ecut_eff,qptn(1:3),rprimd(1:3,1:3),stmbias,tphysel,tsmear,usewvl

     write(unit=header) istwfk(1:nkpt),nband(1:nkpt*nsppol),&
    & npwarr(1:nkpt),so_psp(1:npsp),symafm(1:nsym),symrel(1:3,1:3,1:nsym),typat(1:natom),&
    & kpt(1:3,1:nkpt),occ(1:bantot),tnons(1:3,1:nsym),znucltypat(1:ntypat),wtk(1:nkpt)
     do ipsp=1,npsp
    ! (npsp lines, 1 for each pseudopotential; npsp=ntypat, except if alchemical pseudo-atoms)
      write(unit=unit) title,znuclpsp,zionpsp,pspso,pspdat,pspcod,pspxc,lmn_size
     enddo
    !(in case of usepaw==0, final record: residm, coordinates, total energy, Fermi energy)
     write(unit=unit) residm,xred(1:3,1:natom),etotal,fermie
    !(in case of usepaw==1, there are some additional records)
     if (usepaw==1)then
      write(unit=unit)( pawrhoij(iatom)%nrhoijsel(1:nspden),iatom=1,natom), cplex, nspden
      write(unit=unit)((pawrhoij(iatom)%rhoijselect(1:      nrhoijsel(ispden),ispden),ispden=1,nspden),iatom=1,natom),&
    &                 ((pawrhoij(iatom)%rhoijp     (1:cplex*nrhoijsel(ispden),ispden),ispden=1,nspden),iatom=1,natom)
     endif
```

where the type of the different variables was:

```fortran
    character*6 :: codvsn
    integer :: headform,fform
    integer :: bantot,date,intxc,ixc,natom,ngfft(3),nkpt,npsp,
     nspden,nspinor,nsppol,nsym,ntypat,occopt,pertcase,usepaw
     integer :: usewvl, cplex, nspden
    double precision :: acell(3),ecut,ecutdg,ecutsm,ecut_eff,qptn(3),rprimd(3,3),stmbias,tphysel,tsmear
    integer :: istwfk(nkpt),nband(nkpt*nsppol),npwarr(nkpt),so_psp(npsp),&
    & symafm(nsym),symrel(3,3,nsym),typat(natom),nrhoijsel(nspden),rhoijselect(*,nspden)
    double precision :: kpt(3,nkpt),occ(bantot),tnons(3,nsym),znucltypat(ntypat),wtk(nkpt)
    character*132 :: title
    double precision :: znuclpsp,zionpsp
    integer :: pspso,pspdat,pspcod,pspxc,lmax,lloc,mmax=integers
    double precision :: residm,xred(3,natom),etotal,fermie,rhoij(*,nspden)
```
   
<a id="denfile"></a>
### 5.5 The density output file
  
This is an unformatted data file containing the electron density on the real
space FFT grid. It consists of the [header records](#header) followed by

```fortran
    do ispden=1,nspden
     write(unit) (rhor(ir),ir=1,cplex*ngfft(1)*ngfft(2)*ngfft(3))
    enddo
```

where **rhor** is the electron density in electrons/Bohr^3, and **cplex** is the
number of complex components of the density (**cplex**=1 for GS calculations -the
density is real-, and **cplex**=1 or 2 for RF). The input variable [[nspden]]
describes the number of components of the density. The first component (the
only one present when [[nspden]]=1) is always the total charge density. When
[[nspden]]=2, the second component is the density associated with spin-up
electrons. When [[nspden]]=4, the second, third and fourth components
correspond to the x, y and z projections of the local magnetization, in units
of $\hbar/2$. Note that the meaning of the different components of the density
differs for the density array (rhor) and for the different potential arrays
(vxc...), see the [next section](#localpotfile).

To identify the points in real space which correspond with the index "ir"
above, consider the following.  
The first array value (ir=1) corresponds with the first grid point which is at
the origin of the unit cell, (x=0, y=0, z=0).  
The next grid point (ir=2) lies along the first primitive translation at the
next fft grid point, which is (1/[[ngfft]](1))*[[acell]](1)*[[rprim]](mu,1).
This is 1/[[ngfft]](1) of the way along the first primitive translation.  
The rest of the values up to ir=[[ngfft]](1) lie along this vector, at
(ir-1)/[[ngfft]](1) of the way along the first primitive translation. The
point at ir=[[ngfft]](1)+1 lies at 1/[[ngfft]](2) along the second primitive translation.  
The next points up to ir=[[ngfft]](1)+[[ngfft]](1) are displaced in the
direction of the second primitive translation by 1/[[ngfft]](2) and in the
first translation by (ir-[[ngfft]](1)-1)/[[ngfft]](1).  
This pattern continues until ir=[[ngfft]](1)*[[ngfft]](2).  
The next point after that is displaced along the third primitive translation
by 1/ngfft(3), and so forth until ir varies all the way from 1 to
[[ngfft]](1)*[[ngfft]](2)*[[ngfft]](3). This last point is in the corner
diagonally opposite from the origin, or right alongside the origin if the
whole grid is viewed as being periodically repeated.

<a id="localpotfile"></a>
### 5.6 The potential files
  
Also unformatted files consisting of the [header records](#header) and
    
```fortran
    do ispden=1,nspden
     write(unit) (potential(ir),ir=1,cplex*ngfft(1)*ngfft(2)*ngfft(3))
    enddo
```

where **potential** can be either the sum of the Hartree potential, exchange-
correlation and local pseudopotential (see [[prtpot]]), the Hartree potential
(see [[prtvha]]), the Hartree+XC potential (see [[prtvhxc]]), the local
pseudopotential (see [[prtvpsp]]) or the XC potential (see [[prtvxc]]), These
are defined on the real space grid in Hartree energy units. The underlying
grid is as described above. If [[nspden]]=2, the different components are the
spin-up potential and the spin-down potential. In the case [[nspden]]=4, the
components correspond to the up-up potential, the down-down potential, the
real part of the up-down potential, and the imaginary part of the up-down
potential. Note that the Hartree potential is NOT spin-dependent, but in order
to use the same format as for the other potential files, the spin-independent
array is written twice, once for spin-up and one for spin-down.

<a id="wfkfile"></a>
###5.7 The wavefunction output file

This is an unformatted data file containing the planewaves coefficients of all
the wavefunctions, and different supplementary data.

The **ground-state** wf file consists of the [header records](#header), and data written
with the following lines of FORTRAN:

```fortran    
          bantot=0                                    <-- counts over all bands
          index=0                                     <-- index for the wavefunction location
          do isppol=1,nsppol
           do ikpt=1,nkpt
            write(unit) npw,nspinor,nband                    <-- for each k point
            write(unit) kg(1:3,1:npw)                        <-- plane wave reduced coordinates
            write(unit) eigen(1+bantot:nband+bantot),        <-- eigenvalues for this k point
                        occ(1+bantot:nband+bantot)           <-- occupation numbers for this k point
            do iband=1,nband
             write(unit) (cg(ii+index),ii=1,2*npw*nspinor)   <-- wavefunction coefficients
            enddo                                            for a single band and k point
            bantot=bantot+nband
            index=index+2*npw*nspinor*nband
           enddo
          enddo
```

If the job ended without problem, a few supplementary lines are added, in
order to give the history of atomic positions and corresponding forces. The
integer nxfh gives the number of pairs (x,f) of positions and forces in reduced coordinates:
    
```fortran
     write(unit)nxfh
     do ixfh=1,nxfh
      write(unit) xred(1:3,1:natom,ixfh),dummy(1:3,1:4),&
    &             fred(1:3,1:natom,ixfh),dummy(1:3,1:4)
     enddo
```

The dummy variables might contain, in the future, the description of the unit
cell, and the stresses. The type of the different variables is:
    
```fortran
    integer :: kg,nband,npw,nspinor,nxfh
    double precision :: cg,dummy,eigen,fred,occ,xred
```

The **response-function** wf file consists of the [header records](#header), and data
written with the following lines of FORTRAN:
    
```fortran
    bantot=0                                    <-- counts over all bands
    do isppol=1,nsppol
     do ikpt=1,nkpt
      write(unit) npw,nspinor,nband                    <-- for each k point
      write(unit) kg(1:3,1:npw)                        <-- plane wave reduced coordinates
      do iband=1,nband
       write(unit) (eigen(jband+(iband-1)*nband+bantot),jband=1,2*nband)  <-- column of eigenvalue matrix
       write(unit) (cg(ii+index),ii=1,2*npw*nspinor)     <-- wavefunction coefficients
      enddo                                            for a single band and k point
      bantot=bantot+nband
     enddo
    enddo
```

Note that there is an alternative format (_KSS) for the output of the
wavefunction coefficients, activated by a non-zero value of [[nbandkss]].

### 5.8 Other output files

  
There are many other output files, optionally written, all formatted files at
present. Their use is usually governed by a specific input variable. Please
consult the description of this input variable, in order to have more
information on such files:

  * [[prtdos]] to print a file with the electronic Density-Of-States
  * [[prteig]] to print a file with the list of k points and eigenenergies
  * [[prtgeo]] to print a file with a geometrical analysis (bond lengths and bond angles), that also contains an XMOL section
  * [[prt1dm]] to print a one-dimensional projection of potential and density, for the three axes.

### 5.9 Control of output in the parallel case
  
For massively parallel runs, one cannot afford to have some of the output
files that are usually created. Explicitly, the log file and also the status
file become problematic. By default, with $N=2$ processors or less than $N=2$ processors, they are
created, but beyond $N=2$ processors, they are deactivated except for the main log
file (master processor).

This default behaviour can be changed as follows. If a file named "_NOLOG"
exists in the current directory, then no log file and no status file will be
created, even with less than $N=2$ processors. By contrast, if a file "_LOG"
exists in the current directory, then a log file and the status files will be
created, even with more than $N=2$ processors. Alternatively, if a file named
"_MAINLOG" exists and there are less than $N=2$ processors, only the master
processor writes the log and status files (this mimic the default behavior
when using more than $N=2$ processors but with less than $N=2$ processors)

In ABINITv7, $N$ was set at $N=100$. However, with ABINITv8, $N$ has been switched
to $2$. It can be changed "by hand", though: modify NPROC_NO_EXTRA_LOG in
src/10_defs/defs_basis.F90 and recompile. See 44_abitypes_defs/m_dtfil.F90 for more explanation.


<a id="numerical-quality"></a>
## 6 Numerical quality of the calculations
  
The following section describes various parameters which affect convergence
and the numerical quality of calculations.

The list of these input parameters is

  * (1) [[ecut]] 
  * (2) [[toldfe]], [[toldff]], [[tolwfr]], and [[tolvrs]], as well as [[nstep]] 
  * (3) [[nkpt]] 
  * (4) [[ngfft]] 
  * (5) [[tolmxf]], as well as [[amu]], [[dtion]], [[vis]], [[ntime]] 
  * (6) [[acell]] and [[rprim]] 
  
The technical design of the pseudopotential also affects the quality of the results.

(1) The first issue regarding convergence is the number of planewaves in the
basis for a given set of atoms. Some atoms (notably those in the first row or
first transition series row) have relatively deep pseudopotentials which
require many planewaves for convergence. In contrast are atoms like Si for
which fewer planewaves are needed. A typical value of [[ecut]] for silicon
with a norm-conserving pseudopotential
might be 10...15 Hartree for quite good convergence, while the value for oxygen
might be 35...40 Hartree or more depending on the convergence desired and the
design of the pseudo-potential. With PAW, the [[ecut]] to be used is usually 
smaller, e.g. 17 Hartree for oxygen. The pseudo-dojo <http://www.pseudo-dojo.org/>
provides hints with the available pseudopotentials.

NOTE: It is necessary in every new problem to **TEST** the convergence, by
**RAISING** [[ecut]] for a given calculation, until the results being computed
are constant to within some tolerance. This is up to the user and is very
important. For a given [[acell]] and [[rprim]], [[ecut]] is the parameter
which controls the number of planewaves. Of course if [[rprim]] or [[acell]]
is varied then the number of planewaves will also change.

Let us reiterate that extremely careful pseudopotential design can optimize
the convergence of _e.g._ the total energy within some range of planewave
number or [[ecut]]. It is appropriate to attempt to optimize this convergence,
especially for difficult atoms like oxygen or copper, as long as one does not
significantly compromise the quality or transferability of the
pseudopotential. 

For information on optimizing the convergence of pseudopotentials, see 
[[cite:Rappe1990]]. For the generation of ONCVPSP pseudopotentials, see 
[[cite:Hamann2013]]. For the pseudodojo, see [[cite:Vansetten2018]].
For the JTH table of PAW atomic data, see [[cite:Jollet2014]].

(2) In addition to achieving convergence in the number of planewaves in the
basis, one must ensure that the SCF iterations which solve the electronic
structure for a given set of atomic coordinates are also converged. This
convergence is controlled by the parameters [[toldfe]], [[toldff]],
[[tolwfr]], and [[tolvrs]], as well as the parameter [[nstep]]. One of the
"tolerance" parameters must be chosen, and, when the required level of
tolerance is fulfilled, the SCF cycles will stop. The [[nstep]] variable also
controls convergence in preconditioned conjugate gradient iterations by
forcing the calculation to stop whenever the number of such iterations exceeds
nstep. Usually one wants nstep to be set larger than needed to reach a given
tolerance, or else one wants to restart insufficiently converged calculations
until the required tolerance is reached.

Note that, if the gap in the system closes (e.g. due to defect formation or if
the system is metallic in the first place), the presently coded algorithm will
be slower to converge than for insulating materials. Convergence trouble
during iterations usually signals closure of the gap. The code will suggest to
treat at least one unoccupied state (or band) in order to be able to monitor such a closure.

(3) For self consistent calculations ([[iscf]] positive) it is important to
test the adequacy of the k point integration. If symmetry is used then one
usually tests a set of "special point" grids. Otherwise one tests the addition
of more and more k points, presumably on uniform grids, to ensure that a
sufficient number has been included for good k point integration. The
parameter nkpt indicates how many k points are being used, and their
coordinates are given by kpt and kptnrm, described above. The weight given to
each k point is provided by input variable [[wtk]]. Systematic tests of k
point integration are much more difficult than tests of the adequacy of the
number of planewaves. The difficulty I refer to is simply the lack of a very
systematic method for generating k point grids for tests.

(4) It is possible to run calculations for which the fft box is not quite
large enough to avoid aliasing error in fft convolutions. An aliasing error,
or a Fourier filter approximation, is occurring when the output variable "
**boxcut** " is less than 2. boxcut is the smallest ratio of the fft box side
to the planewave basis sphere diameter. If this ratio is 2 or larger then e.g.
the calculation of the Hartree potential from the charge density is done
without approximation.  

NOTE: the values of [[ngfft]](1:3) are chosen automatically by the code to
give boxcut > 2, if [[ngfft]] has not been set by hand. At ratios smaller than
2, certain of the highest Fourier components are corrupted in the convolution.
If the basis is nearly complete, this Fourier filter can be an excellent
approximation. In this case values of boxcut can be as small as about 1.5
without incurring significant error. For a given [[ecut]], [[acell]], and
[[rprim]], one should run tests for which [[ngfft]] is large enough to give
boxcut >= 2, and then one may try smaller values of [[ngfft]] if the results
are not significantly altered. See the descriptions of these variables above.

(5) If you are running calculations to relax or equilibrate structures, i.e.
with [[ionmov]]=1 and possibly [[vis]]>0, then the quality of your molecular
dynamics or relaxation will be affected by the parameters [[amu]], [[dtion]],
[[vis]], [[ntime]], [[tolmxf]]. Clearly if you want a relaxed structure you
must either run long enough or make repeated runs until the largest force in
the problem (output as fmax) is smaller than what you will tolerate (see
[[tolmxf]]).  
If [[dtion]] is too large for the given values of masses ([[amu]]) and
viscosity ([[vis]]) then the molecular dynamics will be unstable. If [[dtion]]
is too small, then the molecular dynamics will move inefficiently slowly. A
consensus exists in the community that forces larger than about 0.1
eV/Angstrom are really too large to consider the relaxation to be converged.
It is best for the user to get experience with this in his/her own
application.  
The option [[ionmov]]=2, 3 or 7 are also available This uses the Broyden
(BFGS) scheme for structural optimization and is much more efficient than
viscous damping for structural relaxation.

(6) If you are running supercell calculations (i.e. an isolated atom or
molecule in a big box, or a defect in a solid, or a slab calculation) you must
check the convergence of your calculation with respect to the supercell and
system size.

  * For an isolated molecule in a big box: increase concurrently the three dimensions 
    of your supercell ([[acell]]), and check the convergence of your physical property. 
  * For a defect in a solid: your supercell must be a multiple of the primitive cell of the bulk solid, 
    so you have less freedom. Still, be sure that your supercell is large enough for your properties 
    of interest to be accurate at the level you want it to be. 
  * For a slab calculation: you must increase the vacuum in the cell, but also the thickness of your slab systematically... 
---
authors: AM,ACGC,FR
---

# The multibinit software

The MULTIBINIT software implements a second-principles approach for lattice dynamics simulations
based on atomic potentials fitted on first-principles calculations [[cite:Wojdel2013]].
The second-principles effective potential accounts for harmonic (short-range and long-range dipole-dipole interactions)
and anharmonic contributions,
as well as the explicit treatment of homogeneous strain and its coupling with the lattice.
On the one hand, parameters associated to second energy derivatives
(harmonic interatomic force constants -IFC-, elastic constants, strain-phonon coupling, etc.)
are determined exactly and provided by the Density Functional Perturbation Theory ([[topic:DFPT|DFPT]]).
On the other hand, the anharmonic lattice contribution is restricted to a limited number of terms
(short-range interaction and low order)
and treated in a more effective way: it is fitted [[cite:Escorihuela-Sayalero2017]]
to reproduce stresses and forces on a training set which should include representative configurations
properly sampling the phase space.


## 0 Installation

The MULTIBINIT software is included in the [[help:abinit|ABINIT]] package,
thus to install this code,
you can follow the instructions of the [[help:../installation|installation guide]] of the package.

However, in order to be able to use all the MULTIBINIT features, you might need to recompile ABINIT
if you have not activated some flags in the configure (config.ac) file:

* MULTIBINIT generates the output of the molecular dynamics in the NetCDF format: the NetCDF library is linked by using the flag:

        with_trio_flavor="netcdf"

* MULTIBINIT uses Libxml2 to parse the XML files (not mandatory, but more efficient for heavy XML files).
  In principle, the configure script should automatically detect libxml2. 
  If needed, specify the options with:

        enable_xml="yes"
        CFLAGS_EXTRA="`xml2-config --cflags`"
        FC_LIBS_EXTRA="`xml2-config --libs`"


## 1 How to run the code

### 1.1 Introducing the "files" file

Once you have prepared the input file (see below for the parameters description)
and the required files for the generation of the model, you must create a ".files" file which lists (one for each line)
the filenames the job will require: the main input file, the main output file, the file for the model (model.DDB or model.XML),
the XML file for the anharmonic part of the model and the NetCDF file for the training set.
The files file (called for example multibinit.files) looks like:

      multibinit.in
      multibinit.out
      model.DDB or model.XML
      model_anharmonic.XML
      training_set_HIST.nc

In this example:

  * The main input file is called "multibinit.in".
  * The main output will be written as "multibinit.out".
  * model.DDB or model.XML is the Database from ABINIT or XML.
  * model_anharmonic.XML (_optional_) is the XML with the coefficients from fitted polynomial.
  * training_set_HIST.nc is the history file in NetCDF format containing the training set.

The model.DDB (or model.XML) file contains the system definition and the list of the total energy derivatives
with respect to three kind of perturbations: phonons, electric field and strain.
The _optional_ XML file contains the list of coefficients obtained by fitting a polynomila to the energy.
The last file is mandatory to obtain the "model_anharmonic.XML" file.

### 1.2 Running the code

The main executable file is called "multibinit". Supposing that the executable is located in your working
directory, you can run it interactively (in Unix) with the command:

    multibinit < multibinit.files > log

or, in the background, with the command:

    multibinit < multibinit.files > log &

Here, the standard output and standard error are piped to the file called "log".

The user has the full freedom to change the filenames. Moreover, modifications of the above commands could be needed, depending on the UNIX flavour that is used on the platform supposed to execute the code.

The syntax of the input file is completely similar to the one of the main ABINIT: this file is parsed, keywords are identified, comments are also identified. However, the [[topic:multidtset|multidataset]] mode is not available.

## 2 Input variables

Multibinit is able to perform many different tasks.
For example, it is possible to generate a second principles model by extracting the harmonic part from a DFPT calculation
and fitting the related anharmonic part with a polynomial expression.
Such a model can contain phonons instabilities and can be bound with an automatic procedure.
In addition, for a given model, it is also possible to perform a molecular dynamics using MULTIBINIT.
Each feature can be selected by a set of input variables
for which the corresponding 'flag' variables activate the different tasks,
e.g. [[multibinit:prt_model]], [[multibinit:fit_coeff]], [[multibinit:bound_model]] and [[multibinit:dynamics]].
The full list of the input variables are presented in the [[varset:multibinit|multibinit variable set]].

## 3 How to generate a model

Before learning how to generate a model, we encourage you to read the papers [[cite:Wojdel2013]] and [[cite:Escorihuela-Sayalero2017]].

### 3.1 How to compute the harmonic contribution using [[help:abinit|ABINIT]]

MULTIBINIT requires a DDB file from ABINIT to build the harmonic part of the potential.
To produce this file, you are first supposed to learn how to perform [[topic:DFPT|DFPT]] calculations with ABINIT.
For this purpose, it is useful to follow the different tutorials:
start with the second lesson about response functions ([[lesson:rf2]])
to compute the phonon dispersion and build the related DDB file,
then follow the lesson on elasticity ([[lesson:elastic]]) to compute elastic contribution and build the related DDB.
At the end, use the [[help:mrgddb|DDB merge tool]] to generate the "model.DDB" file seen before.

To learn the procedure to compute the **harmonic** part of the potential, have a look at the [[topic:LatticeModel]],
where input variables and selected input files are mentioned.
Later, a <!-- [[lesson:lattice_model | tutorial]]--> tutorial will be available.

### 3.2 How to compute the anharmonic contribution using a *training set*

To compute the anharmonic part of the model,
MULTIBINIT requires a training set containing several DFT calculations.
Such training set will contain an ensemble of atomic configurations sampling the potential energy surface.
This file will be used to fit a polynomial following the method developed by [[cite:Escorihuela-Sayalero2017]].
To generate the training set, the user requires to get some experience in [[topic:MolecularDynamics|molecular dynamics]] simulation within ABINIT.
Alternatively, the user can generate the training set configurations by means of phonon population procedure
at fixed temperature (DDB file mandatory) activated by imposing [[abinit:ionmov]]=27.

### 3.3 How to bound a model

MULTIBINIT includes an automatic binding process.
Have a look at the [[topic:BoundingProcess]],
where input variables and selected input files are mentioned.
Later, a <!-- [[lesson:lattice_model | tutorial]]--> tutorial will be available.

### 3.4 How to run a dynamics

MULTIBINIT uses routines of ABINIT which perform atomic relaxation, molecular dynamics, and Monte Carlo simulations.
Have a look at the [[topic:DynamicsMultibinit]],
where input variables and selected input files are mentioned.
Later, a <!-- [[lesson:lattice_model | tutorial]]--> tutorial will be available.
---
authors: SS, XG, YG, NAP
---

# The optic utility  

Frequency dependent linear optical dielectric function and second order nonlinear optical susceptibility.  

This file explains the i/o parameters needed for the calculation of the
frequency-dependent linear optical dielectric function and second-order
nonlinear optical susceptibility, in the RPA approximation (sum-over-states
using independent electronic states) thanks to the Optic utility of the Abinit package.
The user is advised to be familiar with the main [[help:abinit]] before reading the present file.

A knowledge of the computation of the linear response $\partial /\partial k$ perturbation,
explained in the [[help:respfn]], is also advised.

It will be easier to discover the present file with the help of the [[tutorial:optic|optic tutorial]].  

## 1 Introduction
  
The Optic utility allows the computation of the frequency-dependent linear optical dielectric
function and second-order nonlinear optical susceptibility including the nonlinear susceptibility and 
the linear electro-optical coefficient. An introduction to such computations is given in [[cite:Sharma2004]],
online at <http://arxiv.org/abs/cond-mat/0305016>.
Other useful references include [[cite:Hughes1996]],
[[cite:Draxl2006]], online at <http://arxiv.org/abs/cond-mat/0402523>,
and [[cite:Sharma2003]], online at <http://arxiv.org/abs/cond-mat/0211512>.

Before going to the detailed explanation of the Optic utility, the user is
advised to become familiar to the theory behind it, explained in these
references. Familiarity at least with [[cite:Sharma2004]] is strongly advised.

The specific purpose of the Optic utility is to read in the momentum matrix elements 
generated by Abinit (actually matrix elements of the $\partial H/\partial k$ operator), and
then use the approach of [[cite:Sharma2004]] to determine the
linear (Eq. 46) and nonlinear (Eqs. 49-51) optical response of the material under investigation.

Note that optic works in the independent-particle approximation (no excitons), 
and without local-field effects. For higher level of theories, use the Bethe-Salpeter equation, see the [[tutorial:bse|BSE tutorial]].
The intraband Drude term (for metals only) is also not included.
Usually it has only small effect at visible frequencies, but important effect
at smaller frequencies. See e.g. [[cite:Cazzaniga2010]], Fig. 3.


## 2 How to run Optic
  
The use of Optic is quite simple:
    
    optic optic.abi > optic.log 2> optic.stderr
    
However, before being able to use Optic, you must have obtained, from the main
Abinit program, four different files, corresponding to the physical system that you want to study:

  * The ground state wavefunction file, indexed with _WFK 

  * Three files containing the matrix elements of the $\partial H/ \partial k$ operators of the unperturbed wavefunctions, 
    one for each direction of space

Supposing you have read the [[help:abinit|main Abinit help file]], the
production of the first file should not require any additional explanation.
However, the way to obtain the three other files, containing the matrix elements of the dH/dk operators is worth explaining.  

A straightforward relationship exists between the dH/dk matrix
elements and the matrix elements of the momentum operator.
The computation of the dH/dk matrix elements is the initializing step in the
treatment of the response to an electric field in DFPT.
The long-wave method as well as the Berry-phase treatment of electric field,
allow to establish the equivalence between the off-diagonal matrix elements of
the position operator, and the off-diagonal matrix elements of the derivative
with respect to the wavevector ($\partial /\partial k$), for the periodic part of the Bloch
functions (see for example section VI of [[cite:Gonze1997]], and [[cite:Nunes2001]]).
Such calculation involve applying the dH/dk operator to the unperturbed wavefunctions,
in the Sternheimer equation.

The main Abinit program has the capability to compute derivatives of
wavefunctions with respect to their wavevector. This is explained in [[help:respfn]]. 
Such a calculation implies treating three d/dk perturbations, indexed by the numbers 3*natom+1, 3*natom+2 and
3*natom+3 (thus for example, for a unit cell with 2 atoms, perturbations number 7, 8
and 9). In the two-atom case, the associated files needed for Optic have the index _1WF7 , _1WF8 , and _1WF9.
However, as mentioned above, the computation of the dH/dk matrix elements is only the initializing step
in such calculations. Still, the $\partial H/\partial k$ will be made available in such files.

The formalism implemented in Optic treats explicitly the eigenstates lying in
the range of energy between the lowest occupied wavefunction and the highest
one plus the maximal excitation energy (chosen by the user). 
All the other ones are neglected. This has two important consequences for the preliminary runs:

  * The ground calculation must produce explicitly all the eigenstates and eigenvalues 
    for that target range of energy, so it cannot be restricted to the occupied wavefunctions only

  * One does not need the full change of Bloch wavefunctions with respect to $\partial /\partial k$, but 
    only the matrix elements of $\partial H/\partial k$ between the wavefunctions of this range of energy

Because of the latter, the computation of the response to $\partial /\partial k$ perturbations
is much shorter than usual: indeed, the matrix elements between the
explicitly ground-state wavefunctions are computed at the very beginning of
the abinit(respfn) run. It is not necessary to make a full calculation of the
modification of the wavefunctions due to a change of wavevector. This explains why [[nstep]] has been set to 0 in such calculations.

<a id="input"></a>
## 3 Optic input file and input variables
  
Please note that the format of input files for Optic has changed as of Abinit
v8.0. Since very few input parameters are required for Optic, the optic.in file
contains them with the namelist format. The order of the three parts, namely
FILES, PARAMETERS and COMPUTATIONS must be kept unaltered.
    
```fortran
    &FILES
     ddkfile_1 = 'toptic_1o_DS4_1WF7',
     ddkfile_2 = 'toptic_1o_DS5_1WF8',
     ddkfile_3 = 'toptic_1o_DS6_1WF9',
     wfkfile = 'toptic_1o_DS3_WFK'
    /
    &PARAMETERS
     broadening = 0.002,
     domega = 0.0003,
     maxomega = 0.3,
     scissor = 0.000,
     tolerance = 0.002
    /
    &COMPUTATIONS
     num_lin_comp = 1,
     lin_comp = 11,
     num_nonlin_comp = 2,
     nonlin_comp = 123,222,
     num_linel_comp = 1,
     linel_comp = 123,
    /
```

The list of input variables for the optic input file is presented in [[topic:Optic]].

<a id="output"></a>
## 4 Optic output files
  
#### **4.1. Linear optical response data files**

Name: case_a_b-linopt.out  
Contains the following 3 data sets

  * 1) Column 1 - energy(eV), column 2 - imaginary part of the ab component of the frequency-dependent linear dielectric tensor.
  * 2) Column 1 - energy(eV), column 2 - real part of the ab component of the frequency-dependent linear dielectric tensor.
  * 3) Column 1 - energy(eV), column 2 - absolute value of the ab component of the frequency-dependent linear dielectric tensor.
  * 4) Column 1 - energy(eV), column 2 - imaginary part of the ab component of the frequency-dependent linear refractive index ($\kappa$).
  * 5) Column 1 - energy(eV), column 2 - real part of the ab component of the frequency-dependent linear refractive index ($n$).
  * 6) Column 1 - energy(eV), column 2 - frequency-dependent reflectivity at normal incidence.
  * 7) Column 1 - energy(eV), column 2 - absorption coeff (in m-1) = $\omega \Im(\epsilon) / c n(\epsilon)$

In the header of the file you can find information about the calculation. 

In case [[nsppol]]==2, the spin-decomposition of items 1 and 2 above is provided in columns 3-4 for the imaginary part, and 3-5
for the real part (the spin-independent contribution $\delta(ab)$ is mentioned as well).

#### **4.2. Non-linear optical response data files**

Name: case_a_b_c-ChiKIND1.out  
KIND1:This can be TotIm, TotRe or TotAbs  
Contains: column 1 - energy(eV), column 2 and 3 - imaginary (KIND1=TotIm),
real (KIND1=TotRe) or absolute (KIND1=TotAbs) value of the abc component of
the nonlinear optical susceptibility. Second column contains values in
electro-static units (esu) and third column contains values in the SI units.  

Name: case_a_b_c-ChiKIND2.out  
KIND2:This can be Im, Re or abs  
Contains: column 1 - energy(eV), column 2, 3 inter and column 4, 5 intra band
contributions to the imaginary (KIND2=Im), real (KIND2=Re) or absolute
(KIND2=Abs) value of the abc component of the nonlinear optical
susceptibility. These components are labeled as inter and intra in Eqs. 49-51 in [[cite:Sharma2004]]

All the values are in electro-static units (esu). In the header of all the
above files you can find information about the calculation. 

<a id="troubleshooting"></a>
## 5 Troubleshooting
  
1) All I get are zeros in my *-linopt.out file. Why?

There are several possibilities.  
Let us explore some of them here:

  * (i) The component of the dielectric tensor that you are looking at could be zero due to symmetry of the crystal. 
    Normally zz component is a good place to start. It is almost never zero. So check the file case__0003_0003-linopt.out.

  * (ii) If the components zz is zero this is more serious, if you are using the default input file t57.in 
    then please check that on the line number 10 the second number is 33. 
    If you are not using the default input file please calculate the 33 (or zz) component and make sure it is not zero.

  * (iii) If even zz component is zero then possibilities are endless maximum frequency on 
    line number 6 of t57.in is too small, or the number of bands used for performing ground state calculation are too small.

2) All I get is zeros in my *-ChiKIND.out file. Why?

The two most common mistakes are:

  * (i) You are calculating the second order response for material with inversion symmetry, so 
    in this case all the components will be correctly zero or very small (order $10^{-15}$).

  * (ii) Most components out of the 27 are zero due to the symmetry of the crystal. Please calculate a different component.
---
authors: MG, MJV
---

# DFPT FAQs

This page collects FAQs related to the DFPT part of Abinit.

## Can I use paral_kgb 1 in the DFPT part?

No, the DFPT code uses a completely different parallel algorithm in which
the workload and memory is automatically distributed at runtime depending on the
number of MPI processes and the perburbation to be computed. The parallelism is over the
k points and the bands, so whatever the size of the system, one can use several dozen processors efficiently.

In principle it is also possible to parallelize over the perturbations
(see [[paral_rf]] and [[nppert]]) but keep in mind that this approach is not optimal
from the point of view of the worload distribution as each perturbation has its own list of irreducible k-points
(see discussion below).
Moreover if one perturbation does not converge withing [[nstep]] iterations,
ABINIT will abort and we may end up with zero DDB files produced!
Finally, it is also possible to run different d points concurrently (with different ABINIT runs), and to merge their DDB files.
The workflows with Abipy can do the launch of many concurrent jobs for you.

## How to get the irreducible q-points for phonon calculations

When doing a phonon calculation, only the irreducible perturbations need to be calculated,
and abinit and anaddb will complete them by symmetry.
In particular, the dynamical matrix will be completed and symmetrized at each q-point.

To get the irreducible q-points for your system, you can do a small auxiliary calculation
(with low [[ecut]] and minimal [[nband]]).
Make a new directory, and copy your input file for the ground state there.
If you set the k-point grid [[ngkpt]] to be equal to the q-point grid you want for the phonons, and set [[kptopt]] to 1.
Also, don't forget to set [[shiftk]] to 0 0 0, in order to get a non-shifted grid with the Gamma point.
From the header of the log or output file, the [[kpt]] points can be copied directly to the phonon input file, as q-points.

One can also use this one, but one should change the output format

    abistruct.py abikmesh si.cif --ngkpt 2 2 2 --shiftk 0 0 0

## warning The dynamical matrix was incomplete

> I am getting the following warning in phonon calculations:
  The dynamical matrix was incomplete : phonon frequencies may be wrong
  Does anyone knows what is wrong and how can I fix it?

This warning is given by abinit but is not a problem: when you calculate certain perturbations
at a given q-point abinit tries to immediately complete and diagonalize the dynamical matrix.
In general this is not possible, as you have not done a full set of irreducible perturbations
all at once (hence the warning).
The frequencies are output, but many may be zero because certain dynamical matrices have not been calculated yet.

You need to continue the calculations and use mrgddb to merge the DDB files, and then anaddb
to do the reconstruction of the dynamical matrix.
Follow out the whole of the rf tutorials and you will see the phonon frequencies will come out when anaddb is run.

## Why some perturbations are faster to compute than others?

Each perturbation breaks the initial symmetry of the crystal thus only a subset of the
crystalline symmetries can be used to define the irreducible set of wavevectors used in the DFPT equations.
For instance, phonon calculations at $\Gamma$ are much faster that calculations done at non-zero $\qq$-points
since more symmetries can be exploited.
Note, however, that even perturbations with the same $\qq$-point may have a different workload.
For best performance, each perturbation should be in principle computed in a different run and with a different
number of MPI procs.

## What to do if a perturbation does not converge?

Firs of all, try to restart from the first-order WFK file using [[ird1wf]], [[get1wf]].
In principle, it is also possible to restart from the first-order density via [[get1den]]
but use this approach only if the first-order WFK file is not available.
You may want to use [[prtwf]] = -1 in the DFPT part to produce the first-order WFK file only
when the DFPT SCF cycle does not converge in order to reduce the amount of data written to disk.

You may also try to increase [[nline]].

## Is it a good idea to compute all the perturbations in parallel with a single input file?

If you care about perfomance, the answer is **definitely NO!**
As already mentioned, each perturbation has a different workload so it is much better
the compute the different perturbations with separate input file and use more MPI processes
for the heaviest perturbations.

## Is there an easy way to compute the phonon band structure from the DDB file?

```
abiview.py ddb DDB_FILE
```

## How can I reduce the breaking of the ASR?

## Is there an easy way to visualize the breaking of the ASR?

```
abiview.py ddb_asr DDB_FILE
```

## Can I convert the DDB into phonopy format?

Yes, but you will need to use AbiPy to convert from the two formats.

---
authors: MG
---

# Configuration and compilation

This page collects FAQs related to

## I'm new to configure. Can you explain how to use it?

Please consult <https://wiki.abinit.org/doku.php?id=build:configure>

To obtain the full list of available options and their documentation, use

```
./configure --help
```

As an example, the option to specify the Fortran compiler vendor appears as with_fc_vendor in the config file,
while it is typed --with-fc-vendor on the command line.

You will find detailed instructions on how to set the various options of the configure script
in the template file ~abinit/doc/build/config-template.ac, including how to have your config file
included automatically each time you build Abinit.

There are only 2 undocumented options in this template:

the --with-config-file option, which lets you use arbitrary names for your config files,
as long as you specify it each time on the command line;
the --disable-config-file option, to force Abinit not to use any config file.
For obvious reasons, they are the only options that cannot be stored in a config file.

## Configure complains that Abinit cannot be built!

Well, there are many reasons why configure can fail.

First of all, make sure that your LD_LIBRARY_PATH (DYLD_LIBRARY_PATH on MacOs), PATH environment variables
are properly set **before** running configure.
By the same token, one should load all the modules before configuring.
If your environment is propertly set and configure keeps on failing, you will need to search 
for inside *config.log*.

## Compilation stops with "internal compiler error"

This usually indicates a bug in the compiler that should be reported to the compiler vendor.
To bypass the problem, reduce the optimization level from e.g. -O2 (default) to -O1 or even -O0.
If you don't like the idea of compiling all the source file with suboptimal options, you may try
to disable optimation only for the problematic source files using this quick and dirty recipe:

- Run *make* till you trigger the internal compiler error
- Copy the compilation command that triggers the internal error
- Add `-O0` to the compiler options and execute this command line to compile the module.

## Cannot run Abinit due to "undefined reference" error

This means that your Abinit executable is **dynamically linked** with external libraries
but the OS cannot find these libraries at runtime.
Please make sure that LD_LIBRARY_PATH (DYLD_LIBRARY_PATH on MacOs) is properly set that is you have
the same value as the one used when configuring/compiling ABINIT.
The same error message may show up if you forgot to load modules before trying to run the executable.
Again, you are supposed to **load the same modules** that were used for configuring/compiling Abinit.

## Is there any precompiled package for Abinit?

Yes, please consult the different subsections available in the [installation](/installation) page.

## Where can I find examples of configuration files for HPC clusters?

The |abiconfig| package provides configuration files used to configure and compile Abinit on HPC clusters.
Each configuration file contains a header with metadata in json format followed by the configure options
supported  by the Abinit build system.
Examples for Abinit v9 are available [here](https://github.com/abinit/abiconfig/tree/master/abiconfig/clusters).

## Is there any EasyBuild recipe for Abinit?

Yes. An HowTo tutorial is also available [here](/INSTALL_EasyBuild)

## Is there any Spack recipe for Abinit?

Yes but, at present, only Abinit8 is supported.

## Is there any conda package for Abinit?

Yes. conda install abinit --channel conda-forge

Note, however, we do not reccomend using precompiled version on HPC clusters.
The version provided by Spack/EasyBuild pa

## Why do we provide fallbacks?

Abinit Fallbacks is a package builder for the external dependencies of ABINIT in environments lacking these components.
Note, indeed, that **all** the external libraries should be compiled with the same MPI wrapper/compiler
used to build ABINIT.
Unfortunately, not all HPC centers provide a complete set modules covering the external dependencies required by ABINIT.
<!--
In the worst case scenario, the modules are available but they do not work properly
They do not provide full support for the advanced features of Abinit nor HPC-grade calculation capabilities.
They are designed to let developers quickly test new versions of these external dependencies in various situations
before proposing upgrades, as well as to allow end-users to run calculations on their favorite PCs.
In case some dependencies are missing on your computers,
Abinit provides fallback libraries that you can build and install from their sources before compiling Abinit itself.
-->

!!! important

    Feel free to contact the sysadmin of your cluster to ask him/her to install these libraries.

## How can I compile Abinit with OpenMP support?

Compiling ABINIT with OpenMP support is not that difficult once we understand that OpenMP can be activated
at two different levels.

1. The Abinit Fortran routines
2. External libraries for BLAS/Lapack and FFTs.

To activate OpenMP in 1), add to the .ac file

```
enable_openmp="yes"
FCFLAGS_OPENMP="-fopenmp"    # for the GFORTRAN compiler
#FCFLAGS_OPENMP="-qopenmp"   # for the INTEL FORTRAN compiler
```

Finally, remember to set the environment variable

```
export OMP_NUM_THREADS=1
```

!!! important

    Do not mix MKL with FFTW3, especially when activating OpenMP.
---
authors: MG
---

# Ground-state FAQs

This page collects FAQs related to the GS part of Abinit.

## What are the dimensions defining the workload of a standard GS run?

In a GS run, we have to find the set of KS orbitals $\psi_\nks$ that solve the self-consistent problem:

\begin{align*}
H_\KS[n] \psi_\nks &= \epsilon_\nks \psi_\nks \\
n(\rr) &= \sum_\nks f_\nks \psi_\nks(\rr)
\end{align*}

where the number of bands $n$ is given by [[nband]], the number of **k**-points (in the IBZ) is [[nkpt]]
and the number of spin components $\sigma$ is given by [[nsppol]]  (note that we are assuming scalar wavefunctions,
that are either spin up or spin down. 
Two-component spinors require [[nspinor]] == 2).
The periodic part of the Bloch state is expanded in a planewave basis set according to:

$$
u_\nks(\rr) = \sum_{\GG}^{\text{npw}_\kk} u_\nks(\GG) e^{i\GG.\rr}
$$

For a given $\kk$ point only the $\text{npw}_\kk$ $\GG$-vectors with kinetic energy
$|\kG|^2/2 <= \text{ecut}_\text{eff}$ are included in the Fourier series
where $\text{ecut}_\text{eff} =$ [[ecut]] * [[dilatmx]] ** 2.
The maximum number of $\GG$-vectors over all the **k**-points is called [[mpw]].
Time-reversal symmetry is automatically exploited to halve the number of planewaves for
particular **k**-points, see the [[istwfk]] input variable.

The GS code stores in memory the Fourier components $u_\nks(\GG)$ and Fast Fourier Transforms
are used to go from $u_\nks(\GG)$ to $u_\nks(\rr)$.
The FFT mesh is defined by the first three entries of the [[ngfft]] array that are automatically
computed from [[ecut]] and [[boxcutmin]].
There are also other additional arrays that depend on [[natom]], especially if PAW is used.
All these variables are reported at the beginning of the main output file.

Keep in mind that there is a strong interplay between these dimensions and the MPI parallelization.
The parallelism over **k**-points and (collinear) spins is rather efficient
whereas the parallelism over bands and $\GG$-vectors is more network intensive.

!!! warning

    The conjugate gradient solver (default algorithm for GS calculations)
    **cannot use** more than [[nkpt]] * [[nsppol]] MPI processes.
    To take advantage of the band + $\GG$ parallelim use [[paral_kgb]] == 1 in the input file,
    possibly with [[autoparal]] = 1

## Is it a good idea to set the number of bands to the highest occupied state to accelerate a GS calculation?

Usually, the answer is **NO** and the explanation requires some technical discussion about the KS solver.
In principle, the workload as well as the memory footprint are proportional to [[nband]] so, at first sight,
reducing the number of KS states seems to be a smart choice.
On the other hand, a larger number of bands allows the eigenvalue solver to operate on a larger Hilbert subspace
and this can make a huge difference at the level of the number of iterations required to convergence.
This is especially true for algorithms such as the RMM-DIIS method ([[rmm_diis]] = 1).

In other words, more bands means more wall-time per SCF iteration but, on the other hand,
the SCF cycle is expected to reach converge in less iterations.

!!! important

    In the case of metals, one needs partially occupied states to account for the tail of the occupation
    function [[occopt]] with the broadening parameter [[tsmear]].
    Decreasing the number of bands is therefore a **very bad idea** and ABINIT will stop if there are not enough
    partially occupied states.

## Which convergence criterion do you recommend for the SCF cycle?

Well, the answer depends on the physical properties you are interested in.

Using [[toldfe]] to stop the SCF cycle is an excellent choice if you are interested in total energies,
but the other convergence criteria are much better if you are interested in other physical properties.

[[tolvrs]] is an excellent choice when computing the DEN file needed for a NSCF band structure calculation
since we want to make sure that our KS potential is well converged before using it in the NSCF run.

For structural relaxations or MD runs, one can use [[toldff]] or [[tolrff]] to obtain accurate forces
so that atoms are moved in the right direction at each relaxation step.
Note, however, that in high-symmetry systems the forces may be zero by symmetry.
To avoid this problem, use [[tolvrs]]

[[tolwfr]] is the most stringent convergence criterion since it takes into account the convergence of
all the [[nband]] - [[nbdbuf]] states (including **empty** states, if any).
This is the recommended approach when computing GS wavefunctions to be used in the DFPT code
or in the many-body part.
Keep in mind, however, that the last states usually require many more iterations to converge than the
low-energy states so we strongly recommend using [[tolwfr]] in conjunction with a [[nbdbuf]].

In the case of NSCF run, [[tolwfr]] is the only available convergence criterion

## The SCF cycle does not converge

The default SCF algorithm for SCF (see [[iscf]]) is an excellent compromise between speed and robustness.
It is referred to as Pulay II in Fig. 10 of
N. D. Woods and M. C. Payne and P. J. Hasnip, J. Phys.: Condens. Matter 31, 453001 (2019),
and appears on the Pareto frontier.
Some parameters in the Kerker preconditioning have also an effect on the balance between speed and robustness,
and the default values are good for metallic systems.
As thoroughly discussed in this reference, there is no fool-proof fast algorithm.

If your SCF cycle do not converge (see nres2 or vres2 column in the output file), the reasons can be:
(1) Insufficient underlying accuracy in the solution of the Schrödinger at fixed potential
(2) Transient non-linear behaviour of the SCF, due either to
    (2a) sudden change of occupation numbers (usually only for metals), or
    (2b) long-wavelength fluctuations of potential, bigger than the gap.

Start first to address (2), by some tuning which can come without significantly slowing ABINIT.
Test different values of [[diemac]].
The default is very large, and suitable fo metals.
Try running with a small value (perhaps 5) if your system is a bulk semiconductor or a molecule.
If it is a doped solid, some intermediate value (perhaps 50) might be appropriate.

If this does not work, try to address (1), set [[tolrde]] to 0.001, increase the value of [[nline]]
(e.g. 6 or 8 instead of the default 4), as well as of [[nnsclo]]  (e.g. 2 instead of the default).
Your residm should be significantly lower than without such modifications, and perhaps your problem will be solved.

If this still does not work, but your residm did not look bad after all before addressing (1),
then revert back to the default values of [[tolrde]], [[nline]] and [[nnsclo]], as this indeed slows down ABINIT.

Then, try to use [[iscf]] 2.
This is a very slow but unconditionally convergent algorithm provided [[diemix]] is small enough.
At some stage of convergence, you might restart with the obtained DEN a better algorithm,
as non-linear effects should have been eliminated at some stage.
---
authors: MG
---

# Pre-processing and post-processing FAQs

This page collects FAQs related to pre-processing and post-processing tools as well
as tricks to be more productive when preparing ABINIT input files.

## Can I use a xyx file to specify the unit cell?

Yes, see [[xyzfile]]

## Can I use a cif file to specify the unit cell?

ABINIT does not accept cif files in input but it is possible to convert cif to a format that ABINIT understands.
Perhaps, the easiest solution is to use the AbiPy |abistruct| script:

```
abistruct.py convert CIF_FILE
```

It is also possible to generate a cif file at the end of the GS calculation with the [[prtcif]] variable,
although it is much easier to use AbiPy to perform such task with the command line interface.

```
abistruct.py convert FILE -f cif
```

where FILE is **any file** with a structural info such as the *GSR.nc*, the **DDB** file, 
an ABINIT input file, etc.

!!! important

    In the cif format, the lattice is usually specified in terms of the three angles 
    formed by the direct lattice vectors ($\alpha, \beta, \gamma$ and their lengths).
    Note that these six parameters are not enough to specify the nine entries of 
    the lattice matrix [[rprimd]] expected by Abinit since any rigid rotation of the lattice
    will give the same angles and the lengths reported in the CIF file.
    The CIF parser implemented in pymatgen fixed this arbitrariness using some conventions.

    This can lead to unexpected behaviour if one tries to convert an Abinit structure to CIF format
    and then reuse this CIF file to reconstruct the structure since you are not guranteed to 
    get that same orientation of the lattice.
    Vectors or tensors components expressed in the reduced coordinates obviously depend 
    on the value of [[rprimd]] used in the calculation.
    Obviously one can always transform from one lattice to the other one but, in order to be on the safe side,
    we suggest using CIF files only when specifying the initial lattice and then rely on the ABINIT usual variables.

## Can I use POSCAR files with ABINIT?

Yes, please consult the documentation of the [[structure]] variable.
Note that it is also possible to convert a POSCAR to the Abinit format with:

```
abistruct.py convert POSCAR
```

## How can I convert a structure to primitive?

Use abipy :

```
abistruct.py primitive FILE
```

## Wyckoff positions and site symmetries

Use abipy :

```
abistruct.py wyckoff FILE
```

## How can I compare my structure with the materials project database?

Use abipy :

```
abistruct.py mp_match FILE
```

## How can I visualize a structure?

With abitk, one can read a DEN/WFK file and produce an xsf file

```
abistruct.py visualize FILE -a vesta
```

```
abistruct.py visualize --help

  ...

  -a APPNAME, --appname APPNAME
                        Application name. Possible options: avogadro, ovito, v_sim, vesta, xcrysden, mpl
                        (matplotlib), mayavi, vtk
```


## Can I include external files in the input?

Sure you can.
Abinit supports the `include` statement that can be used to include external files. In your input file,
add a line with the syntax:

```
include "name_of_file_to_be_included_in_the_input_file"
```

where you must obviously change the argument name_of_file... but keep the quotation marks.

!!! note

    **include** is not a standard variable thus it not listed in the variables page.


## Is there an easy way to print the warnings/comments in the log file?

Warnings, Comments and Errors are written in Yaml format.

```
abiopen.py LOG_FILE --print   # or -p
```

## Is there an easy way to obtain a high-symmetry k-path?

Use abipy :

```
abistruct.py kpath FILE
```

## Is there an easy way to plot an electronic band structure?

The most important results of a GS run, including the KS energies, 
are stored in the GSR.nc output file.
To plot the band structure with matplotlib, use the |abiopen| script and the syntax:

```
abiopen.py GSR_FILE --expose  # or -e
```

Other options are available. See `abiopen.py --help`.

To generate an xmgrace file with electron bands, use |abiview| and the command:

```
abiview.py ebands out_GSR.nc --xmgrace
```

## How can I visualize the evolution of the cell during a structural relaxation run.

The lattice paramenters, atomic positions, forces at each relaxation step are stored in the 
HIST.nc output file.

To print

```
abiopen.py HIST_FILE -p
```

To visualize the results with matplotlib, use:

```
abiopen.py HIST_FILE -e
```

To visualize the results with matplotlib, use:

```
abiview.py hist out_HIST.nc -a ovito     ==> Visualize relaxation/molecular dynamics results with ovito.
abiview.py hist out_HIST.nc --xdatcar    ==> Convert HIST.nc into XDATCAR format (caveat: assume fixed unit
```
---
authors: MG
---

# General questions

This page gives a beginner's introduction to the ABINIT resources,
the package, and the main ABINIT applications.

## How can I print the Abinit version?

```
abinit --version
```

See ``abinit --help`` for additional command line options.

## Is there any option to check the validity of the input without running a full calculation?

```
abinit --dry-run run.abi
```

## ABINIT runs but I get unphysical results

Many mistakes done by beginners are related to **incorrect starting geometry**.
Here is a check list:

- Check that the units are correct for your cell parameters and atomic positions.
  Remember that ABINIT uses **atomic unit by default**, but can use several other units if specified by the user,
  see ABINIT input parameters.
- Check that the [[typat]] atom types are correct with respect to [[xred]] or [[xcart]]
- Check that the number of atoms [[natom]] is coherent with your list of coordinates, xred or xcart.
  ABINIT reads only the coordinates of natom nuclei and ignore others.
- Try to visualize your primitive cell. There are numerous possibilities, using Abipy, VESTA, XCrysDen (to be documented).

- ABINIT can read VASP POSCAR or Netcdf external files containing unit cell parameters and atomic positions.
  See the input variable structure. This might help in setting the geometry correctly.

- Relax first the atomic positions at fixed primitive vectors before optimizing the cell.
  Explicitly, use a first datadet with [[optcell]] = 0, then a second dataset with non-zero optcell,
  in which you tell ABINIT to read optimized atomic positions using [[getxred]] or [[getxcart]].
  In this second dataset, do not forget to use [[dilatmx]] bigger than 1 if you expect the volume
  of the cell to increase during the optimization.
  Possibly after the atomic position relaxation, make a run with [[chkdilatmx]] = 0, then a third run with [[dilatmx]] = 1.
  See the additional suggestions in the documentation of [[optcell]].

## What if ABINIT stops without finishing its tasks?

Make sure you get and read the error messages.
They are usually written at the end of the log file, and look like:

```yaml
--- !ERROR
src_file: m_kg.F90
src_line: 179
mpi_rank: 1
message:   (here follows the error message)
...
```

If you do not see such error message at the end of the log file, the error message
might be found alternatively in an “error” file, depending on the command that you used to launch ABINIT.
Also, if you are running in batch, the behaviour might be different, and error messages
might end up in another file than the log file.
If you run in parallel, the error messages might have been written in still another, separate file, entitled

__ABI_MPIABORTFILE__

If ABINIT stops quickly, consider to run in sequential, interactively (not in parallel neither in batch)
to ease the understanding of what is happening.
Sometimes, the error message is prepared by another message giving preliminary information.
Please, be sure to have identified the relevant information in the log file.

If you think to have obtained all information that ABINIT was supposed to give you, try to identify whether
ABINIT stops because of geometry optimization problem or SCF problem.
In the first case, your input geometry is perhaps crazy.
Anyhow, see the next items in this troubleshooting page.

## Is the memory estimate provided by Abinit at the beginning of the run reliable?

Well, **it depends**.
The value reported by the code in the case of ground-state or DFPT calculations should
represent a reasonable estimate.
Very likely, it is smaller than the real value because not all workspace arrays are taken into account.

Unfortunately, it is not easy to estimate the memory requirements for GW, BSE, EPH calculations
so do not rely on the values printed by the code at the beginning.
To have a better idea of the memory footprint, we suggest to use the log file:

```
grep "<<< MEM" run.log -A 3
```

## Is the paral_kgb = 1 algorithm supported everywhere?

No, [[paral_kgb]] = 1 refers to a particular MPI-distribution of the wavefunctions that is only available
for GS calculations (total energy calculations, relaxations, molecular dynamics as well
as NSCF band structure calculations).
Variables such as [[npkpt]], [[npband]], [[npfft]], [[npspinor]] are relevant only if [[paral_kgb]] = 1.
All the other Abinit drivers (e.g. DFPT, GW, BSE, EPH) use a completely different approach to parallelize
the calculation and distribute memory.

## Can I enforce a time limit?

## Where can I find "old pseudos"?

Legacy pseudopotential files are available
[here](https://www.abinit.org/sites/default/files/PrevAtomicData/psp-links/pseudopotentials.html)

## Can I mix NC-pseudos with PAW?

No, the two implementations are mutually exclusive.

## Can I use UPF pseudos with Abinit?

Pseudos in UPF2 format are not supported.
Note that it is not just a matter of format.
There are indeed fundamental differences between the formalism implemented in
Abinit and the one used by QE, especially for PAW and NC-pseudos with SOC.
The pseudodojo project provides NC-pseudos.

## Can I run calculations with an XC functional different from the one used to generate the pseudos?

Yes, use [[ixc]] to change the XC functional in the input file.

## Is there any tool to print a pseudo in a more human-readable format?

Use abipy :

```
abiopen.py PSEUDO_FILE -p
```

## chkprim and unit cell multiplicity

In highly symmetric crystals you may end up with the following error message:

```
chkprimit : ERROR -
According to the symmetry finder, the unit cell is
NOT primitive. The multiplicity is 4 .
The use of non-primitive unit cells is allowed
only when the input variable chkprim is 0.
Action : either change your unit cell (rprim or angdeg),
or set chkprim to 0.

leave_new : decision taken to exit ...
```

By default abinit checks that the unit cell is primitive i.e. that it contains the smallest possible number of atoms.
For example, instead of the conventional cubic FCC unit cell with 4 atoms

```
rprim
1 0 0
0 1 0
0 0 1
natom 4
xred
0 0 0
0 0.5 0.5
0.5 0 0.5
0.5 0.5 0
```

you should be using the following unit cell vectors:

```
rprim
0.0 0.5 0.5
0.5 0.0 0.5
0.5 0.5 0.0
natom 1
xred
0 0 0
```

to get a primitive unit cell with 3 axes separated by angles of 60 degrees.
Another possibility is:

```
angdeg 3*60.
natom 1
xred
0 0 0
```

Using the primitive cell ensures the **fastest calculation** and the **best use of symmetry operations**,
so in general you should listen to abinit and reduce your unit cell.
If you have a good reason not to use the primitive cell, you can override abinit and force it to use a non-primitive unit cell,
by setting [[chkprim]] 1 in the input file.

One possible reason to do this is, for example, making a large supercell of a crystal (say 3x3x3 primitive unit cells)
in which you want to introduce a defect. 
Doing the pristine crystal calculation in the 3x3x3 supercell is possible, but not useful 
(you will just get 27 times the energy). Moreover, the pure translation symmetries will be incorporated in the symmetry recognition algorithm,
and you might end up with a number of symmetry operations that exceeds ABINIT internal maximum.
Once you have introduced the defect, of course,
you will lower the supercell's symmetry and abinit will no longer complain that the cell is not primitive.
[[chkprim]] 1 is no longer to be used for the defected cell.

## pspxc from pseudopotential not equal to ixc

Abinit complains if the exchange correlation functional (variable [[ixc]])
used in the input is not the same as that specified in the pseudopotential / atomic data files used.

```
 pspatm: WARNING -
  Pseudopotential file pspxc=       7,
  not equal to input ixc=       1.
  These parameters must agree to get the same xc
  in ABINIT code as in psp construction.
  Action : check psp design or input file.
  Assume experienced user. Execution will continue.
```

This is usually dangerous, as you are making uncontrollable errors in compensating for the Vxc of the core
in the pseudopotential with a different functional.

However, there is an important exception: all the LDA functionals (ixc 1-7) are basically identical,
but with different functional forms to fit the same data.
As a result, mixing these ixc is mostly harmless (as above).
The warning appears often, as the many pseudopotentials on the web site are created with a variety of ixc,
not necessarily the default value 1.

## ABINIT does not find the spacegroup I would expect

Firt of all, make sure that the atomic positions are given with enough digits (let's say more than six digits).
Prefer [[xred]] over [[xcart]] and remember that one can use fractions in the input file using the syntax:

```
natom 3
xred
0   0   0
1/2 2/3 1/2
2/3 1/3 1/2
```

Note that whitespaces between tokens are not allowed i.e. ` 1 / 2` is not accepted by the parser.

Also, the lattice can be specified in terms of [[angdeg]] and [[acell]] rather than [[rprim]].
This may help solve possible problems with **hexagonal** or **rhombohedral** lattices that
seem to be more sensitive to truncation errors.

If this does not solve your problem, you may try to gradually increase the value of [[tolsym]].

The |abistruct| script provides a simplied command line interface that allows you
to invoke Abinit compute the space group:

```
abistruct.py abispg FILE --tolsym=1e-8
```

It is also possible to use the |spglib| library with the command:

```
abistruct.py spglib FILE
```

although spglib does not take into account the [[symafm]] magnetic symmetries.
---
description: How to define frequency meshes (on the imaginary and real axes) for MBPT calculations
authors: MG
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to define frequency meshes (on the imaginary and real axes) 
for MBPT calculations with the ABINIT package.

## Introduction

In the contour deformation technique, and other cases, one has to define a set
of frequencies on which different numerical operations will be performed
(computation of susceptibility matrices, integration along the real+imaginary
axes). In the present topic, the input variables connected to such definitions
are gathered.



## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

## Tutorials

* [[tutorial:gw2|The second tutorial on GW]] deals with the computation of the quasi-particle band structure of Aluminum, in the GW approximation (so, much better than the Kohn-Sham LDA band structure) without using the plasmon-pole model. 

---
description: How to set parameters for a PAW calculation
authors: FJ
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to set parameters for a PAW calculation with the ABINIT package.

## Introduction

The PAW atomic data can be used with plane waves as well as with wavelets.
Specificities of PAW for use with planewaves are presented here. See
[[topic:Wavelets]] for its use with wavelets.

The way the PAW method is implemented with planewaves in ABINIT is described
in [[cite:Torrent2008]].  
The use of PAW atomic data (equivalent to pseudopotential file for the norm-
conserving case) automatically launch a PAW calculation. ABINIT is provided
with the JTH [[cite:Jollet2014]] PAW atomic data table on the ABINIT web site.  
To perform a standard PAW calculation, the input file is the same than for a
norm-conserving one, except that the variable [[pawecutdg]] must be specified
(see below). In the case the input variable [[accuracy]] is used, the input
variable [[pawecutdg]] is automatically used.  
Some physical functionalities are available only in the PAW framework: DFT+U,
DMFT, local exact exchange,...



## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

## Tutorials

* The tutorial on the use of PAW ([[tutorial:paw1|PAW1]]) presents the Projector-Augmented Wave method, implemented in ABINIT as an alternative to norm-conserving pseudopotentials, with a sizeable accuracy and CPU time advantage.
* The tutorial on the generation of PAW atomic data files ([[tutorial:paw2|PAW2]]) presents the generation of atomic data for use with the PAW method. Prerequisite: PAW1.
* The tutorial on the validation of a PAW atomic datafile ([[tutorial:paw3|PAW3]]) demonstrates how to test a generated PAW dataset using ABINIT, against the ELK all-electron code, for diamond and magnesium. Prerequisite: PAW1 and PAW2.

---
description: How to perform calculation with constrained atomic magnetic moments
authors: EB
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to perform calculation with constrained atomic magnetic moments with the ABINIT package.

## Introduction

A complementary magnetic constraint method has been implemented in the ABINIT
code, wherein the magnetization around each atom $I$ is pushed to a desired
(vectorial) value. The constraint can either be on the full vector quantity,
$\vec{M}_I$, or only on the direction $\vec{e}_I$. This is mainly useful for non
collinear systems, where the direction and amplitude of the magnetic moment
can change. The method follows that used in
<!--- Quantum Espresso [[cite:Moscaconte2007]] and -->
VASP [[cite:Ma2015]]: a Lagrangian
constraint is applied to the energy, and works through a resulting term in the
potential, which acts on the different spin components. The magnetization in a
sphere  $\Omega_I$ around atom $I$ at position $\vec{R}_I$ is calculated as:

$$ \vec{M}_I = \int_{\Omega_I} \vec{m}(\vec{r}) F_I(|\vec{r}-\vec{R_I}|) d\vec{r} $$

and the corresponding potential for spin component $\alpha$  is written as:

$$V_{\alpha}(\vec{r}) = -2 \lambda F_I(|\vec{r}-\vec{R_I}|) \vec{c}  \vec{\sigma}_{\alpha}. $$

The function $F_I$ is zero outside of a sphere of radius [[ratsph]] for atom $I$,
and inside the sphere,  the function $f(x) = x^2(3+x(1+x(-6+3x)))$ (see Eq. (B4) in [[cite:Laflamme2016]]),
is applied to smooth the transition in a region of thickness $r_s$ (fixed to 0.05 bohr), otherwise it is 1.
This minimizes discontinuous variations of the potential from iteration to iteration.

The constraint is managed by the keyword [[magconon]]. Value 1 gives a
constraint on the direction:

$$\vec{c}  = (\vec{M}_I/|\vec{M}_I|)-(\vec{M}^{spinat}_I / |\vec{M}^{spinat}_I|),$$

while value 2 gives a full constraint on the vector 

$$\vec{c}  = \vec{M}_I - \vec{M}^{spinat}_I,$$

where in both cases $\vec{M}^{spinat}_I$ defined by [[spinat]], giving a 3-vector magnetic potential for
each atom. The latter is quite a stringent constraint, and often may not
converge. The former value usually works, provided sufficient precision is
given for the calculation of the magnetic moment (kinetic energy cutoff in
particular).

The strength of the constraint is given by the keyword [[magcon_lambda]] ($\lambda$
above - real valued). Typical values are $10^{-2}$ but vary strongly with system
type: this value should be started small (here the constraint may not be
enforced fully) and increased. A too large value leads to oscillations of the
magnetization (the equivalent of charge sloshing) which do not converge. A
corresponding Lagrange penalty term is added to the total energy, and is
printed to the log file, along with the effective magnetic field being
applied. In an ideal case the energy penalty term should go to 0 (the
constraint is fully satisfied).

## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

---
description: How to specify bands and occupation numbers, for metals or insulators
authors: FJ
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to specify bands and occupation numbers, 
for metals or insulators with the ABINIT package.

## Introduction


Metallic as well as insulating systems can be treated, depending on the value
of [[occopt]]. The default value of [[occopt]] corresponds to an insulator (or
finite molecule): the number of bands (or states for a molecule) is deduced
from the number of electrons brought by each pseudopotential ion, and then all
the bands are occupied (by two electrons in case of a non-spin-polarized
system, or by one electron in the case of a spin-polarized system), and a small
number of empty bands are added, e.g. to obtain the band gap.

For a metallic system, use a value of [[occopt]] between 3 and 7. ABINIT will
compute a default number of bands, including some nearly unoccupied ones, and
find the occupation numbers. The different values of [[occopt]] correspond to
different smearing schemes (smearning defined by [[tsmear]] for defining the
occupation numbers, e.g. Fermi broadening, the Gaussian broadening, the
Gaussian-Hermite broadening, as well as the modifications proposed by Marzari.
Finite temperatures can also be treated thanks to a smearing scheme
([[cite:Verstraete2002]] scheme) using [[tphysel]].

For gapped materials, the treatment of quasi-Fermi energies in the valence and conduction band,
with populations of electrons (in the conduction bands) and holes (in the valence bands)
is available with [[occopt]]=9.

It is possible to define manually the number of bands (input variable
[[nband]]) as well as the occupation numbers (input variable [[occ]]). This
might be useful to perform a Δ-SCF calculation for excited states.



## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

## Tutorials

* [[tutorial:base1|The tutorial 1]] deals with the H2 molecule: get the total energy, the electronic energies, the charge density, the bond length, the atomisation energy
* [[tutorial:base2|The tutorial 2]] deals again with the H2 molecule: convergence studies, LDA versus GGA
* [[tutorial:base3|The tutorial 3]] deals with crystalline silicon (an insulator): the definition of a k-point grid, the smearing of the cut-off energy, the computation of a band structure, and again, convergence studies ...
* [[tutorial:base4|The tutorial 4]] deals with crystalline aluminum (a metal), and its surface: occupation numbers, smearing the Fermi-Dirac distribution, the surface energy, and again, convergence studies ...
---
description: How to perform a molecular dynamics calculation
authors: GG
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to perform a molecular dynamics calculation with the ABINIT package.

## Introduction

Three molecular dynamics algorithm (Numerov, Verlet, Blanes and Moanes) allow
to perform simulations in real (simulated) time, see [[ionmov]]. The
displacement of atoms may be computed according to Newton's law, or by adding
a friction force to it. Nose-Hoover thermostat is available with Verlet
algorithm. Langevin dynamics is also available.

Specified lattice parameters, or angles, or atomic positions, can be kept
fixed if needed, see [[topic:GeoConstraints]].

The trajectories can be analyzed thanks to the [[topic:APPA|APPA postprocessor]].


## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

## Tutorials

* [[tutorial:paral_moldyn]] Parallelism for molecular dynamics calculations

---
description: How to obtain a Scanning Tunneling Microscopy map
authors: XG
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to obtain a Scanning Tunneling Microscopy map with the ABINIT package.

## Introduction

The charge density of states close to the Fermi energy can be output, and
provide a simple approach to STM image calculations (not for PAW). See input
variable [[prtstm]] and [[stmbias]] for explanations. The example input file
will also be useful. The angular momentum decomposed DOS can be evaluated in a
sphere around any point, and provide STS analysis.



## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

---
description: How compute the frequency-dependent susceptibility matrix, and related screened interaction matrix, and inverse dielectric marix
authors: MG
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to compute the frequency-dependent susceptibility matrix, and related screened
interaction matrix, and inverse dielectric marix with the ABINIT package.

## Introduction

In the independent-particle approximation, the frequency-dependent
susceptibility matrix, and related screened interaction matrix, and inverse
dielectric matrix can be computed.

This can be done on top of eigenfunctions and eigenvalues obtained from Kohn-
Sham, generalized Kohn-Sham (e.g. hybrid functionals), as well as self-
consistent quasiparticle methodology in the (generalized) Kohn-Sham basis.

This is a prerequisite to many-body perturbation theory calculations, see
[[topic:GW]] and [[topic:BSE]], to which we refer.

The frequency meshes, used e.g. for integration along the real and imaginary
axes, on which the susceptibility matrices (and related matrices) have to be
computed are described in [[topic:FrequencyMeshMBPT]].



## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

---
description: How to perform a geometry optimization
authors: GG
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to perform a geometry optimization with the ABINIT package.

## Introduction

Different algorithms (Broyden; modified Broyden; Verlet with sudden stop of
atoms, FIRE) allows to find the equilibrium configuration of the nuclei, for which
the forces vanish, see [[ionmov]]. The cell parameters can also be optimized
concurently with the atomic positions [[optcell]], possibly with a state of
given stress as a target, [[strtarget]].

Specified lattice parameters, or angles, or atomic positions, can be kept
fixed if needed, see [[topic:GeoConstraints]].

A genetic algorithm has been coded, for global optimisation. See [[ga_rules]]. Not in production yet. 

The results of the structural relaxation are saved in the HIST.nc file.
This file can be used to restart the calculation or to analyze the results at the end of calculation.
For further details about the |AbiPy| API please consult the |HistFileNb|.

When performing cell parameter optimization, the default setting of ABINIT will not permit increase
of the lattice parameters from the initial ones. One needs to set [[dilatmx]] to some value larger
than the default 1.0 . If the starting lattice parameters are thought to be accurate and should
not increase by more than 5%, set [[dilatmx]] to 1.05 , which will lead to a moderate increase
of CPU time by about 15% . If the starting lattice parameters are just a guess, perform a first run
with [[chkdilatmx]]=0 . This will yield an estimate of the correct lattice parameters, but such estimate are 
not the numerically exact ones for the [[ecut]] value. 
You are done if you do not need accurate geometry estimation anyhow. Otherwise,
after this first run, perform a second run by restarting from the newly estimated value, 
with [[chkdilatmx]]=1 and [[dilatmx]] to 1.05 .


## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

---
description: How to specify the unit cell
authors: FJ
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to specify the unit cell with the ABINIT package.

## Introduction

ABINIT needs three dimensioned non-coplanar vectors, forming the unit cell, to
set up the real space lattice.

An initial set of three vectors, specified in real space by [[rprim]] or as
unit vectors with angles [[angdeg]], are dimensioned in a second step using
scaling factors as specified by [[acell]] or by rescaling their cartesian
coordinates, as specified by [[scalecart]]. Internally, only the final result,
[[rprimd]] matters. The most detailed explanation can be found by looking at
[[rprim]].

Note that ABINIT expects the mixed product of the three vectors (R1xR2).R3 to
be positive. If it is not the case, exchange two of them with the associated reduced coordinates.
More information about the way the real space lattice, the reciprocal lattice,
and symmetries are defined in ABINIT can be found [[pdf:geometry|here]].

Also note that that Abinit space group routines uses by default strict tolerances for the 
recognition of the symmetry operations. 
This means that lattice vectors and atomic positions must be give with enough figures 
so that the code can detect the correct space group.
This is especially true in the case of hexagonal or rhombohedral lattices.
Remember that one can specify rational numbers with the syntax:

    xred 1/3 1/3 1/3

instead of the less precise:

    xred 0.33333 0.33333 0.33333


Using [[acell]] and [[angdeg]] instead of [[rprimd]] may solve possible issues 
with the space group recognition.

If your input parameters correspond to a high-symmetry structure but the numerical values at hand
are *noisy*, you may want to increase the value of [[tolsym]] in the input file  
so that Abinit will resymmetrize automatically the input parameters.

Finally, one can use the [[structure]] input variable to initialize the crystalline geometry 
from an external file.
For instance, it is possible to read the crystalline structure from an external netcdf file or 
other formats such as POSCAR without having to specify the value of 
[[natom]], [[ntypat]], [[typat]] and [[znucl]].


**Smart symmetriser**

ABINIT has also a smart symmetriser capability, when [[spgroup]]!=0 and
[[brvltt]]=-1. In this case, the CONVENTIONAL unit cell must be input through
the usual input variables [[rprim]], [[angdeg]], [[acell]] and/or
[[scalecart]]. ABINIT will fold the conventional unit to the primitive cell,
and also generate all the nuclei positions from the irreducible ones. 
See [[topic:SmartSymm]].


## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

## Tutorials

* [[tutorial:base4|Fourth basic tutorial]] Determination of the surface energy of aluminum (100): changing the orientation of the unit cell.

---
description: How to become convinced that results are numerically correct
authors: XG
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to become convinced that results are numerically correct with the ABINIT package.

## Introduction

One can never be sure that an implementation of a complicated formalism is
numerically correct. However, several studies have helped to gain confidence
in selected properties computed within ABINIT, by making detailed comparisons
with independently developed software applications.

The ground state total energy, geometry relaxation, phonon frequencies,
electron-phonon matrix elements, temperature dependence of the electronic gap
has been cross checked with Quantum Espresso and YAMBO. See [[cite:Ponce2014]].

It is possible to choose the same convention for the definition of the average
eletrostatic potential, than Quantum Espresso, allowing verification of
results for charged systems. See [[cite:Bruneval2014]].

See the "Validation" section in [[topic:PseudosPAW]].



## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

---
description: How to manipulate atoms and groups of atoms to generate the set of atomic positions
authors: FJ
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to manipulate atoms and groups of atoms to generate 
the set of atomic positions with the ABINIT package.

## Introduction

ABINIT has some (non-graphical) capabilities to manipulate atoms and group of
atoms, to help establishing the set of atomic positions in the input file when
big cells are considered.

Explicitly, one or two groups of atoms, forming e.g. a molecule, a cluster, or
a small primitive cell, can be repeated in arbitrary number, then translated
and rotated. Then, atoms can be removed, to form e.g. vacancies. See [[nobj]]
as entry point.

The related input variables being used for preprocessing of the input file,
they are not echoed in the output file ([[INPUT_ONLY]] characteristics).



## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

---
description: How to perform some artificial modifications of the physics
authors: XG
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to perform some artificial modifications of the physics with the ABINIT package.

## Introduction

With a computer, one can consider non-physical modifications of the system to
help the understanding the physics.

The nuclear masses and free electron mass can be modified. A fake space
dependent chemical potenial can be added, see [[chempot]], as well as the
electrostatic potential of a slab, [[jellslab]]. A simple sine/cosine
potential with a wavevector compatible with the (super)call can also be added,
see [[qprtrb]].


## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

---
description: How to master the use of norm-conserving pseudopotentials and PAW atomic data, and their consequences
authors: XG
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to master the use of norm-conserving pseudopotentials and PAW atomic data, and
their consequences with the ABINIT package.

## Introduction

ABINIT can read many different types of norm-conserving (NC) pseudopotentials and
PAW atomic data. The names of the files to be used in an ABINIT run are to be
provided in the input file, one for each [[topic:AtomTypes|type of atom]]. 
The list of pseudopotentials with the [[pseudos]] input variable and (optionally)
the directory where all pseudos are located with [[pp_dirpath]].

Note that one **cannot mix** NC pseudopotentials with PAW atomic data files in a single ABINIT run, 
even for different datasets. One has to stick either to NC pseudopotentials or to PAW.
The ultrasoft formalism is not implemented.

**Norm-conserving pseudopotential tables**

There are several sets of NC pseudopotentials available for most
elements of the periodic table, either in LDA or in GGA-PBE. The
[[https://www.abinit.org/psp-tables|recommended one]]
(GGA-PBE, [[ixc]]=11) comes from the ONCVPSP generator.
For the formalism please consult [[cite:Hamann2013]]. 
The generation and validation of the table is discussed in [[cite:Setten2018]].

[Other tables](https://www.abinit.org/atomic-data-files) are also available.
The Goedecker HGH table (e.g. LDA, [[ixc]]=1), also including spin-orbit
is rather accurate, but requires often large
[[ecut]]. The old Troullier-Martins type tables 
are deprecated, because not accurate enough, and also not
including spin-orbit coupling. The different formats for norm-conserving
pseudopotentials are described in the Pseudos page of the [developers section](/developers/psp8_info/)
A much more flexible and powerful file format (PSML) has been recently proposed in [[cite:Garcia2018]].
The format is supported by Abinit via the psml library that must be activated by user during 
the configuration of the package.

!!! important

    Abinit can read NC pseudos in UPF1 format but not pseudos written following the 
    more recent UPF2 specifications.
    Also, ultrasoft or PAW pseudos in UPF1/UPF2 format are not supported.

**PAW atomic data tables**

Five large sets of PAW atomic data can be read by ABINIT. The
[[https://www.abinit.org/psp-tables|JTH table]]
is recommended (GGA-PBE, [[ixc]]=11).                
It uses the PAW-XML format.
[Others](https://www.abinit.org/atomic-data-files) are also available.

**Validation**

Many pseudopotentials (NC as well as PAW) have been tested
against all-electron calculations, see the section "Documents and tools"
paragraph about the "Delta" project on
[[https://www.abinit.org/downloads/PAW2|the ABINIT web site PAW
page]]. In [[tutorial:paw3]] it is explained How to perform more detailed
comparisons with an all-electron code.

**Generation**

Six codes are available to generate new pseudopotentials when needed, see
[[https://www.abinit.org/atomic-data-files|the related ABINIT web
page]]. For norm-conserving pseudopotentials, ONCVPSP is the preferred one.
The ultra-soft pseudopotentials generated by "USPP" are approximations of the
Projector Augmented Wave approach, and are treated within the PAW framework in
ABINIT. The ATOMPAW generator is actually tightly connected to ABINIT such
that there are ABINIT automatic tests in which a PAW atomic data is generated,
and then used in ABINIT. There is also a dedicated tutorial to ATOMPAW/ABINIT.

**Miscellaneous**

Norm-conserving pseudopotentials can be mixed, to generate "alchemical"
pseudoatoms, see [[topic:AtomTypes]].
This feature is not available for PAW.

## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

## Tutorials

* [[tutorial:paw2|Second tutorial on the projector-augmented wave technique]] The generation of atomic data. 
* [[tutorial:paw3|Third tutorial on the projector-augmented wave technique]] Testing PAW datasets against an all-electron code.

---
description: How to bound a model in multibinit
authors: AM,ACGC
---

This page gives hints on how to bound a model in multibinit.

## Introduction

When a model is build, it can contains phonons instabilities and may produce a divergence during the dynamics.
To avoid such behaviour,  multibinit includes an automatic procedure to bound the model.

Multibinit will first generate a set of coefficients with a power range defined by [[multibinit:bound_rangePower]] and keep only the coefficients with even power.
Then, the procedure is similar to the fit process with the constrains to only keep positive coefficients. Next, the bounding process will add the new coefficients to the model up to [[multibinit:bound_maxCoeff]] checking if the model is bound in each step of the process.

## Tutorials
The [[tutorial:lattice_model|First lesson on Multibinit]] explains how to build a lattice model and to run a dynamics.
  
## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

---
description: How to select the SCF algorithm
authors: XG
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to select the SCF algorithm with the ABINIT package.

## Introduction

Self-Consistent Field calculations allow to determine the solution of the
Kohn-Sham equations, ending with converged "self-consistent" wavefunctions,
density, and Kohn-Sham potentials. 

Different algorithms can be chosen to
converge to the solution of this set of equations.
The input variables [[iscf]] governs 
the choice of density/potential
self-consistency algorithms, while [[wfoptalg]] focuses on the determination
of the wavefunction through the solution of the Schrodinger equation with fixed
Kohn-Sham potential.

In the first class of algorithms, selected by [[iscf]]), Pulay
mixing is one of the most efficient. Also, an efficient preconditioner will
speed up the convergence. Among different choices, a generalized Kerker
preconditioner is implemented, see [[diemac]], [[diemix]] and [[dielng]].  
In order to perform a non-self-consistent calculations of wavefunctions and
corresponding eigenvalues in a fixed potential, as for representing a full
band structure, the loop over density/potentials self-consistency must be
disabled, for which [[iscf]]=-2 must be chosen.

Among the algorithms to find the wavefunctions, selected by [[wfoptalg]], the
conjugate-gradient and the LOBPCG ones are the favourite. 
The RMM-DIIS algorithm is faster, but might be unstable. Use it for molecular dynamics run
or long geometry optimizations. Use the Chebyshev filtering for massive parallel runs.

Inner electronic eigenvalues can be computed thanks to the minimisation of the
residual with respect to a target energy value, see [[eshift]].


## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

---
description: How to tune the output of computed quantities
authors: FJ
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to tune the output of computed quantities with the ABINIT package.

## Introduction

The following input variables can be used to trigger or modify the output of
some quantities in the main output file.


## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

---
description: How to unfold supercell band structures
authors: OR
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to unfold supercell band structures with the ABINIT package.

## Introduction

Supercells are often used in ab initio calculations to model compound alloys,
surfaces and defects. Band structure plots obtained from supercell
calculations are difficult to interpret and compare to the reference band
structure of a corresponding primitive crystallographic unit due to the zone
folding. The purpose of the executable file [[help:fold2bloch]] is to assist
with interpretation of supercell electronic structure calculations by
recovering the Bloch character of electronic eigenstates perturbed by
disorder. In general, there is no single wave-vector **k** associated with a
particular eigenvalue E, but rather a distribution of k's. This distribution
is also known as a Bloch spectral density or spectral weight. The spectral
weight w n( **k** ) amounts to a Bloch **k** -character of the n'th energy
eigenstates εn and fulfills the normalization ∑ **k** wn( **k** )=1. In the
case of a non-local basis set, such as plane waves, the spectral weight can be
constructed from the Fourier expansion coefficients by gathering them in
groups associated with a particular Bloch wave vector in the primitive cell
[[cite:Wang1998]], [[cite:Popescu2010]], [[cite:Popescu2012]],
[[cite:Allen2013]], [[cite:Medeiros2014]]

wn( **k** )= ∑ **G** |C n, **K** ( **G** )|2 for all **G** that fulfill **K**
+ **G** = **k** + **g** ,

where Cn, **K** ( **G** ) refers to plane wave coefficients for nth eigenstate
with the wave vector **K** within the first Brillouin zone (BZ) of the
supercell, **G** and **g** are the reciprocal lattice vectors of the supercell
and primitive cell, respectively. More technical details on this unfolding
procedure are available in [[cite:Rubel2014]]. It should be noted that the
primitive and supercell reciprocal lattices should be commensurate as required
by the above equation. This condition implies that a supercell needs to be
generated by translation of the primitive cell along its lattice vectors in
real space using a desired multiplicity N x×Ny×Nz . Care should be taken when
working with lattices whose conventional unit cell is different from the
primitive cell (e.g., zinc-blende or rock salt structures).

A prerequisite for unfolding is the presence of wave function files (e.g.,
abo_WFK) that can be generated in a way similar to standard band structure
calculations. The only difference from the conventional band structure
calculation is the k-path selection approach. For instance, when aiming for
the band structure plot with Γ(0,0,0)-Z(0,0,1/2) path in the primitive
Brillouin zone (BZ), the [[kptbounds]] in supercell BZ should include
Z-(0,0,-1/2)-Γ(0,0,0)-Z(0,0,1/2). At first, the part Z--Γ seems redundant
lying outside of the range of interest. However, it is easy to show that those
wave vectors contribute to the range Γ-Z after unfolding. For instance, let us
consider a k-point (0,0,-K) in the 1×1×3 supercell. This point ``unfolds" into
3 k-points in the primitive BZ: (0,0,(-K-1)/3), (0,0,-K/3), and
(0,0,(-K+1)/3). The first two points are irrelevant for the purpose of the
plot as they do not belong to the Γ-Z segment, but the last point falls into
the range of interest and needs to be included.

Once the wave function file is prepared, the unfolding proceeds by invoking
fold2Bloch abo_WFK Nx:Ny:Nz that generates an output file abo_WFK.f2b. The
output file contains 5 columns: the list of unfolded wave vectors (kx,ky,kz)
in the primitive BZ, the corresponding eigenvalue εn (Ha) and the Bloch
spectral weight wn.

A MATLAB script _ubs_dots.m_ is designed to assist with plotting the unfolded
band structure. The unfolded band structure plot is represented as a scatter
plot where the size and colour of the markers carry information about the
Bloch character of energy bands. Users are asked to provide details about the
k-path, name of the fold2Bloch output file abo_WFK.f2b, position of the Fermi
energy, and reciprocal lattice vectors that can be found in the main output
file ab_out.out.



## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

## Tutorials

* [[tutorial:fold2bloch|The tutorial on unfolding supercell band structures]]

---
description: How to perform orbital-free calculations
authors: FJ
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to perform orbital-free calculations with the ABINIT package.

## Introduction

It is possible to use Thomas-Fermi kinetic functional (explicit functional of
the density) or Thomas-Fermi-Weizsacker kinetic functional (with Gradient
Corrections) instead of Kohn-Sham kinetic energy functional (implicit
functional of the density through Kohn-Sham wavefunctions).  
See [[cite:Perrot1979]].  
The Recursion Method may be used in order to compute electronic density,
entropy, Fermi energy and eigenvalues energy. This method computes the density
without computing any orbital, is efficient at high temperature, with a
efficient parallelization (almost perfect scalability).

At present, it only works for local pseudopotentials, severely restricting the
use of this method.



## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

---
description: How to enable the Extended FPMD method for high temperature simulations
authors: A. Blanchet
---
<!--- This is the source file for this topics. Can be edited. -->
This page gives hints on how to perform calculation with the extended FPMD model.

## Introduction

Extended First Principle Molecular Dynamics (Ext. FPMD) method allows to perform high temperature simulations from few Kelvins to thousands of eVs, by drastically reducing the needed number of bands for high temperature simulations. The implementation and usage will be described in an upcoming paper which is currently under review (Authors: *A. Blanchet, J. Clérouin, M. Torrent, F. Soubiran*).

High energy orbitals are replaced with pure single plane waves description based on the Fermi gas model. Bands from 1 to [[nband]] are treated with the complete plane waves basis set as usual, and the rest of occupied bands from [[nband]] to the infinity are treated with the Fermi gas model. Contributions to the energy, entropy, stresses, number of electrons and chemical potential are computed automatically after enabling the model with variable [[useextfpmd]].

Contributions to the number of electrons and to the energy are explicitly shown in the *_GSR.nc* output file with key **nelect_extfpmd** and **e_extfpmd** (**edc_extfpmd** for the double counting term). The energy shift factor (resulting from the constant background potential) is also printed in the *_GSR.nc* output file with key **shiftfactor_extfpmd**.

## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}
---
description: How to perform a PIMD calculation
authors: GG
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to perform a PIMD calculation with the ABINIT package.

## Introduction

Path-Integral Molecular Dynamics (PIMD) is a technique allowing to simulate
the quantum fluctuations of the nuclei at thermodynamic equilibrium
[[cite:Marx1996]]. It is implemented in ABINIT in the NVT ensemble since v7.8.2.

In the Path-Integral formalism of quantum statistical mechanics, the (quantum)
nuclei are replaced by a set of images (beads) treated by means of classical
mechanics, and interacting with each other through a specific effective
potential. In the limit of an infinite number of beads, the quantum system and
this many-beads classicle system have the same partition function, and thus
the same static observables. In PIMD, the classical system of beads is
simulated by standard Molecular Dynamics. The PIMD equations of motion are
integrated by using the Verlet algorithm. At each time step, a ground state
DFT calculation is performed for each image. PIMD can be used with any XC
functional and works in the PAW framework as well as in the norm-conserving
pseudopotential (NCPP) case.

PIMD in ABINIT follows the set of numerical schemes developed by several
authors in the 90's [[cite:Marx1996]], [[cite:Tuckerman1996]]. PIMD in the
canonical ensemble needs specific thermostats to ensure that the trajectories
are ergodic: the Nose-Hoover chains are implemented, as well as the Langevin
thermostat (controlled by the value of [[imgmov]]). Also, it is possible to
use coordinate transformations (staging and normal mode), that are controlled
by the keyword [[pitransform]]. In standard equilibrium PIMD, only static
observables are relevant (quantum time-correlation functions are not
accessible): the masses associated to the motion of the beads are controlled
by the keyword [[pimass]], whereas the true masses of the atoms are given by
[[amu]]. The values given in [[pimass]] are used to fix the so-called
fictitious masses [[cite:Marx1996]]. In the case where a coordinate
transformation is used, the fictitious masses are automatically fixed in the
code to match the so-called staging masses or normal mode masses. The number
of time steps of the trajectory is controlled by [[ntimimage]], the initial
and thermostat temperature by [[mdtemp]]. Except if specified, the images in
the initial configuration are assumed to be at the same position, and a random
distribution of velocities is applied (governed by [[mdtemp]]) to start the
dynamics.

At each time step, ABINIT delivers in the output file:

* (i) information about the ground state DFT calculation of the ground state for each image
* (ii) the instantaneous temperature, the instantaneous energy as given by the primitive and virial estimators, and the pressure tensor as given by the primitive estimator.

The number of images (keyword [[nimage]]) is associated to a specific
parallelization level (keyword [[npimage]]).

PIMD has been used with ABINIT to reproduce the large isotope effect on the
phase transition between phase I and phase II of dense hydrogen
[[cite:Geneste2012]], and also some aspects of diffusion at low and room
temperature in proton-conducting oxides for fuel cells [[cite:Geneste2015]].
PIMD in the NPT ensemble is not available yet.



## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

---
description: How to calculate crossing barriers
authors: GG
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to calculate barriers for crossings with the ABINIT package,
as well as ensemble DFT, or the pSIC approach to polaron formation.

## Introduction

The knowledge of geometries at which crossings between two electronic states happen,
with minimal energy, or geometries at which the energy difference between the ground state and the excited
state is small, and the energy is still low, plays an important role
in the study of non-radiative transitions.

It is possible to formulate the search for such geometries in terms of minimisation
of a functional that is the linear combination of the energy of the two states at the same geometry, 
with Lagrange multipliers [[cite:Jia2019]].
This is also related with a simple approach to Ensemble DFT: just make a linear combination of the DFT energies, the XC correlation
energy being not computed with a single common density, but from each density separately.
Also, the pSIC, polaron self-interaction corrected method [[cite:Sadigh2015]], [[cite:Sadigh2015a]], can be formulated in the same terms.

In ABINIT, with [[imgmov]]==6, it is possible to deal with such 
linear combination of systems with the same geometry, but differing occupation factors [[occ]], and even 
with different [[cellcharge]]..
It is possible to find the geometry at which the resulting energy is minimal, for a given value of the mixing factors [[mixesimgf]].
Set [[nimage]]=2, and set the occupation numbers for image 1 to the ground-state occupations, and for image 2 to the excited-state occupations.

## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

## Tutorials

---
description: How to perform macroscopic averages of the densities and potentials
authors: XG
---
<!--- This is the source file for this topics. Can be edited. -->

This page explains how to perform macroscopic averages of the densities and potentials with the ABINIT package.

## Introduction

The MACROAVE program implements the macroscopic average technique,
introduced by A. Baldereschi et al [[cite:Baldereschi1988]].
This powerful method relates microscopic quantities, typical outputs of first-principles codes,
with macroscopic magnitudes, needed to perform electrostatic analysis.
Within this methodology, one is able to wash out all the
wiggles of the rapidly-varying functions of position (resembling the underlying atomic structure) 
of the microscopic quantities,
blowing up only the macroscopic features.
It can be used to compute band offsets, work functions, effective
charges, and high frequency dielectric constants, among others.

See also [[cite:Colombo1991]] as well as [[cite:Shaltaf2008]], the latter for an application using 
the MACROAVE program delivered with ABINIT, see Fig.1 .

To use the MACROAVE program, prepare an input file named "macroave.in" . No other name is admitted.
The examples below should be renamed "macroave.in". 
Issue simply "macroave", without any argument. Two files will be generated, whose names
are constructed from the name of the density or potential file in the third line of the input macroave.in file,
let us take t41_DEN as an example.
The first one will have been appended with .PAV , which is an abbreviation for Planar AVerage.
Only the x-y average has been done to generate this file. From t41_DEN, one generates t41_DEN.PAV.
The second one will have been appended with .MAV , which is an abbreviation for Macroscopic AVerage.
The x-y average has been followed by the macroscopic average to generate this file. From t41_DEN, one generates t41_DEN.MAV.
Note that the planar average is always done on the x-y plane, and the macroscopic average in the z direction.

Note that macroave works in reduced coordinates, so it will not encounter problems with non-orthogonal primitive vectors.

## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

---
description: How to perform a molecular dynamics calculation with Multibinit
authors: AM
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to perform a molecular dynamics calculation with the Multibinit package.

## Introduction

Multibinit uses the molecular dynamics algorithms implemented in the Abinit software (see [[topic:MolecularDynamics|Molecular dynamics in Abinit]]). For now, only NPT ([[ionmov]]=13) and NVT ([[ionmov]]==12) simulations are available.
  
## Tutorials
The [[tutorial:lattice_model|First lesson on Multibinit]] explains how to build a lattice model and to run a dynamics.

## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}



---
description: How to fit the anharmonic part of a model in multibinit
authors: AM,ACGC
---

This page gives hints on how to fit the anharmonic part of a model in multibinit.

## Introduction

The fit process implemented in multibinit is based on [[cite:Escorihuela-Sayalero2017]].
The fit process of multibinit contains two main options:

* Generate the list of symmetry-adapted terms
* Select the best coefficients in the list with the fit process
  
In the first option, multibinit will generate a set of coefficients only if [[multibinit:fit_generateCoeff]] is set to one. This generation is mainly parametrized by [[multibinit:fit_rangePower]] and [[multibinit:fit_cutoff]]. You can avoid the generation by providing a list of coefficients with the model_anharmonic.XML file (see [[help:multibinit]]).


Then, the fit process will select the coefficients one by one up to [[multibinit:fit_ncoeff]] according to the procedure details in [[cite:Escorihuela-Sayalero2017]]. This process requires to provide the training_set_HIST.nc list file (see [[help:multibinit]])
  

## Tutorials
The [[tutorial:lattice_model|First lesson on Multibinit]] explains how to build a lattice model and to run a dynamics.
    
## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

---
description: How to calculate transition paths
authors: GG
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to calculate transition paths with the ABINIT package.

## Introduction

Similarly to the PIMD, finding minimum energy paths and in particular
transition states for chemical transformations is of great importance in many
different fields. In ABINIT we have implemented two different flavours based
on interpolation methods [[cite:Weinan2007]], [[cite:Henkelman2000]],
[[cite:Mills1994]] and controlled by the keyword [[imgmov]].

The calculation starts with the knowledge of the initial and final state
(local minima in the configuration space) and an educated guess for the
reaction pathway.

If the reaction path is not given, a linear interpolation between the
reactants and final products is constructed by a series of images
(configurations) that connect the two states, which are given by the keyword
[[nimage]].

The energy path that joins the series of images is then modified at each step
to allow the search over the lowest energy path joining the reactants and
products.

In the Nudged Elastic Band method (NEB), the images are connected through
springs, with a spring constant that has to be chosen such that the images are
uniformly spaced during the path search. The forces on each image come from
the potential energy surface of that configuration and a spring force from the
two closest configurations. The change in images is calculated by projecting
out the true force perpendicular to the path and the parallel projection of
the spring force with respect to the path [[cite:Henkelman2000]].
The spring constant is obtained from the keyword [[neb_spring]] and the number
of iterations is given by [[ntimimage]]. 

In the String method, the system set
up is exactly the same as in the NEB method with the difference that no spring
constant needs to be defined. In this case, the forces are obtained as in the
NEB method from the true force perpendicular but now the configurations are
equally redistributed along the path at each iteration [[cite:Weinan2007]]. In
both methods, the search stops if the number of predefined iterations
([[ntimimage]]) or the tolerance convergence criteria ([[tolimg]]) is reached.

As in the PIMD, each of the images can be treated in parallel and the
requested parallelization mode is set with the keyword [[npimage]].

Specified lattice parameters, or angles, or atomic positions, can be kept
fixed if needed, see [[topic:GeoConstraints]].



## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

## Tutorials

* [[tutorial:paral_images|Parallelism based on "images"]], e.g. for the determination of transitions paths (string method), that can be activated on top of the "KGB" parallelism for force calculations.

---
description: How to compute the polarisation and take into account a finite homogeneous electric field
authors: JZ
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to compute the polarisation and take into account a finite homogeneous electric
field with the ABINIT package.

## Introduction

The effect of an homogeneous static electric field on an insulator may be
treated in ABINIT from two perspectives. One is perturbative, and yields the
susceptibility in the form of the second derivative of the total energy with
respect to the electric field, at zero field strength (see [[topic:DFPT]]).

ABINIT can also be used to compute the effect of an electric field of finite
amplitude, using techniques from the Modern Theory of Polarization
[[cite:Resta1994]],[[cite:Nunes2001]],[[cite:Souza2002]]. The latter is based
on the notion of "Berry phase". In this approach, the total energy to minimize
includes the contribution due to the interaction of the external electric
field with the material polarization **P** Tot, as follows:

E = E0 \- Ω **P** Tot **.E** , where E0 is the usual ground state energy
obtained from Kohn-Sham DFT in the absence of the external field **E** , **P**
Tot is the polarization, made up of an ionic contribution and an electronic
contribution, and Ω the volume of the unit cell.

Some details of the implementation of The Modern Theory of Polarization in
ABINIT are given in [[cite:Gonze2016|the 2016 ABINIT publication]].

In the NCPP case, the electric field has no additional contribution to the
Hellmann-Feynman forces, because the electronic states do not depend
explicitly on ionic position [[cite:Souza2002]]. In the PAW case however, as
the projectors do depend on ion location, an additional force and additional
stresses terms arise [[cite:Zwanziger2012]].

The generalisation to fixed D-field or fixed reduced fields are also
available, as described in M. Stengel, N.A. Spaldin and D. Vanderbilt, Nat. Phys. 5,304 (2009).

The polarization and finite electric field calculation in ABINIT is accessed
through the variables [[berryopt]] and [[efield]]. In addition, displacement
fields and mixed boundary conditions (a mix of electric field and displacement
field) can be computed as well.


## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

## Tutorials

* The [[tutorial:ffield]|tutorial on polarization and finite electric field deals with the computation of the polarization of an insulator (e.g. ferroelectric, or dielectric material) thanks to the Berry phase approach, and also presents the computation of materials properties in the presence of a finite electric field (also thanks to the Berry phase approach).
---
description: How to plot phonon band structures
authors: SS,XG,YG
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to plot phonon band structures with the ABINIT package.

## Introduction

The post-processor band2eps (written in F90, it is one of the executables in
src/98_main) allows one to draw phonon dispersion curves automatically, in a
file written in Encapsulated PostScript (eps). Moreover, a color code allows
to emphasize the contribution of individual atoms to the corresponding
eigenvector (at most three types of atoms).


## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

---
description: How to calculate electric fields gradients and Mossbauer Fermi contact interaction
authors: JZ
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to calculate electric fields gradients 
and Mossbauer Fermi contact interaction with the ABINIT package.

## Introduction

Because the PAW formalism provides a robust way to reconstruct the all-electron 
wavefunctions in the valence space, it is suitable for computing
expectation values of observables localized even very close to the nuclei.
Obtaining equivalent accuracy within the norm-conserving pseudopotential
framework would require very small atomic radii for the pseudization
procedure, and concomitantly high planewave cutoff energies and lengthy
calculations. There remains the question of whether even all-electron accuracy
in the valence space is sufficient for accurate representation of observables
close to the nuclei, where conventional wisdom would suggest that deep core
polarizations might be quite significant for properties such as the electric
field gradient or Fermi contact interaction. Such concerns turn out to be
unwarranted, however, as our experience and others have shown that the PAW
formalism together with a typical chemical valence/core separation are
sufficient for accurate description of nuclear point properties such as the
electric field gradient [[cite:Petrilli1998]], [[cite:Profeta2003]],
[[cite:Zwanziger2008]], Fermi contact interaction [[cite:Zwanziger2009]] and
magnetic chemical shielding [[cite:Pickard2001]].

Both the electric field gradient and Fermi contact interaction are ground-
state observables, and their computation adds negligible time to a normal
self-consistent ground state calculation. The total charge density in the PAW
formalism contains the pseudovalence density, the nuclear ionic charges, and
the all-electron and pseudo charge densities within the PAW spheres. As done
in earlier work, the electric field gradient due to the pseudovalence density
is computed in reciprocal space, and the gradient due to the (fixed) ionic
charges is computed with an Ewald sum approach. The PAW sphere charge
densities contribute matrix elements of (3xα xβ -r2δαβ)/r5, weighted by the
charge densities in each channel determined by the self-consistent
minimization procedure. This treatment [[cite:Zwanziger2008]] is more flexible
than for example assuming all bands are doubly occupied, and permits the
current approach to be used with more complex electronic and magnetic states
than just insulators.

Within ABINIT, the electric field gradient computation is invoked with the key
word [[prtefg]] (for Print EFG), together with the key word [[quadmom]]. The
[[prtefg]] key word takes the values 1--3. For value 1, the electric field
gradient coupling in MHz is reported, where the conversion is made by atom by
combining the gradient with the nuclear quadrupole moments supplied by
[[quadmom]]. When [[prtefg]] is input as 2, the additional breakdown of the
field gradient in terms of valence and on-site PAW terms is reported, along
with the eigenvectors of the EFG matrix, so that principal directions may be
determined with respect to the crystal axes. Finally, [[prtefg]]=3 allows
additional computation of a point-charge model of the gradient, for comparison
purposes. The point charges by atom are supplied through the additional
variable [[ptcharge]]. Detailed examples of the use of ABINIT to compute EFG's
can be found in [[cite:Zwanziger2008]], [[cite:Zwanziger2009a]].

The Fermi contact interaction, which is just the electron density evaluated
precisely at the nuclear location, is an important component of the isomer
shift measured in Mossbauer spectroscopy. The isomer shift is directly
proportional to nabs( **R** )-nsrc( **R** ), the difference in electron
density at the absorber (sample) and the source. Evaluating the density at a
nuclear position can be accomplished by treating δ( **r** - **R** ) as the
observable, that is, the three-dimensional Dirac delta function centered on
the nuclear position **R**. Because of the short-range nature of the delta
function, in the PAW-transformed version of the observable only matrix
elements of the on-site all-electron valence functions are required
[[cite:Zwanziger2009]], and these are evaluated from a linear fit to the first
few points of the PAW radial functions.

Within ABINIT the Fermi contact interaction is invoked with the key word
[[prtfc]] (for Print Fermi Contact), which only takes the value 1. When
called, the electron density at each nuclear position is reported, in atomic
units (electrons per cubic Bohr). The isomer shift as measured in Mossbauer
spectroscopy is typically reported in velocity units and is obtained from the
formula

δ = (2πcZe2/3Eγ) [nabs( **R** )-nsrc( **R** )] Δ〈r2〉

where c is the speed of light, Eγ the γ-ray energy, Z the atomic number, e the
electron charge, and Δ〈r2〉 the change in the mean square nuclear radius for
the transition. The electronic densities nabs and nsrc refer to the absorber
and source respectively. Because of the linearity of this formula in the
density at the absorber (sample) nucleus, the only unknown (Δ〈r2〉) can be
obtained by comparing the calculated values in several standards to experiment
and then the computations can be used to interpret the measurements of new
materials. In [[cite:Zwanziger2009]] it is showed how to perform such studies
on a variety of compounds.



## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

## Tutorials

* [[tutorial:nuc|The tutorial on the properties of the nuclei]] shows how to compute the electric field gradient and Mossbauer Fermi contact interaction. Prerequisite: PAW1.

---
description: How to set parameters for a parallel calculation
authors: MT,FJ
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to set parameters for a parallel calculation with the ABINIT package.

## Introduction

Running ABINIT in parallel (MPI 10 processors) can be as simple as:

```
mpirun -n 10 abinit run.abi > log 2> err
```

or (MPI 10 processors + OpenMP 4 threads):

```
export OMP_NUM_THREADS=4
mpirun -n 10 abinit run.abi > log 2> err   
```

In the latter, the standard output of the application is redirected to `log` while `err` collects the standard error.
The command *mpirun* might possibly be replaced by *mpiexec* depending on your system.

* For ground-state calculations, the code has been parallelized (MPI-based parallelism) 
  on the k-points, the spins, the spinor components, the bands, and the FFT grid and plane wave coefficients. 
  For the k-point and spin parallelisations (using MPI), the communication load is generally very small. 
  and the parallel efficiency very good provided the number of MPI procs divide the number of k-points in the IBZ.
  However, the number of nodes that can be used with this kind of k-point/spin distribution might be small, 
  and depends strongly on the physics of the problem. 
  A combined FFT / band parallelisation (LOBPCG with [[paral_kgb]] 1) is available [[cite:Bottin2008]], and has shown 
  very large speed up (>1000) on powerful computers with a large number of processors and high-speed interconnect. 
  The combination of FFT / band / k point and spin parallelism is also available, and quite efficient for such computers. 
  Available for norm-conserving as well as PAW cases. Automatic determination of the best combination 
  of parallelism levels is available. Use of MPI-IO is mandatory for the largest speed ups to be observed. 

* Chebyshev filtering (Chebfi) is a method to solve the linear eigenvalue problem, and can be used as a SCF solver in Abinit. 
  It is implemented in Abinit [[cite:Levitt2015]]. The design goal is for Chebfi to replace LOBPCG as 
  the solver of choice for large-scale computations in Abinit. 
  By performing less orthogonalizations and diagonalizations than LOBPCG, scaling to higher processor counts is possible. 
  A manual to use Chebfi is available [[pdf:howto_chebfi.pdf|here]] 

* For ground-state calculations, with a set of images (e.g. nudged elastic band method, the string method, 
  the path-integral molecular dynamics, the genetic algorithm), MPI-based parallelism is used. 
  The workload for the different images has been distributed. This parallelization level can be combined 
  with the parallelism described above, leading to speed-up beyond 5000. 

* For ground-state calculations, GPUs can be used. The implementation is based on CUDA+MAGMA. 

* For ground-state calculations, the wavelet part of ABINIT (BigDFT) is also very 
  well parallelized: MPI band parallelism, combined with GPUs. 

* For response calculations, the code has been MPI-parallelized on k-points, spins, bands, 
  as well as on perturbations. For the k-points, spins and bands parallelisation, 
  the communication load is rather small also, and, unlike for the GS calculations, the number of nodes 
  that can be used in parallel will be large, nearly independently of the physics of the problem. 
  Parallelism on perturbations is very similar to the parallelism on images in the ground 
  state case (so, very efficient), although the load balancing problem for perturbations with different number 
  of k points is not adressed at present. Use of MPI-IO is mandatory for the largest speed ups to be observed. 
  
* GW calculations are MPI-parallelized over k-points. 
  They are also parallelized over transitions (valence to conduction band pairs), but the two parallelisation 
  cannot be used currently at present. The transition parallelism has been show to allow speed ups as large as 300. 
  
* Ground state, response function, and GW parallel calculations can be done also by using OpenMP parallelism, 
  even combined with MPI parallelism. 


## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

## Tutorials

* [[tutorial:basepar|An introduction on ABINIT in Parallel]] should be read before going to the next tutorials about parallelism. One simple example of parallelism in ABINIT will be shown.
* [[tutorial:paral_gspw|Parallelism for ground-state calculations, with plane waves]] presents the combined k-point (K), plane-wave (G), band (B), spin/spinor parallelism of ABINIT (so, the "KGB" parallelism), for the computation of total energy, density, and ground state properties 
* [[tutorial:paral_moldyn|Parallelism for molecular dynamics calculations]]
* [[tutorial:paral_images|Parallelism based on "images", e.g. for the determination of transitions paths (NEB, string method) or PIMD]], that can be activated on top of the "KGB" parallelism for force calculations.
* [[tutorial:paral_gswvl|Parallelism for ground-state calculations, with wavelets]] presents the parallelism of ABINIT, when wavelets are used as a basis function instead of planewaves, for the computation of total energy, density, and ground state properties
* [[tutorial:paral_dfpt|Parallelism of response-function calculations]] - you need to be familiarized with the calculation of linear-response properties within ABINIT, see the tutorial [[tutorial:rf1|Response-Function 1 (RF1)]]
* [[tutorial:paral_mbt|Parallelism of Many-Body Perturbation calculations (GW)]] allows to speed up the calculation of accurate electronic structures (quasi-particle band structure, including many-body effects).

---
description: How to perform a Bethe-Salpeter calculation of neutral excitation energies and dielectric function
authors: MG
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to perform a Bethe-Salpeter calculation of neutral excitation energies and
dielectric function with the ABINIT package.

## Introduction

Many-Body Perturbation Theory (MBPT [[cite:Onida2002]]) defines a rigorous
framework for the description of excited-state properties based on the Green's
function formalism. Within MBPT, one can calculate charged excitations (i.e.
electron addition and removal energies), using for example Hedin's GW
approximation [[cite:Hedin1965]] for the electron self-energy. In the same
framework, neutral excitations (experimentally related to optical absorption
spectra) are also well described through the solution of the Bethe-Salpeter
equation (BSE [[cite:Onida2002]],[[cite:Albrecht1998]]). At present, the BSE
represents the most accurate approach for the ab initio study of neutral
excitations in crystalline systems as it includes the attractive interaction
between electrons and holes thus going beyond the random-phase approximation
(RPA) employed in the GW approximation.

The BSE implementation in ABINIT is discussed in the review article [[cite:Giantomassi2011]].

BSE calculations are activated by setting [[optdriver]]= 99. The number of
valence and conduction states in the e-h basis set is controlled by the
variables [[bs_loband]] and [[nband]], respectively. By default, the code
solves the BSE in the so-called Tamm-Dancoff approximation (TDA) in which the
coupling between resonant and anti-resonant transitions is ignored. A full BSE
calculation including the coupling term can be done by setting
[[bs_coupling]]= 1 in the input file. The variable [[bs_coulomb_term]]
specifies the treatment of the screened interaction in the BSE kernel. By
default, the code reads the W matrix from the SCR file generated by the GW
code ([[optdriver]]= 3). Alternatively, one can model the spatial dependency
of W with the model dielectric function proposed by Cappellini
[[cite:Cappellini1993]]. This option requires the specification of
[[bs_coulomb_term]]= 21 and [[mdf_epsinf]]. The contribution due to the
exchange term can be optionally excluded by using [[bs_exchange_term]]= 0.
This is equivalent to computing the macroscopic dielectric function without
local field effects. The frequency mesh for the macroscopic dielectric
function is specified by [[bs_freq_mesh]] while [[zcut]] defines the complex
shift to avoid the divergences due to the presence of poles along the real
axis (from a physical standpoint, this parameter is related to the electron
lifetimes). In addition, a scissor operator of energy [[mbpt_sciss]] can be
used to correct the initial band energies and mimic the opening of the KS gap
introduced by the GW approximation.

Three different algorithms for the solution of the BSE are implemented:

* (1) Direct diagonalization,
* (2) Haydock recursive algorithm,
* (3) Conjugate gradient eigensolver.

The [[bs_algorithm]] input variable allows the user to choose among them. The
Haydock algorithm [[cite:Haydock1980]],[[cite:Benedict1998]] is the
recommended approach for the computation of optical spectra. The variables
[[bs_haydock_niter]] and [[bs_haydock_tol]] control the Haydock iterative method.

Unfortunately, the Haydock solver does not give direct access to the
eigenstates of the Hamiltonian, hence it cannot be used for the study of the
excitonic wavefunctions. The conjugate gradient (CG) method employs standard
iterative techniques [[cite:Payne1992]] to compute the lowest eigenstates of
the BSE Hamiltonian. This solver is more memory demanding than the Haydock
approach since the eigenstates must be stored in memory, but it gives direct
access to the excitonic states.

The CG algorithm should be preferred over the direct diagonalization
especially when the number of eigenstates is much smaller than the size of the
BSE Hamiltonian.

Note, however, that CG has been implemented only for TDA calculations
(Hermitian matrices).

The most important results of the calculation are saved in five different
files. The BSR file stores the upper triangle of the BSE resonant block in
Fortran binary format (BSC for the coupling matrix). The HAYDR_SAVE file
contains the coefficients of the tridiagonal matrix and the three vectors
employed in the iterative algorithm. This file can be used to restart the
calculation if convergence has not been achieved (related input variables
[[gethaydock]] and [[irdhaydock]]). Finally, the macroscopic dielectric
function with excitonic effects is reported in the EXC_MDF file while
RPA_NLF_MDF and GW_NLF_MDF contain the RPA spectrum without local field
effects obtained with KS energies and the GW energies, respectively.

Different schemes with different degrees of accuracy and computational load
are available in ABINIT [[cite:Gillet2016]] in order to improve the
convergence rate of BSE calculations. They are are available in ABINITv8 with
the `bs_interp_*` input variables.

In the non-spin-polarized case, spin-singlet as well as spin-triplet
excitations are computed. Spin-polarized case is also available.


## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

## Tutorials

* The [[tutorial:bse]] tutorial on the Bethe-Salpeter Equation (BSE) deals with the computation 
of the macroscopic dielectric function of Silicon within the Bethe-Salpeter equation. 
---
description: How to perform calculation within constrained DFT
authors: EB and XG
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to perform calculation with constrained DFT (atomic charge, atomic magnetic moments) with the ABINIT package.

## Introduction

Constrained Density Functional Theory (cDFT) imposes constraints on the charge density and magnetic moments. Usually
integrals of the charge density and magnetization (real-space functions) inside spheres are constrained to user-defined
values. This is described in e.g. [[cite:Kaduk2012]] or [[cite:Ma2015]].

The algorithm implemented in ABINIT (to be published in 2021) is a clear improvement of the algorithm reported in both papers.
The algorithm in [[cite:Kaduk2012]],
initially reported in [[cite:Wu2005]], implements a double-loop cycle, which is avoided in the present implementation.
It is also an improvement on the algorithm presented in [[cite:Ma2015]], based on a penalty function (it is NOT a Lagrange multiplier approach, unlike claimed by these authors) also implemented in ABINIT,
see [[topic:MagMom]], in that it imposes to arbitrary numerical precision the constraint, instead of an approximate one with a tunable accuracy under the control of
[[magcon_lambda]].
The present algorithm is also not subject to instabilities that have been observed when [[magcon_lambda]] becomes larger and larger
in the [[cite:Wu2005]] algorithm.

ABINIT implements forces as well as stresses in cDFT. Also, derivatives of the total energy with respect to the constraint are delivered.
For the charge constraint, vector magnetization constraint, magnetization length constraint and magnetization axis constraint, the derivative
is determind with respect to the value of the constraint defined directly by the user, while for the magnetization direction constraint,
the derivative is evaluated with respect to the change of angle.

## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}
---
description: How to perform a GW- Lanczos-Sternheimer calculation
authors: JL
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to perform a GW- Lanczos-Sternheimer calculation with the ABINIT package.

## Introduction

**This functionality is not in production.**

A high performance G0W0 implementation [[cite:Laflamme2015]] has been developed
within ABINIT.

## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

---
description: How to compute phonon bands, density of states, interatomic force constants, sound velocity ...
authors: MT
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to compute phonon bands, density of states, interatomic force constants, sound
velocity ... with the ABINIT package.

## Introduction

The Fourier transformation of the phonon dynamical matrices generates
interatomic force constants in real space, as explained in
[[cite:Gonze1997a]]. Backtransforming to reciprocal space gives the Fourier
interpolation of the initial phonon band structure. After such Fourier
interpolation, the DOS can be produced (see [[cite:Lee1995]]), the phonon
eigenenergies plotted along lines, the slope of the energy versus cristalline
momentum evaluated (to give sound velocity).

The two-phonon sum and difference spectra can also be obtained, see [[anaddb:dossum]].

For the related computation of temperature-dependent properties, see [[topic:Temperature]].


## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

## Tutorials

* [[tutorial:rf2|The tutorial Response-Function 2 (RF2)]] presents the analysis of the DDBs that have been introduced in the [[tutorial:rf1]]. The computation of the interatomic forces and the computation of thermodynamical properties is an outcome of this tutorial.

---
description: How to print some useful quantities
authors: FJ
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to print some useful quantities with the ABINIT package.

## Introduction

Many quantities are computed, but not reported in the main ABINIT output file.  
On demand, they can be printed thanks to the following
prt&langle;suffix&rangle; input variables. Usually, a corresponding file
suffixed with _&langle;suffix&rangle; will be printed. In other cases, the
main output will be modified to contain the data.



## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

---
description: How to generically perform DFPT calculations
authors: MT
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to generically perform DFPT calculations with the ABINIT package.

## Introduction

Density-Functional Perturbation Theory (DFPT) allows one to address a large
variety of physical observables. Many properties of interest can be computed
directly from the derivatives of the energy, without the use of finite
differences: phonons modes, elastic tensors, effective charges, dielectric
tensors, etc... Even non-linear properties can be computed, like the Raman
intensities (for the latter, see [[topic:nonlinear]])..

A DFPT calculation workflow is conducted as follows:

* Run a Ground-State calculation in order to extract the Kohn-Sham pseudo wave-functions; these must be extremely well converged.
* If necessary, e.g., for the application of the derivative of the Hamiltonian with respect to an electric field, determine the derivatives of the wave functions with respect to the wave vector **k** , and keep them in a file. The keyword [[rfddk]] is used to perform this type of calculation.
* Compute the 2nd-order derivative matrix (i.e., 2nd derivatives of the energy with respect to different perturbations λ). This can be done thanks to the keywords [[rfphon]] (λ=atomic displacement), [[rfstrs]] (λ=strain), [[rfelfd]] (λ=electric field) or [[rfmagn]] (λ=magnetic field). 
* Launch the anaddb tool (distributed with ABINIT) to analyse the derivative database and compute relaxed tensors and thermodynamical properties.

Note that for PAW calculation, when performing the post-processing with
anaddb, it is recommended to include all the keywords enforcing the sum rules
(acoustic sum and charge neutrality). Indeed the PAW formalism involves, for
each atom, the calculation of a large number of real space integrals, whose
numerical effect may be to break the translational invariance.

Thanks to the locality provided by PAW partial wave basis, it is possible to
perform response function calculations for correlated electron materials. The
DFT+U formalism is usable without any restriction for the PAW+DFPT
calculations.

All the tutorials dedicated to response functions can be followed both with
norm-conserving pseudopotentials and with PAW atomic datasets.

DFPT in ABINIT is implemented for non-magnetic, collinear as well as non-collinear systems [[nspden]]=1, 2 as well as 4.
However, the treatment of the strain perturbation is not yet implemented with [[nspden]]=4 (non-collinear systems).

More detailed explanations to perform a response calculation are given in the [[help:respfn]].


## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

## Tutorials

* [[tutorial:rf1|The tutorial Response-Function 1 (RF1)]] presents the basics of DFPT calculations within ABINIT. The example given is the study of dynamical and dielectric properties of AlAs (an insulator): phonons at Gamma, dielectric constant, Born effective charges, LO-TO splitting, phonons in the whole Brillouin zone. The creation of the "Derivative Data Base" (DDB) is presented.
* [[tutorial:rf2|The tutorial Response-Function 2 (RF2)]] presents the analysis of the DDBs that have been introduced in the preceeding tutorial RF1. The computation of the interatomic forces and the computation of thermodynamical properties is an outcome of this tutorial.
* [[tutorial:elastic|The tutorial on the elastic properties]] presents the computation with respect to the strain perturbation and its responses: elastic constants, piezoelectricity.
* [[tutorial:paral_dfpt|Parallelism of response-function calculations]]. Additional information to use the DFPT in parallel.
* [[tutorial:nlo|Tutorial on static non-linear properties]]. Electronic non-linear susceptibility, non-resonant Raman tensor, electro-optic effect.

---
description: How to tune the speed and memory usage
authors: XG
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to tune the speed and memory usage with the ABINIT package.

## Introduction

The major factors governing the speed of an ABINIT run, for a given physical system, are:

  * the size of the planewave basis set, see [[topic:Planewaves]];
  * the size of the wavevector grid sampling, see [[topic:k-points]];
  * and the parallelism, see [[topic:parallelism]].

For the two first factors, there is a trade-off between CPU time and precision
of the computation, while for the third factor, there is some limit on the
maximal speed-up that can be achieved (and also, the resources must be available.

Beyond these major factors, there is still room for some adjustment. The
needed planewave basis set will depend on the pseudopotential (or PAW atomic
dataset) that is used. Some might be softer than others and need a smaller
planewave basis set. They might possibly be less accurate as well ...

If one is only interested in ground-state properties and forces, one might
also get some speed up by using a real-space representation of density and
potential on a real space FFT grid that does not allow their fine details to
be taken into account (actually, filtering such quantities). This is achieved
by lowering [[boxcutmin]] below its theoretically needed value of 2.0.

The choice of the FFT algorithm implementation, see [[fftalg]] might also lead
to significant speed up, on specific machines.

For specific k-points, time-reversal symmetry can be used to represent the
wavefunctions with their real part, instead of both their real and complex parts. 
This allows halving the memory needs, as well as the CPU time. 
See [[istwfk]].

GW calculations can be made less memory and CPU time consuming,
at the expense of numerical precision,
by compiling ABINIT with the option enable_gw_dpc=“no" in the *.ac9 file.

Aside of using input variables, sometimes, to solve memory problems, the user has to increase the stack size limit.
using Linux/Osx, one can get the stack size using "ulimit -s", and set it to a larger limit using e.g. "ulimit -s 16384" for 16MB cache size,
or "ulimit -s unlimited" if the OS allows you to do so.
Note that in version 9.4.1, this operation is automatically performed by Abinit at the beginning of the run.

Other input variables related to tuning the speed or the memory usage are for expert users only.

## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

---
description: How to specify the types of atoms that form the system
authors: FJ
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to specify the types of atoms that form the system.

## Introduction

ABINIT needs to know the different types of atoms that form the system.
The atoms assembled in a molecule or a solid are physically specified by their
nuclear charge (and their mass for dynamical properties).

However, in a pseudopotential or PAW approach, the knowledge of the nuclear
charge does not define the potential felt by the electron, only the atomic
data file (pseudopotential or PAW) defines it. Thus, in addition to the number
of types of atoms [[ntypat]], and their nuclear charge [[znucl]], ABINIT
requires to know the pseudopotential/PAW to use for each type of atom. The
latters are specified in the [[help:abinit#intro1|"files" file]]. Unless
alchemical potentials are used (see later), the number of pseudopotentials to
be read, [[npsp]], is the same as [[ntypat]]. Note that one cannot mix norm-
conserving pseudopotentials with PAW atomic data files in a single ABINIT run,
even for different datasets. One has to stick either to norm-conserving
pseudopotentials or to PAW.  
More on the pseudos/PAW in [[topic:PseudosPAW]].

ABINIT also has a default table of the atomic masses, but this can be
superceded by specifying [[amu]].

**Alchemical potentials**

For norm-conserving pseudopotentials, ABINIT can mix the pseudopotentials, as
described in [[https://wiki.abinit.org/doku.php?id=developers:pseudos|the ABINIT wiki]], 
to create so-called "alchemical potentials", see [[mixalch]].  
In this case, the number of pseudopotentials to be given, [[npsp]], will
usually be larger than the number of types of atoms, [[ntypat]]. Using
alchemical potentials makes sense to treat alloys in which similar ions are
present, and whose specific chemical properties are not crucial for the
property of interest. Usually it is done only for isovalent species, and ions
of quite similar radii. It is a reasonable interpolation technique for the
electronic properties.



## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

## Tutorials

* [[tutorial:ffield|Tutorial on polarization and finite electric fields]]. Polarization, and responses to finite electric fields for AlAs. In the present topic, it is an example of the definition of several atom types ... 

---
description: How to perform a Δ-SCF calculation of neutral excitations
authors: FJ
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to perform a Δ-SCF calculation of neutral excitations with the ABINIT package.

## Introduction

Although formally not justified, difference in total energy using constrained
occupation numbers sometimes results in surprisingly good agreement with
experimental results, for neutral excitations, in molecules or doped solids.
See e.g. [[cite:Jia2017]].

The manual specification of the occupation numbers is needed in this case.
This is accomplished with the input variable [[occ]], coupled with
[[occopt]]=0 for the homogeneous occupation of a band throughout the Brillouin
Zone, or with [[occopt]]=2 for the specific occupation of a state for a
selected k-wavevector. For very big cells, both should be equivalent, and
[[occopt]]=0 is easier to use.

The "linear combination of images" algorithm allows one to work with a set
of images of the same geometry, but with different electronic states,
and even to optimize this geometry,
according to the linear combination of forces, see [[imgmov]]=6 and [[mixesimgf]].


## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}
---
description: How to compute spatial dispersion properties with the longwave DFPT approach. 
authors: MR and MS
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to compute spatial dispersion properties 
(e.g. flexoelectric tensor or dynamical quadrupoles) with the longwave DFPT
driver of the ABINIT package.

## Introduction
In condensed-matter physics, spatial dispersion refers to the dependence of many material
properties on the wavevector **q** at which they are probed, or equivalently on the gradients of the
external field (electric, magnetic, strain...) and/or the response in real space. Remarkable
examples of such gradient effects include the natural optical rotation, [[cite:Belinicher1980]] whereby some crystals
rotate the polarization plane of the transmitted light, or the flexoelectric tensor, [[cite:Zubko2013]] which
describes the polarization response to a gradient of applied strain.

Since ABINIT v9.0.2, the calculation of several spatial dispersion quantities is accessible
via the longwave driver. The implementation, detailed in [[cite:Romero2020]], follows the formalism developed in 
[[cite:Royo2019]] that adapts the classic longwave method of Born and Huang [[cite:Born1954]] with the modern tools of 
the DFPT. Technically, the driver computes analytical third-order energy derivatives (readily converted to physical 
spatial dispersion quantities) with respect to two of the *standard* 
perturbations (atomic displacements, electric field and strain) and to the wavevector **q**. 

At present ( |today| ), ABINIT enables the calculation of four spatial dispersion tensors required 
in order to build all the contributions to the bulk flexoelectric tensor following the prescriptions exposed in [[cite:Stengel2013]]. 
These include, the clamped-ion flexoelectric tensor (a purely electronic contribution) and the first real-space moment of three other 
tensors (entering the mixed and lattice mediated contributions): the polarization 
response to an atomic displacement, the interatomic force constants and the piezoelectric force-response
tensor. The implementation also provides access, as a by-product of the flexoelectric formalism, to the dynamical 
quadrupoles, which can be considered as the spatial-dispersion counterpart of the Born effective 
charges (dynamical dipoles). After execution, the longwave routines generate a third-order derivative 
database that is subsequently used by ANADDB either to compute and print the different contributions 
to the flexoelectric tensor, or to consider quadrupolar fields within the Fourier interpolation procedure of 
the dynamical matrix [[cite:Royo2020]].

The underlying theory of the long-wave DFPT approach [[cite:Royo2019]] has been developed for its application on 
time-reversal symmetric insulating crystals only. Therefore, the usage of the longwave driver is restricted to materials
of this kind. An extension of the theoretical framework and the ABINIT implementation to magnetic insulators 
and/or metals will be hopefully pursued in the future. 

Regarding the flexoelectric tensor that ABINIT provides, a few remarks are in order. First, recall that 
this is the **bulk** flexoelectric tensor and that a surface counterpart is still missing in order to obtain 
the total flexoelectric tensor of a system.[[cite:Stengel2016]] Even though the implementation can be applied
to slabs or low-dimensional systems (such as 2D materials), via a supercell approach, the outcome of such a 
calculation will not directly 
produce the total (i.e., bulk+surface) flexoelectric tensor, neither the surface contribution can be readily estimated 
from it. How to obtain the total flexoelectric tensor within the current capabilities of ABINIT will be the 
subject of a future publication. 

The interested user must be likewise aware of the physical ambiguities existing in the definition of the bulk 
flexoelectric tensor which inherently affect the longwave driver. One of them precludes its usage to obtain the 
flexoelectric tensor of non-centrosymmetric (i.e., piezoelectric) materials (see section VII.c of [[cite:Stengel2013]]). 
The other one is related with the dependence of the bulk flexoelectric coefficients on the choice of an arbitrary 
reference energy. The average electrostatic potential has been taken as the energy reference 
within the longwave driver. Nonetheless, as illustrated in section IV.d of [[cite:Stengel2016a]], other choices might 
lead to quantitatively and qualitatively different outcomes. On the other hand, such ambiguity is well known to disappear
when the surface-specific part is accounted for, as done e.g. in [[cite:Stengel2014]].

The longwave implementation is still under heavy development. To date it requires the use of norm-conserving 
pseudopotentials without XC nonlinear core corrections and it can be used with LDA and GGA XC functionals. 
The use of spherical harmonics for the nonlocal projectors is mandatory through the option [[useylm]]=1.   

The following steps are required to perform a longwave DFPT calculation of the bulk flexoelectric tensor
(see, e.g., tests [[test:lw_1]] to [[test:lw_3]]):

* Perform ground-state calculation.
* Perform ddk and d2_dkdk response function calculations.
* Perform response function calculations at **q** =Γ to atomic displacements, electric field and strain perturbations, 
including the option [[prepalw]]=1.
* Perform a longwave DFTP calculation of third-order energy derivatives ([[optdriver]]=10 and [[lw_flexo]]=1).
* Use MRGDDB to merge 1st, 2nd and 3rd order DDB files.
* Run ANADDB with [[anaddb:flexoflag]]=1.  

The following steps are required to perform a phonon dispersion calculation including quadrupolar fields in the
nonanalytical part of the dynamical matrix (see, e.g., tests [[test:lw_4]] to [[test:lw_6]]):

* Perform ground-state calculation.
* Perform ddk and d2_dkdk response function calculations.
* Perform response function calculations at **q** =Γ to atomic displacements and electric field, 
including the option [[prepalw]]=1.
* Perform a longwave DFTP calculation of third-order energy derivatives ([[optdriver]]=10 and [[lw_qdrpl]]=1).
* Perform response function calculations to atomic displacements at finite **q** (coarse grid). 
* Use MRGDDB to merge 2nd and 3rd order DDB files.
* Run a phonon dispersion calculation of ANADDB including [[anaddb:dipquad]]=1 and/or [[anaddb:quadquad]]=1.  


## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

## Tutorials

A tutorial is in preparation.

---
description: How to perform a spin dynamics calculation with Multibinit
authors: xuhe
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to perform a spin dynamics calculation with the Multibinit package.

## Introduction

Multibinit uses the Landau-Lifshitz-Gilbert equation to perform spin dynamics at finite temperature.
  

## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}



---
description: How to use hybrid functionals
authors: FJ
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to use hybrid functionals with the ABINIT package.

## Introduction

The Fock exchange term has been implemented in ABINIT, both in the norm-
conserving pseudopotential framework and in the PAW one. Some details about
the implementation in ABINIT can be found [[pdf:hybrids-2017.pdf|here]].  
For an ABINIT user, to make a calculation of Fock exchange:  
\- do a first GGA dataset for the ground state  
\- do a second dataset for the Fock calculation choosing [[ixc]]=40-42 (HF,
PBE0, PBE0-1/3), -402 (B3LYP-Libxc), -406 (PBE0-Libxc), -456 (PBE0-1/3 Libxc),
or -427, -428 (HSE03, HSE06).

  
The energy, forces and stresses are available in the norm-conserving and PAW
frameworks.  
A one-shot G0W0 calculation can follow, only in the norm-conserving case at present.

!!! warning 

    Use [[istwfk]]=1, [[iscf]]=2, [[paral_kgb]]=0, [[paral_atom]]=0.  
    The efficiency of the calculation is not optimal. Work is in progress
    concerning this point.


## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

---
description: How to perform a DMFT calculation
authors: BAmadon
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to perform a DMFT calculation with the ABINIT package.

## Introduction

DFT fails to describe the ground state and/or the excited states such as many
lanthanides, actinides or transition metals. Indeed, exchange correlation
functionals are not (yet) able to describe the strong repulsive Coulomb
interactions occurring among electrons in partly filled localized d or f
orbitals.

A way to improve the description of strongly correlated systems is to
explicitly include the strong repulsive Coulomb interactions in the
Hamiltonian. Solving it in the static mean field approximation, gives the
DFT+U method ([[cite:Anisimov1991]], [[cite:Liechtenstein1995]]), implemented
in ABINIT [[cite:Amadon2008a]]. The Dynamical Mean Field Theory
[[cite:Georges1996]] (DMFT), goes beyond, by solving exactly the local
correlations for an atom in an effective field (i.e., an Anderson model). The
effective field reproduces the effect of the surrounding correlated atoms and
is thus self-consistently related to the solution of the Anderson model
[[cite:Georges1996]].

The combination of DFT with DMFT [[cite:Georges2004]], [[cite:Kotliar2006]]
([[usedmft]]= 1) relies on:

* The definition of correlated orbitals. In ABINIT, we use Wannier functions built using projected local orbitals 
  [[cite:Amadon2008]]. Wannier functions are unitarily related to a selected set of Kohn Sham (KS) wavefunctions, 
  specified in ABINIT by band indices [[dmftbandi]] and [[dmftbandf]]. As empty bands are necessary 
  to build Wannier functions, it is required in DMFT calculations that the KS Hamiltonian is correctly diagonalized: 
  use high values for [[nnsclo]] and [[nline]]. In order to make a first rough estimation of the orbital character 
  of KS bands and choose the band index, the band structure with highlighted atomic orbital character 
  (so called *fatbands* ) can be plotted, using the [[pawfatbnd]] variable. 
  Band structures obtained from projected orbitals Wannier functions can also be plotted using 
  [[plowan_compute]] and related variables. 

* The choice of the screened Coulomb interaction U ([[upawu]]) and J ([[jpawu]]). Note that up to version 7.10.5 (but not in later versions) [[jpawu]]= 0 is required if the density matrix in the correlated subspace is not diagonal.

* The choice of the double counting correction [[cite:Amadon2012]]. The current default choice in ABINIT is [[dmft_dc]]= 1 which corresponds to the full localized limit.

* The method of resolution of the Anderson model. In ABINIT, it can be the Hubbard I method 
  [[cite:Amadon2012]] ([[dmft_solv]]= 2), the Continuous time Quantum Monte Carlo (CTQMC) method 
  [[cite:Gull2011]],[[cite:Bieder2014]] ([[dmft_solv]]= 5) or the static mean field method ([[dmft_solv]]= 1), 
  equivalent to usual DFT+U [[cite:Amadon2012]]).

The practical solution of the DFT+DMFT scheme is usually presented as a double
loop over, first, the local Green's function, and second the electronic local
density [[cite:Amadon2012]]. The number of iterations of the two loops are
determined by [[dmft_iter]] and [[nstep]]. However, in the general case, the
most efficient way to carry out fully consistent DFT+DMFT calculations is to
keep only the loop governed by [[nstep]], while [[dmft_iter]]=1
[[cite:Bieder2014]], [[dmft_rslf]]= 1 (to read the self-energy file at each
step of the DFT loop) and [[prtden]]= -1 (to be able to restart the
calculation of each step of the DFT loop from the density file). Lastly, one
linear and one logarithmic grid are used for Matsubara frequencies
[[cite:Kotliar2006]] determined by [[dmft_nwli]] and [[dmft_nwlo]] (Typical
values are 10000 and 100, but convergence should be studied). More information
can be obtained in the log file by setting [[pawprtvol]]=3.

The main output of the calculations are the imaginary time Green's function ,
from which spectral functions can be obtained using an external maximum
entropy code [[cite:Bergeron2016]], self-energies, from which quasiparticle
renormalization weight can be extracted, the density matrix of correlated
orbitals, and the internal energies [[cite:Amadon2006]]. The electronic
entropic contribution to the free energy can also be obtained using
[[dmft_entropy]] and [[dmft_nlambda]].

The efficient CTQMC code in ABINIT, which is the most time consuming part of
DMFT, uses the hybridization expansion [[cite:Werner2006]], [[cite:Gull2011]]
with a _density-density_ multiorbital interaction [[cite:Gull2011]]. Moreover,
the hybridization function [[cite:Gull2011]] is assumed to be diagonal in the
orbital (or flavor) index. This is exact for cubic symmetry without spin orbit
coupling but, in general, one should always check that the off-diagonal terms
are much smaller than the diagonal ones. A link to the exact rotationally
invariant interaction CTQMC code of the TRIQS library is also available using
[[dmft_solv]]=7.

As the CTQMC solver uses a Fourier transform, the time grid [[dmftqmc_l]] in
imaginary space must be chosen so that the Nyquist frequency, defined by
π[[dmftqmc_l]] [[tsmear]], is around 2 or 3 Ha. A convergence study should
be performed on this variable. Moreover, the number of imaginary frequencies
([[dmft_nwlo]]) has to be set to at least twice the value of [[dmftqmc_l]].
Typical numbers of steps for the thermalization ([[dmftqmc_therm]]) and for
the Monte carlo runs ([[dmftqmc_n]]) are 106 and 109 respectively. The random
number generator can be initialized with the variable [[dmftqmc_seed]].
Several other variables are available. [[dmftctqmc_order]] gives a histogram
of the perturbation orders during the simulation, [[dmftctqmc_gmove]]
customizes the global move tries (mainly useful for systems with high/low spin
configurations), and [[dmftctqmc_meas]] sets the frequency of measurement of
quantities.



## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

## Tutorials

* [[tutorial:dmft|The tutorial on DFT+DMFT]] shows how to perform a DFT+DMFT calculation on SrVO3 using projected Wannier functions. Prerequisite: DFT+U.

---
description: How to fit build a lattice model in Multibinit
authors: AM
---

This page gives hints on how to build a lattice model in multibinit.

## Introduction

The MULTIBINIT software is based on a second-principles approach for lattice dynamics simulations using atomic potentials fitted on first-principles calculations [[cite:Wojdel2013]].
See the [[help:multibinit|MULTIBINIT user guide]].
The set up of a lattice model is the first step of the second-principles approach. 
It has a harmonic part and possibly an anharmonic part.
For the harmonic part, see the tutorial below.
For the anharmonic part, see other topics namely, the [[topic:FitProcess |topic for the fit process]] and the [[topic:BoundingProcess |topic for the bound process]].
Then, one can run a dynamics, see the [[topic:DynamicsMultibinit |topic how to run a dynamics]].

## Tutorials

The [[tutorial:lattice_model|First lesson on Multibinit]] explains how to build a lattice model and to run a dynamics.
  
## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

---
description: How to perform time-dependent density-functional theory calculations of neutral excitation energies
authors: FJ
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to perform time-dependent density-functional theory calculations of neutral
excitation energies with the ABINIT package.

## Introduction

For finite systems (atoms and molecules), excited states can be computed
within TDDFT (Casida approach - only norm-conserving pseudopotentials). See
the explanations given in the [[tutorial:tddft]] tutorial of tutorial. The
[[iscf]] input variable must be set to -1.

In the non-spin-polarized case, spin-singlet as well as spin-triplet
excitations are computed. Spin-polarized case is also available.



## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

## Tutorials

* [[tutorial:tddft|The tutorial on TDDFT]] deals with the computation of the excitation spectrum of finite systems, thanks to the Time-Dependent Density Functional Theory approach, in the Cassida formalism.

---
description: How to modify ABINIT behaviour for developers
authors: XG
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to modify ABINIT behaviour for developers with the ABINIT package.

## Introduction

Specialized input variables to tune the inner behaviour of ABINIT freely.


## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

---
description: How to perform Wannier functions calculation
authors: BAmadon
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to perform Wannier functions calculation with the ABINIT package.

## Introduction

There are two ways to obtain Wannier functions with ABINIT:

* The first one is to use an internal implementation of Projected Local Orbital Wannier functions [[cite:Amadon2008]], [[cite:Amadon2015]] that can be activated with the input variable [[plowan_compute]]. The variables [[plowan_bandi]] and [[plowan_bandf]] specifies the Kohn Sham bands that are used to built Wannier functions, whereas variables [[plowan_natom]],[[plowan_iatom]],[[plowan_nbl]], [[plowan_lcalc]], and [[plowan_projcalc]] specify the atoms, angular momentum and projectors corresponding to the projected local Orbital Wannier functions. 

In order to do Wannier interpolation or analysis of hopping integrals, real
space Wannier functions can be built using the variables [[plowan_realspace]]
and [[plowan_nt]] as well as [[plowan_it]]. Real space Hamiltonian for
analysis is given in latex file whose name ending is "wanniereigen" and the
band structure is given in the output and log file.

* The second one is to use the external code wannier90 ([www.wannier.org](http://www.wannier.org)) 
  that calculates Maximally Localized Wannier Functions (MLWF). 
  After a ground state calculation the Wannier90 code will obtain the MLWFs requiring just two ingredients: 

  * The overlaps between the cell periodic part of the Bloch states.
  * As a starting guess the projection of the Bloch states onto trial localized orbitals.

What ABINIT do is to take the Bloch functions from a ground state calculation
and compute these two ingredients. Then, Wannier90 is run. Wannier90 is
included as a library and ABINIT and the process is automatic, so that in a
single run you can do both the ground state calculation and the computation of
MLWFs. The input variables [[prtwant]], [[w90iniprj]] and [[w90prtunk]] are
related to the use of the wannier90 librairy.



## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

## Tutorials

* [[tutorial:wannier90|The tutorial on Wannier90]] deals with the Wannier90 library to obtain Maximally Localized Wannier Functions.

---
description: How to perform a DFT+U calculation
authors: BAmadon
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to perform a DFT+U calculation with the ABINIT package.

## Introduction

This feature is available only in PAW. The DFT+U framework is described in
[[cite:Anisimov1991]] and [[cite:Liechtenstein1995]]. In ABINIT, the DFT+U
approximation is implemented inside the PAW atomic spheres only. Two choices
of double counting are provided: the Full Localized limit and the Around Mean
Field approximation. Our implementation is described in [[cite:Amadon2008a]].
It follows the main lines of [[cite:Bengone2000]]. See also
[[cite:Czyzyk1994]]. Forces and stress are implemented. For details on
keywords ([[lpawu]], [[upawu]], [[jpawu]], [[usedmatpu]], [[dmatpuopt]],
[[dmatudiag]]) see keyword [[usepawu]] in input variables.

In both the output and log files, we can find:

- The DFT+U contribution of energy which is contained inside the PAW
  Spherical terms in the output file.

- The Decomposition of the DFT+U energy is given (Interaction energy, Double
  counting term, and sum of the two) in the log file.

- The orbital density matrix ($n_{m,m'}^{\sigma}$), also called occupation
matrix (corresponding to Eq.(9) of [[cite:Bengone2000]] and Eq.(1) of
[[cite:Liechtenstein1995]], see also [[cite:Amadon2008a]] and variable
[[dmatpuopt]]) is also given for each atom in the basis of real spherical
harmonics. It is given at each SCF step in the log file: one can thus check
the convergency of the calculation.

Consistency between total energy and forces in DFT+U have been checked.

The implementation of DFT+U in ABINIT allows also to impose a starting density
matrix in order to compare the energy of various electronic configuration (see
keywords [[usedmatpu]] and [[dmatpawu]]).



## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

## Tutorials

* [[tutorial:dftu|The tutorial on DFT+U]] shows how to perform a DFT+U calculation using ABINIT, and will lead to compute the projected DOS of NiO. Prerequisite: PAW1.

---
description: How to specify a crystal, with atomic positions and symmetries
authors: FJ
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to specify a crystal, with atomic positions and symmetries with the ABINIT package.

## Introduction

In addition to the [[topic:UnitCell|Specification of the unit cell]] and
[[topic:AtomTypes|Atom types]], ABINIT must know the number of atoms inside
the cell, their type, and position. This is described by [[natom]], [[typat]]
and one of [[xred]] or [[xcart]].

ABINIT can automatically detect the Bravais lattice and space group, and
generate symmetries (e.g. [[nsym]], [[symrel]], [[tnons]]), from the primitive
cell and the position of atoms (provided they are not too inaccurate, see
[[tolsym]]). For this purpose, in the magnetic case, ABINIT will also take
into account the input atomic spin, through the knowledge of [[spinat]].

Alternatively, ABINIT can start from the specification of symmetries (either
from [[spgroup]] or from the list of symmetries -
[[nsym]], [[symrel]], [[tnons]]) and generate the atomic positions from the
asymmetric (irreducible) part of the primitive cell. This is described in the
[[topic:SmartSymm|Smart Symmetrizer]] topic.

ABINIT can treat antiferromagnetic symmetry operations, see [[symafm]].

In ABINIT, a database with the 230 spatial groups of symmetry (see
[[spgroup]]) and the 1191 Shubnikov anti-ferromagnetic space groups is present
(see also [[spgroupma]] and [[genafm]]).

There is also a (non-graphical) atom manipulator in ABINIT, see [[topic:AtomManipulator]].

ABINIT can read XYZ files, see [[xyzfile]].

Atomic positions can also be generated at random, see [[random_atpos]].

Details about the way the crystal structure is defined in ABINIT can be found [[theory:geometry|here]].

!!! tip

    If |AbiPy| in installed on your machine, you can use the |abistruct| script
    to automate several operations related to crystalline structures.
    Further details about the python API are available in the |AbipyStructureNb|.

## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

## Tutorials

* [[tutorial:base1|The tutorial 1]] deals with the H2 molecule : get the total energy, the electronic energies, the charge density, the bond length, the atomisation energy
* [[tutorial:base2|The tutorial 2]] deals again with the H2 molecule: convergence studies, LDA versus GGA
* [[tutorial:base3|The tutorial 3]] deals with crystalline silicon (an insulator): the definition of a k-point grid, the smearing of the cut-off energy, the computation of a band structure, and again, convergence studies ...
* [[tutorial:base4|The tutorial 4]] deals with crystalline aluminum (a metal), and its surface: occupation numbers, smearing the Fermi-Dirac distribution, the surface energy, and again, convergence studies ...
---
description: How to control the SCF cycle
authors: XG
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to control the SCF cycle with the ABINIT package.

## Introduction

The numerical precision of the calculations depends on many settings, among
which the precision in solving the Kohn-Sham self-consistent equation.

Several parameters govern the SCF loop. The maximum number of cycles is given
by [[nstep]], but the iterative procedure might be stopped earlier, as soon as
the criterion chosen by the user is fulfilled. The user is asked to give a
tolerance on some measure of the convergence. The user must choose among
[[toldfe]], [[toldff]], [[tolrff]], [[tolvrs]] and [[tolwfr]].

  * The most theoretically justified for the density/potential self-consistency is [[tolvrs]].
  * [[tolwfr]] is interesting for non-self-consistent calculations.
  * For molecular dynamics (which rely on the accuracy of forces), one might prefer [[tolrff]].

Some input variables relate to the solution of the Schrodinger equation.
However, usually the related iterative techniques are well-tuned, so that
these input variables ([[nline]] and [[tolrde]]) are usually used only by
experts. However, in cases where the convergence is difficult, it might be
interesting to test improving them, as well as modifying [[nnsclo]].

The [[accuracy]] variable enables one to tune the accuracy of a calculation by
setting automatically up to seventeen variables.


## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

## Tutorials

* [[tutorial:base2|The tutorial 2]] deals again with the H2 molecule: convergence studies, LDA versus GGA 
* [[tutorial:base3|The tutorial 3]] deals with crystalline silicon (an insulator): the definition of a k-point grid, the smearing of the cut-off energy, the computation of a band structure, and again, convergence studies ...

---
description: How to take into account an external magnetic field
authors: EB
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to take into account an external magnetic field with the ABINIT package.

## Introduction

An applied external magnetic field has several types of interactions with a system of electrons and nuclei.
In particular, it couples to the electronic spins (Zeeman term) as well as to the electronic orbital motion (orbital term).
Input variables to deal with these two situations are decoupled in ABINIT.
[[zeemanfield]] gives access to the Zeeman coupling, and is in production.
The coupling to the electronic orbital motion is much more delicate to treat, and is currently in development.
Chern number calculation is available.
Both types of input variables are listed below.

## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

---
description: How to perform a Tdep calculation
authors: YB, FBottin
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to perform thermodynamic, elastic and transport properties calculations including explicit temperature effects with the ABINIT package.  

User guide: [[pdf:a-TDEP_Guide| a-TDEP guide]]  
Theory: [[pdf:a-TDEP_Paper|a-TDEP paper]]

## Introduction

The Temperature Dependent Effective Potential (TDEP) method
has been developped by O. Hellman *et al.* [[cite:Hellman2011]],
[[cite:Hellman2013]], [[cite:Hellman2013a]] in 2011 and the |a-TDEP| implementation
in ABINIT has been performed and used for the first time in 2015 by
J. Bouchet and F. Bottin [[cite:Bouchet2015]], [[cite:Bouchet2017]].

The capture of thermal effects in solid state physic is a long standing
issue and several stand-alone or post-process computational codes are
available. Using different theoretical frameworks, they propose to provide
some thermodynamic quantities involving the so called anharmonic effects.
|a-TDEP| calculation can produce almost all the temperature-dependent
thermodynamic quantities you want, from a single *ab initio*
molecular dynamic (AIMD) trajectory and by means of a Graphical User
Interface (GUI) very easy to use ([[https://github.com/abinit/abiout|AGATE]]).

The original TDEP method [[cite:Hellman2011]] is implemented in ABINIT.
In particular, various algorithms can be used to obtain the Interatomic Force Constants (IFC).
The 2nd-order (and soon 3rd-order) IFCs are produced self-consistently using a least-square
method fitting the AIMD forces on a model Hamiltonian function of the displacements.  
Numerous thermodynamic quantities can be computed starting from the
2nd order IFCs. The 1st one is the phonon spectra, from which a large
number of other quantities flow : internal energy, entropy, free energy, specific heat...
The elastic constants and other usual elastic moduli (the bulk,
shear and Young moduli) can be also produced at this level. Using the 3rd
order IFCs, we could extract the Gruneisen parameter, the thermal
expansion, the sound velocities... and in particular, how to take into account
the anisotropy of the system within.

## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

---
description: How to compute phonon frequencies and modes, IR and Raman spectra, Born effective charges, IR reflectivity ...
authors: MT
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to compute phonon frequencies and modes, IR and Raman spectra, Born effective
charges, IR reflectivity with the ABINIT package.

## Introduction

The computation of the second-order derivative of the total energy with
respect to atomic displacements at an arbitrary wavevector, using
[[topic:DFPT]], opens the possibility to compute the dynamical matrix at that
wavevector, and hence, to compute the phonon eigenfrequency and
eigendisplacements. When the wavevector is (0,0,0), usually denoted as the
Gamma point, the combination of the atomic displacements and electric field
type perturbations opens also the access to Born effective charges, electronic
(for frequencies lower than the electronic band gap) dielectric constants, and
then, to infra-red reflectivity of materials (in the infinite lifetime
approximation). See [[cite:Gonze1997a]] for the presentation of the theory
with DFPT.

In ABINIT, with one dataset for a fixed wavevector (see [[topic:q-points]]),
one can compute all such second-order derivatives. ABINIT will already perform
some post-processing treatment of the second-order derivatives (e.g.
computation of the dynamical matrix, and corresponding eigenenergies and
eigendisplacements), although the most extended post-processing treatment is
provided by ANADDB. Thus, there is some overlap of the two executables, with
some common input variables. Usually, the action of an input variable with the
same name in the two executables is very similar, although there are some
input variables that govern more options in ANADDB then in ABINIT, because of
the previously mentioned difference in capabilities. In the database of input
variables, the input variables related to ABINIT or ANADDB are clearly
distinguished.

The band-by-band decomposition of the Born effective charge tensors can be
computed thanks to [[prtbbb]]. The related localization tensor (see
[[cite:Veithen2002]] can also be computed.

Phonon calculations are arbitrary q-points can be done under finite electric
field ([[topic:Berry]]).

It will be the easiest to discover the capabilities of these two executables
through the [[tutorial:rf1]] of the tutorial.

See [[topic:DFPT]] for the general information about DFPT, [[topic:q-points]]
for the specification of q-points, and [[topic:PhononBands]] for the
computation of full phonon bands.

!!! important

    More than 1500 phonon band structures for insulators, computed with ABINIT, are now available 
    on the [Materials Project web site](https://materialsproject.org), accompanied with derived 
    thermodynamic quantities, Born effective charges, and dielectric tensor [[cite:Petretto2018a]].
    The DDB file can be downloaded automatically with |AbiPy| starting from the materials project
    identifier. For futher information, please consult the |DdbFileNb|.

## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

## Tutorials

* [[tutorial:rf1|The tutorial Response-Function 1 (RF1)]] presents the basics of DFPT calculations within ABINIT. The example given is the study of dynamical and dielectric properties of AlAs (an insulator): phonons at Gamma, dielectric constant, Born effective charges, LO-TO splitting, phonons in the whole Brillouin zone. The creation of the "Derivative Data Base" (DDB) is presented.

---
description: How to use the Abipy python package
authors: MG
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to use the Abipy python package with the ABINIT package.

## Introduction

[AbiPy](https://github.com/abinit/abipy) is a Python library to analyze the
results produced by Abinit, It also provides tools to generate input files and
workflows to automate ab-initio calculations and typical convergence studies.
AbiPy is interfaced with [pymatgen](http://pymatgen.org) and this allows
users to benefit from the different tools and python objects available in the
pymatgen ecosystem. AbiPy can be used in conjunction with matplotlib, pandas,
ipython and jupyter thus providing a powerful and user-friendly environment
for data analysis and visualization.

The official documentation of the stable version is available at. 
Check out our [gallery of plotting scripts](http://abinit.github.io/abipy/gallery/index.html) 
and the [gallery of AbiPy workflows](http://abinit.github.io/abipy/flow_gallery/index.html). 
To learn more about the integration between jupyter and AbiPy, visit our
[collection of notebooks](https://nbviewer.jupyter.org/github/abinit/abitutorials/blob/master/abitutorials/index.ipynb)


## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

---
description: To to calculate the temperature dependence of the electronic structure
authors: SP
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to calculate the temperature dependence of the electronic structure with the ABINIT package.

## Introduction

The electronic structure changes with temperature. In most materials, such
changes are mainly driven by the electron-phonon interaction, which is also
present at zero Kelvin, inducing the so-called zero-point motion
renormalization (ZPR) of the eigenvalues. These effects can be computed thanks
to the Allen-Heine-Cardona (AHC) theory [[cite:Allen1976]],
[[cite:Allen1981]], [[cite:Allen1983]], which is based on diagrammatic method
of many-body perturbation theory. An extension to the standard AHC theory also
gives access to the electronic lifetime and decay rates. These physical
properties are available from ABINIT since v7.10.4.

The AHC formalism and the implemented equations can be found in
[[cite:Ponce2014a]]. An extended verification and validation study (also
versus other first-principle codes) of the ABINIT implementation can be found
in [[cite:Ponce2014]]. The AHC implementation can be used with any XC
functional working with the response-function (RF) part of the code, and
requires the use of norm-conserving pseudopotentials. NetCDF support is
mandatory.

The AHC implementation in ABINIT is still under heavy development.
Most of the present information relates to the legacy implementation,
although some also relates to the 
most recent procedure that relies on [[optdriver]]=7. The documentation
of the new procedure is given mostly by the related tutorials.
We do not describe the Frohlich model computations.

In the oldest, well-established, AHC implementation in ABINIT, 
the sum over highly energetic bands appearing in the AHC
equations [[cite:Gonze2011]] is efficiently
computed. Such behavior is controlled by the input variable [[ieig2rf]].

The **k** -point convergence can be strongly improved by restoring the charge
neutrality through the reading of the Born effective charge and dielectric
tensor (controlled by the input variable [[getddb]]). More information on the
importance of charge neutrality fulfillment can be found in
[[cite:Ponce2015]]. The value of [[elph2_imagden]] sets the imaginary shifts
used to smooth numerical instabilities in the denominator of the sum-over-
states expression.

We have checked that the implementation correctly holds for arbitrarily small
[[elph2_imagden]] parameters, [[cite:Ponce2015]]. The input variable
[[smdelta]] triggers the calculation of the electronic lifetime and the value
of the smearing delta function can be specified through [[esmear]].

A double grid can be used to speed-up the calculations with [[getwfkfine]] or
[[irdwfkfine]]. The variable [[getgam_eig2nkq]] gives the contribution at Γ so
that the Debye-Waller term can be computed. This variable is only relevant for
calculations of AHC using the abinit program only. It is nonetheless
recommended to use the provided python post-processing script
temperature_para.py with its module rf_mods.py in the directory
scripts/post_processing/ to allow for more flexibility. The python scripts
support multi-threading.

The following steps are required to perform an AHC calculation:

* Perform a response function calculation at **q** =Γ with electric field perturbation.
* Perform phonon calculations and produce the EPC for a large set of wavevectors **q** , reading the Born effective charge and dielectric tensor with [[getddb]].
* Gather and compute the impact of the electron-phonon coupling on the electronic eigenenergies using the temperature_para.py python script.

The outputs of the script are provided in text and NetCDF format to allow for
later reading inside ABINIT. This could be used in the future developments of
ABINIT to compute temperature-dependent optical properties for example.

For the temperature dependence of the Fermi energy, see [[topic:ElPhonTransport]].


## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

## Tutorials

* (Legacy procedure) A tutorial is available at [[tutorial:tdepes|the temperature dependence of the electronic structure]]:.
* (New procedure) Two tutorials are available at [[tutorial:eph_intro|an overview of the EPH code]], and
[[tutorial:eph4zpr|Zero-point renormalization of the band gap and temperature-dependent band gaps]]:.

---
description: How to use Van der Waals functionals
authors: YPouillon, BVanTroeye
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to use Van der Waals functionals with the ABINIT package.

## Introduction

It is well known that long range correlations responsible of van der Waals
interactions are out of reach for both LDA and GGA approximations to the
exchange-correlation energy in DFT. In recent years several methods have been
devised to include such interactions, which can be grouped into two
strategies, namely _ad hoc_ methods and self-consistent approaches. Currently
ABINIT can perform calculations based on either the DFT-D methods or the vdW-
WF methods, as described later, both belonging to the first group.

A fully customizable implementation of the vdW-DF method [[cite:Dion2004]], a
self-consistent approach, and an adaptation of the strategy followed by
G.Roman-Perez _et al._ [[cite:Romanperez2009]] to the case of ABINIT are under
development. It will offer around 25 ajustable parameters and be delivered
with graphical tools to help users assess the quality of their kernels. It
does not only aim at performing production calculations with vdW-DF, but also
at helping researchers who develop new density functionals optimised for
systems requiring van-der-Waals interactions.

The DFT-D methods have been implemented inside ABINIT, namely DFT-D2
[[cite:Grimme2006]], DFT-D3 [[cite:Grimme2010]] and DFT-D3(BJ)
[[cite:Grimme2011]]. In these cases, pair-wise terms (and 3-body corrections
for DFT-D3 and DFT-D3(BJ)) are added to the DFT energy, which are independent
of the electronic density, in order to mimic the vdW interactions. The
implementation includes the contributions of these methods to forces and
stresses, in view of geometry optimization, as well as to first-order response
functions like dynamical matrices, clamped elastic constants and internal
strain coupling parameters.

To activate DFT-D dispersion correction, two keywords are in use: [[vdw_xc]] =
5/6/7 to choose between DFT-D2, DFT-D3 and DFT-D3(BJ), and [[vdw_tol]], to
control the inclusion of largely distant pairs (those giving a contribution
below [[vdw_tol]] are ignored). It is also possible to include 3-body
corrections [[cite:Grimme2010]] (for ground-state only) with the keyword
[[vdw_tol_3bt]], which also controls the tolerance over this term.

Methods based on maximally localized Wannier functions (MLWFs) to calculate
vdW energy corrections have also been implemented in ABINIT. In this case the
pair-wise terms come from contributions of pairs of MLWFs rather than from
atoms. Among the implemented methods in ABINIT it is found vdW-WF1
[[cite:Silvestrelli2008]], [[cite:Silvestrelli2009]] vdW-WF2
[[cite:Ambrosetti2012]] and vdW-QHO-WF [[cite:Silvestrelli2013]]. A full
description of the implementation of vdW-WF1 is reported in
[[cite:Espejo2012]].

Selection of one of these 3 methods is achieved by using [[vdw_xc]]=10/11/14
respectivelly. Since vdW-WF1 and vdW-WF2 methods are approximations for the
dispersion energy of non overlapping electronic densities, it is necessary to
define the interacting fragments of the system whose dispersion energy is
going to be calculated. The latter is achieved by using the input variables
[[vdw_nfrag]] and [[vdw_typfrag]] to define the number of interacting
fragments in the unit cell and to assign each atom to a fragment. A given MLWF
belongs to the same fragment as its closer atom. The need for defining the
interacting fragments is overridden in the vdW-QHO-WF, for which these input
variables are not used. When dealing with periodic systems the input variable
[[vdw_supercell]] controls the number of neighbor unit cells that will be
included in the calculation. Each one of the 3 components of [[vdw_supercell]]
indicates the maximum number of cells along both positive or negative
directions of the corresponding primitive vector. This is useful for studying
the spacial convergency of the vdW energy. It should be noticed that the user
must set the variables associated to the calculation of MLWFs and that the
resulting vdW energies strongly depend on the obtained Wannier functions.



## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

---
description:  How to use git with Abinit
authors: XG
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to  with the ABINIT package.

## Introduction

ABINIT is developed under the control of [git](https://git-scm.com/) and 
the development is based on the 
[Gitflow branching model](https://www.atlassian.com/git/tutorials/comparing-workflows/gitflow-workflow).
There is an official [github mirror](https://github.com/abinit/abinit)
where we welcome bug fixes and improvements. 
Note, however, that most of the active developments are hosted on our 
internal [Gitlab server](https://gitlab.abinit.org/).
Before embarking on making significant changes, please contact the Abinit group.

See the many different projects linked to ABINIT on Github
actually gathered in the [Github ABINIT organization](https://github.com/abinit).


## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

---
description: How to compute elastic, piezoelectric and internal strain tensors from DFPT
authors: MT
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to compute elastic, piezoelectric and internal strain tensors 
from DFPT with the ABINIT package.

## Introduction

The DFPT theory to compute the elastic, piezoelectric and internal strain
tensors is presented in [[cite:Hamann2005]].

See the [[topic:DFPT|generalities about DFPT]] as well as the tutorial
[[tutorial:elastic]]. For the preliminary runs of ABINIT, see [[rfstrs]], while
for ANADDB, see [[anaddb:elaflag]] for the elastic tensor, and
[[anaddb:piezoflag]] for the piezoelectric tensor.



## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

## Tutorials

* [[tutorial:elastic|The tutorial on the elastic properties]] presents the computation with respect to the strain perturbation and its responses: elastic constants, piezoelectricity.
* [[tutorial:paral_dfpt|Parallelism of response-function calculations]]. Additional information to use the DFPT in parallel.

---
description: How to generate the electronic band structure related topics
authors: XG
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to generate the electronic band structure related topics with the ABINIT package.

## Introduction

The eigenenergies along a set of segments can be computed 
(non-self-consistent calculations with [[iscf]] = -2) using a negative value of [[kptopt]], with
[[kptbounds]] defining the end points of the segments, and [[ndivsm]] (or [[ndivk]]) defining the sampling. 
Choice of output unit in the main output file  is governed by [[enunit]].

A band structure can even be represented using weights proportional to the
orbital content (so-called "Fat Bands"), in case of PAW calculation, see
[[pawfatbnd]], and related variables.

Different interpolation schemes for the band energies can be defined thanks to [[einterp]]. 
The Wannier interpolation is also available through the use of
the [[tutorial:wannier90]] post-processor.

The band structure from a supercell calculation can be unfolded to the (large)
Brillouin zone of the (small) primitive cell thanks to the [[help:fold2bloch]]
post-processor. See the related [[topic:Unfolding]].

Different plotting postprocessors exist to produce graphical representations
of electronic band structures from ABINIT. 
The most powerful is based on [[topic:Abipy]] that provides several tools
to analyze band structures (more info available in the |GsrFileNb|).

Simpler tools also exist, and can be found in
~abinit/scripts/post_processing, e.g. AbinitBandStructureMaker.py,
plot_bandstructure.py or abinit_eignc_to_bandstructure.py.


## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

---
description: How to use the symetry information to build the system from the irreducible part of the primitive cell
authors: FJ
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to use the symetry information to build the system from the irreducible part of
the primitive cell with the ABINIT package.

## Introduction

Sometimes, the user knows the space group of the system, the conventional cell
vectors, as well as the positions of atoms in the asymmetric (irreducible)
part of the cell. From such data, ABINIT can generate the usual primitive cell
vectors, as well as the coordinates of all the atoms in this cell.

This is activated if [[spgroup]]!=0 and [[brvltt]]=-1. The user needs to
specify the number of atoms to be read [[natrd]] in the asymmetric part of the
cell, as well as the expected number of atoms [[natom]] in the primitive cell.
Some additional information on the axes orientation [[spgaxor]] and the cell
origin [[spgorig]] might also have to be given. 
See [[help:spacegroup|the space group help file]]

The specification of a magnetic space group is even possible
(antiferromagnetic). See [[spgroupma]] and [[genafm]].

See also the [[topic:UnitCell]].


## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

---
description: How to calculate the RPA correlation energy
authors: FB
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to calculate the RPA correlation energy with the ABINIT package.

## Introduction

In the adiabatic-connection fluctuation-dissipation framework, the correlation
energy of an electronic system can be related to the density-density
correlation function, also known as the reducible polarizability. When further
neglecting the exchange-correlation contribution to the polarizability, one
obtains the celebrated random-phase approximation (RPA) correlation energy.
This expression for the correlation energy can alternatively be derived from
many-body perturbation theory. In this context, the RPA correlation energy
corresponds to the GW total energy.

The RPA correlation energy can be expressed as an integral function of the
dielectric matrix (see [[cite:Gonze2016]]). The integral over the frequencies
is performed along the imaginary axis, where the integrand function is very
smooth. Only a few sampling frequencies are then necessary. In ABINIT, the RPA
correlation energy is triggered by setting the keyword [[gwrpacorr]] to 1.

The RPA correlation energy is a post-processed quantity from the GW module of
ABINIT, which takes care of evaluating the dielectric matrix for several
imaginary frequencies.

The RPA correlation has been shown to capture the weak van der Waals
interactions [[cite:Lebegue2010]] and to drastically improve defect formation
energies [[cite:Bruneval2012]].

The convergence versus empty states and energy cutoff is generally very slow.

It requires a careful convergence study. The situation can be improved with
the use of an extrapolation scheme ([[cite:Bruneval2008]], [[cite:Harl2010]]).



## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

---
description: How to constrain the geometry of the system in geometry optimization, molecular dynamics or searches
authors: GG
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to constaint the geometry of the system in geometry optimization, molecular
dynamics or searches with the ABINIT package.

## Introduction

There are two mechanisms to put constraints on the atom positions in ABINIT.
They can be used in [[topic:GeoOpt|geometry optimization]],
[[topic:MolecularDynamics|molecular dynamics]] (including PIMD) or other
geometry algorithms (e.g. [[topic:TransPath|transition path searches]]).

The simplest one (entry point [[iatfix]]) simply define a set of atoms that
are fixed, either entirely, or only along one of the directions (for the
latter, see the warning in [[iatfix]])

A more complex one, but also much more powerful, allows to place constraints
on linear combinations of atomic positions. Thanks to such constraint, the
mean position of two atoms (or a fragment, like a molecule) can be fixed, or
constrained to stay within an arbitrary plane. One can thus also sample
different mean positions. See a complete description in [[wtatcon]].



## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

---
description: How to compute Raman intensity, and the related electro-optic coefficients
authors: RC, XG
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to compute Raman intensity, and the related electro-optic 
coefficients with the ABINIT package.

## Introduction

In Raman experiments, the incident light, usually a polarized or unpolarized
laser, is scattered by the sample, and the energy as well as polarization of
the outgoing light is measured. A Raman spectrum, presenting the energy of the
outgoing photons, will consist of rather well-defined peaks, around an elastic peak.

At the lowest order of the theory, the dominant mechanism is the absorption or
emission of a phonon by a photon. The energy of the absorbed or emitted phonon
corresponds to the energy difference between the outgoing and incident
photons. Thus, even more straightforwardly than the IR spectrum, a Raman
spectrum is directly related to the energy of phonons at the Brillouin-zone
center: when the zero of the energy scale is set at the incident light energy,
the absolute value of the energy of the peaks corresponds to the energy of the
phonons.

The computation of phonon energies is presented in [[topic:Phonons]]. Raman
intensities due to one-phonon emission or absorption are not linked to second-
order derivatives of the total energy, but, within the adiabatic
approximation, to derivative of the dielectric phonon with respect to atomic
displacements. Moreover, when the frequency of the incident light (usually in
the 1.5 eV to 2.5 eV range) is small with respect to the band gap (e.g. for
gaps larger than 4 eV), the static approximation can be made, in which the
Raman intensity will be linked to the third-order derivative of the total
energy with respect (twice) to an homogeneous electric field and (once) with
respect to atomic displacements. Thus, DFPT can be used, see below. For the
case in which the incident light frequency is not negligible with respect to
the gap, the DFPT cannot be used, but, if the adiabatic approximation can be
used (valid when the phonon frequency is much smaller than the gap, and 
also, as a consequence, features
smaller than the largest phonon frequency are not resolved in the Raman
spectrum), one can compute the Raman intensities thanks to finite differences
of dielectric function, see [[cite:Gillet2013]]. For the two-phonon Raman
spectrum, see [[cite:Gillet2017]].

Both the derivatives of the linear electronic dielectric susceptibilities with
respect to atomic displacements and the non-linear electronic dielectric
susceptibilities required to evaluate the Raman intensities are thus non-
linear responses. 

In the ABINIT implementation, they are computed within the
density functional perturbation theory.
A first formalism (PEAD) is described in [[cite:Veithen2005]].
Thanks to the 2n+1 theorem, this formulation only requires the knowledge of
the ground-state and first-order changes in the wavefunctions,
but some quantity is evaluated thanks to a finite-difference in k-space. 
It is implemented only for NC pseudopotentials, within LDA.
There is another formalism, based on 2nd-order Sternheimer equation, 
recently made available (L. Baguet, to be published).
The latter is implemented both for NC pseudopotentials and for PAW, again only for LDA.
Both work for [[nsppol]]=1 or 2.

The PEAD non-linear response formalism has been successfully applied to a large
variety of systems. We have so far studied the Raman spectra of ferroelectric
oxides ( BaTiO3 and PbTiO3 [[cite:Hermet2009]]), different minerals under
pressure conditions characteristic to the interior of the Earth
[[cite:Caracas2007a]] or molecular solids under extreme conditions
[[cite:Caracas2008]]. The computation of the non-linear optical
susceptibilities has also been applied to several polar dielectrics
[[cite:Caracas2007]].

As a by-product of the calculation of the Raman tensor and non-linear optical
coefficients, it is also possible to determine directly within ABINIT the
electro-optic (EO) coefficients rijγ (Pockels effect) which describe the
change of optical dielectric tensor in a (quasi-)static electric field through
the following expression [[cite:Veithen2005]]: Δ(ε-1)ij=∑γ=1,3 rijγΕγ

The clamped (zero strain) EO coefficients include an electronic and an ionic
contribution directly accessible within ABINIT. The unclamped EO coefficients
include an additional piezoelectric contribution which must be computed
separately from the knowledge of the elasto-optic and piezoelectric strain
coefficients. This formalism was for instance applied to different
ferroelectric ABO3 compounds [[cite:Veithen2005a]].


## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

## Tutorials

[[tutorial:nlo|The tutorial on static non-linear properties]] presents the
computation of responses beyond the linear order, within Density-Functional
Perturbation Theory (beyond the simple Sum-Over-State approximation): Raman
scattering efficiencies (non-resonant case), non-linear electronic
susceptibility, electro-optic effect. Comparison with the finite field
technique (combining the computation of linear response functions with finite
difference calculations), is also provided.

---
description: How to optimize the geometry under constrained polarization
authors: MT
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to optimize the geometry under constrained polarization with the ABINIT package.

## Introduction

Compute polarization in cartesian coordinates, and update lattice constants
and atomic positions in order to perform a structural optimization at
constrained polarization, following the formalism described in Na Sai et al,
PRB 66, 104108 (2002). More details in [[anaddb:polflag]]. The geometry
optimization is done in ANADDB.

## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

---
authors: FJ, XG
---

# ABINIT features  

## An overview of ABINIT settings and features, for beginners and more experienced users.

This document gives an overview of the features implemented in the ABINIT
package, grouped in different topics, for the beginner as well as more experienced user. 
It might answer the question "How to ... with ABINIT ?", to some extent.  
It also gives a synthetic view on the needed settings.

## 1 Foreword
  
Documenting the features of a large scientific code is a complex task. The
present list of features refers to different "topics". Each topic has a
dedicated page, which should be quick to read, unlike the
[[lesson:index|lessons of the tutorial]], each of which is usually at least
one hour work. Many of the topics make the link between a physical property or
quantity (including bibliographical references for the theory, and sometimes
pointing to published work using this feature) and the way it is to be
computed with ABINIT (e.g. corresponding input variable, example input files,
and possibly tutorial(s)), and the associated post-processing tools (OPTIC, ANADDB, MULTIBINIT ...).

This "topic"-based approach might be used by the beginner to get a broad
overview of the capabilities of ABINIT applications as well as to the more expert user to
quickly find the way to compute some existing quantity, or to remember which
input variable is useful or mandatory for the calculation of a given property.
Some topics are rather "input"-oriented (e.g. how to specify the atomic
geometry, the occupation numbers, etc), other are more "property"-oriented
(e.g. how to compute the elastic constants, the temperature-dependence of the
electronic structure, etc), other are related to proper/better usage of the code.

Care is taken not to duplicate existing more complete documentation in ABINIT,
but to point to it if appropriate. Not all the ABINIT documentation is covered
by the Web-accessible documents, there are still a few unlinked documents in
the subdirectories of ~abinit/doc (work is in progress to make it all available).
Discussions on the [ABINIT discourse forum](https://discourse.abinit.org) might also allow to get information.

## 2 ABINIT specifications for static DFT calculations
  
The topic [[topic:GSintroduction|Building an input file]] briefly explains the
content of an ABINIT input file. The following topics go more into the
details, however restricting to **static DFT calculations**, without doing
anything fancy with the exchange-correlation functionals. Going beyond these
is left for other sections (Section 3 and beyond). In
particular, for any accurate electronic properties, e.g. correct band
structure, optical response, or for strongly correlated electrons, please go
beyond the present sec. 2. Also, topics related to global control parameters,
that apply, generally speaking, to all types of calculations are explained later.

### 2.1 Settings for atoms: cell, atoms, atomic positions, and symmetries
  
  1. [[topic:UnitCell|Unit cell]]
  2. [[topic:AtomTypes|Types of atoms and alchemy]]
  3. [[topic:crystal|Crystalline structure and symmetries]]
  4. [[topic:SmartSymm|Smart symmetrizer]]
  5. [[topic:AtomManipulator|Atom manipulator]] (advanced topic)

### 2.2 Physical settings for electrons: XC functionals, atomic/pseudo potentials, metal/insulator, spin, Coulomb interaction ...
  
  1. [[topic:xc|Overview of available exchange and correlation functionals]]
  2. [[topic:Hybrids|Hybrid functionals]]
  3. [[topic:vdw|Van der Waals functionals]]
  4. [[topic:RPACorrEn|Correlation energy within RPA]]
  5. [[topic:PseudosPAW|Pseudopotentials and PAW atomic datasets]]
  6. [[topic:BandOcc|Bands and occupation numbers for metals and insulators]]
  7. [[topic:spinpolarisation|Spin-polarised systems and spin-orbit coupling]]
  8. [[topic:Coulomb|Coulomb interaction and charged cells]]

### 2.3 Numerical settings for electrons: basis set, planewaves and real space sampling, Brillouin zone sampling ...
  
  1. [[topic:Planewaves|Planewaves and real space sampling]]
  2. [[topic:PAW|PAW special settings]]
  3. [[topic:Wavelets|Wavelets in ABINIT]]
  4. [[topic:k-points|Wavevector sampling (k point grid)]]

### 2.4 SCF algorithms, tuning and stopping criteria
  
  1. [[topic:SCFAlgorithms|SCF algorithms]]
  2. [[topic:SCFControl|SCF control, tolerances and stopping criteria]]
  3. [[topic:ForcesStresses|Forces and stresses]]
  4. [[topic:TuningSpeedMem|Tuning speed and memory usage]]
  5. [[topic:Recursion|Recursion methods and orbital free calculations]] (not in production)

### 2.5 Added electric/magnetic field, other artificial constraints/modifications, and related properties ...
  
  1. [[topic:Berry|Electric polarization and finite electric field]]
  2. [[topic:MagField|External magnetic field]]
  3. [[topic:ConstrainedDFT|Constrained Density-Functional Theory]]
  4. [[topic:MagMom|Constrained atomic magnetic moment]]
  5. [[topic:EFG|Electric field gradients and Mossbauer Fermi contact interaction]]
  6. [[topic:Artificial|Artificial modifications of the system]]

## 3 Global control parameters: flow, parallelism, output files, output content, timing and memory control ...
  
  1. [[topic:multidtset|Multi-dataset calculations]]
  2. [[topic:parallelism|Parallelism and ABINIT]]
  3. [[topic:printing|Printing files]]
  4. [[topic:Output|Tuning the output content in different files]]
  5. [[topic:Control|Time and memory control]]

## 4 Molecular dynamics, geometry optimization, transition paths
  
  1. [[topic:GeoOpt|Geometry optimization]]
  2. [[topic:MolecularDynamics|Molecular dynamics]]
  3. [[topic:TransPath|Transition path searches: NEB and string method]]
  4. [[topic:GeoConstraints|Geometry constraints]]
  5. [[topic:PIMD|Path-integral molecular dynamics (PIMD)]]
  5. [[topic:CrossingBarriers|Crossing barrier search, linear combination of constrained DFT energies and ensemble DFT]]
  6. [[topic:LOTF|Learn-on-the-flight (LOTF)]] (not in production)

## 5 Correlated electrons
  
When correlated electrons are to be considered (in most cases, when *d* and *f*
orbitals play an active role), it is necessary to go beyond the standard DFT
framework. ABINIT enables the following possibilities:

  1. [[topic:Hybrids|Hybrid functionals]]
  2. [[topic:DFT+U|DFT+U approximation]]
  3. [[topic:DMFT|Dynamical Mean Field Theory (DMFT)]]
  4. [[topic:CRPA|Calculation of the effective Coulomb interaction]]

## 6 Adiabatic response properties (phonons, low-frequency dielectric, Raman, elasticity, temperature dependence ...)
  
Many properties can be obtained in the approximation that the electrons **stay
in their ground state** (adiabatic responses). The poweful Density-Functional
Perturbation Theory (DFPT) framework allows ABINIT to address directly all
such properties in the case that are connected to derivatives of the total
energy with respect to some perturbation. This includes all dynamical effects
due to phonons and their coupling, thus also temperature-dependent properties due to phonons.

  1. [[topic:DFPT|Generalities about DFPT]] 
  2. [[topic:q-points|Wavevectors for phonons (q-points)]] 
  3. [[topic:Phonons|Vibrational and dielectric properties 
     (phonon frequencies and modes, IR and Raman spectra, Born effective charges)]]
  4. [[topic:PhononBands|Phonon bands and DOS, interatomic force constants, sound velocity]]
  5. [[topic:Temperature|Temperature dependent properties (free energy, entropy, specific heat, 
     atomic temperature factors, thermal expansion)]]
  6. [[topic:Elastic|Elasticity and piezoelectricity]]
  7. [[topic:nonlinear|Raman intensities and electro-optic properties]]
  8. [[topic:ElPhonInt|Electron-phonon interaction]]
  9. [[topic:PhononWidth|Phonon linewidth due to the electron-phonon interaction]]
  10. [[topic:ElPhonTransport|Electronic transport properties from electron-phonon interaction 
     (resistivity, superconductivity, thermal)]]
  11. [[topic:TDepES|Temperature dependence of the electronic structure from electron-phonon interaction]]
  12. [[topic:ConstrainedPol|Constrained polarization geometry optimization]] (advanced topic)


## 7 Excited state calculations, and frequency-dependent electronic and optical properties
  
Excited-state calculations and frequency-dependent properties (for frequencies
that are non-negligible with respect to the electronic gap), can be addressed
by a variety of methodologies, usually trading accuracy for speed. At the
lowest level, one encounters the independent-particle approximation, building
upon some previously obtained band structure (e.g. Kohn-Sham band structure
from DFT). For charged excitations, allowing to obtain a quasiparticle band
structure without the well-known DFT band gap problem, one has to resort to
(costly) GW calculations. For neutral excitations (i.e. optical), the (costly)
Bethe-Salpeter approach is the most accurate formalism presently available.
TDDFT and Δ-SCF calculations are cheaper but will work well for molecules and
for isolated defects in a solid, not for e.g. correcting the band gap.

  1. [[topic:Optic|Linear and non-linear optical properties in the independent-particle approximation]]
  2. [[topic:FrequencyMeshMBPT|Definition of frequency meshes for Many-body perturbation theory]]
  3. [[topic:Susceptibility|Frequency-dependent susceptibility matrix, and related dielectric matrix and screened Coulomb interaction]]
  4. [[topic:SelfEnergy|Electronic self-energy]]
  5. [[topic:GW|GW calculations for accurate band structure, including self-consistency]]
  6. [[topic:BSE|Bethe-Salpeter calculations for accurate optical properties]]
  7. [[topic:TDDFT|TDDFT calculations]]
  8. [[topic:DeltaSCF|DeltaSCF calculations]]
  9. [[topic:RandStopPow|Random electronic stopping power]]
  10. [[topic:GWls|GW- Lanczos-Sternheimer method]] (not in production)

## 8 Second-principles calculations with MULTIBINIT: handling millions of atoms with first-principles accuracy

By constructing model Hamiltonians whose linear and selected non-linear characteristics
are identical to those from first-principles calculations, and simulating millions
of atoms with these model Hamiltonians, one can study phase transitions, polarization boundaries,  
and other properties for large-scale systems that cannot be reached from first-principles algorithms
implemented in ABINIT and most DFT codes. Even with respect to linear-scaling codes, the prefactor
is much smaller. 
This is implemented in the MULTIBINIT application.

  1. [[topic:LatticeModel|Lattice model at the harmonic level]]
  2. [[topic:FitProcess|FitProcess]]
  3. [[topic:BoundingProcess|BoundingProcess]]
  4. [[topic:DynamicsMultibinit|DynamicsMultibinit]]

## 9 Electronic properties and analysis tools (DOS, STM, Wannier, band plotting and interpolating...)

Many properties are directly deduced from the knowledge of the electronic
wavefunctions, eigenenergies, density, potential, etc. Some necessitates
additional tuning parameters or are linked to postprocessing tools and are
described in the following topics. Some others are actually activated through
a single printing parameter, such as the Electron Localization Function (ELF -
see [[prtelf]]). See the list of "printing" input variables in [[topic:printing]].

  1. [[topic:ElecBandStructure|Electronic band structure and related topics]]
  2. [[topic:ElecDOS|Electronic DOS and related topics]]
  3. [[topic:EffectiveMass|Effective mass calculations]]
  4. [[topic:Unfolding|Unfolding supercell band structures]]
  5. [[topic:DensityPotential|Manipulating the density and potential]]
  6. [[topic:Macroave|Macroscopic average of density and potential]]
  7. [[topic:STM|Scanning Tunneling Microscopy map]]
  8. [[topic:Wannier|Wannier functions]]
  9. [[topic:Bader|Bader Atom-In-Molecule analysis]]

## 10 Other physical properties (e.g. positron)
  
  1. [[topic:positron|Positron calculations]]
  2. [[topic:LDAminushalf|The LDA-1/2 approach]] 

## 11 Analysis/postprocessing tools
  
  1. [[topic:Abipy|Abipy - ABINIT swiss knife]]
  2. [[topic:APPA|Abinit Post-Processor Application (APPA), for molecular-dynamics trajectory analysis]]
  3. [[topic:Band2eps|Band2eps for phonon dispersion curves]]
  4. [[topic:a-TDEP|Temperature Dependent Effective Potential, for thermodynamical properties]]

## 12 Miscellaneous topics
  
  1. [[topic:Verification|Verification of the implementation]]
  2. [[topic:PortabilityNonRegression|Portability and non-regression tests]]
  3. [[topic:Git|Git, gitlab and github for the ABINIT project]]
  4. [[topic:Dev|Miscellaneous for developers]]
---
description: How to calculate the effective Coulomb interaction
authors: BAmadon
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to calculate the effective Coulomb interaction with the ABINIT package.

## Introduction

DFT+U as well as DFT+DMFT requires as input values the effective Coulomb
interaction. Two ways to compute them are available in ABINIT.

Firstly, the constrained Random Phase Approximation [[cite:Aryasetiawan2004]]
[[ucrpa]] allows one to take into account the screening of the Coulomb
interaction between correlated electrons, by non-interacting electrons. For
non-entangled bands ([[ucrpa]]= 1), the bands excluded from the polarisability
can be specified either by a band index ([[ucrpa_bands]]) or an energy window
([[ucrpa_window]]) [[cite:Amadon2014]].

For entangled bands ([[ucrpa]]= 2}), the scheme used in ABINIT
[[cite:Shih2012]], [[cite:Sakuma2013]],[[cite:Amadon2014]] uses a band and
k-point dependent weight to define the polarisability, using Wannier orbitals
as correlated orbitals.

This method is well adapted to compute the effective interaction for the same
orbitals used in DFT+DMFT. To use the same orbitals as in DFT+U, the Wannier
functions can be ajusted such that the bare interaction is close to the bare
interaction of atomic orbitals as used in DFT+ _U_ (see tutorial).

Secondly, a linear response method [[cite:Cococcioni2005]] is implemented. The
implementation is not yet in production. The implementation in ABINIT takes
into account the truncated atomic orbitals from PAW and therefore differs from
the original work [[cite:Cococcioni2005]] treating full atomic orbitals. In
particular, considerably higher effective values for U are found.



## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

## Tutorials

* The [[tutorial:ucalc_crpa|tutorial]] on the calculation of effective interactions U and J by the cRPA method shows how to determine the U value with the constrained Random Phase Approximation [[cite:Aryasetiawan2004]] using projected Wannier orbitals. Prerequisite: DFT+U.
* [[tutorial:udet|The tutorial on the determination of U]] for DFT+U shows how to determine the U value with the linear response method [[cite:Cococcioni2005]], to be used in the DFT+U approach. Prerequisite: DFT+U.

---
description: How to manage file formats, and the interfacing with other applications outside of the ABINIT organisation
authors: XG
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to manage file formats, and the interfacing with other applications outside of
the ABINIT organisation with the ABINIT package.

## Introduction

For a long time, ABINIT has produced files in either text format or in Fortran
binary format. This has the drawback of being difficult to read by other
codes, and also being difficult to maintain in the long run, as any change
must be propagated to the reading routines.

Following the advent of the [Nanoquanta/ETSF file format](http://www.etsf.eu/fileformats), 
ABINIT has gradually shifted toward
the use of file formats that are addressable by content, especially NetCDF and XML.

NetCDF is used to file that typically store a large amount of data, like
wavefunctions, density, potentials, etc . One should even shift to HDF5 in due
time for these files.

XML is used for smaller files. E.g. PSML pseudopotentials can be used, these
being common with the SIESTA code. YAML files have allso recently appeared,
for the ABINIT documentation.


## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

---
description: How to treat adequately the Coulomb interaction, especially in charged cells,
authors: FB
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to treat adequately the Coulomb interaction, 
especially in charged cells, with the ABINIT package.

## Introduction

ABINIT can treat charged systems (e.g. either for molecules, or for dopants in
a supercell), using the [[cellcharge]] input variable. A careful convergence study
with respect to the cell size must however be done.

Depending on the dimension, different treatment of the Coulomb interaction can
be enforced, governed by [[icutcoul]] for ground-state calculations (possibly alternatively [[icoulomb]]
if the Coulomb interaction is to be computed in real space using wavelets), by
[[gw_icutcoul]] for GW calculations, and [[fock_icutcoul]] for the specific evaluation 
of the Fock exchange energy in ground-state calculations with hybrid functionals. 

For charged systems, we provide now 
additional information concerning [[usepotzero]]. It is well known that the
electrostatic potential (arising from ion-ion, ion-electron, and electron-
electron interactions) is ill-defined within periodic boundary conditions.
However, it is less well known that the total energy of a charged cell is also
ill-defined. In fact, after a careful derivation in [[cite:Bruneval2014]], it
was shown that the above two statements are tightly linked: when the number of
electrons differs from the number of protons in a cell, the necessary
compensating background that enforces the overall charge neutrality is
sensitive to the arbitrary average electrostatic potential.

ABINIT offers the possibility to choose which convention to use for the
average electrostatic potential with the keyword [[usepotzero]].

In PAW, one can choose among 3 options:

* the average of smooth electrostatic potential is set to zero;
* the average of all-electron electrostatic potential is set to zero;
* the average of smooth electrostatic potential is set to a finite value, which follows the Quantum Espresso implementation (see [[cite:Giannozzi2009]] for more details).

Only options 1 and 3 are valid for the NCPP case.

None of these conventions is intrinsically more correct than the other ones.
This is just an arbitrary choice, but ABINIT now permits a straight comparison
to the other codes.

## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

---
description: How to perform random stopping power calculation
authors: FB
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to perform random stopping power calculation with the ABINIT package.

## Introduction

The slowing down of a swift charged particle inside condensed matter has been
a subject of intense interest since the advent of quantum-mechanics. The
Lindhard formula [[cite:Lindhard1954]] that gives the polarizability of the
free electron gas has been developed specifically for this purpose. The
kinetic energy lost by the impinging particle by unit of path length is named
the stopping power. For large velocities, the stopping power is dominated by
its electronic contribution: the arriving particle induces electronic
excitations in the target. These electronic excitations in the target can be
related to the inverse dielectric function ε-1( **q** ,ω) provided that linear
response theory is valid.

As a consequence, the electronic stopping power randomized over all the
possible impact parameters reads

S( **v** ) = (4π Z2/N **q** Ω| **v** |)∑ **q** ∑ **G** Im{- ε-1[ **q** ,
**v.** ( **q** + **G** )]} ( **v.** ( **q** + **G** )/| **q** + **G** |2),

where Z and **v** are respectively the charge and the velocity of the
impinging particle,  Ω is the unit cell volume, N **q** is the number of **q**
-points in the first Brillouin zone, and **G** are reciprocal lattice vectors.

Apart from an overall factor of 2, this equation is identical to the formula
published [[cite:Campillo1998]].

The GW module of ABINIT gives access to the full inverse dielectric function
for a grid of frequencies ω. Then, the implementation of the above equation is
a post-processing employing a spline interpolation of the inverse dielectric
function in order to evaluate it at ω= **v.** ( **q** + **G** ). The energy
cutoff on **G** is governed by the [[ecuteps]], as in the GW module. The
integer [[npvel]] and the cartesian vector [[pvelmax]] control the
discretization of the particle velocity.

Note that the absolute convergence of the random electronic stopping power is
a delicate matter that generally requires thousands of empty states together
with large values of the energy cutoff.



## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

---
description: How to perform a GW calculation, including self-consistency
authors: MG
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to perform a GW calculation, including self-consistency with the ABINIT package.

## Introduction

DFT performs reasonably well for the determination of structural properties,
but fails to predict accurate band gaps. A more rigorous framework for the
description of excited states is provided by many-body perturbation theory
(MBPT) [[cite:Fetter1971]], [[cite:Abrikosov1975]], based on the Green's
functions formalism and the concept of quasi-particles [[cite:Onida2002]].

Within MBPT, one can calculate the quasi-particle (QP) energies, E, and
amplitudes, Ψ, by solving a nonlinear equation involving the non-Hermitian,
nonlocal and frequency dependent self-energy operator Σ.

This equation goes beyond the mean-field approximation of independent KS
particles as it accounts for the dynamic many-body effects in the electron-
electron interaction.

Details about the GW implementation in ABINIT can be found [[theory:mbt|here]]

A typical GW calculation consists of two different steps (following a DFT
calculation): first the screened interaction ε-1 is calculated and stored on
disk ([[optdriver]]=3), then the KS band structure and W are used to evaluate
the matrix elements of Σ, finally obtaining the QP corrections
([[optdriver]]=4).

The computation of the screened interaction is described in
[[topic:Susceptibility]], while the computation of the self-energy is
described in [[topic:SelfEnergy]]. The frequency meshes, used e.g. for
integration along the real and imaginary axes are described in
[[topic:FrequencyMeshMBPT]].

GW calculations can be made less memory and CPU time consuming,
at the expense of numerical precision,
by compiling ABINIT with the option enable_gw_dpc=“no" in the *.ac9 file.

The GW 1-body reduced density matrix (1RDM) from the linearized Dyson equation
can be computed, and when used self-consistently with the Galitskii-Migdal correlation, provides an approximation 
the self-consistent GW total energy.

## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

## Tutorials

* [[tutorial:gw1]] The first tutorial on GW (GW1) deals with the computation of the quasi-particle band gap of Silicon (semiconductor), in the GW approximation (much better than the Kohn-Sham LDA band structure), with a plasmon-pole model. 
* [[tutorial:gw2]] The second tutorial on GW (GW2) deals with the computation of the quasi-particle band structure of Aluminum, in the GW approximation (so, much better than the Kohn-Sham LDA band structure) without using the plasmon-pole model. 
* [[tutorial:paral_mbt|Parallelism of Many-Body Perturbation calculations (GW)]] allows to speed up the calculation of accurate electronic structures (quasi-particle band structure, including many-body effects).

---
description: How to perform numerically precise calculations with planewaves or projector- augmented waves and pseudopotentials
authors: FJ
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to perform numerically precise calculations with planewaves or projector-
augmented waves and pseudopotentials with the ABINIT package.

## Introduction

The numerical precision of the calculations depends on many settings, among
which the definition of a basis set is likely the most important. With
planewaves, there is one single parameter, [[ecut]] that governs the
completeness of the basis set.

The wavefunction, density, potentials are represented in both reciprocal space
(plane waves) and real space, on a homogeneous grid of points. The
transformation from reciprocal space to real space and vice-versa is made
thanks to the Fast Fourier Transform (FFT) algorithm. With norm-conserving
pseudopotential, [[ecut]] is also the main parameter to define the real space
FFT grid, In PAW, the sampling for such quantities is governed by a more
independent variable, [[pawecutdg]]. More precise tuning might be done by
using [[boxcutmin]] and [[ngfft]].

Avoiding discontinuity issues with changing the size of the planewave basis
set is made possible thanks to [[ecutsm]].

The [[accuracy]] variable enables to tune the accuracy of a calculation by
setting automatically up to seventeen variables.

Many more parameters govern a PAW computation than a norm-conserving
pseudopotential calculation. They are described in a specific page
[[topic:PAW]]. For the settings related to wavelets, see [[topic:Wavelets]].



## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

## Tutorials

* [[tutorial:base2|The tutorial 2]] deals again with the H2 molecule: convergence studies, LDA versus GGA 
* [[tutorial:base3|The tutorial 3]] deals with crystalline silicon (an insulator): the definition of a k-point grid, the smearing of the cut-off energy, the computation of a band structure, and again, convergence studies ...
* The first tutorial on the [[tutorial:paw1|the projector-augmented wave]] technique.

---
description: How to build an input file for a ground state calculation
authors: FJ
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to build an input file for a ground state calculation with the ABINIT package.

## Introduction

The computation of the ground state energy of an assembly of nuclei and
electrons placed in a repeated cell can be done using (1) plane waves and
norm-conserving pseudopotentials, or, (2) so-called "Projector-Augmented
Waves" (PAW method), with appropriate pseudoatomic data, or (3) wavelets. The
wavelet framework is described [[topic:Wavelets|here]].  

In the plane wave framework, the program admits many different types of
pseudopotentials. There are several complete sets of norm-conserving
pseudopotentials available for most elements of the periodic table. 

The recommended tables (GGA-PBE, GGA-PBEsol and LDA) come from the |pseudodojo| project 
with ONCVPSP pseudopotentials ([[cite:Hamann2013]]) 
both in scalar-relativistic format and fully-relativistic version with spin-orbit coupling. 
For PAW calculation,the recommended one (GGA-PBE and LDA) is the JTH
table in the PAW XML format ([[cite:Jollet2014]]). 

The choice between norm-conserving
pseudopotentials or PAW is deduced automatically by the choice of the
pseudopotential in the "files" file. An input file must specify the following items: 

* the [[topic:crystal|crystalline structure and symmetries]]
* the set of [[topic:k-points|k-points]] used
* the [[topic:xc|exchange and correlation functional]]
* the convergence settings
* possibly [[topic:PAW|PAW]] special settings
* possibly, input variables for [[topic:spinpolarisation|spin-polarized systems and spin orbit coupling]] calculations.


An example of a minimal input file to calculate the ground state of crystalline aluminium is given here:


```
# Crystalline aluminum. Calculation of the total energy
# at fixed number of k points and broadening.

#Definition of occupation numbers
occopt 4
tsmear 0.05

#Definition of the unit cell
acell 3*7.60           # This is equivalent to   7.60 7.60 7.60
rprim  0.0  0.5  0.5   # FCC primitive vectors (to be scaled by acell)
       0.5  0.0  0.5
       0.5  0.5  0.0

#Definition of the atom types
ntypat 1          # There is only one type of atom
znucl 13          # The keyword "znucl" refers to the atomic number of the
                  # possible type(s) of atom. The pseudopotential(s)
                  # mentioned in the "files" file must correspond
                  # to the type(s) of atom. Here, the only type is Aluminum

#Definition of the atoms
natom 1           # There is only one atom per cell
typat 1           # This atom is of type 1, that is, Aluminum
xred  0.0  0.0  0.0 # This keyword indicate that the location of the atoms
                    # will follow, one triplet of number for each atom
                    # Triplet giving the REDUCED coordinate of atom 1.

#Definition of the planewave basis set
ecut  6.0         # Maximal kinetic energy cut-off, in Hartree
pawecutdg  10.0   #Maximal kinetic energy cut-off, in Hartree for the fine grid in case of PAW calculation

#Definition of the k-point grid
ngkpt 2 2 2       # This is a 2x2x2 FCC grid, based on the primitive vectors
chksymbreak 0
#Definition of the SCF procedure
nstep 10          # Maximal number of SCF cycles
toldfe 1.0d-6     # Will stop when, twice in a row, the difference
                  # between two consecutive evaluations of total energy
                  # differ by less than toldfe (in Hartree)
                  # This value is way too large for most realistic studies of materials       
```

## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

## Tutorials

* [[tutorial:base1|The tutorial 1]] deals with the H2 molecule: get the total energy, the electronic energies, the charge density, the bond length, the atomisation energy 
* [[tutorial:base2|The tutorial 2]] deals again with the H2 molecule: convergence studies, LDA versus GGA 
* [[tutorial:base3|The tutorial 3]] deals with crystalline silicon (an insulator): the definition of a k-point grid, the smearing of the cut-off energy, the computation of a band structure, and again, convergence studies ...
* [[tutorial:base4|The tutorial 4]]] deals with crystalline aluminum (a metal), and its surface: occupation numbers, smearing the Fermi-Dirac distribution, the surface energy, and again, convergence studies ...

---
description: How to check for regressions. 
authors: XG
---
<!--- This is the source file for this topics. Can be edited. -->

This page explains how portability and non-regression is managed in the ABINIT project.

## Introduction

ABINIT is tested at each merge in the trunk, on a dedicated test farm, see
[[https://wiki.abinit.org/doku.php?id=bb:slaves]]
and
[[https://wiki.abinit.org/doku.php?id=bb:builder]]. The
numerous tests contained in ~abinit/tests are executed, and compared with
reference files.



## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

---
description: How to set parameters for a multi dataset calculation
authors: XG
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to set parameters for a multi dataset calculation with the ABINIT package.

## Introduction

The simplest use of ABINIT corresponds to one task, with one set of data: for
example, determination of the total energy for some geometry, with some set of
plane waves and some set of k-points.

It is often needed to redo the calculations for different values of some
parameter, letting all the other things equal. As typical examples, we have
convergence studies needed to determine which cut-off energy gives the needed
accuracy. In other cases, one makes chains of calculations in order to compute
the band structure: first a self-consistent calculation of the density and
potential, then the eigenenergy computation along different lines. Similarly,
DFPT, GW or BSE calculations rely on a preliminary calculation of ground-state
wavefunctions.

For such purpose, the multi-dataset mode has been implemented.

It allows the code to treat, in one run, different sets of data, and to chain
them. The number of datasets to be treated is specified by the variable
[[ndtset]], while the indices of the datasets (by default 1, 2, 3, and so on)
can be eventually provided by the arrays [[jdtset]] or [[udtset]].

A full description of the multidataset capabilities of ABINIT can be found in
[[help:abinit#multidataset|the multidataset section of the ABINIT help file]].

A very important mechanism allows to pass information obtained from some
earlier calculation, by defining `get*` input variables. Important examples
are [[getden]] for chaining a self-consistent determination of the density
with a non-self-consistent calculation of the Kohn-Sham band structure, or
[[getwfk]] for chaining a ground-state determination of wavefunctions with a
DFPT or GW computation.

!!! tip

    |AbiPy| provides a programmatic interface to generate input files from python. 
    For futher detail, please consult the |AbinitInputNb|.


## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

---
description: How to set parameters for a calculation with a positron in the system.
authors: MT
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to set parameters for a calculation with a positron in the system with the ABINIT package.

## Introduction

The lifetime of a positron, its annihilation rate and other associated properties can be computed, using
the self-consistent two-component DFT formalism, including force and stress computation (and thus relaxation),
LDA and GGA, wirhin norm-conserving pseudopotentials as well as PAW  approach [[cite:Wiktor2015]].

Doppler broadening can also be computed. The details of the way to perform such
calculations are given in the description of the [[positron]] input variable,
as well as in [[tutorial:positron|the tutorial on electron-positron
annihilation]].

Examples can be found in [[cite:Wiktor2013]], [[cite:Wiktor2014]],
[[cite:Wiktor2014a]] and [[cite:Wiktor2014b]].



## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

## Tutorials

* [[tutorial:positron|The tutorial on electron-positron annihilation]]

---
description: How to compute the linewidth (or lifetime) of phonons, due to the electron-phonon interaction
authors: MV
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to compute the linewidth (or lifetime) of phonons, due to the electron-phonon
interaction with the ABINIT package.

## Introduction

This topic concerns metals only.

After generating a GKK file (see [[topic:ElPhonInt]]), the Electron-Phonon
Coupling (EPC) analysis is performed in anaddb, setting [[anaddb:elphflag]]
variable to 1. Most of the procedure is automatic, but can be lengthy if a
large number of k-points is being used. The [[anaddb:nqpath]] and
[[anaddb:qpath]] variables must be set, specifying a path in reciprocal space.
anaddb generates files containing the phonon linewidths (suffixed `_LWD`) and
frequencies ωqj (suffixed `_BST`) along [[anaddb:qpath]]. One can calculate the
nesting function n(q) = ∑kii' δ(εk,i) δ(εk+q,i') by setting [[anaddb:prtnest]]
to 1 (output to `_NEST`).


## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

## Tutorials

* The tutorial on the [[tutorial:eph|electron-phonon interaction]] presents the use of the utility MRGKK and ANADDB to examine the electron-phonon interaction and the subsequent calculation of superconductivity temperature (for bulk systems).

---
light_gallery: true
---

## Abinit videos

<!-- https://codepen.io/sachinchoolur/pen/XNavPP -->
<div class="md-container">
  <div class="gallery dark">
    <span class="gallery-note">Click on any of the images to see the gallery</span>
    <ul id="video-gallery">
      <li class="video" data-src="https://www.youtube.com/watch?v=DppLQ-KQA68">
        <a href="">
          <img class="img-responsive" src="//img.youtube.com/vi/DppLQ-KQA68/hqdefault.jpg">
          <div class="gallery-poster">
            <img src="https://sachinchoolur.github.io/lightGallery/static/img/play-button.png">
          </div>
        </a>
      </li>
      <li class="video" data-src="https://youtu.be/EfJcYi1MNBg">
        <a href="">
          <img class="img-responsive" src="//img.youtube.com/vi/EfJcYi1MNBg/hqdefault.jpg">
          <div class="gallery-poster">
            <img src="https://sachinchoolur.github.io/lightGallery/static/img/play-button.png">
          </div>
        </a>
      </li>
      <li class="video" data-src="https://www.youtube.com/watch?v=gcbfb_Mteo4"">
        <a href="">
          <img class="img-responsive" src="//img.youtube.com/vi/gcbfb_Mteo4/hqdefault.jpg">
          <div class="gallery-poster">
            <img src="https://sachinchoolur.github.io/lightGallery/static/img/play-button.png">
          </div>
        </a>
      </li>
      <li class="video" data-src="https://youtu.be/UNlRHw9Avvw">
        <a href="">
          <img class="img-responsive" src="//img.youtube.com/vi/UNlRHw9Avvw/hqdefault.jpg">
          <div class="gallery-poster">
            <img src="https://sachinchoolur.github.io/lightGallery/static/img/play-button.png">
          </div>
        </a>
      </li>
      <li class="video" data-src="https://youtu.be/j9z4AJIx40M">
        <a href="">
          <img class="img-responsive" src="//img.youtube.com/vi/j9z4AJIx40M/hqdefault.jpg">
          <div class="gallery-poster">
            <img src="https://sachinchoolur.github.io/lightGallery/static/img/play-button.png">
          </div>
        </a>
      </li>
    </ul>
  </div>
</div>

<script>
$(function() {
  $('#video-gallery').lightGallery();
});
</script>

<!--
<div class="md-container">
  <div class="row"><h2>TEST: Embedly embeds</h2></div>
  <div class="row"> 
    <div class="col-md-4">
      <h2>Abinit github repo</h2>
      <p>
      <a class="embedly-card" data-card-controls="0" href="https://github.com/abinit/abinit">abinit/abinit</a>
      <script async src="//cdn.embedly.com/widgets/platform.js" charset="UTF-8"></script>
    </div>
 
    <div class="col-md-4">
      <h2>abiconfig</h2>
      <p>
      <a class="embedly-card" data-card-controls="0" href="https://github.com/abinit/abiconfig">abinit/abiconfig</a>
      <script async src="//cdn.embedly.com/widgets/platform.js" charset="UTF-8"></script>
    </div>
 
    <div class="col-md-4">
      <h2>abiconda</h2>
      <p>
      <a class="embedly-card" data-card-controls="0" href="https://github.com/abinit/abiconda">abinit/abiconda</a>
      <script async src="//cdn.embedly.com/widgets/platform.js" charset="UTF-8"></script>
    </div>
 
  </div> <!-- /row -->
  <hr>
</div> <!-- /container -->
-->

<!--
<blockquote class="embedly-card" data-card-controls="0"><h4><a href="http://pythonhosted.org/abipy/">Getting Started - abipy 0.2.0 documentation</a></h4><p>abipy is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p></blockquote>
<script async src="//cdn.embedly.com/widgets/platform.js" charset="UTF-8"></script>
-->

<a class="embedly-card" data-card-controls="0" href="http://www.pseudo-dojo.org/">The Periodic Table with tested pseudopotentials</a>
<script async src="//cdn.embedly.com/widgets/platform.js" charset="UTF-8"></script>

<a class="embedly-card" data-card-controls="0" href="http://jp-minerals.org/vesta/en/">VESTA</a>
<script async src="//cdn.embedly.com/widgets/platform.js" charset="UTF-8"></script>

<a class="embedly-card" data-card-controls="0" href="https://www.materialsproject.org/">Materials Project</a>
<script async src="//cdn.embedly.com/widgets/platform.js" charset="UTF-8"></script>

<a class="embedly-card" data-card-controls="0" href="http://www.aflowlib.org/">Card</a>
<script async src="//cdn.embedly.com/widgets/platform.js" charset="UTF-8"></script>

<a class="embedly-card" data-card-controls="0" href="http://www.aflowlib.org/CrystalDatabase/">Card</a>
<script async src="//cdn.embedly.com/widgets/platform.js" charset="UTF-8"></script>

<a class="embedly-card" data-card-controls="0" href="http://schmeling.ac.rwth-aachen.de/cohp/index.php?menuID=6">COHP - Crystal Orbital Hamilton Population * Download</a>
<script async src="//cdn.embedly.com/widgets/platform.js" charset="UTF-8"></script>
---
description: How to control the flow of ABINIT
authors: XG
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to control the flow of ABINIT with the ABINIT package.

## Introduction

A few input variables governs the naming or prefix of input/output/temporary files (e.g. [[pseudos]], [[output_file]] ...), their path,
the possible stopping of ABINIT (e.g. according to a maximal limit on CPU time), or the details of the ABINIT flow for specific cases.

## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

---
description: How to compute the matrix elements of the electron-phonon interaction
authors: MV
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to compute the matrix elements of the electron-phonon interaction with the ABINIT package.

## Introduction

The theory and details of the implementation are described in [[cite:Gonze2009]] and [[cite:Gonze2016]].

Basic calculations of electron-phonon interaction in ABINIT: one performs a
normal ground state, then DFPT phonon calculations (using [[rfphon]], with
added keywords [[prepgkk]] and [[prtgkk]], which saves the matrix elements to
files suffixed GKK. The main change in this respect is that [[prtgkk]] now
disables the use of symmetry in reducing q-points and perturbations. This
avoids ambiguities in wave function phases due to band degeneracies. The
resulting GKK files are merged using the mrggkk utility, and processed by anaddb.

With the implementation of phonons in PAW DFPT, the electron phonon coupling
is also available in PAW, though this has not yet been tested extensively. The
input variables for electron-phonon coupling in anaddb are described in
[[cite:Gonze2009]] and [[cite:Gonze2016]].

Some details about the calculation of electron-phonon quantities in ABINIT and
ANADDB can be found [[pdf:elphon_manual.pdf|here]].

Subsequently, the GKK file is used to compute many quantities, as explained in
[[topic:PhononWidth]], [[topic:TDepES]] and [[topic:ElPhonTransport]].

A brand new ABINIT driver, focusing on the treatment of electron-phonon
interaction is under heavy development. Most of the input variables for experts,
with [[optdriver]]==7 are related to this development. It is operational
as of v9.2, although the documentation is not yet fully upgraded.

## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

## Tutorials

* [[tutorial:rf1|The tutorial Response-Function 1 (RF1)]] presents the basics of DFPT calculations within ABINIT. The example given is the study of dynamical and dielectric properties of AlAs (an insulator): phonons at Gamma, dielectric constant, Born effective charges, LO-TO splitting, phonons in the whole Brillouin zone. The creation of the "Derivative Data Base" (DDB) is presented.

* (Legacy implementation) [[tutorial:eph|The tutorial on the electron-phonon interaction]] presents the use of the utility MRGKK and ANADDB to examine the electron-phonon interaction and the subsequent calculation of superconductivity temperature (for bulk systems).
Also there is a tutorial for [[tutorial:tdepes|the temperature dependence of the electronic structure]]:.

* (New implementation) Three tutorials for the new procedure are available at [[tutorial:eph_intro|an overview of the EPH code]], 
[[tutorial:eph4zpr|Zero-point renormalization of the band gap and temperature-dependent band gaps]], and
[[tutorial:eph4mob|Phonon-limited mobility]]
:.
---
description: How to use the APPA post-processing tool for the analysis of molecular dynamics output files (trajectories)
authors: SS, XG, YG
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to use the APPA post-processing tool for the analysis of molecular dynamics
output files (trajectories) with the ABINIT package.

## Introduction

APPA is a graphical (or text) post-processor for ABINIT dedicated to the
analysis of Molecular Dynamics output files (trajectories).

Output files from large scale molecular dynamics simulations are not easy to
manage. All the data can be extracted from output files in NetCDF or
ACSCII(text) formats. APPA is a graphical tool written in python (PyQt4
library) able to read ABINIT output, easy to use in order to follow a
trajectory from molecular dynamics simulation (total energy, pressure...). It
is also possible to calculate velocity autocorrelation function from the
simulation, radial pair distribution, vibrational density of states, etc....
If Python is able to handle efficiently these kinds of output files, this one
is not suited to compute some quantities coming from large-scale MD
simulation. That?s why APPA also uses Fortran, and thanks to the F2PY library,
the connection between these both languages is possible.



## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

---
description: How to set parameters related to the phonon wavevectors (q-points) in DFPT calculations
authors: FJ
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to set parameters related to the phonon wavevectors (q-points) in DFPT
calculations with the ABINIT package.

## Introduction

Like the electronic wavefunctions, the collective atomic displacements that
are eigenmodes of the corresponding periodic Hamiltonian can be characterized
by a wavevector, denoted q-point.

In ABINIT, DFPT calculations for one dataset are done for one specific
q-point, that must be specified. In the simplest case, the user gives the
corresponding q-point for each dataset, setting [[nqpt]]=1 and specifying the
corresponding single [[qpt]]. However, very often, it is needed to run
calculations for dozens or hundreds of q-points. Hence, the following
mechanism has been set: the use can specify a set of q points, using input
variables similar to the k-points, and then, for each dataset, the number of
the q-point in the set is indicated thanks to [[iqpt]]. This applies to the
generation of q-point grids as well as to q-point paths to produce phonon band
structures.

The input variables for specifying q-points in ANADDB are specified in
[[topic:Phonons]] and [[topic:PhononBands]].



## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

---
description: How to analyze the densities and potentials
authors: SS, XG, YG
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to analyze the densities and potentials with the ABINIT package.

## Introduction

All the files that have the density and potential format, see
[[topic:printing]] can be analyzed with the "Cut3D" postprocessor. In
particular, it can produce two-dimensional cuts (or one-dimensional cuts)
through the three-dimensional data, suitable for later visualisation using
e.g. [[topic:Abipy]]. It can perform the Hirshfeld computation of atomic
charges. It can analyse the charge contained in an atomic sphere, and
determine the angular momentum projected charge (l=0 to 4) contained in that
sphere. (only available for norm-conserving pseudopotentials)

See the [[help:cut3d]], as well as the [[tutorial:cut3d]].

## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

## Tutorials

* [[tutorial:cut3d]] explains the use and input parameters needed for the "Cut 3-Dimensional files" post-processor of the ABINIT package

---
description: How to set parameters for a spin-polarized calculation
authors: FJ
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to set parameters for a spin-polarized calculation with the ABINIT package.

## Introduction

The electronic system may be computed in the spin-unpolarized or spin-
polarized case, with the possibility to impose occupation numbers of majority
and minority spins, and the spins of the starting configuration. A specific
option for efficient treatment of anti-ferromagnetism (Shubnikov groups) is
available. The treatment of non-collinear magnetism is available (some details
of the implementation can de found [[pdf:noncol|here]]. The total magnetic
moment of the unit cell can be constrained. The local magnetization can also
be constrained.



## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

## Tutorials

* [[tutorial:spin|The tutorial on spin in ABINIT]] presents the properties related to spin: spin-polarized calculations and spin-orbit coupling. 

---
description: How to use the Learn-of-the-flight feature
authors: XG
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to use the Learn-of-the-flight feature with the ABINIT package.

## Introduction

Under development, for experts only.


## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

---
description: How to compute linear and non-linear optical properties in the independent-particle approximation
authors: SS, XG, YG 
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to compute linear and non-linear optical properties 
in the independent-particle approximation with the Abinit package.

## Introduction

Optical and non-linear optical properties can be computed with different
levels of approximation.

The simplest (and fastest) approach relies on the independent-particle
approximation (IPA): the electrons are supposed independent of each other when
reacting to the optical perturbation (even if the initial computation of the
band structure includes interactions in a mean-field sense, like with DFT).
This approximation is also referred to as a "Sum-Over-States" approach (SOS).
This neglects all electron-hole interaction (so no excitonic effects), but
might provide meaningful results in many case, sometimes even quantitatively.
A first problem is linked with the erroneous band gap of the material, but
this can be corrected by a scissor approximation, see [[scissor@optic]].

In Abinit one can either work in the IPA (see below), or take into account the
excitonic effects, see [[topic:BSE]].

In the Abinit package, there are two different utilities to compute optical
responses in the independent-particle approximation: [[help:optic]] and
conducti [[cite:Mazevet2010]]. They have been developed independently of each other, and thus
overlap significantly. The first one computes the linear and non-
linear optical properties as a function of the frequency. It provides the
optical dielectric tensor, the second-harmonic generation (SHG) as well as the
optical rectification tensor (or electro-optic tensor) - without the
contribution from the nuclear displacements. For the further inclusion of the
contribution from nuclear displacements, see [[topic:nonlinear]].

The second utility "conducti" has more capabilities but only at the linear level,
providing the electronic conductivity, dielectric tensor, index of refraction,
reflectivity, absorption, the thermal conductivity, and the thermopower
(electron transport, high temperature, Kubo-Greenwood formalism), the real as well
as imaginary parts.



## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

## Tutorials

* See [[tutorial:optic|The tutorial on Optic]], the utility that allows to obtain the
frequency dependent linear optical dielectric function and the frequency
dependent second order nonlinear optical susceptibility, in the simple
"Sum-Over-States" approximation.

---
description: How to compute the electronic self-energy (due to electron-electron interaction)
authors: MG
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to compute the electronic self-energy (due to electron-electron interaction) with the ABINIT package.

## Introduction

In principle, the exact self-energy can be obtained by solving self-consistently 
the set of coupled integro-differential equations proposed by
Hedin [[cite:Hedin1965]]. The fundamental building blocks of Hedin's equations
are, besides $\Sigma(1,2)$, the Green's function of the interacting many-body system,
$G(1,2)$, the Green's function of an appropriate non-interacting system, $\Go(1,2)$,
and the irreducible polarizability, $\tchi(1,2)$, which, through the inverse
dielectric matrix $\ee^{-1}(1,2)$, re-normalizes the static Coulomb potential,
resulting in the dynamical screened interaction $W(1,2)$. Finally, the vertex
function $\Gamma(1,2,3)$ describes the interactions between virtual holes and
electrons.

A typical self-energy calculation combines a quasi-particle band structure
with a screened interaction and possibly a vertex correction to the QP
corrections ([[optdriver]]=4).

In the frequency domain, the GW self-energy $\Sigma(\omega)$ can be evaluated in ABINIT
with two different, more effective, techniques:

* integration with a plasmon-pole model (PPM)
* integration with contour deformation (CD).

In the former case, the frequency dependence of $\ee^{-1}(\omega)$, is modeled with a
simple analytic form, and the frequency convolution is carried out analytically.
In the latter approach, the integral is evaluated numerically extending the
functions in the complex plane in order have a smoother integrand.

Four different plasmon pole models (PPMs) are available in ABINIT. The choice
of the particular PPM to be used is controlled by the variable [[ppmodel]].
The first two options ([[ppmodel]] = 1, 2) refer to approximations employed in
the pioneering implementations of the GW formalism: the plasmon-pole models of
Godby-Needs [[cite:Godby1989]] (GN) and Hybertsen and Louie [[cite:Hybertsen1986]] (HL).

The contour deformation technique is activated by setting the input variable
[[gwcalctyp]] to 2. The integration along the imaginary axis requires the
calculation of $\ee^{-1}(\omega)$, for purely imaginary frequencies. 
The frequency mesh for the quadrature is governed by the input variable [[nfreqim]], and can be very
coarse since the integrands is very smooth in this region.

The evaluation of the residue of the poles requires the calculation of $\ee^{-1}(\omega)$
on a fine mesh along the real axis. This regular mesh, sampling the interval
[0, +∞], is defined by the two input variables [[nfreqre]] and [[freqremax]].

The CD approach requires many evaluations of $\ee^{-1}(\omega)$ and can therefore be
computationally highly demanding. On the other hand, it is the preferred
approach for calculating the QP correction of low-lying states. Moreover, it
is the only technique available in ABINIT to compute the imaginary part of
$\Sigma(\omega)$ and the spectral function $A(\omega)$.

It is possible to disable the full computation, and actually do an Hartree-Fock, 
screened exchange, COHSEX or hybrid functional calculation.
The calculation is done in a precomputed basis set, that can be Kohn-Sham
(e.g. PBE) or generalized Kohn-Sham (e.g. HSE06).

As vertex corrections, the bootstrap kernel and others can be included in the
self-consistent W. 
The Faleev method ([[cite:Faleev2004]]), is implemented.

Convergence over the number of unoccupied band is much improved with respect
to usual implementations of GW, thanks to the "extrapolar" method.

The frequency meshes, used e.g. for integration along the real and imaginary
axes are described in [[topic:FrequencyMeshMBPT]].


## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

## Tutorials

* The first tutorial on GW ([[tutorial:gw1|GW1]]) deals with the computation of the quasi-particle band gap of Silicon (semiconductor), in the GW approximation (much better than the Kohn-Sham LDA band structure), with a plasmon-pole model. 
* The second tutorial on GW ([[tutorial:gw1|GW2]]) deals with the computation of the quasi-particle band structure of Aluminum, in the GW approximation (so, much better than the Kohn-Sham LDA band structure) without using the plasmon-pole model. 
* [[tutorial:paral_mbt|The tutorial on Parallelism of Many-Body Perturbation calculations (GW)]] allows to speed up the calculation of accurate electronic structures (quasi-particle band structure, including many-body effects).

---
description: How to generate the electronic DOS and related topics
authors: XG
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to generate the electronic DOS and related topics with the ABINIT package.

## Introduction

A `_DOS` file can be produced either using the tetrahedron method or smearing
approaches. The partial (orbital weighted) DOS can also be obtained. See the
detailed explanation in [[prtdos]].

Different plotting post-processor allows one to represent the DOS, including [[topic:Abipy|Abipy]].


## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

## Tutorials

* PAW1 tutorial, section on [[tutorial:paw1#dos|Plotting PAW contributions to the Density of States]]

---
description: How to perform an effective mass calculation
authors: JLaflamme
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to perform an effective mass calculation with the ABINIT package.

## Introduction

The direct estimation of effective masses from DFT band curvature using
[[topic:DFPT]] has been implemented within the linear response part of ABINIT
[[cite:Laflamme2016]]. This method avoids the use of finite differences to
estimate these masses, which eliminates the associated numerical noise and
convergence study. To compute the effective masses, one has to set the keyword
[[efmas]] to 1 within a calculation of the derivative of ground-state
wavefunctions with respect to wavevector ([[rfelfd]] = 1 or 2). The effective
masses will then be computed for all k-points and bands present in the
calculation. One can optionally specify the range of bands to be treated for
each k-point with the keyword [[efmas_bands]].

An additional feature of the effective mass implementation is the correct
treatment of degenerate bands. Indeed, the concept of effective mass breaks
down at degenerate band extrema since it is no longer possible to describe
band curvature using a tensor [[cite:Luttinger1955]], [[cite:Mecholsky2014]].
However, using the concept of ``transport equivalent effective mass''
[[cite:Mecholsky2014]] and its adaptation to the **k.p** framework, the
implementation is able to provide the user with effective mass tensors which,
while not describing the band curvature, describe accurately the contribution
of the individual bands to transport properties.

The implementation supports both NCPP and PAW schemes.

Spin-polarized systems ([[nspden]] = 2) as well as spinors ([[nspinor]] = 2)
can be treated, although the spin-orbit interaction can only be treated in the
PAW case.

The treatment of degeneracies is limited to the extremal points of the band
structure (which are the most relevant in any case).

By the way, the first derivative of the eigenenergies is also computed and
printed during a d/dk calculation, and corresponds to the electronic velocity.



## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

---
description: How to set parameters related to the electronic wavevectors (k-points)
authors: FJ
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to set parameters related to the electronic wavevectors (k-points) with the ABINIT package.

## Introduction

Since ABINIT is based on periodic boundary conditions, every wavefunction is
characterized by a wavevector, usually denoted k-point.
Any list of k-point can be specified, thanks to the keywords [[nkpt]] and [[kpt]].

Still, k-points are used in two different contexts in the vast majority of cases:

  * the sampling of the Brillouin Zone, with the goal to produce integrated quantities 
    (e.g. the charge density, the electronic energy, the electronic DOS ...) that are numerically precise;
  * or the specific computation of wavefunctions and eigenenergies e.g. to get an electronic band structure. 

In the first case, please complete the present topic by reading
[[topic:BandOcc]], while in the second case, please read [[topic:ElecBandStructure]].

In the first case, the Brillouin zone must be sampled adequately, with grids
that, in general will be homogeneous distributions of k-points throughout the
Brillouin Zone (e.g. Monkhorst-Pack grids, or their generalisations). 
For such grids, see [[ngkpt]], [[nshiftk]], [[shiftk]] or even the more general [[kptrlatt]]. 
A list of interesting k point sets can be generated automatically, including a measure 
of their accuracy in term of integration within the Brillouin Zone, see [[prtkpt]], [[kptrlen]]. 
For metals, a joint convergence study on [[tsmear]] and the k-point grid is important.

For the definition of a path of k-points, see [[topic:ElecBandStructure]].  

More detailed explanation concerning the convergence with respect to the
k-point sampling. The number of k-points to be used for this sampling, in the
full Brillouin zone, is inversely proportional to the unit cell volume, but
may also vary a lot from system to system. As a rule of thumb, a system with a
large band gap will need few k-points, while metals will need lot of k-points
to produce converged results. For large systems, the inverse scale with
respect to the unit cell volume is unfortunately stopped because at least one
k-point must be used. The effective number of k-points to be used will be
strongly influenced by the symmetries of the system, since only the
irreducible part of the Brillouin zone must be sampled. 
Moreover the time-reversal symmetry (k equivalent to -k) can be used for ground-state
calculations, to reduce sometimes even further the portion of the Brillouin
zone to be sampled. 
The number of k points to be used in a calculation is named [[nkpt]]. 

There is another way to take advantage of the time-reversal
symmetry, in the specific case of k-points that are invariant under k => -k ,
or are sent to another vector distant of the original one by some vector of
the reciprocal lattice. See below for more explanation about the advantages of
using these k-points.  

As a rule of thumb, for homogeneous systems, a reasonable accuracy may be
reached when the product of the number of atoms by the number of k-points in
the full Brillouin zone is on the order of 50 or larger, for wide gap
insulators, on the order of 250 for small gap semiconductors like Si, and
beyond 500 for metals, depending on the value of the input variable [[tsmear]]. 
As soon as there is some vacuum in the system, the product [[natom]] * [[nkpt]] can be
much smaller than this (for an isolated molecule in a sufficiently large
supercell, one k-point is enough).


## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

## Tutorials

* [[tutorial:base3|The tutorial 3]] deals with crystalline silicon (an insulator): the definition of a k-point grid, the smearing of the cut-off energy, the computation of a band structure, and again, convergence studies ...

---
description: How to perform calculations on a wavelet basis
authors: MT
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to perform calculations on a wavelet basis with the ABINIT package.

## Introduction

A wavelet basis (instead of a plane wave basis) can be used in ABINIT. With a
wavelet basis, one can perform basic static DFT calculations with selected
norm-conserving pseudopotentials (HGH or GTH pseudopotentials
[[cite:Genovese2008]]), but also with PAW atomic data [[cite:Rangel2016]]).
Available also : the finite size corrections to the total energy, restart on
wavefunctions following the ETSF norm and geometry relaxation using BFGS.
Molecular dynamic is also available for test purposes.

However, DFPT or excited-state calculations (except Δ-SCF) cannot be
performed.



## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

## Tutorials

* [[tutorial:paral_gswvl|Parallelism for ground-state calculations, with wavelets]] presents the parallelism of ABINIT, when wavelets are used as a basis function instead of planewaves, for the computation of total energy, density, and ground state properties

---
description: How to compute transport properties that are determined by the electron-phonon interaction (electrical resistivity, superconductivity, thermal conductivity)
authors: MV
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to compute transport properties that are determined by the electron-phonon
interaction (electrical resistivity, superconductivity, thermal conductivity) with the ABINIT package.

## Introduction

Warning : this topic concerns metals only.

The calculation of bulk transport quantities (electrical and thermal
resistivities - the part that is determined by the electron-phonon
interaction) is possible using anaddb. Analogous quantities are obtained from
the conducti post-processor, but due to electron-electron scattering, instead
of electron-phonon.

A preliminary calculation of the derivatives of the wavefunctions with respect
to k-vector must be carried out. After generating a GKK file (see
[[topic:ElPhonInt]]), the Electron-Phonon Coupling (EPC) analysis is performed
in anaddb, setting [[anaddb:elphflag]] variable to 1. Most of the procedure is
automatic, but can be lengthy if a large number of k-points is being used.

While the legacy implementation of the transport properties in ABINIT is quite stable,
there is a new implementation under heavy development.
Most of the present information relates to the legacy implementation,
although some also relates to the
most recent procedure that relies on [[optdriver]]=7. The documentation
of the new procedure is given mostly by the related tutorials (introduction and mobility), see below.
Another tutorial for the new procedure for superconductivity calculations is still under development.


For the superconductivity calculations (legacy implementation), The electron-phonon interaction is
interpolated in reciprocal space, then integrated over the Fermi surface to
give the Eliashberg function. Several quadrature methods are available. The
default ([[anaddb:telphint]]=1) is to use Gaussian weighting, with a width
[[anaddb:elphsmear]]. Another option is the improved tetrahedron
[[cite:Bloechl1994a]] ([[anaddb:telphint]]=0). Finally
([[anaddb:telphint]]=2), one can integrate a given set of electron bands,
between [[anaddb:ep_b_max]] and [[anaddb:ep_b_min]]. The resulting integrated
quantities are the Eliashberg function (in a file suffixed `_A2F`), and the EPC
strength λ which is printed in the main output file.

The transport calculation is turned on by setting [[anaddb:ifltransport]] to 1
in anaddb. The transport quantities depend on the Fermi velocity for each
band, and the electronic band-dependence of the matrix elements must be
preserved before integration, by setting [[anaddb:ep_keepbands]] to 1. This
increases the memory used, by the square of the number of bands crossing EF.
The results are the transport Eliashberg function (in file `_A2F_TR`), the
electrical resistivity (in file `_RHO`), and the thermal conductivity (in file `_WTH`).

It is also possible to consider the temperature dependence of the Fermi
energy: cubic spline interpolation ([[anaddb:ep_nspline]]) enables to linearly
interpolate the transport arrays and reduce the memory usage. Besides setting
the Fermi level with [[anaddb:elph_fermie]] (in Hartree), it is also possible
to specify the extra electrons per unit cell, (i.e., the doping concentration
often expressed in cm-3) with [[anaddb:ep_extrael]].

Some details about the calculation of electron-phonon quantities in ABINIT and
ANADDB can be found [[pdf:elphon_manual|here]].


## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

## Tutorials

* (Legacy approach) [[tutorial:eph|The tutorial on the electron-phonon interaction]] presents the use of the utility MRGKK and ANADDB to examine the electron-phonon interaction and the subsequent calculation of superconductivity temperature (for bulk systems).

* (New procedure) Two tutorials are available at [[tutorial:eph_intro|an overview of the EPH code]], and
at [[tutorial:eph4mob|Phonon-limited mobility]]:.
---
description: How to compute vibrational free energy, entropy, specific heat, thermal expansion, as well as atomic temperature factors
authors: XG
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to compute vibrational free energy, entropy, specific heat, thermal expansion, as
well as atomic temperature factors with the ABINIT package.

## Introduction

When the phonon band structure and corresponding eigenvectors are known over
the whole Brillouin Zone, thanks fo Fourier interpolation (see
[[topic:PhononBands]]), integrals can be performed, allowing to obtain a
wealth of properties, like free energy, entropy, specific heat, as well as
atomic temperature factors. For applications of this technique, see
[[cite:Lee1995]].

Moreover, knowing such information for different volumes allows one to compute
the thermal expansion, see [[anaddb:gruns_ddbs]].

The input variables needed to perform the interpolation over the Brillouin
Zone are described in [[topic:PhononBands]] and are not listed again in the
present topic.


## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

## Tutorials

* [[tutorial:rf2|The tutorial Response-Function 2 (RF2)]] presents the analysis of the DDBs that have been introduced in the preceeding tutorial RF1. The computation of the interatomic forces and the computation of thermodynamical properties is an outcome of this tutorial.

---
description: How to perform a Bader analysis
authors: PCasek, FF, XG
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to perform a Bader analysis with the ABINIT package.

## Introduction

The postprocessor of ABINIT, called AIM (Atom-In-Molecule), performs the Bader
analysis on the basis of the charge density, norm-conserving pseudopotential
cases only, complemented by the core charge densities provided in
[[https://www.abinit.org/downloads/core_electron]]. There is a specific
[[help:aim]] for this ABINIT postprocessor.


## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

---
description: How to perform a LDA-1/2 calculation
authors: F. Jollet, G. Zerah
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to perform a LDA-1/2 calculation with the ABINIT package.

## Introduction

This feature is available only in PAW. The LDA-1/2 framework is described in
[[cite:Ferreira2008]]. In ABINIT, the LDA-1/2 approximation needs to use PAW
data files that contain a small potential vminushalf that is added to the
local potential inside the code. The LDA-1/2 approach is based on Slater's
half occupation technique to restore the band gap of semi-conductors and
insulators.


## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

---
description: How to set parameters related to the exchange and correlation functionals
authors: FJ
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to set parameters related to the exchange and correlation functionals with the ABINIT package.

## Introduction

Total energy computation in ABINIT is done according to Density Functional
Theory (DFT). Although formally exact, an approximate exchange-correlation
(XC) functional must be chosen. This is governed by the input variable
[[ixc]].  
However, the pseudopotentials (or PAW data sets) are constructed for one
specific XC functional. If [[ixc]] is not specified, ABINIT will simply take
the [[ixc]] of the given pseudopoential(s) - hoping they are coherent with
each others. One introduces an error by using a pseudopotential generated with
an XC functional that is not the same as the one explicitly specified by
[[ixc]]. However, ABINIT will nevertheless do the calculation.

Many exchange-correlation functionals are available (see the list in the
description of [[ixc]]), through two different implementations: one is the
native ABINIT implementation, the other is the ETSF library of XC functionals
(LibXC is a plug-in to ABINIT). In the native ABINIT set, most of the
important local approximations (LDA) are available, including the Perdew-
Zunger one. Two different local spin density (LSD) are available, including
the Perdew Wang 92, and one due to M. Teter. The Perdew-Burke-Ernzerhof, the
revPBE, the RPBE and the HCTH GGAs (spin unpolarized as well as polarized) are
also available.  
In the LibXC 2.0 library, as interfaced with ABINIT, there are 24 functional
forms of the 3D LDA type, and over 80 functional forms of the GGA type. They
can be used with norm-conserving pseudopotentials as well as PAW atomic data.
Also, some metaGGA can be used with ABINIT (norm-conserving case only). 
They need [[usekden]]=1.
In particular, the TB09 (not delivering reliable total energies) 
allows one to get cheap corrected band structures (use [[ixc]]=-12208, with HGH pseudopotentials).
For response-function type calculations, the native ABINIT LDA and GGA kernels can
be used as well as the LibXC ones.  

#### **Hybrid functionals:**

  
The exchange can also be computed on the basis of the Fock expression (exact
exchange), and the correlation can be computed on the basis of the RPA
approximation (see the GW section). [[topic:Hybrids|Hybrid functionals]]
calculations (HSE06, PBE0, B3LYP) can be performed. The implementation of the
exact exchange, correlation and hybrid does not deliver the forces and
stresses at present, at the exception of forces with norm-conserving
pseudopotentials.

#### **Local exact exchange:**

When [[useexexch]]=1, the hybrid functional PBE0 is used in PAW, inside PAW
spheres only, and only for correlated orbitals given by [[lexexch]]. To change
the ratio of exact exchange, see also [[exchmix]]. The implementation of local
exact exchange in ABINIT is provided in [[cite:Jollet2009]]. See useful input
variables [[exchmix]], [[lexexch]] and [[useexexch]].  
  

#### **Van der Waals functionals:**

  
[[topic:vdw|Several Van der Waals functionals]] are available: Grimme (D2, D3,
D3(Becke-Johnson)), Silvestrelli.



## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

## Tutorials

* The [[tutorial:base2]] deals with the H2 molecule: convergence studies, LDA versus GGA 

---
description: How to tune the computation of forces and stresses
authors: FJ
---
<!--- This is the source file for this topics. Can be edited. -->

This page gives hints on how to tune the computation of forces and stresses with the ABINIT package.

## Introduction

Hellman-Feynman forces are computed from an analytical formula, and
corresponds exactly to the limit of finite differences of energy for
infinitesimally small atomic displacements when the ground-state calculation
is at convergence. This feature is available for all the cases where the total
energy can be computed. A correction for non-converged cases allows to get
accurate forces with less converged wavefunctions than without it. The
decomposition of the forces in their different components can be provided.

Stress can also be computed. This feature is available for all the cases where
the total energy can be computed (except wavelets). The decomposition of the
stresses in their different components can be provided. A smearing scheme
applied to the kinetic energy [[ecutsm]] allows one to get smooth energy
curves as a function of lattice parameters and angles. A target stress can be
given by the user ([[strtarget]]), the geometry optimization algorithm will
try to find the primitive cell and atomic positions that deliver that target stress.

The computation of forces and stresses is optional, see [[optforces]] and
[[optstress]]. They are used to define SCF stopping criteria ([[toldff]],
[[tolrff]]) or geometry optimization stopping criteria ([[tolmxf]]). For the
geometry optimization, combined cell shape and atomic position optimization
need a conversion scale, set by [[strprecon]].



## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}

## Tutorials

* [[tutorial:base1|The tutorial 1]] deals with the H2 molecule: get the total energy, the electronic energies, the charge density, the bond length, the atomisation energy
* [[tutorial:base2|The tutorial 2]] deals again with the H2 molecule: convergence studies, LDA versus GGA
* [[tutorial:base3|The tutorial 3]] deals with crystalline silicon (an insulator): the definition of a k-point grid, the smearing of the cut-off energy, the computation of a band structure, and again, convergence studies ...
* [[tutorial:base4|The tutorial 4]] deals with crystalline aluminum (a metal), and its surface: occupation numbers, smearing the Fermi-Dirac distribution, the surface energy, and again, convergence studies ...
jquery-dialogextend 2.0.4 [![project status](http://stillmaintained.com/ROMB/jquery-dialogextend.png)](http://stillmaintained.com/ROMB/jquery-dialogextend) [![Build Status](https://travis-ci.org/ROMB/jquery-dialogextend.png?branch=master)](https://travis-ci.org/ROMB/jquery-dialogextend)
===
Download
===
[development build](https://raw.github.com/ROMB/jquery-dialogextend/master/build/jquery.dialogextend.js)

[minified library](https://raw.github.com/ROMB/jquery-dialogextend/master/build/jquery.dialogextend.min.js)

Compatible
===
- jQuery 1.11.1
- jQueryUI 1.11.0

Overview
===
- Neat, simple, and ABSOLUTELY unobtrusive
- Extending (instead of replacing) original jQuery UI dialog
- Maximize and minimize buttons
- Show/Hide close button
- Double-clickable title bar
- Enhanced title bar options
- Configurable icons
- Custom events

Demo
===
- Test Tool : [http://romb.github.io/jquery-dialogextend/example.html](http://romb.github.io/jquery-dialogextend/example.html)

Tested Browsers
===
- Chrome 35
- Firefox 14
- IE 8

Please support this project
===

Donate Bitcoins: 1G8T7Xh2AN5ceduHmHT5TpPFUeddsnQHLQ

Options
===

#### closable ####
Type: *Boolean*

Usage: enable/disable close button

Default: *true*


#### maximizable ####
Type: *Boolean*

Usage: enable/disable maximize button

Default: *false*

#### minimizable ####

Type: *Boolean*

Usage: enable/disable minimize button

Default: *false*

#### collapsable ####
Type: *Boolean*

Usage: enable/disable collapse button

Default: *false*

#### minimizeLocation ####

Type: *String*

Usage: sets alignment of minimized dialogues

Default: *'left'*

Valid: *'left'*, *'right'*

#### dblclick ####

Type: *Boolean*, *String*

Usage: set action on double click

Default: *false*

Valid: *false*, *'maximize'*, *'minimize'*, *'collapse'*


#### titlebar ####

Type: *Boolean*, *String*

Default: *false*

Valid: *false*, *'none'*, *'transparent'*


#### icons ####

Type: *Object*

Default:

    {
      "close" : "ui-icon-circle-closethick", // new in v1.0.1
      "maximize" : "ui-icon-extlink",
      "minimize" : "ui-icon-minus",
      "restore" : "ui-icon-newwin"
    }

Valid: *&lt;jQuery UI icon class&gt;*

Events
===

#### load ####

Type: *load*

Example:

	//Specify callback as init option
	$("#my-dialog").dialogExtend({
	  "load" : function(evt, dlg) { ... }
	});
	//Bind to event by type
	//NOTE : You must bind() the <dialogextendload> event before dialog-extend is created
	$("#my-dialog")
	  .bind("dialogextendload", function(evt) { ... })
	  .dialogExtend();


#### beforeCollapse ####

Type: *beforeCollapse*

Example:

    //Specify callback as init option
    $("#my-dialog").dialogExtend({
      "beforeCollapse" : function(evt) { ... }
    });
    //Bind to event by type
    $("#my-dialog").bind("dialogextendbeforeCollapse", function(evt) { ... });

#### beforeMaximize ####

Type: *beforeMaximize*

Example:

	//Specify callback as init option
	$("#my-dialog").dialogExtend({
	  "beforeMaximize" : function(evt) { ... }
	});
	//Bind to event by type
	$("#my-dialog").bind("dialogextendbeforeMaximize", function(evt) { ... });

#### beforeMinimize ####

Type: *beforeMinimize*

Example:

	//Specify callback as init option
	$("#my-dialog").dialogExtend({
	  "beforeMinimize" : function(evt) { ... }
	});
	//Bind to event by type
	$("#my-dialog").bind("dialogextendbeforeMinimize", function(evt) { ... });

#### beforeRestore ####

Type: *beforeRestore*

Example:

	//Specify callback as init option
	$("#my-dialog").dialogExtend({
	  "beforeRestore" : function(evt) { ... }
	});
	//Bind to event by type
	$("#my-dialog").bind("dialogextendbeforeRestore", function(evt) { ... });

#### collapse ####

Type: *collapse*

Example:

	//Specify callback as init option
	$("#my-dialog").dialogExtend({
	  "collapse" : function(evt) { ... }
	});
	//Bind to event by type
	$("#my-dialog").bind("dialogextendcollapse", function(evt) { ... });

#### maximize ####

Type: *maximize*

Example:

	//Specify callback as init option
	$("#my-dialog").dialogExtend({
	  "maximize" : function(evt) { ... }
	});
	//Bind to event by type
	$("#my-dialog").bind("dialogextendmaximize", function(evt) { ... });

#### minimize ####

Type: *minimize*

Example:

	//Specify callback as init option
	$("#my-dialog").dialogExtend({
	  "minimize" : function(evt) { ... }
	});
	//Bind to event by type
	$("#my-dialog").bind("dialogextendminimize", function(evt) { ... });

#### restore ####

Type: *restore*

Example:

	//Specify callback as init option
	$("#my-dialog").dialogExtend({
	  "restore" : function(evt) { ... }
	});
	//Bind to event by type
	$("#my-dialog").bind("dialogextendrestore", function(evt) { ... });

Methods
===
#### collapse ####

Usage: Collapse the dialog without double-clicking the title bar

Trigger: *dialogextendbeforeCollapse*, *dialogextendcollapse*

Example:

	$("#my-dialog").dialogExtend("collapse");
#### maximize ####

Usage: Maximize the dialog without clicking the button

Trigger: *dialogextendbeforeMaximize*, *dialogextendmaximize*

Example:

	$("#my-dialog").dialogExtend("maximize");

#### minimize ####

Usage: Minimize the dialog without clicking the button

Trigger: *dialogextendbeforeMinimize*, *dialogextendminimize*

Example:

	$("#my-dialog").dialogExtend("minimize");

#### restore ####

Usage: Restore the dialog from maximized/minimized/collapsed state without clicking the button

Trigger: *dialogextendbeforeRestore*, *dialogextendrestore*

Example:

	$("#my-dialog").dialogExtend("restore");

#### state ####

Usage: Get current state of dialog

Return: *String*

Value: *'normal'*, *'maximized'*, *'minimized'*, *'collapsed'*

Example:

	switch ( $("#my-dialog").dialogExtend("state") ) {
	  case "maximized":
	    alert("The dialog is maximized");
	    break;
	  case "minimized":
	    alert("The dialog is minimized");
	    break;
	  case "collapsed":
	    alert("The dialog is collapsed");
	    break;
	  default:
	    alert("The dialog is normal");
	}

Theming
===
The dialog will have class according to its current state.

	<div class="ui-dialog">
	  <div class="ui-dialog-titlebar">...</div>
	  <div class="ui-dialog-content ui-dialog-{normal|maximized|minimized|collapsed}">...</div>
	</div>
The buttons are wrapped by title bar of jQuery UI Dialog.

*Note : After using dialogExtend, close button will not be a direct child of title bar anymore. It will be wrapped by a button pane element*

	<div class="ui-dialog-titlebar ui-widget-header ui-corner-all ui-helper-clearfix">
	  ...
	  <div class="ui-dialog-titlebar-buttonpane">
	    <a class="ui-dialog-titlebar-close ui-corner-all" href="#">...</a>
	    <a class="ui-dialog-titlebar-maximize ui-corner-all" href="#"><span class="ui-icon {icons.maximize}">maximize</span></a>
	    <a class="ui-dialog-titlebar-minimize ui-corner-all" href="#"><span class="ui-icon {icons.minimize}">minimize</span></a>
	    <a class="ui-dialog-titlebar-restore ui-corner-all" href="#"><span class="ui-icon {icons.restore}">restore</span></a>
	  </div>
	  ...
	</div>

Example - Basic Config
===
	$(function(){
	  $("#my-button").click(function(){
	    $("<div>This is content</div>")
	      .dialog({ "title" : "My Dialog" })
	      .dialogExtend({
	        "maximizable" : true,
	        "dblclick" : "maximize",
	        "icons" : { "maximize" : "ui-icon-arrow-4-diag" }
	      });
	  });
	});
Example - Full Config
===
	$(function(){
	  $("#my-button").click(function(){
	    $("<div>This is  content</div>")
	      .dialog({
	        "title" : "This is dialog title",
	        "buttons" : { "OK" : function(){ $(this).dialog("close"); } }
	       })
	      .dialogExtend({
	        "closable" : true,
	        "maximizable" : true,
	        "minimizable" : true,
	        "collapsable" : true,
	        "dblclick" : "collapse",
	        "titlebar" : "transparent",
	        "minimizeLocation" : "right",
	        "icons" : {
	          "close" : "ui-icon-circle-close",
	          "maximize" : "ui-icon-circle-plus",
	          "minimize" : "ui-icon-circle-minus",
	          "collapse" : "ui-icon-triangle-1-s",
	          "restore" : "ui-icon-bullet"
	        },
	        "load" : function(evt, dlg){ alert(evt.type); },
	        "beforeCollapse" : function(evt, dlg){ alert(evt.type); },
	        "beforeMaximize" : function(evt, dlg){ alert(evt.type); },
	        "beforeMinimize" : function(evt, dlg){ alert(evt.type); },
	        "beforeRestore" : function(evt, dlg){ alert(evt.type); },
	        "collapse" : function(evt, dlg){ alert(evt.type); },
	        "maximize" : function(evt, dlg){ alert(evt.type); },
	        "minimize" : function(evt, dlg){ alert(evt.type); },
	        "restore" : function(evt, dlg){ alert(evt.type); }
	      });
	  });
	});
2.0.4 /2014-07-08
===

- fix bug: iframe reset after minimization
- fix bug: forgot to restore state to normal before minimization
- fix bug: dialog lose focus after restore
- fix bug: invalid dialog position on collapse
- enhancement: added tooltips to title buttons
- update to jquery 1.11.1 and jquery.ui 1.11.0

2.0.3 /2013-11-10
===

- fix bug: order of titlebar buttons
- fix bug: restore from collapse

2.0.2 /2013-05-01
===

- published on jquery plugins

2.0.1 / 2013-04-26
===

- fix bug : wrong restore size
- fix bug : wrong restore position on long and/or wide pages
- fix bug : can't reopen dialog after closing it minimized
- fix bug : broken style on reopen dialog after closing it collapsed
- many internal fixes

2.0.0 / 2013-04-25
===

- make plugin be more modular
- now jquery.ui plugin

1.0.2 / 2013-04-24
===

- make plugin be compatible with jquery 1.9.1 and jquery-ui 1.10.2

1.0.1 / 2012-08-04 / hin
===

- make plugin be compatible with jquery 1.7.2 and jquery-ui 1.8.22
- new : add option to hide 'close' button and control its icon
- new : add function 'getState' to retrieve current status of extended dialog
- fix bug : disable resize when dialog is collapsed
- fix bug : make minimized dialog on-top of overlay
- fix bug : use {position:absolute} instead of {position:fixed} on IE6
- fix bug : buttons will not disappear anymore
- fix bug : avoid title overlapping buttons when dialog minimized
- fix bug : no more exception when invoke restore method at normal state
- fix bug : open dialog => min => max => restore => resize => dialog disappear
 - ===> restore from min (w/o trigger event) before go to max
 - ===> dialog will not disappear anymore
- fix bug : open dialog => max => restore => wrong position (always at upper-left-hand corner)
 - ===> restore position after restore dialog size
 - ===> dialog appear in correct position now



1.0 / 2010-01-05 / hin
===

- fix bug of button-pane in 'minimized' state
- fix bug of title-bar word-wrap in 'minimized' state
- apply <titlebar=none|transparent> as init option for enhancing title-bar feature
- apply <dblclick=collapse> as init option for enhancing double-click feature



0.9.2 / 2010-12-16 / hin
===

- fix bug of not firing <load.dialogExtend> event
- apply <events> as init option for defining event-callback

0.9.1 / 2010-11-16 / hin
===

- fix bug of zero-config



0.9 / 2010-11-04 / hin
===

- creation of plugin


---
authors: GB, MG
---

# Phonon-limited mobility

This tutorial discusses how to compute phonon-limited carrier mobilities in semiconductors within
the self-energy relaxation time approximation (SERTA) and the momentum relaxation time approximation (MRTA),
taking the specific case of AlAs as an example.

It is assumed the user has already completed the two tutorials [RF1](/tutorial/rf1) and [RF2](/tutorial/rf2),
and that he/she is familiar with the calculation of ground state and response properties,
in particular phonons, Born effective charges and dielectric tensor.
The user should have read the [introduction tutorial for the EPH code](/tutorial/eph_intro)
before running these examples.

This lesson should take about 1.5 hour.

## Formalism

Before starting, it is worth summarizing the most important equations implemented in the code.
For a more detailed description of the ABINIT implementation, please consult [[cite:Brunin2020b]].

Our goal is to find an approximated solution to the linearized
Boltzmann transport equation (BTE) [[cite:Ashcroft1976]] within the SERTA/MRTA approximation [[cite:Giustino2017]].
SERTA/MRTA are more accurate than the constant relaxation time approximation (CRTA) as the
microscopic e-ph scattering mechanism is now included thus leading to carrier lifetimes $\tau$
that depend on the band index $n$ and the wavevector $\kk$.
Keep in mind, however, that both SERTA and MRTA are still a **approximated solutions to the BTE**
and that a more rigorous approach would require to solve the BTE iteratively and/or
the inclusion of many-body effects at different levels.
For a review of the different possible approaches see the review paper [[cite:Ponce2020]].

In the SERTA, the transport linewidth is given by
the imaginary part of the electron-phonon (e-ph) self-energy evaluated at the KS energy [[cite:Giustino2017]].
The linewidth of the electron state $n\kk$ due to the scattering with phonons is given by

\begin{equation}
\begin{split}
    \lim_{\eta \rightarrow 0^+} & \Im\{\Sigma^\FM_{n\kk}(\enk)\} =
                \pi \sum_{m,\nu} \int_\BZ \frac{d\qq}{\Omega_\BZ} |\gkkp|^2\\
                & \times \left[ (n_\qnu + f_{m\kk+\qq})
                                \delta(\enk - \emkq  + \wqnu) \right.\\
                & \left. + (n_\qnu + 1 - f_{m\kk+\qq})
                                \delta(\enk - \emkq  - \wqnu ) \right]
\end{split}
\label{eq:imagfanks_selfen}
\end{equation}

where $\nu$ is the phonon mode, $m$ the final electron state (after the scattering),
$n_\qnu(T)$ is the Bose-Einstein occupation number, $f_{m\kk+\qq}(T, \ef)$ is the Fermi-Dirac occupation function,
$\enk$ is the energy of the electron state, and $\wqnu$ is the phonon frequency for the phonon wavevector $\qq$.
The integration is performed over the BZ for the phonon wavectors and $\gkkp$ is the e-ph matrix element.
Only the Fan-Migdal (FM) part contributes to the linewidth as the Debye-Waller term is Hermitian.

In the SERTA, the **transport lifetime** $\tau_{n\mathbf{k}}$ is inversely proportional to the e-ph self-energy linewidth:

\begin{align}
\frac{1}{\tau_{n\kk}} =
    2 \lim_{\eta \rightarrow 0^+} \Im\{\Sigma^\FM_{n\kk}(\enk)\}.
\label{eq:fanlifetime}
\end{align}

In the MRTA, the back-scattering is included by expressing the transport lifetime as:

\begin{equation}
\begin{split}
    \frac{1}{\tau_{n\kk}} & =
                2 \pi \sum_{m,\nu} \int_\BZ \frac{d\qq}{\Omega_\BZ} |\gkkp|^2 \left( 1 - \frac{\vnk \cdot \vmkq}{|\vnk|^2} \right) \\
                & \times \left[ (n_\qnu + f_{m\kk+\qq})
                                \delta(\enk - \emkq  + \wqnu) \right.\\
                & \left. + (n_\qnu + 1 - f_{m\kk+\qq})
                                \delta(\enk - \emkq  - \wqnu ) \right].
\end{split}
\label{eq:mrta}
\end{equation}

where $\vnka$ is the $\alpha$-th Cartesian component of the velocity operator

$$\vnk = \PDER{\enk}{\kk} = \langle \nk | \dfrac{\partial{H}}{\partial{\kk}} | \nk \rangle.$$

that can be computed with DFPT.

!!! important

    Note that the present formalism does not take into account contributions to the transport lifetime given by
    other scattering processes such as **defects, ionized impurities in doped semiconductors, e-e interaction,
    grain boundary scattering etc**.
    These effects may be relevant depending on the system and/or the temperature under investigation
    but they are not treated in this tutorial as here we are mainly focusing on **room temperature and non-degenerate
    semiconductors**, conditions in which e-ph scattering is one of the most important contributions.

    Last but not least, we are assuming that carriers can be described by Bloch states with a well-defined excitation
    energy (**band picture**). Polaronic effects such as those discussed in [this tutorial](/tutorial/eph4zpr)
    are not captured by the present approach.

The generalized transport coefficients are defined by:

\begin{equation}
    \mcL^{(m)}_{\alpha\beta} =
    - \sum_n \int \frac{d\kk}{\Omega_\BZ} \vnka \vnkb\, \tau_{n\kk}
    (\enk-\ef)^m
    \left.\frac{\partial f}{\partial\varepsilon}\right|_{\enk}
    \label{eq:transport_lc}
\end{equation}

These quantities can be used to obtain different transport tensors such as
the electrical conductivity $\sigma$, Peltier ($\Pi$) and Seebeck coefficient (S),
and charge carrier contribution to the thermal conductivity tensors [[cite:Madsen2018]].
The electrical conductivity tensor, for instance, is given by

\begin{equation}
    \sigma_{\alpha\beta} =
    \frac{1}{\Omega} \mcL_{\alpha\beta}^{(0)} \label{eq:transport_sigma}
\end{equation}

and can be divided into hole and electron contributions

\begin{equation}
\sigma = n_e \mu_{e} + n_h \mu_{h} \label{eq:mobility}
\end{equation}

where $n_e$ and $n_h$ are the electron and hole concentrations in the conduction and valence bands respectively,
and $\mu_e$ and $\mu_h$ are the electron and hole mobilities,
which can be obtained by selecting the conduction or valences states $n$ in Eq. \eqref{eq:transport_lc}.

For electrons, we have

\begin{equation}
\begin{split}
    n_e = \sum_{n\in \text{CB}} \int \dfrac{d\kk}{\Omega_\BZ} f_{n\kk}, \\
    \mu_e = \dfrac{1}{n_e \Omega}\, \mcL_{n\in \text{CB}}^{(0)}
\end{split}
\end{equation}

where $n\in\text{CB}$ denotes states in the conduction bands.
Similar expressions hold for holes.
At zero total carrier concentration, the Fermi level $\ef$ is located inside the band gap so that $n_e = n_h$.

A typical computation of mobilities requires different steps that are summarized
in the [introduction page for the EPH code](/tutorial/eph_intro).
Here we only describe the e-ph related part, i.e the blue-box in the workflow presented in the previous page.
For this purpose, we use [[eph_task]] **-4** to compute **only the imaginary part of the SE at the KS energy**
and explain other important aspects related to this kind of calculation.

<!--
  * Ground state computation to obtain the DEN and the WFK file
  * DFPT calculation for a set of $\qq$-points in the IBZ associated to a homogeneous mesh.
    Each DFPT run reads the WFK file produced in the previous step and produces **partial** DDB and POT1 files.
  * Merging of the partial DDB and POT1 files with *mrgddb* and *mrgdv*, respectively
  * Computation of the GS wavefunctions on a $\kk$-mesh that is much denser than the one
    used for the DFPT part as e-ph properties converge much slower than phonons.
  * Interpolation of the e-ph scattering potentials in $\qq$-space and computation of the lifetimes
    for the relevant $\kk$-points contributing to the mobility
  * Computation of transport properties

These steps can be summarized by the following graph:

![](eph4mob_assets/workflow.png ){: style="height:500px;width:400px"}
-->

All the results of the calculation are saved in netcdf format in the **SIGPEPH.nc** file,
while the main output file is used to output selected quantities, mainly for testing purposes.
Post-processing and visualisation tools are provided by AbiPy.
See e.g. the README of [AbiPy](https://github.com/abinit/abipy)
and the [AbiPy tutorials](https://github.com/abinit/abitutorials).

[TUTORIAL_README]

## Ground state and phonons of fcc AlAs

*Before beginning, you might consider creating a different subdirectory to work in.
Why not create Work_eph4mob ?*

The file *teph4mob_1.abi* is the input file for the first step
(GS + DFPT perturbations for all the $\qq$-points in the IBZ).
Copy it to the working directory with:

```sh
cd $ABI_TUTORESPFN/Input
mkdir Work_eph4mob
cd Work_eph4mob
cp ../teph4mob_1.abi .
```

{% dialog tests/tutorespfn/Input/teph4mob_1.abi %}

This step might be quite time-consuming so you may want to immediately start the job in background with:

```sh
abinit teph4mob_1.abi > teph4mob_1.log 2> err &
```

The calculation is done for AlAs, the same crystalline material as in the first two DFPT tutorials.
For further details about this first step, please refer to the first and second tutorials on DFPT.

!!! important

    Since AlAs is a **polar semiconductor**, we need to compute with DFPT the Born effective charges $\bm{Z}^*$
    as well and the static dielectric tensor $\bm{\ee}^\infty$.
    These quantities are then used to treat the long-range (LR) part of the dynamical matrix in
    the Fourier interpolation of the phonon frequencies as well as in
    the Fourier interpolation of the DFPT potentials, as discussed in the EPH introduction.

<!--
Only the (partial) DDB and POT files produced at the end of the DFPT run
are needed to perform e-ph calculation.
The files containing the first order wavefunctions (*1WF*) due to an atomic perturbation are not needed.
-->

## Merging the derivative databases and potentials

Once the DFPT calculation is completed, use the *mrgddb* tool to merge the eight **partial DDB files**
corresponding to datasets 3-10 of *teph4mob_1*.
These partial DDB files contain the dynamical matrices for the
8 $\qq$-points in the IBZ, as well as the dielectric tensor and the Born effective charges.
Name the new DDB file *teph4mob_2_DDB*.

File *\$ABI_TUTORESPFN/Input/teph4mob_2.abi* is an example of input file for *mrgddb*.

{% dialog tests/tutorespfn/Input/teph4mob_2.abi %}

Copy the file in the *Work_eph4mob* directory, and run *mrgddb* using:

```sh
mrgddb < teph4mob_2.abi
```

!!! tip

    Alternatively, one can specify the name of the output DDB and the list of input DDB files
    to be merged directly via the command line.
    This approach is quite handy especially if used in conjuction with shell globbing and the "star" syntax:

    ```sh
    mrgddb teph4mob_2_DDB teph4mob_1o_DS*_DDB
    ```

    Use **mrgddb --help** to access the documentation.

Now use the *mrgdv* tool to merge the 29 DFPT POT files corresponding to datasets 3-10 of *teph4mob_1*.
Name the new file *teph4mob_3_DVDB*.

File *\$ABI_TUTORESPFN/Input/teph4mob_3.abi* is an example of input file for *mrgdv*.

{% dialog tests/tutorespfn/Input/teph4mob_3.abi %}

You can copy it in the *Work_eph4mob* directory, and then merge the files with:

```sh
mrgdv < teph4mob_3.abi
```

!!! tip

    Alternatively, one can use the command line.

    ```sh
    mrgdv merge teph4mob_3_DVDB teph4mob_1o_DS*_POT*
    ```

    Use **mrgdv --help** to access the documentation.

We now have all the phonon-related files needed to compute the mobility.
The DDB will be used to Fourier interpolate the phonon frequencies on an **arbitrarily** dense $\qq$-mesh while
the DVDB will be used to Fourier interpolate the DFPT scattering potentials [[cite:Brunin2020b]].
The only ingredient that is still missing is the WFK file with the GS wavefunctions on the dense $\kk$-mesh.

!!! warning

    In real computations, you should always compute the electronic band structure along a $\kk$-path
    to have a qualitative understanding of the band dispersion, the position of the band edges,
    and the value of the band gap(s).
    Note also that there are several parts of the EPH code in which it is assumed that
    **no vibrational instability** is present so you should **always look
    at the phonon spectrum computed by the code**.
    Do not expect to obtain meaningful results if purely imaginary phonon frequencies
    (a.k.a **negative frequencies**) are present.


## Calculation of the dense WFK file

Converging transport properties requires careful convergence tests both for $\kk$-points and $\qq$-points.
A dense $\qq$-mesh is needed to obtain **high-quality lifetimes**, whereas a dense $\kk$-sampling
is needed to have a **good sampling of the electron (hole) pockets**.
All these studies are explained later and left as an additional excercise.
In this tutorial, indeed, we need to find some compromise between accuracy and computational cost
hence a single $\kk$-mesh is used in all our examples.

The computation of the dense WFK file is similar to a NSCF band structure computation.
The main difference is that we need wavefunctions on a $\kk$-mesh instead of a $\kk$-path
because these wavevectors are needed to evaluate integrals in the BZ.
The file *\$ABI_TUTORESPFN/Input/teph4mob_4.abi* is an example of such computation.

{% dialog tests/tutorespfn/Input/teph4mob_4.abi %}

It consists of two parts:

1. the first one (dataset 1) computes the GS wavefunctions,
2. the second one (datasets 2-3) computes the dense WFK that will be used to compute mobilities.
   We also compute another (denser) WFK file that will be used with the **double-grid method** explained later.
   As we are mainly interested in electron mobilities (conduction bands)
   we need to include enough empty bands in the NSCF WFK computation ([[nband]] = 8).

Copy the file in the *Work_eph4mob* directory, and run ABINIT:

```sh
abinit teph4mob_4.abi > teph4mob_4.log 2> err &
```

!!! important

    In the last part of the tutorial, we explain how to avoid the NSCF computation
    for all the $\kk$-points in the IBZ and produce a partial WFK file containing
    only the wavevectors relevant for transport properties. This trick is
    **crucial to reach dense meshes** but can also be used for coarser meshes if
    you want to accelerate the NSCF part and reduced the size of the WFK file.

## Calculation of the mobility

We begin by explaining how to specify the basic input variables required for a standard mobility calculation.
The file *\$ABI_TUTORESPFN/Input/teph4mob_5.abi* is an example of such computation.

{% dialog tests/tutorespfn/Input/teph4mob_5.abi %}

First of all, we need to read the **WFK**, the **DDB** and the **DVDB** files produced previously.
Since it is not possible to run mobility calculations with a single input file
and datasets, we use **strings to specify the path of the input files**:

```sh
getwfk_filepath "teph4mob_4o_DS2_WFK"
getddb_filepath "teph4mob_2_DDB"
getdvdb_filepath "teph4mob_3_DVDB"
```

Now copy the input file in the *Work_eph4mob* directory, and run the code with:

```sh
abinit teph4mob_5.abi > teph4mob_5.log 2> err &
```

The job should take $\sim$15 seconds on a recent CPU.

We now discuss the meaning of the e-ph variables in more detail:

* [[optdriver]] 7 activates the EPH driver

* [[eph_task]] -4 tells ABINIT that we only need the imaginary part
  of the e-ph self-energy at the KS energy.

* The homogeneous $\kk$-mesh corresponding to the WFK file is specified by [[ngkpt]] 24 24 24.
  The code aborts with an error if [[ngkpt]] is not the same as the one found in the
  input WFK file. At present, multiple shifts ([[nshiftk]] > 1) are not supported.

* [[ddb_ngqpt]] defines the initial $\qq$-grid used for the DFPT computation (4×4×4 in this example)

* [[eph_ngqpt_fine]] defines the dense $\qq$-mesh where the scattering potentials are interpolated
  and the e-ph matrix elements are computed.

    !!! warning

        [[ngkpt]] and [[eph_ngqpt_fine]] should be commensurate.
        More specifically, [[ngkpt]] must be a **multiple** of the $\qq$-mesh ([[eph_ngqpt_fine]])
        because the WFK should contain all the $\kk$- and $\kq$-points.
        In most cases, [[ngkpt]] == [[eph_ngqpt_fine]].
        It is however possible to use fewer $\qq$-points.
        Note also that [[ngkpt]] does not necessarily correspond to the $\kk$-mesh
        used for the computation of transport quantities, see the following discussion.

* We work within the rigid band approximation and introduce a small electron doping: [[eph_doping]] = -1e+15
  that corresponds to 1e+15 electrons per cm$^3$.
  To obtain results that are representative of the intrinsic mobility, we suggest to use a very small value,
  for instance $10^{15}$ to $10^{18}$ electrons per cm$^3$.
  Alternatively, one can specify the doping via [[eph_extrael]] or [[eph_fermie]].
  We also set [[occopt]] to 3 to correctly compute the location of the Fermi level
  using the Fermi-Dirac occupation function as we are dealing with the **physical temperature**
  and not a fictitious broadening for numerical integration purposes.

* The list of physical temperatures is specified by [[tmesh]].

    !!! note

        The computational cost increases with the number of temperatures although not necessarily in a linear fashion.
        For the initial convergence studies, we suggest to start from a relatively small number
        of temperatures **covering the range of interest**.
        The T-mesh can be densified aftwerwards while keeping the same T-range once converged parameters are found.

        Note that transport properties at low temperatures are more difficult to converge as the
        derivative of the Fermi-Dirac distribution is strongly peaked around the Fermi level hence
        a very dense sampling is needed to converge the BZ integrals.
        In a nutshell, **avoid low temperatures unless you are really interested in this regime**.

<!--In this tutorial, we will use the same dense $\kk$- and $\qq$-meshes.
As a rule of thumb, a $\qq$-mesh twice as dense in each direction as the $\kk$-mesh,
is needed to achieve fast convergence of the integrals [[cite:Brunin2020b]].
In this case, [[ngkpt]] = [[eph_ngqpt_fine]], but the use of [[sigma_ngkpt]]
allows to downsample the $\kk$-mesh used for the integration and it should be set to half
the values of [[ngkpt]].
An example will be given in this tutorial.
Possible exceptions are systems with very small effective masses (e.g. GaAs) in which
a very dense $\kk$-sampling is needed to sample the electron (hole) pocket.
In this case, using the same sampling for electrons and phonons may be enough to converge.-->

* By default, the code use the tetrahedron method [[cite:Blochl1994]]
  to perform the integration in $\qq$-mesh.
  This allows to efficiently filter out the $\qq$-points that do not contribute to the lifetimes
  since these transitions are not compatible with energy and crystalline-momentum conservation.
  The use of the tetrahedron method is **automatically activated** when [[eph_task]] is set to -4.
  It is possible to change this behaviour by using [[eph_intmeth]] albeit not recommended
  as the calculation will become significantly slower.

* The [[sigma_erange]] variable defines the energy window, below the VBM and above the
  CBM, where the lifetimes will be computed.
  Since the mobility integrals involve the derivative of the Fermi-Dirac occupation function centered on the Fermi level,
  it is possible to restrict the computation to those $\kk$-points that contribute to the mobility integral.
  The value of the derivative, indeed, decreases rapidly
  as we go further from the Fermi level hence only the states close to the band edges contribute.
  **This variable should be subject to a convergence study** as explained in the next section.
<!--
A value of 0.2 eV represents a good starting point for further analysis.
Hopefully, in the next version this parameter will be automatically computed by the code.
-->

* [[boxcutmin]] and [[mixprec]] are used to accelerate the computation, see [the introductory tutorial](/tutorial/eph_intro).

We now examine the log file in detail.
After the standard output of the input variables, the code reports the different parameters
used for the treatment of the long-range part of the DFPT potentials:
the **Born effective charges, the high-frequency dielectric constant and the dynamical quadrupole tensor**.
Make sure to have all of them in order to obtain an
accurate interpolation of the scattering potentials, see discussion in [[cite:Brunin2020]].

!!! important

    At present ( |today| ), the inclusion of the dynamical quadrupoles in the EPH code
    is not available in the public version so you should have the following in the log file:

    ```sh
    Have dielectric tensor: yes
    Have Born effective charges: yes
    Have quadrupoles: no
    Have electric field: no
    ```

The code then outputs different quantities.
For instance, ABINIT finds the list of $\kk$-points belonging to the dense mesh that
are located within the energy window ([[sigma_erange]]):

```sh
Found 3 k-points within erange:  0.000  0.150  (eV)
```
Over the initial 413 $\kk$-points in the IBZ, only 3 will be computed!

The value of the Fermi level (a.k.a electronic chemical potential $\mu_e(T)$)
as a function of T is computed and printed afterwards.
Make sure that $\mu_e$ is far enough from the band
edges so that the computed mobility can be considered as intrinsic:
the values of *D_v* and *D_c* should be large compared to ~3 kT
else you enter the **degenerate** regime or the **highly-degenerate** case (when the Fermi level is inside the bands)
and additional physical phenomena start to play a role.

```sh
 Position of CBM/VBM with respect to the Fermi level:
 Notations: mu_e = Fermi level, D_v = (mu_e - VBM), D_c = (CBM - mu_e)

  T(K)   kT (eV)  mu_e (eV)  D_v (eV)   D_c (eV)
   5.0     0.000     3.521     1.165     0.004
  64.0     0.006     3.475     1.118     0.051
 123.0     0.011     3.428     1.071     0.098
 182.0     0.016     3.379     1.023     0.146
 241.0     0.021     3.328     0.972     0.197
 300.0     0.026     3.274     0.918     0.251
```

ABINIT then reads the WFK file and interpolates the scattering potentials
to obtain the e-ph matrix elements.
The use of the tetrahedron method allows to significantly reduce the $\qq$-points:

```sh
qpoints_oracle: calculation of tau_nk will need: 15 q-points in the IBZ. (nqibz_eff / nqibz):   3.6 [%]
```

Once this is done, the code starts looping over the 3 $\kk$-points for which the lifetimes are needed.

```sh
Computing self-energy matrix elements for k-point: [ 4.5833E-01,  4.5833E-01,  0.0000E+00] [ 1 / 3 ]
```

You can find various information for each $\kk$-point, such as:

* the total number of $\qq$-points in the irreducible zone defined by
  the little group of $\kk$ (called IBZ(k) in the code),

* the number of $\qq$-point in the $\text{IBZ}_k$ contributing to the imaginary part of $\Sigma_\nk$
  (in most cases, this number will be much smaller than the total number of $\qq$-points in the $\text{IBZ}_k$)

* the wall-time each step takes.

Finally, we have the results for the lifetimes (TAU) in the *teph4mob_5.abo* file:

```sh
K-point: [ 4.5833E-01,  4.5833E-01,  0.0000E+00], T:    5.0 [K], mu_e:    3.521
   B    eKS    SE2(eKS)  TAU(eKS)  DeKS
   5   3.573    0.000  36639.9    0.000
```

{% dialog tests/tutorespfn/Refs/teph4mob_5.abo %}


!!! tip

    As already mentioned in the introduction, all the results are stored in the SIGEPH.nc file.
    With |AbiPy|, one can easily access to all the data of the computation.
    For instance, one can plot the electron linewidths as a function of the KS energy
    using the |abiopen| script:

    ```sh
    abiopen.py teph4mob_5o_SIGEPH.nc --expose
    ```

    ![](eph4mob_assets/linewidths.png)

    Well, the figure is not so impressive but this is normal as we are computing only 3 $\kk$-wavevectors
    still there are some points that are worth discussing.
    Note how the linewidths at the CBM are very small at low temperature.
    For the CBM, indeed, only phonon absorption is allowed
    and there are few vibrational states populated at low T.
    The linewidth at the CBM increses with T since high energy phonon states
    starts to be populated and more scattering channels become available.


At the end of the main output file, the diagonal elements of the SERTA/MRTA
mobility tensor $\sigma_{ij}$ are reported for the three **Cartesian directions** and all temperatures.

```sh
 Cartesian component of SERTA mobility tensor: xx
 Temperature [K]             e/h density [cm^-3]          e/h mobility [cm^2/Vs]
            5.00        0.10E+16        0.00E+00            0.00            0.00
           64.00        0.10E+16        0.00E+00           40.76            0.00
          123.00        0.10E+16        0.00E+00          356.24            0.00
          182.00        0.10E+16        0.00E+00          435.51            0.00
          241.00        0.10E+16        0.00E+00          433.89            0.00
          300.00        0.10E+16        0.25E+05          379.06            0.00

 Cartesian component of MRTA mobility tensor: xx
 Temperature [K]             e/h density [cm^-3]          e/h mobility [cm^2/Vs]
            5.00        0.10E+16        0.00E+00            0.00            0.00
           64.00        0.10E+16        0.00E+00           39.67            0.00
          123.00        0.10E+16        0.00E+00          374.94            0.00
          182.00        0.10E+16        0.00E+00          470.73            0.00
          241.00        0.10E+16        0.00E+00          469.55            0.00
          300.00        0.10E+16        0.25E+05          411.23            0.00
```

The temperature is first given then the electron (e) and hole (h) densities followed by
the corresponding (SERTA/MRTA) mobilities.
In our input file, we considered only electrons and this explains why the values for holes are zero.
In this particular case, the difference between SERTA and MRTA is not very large
but the two approximation may give significantly different results in other systems.
According to recent works the MRTA results are expected to be closer to ones obtained by iteratively solving the BTE.

!!! tip

	You can also run the transport driver in standalone mode by setting [[eph_task]] 7,
	provided you already have the lifetimes in an external SIGEPH.nc file specified via [[getsigeph_filepath]].
	This task is relatively fast even in serial execution although some parts
	(in particular the computation of DOS-like quantities) can benefit from MPI.

Now that we know how to obtain the mobility in a semiconductor for given $\kk$- and $\qq$-meshes,
we can give additional details about convergence studies and discuss extra tricks
to significantly decrease the computational cost.

## Convergence w.r.t. the energy range

The first convergence study consists in determining the energy range around the band edge
to be used for the computation of $\tau_{n\kk}$.
We can do that by performing mobility computations with fixed $\kk$- and $\qq$-meshes
and increasing values of [[sigma_erange]].

!!! tip

    The code can compute both electron and hole mobilities in a single run
    but this is not the recommended procedure as the $\qq$-point filtering is expected to be less efficient.
    Moreover electrons and holes may require a different $\kk$-sampling to convergence depending on the dispersion
    of the bands. As a consequence, **we suggest to compute electrons and holes with different input files**.

The file *$\$ABI_TUTORESPFN/Input/teph4mob_6.abi* is an example of such computation.

{% dialog tests/tutorespfn/Input/teph4mob_6.abi %}

Copy the input file in the *Work_eph4mob* directory, and run ABINIT:

```sh
abinit teph4mob_6.abi > teph4mob_6.log 2> err &
```

This run should take a few minutes.

We can now analyze the variation of the mobility with respect to [[sigma_erange]].

TODO: Add script to analyze convergence wrt sigma_erange

This study shows that an energy window of ~0.15 above the CBM is enough to converge
so we use this value in the subsequent calculations.

!!! warning

    One should perform this convergence study with a $\kk$-mesh that is already dense enough to
    capture the band dispersion correctly.
    In this case, we are using a 24×24×24 mesh, which is not very dense for such computations.
    This means that, when increasing [[sigma_erange]],
    no additional $\kk$-point is included as the sampling is **too coarse**.
    This is the case for the first three datasets (3 $\kk$-points), and the last two datasets (6 $\kk$-points).
    If a finer mesh was used, the number of $\kk$-points would have increased in a more monotonic way.
    For instance, in Silicon, a 45×45×45 $\kk$-mesh could be used to determine [[sigma_erange]].

<!--
Other quantities (Seebeck etc may have a different convergence behaviour
In principle, one can run a single calculation with relatively large sigma_erange and then decrease
the window in the RTA part. This trick however is not yet implemented.
-->

## Convergence w.r.t. the k/q meshes

Once the energy window is set, we can start to converge the mobility with respect to the
dense $\kk$- and $\qq$-meshes.
The previous computations used 24×24×24 $\kk$- and $\qq$-meshes.
This is quite far from convergence.
Just to give you an idea, silicon requires a 45×45×45 $\kk$-mesh and 90×90×90 $\qq$-mesh
to reach convergence within 5%.

!!! tip

    As a rule of thumb, a $\qq$-mesh twice as dense in each direction as the $\kk$-mesh,
    is needed to obtain accurate values for the linewidth and achieve fast convergence
    of the integrals in $\kk$-space [[cite:Brunin2020b]].
    Possible exceptions are systems with very small effective masses (e.g. GaAs) in which
    a very dense $\kk$-sampling is needed to sample the electron (hole) pockets.
    In this case, using the same sampling for electrons and phonons may be enough to converge.

<!--
In this tutorial, we use [[ngkpt]] == [[eph_ngqpt_fine]], but the use of [[sigma_ngkpt]]
allows to downsample the $\kk$-mesh used for the integration and it should be set to half
the values of [[ngkpt]].
-->

To compute the mobility with a $\qq$-mesh twice as dense as the $\kk$-mesh,
there are two possible approaches:

1. Run a computation with:

    * [[ngkpt]] 90 90 90 ([[nshiftk]] 1 and [[shiftk]] 0 0 0)
    * [[eph_ngqpt_fine]] 90 90 90,
    * [[sigma_ngkpt]] 45 45 45.

    using [[sigma_ngkpt]] to select the $\kk$-points in $\Sigma_\nk$ belonging to the 45×45×45 mesh,
    but now each lifetime is computed with a 90×90×90 $\qq$-mesh.

2. Run a computation with:

      * [[ngkpt]] 90 90 90, ([[nshiftk]] 1 and [[shiftk]] 0 0 0)
      * [[eph_ngqpt_fine]] 90 90 90,
      * [[sigma_ngkpt]] 90 90 90.

    Following this approach, we compute lifetimes on a 90×90×90 $\kk$- and the integration
    is perfomed on the same $\qq$-mesh.
    You can then run again the transport driver only,
    by setting [[eph_task]] 7 and [[transport_ngkpt]] 45 45 45
    to downsample the $\kk$-mesh used to integrate the mobility in $\kk$-space.
    This second option has the advantage that it delivers **two mobilities** in one-shot
    but it would be overkilling if a 45×45×45/90×90×90 k/q sampling is already enough.

You can run again the previous input files by densifying the different meshes.
For the densest grids, you might need to run with multiple MPI processes.
You should obtain something like this for $T$ = 300 K:

![](eph4mob_assets/teph4mob_conv.png)

Sorry for repeating it again but,
the inputs of this tutorial have been tuned to make the computations quite fast,
but the final results are far from convergence.
In order to get sensible results, one should
use a denser DFPT $\qq$-mesh (around 9×9×9), and a larger cutoff energy [[ecut]].
Obviously, these parameters depend on the system under investigation.

## Double-grid technique

Another possibility to improve the results without increasing the computation time significantly
is the double-grid (DG) technique [[cite:Brunin2020b]].
In this method, a coarse sampling is used for the $\kk$- and the $\qq$-mesh for the e-ph matrix elements,
but a **finer mesh** is used to interpolate the weights for the delta functions associated to phonon absorption/emission.
This technique allows one to accelerate the convergence while keeping
the computational cost and the memory requirements at a reasonable level.

!!! important

    The efficiency of the DG approach depends on the strength of the polar Frohlich divergence:
    if the divergence is difficult to integrate numerically,
    the coarse $\qq$-mesh for the e-ph matrix elements will have to be densified.

The DG technique requires a second WFK file, containing the KS eigenvalues on the fine mesh.
You can specify the path to the fine WFK file using [[getwfkfine_filepath]] as in:

```sh
getwfkfine_filepath "teph4mob_4o_DS3_WFK"
```

The file *$\$ABI_TUTORESPFN/Input/teph4mob_7.abi* (first dataset) is an example of such computation.

{% dialog tests/tutorespfn/Input/teph4mob_7.abi %}

Copy the input file in the *Work_eph4mob* directory, and run ABINIT:

```sh
abinit teph4mob_7.abi > teph4mob_7.log 2> err &
```

In the log file, you will now find information about the double-grid method:

```sh
coarse:                24          24          24
fine:                  48          48          48
```

The SERTA mobility obtained at 300 K is 163.84 cm$^2$/V/s.
Using a 48×48×48 $\qq$-mesh for the matrix elements as well would give 96.97 (using [[sigma_ngkpt]] 24 24 24).
The result is indeed improved, since using a 24×24×24 mesh both for electrons and phonons gives 379.06.
You can also use a finer mesh, but always multiple of the initial coarse mesh
(in this case, 72×72×72, 96×96×96, etc).
It is worth noticing that, according to our tests,
there is very little use to go beyond a mesh three times as dense as the coarse one.
Using a 72×72×72 fine mesh for the energies gives a mobility of 152.30 cm$^2$/V/s,
and a 96×96×96 mesh leads to 149.38 cm$^2$/V/s: the improvement is indeed rather limited.

!!! important

    As a rule of thumb, consider to use the DG method for systems in which the tetrahedron filter
    is not able to reduce the number of $\qq$-points in the integrals below 5% for a significant fraction
    of the $\kk$-points in the [[sigma_erange]] energy window.
    This may happen if there are multiple equivalent pockets and thus many intra-valley scattering channels.
    In this case, the computation of $\tau_\nk$ may require several minutes (2-10) per $\kk$-point and calculations
    performed with the same $\kk$- and $\qq$-mesh start to be expensive when the BZ sampling gets denser.

## In-place restart

All the results of the calculation are stored in a single SIGEPH.nc file
for all the $\kk$-points (and spins) considered.
The list of $\kk$-points is initialized at the beginning of the calculation and an internal table
in the netcdf file stores the status of each $\kk$-point (whether it has been computed or not).
This means that calculations that are killed by the resource manager due to time limit can reuse
the SIGEPH file to perform an **automatic in-place restart**.
Just set [[eph_restart]] to 1 in the input file and rerun the job

!!! important

    There is no harm in setting [[eph_restart]] to 1 from the begining but keep in mind that
    the code will restart the calculation from scratch if all the $\kk$-points in the SIGEPH.nc have
    been computed (a backup copy is kept).
    So we do not recommended the use of this option in MultiDataset mode.
    Again, MultiDataset are evil when it comes to high-performance!

## Transport calculation from SIGEPH.nc

The routine that computes carrier mobilites is automatically invoked when [[eph_task]] -4 is used
and a **RTA.nc** file with the final results is produced.
There are however cases in which one would like to compute mobilities starting from
a **pre-existent SIGEPH.nc** without performing a full calculation from scratch.
In this case, use [[eph_task]] 7 and specify the name of the SIGEPH.nc file with [[getsigeph_filepath]].
The advanced input variable [[transport_ngkpt]] can be used to downsample
the $\kk$-mesh used in the mobility integrals.

## MPI parallelism and memory requirements

There are **five different MPI levels** that can be used to distribute the workload
and the most memory-demanding data structures.
By default, the code tries to reach some compromise between memory requirements and time to solution
by activating the parallelism over perturbations and then the $\qq$-point parallelism
if no input is provided by the user.
You can however specify manually the MPI distribution across the five different levels
by using [[eph_np_pqbks]] (a list of 5 integers).
The product of these five numbers **must be equal to the total number of MPI processes**.

The first entry defines the number of processes for the parallelization over perturbations.
The allowed value range between 1 and 3 × [[natom]], and should be a divisor
of 3 × [[natom]] to distribute the work equally.
The higher this number, the lower the memory requirements at the price of increased MPI communication.
The second entry determines the parallelization over the $\qq$-points in the IBZ.
This parallelization level allows one to decrease both the computational time as well as memory although
it's not always possible to distribute the load equally among the processes.
The parallelization over bands is usually not relevant for mobility computations
as only a few states close to the VBM or CBM are usually included.
The MPI parallelism over $\kk$-points and spins is very efficient
but it requires HDF5 with MPI-IO support and, besides, memory won't scale.
Use these additional two levels if the memory requirements are under control
and you want to boost the calculation.

## How to reduce the memory requirements

As mentioned above, the memory will scale with the number of MPI processors used for the $\qq$-point and
the perturbation communicators.
However, there might be tricky systems in which you start to experience memory shortage that
prevents you from running with many MPI processes.
This problem should show up for very dense $\kk$/$\qq$ meshes.
As a rule of thumb, mobility calculations with meshes denser than e.g 200x200x200 start to be very memory demanding
and the execution will slow down because several algorithms and internal tables for the BZ sampling
and the tetrahedron method start to dominate.
The double grid technique helps mitigate this bottleneck.
In some cases, you may try to reduce slightly the value of [[sigma_erange]] to reduce the memory requirements.

Note also that the EPH code allocates a relatively small buffer to store the Bloch states involved
in transport calculations but unfortunately the $\kk$-points are not easy to distribute with MPI.
Moreover the size of this array depends on the electronic dispersion:
systems with several relatively flat bands around the band edges require more memory.
To reduce the memory for the wavefunctions, the code uses internal buffers in single precision.
This option is enabled at configure time by using `enable_gw_dpc="no"` (this is the default behaviour).

If these tricks do not solve your problem, consider using OpenMP threads.
The code is not highly-optimized for OpenMP but a couple of threads may be useful to avoid replicating memory at the MPI level.
As a rule of thumb, 2-4 OpenMP threads should be OK provided you link with threaded FFT and BLAS libraries.
To compile ABINIT with OpenMP support and link with a threaded library see the
corresponding section in the [compilation tutorial](/tutorial/compilation).

!!! warning

    Last but not least, **do not use datasets**: split the calculation into different input files
    and optimize the number of MPI processes according to the dimension of the problem.
    You have been warned!

<!-- part of the discussion can be moved to the eph_intro as SKW will be used also in phgamma -->

## How to compute the WFK only for k-points close to the band edges

As we have already seen in the previous sections, a relatively small number of $\kk$-points
close to the band edges is usually sufficient to converge mobilities.
Yet, in the NSCF run, we computed a WFK file for all the $\kk$-points in the IBZ
hence we spent a lot of resources to compute and store states that
are **not needed for phonon-limited mobilities**.

In principle, it is possible to restrict the NSCF calculation to the relevant $\kk$-points
provided we have a cheap and good-enough method to predict whether the wavevector
is inside the energy window **without solving the KS eigevalue problem exactly**.
For example, one can use the star-function interpolation by Shankland-Koelling-Wood (SKW)
[[cite:Shankland1971]], [[cite:Koelling1986]], [[cite:Madsen2006]], [[cite:Madsen2018]]
which requires as input a set of eigenvalues in the IBZ and a single parameter defining the basis set for the interpolation.

There are several technical problems that should be addressed at the level of the internal implementation but
the idea is relatively simple and goes as follows:

1. Compute the KS eigenvalues on a relatively coarse $\kk$-mesh in the IBZ
2. Use this *coarse* WFK file to interpolate the eigenvalues on a **much denser** $\kk$-mesh specified by the user.
3. Find the wavevectors of the dense mesh inside an energy window specified by the user and
   store the list of $\kk$-points in a external file (**KERANGE.nc**).
4. Use this external file to run a NSCF calculation only for these $\kk$-points.
   At the end of the NSCF job, ABINIT will produce a **customized** WFK file on the dense mesh that
   can be used to run calculations for phonon-limited mobilities

An example will help clarify.

Suppose we have computed a WFK file with a NSCF run using a 16x16x16 $\kk$-mesh (let's call it *161616_WFK*)
and we want to compute mobilites with the much denser 64x64x64 $\kk$-mesh.
In this case, use

- [[optdriver]] = 8
- [[wfk_task]] = "wfk_kpts_erange"

to read the WFK file specified by [[getwfk_filepath]], find the wavevectors belonging to the [[sigma_ngkpt]] $\kk$-mesh
inside the energy window defined by [[sigma_erange]] and produce a *KERANGE.nc* file.
The parameters defining the SKW interpolation are specified by [[einterp]].

A typical input file for this step looks like:

```sh
optdriver 8
wfk_task "wfk_kpts_erange"
getwfk_filepath "161616_WFK"

# Define fine k-mesh for the interpolation
sigma_ngkpt   64 64 64
sigma_nshiftk 1
sigma_shiftk  0 0 0

sigma_erange 0.0 0.2 eV   # Select kpts in fine mesh within this energy window.
einterp 1 5 0 0           # Parameters for star-function interpolation
```

This input produces a *KERANGE.nc* file (let's call it *out_KERANGE*) that can be used
via the [[getkerange_filepath]] variable as a starting point to perfom a NSCF run with the following variables:

```sh
getkerange_filepath "out_KERANGE.nc"
getden_filepath "161616_DEN"    # Read DEN file to initialize NSCF run
getwfk_filepath "161616_WFK"    # Init GS wavefunctions from this file (optional)
iscf  -2
tolwfr 1e-18
kptopt 0                        # Important!

# These variables must be consistent with the values of
# sigma_ngkpt, sigma_shiftk used in the previous step
ngkpt    64 64 64
nshiftk  1
shiftk   0.0 0.0 0.0
```

This part will produce a **customized Fortran WFK file** with [[ngkpt]] = 64 64 64 in which
only the states listed in the **KERANGE.nc** netcdf file have been computed.
This WKF file can then be used in the EPH code to compute mobilites.
For further examples see [[test:v9_57]], and [[test:v9_61]].

Note that the two tests **cannot be executed in multidataset mode with a single input file**.
Also, keep in mind that the quality of the interpolation depends on the initial coarse $\kk$-mesh
so we recommended to look at the interpolant, see discussion at the end of [the introductory tutorial](/tutorial/eph_intro).

!!! important

    It is also a good idea to use an energy window that is **larger** than the one that will be employed
    to compute the mobility in the EPH code.
    As a rule of thumb, increase [[sigma_erange]] by 0.15 eV when computing the KERANGE.nc file.
    This should be however tested in each case.
---
authors: XG, RC
---

# First tutorial

## The H<sub>2</sub> molecule without convergence studies

This tutorial aims at showing how to get the following physical properties:

* the (pseudo) total energy
* the bond length
* the charge density
* the atomisation energy

You will learn about the two input files, the basic input variables, the
existence of defaults, the actions of the parser, and the use of the multi-dataset feature.
You will also learn about the two output files as well as the density file.

This first tutorial covers the first sections of the [[help:abinit]].
The very first step is a detailed tour of the input and output files: you are
like a tourist, and you discover a town in a coach.
You will have a bit more freedom after that first step.
It is supposed that you have some good knowledge of UNIX/Linux.

Visualisation tools are NOT covered in the basic ABINIT tutorials.
Powerful visualisation procedures have been developed in the Abipy context,
relying on matplotlib. See the README of [Abipy](https://github.com/abinit/abipy)
and the [Abipy tutorials](https://github.com/abinit/abitutorials).

This tutorial should take about 2 hours.

[TUTORIAL_README]

## Computing the (pseudo) total energy and some associated quantities

For this tutorial, one needs a working directory.
So, you should create a *Work* subdirectory inside $ABI_TESTS/tutorial, e.g. with the commands:

```sh
cd $ABI_TESTS/tutorial/Input
mkdir Work   # ~abinit/tests/tutorial/Input/Work
cd Work
```

We will do most of the actions of this tutorial in this working directory.
Let us now run the code ...

!!! tip

    The [[help:abinit#intro|section 1.1]] of the ABINIT help file explains how to run ABINIT.
    You might read it now, then follow the commands hereafter.

Copy *$ABI_TESTS/tutorial/Input/tbase1_1.abi* in *Work*:

    cp ../tbase1_1.abi .

Later, we will look at this file, and learn about its content.
For now, you will try to run the code.
In the *Work* directory, type:

    abinit tbase1_1.abi >& log &

Wait a few seconds ... it's done!
You can look at the content of the *Work* directory with the **ls** command.
You should get something like:

```sh
ls
log           tbase1_1o_DDB	    tbase1_1o_EIG     tbase1_1o_OUT.nc
tbase1_1.abi  tbase1_1o_DEN	    tbase1_1o_EIG.nc  tbase1_1o_WFK
tbase1_1.abo  tbase1_1o_EBANDS.agr  tbase1_1o_GSR.nc
```

Different output files have been created, including a *log* file and the output file *tbase1_1.abo*.
To check that everything is correct, you can make a diff of
*tbase1_1.abo* with the reference file *$ABI_TESTS/tutorial/Refs/tbase1_1.abo*

```sh
diff tbase1_1.abo ../../Refs/tbase1_1.abo | less
```

That reference file uses slightly different file names.
You should get some difference, but rather inoffensive ones, like differences in the name of input
files, slightly different numerical results, or timing differences, e.g.:

```diff
2,3c2,3
< .Version 9.4.1 of ABINIT 
< .(MPI version, prepared for a x86_64_darwin18.7.0_gnu9.3 computer) 
---
> .Version 9.3.3 of ABINIT 
> .(MPI version, prepared for a x86_64_linux_gnu9.3 computer) 
17,18c17,18
< .Starting date : Mon 25 Jan 2021.
< - ( at 21h07 )
---
> .Starting date : Wed 30 Dec 2020.
> - ( at 19h15 )
20,21c20,21
< - input  file    -> tbase1_1.abi
< - output file    -> tbase1_1.abo
---
> - input  file    -> /home/buildbot/ABINIT/alps_gnu_9.3_serial/trunk_beauty/tests/TestBot_MPI1/tutorial_tbase1_1/tbase1_1.abi
> - output file    -> tbase1_1.abo
117,118c117,118
< - pspini: atom type   1  psp file is /Users/gonze/_Research/ABINIT_git/beauty/tests/Psps_for_tests/Pseudodojo_nc_sr_04_pw_standard_psp8/H.psp8
< - pspatm: opening atomic psp file    /Users/gonze/_Research/ABINIT_git/beauty/tests/Psps_for_tests/Pseudodojo_nc_sr_04_pw_standard_psp8/H.psp8
---
> - pspini: atom type   1  psp file is /home/buildbot/ABINIT/alps_gnu_9.3_openmpi/trunk_beauty/tests/Psps_for_tests/Pseudodojo_nc_sr_04_pw_standard_psp8/H.psp8
> - pspatm: opening atomic psp file    /home/buildbot/ABINIT/alps_gnu_9.3_openmpi/trunk_beauty/tests/Psps_for_tests/Pseudodojo_nc_sr_04_pw_standard_psp8/H.psp8
216,217c216,217
<     1     -1.38336201933863    -0.00000000000000    -0.00000000000000
<     2      1.38336201933863    -0.00000000000000    -0.00000000000000
---
>     1     -1.38336201933879    -0.00000000000000    -0.00000000000000
>     2      1.38336201933879    -0.00000000000000    -0.00000000000000
230,232c230,232
< kinetic             :  1.01705426532945E+00
< hartree             :  7.26359620833820E-01
< xc                  : -6.39065298290653E-01
---
> kinetic             :  1.01705426532942E+00
> hartree             :  7.26359620833802E-01
> xc                  : -6.39065298290642E-01
239c239
< band_energy         : -7.38843481479430E-01
---
> band_energy         : -7.38843481479398E-01
314c314
< - Total cpu        time (s,m,h):          4.7        0.08      0.001
---
> - Total cpu        time (s,m,h):          4.6        0.08      0.001
321,329c321,328
```

If you do not run on a PC under Linux with GNU Fortran compiler, e.g. the
Intel compiler, you might also have small numerical differences, on the order of 1.0d-10 at most.
You might also have other differences in the paths of files.
Finally, it might also be that the default FFT algorithm differs from
the one of the reference machine, in which case the line mentioning [[fftalg]]
will differ (*fftalg* will not be 312). If you get something else, you should ask for help!

You can have a very quick look at the beginning of the output file *tbase1_1.abo*. 
In this part of the output file, note the dot `.` or dash `-` that is inserted in the first column.
This is not important for the user: it is used to post-process the output file using some automatic tool.
As a rule, you should ignore symbols placed in the first column of the abinit output file.

Supposing everything went well, we will now detail the different steps that
took place: how to run the code, what is in the *tbase1_1.abi* input file, and,
later, what is in the *tbase1_1.abo* and *log* output files.

!!! tip

    If you have not read it, please read [[help:abinit#intro|section 1.1]] of the ABINIT help file.

It is now time to edit the *tbase1_1.abi* input file.

{% dialog tests/tutorial/Input/tbase1_1.abi %}

You can have a first glance at it. It is not very long: about 50 lines, mostly comments.
Do not try to understand everything immediately.
After having gone through it, you should read general explanation about its content, and the format of such
input files in the [[help:abinit#input|section 3.1]] of the abinit help file.

You might now examine in more details some input variables.
An alphabetically ordered [[varset:allvars|index of all variables]] is provided, and their description is
found in different files (non-exhaustive list):

  * [[varset:basic|Basic variables]]
  * [[varset:files|Files handling variables]]
  * [[varset:gstate|Ground-state calculation variables]]
  * [[varset:gw|GW variables]]
  * [[varset:paral|Parallelisation variables]]
  * [[varset:dfpt|Density Functional Perturbation Theory (DFPT) variables]]

However, the number of such variables is rather large!
Note that only a dozen of input variables were needed to run the first test case.
This is possible because there are defaults values for the other input variables.
When it exists, the default value is mentioned at the fourth line of the section
related to each input variable.
Some input variables are also preprocessed, in order to derive convenient
values for other input variables.
Defaults are not existing or were avoided for the few input variables that you find in *tbase1_1.abi*.
These are particularly important input variables.
So, take a few minutes to have a look at the input variables of *tbase1_1.abi*:

  * [[acell]]
  * [[ntypat]]
  * [[znucl]]
  * [[pp_dirpath]]
  * [[pseudos]]
  * [[natom]]
  * [[typat]]
  * [[xcart]]
  * [[ecut]]
  * [[nkpt]]
  * [[nstep]]
  * [[toldfe]]
  * [[diemac]]

Have also a look at [[kpt]] and [[iscf]].

It is now time to have a look at the two output files of the run.

First, open the *log* file. You can begin to read it. It is nasty. Jump to its end.
Twenty lines before the end, you will find the number of WARNINGS and COMMENTS that were issued by the code during execution.
You might try to find them in the file (localize the keywords WARNING or COMMENT in this file).
Some of them are for the experienced user. For the present time, we will ignore them.
You can find more information about messages in the log file in
[[help:abinit#logfile|this section]] of the abinit help file.

!!! tip

    If |AbiPy| is installed on your machine, you can use the |abiopen| script
    to extract the messages from the Abinit log file with the syntax:

        abiopen.py log -p

    to get:

        Events found in /Users/gonze/_Research/ABINIT_git/beauty/tests/tutorial/Input/Work/log

        [1] <AbinitComment at m_dtfil.F90:1470>
            Output file: tbase1_1.abo already exists.
    
        [2] <AbinitComment at m_dtfil.F90:1494>
            Renaming old: tbase1_1.abo to: tbase1_1.abo0001
    
        [3] <AbinitWarning at m_ingeo.F90:887>
            The tolerance on symmetries =   1.000E-05 is bigger than 1.0e-8.
            In order to avoid spurious effects, the atomic coordinates have been
            symmetrized before storing them in the dataset internal variable.
            So, do not be surprised by the fact that your input variables (xcart, xred, ...)
            do not correspond to the ones echoed by ABINIT, the latter being used to do the calculations.
            In order to avoid this symmetrization (e.g. for specific debugging/development), decrease tolsym to 1.0e-8 or lower.
    
        [4] <AbinitComment at m_symfind.F90:999>
            The Bravais lattice determined only from the primitive
            vectors, bravais(1)=  7, is more symmetric
            than the real one, iholohedry=  4, obtained by taking into
            account the atomic positions. Start deforming the primitive vector set.
    
        [5] <AbinitComment at m_memeval.F90:2397>
            Despite there is only a local part to pseudopotential(s),
            lmnmax and lnmax are set to 1.
    
        [6] <AbinitComment at m_xgScalapack.F90:236>
            xgScalapack in auto mode
    
        [7] <AbinitComment at m_memeval.F90:2397>
            Despite there is only a local part to pseudopotential(s),
            lmnmax and lnmax are set to 1.
    
        [8] <AbinitWarning at m_drivexc.F90:711>
            Density went too small (lower than xc_denpos) at 38 points
            and was set to xc_denpos =   1.00E-14. Lowest was  -0.13E-13.
            This might be due to (1) too low boxcut or (2) too low ecut for
            pseudopotential core charge, or (3) too low ecut for estimated initial density.
            Possible workarounds : increase ecut, or define the input variable densty,
            with a value larger than the guess for the decay length, or initialize your,
            density with a preliminary LDA or GGA-PBE if you are using a more exotic xc functional.
    
        num_errors: 0, num_warnings: 2, num_comments: 6, completed: True

Now open the *tbase1_1.abo* file. Alternatively, you might have a look at the reference file we provide below.

{% dialog tests/tutorial/Refs/tbase1_1.abo %}

You find some general information about the output file [[help:abinit#outputfile|here]] .
You should also:

* examine the header of *tbase1_1.abo*
* examine the report on memory needs (do not read each value of parameters)
* examine the echo of preprocessed input data,

until you reach the message:

    chkinp: Checking input parameters for consistency.

If the code does not stop there, the input parameters are consistent.
At this stage, many default values have been provided, and the preprocessing is finished.

It is worth to come back to the echo of preprocessed input data.
You should first examine the *tbase1_1.abi* file in more details, and read the meaning of
each of its variables in the corresponding input variables file, if it has not yet been done.
Then, you should examine some variables that were **not** defined
in the input file, but that appear in the echo written in *tbase1_1.abo*. Search for:

[[nband]]
:   Its value is 2.
    It is the number of electronic states that will be treated by the code. It has
    been computed by counting the number of valence electrons in the unit cell
    (summing the valence electrons brought by each pseudopotential) then occupying
    the lowest states (look at the [[occ]] variable), and adding some states (at
    least one, maybe more, depending on the size of the system).

[[ngfft]]
:   Its value is 30 30 30.
    It is the number of points of the three-dimensional FFT grid. It has been
    derived from [[ecut]] and the dimension of the cell ([[acell]]).

[[mpw]]
:
    The maximal number of plane waves ([[mpw]]) is mentioned in the memory evaluation section: it is **752**.
    Well, this is not completely right, as the code took advantage of the time-reversal symmetry,
    valid for the k-point (0, 0, 0), to decrease the number of planewave by about a factor of two.
    The full set of plane waves is **1503** (search for npw in the *tbase1_1.abo* file).
    The code indicates the time-reversal symmetry by a value of [[istwfk]] = 2,
    instead of the default [[istwfk]] = 1.

[[nsym]]
:   It is the number of symmetries of the system. Its value is 16.
    The 3x3 matrices [[symrel]]
    define the symmetry operations. In this case, none of the symmetries is
    accompanied by a translation, that would appear in the variable [[tnons]]. The
    code did an automatic analysis of symmetries.
    They could alternatively be set by hand, or using the symmetry builder (to be described later).

[[xred]]
:   Alternative to [[xcart]] to specify the positions of atoms within the primitive cell.

Now, you can start reading the description of the remaining of the
*tbase1_1.abo* file in [[help:abinit#53-more-on-the-main-output-file|section 5.3]] of the abinit help file.
Look at the *tbase1_1.abo* file at the same time.

You have read completely an output file! Could you answer the following questions?
(There might be numerical differences, from platform to platform, in the quoted results!)

??? note "Q1. How many SCF cycles were needed to have the [[toldfe]] criterion satisfied?"

    6 SCF cycles were needed:

             iter   Etot(hartree)      deltaE(h)  residm     vres2
         ETOT  1  -1.1093804698962    -1.109E+00 6.384E-04 1.736E+01
         ETOT  2  -1.1170431098364    -7.663E-03 7.449E-08 2.737E-01
         ETOT  3  -1.1171736936408    -1.306E-04 5.867E-06 6.992E-02
         ETOT  4  -1.1171842773283    -1.058E-05 8.862E-07 5.492E-04
         ETOT  5  -1.1171843458762    -6.855E-08 1.683E-09 1.404E-05
         ETOT  6  -1.1171843463443    -4.681E-10 2.580E-11 3.672E-07

         At SCF step    6, etot is converged :
          for the second time, diff in etot=  4.681E-10 < toldfe=  1.000E-06

    Note that the number of steps that were allowed, [[nstep]] = 10, is larger than
    the number of steps effectively needed to reach the stopping criterion. As a
    rule, you should always check that the number of steps that you allowed was
    sufficient to reach the target tolerance. You might now play a bit with [[nstep]],
    as e.g. set it to 5, to see how abinit reacts.

    Side note: in most of the tutorial examples, [[nstep]] will be enough to reach
    the target tolerance, defined by one of the **tolXXX** input variables. However,
    this is not always the case (e.g. the test case 1 of the [tutorial DFPT1](/tutorial/rf1)
    because of some portability problems, that could only be
    solved by stopping the SCF cycles before the required tolerance.

??? note "Q2. Is the energy likely more converged than [[toldfe]]?"

    The information is contained in the same piece of the output file.
    Yes, the energy is more converged than [[toldfe]], since the stopping criterion asked for the difference
    between successive evaluations of the energy to be smaller than [[toldfe]] twice in a row,
    while the evolution of the energy is nice, and always decreasing by smaller and smaller amounts.

??? note "Q3. What is the value of the force on each atom, in Ha/Bohr?"

    These values are:

         cartesian_forces: # hartree/bohr
         - [ -2.69021104E-02,  -0.00000000E+00,  -0.00000000E+00, ]
         - [  2.69021104E-02,  -0.00000000E+00,  -0.00000000E+00, ]
         force_length_stats: {min:   2.69021104E-02, max:   2.69021104E-02, mean:   2.69021104E-02, }

    On the first atom (located at -0.7 0 0 in cartesian coordinates, in Bohr), the
    force vector is pointing in the $-x$ direction, and in the $+x$ direction
    for the second atom located at +0.7 0 0 .
    The H$_2$ molecule would like to expand...

??? note "Q4. What is the difference of eigenenergies between the two electronic states?"

    The eigenvalues (in Hartree) are mentioned at the lines

        Eigenvalues (hartree) for nkpt=   1  k points:
        kpt#   1, nband=  2, wtk=  1.00000, kpt=  0.0000  0.0000  0.0000 (reduced coord)
        -0.36942   -0.01446

    As mentioned in the [[help:abinit#averagepot|abinit help file]]
    the absolute value of eigenenergies is not meaningful. Only differences of eigenenergies, as well
    as differences with the potential. The difference is 0.35496 Hartree, that is 9.6588 eV .
    Moreover, remember that Kohn-Sham eigenenergies are formally **not connected** to
    experimental excitation energies! (Well, more is to be said later about this in the GW tutorials).


??? note "Q5. Can you set [[prtvol]] to 2 in the input file, run again abinit, and find where is located the maximum of the electronic density, and how much is it, in electrons/Bohr^3 ?"

    The maximum electronic density in electron per Bohr cube is reached at the mid-point between the two H atoms:

        Total charge density [el/Bohr^3]
              Maximum=    2.7281E-01  at reduced coord.    0.0000    0.0000    0.0000

!!! tip

    If |AbiPy| is installed on your machine, you can use the |abiopen| script
    with the `--expose` option to visualize the SCF cycle from the main output file:

        abiopen.py tbase1_1.abo --expose --seaborn

    ![](base1_assets/abiopen_tbase1_1.png)

    For further info, please consult this
    [jupyter notebook](https://nbviewer.jupyter.org/github/abinit/abitutorials/blob/master/abitutorials/base1/lesson_base1.ipynb)
    that reformulates the present tutorial using AbiPy.


## Computation of the interatomic distance (method 1)

Starting from now, every time a new input variable is mentioned,
you should read the corresponding descriptive section in the [[help:abinit#averagepot|abinit help file]].

We will now complete the description of the meaning of each term: there are
still a few indications that you should be aware of, even if you will not use them in the tutorial.
These might appear in the description of some input variables.
For this, you should read the [[help:abinit#parameters|section 3.3]] of the abinit help file.

There are three methodologies to compute the optimal distance between the two Hydrogen atoms.
One could:

  * compute the **total energy** for different values of the interatomic distance, make a fit through the different points,
    and determine the minimum of the fitting function;
  * compute the **forces** for different values of the interatomic distance, make a fit through the different values,
    and determine the zero of the fitting function;
  * use an automatic algorithm for minimizing the energy (or finding the zero of forces).

We will begin with the computation of energy and forces for different values of the interatomic distance.
This exercise will allow you to learn how to use multiple datasets.

The interatomic distance in the *tbase1_1.abi* file was 1.4 Bohr.
Suppose you decide to examine the interatomic distances from 1.0 Bohr to 2.0 Bohr, by steps of 0.05 Bohr.
That is, 21 calculations.
If you are a UNIX guru, it will be easy for you to write a script that will
drive these 21 calculations, changing automatically the variable [[xcart]] in
the input file, and then gather all the data, in a convenient form to be plotted.

Well, are you a UNIX guru? If not, there is an easier path, all within abinit!
This is the multi-dataset mode. Detailed explanations about it can be found in sections
[[help:abinit#multidatasets|3.4]],
[[help:abinit#series|3.5]],
[[help:abinit#loop|3.6]] and
[[help:abinit#filenames-multidataset|3.7]] of the abinit help file.

Now, can you write an input file that will do the computation described above
(interatomic distances from 1.0 Bohr to 2.0 Bohr, by steps of 0.05 Bohr)? You might start from *tbase1_1.abi*.
Try to define a series, and to use the [[getwfk]] input variable (the latter will make the computation much faster).

You should likely have a look at the section that describes the [[irdwfk]] and
[[getwfk]] input variables: in particular, look at the meaning of *getwfk -1*
Also, define explicitly the number of states (or supercell "bands") to be one, using the input variable [[nband]].

The input file *\$ABI_TESTS/tutorial/Input/tbase1_2.abi* is an example of file that will do the job,

{% dialog tests/tutorial/Input/tbase1_2.abi %}

while *\$ABI_TESTS/tutorial/Refs/tbase1_2.abo* is the reference output file.

{% dialog tests/tutorial/Refs/tbase1_2.abo %}

Run the code with *tbase1_2.abi* (this might take fifteen seconds or so on a PC at 3 GHz),

```sh
cp ../tbase1_2.abi .
abinit tbase1_2.abi  >& log 
```

Now examine the output file quickly (there are many
repetition of sections, for the different datasets), and get the output
energies gathered in the **final echo of variables**:

           etotal1    -1.0557522713E+00
           etotal2    -1.0719698068E+00
           etotal3    -1.0847711611E+00
           etotal4    -1.0947672022E+00
           etotal5    -1.1024550352E+00
           etotal6    -1.1082397355E+00
           etotal7    -1.1124515924E+00
           etotal8    -1.1153599391E+00
           etotal9    -1.1171843464E+00
           etotal10   -1.1181037489E+00
           etotal11   -1.1182639350E+00
           etotal12   -1.1177837211E+00
           etotal13   -1.1167600633E+00
           etotal14   -1.1152722975E+00
           etotal15   -1.1133856613E+00
           etotal16   -1.1111542144E+00
           etotal17   -1.1086232522E+00
           etotal18   -1.1058312878E+00
           etotal19   -1.1028116615E+00
           etotal20   -1.0995938277E+00
           etotal21   -1.0962043631E+00

You might try plot to these data:

![EOS plot](base1_assets/Plot1.png)

The minimum of energy in the above list is clearly between dataset 10 and 11, that is:

            xcart10   -7.2500000000E-01  0.0000000000E+00  0.0000000000E+00
                       7.2500000000E-01  0.0000000000E+00  0.0000000000E+00
            xcart11   -7.5000000000E-01  0.0000000000E+00  0.0000000000E+00
                       7.5000000000E-01  0.0000000000E+00  0.0000000000E+00

corresponding to a distance of H atoms between 1.45 Bohr and 1.50 Bohr.
The forces change sign also between datasets 10 and 11:

            fcart10   -1.0358194799E-02 -0.0000000000E+00 -0.0000000000E+00
                       1.0358194799E-02 -0.0000000000E+00 -0.0000000000E+00
            fcart11    3.5571726338E-03 -0.0000000000E+00 -0.0000000000E+00
                      -3.5571726338E-03 -0.0000000000E+00 -0.0000000000E+00

From these two values, using a linear interpolation, one get the optimal value of 1.487 Bohr.
Note that *the number of SCF cycles drops from 6 to 5 when the wavefunctions are read from the previous dataset*.

## Computation of the interatomic distance (method 2)

The other methodology is based on an automatic computation of the minimum.
There are different algorithms to do that. See the input variable [[ionmov]], with values 2, 7, 15, and 22.
In the present case, with only one degree of freedom to be optimized, the best choice is [[ionmov]] 22.

You have also to define the maximal number of time steps for this optimization.
Set the input variable [[ntime]] to 10, it will be largely enough.
For the stopping criterion [[tolmxf]], use the reasonable value of 5.0d-4 Ha/Bohr.
This defines the force threshold to consider that the geometry is converged.
The code will stop if the residual forces are below that value before reaching [[ntime]].

It is also worth to change the stopping criterion for the SCF cycle, in order
to be sure that the forces generated for each trial interatomic distance are
sufficiently converged. Indeed, the value used for [[toldfe]], namely 1.0d-6,
might be sufficient for total energy calculations, but definitely not for the
accurate computation of other properties. So, change [[toldfe]] in [[tolrff]],
and set the latter input variable to 0.02.
The input file *tbase1_3.abi* is an example of file that will do the job

{% dialog tests/tutorial/Input/tbase1_3.abi %}

while *tbase1_3.abo* is an example of output file:

{% dialog tests/tutorial/Refs/tbase1_3.abo %}

So, you run the code with your input file (a few seconds), examine quietly this file (which is much smaller
than the *tbase1_2.abo* file), and get some significant output data gathered in the final echo of variables:

           etotal     -1.1182883138E+00
            fcart     -6.6032223725E-05 -0.0000000000E+00 -0.0000000000E+00
                       6.6032223725E-05 -0.0000000000E+00 -0.0000000000E+00
             ...
            xcart     -7.4307198349E-01  0.0000000000E+00  0.0000000000E+00
                       7.4307198349E-01  0.0000000000E+00  0.0000000000E+00

According to these data (see [[xcart]]), the optimal interatomic distance is
about 1.486 Bohr, in good agreement with the estimation of *tbase1_2.abo*.
If you have time (this is to be done at home), you might try to change the
stopping criteria, and redo the calculation, to see the level of convergence
of the interatomic distance.

Note that the final value of *fcart* in your run might differ slightly from the
one shown above (less than one percent change). Such a fluctuation is quite
often observed for a value converging to zero (remember, we ask the code to
determine the equilibrium geometry, that is, forces should be zero) when the
same computation is done on different platforms.

!!! tip

    With |AbiPy|, we can analyze easily the results of the structural relaxation
    with the |abiopen| script:

        abiopen.py tbase1_3o_HIST.nc --expose --seaborn

    ![](base1_assets/abiopen_tbase1_3o_HIST.png)

## Computation of the charge density

The charge density has already been computed, for all geometries, in the
above-mentioned runs. Here, we will print this quantity.

We start from the optimized interatomic distance 1.486 Bohr, and make a run at fixed geometry.
The input variable [[prtden]] must be set to 1.
To understand correctly the content of the [[prtden]] description, it is worth to read a much
more detailed description of the file names in ABINIT, in [[help:abinit#32_file_names_in_abinit|section 3.2]] of the abinit_help file.

The input file *tbase1_4.abi* is an example of input file for a run that will print a density.

{% dialog tests/tutorial/Input/tbase1_4.abi %}

The run will take a few seconds.

The density will be output in the *tbase1_4o_DEN* file. Try to edit it...
No luck! This file is unformatted, not written using the ASCII code.
Even if you cannot read it, its description is provided in the abinit help file. It
contains first a header, then the density numbers. The description of the
header is presented in [[help:abinit#header|section 6.4]] of the abinit_help
file, while the body of the _DEN file is presented in [[help:abinit#denfile|section 6.5]].
It is the appropriate time to read also the description of the potential files and
wavefunctions files, as these files contain the same header as the density
file, see sections [[help:abinit#localpotfile|6.6]] and [[help:abinit#wfkfile|6.7]].

Such a density file can be read by abinit, to restart a calculation
(see the input variable [[iscf]], when its value is -2), but more usually, by an utility called *cut3d*.
This utility is available in the ABINIT package.
You might try to use it now, to generate two-dimensional cuts in the density, and visualize the charge density contours.
Read the corresponding [[help:cut3d|Cut3D help file]]

Then, try to run cut3d to analyse *tbase1_4o_DEN*.
You should first try to translate the unformatted
density data to indexed formatted data, by using option 6 in the adequate menu.
Save the indexed formatted data to file *tbase1_xo_DEN_indexed*.
Then, edit this file, to have an idea of the content of the *_DEN* files.
For further treatment, you might choose to select another option than 6.
In particular, if you have access to MATLAB, choose option 5. With minor
modifications (set ngx=ngy=ngz to 30) you will be able to use the file [dim.m](base1_assets/dim.m)
to visualize the 3-Dimensional isosurfaces.
Another option might be to use the |xcrysden| software, for which you need to use option 9.

If you have a density file in netcdf format, it is possible to use |AbiPy| to
export the data in different formats and invoke an external graphical tool.
This is, for example, the density isosurfaces produced with |vesta|
as discussed in this [jupyter notebook](https://nbviewer.jupyter.org/github/abinit/abitutorials/blob/master/abitutorials/base1/lesson_base1.ipynb#Analysis-of-the-charge-density)

![](https://github.com/abinit/abipy_assets/blob/master/h2_density.png?raw=true)

## Computation of the atomisation energy

The atomisation energy is the energy needed to separate a molecule in its constituent atoms, each being neutral.
In the present case, one must compute first the total energy of an isolated hydrogen atom.
The atomisation energy will be the difference between the total
energy of H$_2$ and twice the total energy of H.
There are some subtleties in the calculation of an isolated atom.

* In many cases, the ground state of an isolated atom is spin-polarized, see the variables [[nsppol]] and [[spinat]];

* The highest occupied level might be degenerate with the lowest unoccupied level of the same spin,
  in which case the techniques usually appropriate for metals are to be used (see [tutorial 4](/tutorial/base4))

* also often, the symmetry of the ground-state charge density **will not be spherical**, so that the automatic
  determination of symmetries by the code, based on the atomic coordinates, should be disabled,
  see the input variable [[nsym]], to be set to 1 in this case.

For Hydrogen, we are lucky that the ground state is spherical (1s orbital),
and that the highest occupied level and lowest unoccupied level, although
degenerate, have a different spin. We will define by hand the occupation of
each spin, see the input variables [[occopt]] (to be set to 2), and [[occ]].
Finally, in order to make numerical errors cancel, it is important to compute
the above-mentioned difference in the same box, for the same energy cut-off, and even
for a location in the box that is similar to the molecule case (although the
latter might not be so important).

The input file *tbase1_5.abi* is an example of file that will do the job,

{% dialog tests/tutorial/Input/tbase1_5.abi %}

while *tbase1_5.abo* is an example of output file.

{% dialog tests/tutorial/Refs/tbase1_5.abo %}

The run lasts a few seconds.

You should read the output file, and note the tiny differences related with
the spin-polarisation:

The electronic eigenvalues are now given for both spin up and spin down cases:

```
 Eigenvalues (hartree) for nkpt=   1  k points, SPIN UP:
 kpt#   1, nband=  1, wtk=  1.00000, kpt=  0.0000  0.0000  0.0000 (reduced coord)
  -0.26661
 Eigenvalues (hartree) for nkpt=   1  k points, SPIN DOWN:
 kpt#   1, nband=  1, wtk=  1.00000, kpt=  0.0000  0.0000  0.0000 (reduced coord)
  -0.11113
```

As [[prtvol]]=2, The charge density at several points (maximum, minimum) is echoed, but also
the spin polarisation, because of the value of [[nspden]] and [[occopt]]:

```
 Total charge density [el/Bohr^3]
      Maximum=    1.4353E-01  at reduced coord.    0.0000    0.0000    0.0000
 Next maximum=    1.2266E-01  at reduced coord.    0.0000    0.0000    0.9667
      Minimum=    3.2349E-06  at reduced coord.    0.4667    0.4333    0.4333
 Next minimum=    3.2349E-06  at reduced coord.    0.5333    0.4333    0.4333
   Integrated=    1.0000E+00
 Spin up density      [el/Bohr^3]
      Maximum=    1.4353E-01  at reduced coord.    0.0000    0.0000    0.0000
 Next maximum=    1.2266E-01  at reduced coord.    0.0000    0.0000    0.9667
      Minimum=    3.2349E-06  at reduced coord.    0.4667    0.4333    0.4333
 Next minimum=    3.2349E-06  at reduced coord.    0.5333    0.4333    0.4333
   Integrated=    1.0000E+00
 Spin down density    [el/Bohr^3]
      Maximum=    0.0000E+00  at reduced coord.    0.9667    0.9667    0.9667
 Next maximum=    0.0000E+00  at reduced coord.    0.9333    0.9667    0.9667
      Minimum=    0.0000E+00  at reduced coord.    0.0000    0.0000    0.0000
 Next minimum=    0.0000E+00  at reduced coord.    0.0333    0.0000    0.0000
   Integrated=    0.0000E+00
 Magnetization (spin up - spin down) [el/Bohr^3]
      Maximum=    1.4353E-01  at reduced coord.    0.0000    0.0000    0.0000
 Next maximum=    1.2266E-01  at reduced coord.    0.0000    0.0000    0.9667
      Minimum=    3.2349E-06  at reduced coord.    0.4667    0.4333    0.4333
 Next minimum=    3.2349E-06  at reduced coord.    0.5333    0.4333    0.4333
   Integrated=    1.0000E+00
 Relative magnetization (=zeta, between -1 and 1)
      Maximum=    1.0000E+00  at reduced coord.    0.9667    0.9667    0.9667
 Next maximum=    1.0000E+00  at reduced coord.    0.9333    0.9667    0.9667
      Minimum=    1.0000E+00  at reduced coord.    0.0000    0.0000    0.0000
 Next minimum=    1.0000E+00  at reduced coord.    0.0333    0.0000    0.0000
```

The **zeta** variable is the ratio between the spin-density difference and the
charge density. It varies between +1 and -1. In the present case of Hydrogen,
there is no spin down density, so the zeta variable is +1.

The total energy is

    etotal     -4.7393103688E-01

while the total energy of the H$_2$ molecule is (see test 13):

    etotal     -1.1182883138E+00

The atomisation energy is thus 0.1704 Ha (The difference between the total
energy of the H$_2$ molecule and twice the energy of an isolated Hydrogen atom).

At this stage, we can compare our results:

  * bond length: 1.486 Bohr
  * atomisation energy at that bond length: 0.1704 Ha = 4.637 eV

with the experimental data as well as theoretical data using a much more
accurate technique (see [[cite:Kolos1960]], especially p.225)

  * bond length: 1.401 Bohr
  * atomisation energy: 4.747 eV

|                       | Our results | Experiment
| :--                   | :--         | :--
bond length [Bohr]      | 1.486       | 1.401
atomisation energy [eV] | 4.637       | 4.747

The bond length is rather bad (about 6% off), and the atomisation energy is a bit too low, 2.5% off.
What is wrong??

Well, are you sure that the input parameters that we did not discuss are correct?
These are:

* [[ecut]] (the plane-wave kinetic energy cut-off)
* [[acell]] (the supercell size)
* the pseudopotential
* [[ixc]] (not even mentioned until now, this input variable specifies what kind of
  exchange-correlation functional is to be used, and is by default deduced from the pseudopotential - a choice
  of exchange-correlation functional is mandatory to produce a pseudopotential, and mixing different exchange-correlation
  functionals for pseudopotentials generation and ABINIT calculations is bad practice)

We used 10 Ha as cut-off energy, a 10x10x10 Bohr^3 supercell, 
the LDA (=local-density approximation, as well as the local-spin-density approximation in the spin-polarized case) in the
Perdew-Wang parametrization ([[ixc]]=-1012), and a LDA pseudopotential from the pseudodojo <http://www.pseudo-dojo.org/>,
copied in the ABINIT directory $ABI_PSPDIR/Pseudodojo_nc_sr_04_pw_standard_psp8 . You might have a look at
the file $ABI_PSPDIR/Pseudodojo_nc_sr_04_pw_standard_psp8/README.md to learn more about pseudopotentials.

We will see in the [next tutorial](/tutorial/base2) how to address the choice
of these parameters (except the pseudopotential).
---
authors: SP
---

# Tutorial TDepES

## Temperature-DEPendence of the Electronic Structure.

This tutorial aims at showing how to get the following physical properties, for periodic solids:

  * The zero-point-motion renormalization (ZPR) of eigenenergies

  * The temperature-dependence of eigenenergies

  * The lifetime/broadening of eigenenergies

It should take about 1 hour.

For the theory related to the temperature-dependent calculations, please read
the following papers: [[cite:Ponce2015]], [[cite:Ponce2014]] and [[cite:Ponce2014a]].

There are three ways to compute the temperature dependence with Abinit:

  * **Using Anaddb**: historically the first implementation.

  * **Using post-processing python scripts**: This way provides more options and is more efficient (less disk space, less memory demanding). 
    This option **requires Netcdf** (both in Abinit and python). In this tutorial, we only focus on this approach.

  * **Using an interpolation of the perturbed potential**: This new way is covered 
   in the [ZPR and T-dependent band structures](/tutorial/eph4zpr) tutorial.  

!!! important

    In order to run the python script you need:

      * python 2.7.6 or higher, python3 is not supported
      * numpy 1.7.1 or higher
      * netCDF4 and netCDF4 for python
      * scipy 0.12.0 or higher

    This can be done with:

        sudo apt-get install netcdf-bin
        sudo apt-get install python-dev
        pip install numpy
        pip install scipy
        pip install netcdf4

    A list of configuration files for clusters is available in the
    [abiconfig repository](https://github.com/abinit/abiconfig)

    If you have a prebuilt abinit executable, use:

        ./abinit -b

    to get the list of libraries/options activated in the build.
    You should see netcdf in the `TRIO flavor` section:

         === Connectors / Fallbacks === 
          LINALG flavor  : netlib
          FFT flavor     : goedecker
          HDF5           : yes
          NetCDF         : yes
          NetCDF Fortran : yes
          LibXC          : yes
          Wannier90      : no

[TUTORIAL_README]

Visualisation tools are NOT covered in this tutorial.
Powerful visualisation procedures have been developed in the Abipy context,
relying on matplotlib. See the README of [Abipy](https://github.com/abinit/abipy)
and the [Abipy tutorials](https://github.com/abinit/abitutorials).

## 1 Calculation of the ZPR of eigenenergies at q=Γ.

The reference input files for this tutorial are located in
~abinit/tests/tutorespfn/Input and the corresponding reference output files
are in ~abinit/tests/tutorespfn/Refs.
The prefix for files is **tdepes**. As usual, we use the shorthand `~abinit` to indicate
the root directory where the abinit package has been deployed, but most often
consider the paths relative to this directory.

First, examine the [[tests/tutorespfn/Input/tdepes_1.abi]] input file.
{% dialog tests/tutorespfn/Input/tdepes_1.abi %}

Note that there are three datasets ([[ndtset]]=3). The first dataset corresponds to a standard
self-consistent calculation, with an unshifted eight k-point grid,
producing e.g. the ground-state eigenvalue file tdepes_1o_DS1_EIG.nc ,
as well as the density file tdepes_1o_DS1_DEN. The latter is read ([[getden]]2=1)
to initiate the second dataset calculation,
which is a non-self-consistent run, specifically at the Gamma point only (there is no real recomputation
with respect to the dataset 1, it only extract a subset of the eight k-point grid).
This second dataset produces the wavefunction file tdepes_1o_DS2_WFQ, that is read by the third dataset ([[getwfq]]3=2),
as well as the tdepes_1o_DS1_WFK file from the first dataset ([[getwfk]]3=1).

The third dataset corresponds to a DFPT phonon calculation ([[rfphon]]3=1)
with displacement of all atoms ([[rfatpol]]3= 1 2) in all directions ([[rfdir]]3= 1 1 1).
This induces the creation of the Derivative DataBase file tdepes_1o_DS3_DDB.
The electron-phonon matrix elements are produced because of [[ieig2rf]]3=5 ,
this option generating the needed netCDF files tdepes_1o_DS3_EIGR2D.nc and tdepes_1o_DS3_GKK.nc .

In order to run abinit, we suggest that you create a working directory, why not call it `Work`,
as subdirectory of ~abinit/tests/tutorespfn/Input, then
copy/modify the relevant files. Explicitly:

    cd ~abinit/tests/tutorespfn/Input
    mkdir Work
    cd Work
    cp ../tdepes*in .

Finally, issue

    abinit tdepes_1.abi 

The calculation will produce several _EIG.nc, _DDB, EIGR2D.nc and EIGI2D.nc files, 
that contain respectively the eigenvalues (GS or perturbed), 
the second-order derivative of the total energy with respect to
two atomic displacements, the electron-phonon matrix elements used to compute
the renormalization of the eigenenergies and the electron-phonon matrix
elements used to compute the lifetime of the electronic states.

You can now copy three post-processing python files from
~abinit/scripts/post_processing/temperature-dependence .
Make sure you are in the directory containing the output files produced by the code and issue:

    cp ~abinit/scripts/post_processing/temperature-dependence/temperature_final.py .
    cp ~abinit/scripts/post_processing/temperature-dependence/rf_final.py .
    cp ~abinit/scripts/post_processing/plot_bs.py .

in which ~abinit has been replaced by the proper path.


<!--
as well as the python file
containing the required classes from ~abinit/scripts/post_processing/mrgeignc.py
into the directory where you did the calculations.
-->

You can then simply run the python script with the following command:

    python temperature_final.py

and enter the information asked by the script, typically the following
(data contained in ~abinit/tests/tutorespfn/Input/tdepes_1_temperature.in):

```
1                          # Number of cpus
2                          # Static ZPR computed in the Allen-Heine-Cardona theory
temperature_1              # Prefix for output files
0.1                        # Value of the smearing parameter for AHC (in eV)
0.1                        # Gaussian broadening for the Eliashberg function and PDOS (in eV)
0 0.5                      # Energy range for the PDOS and Eliashberg calculations (in eV)
0 1000 50                  # min, max temperature and temperature step
1                          # Number of Q-points we have (here we only computed $\Gamma$)
tdepes_1o_DS3_DDB          # Name of the response-funtion (RF) DDB file
tdepes_1o_DS2_EIG.nc       # Eigenvalues at $\mathbf{k+q}$
tdepes_1o_DS3_EIGR2D.nc    # Second-order electron-phonon matrix element
tdepes_1o_DS3_GKK.nc       # Name of the 0 GKK file
tdepes_1o_DS1_EIG.nc       # Name of the unperturbed EIG.nc file with Eigenvalues at $k$
```

Alternatively, copy this example file in the Work directory if not yet done, and then run

    python temperature_final.py < tdepes_1_temperature.in

{% dialog tests/tutorespfn/Input/tdepes_1_temperature.in %}

!!! warning

    Remember to install the libraries required by the script before running.

    For pip, use:

        pip install netcdf4

    or:

        conda install netcdf4

    if you are using [conda](https://docs.conda.io/en/latest/miniconda.html)


You should see on the screen an output similar to:

```shell
Start on 21/12/2020 at 15h21 

    ____  ____       _                                      _                   
   |  _ \|  _ \     | |_ ___ _ __ ___  _ __   ___ _ __ __ _| |_ _   _ _ __ ___  
   | |_) | |_) |____| __/ _ \ '_ ` _ \| '_ \ / _ \ '__/ _` | __| | | | '__/ _ \ 
   |  __/|  __/_____| ||  __/ | | | | | |_) |  __/ | | (_| | |_| |_| | | |  __/ 
   |_|   |_|         \__\___|_| |_| |_| .__/ \___|_|  \__,_|\__|\__,_|_|  \___| 
                                      |_|                              Version 1.5         
  
This script compute the static/dynamic zero-point motion 
  and the temperature dependence of eigenenergies due to electron-phonon interaction.
  The electronic lifetime can also be computed. 

  WARNING: The first Q-point MUST be the Gamma point.

Enter the number of cpu on which you want to multi-thread
Define the type of calculation you want to perform. Type:
                         1 if you want to run a non-adiabatic AHC calculation
                         2 if you want to run a static AHC calculation
                         3 if you want to run a static AHC calculation without control on active space (not recommended !)
   Note that for 1 & 2 you need _EIGR2D.nc and _GKK.nc files obtained through ABINIT option "ieig2rf 5"
Enter name of the output file
Enter value of the smearing parameter for AHC (in eV)
Enter value of the Gaussian broadening for the Eliashberg function and PDOS (in eV)
Enter the energy range for the PDOS and Eliashberg calculations (in eV): [e.g. 0 0.5] 
Introduce the min temperature, the max temperature and the temperature steps. e.g. 0 200 50 for (0,50,100,150)
Enter the number of Q-points you have
Enter the name of the 0 DDB file
Enter the name of the 0 eigq file
Enter the name of the 0 EIGR2D file
Enter the name of the 0 GKK file
Enter the name of the unperturbed EIG.nc file at Gamma
Inside the dynamic_zpm_temp def
Q-point:  0  with wtq = 1.0  and reduced coord. [0. 0. 0.]
WARNING: An eigenvalue is negative with value:  -2.8630004909173537e-10  ... but proceed with value 0.0
Now compute active space ...
Now compute generalized g2F Eliashberg electron-phonon spectral function ...
End on 21/12/2020 at 15 h 21 
Runtime: 0 seconds (or 0.0 minutes)
```

The python code has generated the following files:

**temperature_1.txt**
: This text file contains the zero-point motion renormalization (ZPR) at each k-point for each band.
  It also contain the evolution of each band with temperature at k=$\Gamma$.
  At the end of the file, the Fan/DDW contribution is also reported.

**temperature_1_EP.nc**
: This netcdf file contains a number for each k-point,
  for each band and each temperature. The real part of this number is the ZPR correction
  and the imaginary part is the lifetime.

<!--
**temperature_BRD.txt**
: This text file contains the lifetime of the electronic states
  at each k-point for each band. It also contains the evolution of each band with temperature at k=$\Gamma$.
-->

We can for example visualize the temperature dependence at k=$\Gamma$  of the LUMO bands
(`Band: 4` section in the **temperature_1.txt** file, that you can examine)
with the contribution of only q=$\Gamma$.

![](tdepes_assets/plot1.png)

As you can see, the LUMO correction goes down with temperature. 
If the calculations were converged, the HOMO eigenenergies correction should go up with temperature. 
In general, the ZPR correction as well as their temperature dependence usually closes the gap
of semiconductors.

As usual, checking whether the input parameters give converged values is of course important.


<!--   OBSOLETE

### If Abinit is **not** compiled with Netcdf ...

In this case, we should first use [[help:mrgddb|mrgddb]] to merge the _DDB and _EIGR2D/_EIGI2D
but since we only have one q-point we do not have to perform this step.
The static temperature dependence and the G2F can be computed thanks to anaddb
with the files file tdepes_2.files and the input
file [[tests/tutorespfn/Input/tdepes_2.abi]].

The information contained in the files file can be understood by looking at the echo
if its reading in the standard output:

```
  Give name for formatted input file:
-   tdepes_2.abi
  Give name for formatted output file:
-   tdepes_2.out
  Give name for input derivative database:
-   tdepes_1o_DS3_DDB
  Give name for output molecular dynamics:
-   dummyo.md
  Give name for input elphon matrix elements (GKK file):
-   tdepes_1o_DS3_EIGR2D
  Give root name for elphon output files:
-   tdepes_1_ana
  Give name for file containing ddk filenames for elphon/transport:
-   dummy.ddk
```

{% dialog tests/tutorespfn/Input/tdepes_2.abi %}

As concern the anaddb input file, note that the electron-phonon analysis is triggered by
[[anaddb:thmflag]] 3, as well as [[anaddb:telphint]] 1 .

Launch anaddb by the command

    anaddb tdepes_2.abi 

(where `anaddb` might have to be replaced by the proper location of the anaddb executable).

The run will generate 3 files:

**tdepes_2.out_ep_G2F**
:  This g2F spectral function represents the contribution of the phononic modes of energy E
   to the change of electronic eigenenergies according to the equation

**tdepes_2.out_ep_PDS**
:  This file contains the phonon density of states

**tdepes_2.out_ep_TBS**
:  This file contains the eigenenergy corrections as well
   as the temperature dependence one.
   You can check that the results are the same as with the python script approach here above.


END OF OBSOLETE

-->



## 2 Converging the calculation with respect to the grid of phonon wavevectors

Convergence studies with respect to most of the parameters will rely on obvious modifications
of the input file detailed in the previous section. However, using more than one
q-point phonon wavevector needs a non-trivial generalisation of this procedure.
This is because each q-point needs to be treated in a different dataset in the current version of ABINIT.

<!--
From now on we will only describe the approach with Abinit **compiled with Netcdf support**.
The approach with Anaddb is similar to what we described in the previous sections.
Note, however, that Anaddb only supports integration with homogenous q-point grids.
-->
The code can perform the q-wavevector integration either with random q-points or
homogenous Monkhorst-Pack meshes.
Both grids have been used in the Ref. [[cite:Ponce2014]], see e.g. Fig. 3 of this paper.

For the random integration method you
should create a script that generates random q-points, perform the Abinit
calculations at these points, gather the results and analyze them.
The temperature_final.py script will detect that you used random
integration thanks to the weight of the q-point stored in the _EIGR2D.nc file
and perform the integration accordingly.
The random integration converges slowly but in a smooth manner.

However, since this method is a little bit less user-friendly than the one based on homogeneous grids,
we will focus on this homogenous integration. 
In this case, the user must specify in the ABINIT input file the homogeneous q-point grid,
using input variables like
[[ngqpt]], [[qptopt]], [[shiftq]], [[nshiftq]], ..., i.e. variables whose names
are similar to those used to specify the k-point grid (for electrons).

There are several difficulties here.
First, since we focus on the k=$\Gamma$ point, we expect to be able to use symmetries to decrease the computational
load, as $\Gamma$ is invariant under all symmetry operations of the crystal. The symmetry operations of the crystal will be used
to decrease the number of q-wavevectors, but they cannot be used as well to decrease the k-point grid during the corresponding
self-consistent phonon computation.
How this different behaviour of k-grids and q-grids can be handled by ABINIT ?
By convention, in such case, with [[nsym]]=1 the k-point grid will be generated in the Full Brillouin zone,
without use of symmetries, while the q-point grid with [[qptopt]]=1 with be generated in the irreducible Brillouin Zone,
despite [[nsym]]=1. In order to generate q-point grids that are not folded in the irreducible Brillouin Zone, one need to use another value of [[qptopt]].
In particular [[qptopt]]=3 has to be used to generate q points in the full Brillouin zone.

Second, the number of ABINIT datasets is expected to be given in the input file, by the user,
but not determined on-the-flight by ABINIT. Still, this number of datasets is determined by the number of q points.
Thus, the user will have to compute it before being able to launch the real q-point calculations, since it determines [[ndtset]].

How to determine the number of irreducible q points ?

Well, the easiest procedure is to compute it for an equivalent k-point grid, by a quick run.

An example will clarify this.
Suppose that one is looking for the number of q-points corresponding to

    ngqpt 4 4 4
    qptopt 1
    nshiftq 1
    shiftq 0.0 0.0 0.0

One make a quick ABINIT run with [[tests/tutorespfn/Input/tdepes_2.abi]].
Note that several input variables have been changed with respect to [[tests/tutorespfn/Input/tdepes_1.abi]]:

    ndtset 1
    nstep 0
    prtebands 0
    ngkpt 4 4 4
    nshiftk 1
    shiftk 0.0 0.0 0.0
    nsym 0

In this example, the new values of [[ndtset]] and [[nstep]], and the definition of [[prtebands]]
allow a fast run ([[nline]]==0 might be specified as well,
or even, the run might be interrupted after a few seconds, since the number of k points is very quickly available).
Then, the k-point grid is
specified thanks to [[ngkpt]], [[nshiftk]], [[shiftk]], replacing the corresponding input variables for the q-point
grid. The use of symmetries has been reenabled thanks to [[nsym]]=0.

To run it, issue:

    abinit  tdepes_2.abi  

Now, the number of points can be seen in the output file :

```
             nkpt           8
```

the list of these eight k-points being given in
```
              kpt      0.00000000E+00  0.00000000E+00  0.00000000E+00
                       2.50000000E-01  0.00000000E+00  0.00000000E+00
                       5.00000000E-01  0.00000000E+00  0.00000000E+00
                       2.50000000E-01  2.50000000E-01  0.00000000E+00
                       5.00000000E-01  2.50000000E-01  0.00000000E+00
                      -2.50000000E-01  2.50000000E-01  0.00000000E+00
                       5.00000000E-01  5.00000000E-01  0.00000000E+00
                      -2.50000000E-01  5.00000000E-01  2.50000000E-01
```

We are now ready to launch the determination of the
_EIG.nc, _DDB, EIGR2D.nc and EIGI2D.nc files, with 8 q-points.
As for the $\Gamma$ calculation of the previous section, we will rely on three
datasets for each q-point. This permits a well-structured set of calculations,
although there is some redundancy. Indeed, the first of these datasets will correspond
to an unperturbed ground-state calculation identical for all q. It is done very quickly because
the converged wavefunctions are already available. The second dataset will correspond to
a non-self-consistent ground-state calculation at k+q (it is also quick thanks to previously available wavefunctions),
and the third dataset will correspond to the DFPT calculations at k+q (this is the CPU intensive part) .

So, compared to the first run in this tutorial, we have to replace

    ndtset 3     by      ndtset 24 udtset 8 3

in the input file [[tests/tutorespfn/Input/tdepes_3.abi]], and adjusted accordingly all input variables that were dataset-dependent.

{% dialog tests/tutorespfn/Input/tdepes_3.abi %}

Please, refer to the
[[help:abinit#35-defining-a-double-loop-dataset|explanation of the usage of a double-loop of datasets]]
if you are confused about the meaning of [[udtset]], and the usage of the corresponding metacharacters.
We have indeed also introduced

    iqpt:? 1
    iqpt+? 1

that translates into

    iqpt11 1
    iqpt12 1
    iqpt13 1
    iqpt21 2
    iqpt22 2
    iqpt23 2
    iqpt31 3
     ...

allowing to perform calculations for three datasets at each q-point.

Then issue:

    abinit tdepes_3.abi

This is a significantly longer ABINIT run (still less than two minutes), also producing many files.

When the run is finished, copy the file [[tests/tutorespfn/Input/tdepes_3_temperature.in]] in the
working directory (if not yet done) and launch the python script with:

    ./temperature_final.py < tdepes_3_temperature.in

{% dialog tests/tutorespfn/Input/tdepes_3_temperature.in %}

Examination of the same HOMO and LUMO bands at k=$\Gamma$ for a 4x4x4 q-point grid gives a very different result
than previously. 
The zero-point renormalization (ZPR) is the change of the bandgap at 0 K and was (band 4 - band 3):
   
    -0.012507 - 0.017727 = -0.030234 eV

and is now:

    -0.351528 - 0.095900 = -0.447428 eV

This means that the bandgap was closing by 30 meV at 0 K and is now closing by 447 meV at 0 K. 
For comparison, the converged direct bandgap ZPR of diamond is 438.6 meV from Ref. [[cite:Ponce2015]].

As a matter of fact, diamond requires an extremely dense q-point grid (40x40x40) to be converged.
On the bright side, each q-point calculation is independent and thus the parallel scaling is ideal.
Running separate jobs for different q-points is quite easy thanks to the dtset approach.

## 3 Calculation of the eigenenergy corrections along high-symmetry lines

The calculation of the electronic eigenvalue correction due to electron-phonon
coupling along high-symmetry lines requires the use of 6 datasets per q-point.
Moreover, the choice of an arbitrary k-wavevector breaks all symmetries of the crystal.
Different datasets are required to compute the following quantites:

$\Psi^{(0)}_{kHom}$ : The ground-state wavefunctions on the Homogeneous k-point sampling.

$\Psi^{(0)}_{kBS}$ : The ground-state wavefunctions computed along the bandstructure k-point sampling.

$\Psi^{(0)}_{kHom+q}$ : The ground-state wavefunctions on the shifted Homogeneous k+q-point sampling.

$n^{(1)}$ : The perturbed density integrated over the homogeneous k+q grid.

$\Psi^{(0)}_{kBS+q}$ : The ground-state wavefunctions obtained from reading the perturbed density of the previous dataset.

Reading the previous quantity we obtain the el-ph matrix elements along the bandstructure with all physical
quantities integrated over a homogeneous grid.

We will use the [[tests/tutorespfn/Input/tdepes_4.abi]] input file

{% dialog tests/tutorespfn/Input/tdepes_4.abi %}

Note the use of the usual input variables to define a path in the Brillouin Zone to build an electronic band structure:
[[kptbounds]], [[kptopt]], and [[ndivsm]]. Note also that we have defined [[qptopt]]=3. The number of q-points
is thus very easy to determine, as being the product of [[ngqpt]] values times [[nshiftq]]. Here a very rough 2*2*2 grid has been chosen,
even less dense than the one for section 2.

Then issue:

    abinit tdepes_4.abi

This is a significantly longer ABINIT run (5-10 minutes), also producing many files.


then use [[tests/tutorespfn/Input/tdepes_4_temperature.in]] for the python script.

{% dialog tests/tutorespfn/Input/tdepes_4_temperature.in %}

with the usual syntax:

    ./temperature_final.py < tdepes_4_temperature.in

<!-- THIS SECTION DOES NOT SEEM CORRECT : there is no other k point computed in section 2 ...
Of course, the high symmetry points computed in section 2 have the same value here.
It is a good idea to check it by running the script with the file tdepes_3bis.files.

-->

You can now copy the plotting script (Plot-EP-BS) python file from
~abinit/scripts/post_processing/plot_bs.py into the directory where you did all the calculations.
Now run the script:

    ./plot_bs.py

with the following input data:

```
temperature_4_EP.nc
L \Gamma X W K L W X K \Gamma
-20 30
0
```

or more directly

    ./plot_bs.py < tdepes_4_plot_bs.abi

This should give the following bandstructure

![](tdepes_assets/plot4.png)

where the solid black lines are the traditional electronic bandstructure, the
dashed lines are the electronic eigenenergies with electron-phonon
renormalization at a defined temperature (here 0K). Finally the area around
the dashed line is the lifetime of the electronic eigenstates.

Notice all the spikes in the electron-phonon case. This
is because we did a completely under-converged calculation
with respect to the q-point sampling.

It is possible to converge the calculations using [[ecut]]=30 Ha, a [[ngkpt]]
grid of 6x6x6 and an increasing [[ngqpt]] grid to get converged results:

    | Convergence study ZPR and inverse lifetime(1/τ) [eV] at 0K                |
    | q-grid   | Nb qpt |       Γ25'      |        Γ15       |      Min Γ-X     |
    |          | in IBZ |  ZPR   |  1/τ   |   ZPR   |  1/τ   |   ZPR   |  1/τ   |
    | 4x4x4    |  8     | 0.1175 | 0.0701 | -0.3178 | 0.1916 | -0.1570 | 0.0250 |
    | 10x10x10 |  47    | 0.1390 | 0.0580 | -0.3288 | 0.1847 | -0.1605 | 0.0308 |
    | 20x20x20 |  256   | 0.1446 | 0.0574 | -0.2691 | 0.1823 | -0.1592 | 0.0298 |
    | 26x26x26 |  511   | 0.1448 | 0.0573 | -0.2736 | 0.1823 | -0.1592 | 0.0297 |
    | 34x34x34 |  1059  | 0.1446 | 0.0573 | -0.2699 | 0.1821 | -0.1591 | 0.0297 |
    | 43x43x43 |  2024  | 0.1447 | 0.0572 | -0.2650 | 0.1821 | -0.1592 | 0.0297 |

As you can see the limiting factor for the convergence study is the
convergence of the LUMO band at $\Gamma$. This band is not the lowest in energy (the
lowest is on the line between $\Gamma$ and X) and therefore this band is rather
unstable. This can also be seen by the fact that it has a large electronic
broadening, meaning that this state will decay quickly into another state.

Using the relatively dense q-grid of 43x43x43 we can obtain the following
converged bandstructure, at a high temperature (1900K):

![](tdepes_assets/plot5.png)

Here we show the renormalization at a very high temperature of 1900K in order
to highlight more the broadening and renormalization that occurs. If you want
accurate values of the ZPR at 0K you can look at the table above.


!!! Important

    If you use an extremely fine q-point grid, the acoustic phonon frequencies for
    q-points close to $\Gamma$ will be wrongly determined by Abinit. Indeed in order to
    have correct phonon frequencies close to $\Gamma$, one has to impose the acousting sum rule
    with anaddb and [[asr@anaddb]].
    However, this feature is not available in the python script. Instead, the script reject the
    contribution of the acoustic phonon close to $\Gamma$ if their phonon frequency is
    lower than 1E-6 Ha. Otherwise one gets unphysically large contribution.

    One can tune this parameter by editing the variable "tol6 = 1E-6" in the beginning of the script.

    For example, for the last 43x43x43 calculation, it was set to 1E-4.

!!! important

    It is possible to speed up the convergence with respect to increasing q-point density by noticing
    that the renormalization behaves analytically with increasing q-point grid and smaller broadening.

    It is therefore possible to extrapolate the results. Different analytical behavior extists depending
    if the material is polar and if the state we are considering is a band extrema or not.
    More information can be found in Ref. [[cite:Ponce2015]]

---
authors: MG, MS
---

# Tutorial on Bethe-Salpeter calculations

## Absorption spectra including excitonic effects.

This tutorial discusses how to calculate the macroscopic dielectric function
including excitonic effects within the Bethe-Salpeter (BS) approach.
Crystalline silicon is used as test case. A brief description of the formalism
can be found in the [[theory:bse|Bether-Salpeter notes]].

The user should be familiarized with the four basic tutorials of ABINIT and the [first GW tutorial](/tutorial/gw1).

Visualisation tools are NOT covered in this tutorial.
Powerful visualisation procedures have been developed in the Abipy context,
relying on matplotlib. See the README of [Abipy](https://github.com/abinit/abipy)
and the [Abipy tutorials](https://github.com/abinit/abitutorials).

This tutorial should take about one hour to be completed.

[TUTORIAL_README]

## Preparatory steps (WFK and the SCR file)

Before starting, you might consider to work in a different subdirectory as
for the other tutorials. Why not Work_bs?

Copy the input files *\$ABI_TESTS/tutorial/Input/tbs_\*.abi*.
in the working directory *Work_bs*.

Make sure that the Si.psp8 pseudopotential needed for running the tutorial is placed under 
*\$ABI_PSPDIR/Pseudodojo_nc_sr_04_pw_standard_psp8*.

Now run immediately the calculation with the command:

    abinit tbs_1.abi > tbs_1.log 2> err &

so that we can analyze the input file while the code is running.

The input file is located in *\$ABI_TESTS/tutorial/Input/tbs_1.abi*.
The header reports a brief description of the calculation so read it carefully.
Do not worry if some parts are not clear to you as we are going to discuss the
calculation in step by step fashion.

{% dialog tests/tutorial/Input/tbs_1.abi %}

This input file generates the two WFK files and the SCR file needed for
the subsequent Bethe-Salpeter computations. The first dataset performs a
rather standard ground-state calculation on an $\Gamma$-centered 4x4x4 grid (64 k
points in the full Brillouin Zone, folding to 8 k points in the irreducible wedge).
Then the ground-state density is used in dataset 2 and 3 to generate
two WFK files with a standard NSCF run and the conjugate-gradient method.

Note that the WFK file computed in dataset 2 contains **100 bands** on the 4x4x4
**gamma-centered** k-mesh whereas the WFK file produced in dataset 3 has only
**10 bands** on a 4x4x4 k-mesh **shifted** along the direction

    shiftk3    0.11 0.21 0.31  # This shift breaks the symmetry of the k-mesh.

The $\Gamma$-centered k-mesh contains 8 points in the IBZ while the
shifted k-mesh breaks the symmetry of the crystal leading to 64 points in the
IBZ (actually the IBZ now coincides with the full Brillouin zone). The second
mesh is clearly inefficient, so you might wonder why we are using such a
bizarre sampling and, besides, why we need to generate two different WFK files!

Indeed this approach strongly differs from the one we followed in the GW
tutorials, but there is a good reason for doing so.
It is anticipated that optical spectra **converge slowly** with the BZ sampling, and that
**symmetry-breaking k-meshes** lead to faster convergence in [[nkpt]] than the
standard symmetric k-meshes commonly used for ground-state or GW calculations.

This explains the bizarre shift but, still, why two WFK files? Why not 
simply use the WFK file on the shifted k-mesh to compute the screening?

The reason is that a screening calculation done with many empty bands on the
shifted k-mesh would be very memory demanding as the code should allocate a
huge portion of memory whose size scales with ([[nband]] * [[nkpt]]), and no
symmetry can be used to reduce the number of k-points.

To summarize: the WFK with the symmetric k-point sampling and 100 bands will
be used to compute the screening, while the WFK file with the shifted k-mesh
and 10 bands will be used to construct the transition space employed for
solving the Bethe-Salpeter equation. The two k-meshes differ just for the
shift thus they produce the same set of q-points (the list of q-points in the
screening is defined as all the possible differences between the k-points of
the WFK file). This means that, in the Bethe-Salpeter run, we can use the SCR file
generated with the symmetric mesh even though the transition space is
constructed with the shifted k-mesh.

After this lengthy discussion needed to clarify this rather technical point,
we can finally proceed to analyze the screening computation performed in the
last dataset of *tbs_1.abi*.

The SCR file is calculated in dataset 4 using [[nband]] = 100 and [[ecuteps]] = 6.0 Ha.
In the [first GW tutorial](/tutorial/gw1), these
values were found to give QP energies converged within 0.01 eV, so we are
confident that our SCR file is well converged and it can be safely used for
performing convergence tests in the Bethe-Salpeter part.

Note that, for efficiency reasons, only the static limit of W is computed:

    nfreqre4  1   # Only the static limit of W is needed for standard BSE calculations.
    nfreqim4  0

Indeed, in the standard formulation of the Bethe-Salpeter equation, only the
static limit of the screened interaction is needed to construct the Coulomb
term of the BS Hamiltonian. Using a single frequency allows us to save some
CPU time in the screening part, but keep in mind that this SCR file can only
be used either for Bethe-Salpeter computations or for GW calculations
employing the plasmon-pole models corresponding to [[ppmodel]] = 3, 4.

At this point the calculation should have completed, but there is still one
thing that we have to do before moving to the next paragraph.

As we said, we will need the WFK file on the shifted k-mesh and the SCR file
for our BS calculations so do not delete them! It is also a good idea to
rename these precious files using more meaningful names *e.g.*:

    mv tbs_1o_DS2_WFK 444_gamma_WFK
    mv tbs_1o_DS3_WFK 444_shifted_WFK
    mv tbs_1o_DS4_SCR 444_SCR

!!! important

    The list of k-points specified in the BS input files **must equal** the one used
    to generate the WFK file. Two new WFK files and a new SCR file must be
    generated from scratch if we want to change the k-point sampling used to
    construct the transition space.

## Computing the absorption spectrum within the Tamm-Dancoff approximation

This section is intended to show how to perform a standard excitonic
calculation within the Tamm-Dancoff approximation (TDA) using the Haydock iterative technique.
The input file is *\$ABI_TESTS/tutorial/tutorial/Input/tbs_2.abi*.

Before running the job, we have to connect this calculation with the output
results produced in *tbs_1.abi*.

Use the Unix commands:

    ln -s 444_shifted_WFK tbs_2i_WFK
    ln -s 444_SCR tbs_2i_SCR

to create two symbolic links for the shifted WFK and the SCR file. The reason
for doing so will be clear afterwards once we discuss the input file.

This job lasts 1-2 minutes on a modern machine so it is worth running it
before inspecting the input file.

Copy the file *\$ABI_TESTS/tutorial/Input/tbs_2.abi* in the working
directory and issue:

    abinit tbs_2.abi > tbs_2.log 2> err &

to put the job in background so that we can examine *tbs_2.abi*.

Now open *\$ABI_TESTS/tutorial/Input/tbs_2.abi* in your preferred editor and go
to the next section where we discuss the most important variables governing a
typical BS computation.

{% dialog tests/tutorial/Input/tbs_2.abi %}

### The structure of the input file

First we need to set [[optdriver]] = 99 to call the BSE routines

    optdriver  99   # BS calculation

The variables [[irdwfk]] and [[irdscr]] are similar to other "ird" variables
of ABINIT and are used to read the files produced in the previous paragraph

    irdwfk  1  # Read the WFK file produced in tbs_1
    irdscr  1  # Read the SCR file produced in tbs_1

The code expects to find an input WFK file and an input SCR file.
This is the reason why we had to create the two symbolic links before running the code.

Then we have a list of five variables specifying how to construct the excitonic Hamiltonian.

    bs_calctype       1    # L0 constructed with KS orbitals and energies.
    mbpt_sciss   0.8 eV    # Scissors operator used to correct the KS band structure.
    bs_exchange_term  1    # Exchange term included.
    bs_coulomb_term  11    # Coulomb term included using the full matrix W_GG'
    bs_coupling       0    # Tamm-Dancoff approximation.

The value [[bs_calctype]] = 1 specifies that the independent-particle
polarizability should be constructed with the Kohn-Sham orbitals and energies
read from the WFK file. To simulate the self-energy correction, the KS
energies are corrected with a scissors operator of energy [[mbpt_sciss]] = 0.8 eV.
This permits us to avoid a cumbersome GW calculation for each state
included in our transition space. The use of the scissors operator is a
reasonable approximation for silicon but it might fail in more complicated
systems in which the GW corrections cannot be simulated in terms of a simple
rigid shift of the initial KS bands structure.

The remaining three variables specify how to construct the excitonic Hamiltonian.
[[bs_exchange_term]] = 1 tells the code to calculate the exchange
part of the kernel, hence this calculation includes local-field effects.
The variable [[bs_coulomb_term]] is used to select among different options that
are available for the Coulomb term (please take some time to read the
description of the variable and the relevant equations in the [[theory:bse|Bethe-Salpeter notes]].
Finally [[bs_coupling]] = 0 specifies that the off-diagonal coupling blocks
should be neglected (Tamm-Dancoff approximation).
This particular combination of parameters thus corresponds to a Bethe-Salpeter
calculation within the Tamm-Dancoff approximation with local field effects included.

Then we have the specification of the bands used to construct the transition space:

    bs_loband   2
    nband       8

In this case all the bands around the gap whose index is between 2 and 8 are
included in the basis set.

The frequency mesh for the macroscopic dielectric function is specified by [[bs_freq_mesh]]

    bs_freq_mesh 0 6 0.02 eV  # Frequency mesh.

This triplet of real values defines a linear mesh that covers the range [0, 6] eV with a step of 0.02 eV.
The number of frequency points in the mesh does not
have any significant effect on the CPU time, but it is important to stress
that the number of bands included in the transition space defines, in
conjunction with the number of k-points, the frequency range that can be
described. As a consequence [[bs_loband]] and [[nband]] should be subject to
an accurate converge study.

Then we have the parameters that define and control the algorithm employed to
calculate the macroscopic dielectric function

    bs_algorithm        2      # Haydock method (this is the default value).
    bs_haydock_niter   100     # Max number of iterations for the Haydock method.
    bs_haydock_tol     0.05    # Tolerance for the iterative method.
    zcut               0.15 eV # Complex shift to avoid divergences in the continued fraction.

[[bs_algorithm]] specifies the algorithm used to calculate the macroscopic dielectric function.
In this case we use the iterative Haydock technique whose
maximum number of iterations is given by [[bs_haydock_niter]].
The iterative algorithm stops when the difference between two consecutive evaluations of the
optical spectra is less than [[bs_haydock_tol]].
The input variable [[zcut]] gives the complex shift to avoid divergences in the continued fraction.
From a physical point of view, this parameters mimics the experimental broadening of
the absorption peaks. In this test, due to the coarseness of the k-mesh, we
have to use a value slightly larger than the default one (0.1 eV) in order to
facilitate the convergence of the Haydock algorithm.
Ideally, one should perform a convergence study by decreasing the value of [[zcut]] for increasing number of k-points.

The k-point sampling is specified by the set of variables.

    kptopt 1                # Option for the automatic generation of k points,
    ngkpt  4 4 4            # This mesh is too coarse for optical properties.
    nshiftk 1
    shiftk  0.11 0.21 0.31  # This shift breaks the symmetry of the k-mesh.
    chksymbreak 0           # Mandatory for using symmetry-breaking k-meshes in the BS code.


!!! important

    The values of [[kptopt]], [[ngkpt]], [[nshiftk]], and [[shiftk]] **must equal**
    the ones used to specify the grid for the WFK file.
    [[chksymbreak]] = 0 is used to bypass the check on symmetry breaking that, otherwise, would make the code stop.


The last section of the input file

    ecutwfn 12.0   # Cutoff for the wavefunction.
    ecuteps 2.0   # Cutoff for W and /bare v used to calculate the BS matrix elements.
    inclvkb 2     # The Commutator for the optical limit is correctly evaluated.

specifies the parameters used to calculate the kernel matrix elements and the
matrix elements of the dipole operator.
We have already encountered these variables in the [first GW tutorial](/tutorial/gw1) of the GW tutorial
so their meaning is (hopefully) familiar to you.
A more detailed discussion of the role played by these variables in the BS code
can be found in the [[theory:bse|Bether-Salpeter notes]].

### Output files

The output file, *tbs_2.abo*, reports the basic parameters of the calculation
and eventual WARNINGs that are issued if the iterative method does not converge.
Please take some time to understand its structure.

Could you answer the following questions?

  1. How many transitions are included in the basis set?
  2. How many directions are used to evaluate the optical limit?
  3. What is the value of the Lorentzian broadening used in the continued fraction?

After this digression on the main output file, we can finally proceed to
analyse the output data of the computation.

The most important results are stored in five different files:

  * *tbs_2o_BSR*
  * *tbs_2o_HAYDR_SAVE*
  * *tbs_2o_RPA_NLF_MDF*
  * *tbs_2o_GW_NLF_MDF*
  * *tbs_2o_EXC_MDF*

In what follows, we provide a brief description of the format and of the
content of each output file.

tbs_2o_BSR:

:   This binary file stores the upper triangle of the resonant block (the matrix
    is Hermitian hence only the non-redundant part is computed and saved on file).
    The BSR file can be used to restart the run from a previous computation using
    the variables [[getbsreso]] or [[irdbsreso]]. This restart capability is
    useful for restarting the Haydock method if convergence was not achieved or to
    execute Haydock computations with different values of [[zcut]]. [[getbsreso]]
    and [[irdbsreso]] are also handy if one wants to include the coupling on top
    of a pre-existing TDA calculation since the code uses two different files to
    store the resonant and the coupling block (BSC is the prefix used for the
    files storing the coupling term).

tbs_2o_HAYDR_SAVE:

:   It is a binary file containing the results of the Haydock method: the
    coefficient of the tridiagonal matrix and the three vectors employed in the
    iterative algorithm. It is usually used to restart the algorithm if
    convergence has not been achieved (see the related input variables [[gethaydock]] and [[irdhaydock]]).

tbs_2o_RPA_NLF_MDF and tbs_2o_GW_NLF_MDF

:   The RPA spectrum without local field effects obtained with KS energies and the
    GW energies, respectively (mnemonics: NLF stands for No Local Field, while MDF
    stands for Macroscopic Dielectric Function).

tbs_2o_EXC_MDF

:   Formatted file reporting the macroscopic dielectric function with excitonic effects.

The *EXC_MDF* file contains the most important results of our
calculation so it is worth spending some time to discuss its format.

First we have a header reporting the basic parameters of the calculation:

```sh
# Macroscopic dielectric function obtained with the BS equation.
#  RPA L0 with KS energies and KS wavefunctions     LOCAL FIELD EFFECTS INCLUDED
# RESONANT-ONLY calculation
# Coulomb term constructed with full W(G1,G2)
# Scissor operator energy =  0.8000 [eV]
# Tolerance =  0.0500 0.0000
# npweps  = 27
# npwwfn  = 531
# nbands  = 8
# loband  = 2
# nkibz   = 64
# nkbz    = 64
# Lorentzian broadening =  0.1500 [eV]
```

then the list of q-points giving the direction of the incident photon:

```sh
#  List of q-points for the optical limit:
# q =  0.938821, 0.000000, 0.000000, [Reduced coords]
# q =  0.000000, 0.938821, 0.000000, [Reduced coords]
# q =  0.000000, 0.000000, 0.938821, [Reduced coords]
# q =  0.000000, 0.813043, 0.813043, [Reduced coords]
# q =  0.813043, 0.000000, 0.813043, [Reduced coords]
# q =  0.813043, 0.813043, 0.000000, [Reduced coords]
```

By default the code calculates the macroscopic dielectric function considering
six different directions in q-space (the three basis vectors of the reciprocal
lattice and the three Cartesian axis). It is possible to specify custom
directions using the input variables [[gw_nqlwl]] and [[gw_qlwl]].

Then comes the section with the real and the imaginary part of the macroscopic
dielectric as a function of frequency for the different directions:

```sh
# omega [eV]    RE(eps(q=1)) IM(eps(q=1) RE(eps(q=2) ) ... 
  0.000   17.9912    0.0000   17.9578    0.0000   14.2584    0.0000   13.9627    0.0000   17.0848    0.0000   17.0421    0.0000
.... .... ...
```

You can visualize the data using your preferred software. For instance, with |gnuplot|

```gnuplot
p "tbs_2o_EXC_MDF" u 1:3 w l
```

will plot the imaginary part of the macroscopic dielectric function (the
absorption spectrum) for the first q-point. You should obtain a graphic
similar to the one reported below

![](bse_assets/tbs2_1.png)

!!! note

    These results are not converged, we postpone the discussion
    about convergence tests to the next paragraphs of this tutorial.

The most important feature of the spectrum is the presence of two peaks
located at around 3.4 and 4.3 eV. To understand the nature of these peaks and
the role played by the BS kernel, it is useful to compare the excitonic
spectra with the RPA results obtained without local field effects.

Use the sequence of gnuplot command

```gnuplot
p   "tbs_2o_EXC_MDF"     u 1:3 w l
rep "tbs_2o_RPA_NLF_MDF" u 1:3 w l
rep "tbs_2o_GW_NLF_MDF"  u 1:3 w l
```

to plot the absorption spectrum obtained with the three different approaches.
The final result is reported in the figure below.

![](bse_assets/tbs2_2.png)

The RPA-KS spectrum underestimates the experimental optical threshold due to
the well know band gap problem of DFT. Most importantly, the amplitude of the
first peak is underestimated, a problem than is not solved when local-field
effects are correctly included in the calculation.

The RPA-GW results with QP corrections simulated with [[mbpt_sciss]] does not
show any significant improvement over RPA-KS: the RPA-GW spectrum is just
shifted towards higher frequencies due to opening of the gap, but the shape of
the two spectra is very similar, in particular the amplitude of the first peak
is still underestimated.

On the contrary, the inclusion of the BS kernel leads to important changes
both in the optical threshold as well as in the amplitude of the first peak.
This simple analysis tells us that the first peak in the absorption spectrum
of silicon has a strong excitonic character that is not correctly described
within the RPA. Our first BS spectrum is not converged at all and it barely
resembles the experimental result, nevertheless this unconverged calculation
is already able to capture the most important physics.

!!! tip

    If |AbiPy| is installed on your machine, you can use the |abiopen| script
    with the `--expose` option to visualize the dielectric functions stored in the MDF.nc file:

        abiopen.py tbs_2o_MDF.nc --expose --seaborn

    ![](bse_assets/abiopen_tbs_2o_MDF.png)

    For further information about the MDF file and the post-processing tools provided by AbiPy,
    please consult the |MdfFileNb|.


### Optional Exercises

* Change the value of the Lorentzian broadening [[zcut]] used to avoid divergences in the continued fraction.
  Then restart the Haydock algorithm from the *_BSR* and *_HAYDR_SAVE* files using the appropriate variables.
  What is the main effect of the broadening on the final spectrum.
  Does the number of iterations needed to converge depend on the broadening?

* Use the appropriate values for [[bs_exchange_term]] and [[bs_coulomb_term]] to calculate the BS spectrum
  without local field effects. Compare the results obtained with and without local field effects.

* Modify the input file tbs_2.abi so that the code reads in the resonant block produced in the previous run
  and calculates the spectrum employing the method based on the direct diagonalization (use [[irdbsreso]]
  to restart the run but remember to rename the file with the resonant block).
  Compare the CPU time needed by the two algorithms as a function of the number of transitions in the transition space.
  Which one has the best scaling?

### Preliminary discussion about convergence studies

Converging the excitonic spectrum requires a careful analysis of many different parameters:

  * [[bs_loband]]
  * [[nband]]
  * [[ecutwfn]]
  * [[ecuteps]]
  * [[ngkpt]]
  * [[nshiftk]]
  * [[shiftk]]

Since the memory requirements scale quadratically with the number of k-points
in the **full** Brillouin zone **times** the number of valence bands
**times** the number of conduction bands included in the transition space,
it is very important to find a good compromise between accuracy and computational efficiency.

First of all one should select the frequency range of interest since this
choice has an important effect on the number of valence and conduction states
that have to be included in the transition space. The optical spectrum is
expected to converge faster in the number of bands than the GW corrections
since only those transitions whose energy is "close" to the frequency range
under investigation are expected to contribute.

[[ecutwfn]] usually plays a secondary role since it only affects the accuracy
of the oscillator matrix elements. We suggest avoiding any truncation of the
initial basis set by setting [[ecutwfn]] to a value slightly larger than the
value of [[ecut]] used to generate the WFK file. One should truncate the
initial planewave basis set only when experiencing memory problems although
this kind of problems can be usually solved by just increasing the number of
processors or, alternatively, with an appropriate choice of [[gwmem]].

The value of [[ecuteps]] affects the accuracy of the matrix elements of the
Coulomb term, the fundamental term that drives the creation of the excitons.
As a consequence [[ecuteps]] should be subject to an accurate convergence
test. As a rule of thumb, [[ecuteps]] can be chosen equal or, sometimes, even
smaller than the value needed to converge the GW corrections.

As already stated: optical spectra converge slowly with the Brillouin zone
sampling. The convergence in the number of k-points thus represents the most
important and tedious part of our convergence study. For this reason, this
study should be done once converged values for the other parameters have been already found.

## Convergence with respect to the number of bands in the transition space

In this section we take advantage of the multi-dataset capabilities of ABINIT
to perform calculations with different values for [[bs_loband]] and [[nband]]

Before running the test take some time to read the input file *\$ABI_TESTS/tutorial/Input/tbs_3.abi*.

{% dialog tests/tutorial/Input/tbs_3.abi %}

The convergence in the number of transitions is performed by defining two
datasets with different values for [[nband]] and [[bs_loband]]

    ndtset     2
    bs_loband1  3 nband1  6
    bs_loband2  2 nband2  7


The parameters defining how to build the excitonic Hamiltonian are similar to
the ones used in *tbs_2.abi*. The only difference is in the value used for [[bs_coulomb_term]], *i.e.*

    bs_coulomb_term  10  # Coulomb term evaluated within the diagonal approximation.

that allows us to save some CPU time during the computation of the Coulomb term.

Also in this case, before running the test, we have to connect *tbs_3.abi* to the
WFK and the SCR file produced in the first step. Note that *tbs_3.abi* uses
[[irdwfk]] and [[irdscr]] to read the external files, hence we have to create
symbolic links for each dataset:

```sh
ln -s 444_SCR tbs_3i_DS1_SCR
ln -s 444_SCR tbs_3i_DS2_SCR
ln -s 444_shifted_WFK tbs_3i_DS1_WFK
ln -s 444_shifted_WFK tbs_3i_DS2_WFK
```

Now we can finally run the test with

    abinit tbs_3.abi > tbs3.log 2> err &

This job should last 3-4 minutes so be patient!

Let us hope that your calculation has been completed, and that we can examine
the output results.

Use the following sequence of |gnuplot| commands:

```gnuplot
p   "tbs_3o_DS1_EXC_MDF" u 1:3 w l
rep "tbs_3o_DS2_EXC_MDF" u 1:3 w l
```

to plot on the same graphic the absorption spectrum obtained with different
transition spaces. You should obtain a graphic similar (but not equal) to this one

![](bse_assets/tbs3.png)

Note indeed that the above figure has been produced with more datasets in order
to highlight the convergence behaviour.

The results obtained with ([[bs_loband]] = 4, [[nband]] = 5) are clearly
unconverged as the basis set contains too few transitions that are not able to
describe the frequency-dependence of the polarizability in the energy range
under investigation. For a converged spectrum, we have to include the
three higher occupied states and the first four conduction bands (the blue
curve corresponding to [[bs_loband]] = 2, and [[nband]] = 7).

Note that adding the first occupied band, curve (1-8), gives results that are
almost on top of (2,8). This is due to the fact that, in silicon, the bottom
of the first band is located at around 12 eV from the top of the conduction
band therefore its inclusion does not lead to any significant improvement of
the transition space in the frequency range [0, 8] eV. For completeness, we
also report the results obtained in a separate calculation done with
[[bs_loband]] = 2 [[nband]] = 9 to show that four empty states are enough to converge the spectrum.

We therefore fix the number of bands for the transition space using
[[bs_loband]] = 2, [[nband]] = 7 and we proceed to analyse the
convergence of the spectrum with respect to the number of planewaves in the screening.

!!! tip

    If |AbiPy| is installed on your machine, you can use the |abicomp| script
    with the `mdf` command and the `--expose` option to compare
    multiple dielectric functions:

        abicomp.py mdf tbs_3o_DS*_MDF.nc --expose --seaborn

    ![](bse_assets/abicomp_tbs_3o.png)


### Tips for expert users

The use of [[irdwfk]] and [[irdscr]] is not handy when we have several
datasets that are reading the **same** external file as we are forced to use
different names for the input of each dataset. To work around this annoyance,
one can introduce a fictitious dataset (say dataset 99), and let the code use
the output of this nonexistent dataset as the input of the real datasets. An
example will help clarify: Instead of using the lengthy list of links as done
before, we might use the much simpler sequence of commands

    ln -s 444_shifted_WFK tbs_3o_DS99_WFK
    ln -s 444_SCR         tbs_3o_DS99_SCR

provided that, in the input file, we replace [[irdwfk]] and [[irdscr]] with

    getwfk  99   # Trick to read the same file tbs_o3_DS99_WFK in each dataset
    getscr  99   # Same trick for the SCR file

## Convergence with respect to the number of planewaves in the screening

First of all, before running the calculation, take some time to understand
what is done in *\$ABI_TESTS/tutorial/Input/tbs_4.abi*.

The structure of the input file is very similar to the one of *tbs_3.abi*, the
main difference is in the first section:

    ndtset    2
    ecuteps: 2 ecuteps+ 1
    bs_coulomb_term 11

that instructs the code to execute two calculations where the direct term is
constructed using different value of [[ecuteps]]. We also relax the diagonal-only
approximation for the screening by setting [[bs_coulomb_term]] = 11 so that
the non-locality of $W(\rr, \rr')$ is correctly taken into account.

It is important to stress that it is not necessary to recalculate the SCR file
from scratch just to modify the value of [[ecuteps]] used in the BS run. The
SCR file calculated in the preparatory step contains G-vectors whose energy
extends up to ecuteps = 6.0 Ha. This is the **maximum** cutoff energy that can be
used in our convergence tests. If the value of [[ecuteps]] specified in the
input file is smaller than the one stored on disk, the code will read a sub-
block of the initial matrix. A WARNING message is issued if the value
specified in the input file is larger than the one available in the SCR file.

Now we can finally run the calculation. As usual, we have to copy
*\$ABI_TESTS/tutorial/Input/tbs_4.abi* in the working directory *Work_bs*,
then we have to create a bunch of symbolic links for the input WFK and SCR files:

    ln -s 444_SCR tbs_4i_DS1_SCR
    ln -s 444_SCR tbs_4i_DS2_SCR
    ln -s 444_shifted_WFK tbs_4i_DS1_WFK
    ln -s 444_shifted_WFK tbs_4i_DS2_WFK

Now issue

    abinit tbs_4.abi > tbs4.log 2> err &

to execute the test (it should take around 2 minutes).

{% dialog tests/tutorial/Input/tbs_4.abi %}

Once the calculation is completed, plot the spectra obtained with different [[ecuteps]] using |gnuplot|:

```gnuplot
p "tbs_4o_DS1_EXC_MDF" u 1:3 w l
rep "tbs_4o_DS2_EXC_MDF" u 1:3 w l
```

Here we show the results of a convergence study done with *four* different values or `ecuteps`
to highlight the convergence behavior:

![](bse_assets/tbs4.png)

The spectrum is found to converge quickly in [[ecuteps]]. The curves obtained
with [[ecuteps]] = 3 and 4 Ha are almost indistinguishable from each other. Our
final estimate for [[ecuteps]] is therefore 3 Ha.

Note that this value is smaller than the one required to converge the QP
corrections within 0.01 eV (in the [first GW tutorial](/tutorial/gw1) of the GW
tutorial we obtained 6.0 Ha). This is a general behavior, in the sense that
Bethe-Salpeter spectra, unlike GW corrections, are not very sensitive
to truncations in the planewave expansion of W. Reasonable BS spectra are
obtained even when W is treated within the diagonal approximation or,
alternatively, with model dielectric functions.

Note also how the two peaks are affected in a different way by the change of
[[ecuteps]], with the first peak affected the most. This behavior is
consistent with our affirmation that the first peak of silicon has a strong excitonic character.

## Convergence with respect to the number of k-points

The last parameter that should be checked for convergence is the number of
k-points. This convergence study represents the most tedious and difficult
part since it requires the generation of new WFK files and of the new SCR file
for each k-mesh (the list of k-points for the wavefunctions and the set of
q-points in the screening must be consistent with each other).

The file *\$ABI_TESTS/tutorial/Input/tbs_5.abi* gathers the different steps of
a standard BS calculation (generation of two WFK file, screening calculation,
BS run) into a single input. The calculation is done with the converged
parameters found in the previous studies, only [[ngkpt]] has been intentionally left undefined.

{% dialog tests/tutorial/Input/tbs_5.abi %}

Use *tbs_5.abi* as a template for performing BS calculations with different
k-meshes. For example, you might try to compare the three meshes 4x4x4, 5x5x5,
and 6x6x6. To facilitate the analysis of the results, we suggest to run the
calculations in different directories so that we can keep the output results separated.

Be aware that both the CPU time as well as the memory requirements increase
quickly with the number of divisions in the mesh. These are, for example, the
CPU times required by different k-meshes on Intel Xeon X5570:

    4x4x4:    +Overall time at end (sec) : cpu=        112.4  wall=        112.4
    5x5x5:    +Overall time at end (sec) : cpu=        362.8  wall=        362.8
    6x6x6:    +Overall time at end (sec) : cpu=        914.8  wall=        914.8
    8x8x8:    +Overall time at end (sec) : cpu=       5813.3  wall=       5813.3
    10x10x10: +Overall time at end (sec) : cpu=      20907.1  wall=      20907.1
    12x12x12: +Overall time at end (sec) : cpu=      62738.2  wall=      62738.2

6x6x6 is likely the most dense sampling you can afford on a single-CPU
machine. For you convenience, we have collected the results of the convergence
test in the figure below.

![](bse_assets/tbs5.png)

As anticipated, the spectrum converges slowly with the number of k-points and
our first calculation done with the 4x4x4 grid is severely unconverged. The
most accurate results are obtained with the 12x12x12 k-mesh, but even this
sampling leads to converged results only for frequencies below 4.5 eV. This is
a problem common to all BS computations, in the sense that it is extremely
difficult to achieve global converge in the spectra. This analysis shows that
we can trust the 12x12x12 results in the [0:4,5] eV range while the correct
description of the spectrum at higher energies would require the inclusion of
more k-point and, possibly, more bands so that the band dispersion is
correctly taken into account (even the RPA spectrum does not converge at high
frequencies when 12x12x12 is used).

It should be stressed that [[zcut]] plays a very important role in these
converge tests. For example, the results obtained with the 8x8x8 or the
10x10x10 k-mesh can be brought closer to the 12x12x12 by just increasing the
Lorentzian broadening. When comparing theory with experiments, it is common to
treat [[zcut]] as an _a posteriori_ parameter chosen to produce the best
agreement with the experiment.

## Additional exercises

* Use [[bs_coupling]] = 1 to perform an excitonic calculation for silicon including the coupling term.
  Compare the imaginary part of the macroscopic dielectric function obtained with and without coupling.
  Do you find significant differences? (Caveat: calculations with coupling cannot use the Haydock method
  and are much more CPU demanding. You might have to decrease some input parameters to have results in reasonable time.)

* Calculate the one-shot GW corrections for silicon following the [first GW tutorial](/tutorial/gw1).
  Then use the `_GW` file produced by the code to calculate the absorption spectrum.

* Learn how to use AbiPy to automate BS calculations as described in this
  [jupyter notebook](https://nbviewer.jupyter.org/github/abinit/abitutorials/blob/master/abitutorials/bse/lesson_bse.ipynb).

## Notes on the MPI implementation

In this section, we discuss the approach used to parallelize the two steps of
the BS run, *i.e.* the construction of the H matrix and the evaluation of the
macroscopic dielectric function.

First of all, it is important to stress that, unlike the GW code, the BS
routines do not employ any kind of memory distribution for the wavefunctions.
The entire set of orbitals used to construct the transition space is stored on
each node This choice has been dictated by the fact that the size of H is
usually much larger than the array used to store the wavefunction, hence it is
much more important to distribute the matrix than the wavefunctions. Besides,
having all the states on each node simplifies the calculation of several
intermediate quantities needed at run-time.

The memory allocated for the wavefunctions and the screening thus will not
scale with the number of processors. However, for very memory demanding
calculations, the real space orbitals can be calculated on the fly with an
increase in computational time instead. This option is controlled by the
second digit of the input variable [[gwmem]].

When discussing the MPI parallelization of the Bethe-Salpeter routines, we
have to consider the two steps separately.

In the first step, the upper triangle of the resonant (coupling) block is
distributed among the nodes. Each CPU computes its own portion and stores the
results in a temporary array. At the end of the computation, the portions of
the upper triangle are communicated to the master node which writes the binary file BSR (BSC).

In the second step, each node reads the data stored in the external files in
order to build the excitonic Hamiltonian. The matrix is distributed using a
column-block partitioning, so that the matrix-vector multiplications required
in the Haydock iterative scheme can be easily performed in parallel (see the
schematic representation reported below). A similar distribution scheme is
also employed for the conjugate-gradient minimization. For a balanced
distribution of computational work, the number of processors should divide the
total number of resonant transitions.

![](paral_mbt_assets/MPI_mv.png)
---
authors: MG
---

# How to compile ABINIT

This tutorial explains how to compile ABINIT including the external dependencies
without relying on pre-compiled libraries, package managers and root privileges.
You will learn how to use the standard **configure** and **make** Linux tools
to build and install your own software stack including the MPI library and the associated
*mpif90* and *mpicc* wrappers required to compile MPI applications.

It is assumed that you already have a standard Unix-like installation
that provides the basic tools needed to build software from source (Fortran/C compilers and *make*).
The changes required for macOS are briefly mentioned when needed.
Windows users should install [cygwin](https://cygwin.com/index.html) that
provides a POSIX-compatible environment
or, alternatively, use a [Windows Subsystem for Linux](https://docs.microsoft.com/en-us/windows/wsl/about).
Note that the procedure described in this tutorial has been tested with Linux/macOS hence
feedback and suggestions from Windows users are welcome.

!!! tip

    In the last part of the tutorial, we discuss more advanced topics such as using **modules** in supercomputing centers,
    compiling and linking with the **intel compilers** and the **MKL library** as well as **OpenMP threads**.
    You may want to jump directly to this section if you are already familiar with software compilation.

In the following, we will make extensive use of the bash shell hence familiarity with the terminal is assumed.
For a quick introduction to the command line, please consult
this [Ubuntu tutorial](https://ubuntu.com/tutorials/command-line-for-beginners#1-overview).
If this is the first time you use the **configure && make** approach to build software,
we **strongly** recommend to read this
[guide](https://www.codecoffee.com/software-installation-configure-make-install/)
before proceeding with the next steps.
If, on the other hand, you are not interested in compiling all the components from source,
you may want to consider the following alternatives:

* Compilation with external libraries provided by apt-based Linux distributions (e.g. **Ubuntu**).
  More info available [here](/INSTALL_Ubuntu).

* Compilation with external libraries on **Fedora/RHEL/CentOS** Linux distributions.
  More info available [here](/INSTALL_CentOS).

* Homebrew bottles or macports for **macOS**.
  More info available [here](/INSTALL_MacOS).

* Automatic compilation and generation of modules on clusters with **EasyBuild**.
  More info available [here](/INSTALL_EasyBuild).

* Compiling Abinit using the **internal fallbacks** and the *build-abinit-fallbacks.sh* script
  automatically generated by *configure* if the mandatory dependencies are not found.

* Using precompiled binaries provided by conda-forge (for Linux and macOS users).

Before starting, it is also worth reading this document prepared by Marc Torrent
that introduces important concepts and provides a detailed description of the configuration options
supported by the ABINIT build system.
Note that these slides have been written for Abinit v8 hence some examples should be changed in order
to be compatible with the build system of version 9, yet the document represents a valuable source of information.

<embed src="https://school2019.abinit.org/images/lectures/abischool2019_installing_abinit_lecture.pdf"
type="application/pdf" width="100%" height="480px">


!!! important

    The aim of this tutorial is to teach you how to compile code from source but we cannot guarantee
    that these recipes will work out of the box on every possible architecture.
    We will do our best to explain how to **setup your environment** and how to **avoid the typical pitfalls**
    but we cannot cover all the possible cases.

    Fortunately, the internet provides lots of resources.
    Search engines and stackoverflow are your best friends and in some cases one can find the solution
    by just **copying the error message in the search bar**.
    For more complicated issues, you can ask for help on the [ABINIT discourse forum](https://discourse.abinit.org)
    or contact the sysadmin of your cluster but remember to provide enough information about your system
    and the problem you are encountering.

## Getting started

Since ABINIT is written in Fortran, we need a **recent Fortran compiler**
that supports the **F2003 specifications** as well as a C compiler.
At the time of writing ( |today| ), the C++ compiler is optional and required only for advanced features
that are not treated in this tutorial.

In what follows, we will be focusing on the GNU toolchain i.e. *gcc* for C and *gfortran* for Fortran.
These "sequential" compilers are adequate if you don't need to compile parallel MPI applications.
The compilation of MPI code, indeed, requires the installation of **additional libraries**
and **specialized wrappers** (*mpicc*, *mpif90* or *mpiifort* ) replacing the "sequential" compilers.
This very important scenario is covered in more detail in the next sections.
For the time being, we mainly focus on the compilation of sequential applications/libraries.

First of all, let's make sure the **gfortran** compiler is installed on your machine
by issuing in the terminal the following command:

```sh
which gfortran
/usr/bin/gfortran
```

!!! tip

    The **which** command, returns the **absolute path** of the executable.
    This Unix tool is extremely useful to pinpoint possible problems and we will use it
    a lot in the rest of this tutorial.

In our case, we are lucky that the Fortran compiler is already installed in */usr/bin* and we can immediately
use it to build our software stack.
If *gfortran* is not installed, you may want to use the package manager provided by your
Linux distribution to install it.
On Ubuntu, for instance, use:

```sh
sudo apt-get install gfortran
```

To get the version of the compiler, use the `--version` option:

```sh
gfortran --version
GNU Fortran (GCC) 5.3.1 20160406 (Red Hat 5.3.1-6)
Copyright (C) 2015 Free Software Foundation, Inc.
```

Starting with version 9, ABINIT requires gfortran >= v5.4.
Consult the release notes to check whether your gfortran version is supported by the latest ABINIT releases.

Now let's check whether **make** is already installed using:

```sh
which make
/usr/bin/make
```

Hopefully, the C compiler *gcc* is already installed on your machine.

```sh
which gcc
/usr/bin/gcc
```

At this point, we have all the basic building blocks needed to compile ABINIT from source and we
can proceed with the next steps.

!!! tip

    Life gets hard if you are a macOS user as Apple does not officially
    support Fortran (😞) so you need to install *gfortran* and *gcc* either via
    [homebrew](https://brew.sh/) or [macport](https://www.macports.org/).
    Alternatively, one can install *gfortran* using one of the standalone DMG installers
    provided by the [gfortran-for-macOS project](https://github.com/fxcoudert/gfortran-for-macOS/releases).
    Note also that macOS users will need to install **make** via [Xcode](https://developer.apple.com/xcode/).
    More info can be found in [this page](/INSTALL_MacOS).

## How to compile BLAS and LAPACK

BLAS and LAPACK represent the workhorse of many scientific codes and an optimized implementation
is crucial for achieving **good performance**.
In principle this step can be skipped as any decent Linux distribution already provides
pre-compiled versions but, as already mentioned in the introduction, we are geeks and we
prefer to compile everything from source.
Moreover the compilation of BLAS/LAPACK represents an excellent exercise
that gives us the opportunity to discuss some basic concepts that
will reveal very useful in the other parts of this tutorial.

First of all, let's create a new directory inside your `$HOME` (let's call it **local**) using the command:

```sh
cd $HOME && mkdir local
```

!!! tip

    $HOME is a standard shell variable that stores the absolute path to your home directory.
    Use:

    ```sh
    echo My home directory is $HOME
    ```

    to print the value of the variable.

    The **&&** syntax is used to chain commands together, such that the next command is executed if and only
    if the preceding command exited without errors (or, more accurately, exits with a return code of 0).
    We will use this trick a lot in the other examples to reduce the number of lines we have to type
    in the terminal so that one can easily cut and paste the examples in the terminal.


Now create the `src` subdirectory inside $HOME/local with:

```sh
cd $HOME/local && mkdir src && cd src
```

The *src* directory will be used to store the packages with the source files and compile code,
whereas executables and libraries will be installed in `$HOME/local/bin` and `$HOME/local/lib`, respectively.
We use `$HOME/local` because we are working as **normal users** and we cannot install software
in `/usr/local` where root privileges are required and a `sudo make install` would be needed.
Moreover, working inside `$HOME/local` allows us to keep our software stack well separated
from the libraries installed by our Linux distribution so that we can easily test new libraries and/or
different versions without affecting the software stack installed by our distribution.

Now download the tarball from the [openblas website](https://www.openblas.net/) with:

```sh
wget https://github.com/xianyi/OpenBLAS/archive/v0.3.7.tar.gz
```

If *wget* is not available, use *curl* with the `-o` option to specify the name of the output file as in:

```sh
curl -L https://github.com/xianyi/OpenBLAS/archive/v0.3.7.tar.gz -o v0.3.7.tar.gz
```

!!! tip

    To get the URL associated to a HTML link inside the browser, hover the mouse pointer over the link,
    press the right mouse button and then select `Copy Link Address` to copy the link to the system clipboard.
    Then paste the text in the terminal by selecting the `Copy` action in the menu
    activated by clicking on the right button.
    Alternatively, one can press the central button (mouse wheel) or use CMD + V on macOS.
    This trick is quite handy to fetch tarballs directly from the terminal.


Uncompress the tarball with:

```sh
tar -xvf v0.3.7.tar.gz
```

then `cd` to the directory with:

```sh
cd OpenBLAS-0.3.7
```

and execute

```sh
make -j2 USE_THREAD=0 USE_LOCKING=1
```

to build the single thread version.

!!! tip

    By default, *openblas* activates threads (see [FAQ page](https://github.com/xianyi/OpenBLAS/wiki/Faq#multi-threaded))
    but in our case we prefer to use the sequential version as Abinit is mainly optimized for MPI.
    The `-j2` option tells *make* to use 2 processes to build the code in order to speed up the compilation.
    Adjust this value according to the number of **physical cores** available on your machine.

At the end of the compilation, you should get the following output (note **Single threaded**):

```md
 OpenBLAS build complete. (BLAS CBLAS LAPACK LAPACKE)

  OS               ... Linux
  Architecture     ... x86_64
  BINARY           ... 64bit
  C compiler       ... GCC  (command line : cc)
  Fortran compiler ... GFORTRAN  (command line : gfortran)
  Library Name     ... libopenblas_haswell-r0.3.7.a (Single threaded)

To install the library, you can run "make PREFIX=/path/to/your/installation install".
```

<!--
As a side note, a compilation with plain *make* would give:

```md
  Library Name     ... libopenblas_haswellp-r0.3.7.a (Multi threaded; Max num-threads is 12)
```

that indicates that the openblas library now supports threads.
-->

You may have noticed that, in this particular case, *make* is not just building the library but is also
running **unit tests** to validate the build.
This means that if *make* completes successfully, we can be confident that the build is OK
and we can proceed with the installation.
Other packages use a more standard approach and provide a **make check** option that should be executed after *make*
in order to run the test suite before installing the package.

To install *openblas* in $HOME/local, issue:

```sh
make PREFIX=$HOME/local/ install
```

At this point, we should have the following **include files** installed in $HOME/local/include:

```sh
ls $HOME/local/include/
cblas.h  f77blas.h  lapacke.h  lapacke_config.h  lapacke_mangling.h  lapacke_utils.h  openblas_config.h
```

and the following **libraries** installed in $HOME/local/lib:

```sh
ls $HOME/local/lib/libopenblas*

/home/gmatteo/local/lib/libopenblas.a     /home/gmatteo/local/lib/libopenblas_haswell-r0.3.7.a
/home/gmatteo/local/lib/libopenblas.so    /home/gmatteo/local/lib/libopenblas_haswell-r0.3.7.so
/home/gmatteo/local/lib/libopenblas.so.0
```

Files ending with `.so` are **shared libraries** (`.so` stands for shared object) whereas
`.a` files are **static libraries**.
When compiling source code that relies on external libraries, the name of the library
(without the *lib* prefix and the file extension) as well as the directory where the library is located must be passed
to the linker.

The name of the library is usually specified with the `-l` option while the directory is given by `-L`.
According to these simple rules, in order to compile source code that uses BLAS/LAPACK routines,
one should use the following option:

    -L$HOME/local/lib -lopenblas

We will use a similar syntax to help the ABINIT *configure* script locate the external linear algebra library.

!!! important

    You may have noticed that we haven't specified the file extension in the library name.
    If both static and shared libraries are found, the linker gives preference to linking with the shared library
    unless the `-static` option is used.
    **Dynamic is the default behaviour** on several Linux distributions so we assume dynamic linking
    in what follows.

If you are compiling C or Fortran code that requires include files with the declaration of prototypes and the definition
of named constants, you will need to specify the location of the **include files** via the `-I` option.
In this case, the previous options should be augmented by:

```sh
-L$HOME/local/lib -lopenblas -I$HOME/local/include
```

This approach is quite common for C code where `.h` files must be included to compile properly.
It is less common for modern Fortran code in which include files are usually replaced by `.mod` files
*i.e.* Fortran modules produced by the compiler whose location is usually specified via the `-J` option.
Still, the `-I` option for include files is valuable also when compiling Fortran applications as libraries
such as FFTW and MKL rely on (Fortran) include files whose location should be passed to the compiler
via `-I` instead of `-J`,
see also the official [gfortran documentation](https://gcc.gnu.org/onlinedocs/gfortran/Directory-Options.html#Directory-Options).

Do not worry if this rather technical point is not clear to you.
Any external library has its own requirements and peculiarities and the ABINIT build system provides several options
to automate the detection of external dependencies and the final linkage.
The most important thing is that you are now aware that the compilation of ABINIT requires
the correct specification of `-L`, `-l` for libraries, `-I` for include files, and `-J` for Fortran modules.
We will elaborate more on this topic when we discuss the configuration options supported by the ABINIT build system.

<!--
https://gcc.gnu.org/onlinedocs/gcc/Link-Options.html

```sh
nm $HOME/local/lib/libopenblas.so
```
to list the symbols presented in the library.
-->

Since we have installed the package in a **non-standard directory** ($HOME/local),
we need to update two important shell variables: `$PATH` and `$LD_LIBRARY_PATH`.
If this is the first time you hear about $PATH and $LD_LIBRARY_PATH, please take some time to learn
about the meaning of these environment variables.
More information about `$PATH` is available [here](http://www.linfo.org/path_env_var.html).
See [this page](https://tldp.org/HOWTO/Program-Library-HOWTO/shared-libraries.html) for `$LD_LIBRARY_PATH`.

Add these two lines at the end of your `$HOME/.bash_profile` file

```sh
export PATH=$HOME/local/bin:$PATH

export LD_LIBRARY_PATH=$HOME/local/lib:$LD_LIBRARY_PATH
```

then execute:

```sh
source $HOME/.bash_profile
```

to activate these changes without having to start a new terminal session.
Now use:

```sh
echo $PATH
echo $LD_LIBRARY_PATH
```

to print the value of these variables.
On my Linux box, I get:

```sh
echo $PATH
/home/gmatteo/local/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin

echo $LD_LIBRARY_PATH
/home/gmatteo/local/lib:
```

Note how `/home/gmatteo/local/bin` has been **prepended** to the previous value of $PATH.
From now on, we can invoke any executable located in $HOME/local/bin by just typing
its **base name** in the shell without having to the enter the full path.

!!! warning
	Using:

	```sh
	export PATH=$HOME/local/bin
	```

	is not a very good idea as the shell will stop working. Can you explain why?

!!! tip

    macOS users should replace `LD_LIBRARY_PATH` with `DYLD_LIBRARY_PATH`

    Remember also that one can use `env` to print all the environment variables defined
    in your session and pipe the results to other Unix tools.
    Try e.g.:

    ```sh
    env | grep LD_
    ```

    to print only the variables whose name starts with **LD_**

We conclude this section with another tip.
From time to time, some compilers complain or do not display important messages
because **language support is improperly configured** on your computer.
Should this happen, we recommend to export the two variables:

```sh
export LANG=C
export LC_ALL=C
```

This will reset the language support to its most basic defaults and will make sure that you get
all messages from the compilers in English.


## How to compile libxc

At this point, it should not be so difficult to compile and install
[libxc](https://www.tddft.org/programs/libxc/), a library that provides
many useful XC functionals (PBE, meta-GGA, hybrid functionals, etc).
Libxc is written in C and can be built using the standard `configure && make` approach.
No external dependency is needed, except for basic C libraries that are available
on any decent Linux distribution.

Let's start by fetching the tarball from the internet:

```sh
# Get the tarball.
# Note the -O option used in wget to specify the name of the output file

cd $HOME/local/src
wget http://www.tddft.org/programs/libxc/down.php?file=4.3.4/libxc-4.3.4.tar.gz -O libxc.tar.gz
tar -zxvf libxc.tar.gz
```

Now configure the package with the standard `--prefix` option
to **specify the location** where all the libraries, executables, include files,
Fortran modules, man pages, etc. will be installed when we execute `make install`
(the default is `/usr/local`)

```sh
cd libxc-4.3.4 && ./configure --prefix=$HOME/local
```

Finally, build the library, run the tests and install it with:

```sh
make -j2
make check && make install
```

At this point, we should have the following include files in $HOME/local/include

```sh
[gmatteo@bob libxc-4.3.4]$ ls ~/local/include/*xc*
/home/gmatteo/local/include/libxc_funcs_m.mod  /home/gmatteo/local/include/xc_f90_types_m.mod
/home/gmatteo/local/include/xc.h               /home/gmatteo/local/include/xc_funcs.h
/home/gmatteo/local/include/xc_f03_lib_m.mod   /home/gmatteo/local/include/xc_funcs_removed.h
/home/gmatteo/local/include/xc_f90_lib_m.mod   /home/gmatteo/local/include/xc_version.h
```

where `.mod` are Fortran modules generated by the compiler that are needed
when compiling Fortran source using the *libxc* Fortran API.

!!! warning

    The `.mod` files are **compiler- and version-dependent**.
    In other words, one cannot use these `.mod` files to compile code with a different Fortran compiler.
    Moreover, you should not expect to be able to use modules compiled with
    a **different version of the same compiler**, especially if the major version has changed.
    This is one of the reasons why the version of the Fortran compiler employed
    to build our software stack is very important.


Finally, we have the following static libraries installed in ~/local/lib

```sh
ls ~/local/lib/libxc*
/home/gmatteo/local/lib/libxc.a   /home/gmatteo/local/lib/libxcf03.a   /home/gmatteo/local/lib/libxcf90.a
/home/gmatteo/local/lib/libxc.la  /home/gmatteo/local/lib/libxcf03.la  /home/gmatteo/local/lib/libxcf90.la
```

where:

  * **libxc** is the C library
  * **libxcf90** is the library with the F90 API
  * **libxcf03** is the library with the F2003 API

Both *libxcf90* and *libxcf03* depend on the C library where most of the work is done.
At present, ABINIT requires the F90 API only so we should use

    -L$HOME/local/lib -lxcf90 -lxc

for the libraries and

    -I$HOME/local/include

for the include files.

Note how `libxcf90` comes **before** the C library `libxc`.
This is done on purpose as `libxcf90` depends on `libxc` (the Fortran API calls the C implementation).
Inverting the order of the libraries will likely trigger errors (**undefined references**)
in the last step of the compilation when the linker tries to build the final application.

Things become even more complicated when we have to build applications using many different interdependent
libraries as the **order of the libraries** passed to the linker is of crucial importance.
Fortunately the ABINIT build system is aware of this problem and all the dependencies
(BLAS, LAPACK, FFT, LIBXC, MPI, etc) will be automatically put in the right order so
you don't have to worry about this point although it is worth knowing about it.

## Compiling and installing FFTW

FFTW is a C library for computing the Fast Fourier transform in one or more dimensions.
ABINIT already provides an internal implementation of the FFT algorithm implemented in Fortran
hence FFTW is considered an optional dependency.
Nevertheless, **we do not recommend the internal implementation if you really care about performance**.
The reason is that FFTW (or, even better, the DFTI library provided by intel MKL)
is usually much faster than the internal version.

!!! important

    FFTW is very easy to install on Linux machines once you have *gcc* and *gfortran*.
    The [[fftalg]] variable defines the implementation to be used and 312 corresponds to the FFTW implementation.
    The default value of [[fftalg]] is automatically set by the *configure* script via pre-preprocessing options.
    In other words, if you activate support for FFTW (DFTI) at configure time,
    ABINIT will use [[fftalg]] 312 (512) as default.

The FFTW source code can be downloaded from [fftw.org](http://www.fftw.org/),
and the tarball of the latest version is available at <http://www.fftw.org/fftw-3.3.8.tar.gz>.

```sh
cd $HOME/local/src

wget http://www.fftw.org/fftw-3.3.8.tar.gz
tar -zxvf fftw-3.3.8.tar.gz && cd fftw-3.3.8
```

The compilation procedure is very similar to the one already used for the *libxc* package.
Note, however, that ABINIT needs both the **single-precision** and the **double-precision** version.
This means that we need to configure, build and install the package **twice**.

To build the single precision version, use:

```sh
./configure --prefix=$HOME/local --enable-single
make -j2
make check && make install
```

During the configuration step, make sure that *configure* finds the Fortran compiler
because ABINIT needs the Fortran interface.

```md
checking for gfortran... gfortran
checking whether we are using the GNU Fortran 77 compiler... yes
checking whether gfortran accepts -g... yes
checking if libtool supports shared libraries... yes
checking whether to build shared libraries... no
checking whether to build static libraries... yes
```

Let's have a look at the libraries we've just installed:

```sh
ls $HOME/local/lib/libfftw3*
/home/gmatteo/local/lib/libfftw3f.a  /home/gmatteo/local/lib/libfftw3f.la
```

the `f` at the end stands for `float` (C jargon for single precision).
Note that only static libraries have been built.
To build shared libraries, one should use `--enable-shared` when configuring.

Now we configure for the double precision version (this is the default behaviour so no extra option is needed)

```sh
./configure --prefix=$HOME/local
make -j2
make check && make install
```

After this step, you should have two libraries with the single and the double precision API:

```sh
ls $HOME/local/lib/libfftw3*
/home/gmatteo/local/lib/libfftw3.a   /home/gmatteo/local/lib/libfftw3f.a
/home/gmatteo/local/lib/libfftw3.la  /home/gmatteo/local/lib/libfftw3f.la
```

To compile ABINIT with FFTW3 support, one should use:

    -L$HOME/local/lib -lfftw3f -lfftw3 -I$HOME/local/include


Note that, unlike in *libxc*, here we don't have to specify different libraries for Fortran and C
as FFTW3 bundles **both the C and the Fortran API in the same library**.
The Fortran interface is included by default provided the FFTW3 *configure* script can find a Fortran compiler.
In our case, we know that our FFTW3 library supports Fortran as *gfortran* was found by *configure*
but this may not be true if you are using a precompiled library installed via your package manager.

To make sure we have the Fortran API, use the `nm` tool
to get the list of symbols in the library and then use *grep* to search for the Fortran API.
For instance we can check whether our library contains the Fortran routine for multiple single-precision
FFTs (*sfftw_plan_many_dft*) and the version for multiple double-precision FFTs (*dfftw_plan_many_dft*)

```sh
[gmatteo@bob fftw-3.3.8]$ nm $HOME/local/lib/libfftw3f.a | grep sfftw_plan_many_dft
0000000000000400 T sfftw_plan_many_dft_
0000000000003570 T sfftw_plan_many_dft__
0000000000001a90 T sfftw_plan_many_dft_c2r_
0000000000004c00 T sfftw_plan_many_dft_c2r__
0000000000000f60 T sfftw_plan_many_dft_r2c_
00000000000040d0 T sfftw_plan_many_dft_r2c__

[gmatteo@bob fftw-3.3.8]$ nm $HOME/local/lib/libfftw3.a | grep dfftw_plan_many_dft
0000000000000400 T dfftw_plan_many_dft_
0000000000003570 T dfftw_plan_many_dft__
0000000000001a90 T dfftw_plan_many_dft_c2r_
0000000000004c00 T dfftw_plan_many_dft_c2r__
0000000000000f60 T dfftw_plan_many_dft_r2c_
00000000000040d0 T dfftw_plan_many_dft_r2c__
```

If you are using a FFTW3 library without Fortran support, the ABINIT *configure* script will complain that the library
cannot be called from Fortran and you will need to dig into *config.log* to understand what's going on.

!!! note

    At present, there is no need to compile FFTW with MPI support because ABINIT implements its own
    version of the MPI-FFT algorithm based on the sequential FFTW version.
    The MPI algorithm implemented in ABINIT is optimized for plane-waves codes
    as it supports zero-padding and composite transforms for the applications of the local part of the KS potential.

    Also, **do not use MKL with FFTW3** for the FFT as the MKL library exports the same symbols as FFTW.
    This means that the linker will receive multiple definitions for the same procedure and
    the **behaviour is undefined**! Use either MKL or FFTW3 with e.g. openblas.

## Installing MPI

In this section, we discuss how to compile and install the MPI library.
This step is required if you want to run ABINIT with multiple processes and/or you
need to compile MPI-based libraries such as PBLAS/Scalapack or the HDF5 library with support for parallel IO.

It is worth stressing that the MPI installation provides two scripts (**mpif90** and **mpicc**)
that act as a sort of wrapper around the sequential Fortran and the C compilers, respectively.
These scripts must be used to compile parallel software using MPI instead
of the "sequential" *gfortran* and *gcc*.
The MPI library also provides launcher scripts installed in the *bin* directory (**mpirun** or **mpiexec**)
that must be used to execute an MPI application EXEC with NUM_PROCS MPI processes with the syntax:

```sh
mpirun -n NUM_PROCS EXEC [EXEC_ARGS]
```

!!! warning

    Keep in mind that there are **several MPI implementations** available around
    (*openmpi*, *mpich*, *intel mpi*, etc) and you must **choose one implementation and stick to it**
    when building your software stack.
    In other words, all the libraries and executables requiring MPI must be compiled, linked and executed
    **with the same MPI library**.

    Don't try to link a library compiled with e.g. *mpich* if you are building the code with
    the *mpif90* wrapper provided by e.g. *openmpi*.
    By the same token, don't try to run executables compiled with e.g. *intel mpi* with the
    *mpirun* launcher provided by *openmpi* unless you are looking for troubles!
    Again, the *which* command is quite handy to pinpoint possible problems especially if there are multiple
    installations of MPI in your $PATH (not a very good idea!).

In this tutorial, we employ the *mpich* implementation that can be downloaded
from this [webpage](https://www.mpich.org/downloads/).
In the terminal, issue:

```sh
cd $HOME/local/src
wget http://www.mpich.org/static/downloads/3.3.2/mpich-3.3.2.tar.gz
tar -zxvf mpich-3.3.2.tar.gz
cd mpich-3.3.2/
```

to download and uncompress the tarball.
Then configure/compile/test/install the library with:

```sh
./configure --prefix=$HOME/local
make -j2
make check && make install
```

Once the installation is completed, you should obtain this message
(possibly not the last message, you might have to look for it).

```sh
----------------------------------------------------------------------
Libraries have been installed in:
   /home/gmatteo/local/lib

If you ever happen to want to link against installed libraries
in a given directory, LIBDIR, you must either use libtool, and
specify the full pathname of the library, or use the '-LLIBDIR'
flag during linking and do at least one of the following:
   - add LIBDIR to the 'LD_LIBRARY_PATH' environment variable
     during execution
   - add LIBDIR to the 'LD_RUN_PATH' environment variable
     during linking
   - use the '-Wl,-rpath -Wl,LIBDIR' linker flag
   - have your system administrator add LIBDIR to '/etc/ld.so.conf'

See any operating system documentation about shared libraries for
more information, such as the ld(1) and ld.so(8) manual pages.
----------------------------------------------------------------------
```

The reason why we should add `$HOME/local/lib` to `$LD_LIBRARY_PATH` now should be clear to you.

Let's have a look at the MPI executables we have just installed in $HOME/local/bin:

```sh
ls $HOME/local/bin/mpi*
/home/gmatteo/local/bin/mpic++        /home/gmatteo/local/bin/mpiexec        /home/gmatteo/local/bin/mpifort
/home/gmatteo/local/bin/mpicc         /home/gmatteo/local/bin/mpiexec.hydra  /home/gmatteo/local/bin/mpirun
/home/gmatteo/local/bin/mpichversion  /home/gmatteo/local/bin/mpif77         /home/gmatteo/local/bin/mpivars
/home/gmatteo/local/bin/mpicxx        /home/gmatteo/local/bin/mpif90
```

Since we added $HOME/local/bin to $PATH, we should see that *mpi90* is actually
pointing to the version we have just installed:

```sh
which mpif90
~/local/bin/mpif90
```

As already mentioned, *mpif90* is a wrapper around the sequential Fortran compiler.
To show the Fortran compiler invoked by *mpif90*, use:

```sh
mpif90 -v

mpifort for MPICH version 3.3.2
Using built-in specs.
COLLECT_GCC=gfortran
COLLECT_LTO_WRAPPER=/usr/libexec/gcc/x86_64-redhat-linux/5.3.1/lto-wrapper
Target: x86_64-redhat-linux
Configured with: ../configure --enable-bootstrap --enable-languages=c,c++,objc,obj-c++,fortran,ada,go,lto --prefix=/usr --mandir=/usr/share/man --infodir=/usr/share/info --with-bugurl=http://bugzilla.redhat.com/bugzilla --enable-shared --enable-threads=posix --enable-checking=release --enable-multilib --with-system-zlib --enable-__cxa_atexit --disable-libunwind-exceptions --enable-gnu-unique-object --enable-linker-build-id --with-linker-hash-style=gnu --enable-plugin --enable-initfini-array --disable-libgcj --with-isl --enable-libmpx --enable-gnu-indirect-function --with-tune=generic --with-arch_32=i686 --build=x86_64-redhat-linux
Thread model: posix
gcc version 5.3.1 20160406 (Red Hat 5.3.1-6) (GCC)
```

The C include files (*.h*) and the Fortran modules (*.mod*) have been installed in $HOME/local/include

```sh
ls $HOME/local/include/mpi*

/home/gmatteo/local/include/mpi.h              /home/gmatteo/local/include/mpicxx.h
/home/gmatteo/local/include/mpi.mod            /home/gmatteo/local/include/mpif.h
/home/gmatteo/local/include/mpi_base.mod       /home/gmatteo/local/include/mpio.h
/home/gmatteo/local/include/mpi_constants.mod  /home/gmatteo/local/include/mpiof.h
/home/gmatteo/local/include/mpi_sizeofs.mod
```

In principle, the location of the directory must be passed to the Fortran compiler either
with the `-J` (`mpi.mod` module for MPI2+) or the `-I` option (`mpif.h` include file for MPI1).
Fortunately, the ABINIT build system can automatically detect your MPI installation and set all the compilation
options automatically if you provide the installation root ($HOME/local).

## Installing HDF5 and netcdf4

Abinit developers are trying to move away from Fortran binary files as this
format is not portable and difficult to read from high-level languages such as python.
For this reason, in Abinit v9, HDF5 and netcdf4 have become **hard-requirements**.
This means that the *configure* script will abort if these libraries are not found.
In this section, we explain how to build HDF5 and netcdf4 from source including support for parallel IO.

Netcdf4 is built on top of HDF5 and consists of two different layers:

* The **low-level C library**

* The **Fortran bindings** i.e. Fortran routines calling the low-level C implementation.
  This is the high-level API used by ABINIT to perform all the IO operations on netcdf files.

To build the libraries required by ABINIT, we will compile the three different layers
in a bottom-up fashion starting from the HDF5 package (*HDF5 --> netcdf-c --> netcdf-fortran*).
Since we want to activate support for parallel IO, we need to compile the libraries **using the wrappers**
provided by our MPI installation instead of using *gcc* or *gfortran* directly.

Let's start by downloading the HDF5 tarball from this [download page](https://www.hdfgroup.org/downloads/hdf5/source-code/).
Uncompress the archive with *tar* as usual, then configure the package with:

```sh
./configure --prefix=$HOME/local/ \
            CC=$HOME/local/bin/mpicc --enable-parallel --enable-shared
```

where we've used the **CC** variable to specify the C compiler.
This step is crucial in order to activate support for parallel IO.

!!! tip

    A table with the more commonly-used predefined variables is available
    [here](https://www.gnu.org/software/make/manual/html_node/Implicit-Variables.html)


At the end of the configuration step, you should get the following output:

```sh
                     AM C Flags:
               Shared C Library: yes
               Static C Library: yes

                        Fortran: no
                            C++: no
                           Java: no

Features:
---------
                   Parallel HDF5: yes
Parallel Filtered Dataset Writes: yes
              Large Parallel I/O: yes
              High-level library: yes
                    Threadsafety: no
             Default API mapping: v110
  With deprecated public symbols: yes
          I/O filters (external): deflate(zlib)
                             MPE:
                      Direct VFD: no
                         dmalloc: no
  Packages w/ extra debug output: none
                     API tracing: no
            Using memory checker: no
 Memory allocation sanity checks: no
          Function stack tracing: no
       Strict file format checks: no
    Optimization instrumentation: no
```

The line with:

```sh
Parallel HDF5: yes
```

tells us that our HDF5 build supports parallel IO.
The Fortran API is not activated but this is not a problem
as ABINIT will be interfaced with HDF5 through the Fortran bindings provided by netcdf-fortran.
In other words, **ABINIT requires _netcdf-fortran_** and not the HDF5 Fortran bindings.

Again, issue `make -j NUM` followed by `make check` and finally `make install`.
Note that `make check` may take some time so you may want to install immediately and run the tests in another terminal
so that you can continue with the tutorial.

Now let's move to netcdf.
Download the C version and the Fortran bindings from the
[netcdf website](https://www.unidata.ucar.edu/downloads/netcdf/)
and unpack the tarball files as usual.

```sh
wget ftp://ftp.unidata.ucar.edu/pub/netcdf/netcdf-c-4.7.3.tar.gz
tar -xvf netcdf-c-4.7.3.tar.gz

wget ftp://ftp.unidata.ucar.edu/pub/netcdf/netcdf-fortran-4.5.2.tar.gz
tar -xvf netcdf-fortran-4.5.2.tar.gz
```

To compile the C library, use:

```sh
cd netcdf-c-4.7.3
./configure --prefix=$HOME/local/ \
            CC=$HOME/local/bin/mpicc \
            LDFLAGS=-L$HOME/local/lib CPPFLAGS=-I$HOME/local/include
```

where `mpicc` is used as C compiler (**CC** environment variable)
and we have to specify **LDFLAGS** and **CPPFLAGS** as we want to link against our installation of *hdf5*.
At the end of the configuration step, we should obtain

```sh
# NetCDF C Configuration Summary
==============================

# General
-------
NetCDF Version:		4.7.3
Dispatch Version:       1
Configured On:		Wed Apr  8 00:53:19 CEST 2020
Host System:		x86_64-pc-linux-gnu
Build Directory: 	/home/gmatteo/local/src/netcdf-c-4.7.3
Install Prefix:         /home/gmatteo/local

# Compiling Options
-----------------
C Compiler:		/home/gmatteo/local/bin/mpicc
CFLAGS:
CPPFLAGS:		-I/home/gmatteo/local/include
LDFLAGS:		-L/home/gmatteo/local/lib
AM_CFLAGS:
AM_CPPFLAGS:
AM_LDFLAGS:
Shared Library:		yes
Static Library:		yes
Extra libraries:	-lhdf5_hl -lhdf5 -lm -ldl -lz -lcurl

# Features
--------
NetCDF-2 API:		yes
HDF4 Support:		no
HDF5 Support:		yes
NetCDF-4 API:		yes
NC-4 Parallel Support:	yes
PnetCDF Support:	no
DAP2 Support:		yes
DAP4 Support:		yes
Byte-Range Support:	no
Diskless Support:	yes
MMap Support:		no
JNA Support:		no
CDF5 Support:		yes
ERANGE Fill Support:	no
Relaxed Boundary Check:	yes
```

The section:

```sh
HDF5 Support:		yes
NetCDF-4 API:		yes
NC-4 Parallel Support:	yes
```

tells us that *configure* detected our installation of *hdf5* and that support for parallel-IO is activated.

Now use the standard sequence of commands to compile and install the package:

```sh
make -j2
make check && make install
```

Once the installation is completed, use the `nc-config` executable to
inspect the features provided by the library we've just installed.

```sh
which nc-config
/home/gmatteo/local/bin/nc-config

# installation directory
nc-config --prefix
/home/gmatteo/local/
```

To get a summary of the options used to build the C layer and the available features, use

```sh
nc-config --all

This netCDF 4.7.3 has been built with the following features:

  --cc            -> /home/gmatteo/local/bin/mpicc
  --cflags        -> -I/home/gmatteo/local/include
  --libs          -> -L/home/gmatteo/local/lib -lnetcdf
  --static        -> -lhdf5_hl -lhdf5 -lm -ldl -lz -lcurl
  ....
  <snip>
```

*nc-config* is quite useful as it prints the compiler options required to
build C applications requiring netcdf-c (`--cflags` and `--libs`).
Unfortunately, this tool is not enough for ABINIT as we need the Fortran bindings as well.

To compile the Fortran bindings, execute:

```sh
cd netcdf-fortran-4.5.2
./configure --prefix=$HOME/local/ \
            FC=$HOME/local/bin/mpif90 \
            LDFLAGS=-L$HOME/local/lib CPPFLAGS=-I$HOME/local/include
```

where **FC** points to our *mpif90* wrapper (**CC** is not needed here).
For further info on how to build *netcdf-fortran*, see the
[official documentation](https://www.unidata.ucar.edu/software/netcdf/docs/building_netcdf_fortran.html).

Now issue:

```sh
make -j2
make check && make install
```

To inspect the features activated in our Fortran library, use `nf-config` instead of `nc-config`
(note the `nf-` prefix):

```sh
which nf-config
/home/gmatteo/local/bin/nf-config

# installation directory
nf-config --prefix
/home/gmatteo/local/
```

To get a summary of the options used to build the Fortran bindings and the list of available features, use

```sh
nf-config --all

This netCDF-Fortran 4.5.2 has been built with the following features:

  --cc        -> gcc
  --cflags    ->  -I/home/gmatteo/local/include -I/home/gmatteo/local/include

  --fc        -> /home/gmatteo/local/bin/mpif90
  --fflags    -> -I/home/gmatteo/local/include
  --flibs     -> -L/home/gmatteo/local/lib -lnetcdff -L/home/gmatteo/local/lib -lnetcdf -lnetcdf -ldl -lm
  --has-f90   ->
  --has-f03   -> yes

  --has-nc2   -> yes
  --has-nc4   -> yes

  --prefix    -> /home/gmatteo/local
  --includedir-> /home/gmatteo/local/include
  --version   -> netCDF-Fortran 4.5.2
```

!!! tip

    *nf-config* is quite handy to pass options to the ABINIT *configure* script.
    Instead of typing the full list of libraries (`--flibs`) and the location of the include files (`--fflags`)
    we can delegate this boring task to *nf-config* using
    [backtick syntax](https://unix.stackexchange.com/questions/48392/understanding-backtick/48393):

    ```sh
    NETCDF_FORTRAN_LIBS=`nf-config --flibs`
    NETCDF_FORTRAN_FCFLAGS=`nf-config --fflags`
    ```

    Alternatively, one can simply pass the installation directory (here we use the `$(...)` syntax):

    ```sh
    --with-netcdf-fortran=$(nf-config --prefix)
    ```

    and then let *configure* detect **NETCDF_FORTRAN_LIBS** and **NETCDF_FORTRAN_FCFLAGS** for us.

## How to compile ABINIT

In this section, we finally discuss how to compile ABINIT using the
MPI compilers and the libraries installed previously.
First of all, download the ABINIT tarball from [this page](https://www.abinit.org/packages) using e.g.

```sh
wget https://www.abinit.org/sites/default/files/packages/abinit-9.0.2.tar.gz
```

Here we are using version 9.0.2 but you may want to download the
latest production version to take advantage of new features and benefit from bug fixes.

Once you got the tarball, uncompress it by typing:

```sh
tar -xvzf abinit-9.0.2.tar.gz
```

Then `cd` into the newly created *abinit-9.0.2* directory.
Before actually starting the compilation, type:

```sh
./configure --help
```

and take some time to read the documentation of the different options.

The documentation mentions the most important environment variables
that can be used to specify compilers and compilation flags.
We already encountered some of these variables in the previous examples:

```md
Some influential environment variables:
  CC          C compiler command
  CFLAGS      C compiler flags
  LDFLAGS     linker flags, e.g. -L<lib dir> if you have libraries in a
              nonstandard directory <lib dir>
  LIBS        libraries to pass to the linker, e.g. -l<library>
  CPPFLAGS    (Objective) C/C++ preprocessor flags, e.g. -I<include dir> if
              you have headers in a nonstandard directory <include dir>
  CPP         C preprocessor
  CXX         C++ compiler command
  CXXFLAGS    C++ compiler flags
  FC          Fortran compiler command
  FCFLAGS     Fortran compiler flags
```

Besides the standard environment variables: **CC**, **CFLAGS**, **FC**, **FCFLAGS** etc.
the build system also provides specialized options to activate support for external libraries.
For *libxc*, for instance, we have:

```md
LIBXC_CPPFLAGS
            C preprocessing flags for LibXC.
LIBXC_CFLAGS
            C flags for LibXC.
LIBXC_FCFLAGS
            Fortran flags for LibXC.
LIBXC_LDFLAGS
            Linker flags for LibXC.
LIBXC_LIBS
            Library flags for LibXC.
```

According to what we have seen during the compilation of *libxc*, one should pass to
*configure* the following options:

```sh
LIBXC_LIBS="-L$HOME/local/lib -lxcf90 -lxc"
LIBXC_FCFLAGS="-I$HOME/local/include"
```

Alternatively, one can use the **high-level interface** provided by the `--with-LIBNAME` options
to specify the installation directory as in:

```sh
--with-libxc="$HOME/local/lib"
```

In this case, *configure* will try to **automatically detect** the other options.
This is the easiest approach but if *configure* cannot detect the dependency properly,
you may need to inspect `config.log` for error messages and/or set the options manually.

<!--
https://www.cprogramming.com/tutorial/shared-libraries-linux-gcc.html
In this example, we will be taking advantage of the high-level interface provided by the *with_XXX* options
to tell the build system where external dependencies are located instead of passing options explicitly.
-->

In the previous examples, we executed *configure* in the top level directory of the package but
for ABINIT we prefer to do things in a much cleaner way using a **build directory**
The advantage of this approach is that we keep object files and executables separated from the source code
and this allows us to **build different executables using the same source tree**.
For example, one can have a build directory with a version compiled with *gfortran* and another
build directory for the intel *ifort* compiler or other builds done with same compiler but different compilation options.

Let's call the build directory `build_gfortran`:

```sh
mkdir build_gfortran && cd build_gfortran
```

Now we should define the options that will be passed to the *configure* script.
Instead of using the command line as done in the previous examples,
we will be using an **external file** (*myconf.ac9*) to collect all our options.
The syntax to read options from file is:

```sh
../configure --with-config-file="myconf.ac9"
```

where double quotation marks may be needed for portability reasons.
Note the use of `../configure` as we are working inside the build directory `build_gfortran` while
the `configure` script is located in the top level directory of the package.

!!! important

    The name of the options in `myconf.ac9` is in **normalized form** that is
    the initial `--` is removed from the option name and all the other `-` characters in the string
    are replaced by an underscore `_`.
    Following these simple rules, the  *configure* option `--with-mpi` becomes `with_mpi` in the ac9 file.

    Also note that in the configuration file it is possible to use **shell variables**
    and reuse the output of external tools using
    [backtick syntax](https://unix.stackexchange.com/questions/48392/understanding-backtick/48393)
    as is `nf-config --flibs` or, if you prefer, `${nf-config --flibs}`.
    This tricks allow us to reduce the amount of typing
    and have configuration files that can be easily reused for other machines.

This is an example of configuration file in which we use the high-level interface
(`with_LIBNAME=dirpath`) as much as possible, except for linalg and FFTW3.
The explicit value of *LIBNAME_LIBS* and *LIBNAME_FCFLAGS* is also reported in the commented sections.

```sh
# -------------------------------------------------------------------------- #
# MPI support                                                                #
# -------------------------------------------------------------------------- #

#   * the build system expects to find subdirectories named bin/, lib/,
#     include/ inside the with_mpi directory
#
with_mpi=$HOME/local/

# Flavor of linear algebra libraries to use (default is netlib)
#
with_linalg_flavor="openblas"

# Library flags for linear algebra (default is unset)
#
LINALG_LIBS="-L$HOME/local/lib -lopenblas"

# -------------------------------------------------------------------------- #
# Optimized FFT support                                                      #
# -------------------------------------------------------------------------- #

# Flavor of FFT framework to support (default is auto)
#
# The high-level interface does not work yet so we pass options explicitly
#with_fftw3="$HOME/local/lib"

# Explicit options for fftw3
with_fft_flavor="fftw3"
FFTW3_LIBS="-L$HOME/local/lib -lfftw3f -lfftw3"
FFTW3_FCFLAGS="-L$HOME/local/include"

# -------------------------------------------------------------------------- #
# LibXC
# -------------------------------------------------------------------------- #
# Install prefix for LibXC (default is unset)
#
with_libxc="$HOME/local"

# Explicit options for libxc
#LIBXC_LIBS="-L$HOME/local/lib -lxcf90 -lxc"
#LIBXC_FCFLAGS="-I$HOME/local/include"

# -------------------------------------------------------------------------- #
# NetCDF
# -------------------------------------------------------------------------- #

# install prefix for NetCDF (default is unset)
#
with_netcdf=$(nc-config --prefix)
with_netcdf_fortran=$(nf-config --prefix)

# Explicit options for netcdf
#with_netcdf="yes"
#NETCDF_FORTRAN_LIBS=`nf-config --flibs`
#NETCDF_FORTRAN_FCFLAGS=`nf-config --fflags`

# install prefix for HDF5 (default is unset)
#
with_hdf5="$HOME/local"

# Explicit options for hdf5
#HDF5_LIBS=`nf-config --flibs`
#HDF5_FCFLAGS=`nf-config --fflags`

# Enable OpenMP (default is no)
enable_openmp="no"
```

A documented template with all the supported options can be found here

{% dialog build/config-template.ac9 %}

Copy the content of the example in *myconf.ac9*, then run:

```sh
../configure --with-config-file="myconf.ac9"
```

If everything goes smoothly, you should obtain the following summary:

```md
==============================================================================
=== Final remarks                                                          ===
==============================================================================


Core build parameters
---------------------

  * C compiler       : gnu version 5.3
  * Fortran compiler : gnu version 5.3
  * architecture     : intel xeon (64 bits)
  * debugging        : basic
  * optimizations    : standard

  * OpenMP enabled   : no (collapse: ignored)
  * MPI    enabled   : yes (flavor: auto)
  * MPI    in-place  : no
  * MPI-IO enabled   : yes
  * GPU    enabled   : no (flavor: none)

  * LibXML2 enabled  : no
  * LibPSML enabled  : no
  * XMLF90  enabled  : no
  * HDF5 enabled     : yes (MPI support: yes)
  * NetCDF enabled   : yes (MPI support: yes)
  * NetCDF-F enabled : yes (MPI support: yes)

  * FFT flavor       : fftw3 (libs: user-defined)
  * LINALG flavor    : openblas (libs: user-defined)

  * Build workflow   : monolith

0 deprecated options have been used:.

Configuration complete.
You may now type "make" to build Abinit.
(or "make -j<n>", where <n> is the number of available processors)
```

!!! important

    Please take your time to read carefully the final summary and **make sure you are getting what you expect**.
    A lot of typos or configuration errors can be easily spotted at this level.

    You might then find useful to have a look at other examples available [in this page](/developers/autoconf_examples).
    Additional configuration files for clusters can be found in the
    [abiconfig package](https://github.com/abinit/abiconfig).

The *configure* script has generated several **Makefiles** required by *make* as well as the **config.h**
include file with all the pre-processing options that will be used to build ABINIT.
This file is included in every ABINIT source file and it defines the features that will be activated or deactivated
at compilation-time depending on the libraries available on your machine.
Let's have a look at a selected portion of **config.h**:

```c
/* Define to 1 if you have a working MPI installation. */
#define HAVE_MPI 1

/* Define to 1 if you have a MPI-1 implementation (obsolete, broken). */
/* #undef HAVE_MPI1 */

/* Define to 1 if you have a MPI-2 implementation. */
#define HAVE_MPI2 1

/* Define to 1 if you want MPI I/O support. */
#define HAVE_MPI_IO 1

/* Define to 1 if you have a parallel NetCDF library. */
/* #undef HAVE_NETCDF_MPI */
```

This file tells us that

- we are building ABINIT with MPI support
- we have a library implementing the MPI2 specifications
- our MPI implementation supports parallel MPI-IO. Note that this does not mean that *netcdf* supports MPI-IO.
  In this example, indeed, **HAVE_NETCDF_MPI is undefined** and this means the library does not have
  parallel-IO capabilities.

Of course, end users are mainly concerned with the final summary reported
by the *configure* script to understand whether
a particular feature has been activated or not but more advanced users may
find the content of `config.h` valuable to understand what's going on.

Now we can finally compile the package with e.g. *make -j2*.
If the compilation completes successfully (🙌), you should end up with a bunch of executables inside *src/98_main*.
Note, however, that the fact that the compilation completed successfully **does not necessarily imply that
the executables will work as expected** as there are many different things that can go
[wrong at runtime](https://en.wikiquote.org/wiki/Murphy%27s_law).

First of all, let's try to execute:

```sh
abinit --version
```

!!! tip

    If this is a parallel build, you may need to use

    ```sh
    mpirun -n 1 abinit --version
    ```

    even for a sequential run as certain MPI libraries are not able to bootstrap the MPI library
    without *mpirun* (*mpiexec*). On some clusters with Slurm, the syadmin may ask you to use
    *srun* instead of *mpirun*.

To get the summary of options activated during the build, run *abinit* with the `-b` option
(or `--build` if you prefer the verbose version)

```sh
./src/98_main/abinit -b
```

If the executable does not crash (🙌), you may want to execute

```sh
make test_fast
```

to run some basic tests.
If something goes wrong when executing the binary or when running the tests,
checkout the [Troubleshooting](#troubleshooting) section for possible solutions.

Finally, you may want to execute the *runtests.py* python script in the *tests* directory
in order to validate the build before running production calculations:

```sh
cd tests
../../tests/runtests.py v1 -j4
```

As usual, use:

```sh
../../tests/runtests.py --help
```

to list the available options.
A more detailed discussion is given in [this page](/developers/testsuite_howto).

[![asciicast](https://asciinema.org/a/40324.png)](https://asciinema.org/a/40324)


### Dynamic libraries and ldd

Since we decided to compile with **dynamic linking**, the external libraries are not included in the final executables.
Actually, the libraries will be loaded by the Operating System (OS) at runtime when we execute the binary.
The OS will search for dynamic libraries using the list of directories specified
in `$LD_LIBRARY_PATH` (`$DYLD_LIBRARY_PATH` for MacOs).

A typical mistake is to execute *abinit* with a wrong `$LD_LIBRARY_PATH` that is either **empty or
different from the one used when compiling the code**
(if it's different and it works, I assume you know what you are doing so you should not be reading this section!)

On Linux, one can use the *ldd* tool to print the shared objects (shared libraries) required by each
program or shared object specified on the command line:

```sh
ldd src/98_main/abinit

	linux-vdso.so.1 (0x00007fffbe7a4000)
	libopenblas.so.0 => /home/gmatteo/local/lib/libopenblas.so.0 (0x00007fc892155000)
	libnetcdff.so.7 => /home/gmatteo/local/lib/libnetcdff.so.7 (0x00007fc891ede000)
	libnetcdf.so.15 => /home/gmatteo/local/lib/libnetcdf.so.15 (0x00007fc891b62000)
	libhdf5_hl.so.200 => /home/gmatteo/local/lib/libhdf5_hl.so.200 (0x00007fc89193c000)
	libhdf5.so.200 => /home/gmatteo/local/lib/libhdf5.so.200 (0x00007fc891199000)
	libz.so.1 => /lib64/libz.so.1 (0x00007fc890f74000)
	libdl.so.2 => /lib64/libdl.so.2 (0x00007fc890d70000)
	libgfortran.so.3 => /lib64/libgfortran.so.3 (0x00007fc890a43000)
	libm.so.6 => /lib64/libm.so.6 (0x00007fc890741000)
	libmpifort.so.12 => /home/gmatteo/local/lib/libmpifort.so.12 (0x00007fc89050a000)
	libmpi.so.12 => /home/gmatteo/local/lib/libmpi.so.12 (0x00007fc88ffb9000)
	libquadmath.so.0 => /lib64/libquadmath.so.0 (0x00007fc88fd7a000)
	libgcc_s.so.1 => /lib64/libgcc_s.so.1 (0x00007fc88fb63000)
	libc.so.6 => /lib64/libc.so.6 (0x00007fc88f7a1000)
    ...
    <snip>
```

As expected, our executable uses the  *openblas*, *netcdf*, *hdf5*, *mpi* libraries installed in $HOME/local/lib plus
other basic libs coming from `lib64`(e.g. *libgfortran*) added by the compiler.

!!! tip

    On macOS, replace *ldd* with *otool* and the syntax:

    ```sh
    otool -L abinit
    ```

    If you see entries like:

    ```sh
    /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libLAPACK.dylib (compatibility version 1.0.0, current version 1.0.0)
    /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib (compatibility version 1.0.0, current version 1.0.0)
    ```

    it means that you are linking against **macOS VECLIB**.
    In this case, make sure to use `--enable-zdot-bugfix="yes"` when configuring the package
    otherwise the code will crash at runtime due to ABI incompatibility (calling conventions
    for functions returning complex values).
    Did I tell you that macOS does not care about Fortran?
    If you wonder about the difference between API and ABI, please read this
    [stackoverflow post](https://stackoverflow.com/questions/2171177/what-is-an-application-binary-interface-abi).


To understand why LD_LIBRARY_PATH is so important, let's try to reset the value of this variable with

```sh
unset LD_LIBRARY_PATH
echo $LD_LIBRARY_PATH
```

then rerun *ldd* (or *otool*) again.
Do you understand what's happening here?
Why it's not possible to execute *abinit* with an empty `$LD_LIBRARY_PATH`?
How would you fix the problem?

### Troubleshooting

Problems can appear at different levels:

* configuration time
* compilation time
* runtime *i.e.* when executing the code

**Configuration-time errors** are usually due to misconfiguration of the environment, missing (hard) dependencies
or critical problems in the software stack that will make *configure* abort.
Unfortunately, the error message reported by *configure* is not always self-explanatory.
To pinpoint the source of the problem you will need to **search for clues in _config.log_**,
especially the error messages associated to the feature/library that is triggering the error.

This is not as easy as it looks since *configure* sometimes performs multiple tests to detect your architecture
and some of these tests are **supposed to fail**.
As a consequence, not all the error messages reported in *config.log* are necessarily relevant.
Even if you find the test that makes *configure* abort, the error message may be obscure and difficult to decipher.
In this case, you can ask for help on the forum but remember to provide enough info on your architecture, the
compilation options and, most importantly, **a copy of _config.log_**.
Without this file, indeed, it is almost impossible to understand what's going on.

An example will help.
Let's assume we are compiling on a cluster using modules provided by our sysadmin.
More specifically, there is an `openmpi_intel2013_sp1.1.106` module that is supposed to provide
the *openmpi* implementation of the MPI library compiled with a particular version of the intel compiler
(remember what we said about using the same version of the compiler).
Obviously **we need to load the modules before running configure** in order to setup our environment
so we issue:

```sh
module load openmpi_intel2013_sp1.1.106
```

The module seems to work as no error message is printed to the terminal and `which mpicc` shows
that the compiler has been added to $PATH.
At this point we try to configure ABINIT with:

```sh
with_mpi_prefix="${MPI_HOME}"
```

where `$MPI_HOME` is a environment variable set by *module load* (use e.g. `env | grep MPI`).
Unfortunately, the *configure* script aborts at the very beginning complaining
that the C compiler does not work!

```text
checking for gcc... /cm/shared/apps/openmpi/1.7.5/intel2013_sp1.1.106/bin/mpicc
checking for C compiler default output file name...
configure: error: in `/home/gmatteo/abinit/build':
configure: error: C compiler cannot create executables
See `config.log' for more details.
```

Let's analyze the output of *configure*.
The line:

```md
checking for gcc... /cm/shared/apps/openmpi/1.7.5/intel2013_sp1.1.106/bin/mpicc
```

indicates that *configure* was able to find *mpicc* in *${MPI_HOME}/bin*.
Then an internal test is executed to make sure the wrapper can compile a rather simple Fortran program using MPI
but the test fails and *configure* aborts immediately with the pretty explanatory message:

```md
configure: error: C compiler cannot create executables
See `config.log' for more details.
```

If we want to understand why *configure* failed, we have to **open _config.log_ in the editor**
and search for error messages towards the end of the log file.
For example one can search for the string "C compiler cannot create executables".
Immediately above this line, we find the following section:

??? note "config.log"

    ```sh
    configure:12104: checking whether the C compiler works
    configure:12126: /cm/shared/apps/openmpi/1.7.5/intel2013_sp1.1.106/bin/mpicc conftest.c  >&5
    /cm/shared/apps/openmpi/1.7.5/intel2013_sp1.1.106/lib/libmpi.so: undefined reference to `ibv_reg_xrc_rcv_qp@IBVERBS_1.1'
    /cm/shared/apps/openmpi/1.7.5/intel2013_sp1.1.106/lib/libmpi.so: undefined reference to `ibv_modify_xrc_rcv_qp@IBVERBS_1.1'
    /cm/shared/apps/openmpi/1.7.5/intel2013_sp1.1.106/lib/libmpi.so: undefined reference to `ibv_open_xrc_domain@IBVERBS_1.1'
    /cm/shared/apps/openmpi/1.7.5/intel2013_sp1.1.106/lib/libmpi.so: undefined reference to `ibv_unreg_xrc_rcv_qp@IBVERBS_1.1'
    /cm/shared/apps/openmpi/1.7.5/intel2013_sp1.1.106/lib/libmpi.so: undefined reference to `ibv_query_xrc_rcv_qp@IBVERBS_1.1'
    /cm/shared/apps/openmpi/1.7.5/intel2013_sp1.1.106/lib/libmpi.so: undefined reference to `ibv_create_xrc_rcv_qp@IBVERBS_1.1'
    /cm/shared/apps/openmpi/1.7.5/intel2013_sp1.1.106/lib/libmpi.so: undefined reference to `ibv_create_xrc_srq@IBVERBS_1.1'
    /cm/shared/apps/openmpi/1.7.5/intel2013_sp1.1.106/lib/libmpi.so: undefined reference to `ibv_close_xrc_domain@IBVERBS_1.1'
    configure:12130: $? = 1
    configure:12168: result: no
    configure: failed program was:
    | /* confdefs.h */
    | #define PACKAGE_NAME "ABINIT"
    | #define PACKAGE_TARNAME "abinit"
    | #define PACKAGE_VERSION "9.1.2"
    | #define PACKAGE_STRING "ABINIT 9.1.2"
    | #define PACKAGE_BUGREPORT "https://bugs.launchpad.net/abinit/"
    | #define PACKAGE_URL ""
    | #define PACKAGE "abinit"
    | #define VERSION "9.1.2"
    | #define ABINIT_VERSION "9.1.2"
    | #define ABINIT_VERSION_MAJOR "9"
    | #define ABINIT_VERSION_MINOR "1"
    | #define ABINIT_VERSION_MICRO "2"
    | #define ABINIT_VERSION_BUILD "20200824"
    | #define ABINIT_VERSION_BASE "9.1"
    | #define HAVE_OS_LINUX 1
    | /* end confdefs.h.  */
    |
    | int
    | main ()
    | {
    |
    |   ;
    |   return 0;
    | }
    ```

The line

```sh
configure:12126: /cm/shared/apps/openmpi/1.7.5/intel2013_sp1.1.106/bin/mpicc conftest.c  >&5
```

tells us that *configure* tried to compile a C file named *conftest.c* and that the return value
stored in the `$?` shell variable is non-zero thus indicating failure:

```sh
configure:12130: $? = 1
configure:12168: result: no
```

The failing program (the C main after the line "configure: failed program was:")
is a rather simple piece of code and our *mpicc* compiler is not able to compile it!
If we look more carefully at the lines after the invocation of *mpicc*,
we see lots of undefined references to functions of the *libibverbs* library:

```sh
configure:12126: /cm/shared/apps/openmpi/1.7.5/intel2013_sp1.1.106/bin/mpicc conftest.c  >&5
/cm/shared/apps/openmpi/1.7.5/intel2013_sp1.1.106/lib/libmpi.so: undefined reference to `ibv_reg_xrc_rcv_qp@IBVERBS_1.1
```

This looks like some mess in the system configuration and not necessarily a problem in the ABINIT build system.
Perhaps there have been changes to the environment, maybe a system upgrade or the module is simply broken.
In this case you should send the *config.log* to the sysadmin so that he/she can fix the problem or just use
another more recent module.

Obviously, one can encounter cases in which modules are properly configured yet the *configure* script aborts
because it does not know how to deal with your software stack.
In both cases, **_config.log_ is key to pinpoint the problem** and sometimes you will find that
the problem is rather simple to solve.
For instance, you are using a Fortran module files produced by *gfortran* while trying to compile with the
intel compiler or perhaps you are trying to use modules produced by a different version of the same compiler.
Perhaps you forgot to add the include directory required by an external library and the compiler
cannot find the include file or maybe there is a typo in the configuration options.
The take-home message is that several mistakes can be detected by just **inspecting the log messages**
reported in *configure.log* if you know how to search for them.

**Compilation-time errors** are usually due to syntax errors, portability issues or
Fortran constructs that are not supported by that particular version of the compiler.
In the first two cases, please report the problem on the forum.
In the later case, you will need a more recent version of the compiler.
Sometimes the compilation aborts with an **internal compiler error** that should be considered
as a **bug in the compiler** rather than an error in the ABINIT source code.
Decreasing the optimization level when compiling the particular routine that triggers the error
(use -O1 or even -O0 for the most problematic cases) may solve the problem else
try a more recent version of the compiler.
If you have made non-trivial changes in code (modifications in the datatypes/interfaces),
run `make clean` and recompile.

**Runtime errors** are more difficult to fix as they may require the use of a debugger and some basic
understanding of [Linux signals](https://en.wikipedia.org/wiki/Signal_(IPC)).
Here we focus on two common scenarios: **SIGILL** and **SIGSEGV**.

If the code raises the **SIGILL** signal, it means that the CPU attempted to execute
an instruction it didn't understand.
Very likely, your executables/libraries have been compiled for the **wrong architecture**.
This may happen on clusters when the CPU family available on the frontend differs
from the one available on the compute node and aggressive optimization options (-O3, -march, -xHost, etc) are used.
Removing the optimization options and using the much safer -O2 level may help.
Alternatively, one can **configure and compile** the source directly on the compute node or use compilation options
compatible both with the frontend and the compute node (ask your sysadmin for details).

!!! warning

    Never ever run calculations on CPUs belonging to different families unless
    you know what you are doing.
    Many MPI codes assume reproducibility at the binary level:
    on different MPI processes the same set of bits in input should produce the same set of bits in output.
    If you are running on a heterogeneous cluster, select the queue with the same CPU family
    and make sure the code has been compiled with options that are compatibile with the compute node.

Segmentation faults (**SIGSEGV**) are usually due to bugs in the code but they may also be
triggered by non-portable code or misconfiguration of the software stack.
When reporting this kind of problem on the forum, please add an input file so that developers
can try to reproduce the problem.
Keep in mind, however, that the problem may not be reproducible on other architectures.
The ideal solution would be to run the code under the control of the debugger,
use the backtrace to locate the line of code where the segmentation fault occurs and then
attach the backtrace to your issue on the forum.

??? note "How to run gdb"

    Using the debugger in sequential is really simple.
    First of all, make sure the code have been compiled with the `-g` option
    to generate **source-level debug information**.
    To use the `gdb` GNU debugger, perform the following operations:

      1. Load the executable in the GNU debugger using the syntax:

        ```sh
        gdb path_to_abinit_executable
        ```

      2. Run the code with the `run` command and pass the input file as argument:

        ```sh
        (gdb) run t02.in
        ```

      3. Wait for the error e.g. SIGSEGV, then print the **backtrace** with:

        ```sh
        (gdb) bt
        ```

    PS: avoid debugging code compiled with `-O3` or `-Ofast` as the backtrace may not be reliable.
    Sometimes, even `-O2` (default) is not reliable and you have to resort to print statements
    and bisection to braket the problematic piece of code.

## How to compile ABINIT on a cluster with the intel toolchain and modules

On intel-based clusters, we suggest to compile ABINIT with the intel compilers (**_icc_** and **_ifort_**)
and MKL in order to achieve better performance.
The MKL library, indeed, provides highly-optimized implementations for BLAS, LAPACK, FFT, and SCALAPACK
that can lead to a **significant speedup** while simplifying considerably the compilation process.
As concerns MPI, intel provides its own implementation (**Intel MPI**) but it is also possible to employ
*openmpi* or *mpich* provided these libraries have been compiled with the **same intel compilers**.

In what follows, we assume a cluster in which scientific software is managed
with **modules** and the [EasyBuild](https://easybuild.readthedocs.io/en/latest/index.html) framework.
Before proceeding with the next steps, it is worth summarizing the most important *module* commands.

??? note "module commands"

    To list the modules installed on the cluster, use:

    ```sh
    module avail
    ```

    The syntax to load the module `MODULE_NAME` is:

    ```sh
    module load MODULE_NAME
    ```

    while

    ```sh
    module list
    ```

    prints the list of modules currently loaded.

    To list all modules containing "string", use:

    ```sh
    module spider string  # requires LMOD with LUA
    ```

    Finally,

    ```sh
    module show MODULE_NAME
    ```

    shows the commands in the module file (useful for debugging).
    For a more complete introduction to environment modules, please consult
    [this page](https://support.ceci-hpc.be/doc/_contents/UsingSoftwareAndLibraries/UsingPreInstalledSoftware/index.html).

<!--
The first thing we should do is to locate the module(s) proving MKL and the MPI library.
Sorry for repeating it again but this step is crucial as the MPI module
will define our toolchain (MPI library + compilers)
and all the other libraries **must be compiled with the same toolchain**.
-->

On my cluster, I can activate **intel MPI** by executing:

```sh
module load releases/2018b
module load intel/2018b
module load iimpi/2018b
```

to load the 2018b intel MPI [EasyBuild toolchain](https://easybuild.readthedocs.io/en/latest/Common-toolchains.html).
On your cluster, you may need to load different modules but the effect
at the level of the shell environment should be the same.
More specifically, **mpiifort** is now in **PATH** (note how *mpiifort* wraps intel *ifort*):

```sh
mpiifort -v
mpiifort for the Intel(R) MPI Library 2018 Update 3 for Linux*
Copyright(C) 2003-2018, Intel Corporation.  All rights reserved.
ifort version 18.0.3  
```

the directories with the libraries required by the compiler/MPI have been added
to **LD_LIBRARY_PATH** while **CPATH** stores the locations to search for include file.
Last but not least, the env should now define [intel-specific variables](https://software.intel.com/content/www/us/en/develop/documentation/mpi-developer-reference-windows/top/environment-variable-reference/compilation-environment-variables.html)
whose name starts with `I_`:

```sh
$ env | grep I_
I_MPI_ROOT=/opt/cecisw/arch/easybuild/2018b/software/impi/2018.3.222-iccifort-2018.3.222-GCC-7.3.0-2.30
```

Since **I_MPI_ROOT** points to the installation directory of intel MPI, 
we can use this environment variable to tell *configure* how to locate our MPI installation:

```sh
with_mpi="${I_MPI_ROOT}"

FC="mpiifort"  # Use intel wrappers. Important!
CC="mpiicc"    # See warning below
CXX="mpiicpc"

# with_optim_flavor="aggressive"
# FCFLAGS="-g -O2"
```

Optionally, you can use `with_optim_flavor="aggressive` to let *configure* select compilations
options tuned for performance or set the options explicitly via **FCFLAGS**.

!!! warning

    Intel MPI installs **two sets of MPI wrappers**.
    (*mpiicc*, *mpicpc*, *mpiifort*) and (*mpicc*, *mpicxx*, *mpif90*) that use
    Intel compilers and GNU compilers, respectively.
    Use the `-show` option (e.g. `mpif90 -show`) to display the underlying compiler.
    As expected

    ```sh
    $ mpif90 -v

    mpif90 for the Intel(R) MPI Library 2018 Update 3 for Linux*
    COLLECT_GCC=gfortran
    <snip>
    Thread model: posix
    gcc version 7.3.0 (GCC)
    ```

    shows that `mpif90` wraps GNU *gfortran*.
    Unless you really need to use GNU compilers, we strongly suggest the wrappers
    based on the Intel compilers (**_mpiicc_**, **_mpicpc_**, **_mpiifort_**)


If we run *configure* with these options, we should see a section at the beginning
in which the build system is testing basic capabilities of the Fortran compiler.
If *configure* stops at this level it means there's a severe problem with your toolchain.

```text
 ==============================================================================
 === Fortran support                                                        ===
 ==============================================================================

checking for mpiifort... /opt/cecisw/arch/easybuild/2018b/software/impi/2018.3.222-iccifort-2018.3.222-GCC-7.3.0-2.30/bin64/mpiifort
checking whether we are using the GNU Fortran compiler... no
checking whether mpiifort accepts -g... yes
checking which type of Fortran compiler we have... intel 18.0
```

Then we have a section in which *configure* tests the MPI implementation:


??? note "Multicore architecture support"

    ```text
    ==============================================================================
    === Multicore architecture support                                         ===
    ==============================================================================

    checking whether to enable OpenMP support... no
    checking whether to enable MPI... yes
    checking how MPI parameters have been set... yon
    checking whether the MPI C compiler is set... yes
    checking whether the MPI C++ compiler is set... yes
    checking whether the MPI Fortran compiler is set... yes
    checking for MPI C preprocessing flags...
    checking for MPI C flags...
    checking for MPI C++ flags...
    checking for MPI Fortran flags...
    checking for MPI linker flags...
    checking for MPI library flags...
    checking whether the MPI C API works... yes
    checking whether the MPI C environment works... yes
    checking whether the MPI C++ API works... yes
    checking whether the MPI C++ environment works... yes
    checking whether the MPI Fortran API works... yes
    checking whether the MPI Fortran environment works... yes
    checking whether to build MPI I/O code... auto
    checking which level of MPI is supported by the Fortran compiler... 2
    configure: forcing MPI-2 standard level support
    checking whether the MPI library supports MPI_INTEGER16... yes
    checking whether the MPI library supports MPI_CREATE_TYPE_STRUCT... yes
    checking whether the MPI library supports MPI_IBCAST (MPI3)... yes
    checking whether the MPI library supports MPI_IALLGATHER (MPI3)... yes
    checking whether the MPI library supports MPI_IALLTOALL (MPI3)... yes
    checking whether the MPI library supports MPI_IALLTOALLV (MPI3)... yes
    checking whether the MPI library supports MPI_IGATHERV (MPI3)... yes
    checking whether the MPI library supports MPI_IALLREDUCE (MPI3)... yes
    configure:
    configure: dumping all MPI parameters for diagnostics
    configure: ------------------------------------------
    configure:
    configure: Configure options:
    configure:
    configure:   * enable_mpi_inplace = ''
    configure:   * enable_mpi_io      = ''
    configure:   * with_mpi           = 'yes'
    configure:   * with_mpi_level     = ''
    configure:
    configure: Internal parameters
    configure:
    configure:   * MPI enabled (required)                       : yes
    configure:   * MPI C compiler is set (required)             : yes
    configure:   * MPI C compiler works (required)              : yes
    configure:   * MPI Fortran compiler is set (required)       : yes
    configure:   * MPI Fortran compiler works (required)        : yes
    configure:   * MPI environment usable (required)            : yes
    configure:   * MPI C++ compiler is set (optional)           : yes
    configure:   * MPI C++ compiler works (optional)            : yes
    configure:   * MPI-in-place enabled (optional)              : no
    configure:   * MPI-IO enabled (optional)                    : yes
    configure:   * MPI configuration type (computed)            : yon
    configure:   * MPI Fortran level supported (detected)       : 2
    configure:   * MPI_Get_library_version available (detected) : unknown
    configure:
    configure: All required parameters must be set to 'yes'.
    configure: If not, the configuration and/or the build with
    configure: MPI support will very likely fail.
    configure:
    checking whether to activate GPU support... no
    ```

So far so good. Our compilers and MPI seem to work so we can proceed with
the setup of the external libraries.

On my cluster, `module load intel/2018b` has also defined the **MKLROOT** env variable

```sh
env | grep MKL

MKLROOT=/opt/cecisw/arch/easybuild/2018b/software/imkl/2018.3.222-iimpi-2018b/mkl
EBVERSIONIMKL=2018.3.222
```

that can be used in conjunction with the **highly recommended**
[mkl-link-line-advisor](https://software.intel.com/en-us/articles/intel-mkl-link-line-advisor)
to link with MKL. On other clusters, you may need load an *mkl* module explicitly
(or *composerxe* or *parallel-studio-xe*)

Let's now discuss how to configure ABINIT with MKL starting from the simplest cases:

- BLAS and Lapack from MKL
- FFT from MKL DFTI
- no Scalapack
- no OpenMP threads.

These are the options I have to select in the
[mkl-link-line-advisor](https://software.intel.com/en-us/articles/intel-mkl-link-line-advisor)
to enable this configuration with my software stack:

![](compilation_assets/link_line_advisor.png)

The options should be self-explanatory.
Perhaps the tricky part is **Select interface layer** where one should select **32-bit integer**.
This simply means that we are compiling and linking code in which default integer is 32-bits wide
(default behaviour).
Note how the threading layer is set to **Sequential** (no OpenMP threads)
and how we chose to **link with MKL libraries explicitly** the get the full
link line and compiler options.

Now we can use these options in our configuration file:

```sh
# BLAS/LAPACK with MKL
with_linalg_flavor="mkl"

LINALG_CPPFLAGS="-I${MKLROOT}/include"
LINALG_FCFLAGS="-I${MKLROOT}/include"
LINALG_LIBS="-L${MKLROOT}/lib/intel64 -lmkl_intel_lp64 -lmkl_core -lmkl_sequential -lpthread -lm -ldl"

# FFT from MKL
with_fft_flavor="dfti"

FFT_CPPFLAGS="-I${MKLROOT}/include"
FFT_FCFLAGS="-I${MKLROOT}/include"
FFT_LIBS="-L${MKLROOT}/lib/intel64 -lmkl_intel_lp64 -lmkl_core -lmkl_sequential -lpthread -lm -ldl"
```

!!! warning

    **Do not use MKL with FFTW3** for the FFT as the MKL library exports the same symbols as FFTW.
    This means that the linker will receive multiple definitions for the same procedure and
    the **behaviour is undefined**! Use either MKL or FFTW3 with e.g. *openblas*.


If we run *configure* with these options, we should obtain the following output in the
**Linear algebra support** section:

??? note "Linear algebra support"

    ```text
    ==============================================================================
    === Linear algebra support                                                 ===
    ==============================================================================

    checking for the requested linear algebra flavor... mkl
    checking for the serial linear algebra detection sequence... mkl
    checking for the MPI linear algebra detection sequence... mkl
    checking for the MPI acceleration linear algebra detection sequence... none
    checking how to detect linear algebra libraries... verify
    checking for BLAS support in the specified libraries... yes
    checking for AXPBY support in the BLAS libraries... yes
    checking for GEMM3M in the BLAS libraries... yes
    checking for mkl_imatcopy in the specified libraries... yes
    checking for mkl_omatcopy in the specified libraries... yes
    checking for mkl_omatadd in the specified libraries... yes
    checking for mkl_set/get_threads in the specified libraries... yes
    checking for LAPACK support in the specified libraries... yes
    checking for LAPACKE C API support in the specified libraries... no
    checking for PLASMA support in the specified libraries... no
    checking for BLACS support in the specified libraries... no
    checking for ELPA support in the specified libraries... no
    checking how linear algebra parameters have been set... env (flavor: kwd)
    checking for the actual linear algebra flavor... mkl
    checking for linear algebra C preprocessing flags... none
    checking for linear algebra C flags... none
    checking for linear algebra C++ flags... none
    checking for linear algebra Fortran flags... -I/opt/cecisw/arch/easybuild/2018b/software/imkl/2018.3.222-iimpi-2018b/mkl/include
    checking for linear algebra linker flags... none
    checking for linear algebra library flags... -L/opt/cecisw/arch/easybuild/2018b/software/imkl/2018.3.222-iimpi-2018b/mkl/lib/intel64 -lmkl_intel_lp64 -lmkl_core -lmkl_sequential -lpthread -lm -ldl
    configure: WARNING: parallel linear algebra is not available
    ```

Excellent, *configure* detected a working BLAS/Lapack installation, plus some MKL extensions (*mkl_imatcopy* etc).
BLACS and Scalapack (parallel linear algebra) have not been detected but this is expected as we haven't asked
for these libraries in the mkl-link-line-advisor GUI.

This is the section in which *configure* checks the presence of the FFT library
(DFTI from MKL, goedecker means internal Fortran version).

??? note "Optimized FFT support"

    ```text
    ==============================================================================
    === Optimized FFT support                                                  ===
    ==============================================================================

    checking which FFT flavors to enable... dfti goedecker
    checking for FFT flavor... dfti
    checking for FFT C preprocessing flags...
    checking for FFT C flags...
    checking for FFT Fortran flags...
    checking for FFT linker flags...
    checking for FFT library flags...
    checking for the FFT flavor to try... dfti
    checking whether to enable DFTI... yes
    checking how DFTI parameters have been set... mkl
    checking for DFTI C preprocessing flags... none
    checking for DFTI C flags... none
    checking for DFTI Fortran flags... -I/opt/cecisw/arch/easybuild/2018b/software/imkl/2018.3.222-iimpi-2018b/mkl/include
    checking for DFTI linker flags... none
    checking for DFTI library flags... -L/opt/cecisw/arch/easybuild/2018b/software/imkl/2018.3.222-iimpi-2018b/mkl/lib/intel64 -lmkl_intel_lp64 -lmkl_core -lmkl_sequential -lpthread -lm -ldl
    checking whether the DFTI library works... yes
    checking for the actual FFT flavor to use... dfti
    ```

The line

```text
checking whether the DFTI library works... yes
```

tells us that DFTI has been found and we can link against it although this does not necessarily mean
that the final executable will work out of the box.


!!! tip

    You may have noticed that it is also possible to use MKL with GNU *gfortran* but in this case you need
    to use a different set of libraries including the so-called **compatibility layer** that allows GCC code
    to call MKL routines.
    Also, **MKL Scalapack requires either Intel MPI or MPICH2**.


??? note "Optional Exercise"

     Compile ABINIT with BLAS/ScalaPack from MKL.
     Scalapack (or ELPA) may lead to a significant speedup when running GS calculations
     with large [[nband]]. See also the [[np_slk]] input variable.


### How to compile libxc, netcdf4/hdf5 with intel

At this point, one should check whether our cluster provides modules for
*libxc*, *netcdf-fortran*, *netcdf-c* and *hdf5* **compiled with the same toolchain**.
Use `module spider netcdf` or `module keyword netcdf` to find the modules (if any).

Hopefully, you will find a pre-existent installation for *netcdf* and *hdf5* (possibly with MPI-IO support)
as these libraries are quite common on HPC centers.
Load these modules to have `nc-config` and `nf-config` in your $PATH and then use the
`--prefix` option to specify the installation directory as done in the previous examples.
Unfortunately, *libxc* and *hdf5* do not provide similar tools so you will have to find
the installation directory for these libs and pass it to *configure*.

<!--
For libxc and hdf5
```sh
with_netcdf="`nc-config --prefix`"
with_netcdf_fortran="`nf-config --prefix`"
with_hdf5="`installation_dir_for_hdf5`"
with_hdf5="${EBROOTHDF5}"

# libxc
with_libxc="${EBROOTLIBXC}"
```
-->

!!! tip

    You may encounter problems with *libxc* as this library is rather domain-specific
    and not all the HPC centers install it.
    If your cluster does not provide *libxc*, it should not be that difficult
    to reuse the expertise acquired in this tutorial to build
    your version and then install the missing dependencies inside $HOME/local.
    Just remember to:

    1. load the correct modules for MPI with the associated compilers before configuring
    2. *configure* with **CC=mpiicc** and **FC=mpiifort** so that the intel compilers are used
    3. install the libraries and prepend $HOME/local/lib to LD_LIBRARY_PATH
    4. use the *with_LIBNAME* option in conjunction with $HOME/local/lib in the ac9 file.
    5. run *configure* with the ac9 file.

In the worst case scenario in which neither *netcdf4/hdf5* nor *libxc* are installed, you may want to
use the **internal fallbacks**.
The procedure goes as follows.

- Start to configure with a minimalistic set of options just for MPI and MKL (linalg and FFT)
- The build system will detect that some hard dependencies are missing and will generate a
  *build-abinit-fallbacks.sh* script in the *fallbacks* directory.
- Execute the script to build the missing dependencies **using the toolchain specified
  in the initial configuration file**
- Finally, reconfigure ABINIT with the fallbacks.


## How to compile ABINIT with support for OpenMP threads

!!! tip

    For a quick introduction to MPI and OpenMP and a comparison between the two parallel programming paradigms, see this
    [presentation](https://princetonuniversity.github.io/PUbootcamp/sessions/parallel-programming/Intro_PP_bootcamp_2018.pdf).

Compiling ABINIT with OpenMP is not that difficult as everything boils down to:

* Using a **threaded version** for BLAS, LAPACK and FFTs
* Passing **enable_openmp="yes"** to the ABINIT configure script
  so that OpenMP is activated also at level of the ABINIT Fortran code.

On the contrary, answering the questions:

* When and why should I use OpenMP threads for my calculations?
* How many threads should I use and what is the parallel speedup I should expect?

is much more difficult as there are several factors that should be taken into account.


!!! note

    To keep a long story short, one should use OpenMP threads
    when we start to trigger **limitations or bottlenecks in the MPI implementation**,
    especially at the level of the memory requirements or in terms of parallel scalability.
    These problems are usually observed in calculations with large [[natom]], [[mpw]], [[nband]].

    As a matter of fact, it does not make sense to compile ABINIT with OpenMP
    if your calculations are relatively small.
    Indeed, ABINIT is mainly designed with MPI-parallelism in mind.
    For instance, calculations done with a relatively large number of $\kk$-points will benefit more of MPI than OpenMP,
    especially if the number of MPI processes divides the number of $\kk$-points exactly.
    Even worse, do not compile the code with OpenMP support if you do not plan to use threads because the OpenMP
    version will have an **additional overhead** due to the creation of the threaded sections.

    Remember also that increasing the number of threads does not necessarily leads to faster calculations
    (the same is true for MPI processes).
    There's always an **optimal value** for the number of threads (MPI processes)
    beyond which the parallel efficiency starts to deteriorate.
    Unfortunately, this value is strongly hardware and software dependent so you will need to **benchmark the code**
    before running production calculations.

    Last but not least, OpenMP threads are not necessarily Posix threads. Hence if you have a library that provides
    both Open and Posix-threads, link with the OpenMP version.

After this necessary preamble, let's discuss how to compile a threaded version.
To activate OpenMP support in the Fortran routines of ABINIT, pass

```sh
enable_openmp="yes"
```

to the *configure* script via the configuration file.
This will automatically activate the compilation option needed to enable OpenMP in the ABINIT source code
(e..g. `-fopenmp` option for *gfortran*) and the CPP variable **HAVE_OPENMP in _config.h_**.
Note that this option is just part of the story as a significant fraction of the wall-time is spent in the external
BLAS/FFT routines so **do not expect big speedups if you do not link against threaded libraries**.

If you are building your own software stack for BLAS/LAPACK and FFT, you will have to
reconfigure with the correct options for the OpenMP version and then issue
*make and make install* again to build the threaded version.
Also note that some libraries may change.
FFTW3, for example, ships the OpenMP version in **_libfftw3_omp_**
(see the [official documentation](http://www.fftw.org/fftw3_doc/Installation-and-Supported-Hardware_002fSoftware.html#Installation-and-Supported-Hardware_002fSoftware)) hence the list of libraries in **FFTW3_LIBS** should be changed accordingly.

Life is much easier if you are using intel MKL because in this case
it is just a matter of selecting *OpenMP threading* as threading layer
in the [mkl-link-line-advisor interface](https://software.intel.com/en-us/articles/intel-mkl-link-line-advisor)
and then pass these options to the ABINIT build system together with `enable_openmp="yes"`.

!!! Important

    When using threaded libraries remember to set explicitly the number of threads with e.g.

    ```sh
    export OMP_NUM_THREADS=2
    ```

    either in your *bash_profile* or in the submission script (or in both).
    **By default, OpenMP uses all the available CPUs** so it is very easy to overload
    the machine, especially if one uses threads in conjunction with MPI processes.

    When running threaded applications with MPI, we suggest to allocate a number of **physical CPUs**
    that is equal to the number of MPI processes times the number of OpenMP threads.
    Computational intensive applications such as DFT codes have less chance to be improved in performance
    from Hyper-Threading technology (usually referred to as number of **logical CPUs**).

    We also recommend to increase the **stack size limit** using e.g.

    ```sh
    ulimit -s unlimited
    ```

    if the sysadmin allows you to do so.

    To run the ABINIT test suite with e.g. two OpenMP threads, use the `-o2` option of *runtests.py*
---
authors: MT, GG, JB
---

# Parallelism on images, the string method

## String method for the computation of minimum energy paths, in parallel.

This tutorial aims at showing how to use the parallelism on images, by performing a calculation of a minimum energy
path (MEP) using the string method. 

You will learn how to run the string method on a parallel architecture and
what are the main input variables that govern convergence and numerical
efficiency of the parallelism on *images*. Other algorithms use images, 
e.g. path-integral molecular dynamics, hyperdynamics, linear combination of images, ... with different values of [[imgmov]].
The parallelism on images can be used for all these algorithms.

You are supposed to know already some basics of parallelism in ABINIT, explained in the tutorial
[A first introduction to ABINIT in parallel](/tutorial/basepar), and  [ground state with plane waves](/tutorial/paral_gspw).

This tutorial should take about 1.5 hour and requires to have at least a 10 CPU
core parallel computer.

[TUTORIAL_README]

## 1 Summary of the String Method

The string method [[cite:Weinan2002]] is an algorithm that allows the computation of a Minimum
Energy Path (MEP) between an initial (_i_) and a final (_f_) configuration. It is
inspired from the Nudge Elastic Band (NEB) method. A chain of
configurations joining (_i_) to (_f_) is progressively driven to the MEP using an
iterative procedure in which each iteration consists of two steps:

1. **Evolution step**: the images are moved following the atomic forces.
2. **Reparametrization step**: the images are equally redistributed along the string.

The algorithm presently implemented in ABINIT is the so-called *simplified string method* [[cite:Weinan2007]].
It has been designed for the sampling of smooth energy landscapes.

*Before continuing you might work in a different subdirectory as for the other
tutorials. Why not work_paral_images?*

!!! important

    In what follows, the names of files are mentioned as if you were in this subdirectory.
    All the input files can be found in the `\$ABI_TESTS/tutoparal/Input` directory.
    You can compare your results with reference output files located in `\$ABI_TESTS/tutoparal/Refs`.

    In the following, when "run ABINIT over _nn_ CPU cores" appears, you have to use
    a specific command line according to the operating system and architecture of
    the computer you are using. This can be for instance: `mpirun -n nn abinit input.abi`
    or the use of a specific submission file.

!!! tip
    
    In this tutorial, most of the images and plots are easily obtained using the post-processing tool
    [qAgate](https://github.com/piti-diablotin/qAgate) or [agate](https://github.com/piti-diablotin/agate), 
    its core engine.
    Any post-process tool will work though !!

## 2 Computation of the initial and final configurations

We propose to compute the energy barrier for transferring a proton from an
hydronium ion (H<sub>3</sub>O<sup>+</sup>) onto a NH<sub>3</sub> molecule:

\begin{equation} \nonumber
\rm H_3O^+ + NH_3 \rightarrow H_2O + NH_4^+
\end{equation}

Starting from an hydronium ion and an ammoniac molecule, we obtain as final
state a water molecule and an ammonium ion NH<sub>4</sub><sup>+</sup>. In such a process, the MEP
and the barrier are dependent on the distance between the hydronium ion and
the NH<sub>3</sub> molecule. Thus we choose to fix the O atom of H<sub>3</sub>O<sup>+</sup> and the N atom of
NH<sub>3</sub> at a given distance from each other (4.0 Å = 7.5589 bohr). The calculation is performed
using a LDA exchange-correlation functional.

You can visualize the initial and final states of the reaction below (H atoms
are in white, the O atom is in red and the N atom in blue).

![initial state](paral_images_assets/Initial1.png)

![final state](paral_images_assets/Initial2.png)

!!! tip
    To obtain these images, open the `timages_01.abi` and `timages_02.abi` files with `agate` 
    or `qAgate`

Before using the string method, it is necessary to optimize the initial and
final points. The input files `timages_01.abi` and `timages_02.abi` contain
respectively two geometries close to the initial and final states of the
system. You have first to optimize properly these initial and final
configurations, using for instance the Broyden algorithm implemented in ABINIT.

{% dialog tests/tutoparal/Input/timages_01.abi tests/tutoparal/Input/timages_02.abi %}

Open the `timages_01.abi` file and look at it carefully. The unit cell is defined
at the begining. Note that the keywords [[natfix]] and [[iatfix]] are used to keep
fixed the positions of the O and N atoms. The cell is tetragonal and its size
is larger along x so that the periodic images of the system are separated by
4.0 Å (7.5589 bohr) of vacuum in the three directions. The keyword [[cellcharge]] is used to
remove an electron of the system and thus obtain a protonated molecule
(neutrality is recovered by adding a uniform compensating charge background).

Although this input file will run on a single core, the [[paral_kgb]] keyword is set to 1 
to activate the LOBPCG [[cite:Bottin2008]] algorithm.
Note the use of [[bandpp]] 10 to accelerate the convergence.

!!! important
    If your system were larger and required more CPU time, all the usual variables for parallelism
    [[npkpt]], [[npband]], [[npfft]], [[npspinor]] could be used wisely.

Then run the calculation in sequential, first for the initial
configuration (`timages_01.abi`), and then for the final one (`timages_02.abi`). You
should obtain the following positions:

1. for the initial configuration:

    ```
    xcart      0.0000000000E+00  0.0000000000E+00  0.0000000000E+00
              -7.1119330966E-01 -5.3954252784E-01  1.6461078895E+00
              -7.2706367116E-01  1.6395559231E+00 -5.3864186404E-01
               7.5589045315E+00  0.0000000000E+00  0.0000000000E+00
               8.2134747935E+00 -1.8873337293E-01 -1.8040045499E+00
               8.1621251369E+00 -1.4868515614E+00  1.0711027413E+00
               8.2046046694E+00  1.6513534511E+00  7.5956562273E-01
               1.9429112745E+00  4.2303909125E-02  2.9000318893E-02
    ```

2. for the final configuration:

    ```
    xcart      0.0000000000E+00  0.0000000000E+00  0.0000000000E+00
              -5.8991482730E-01 -3.6948430331E-01  1.7099330811E+00
              -6.3146217793E-01  1.6964706443E+00 -3.6340264832E-01
               7.5589045315E+00  0.0000000000E+00  0.0000000000E+00
               8.4775515860E+00 -2.9286989031E-01 -1.6949564152E+00
               7.9555913454E+00 -1.4851844626E+00  1.1974660442E+00
               8.2294855730E+00  1.6454040992E+00  8.0048724879E-01
               5.5879660323E+00  1.1438061815E-01 -2.5524007156E-01
    
    ```


## 3 Related keywords

Once you have properly optimized the initial and final states of the process,
you can turn to the computation of the MEP. 
Let us first have a look at the related keywords.

[[imgmov]]
:       Selects an algorithm using replicas of the unit cell.
        For the string method, choose 2.

[[nimage]]
:       gives the number of replicas of the unit cell including the initial and final ones. Note that when [[nimage]]>1, 
        the default value of several print input variables changes from one to zero. You might want to explicitly set [[prtgsr]] to 1
        to print the _GSR file, and get some visualization possibilities using Abipy.

[[dynimage]]([[nimage]])
:       arrays of flags specifying if the image evolves or not (0: does not evolve; 1: evolves).

[[ntimimage]]
:       gives the maximum number of iterations (for the relaxation of the string).

[[tolimg]]
:       convergence criterion (in Hartree) on the total energy (averaged over the [[nimage]] images).

[[fxcartfactor]]
:       "time step" (in Bohr<sup>2</sup>/Hartree) for the evolution step of
        the string method. For the time being (ABINITv6.10), only steepest-descent
        algorithm is implemented.

[[npimage]]
:       gives the number of processors among which the work load over
        the image level is shared. Only dynamical images are considered (images for
        which [[dynimage]] is 1). This input variable can be automatically set by
        ABINIT if the number of processors is large enough.

[[prtvolimg]]
:       governs the printing volume in the output file (0: full output; 1: intermediate; 2: minimum output).

## 4 Computation of the MEP without parallelism over images

You can now start with the string method.
First, for test purpose, we will not use the parallelism over images and will
thus only perform one step of string method.

{% dialog tests/tutoparal/Input/timages_03.abi %}

Open the `timages_03.abi` file and look at it. The initial and final
configurations are specified at the end through the keywords [[xcart]] and
[[nimage|xcart_lastimg]]. By default, ABINIT generates the intermediate
images by a linear interpolation between these two configurations. In this
first calculation, we will sample the MEP with 12 points (2 are fixed and
correspond to the initial and final states, 10 are evolving). [[nimage]] is
thus set to 12. The [[npimage]] variable is not used yet (no parallelism over
images) and [[ntimimage]] is set to 1 (only one time step).

Since the parallelism over the images is not used, this calculation still 
runs over 1 CPU core.

## 5 Computation of the MEP using parallelism over images

Now you can perform the complete computation of the MEP using the parallelism over the images.

{% dialog tests/tutoparal/Input/timages_04.abi %}

Open the `timages_04.abi` file. The keyword [[npimage]] has been added and set to 10, and
[[ntimimage]] has been increased to 50.
This calculation has thus to be run over 10 CPU cores.

The convergence of the string method algorithm is controlled by [[tolimg]],
which has been set to 0.0001 Ha. In order to obtain a more lisible output
file, you can decrease the printing volume and set [[prtvolimg]] to 2.
Then run ABINIT over 10 CPU cores.

When the calculation is completed, ABINIT provides you with 12 configurations
that sample the Minimum Energy Path between the initial (_i_) and final (_f_)
states. Plotting the total energy of these configurations with respect to a
reaction coordinate that join (_i_) to (_f_) gives you the energy barrier that
separates (_i_) from (_f_). In our case, a natural reaction coordinate can be the
distance between the hopping proton and the O atom of H<sub>2</sub>O (d<sub>OH</sub>), or
equivalently the distance between the proton and the N atom (d<sub>HN</sub>). The graph
below shows the total energy as a function of the OH distance along the MEP.
It indicates that the barrier for crossing from H<sub>2</sub>O to NH<sub>3</sub> is ~1.36 eV. The
6th image gives an approximate geometry of the transition state. Note that in
the initial state, the OH distance is significantly stretched, due to the
presence of the NH<sub>3</sub> molecule.

!!! tip
    Note that the total energy of each of the 12 replicas of the simulation cell
    can be found at the end of the output file in the section:
    
    ```
    -outvars: echo values of variables after computation  --------
    ```
    The total energies are printed out as: etotal_1img, etotal_2img, ...., etotal_12img.

!!! tip
    Also, you can can have a look at the atomic positions in each image: in
    cartesian coordinates (xcart_1img, xcart_2img, ...) or in reduced coordinates
    (xred_1img, xred_2img, ...) to compute the OH distance.


!!! tip
    You can issue the command `:plot xy x="distance 1 8 dunit=A" y="etotal eunit=eV"` in `agate` or `qAgate`
![courbe 1](paral_images_assets/curve1.png)

Total energy as a function of OH distance for the path computed with 12 images

and [[tolimg]]=0.0001 (which is very close to the x coordinate of the proton:
first coordinate of xcart  for the 8th atom in the output file).

The keyword [[npimage]] can be automatically set by ABINIT if [[autoparal]] is set to 1. 

Let us test this functionality. Edit again the `timages_04.abi` file and comment
the [[npimage]] line, then add [[autoparal]]=1. Then run the calculation again over 10 CPU cores.

Open the output file and look at the [[npimage]] value ...

## 6 Converging the MEP

Like all physical quantities, the MEP has to be converged with respect to some
numerical parameters. The two most important are the number of points along
the path ([[nimage]]) and the convergence criterion ([[tolimg]]).

1. [[nimage]]

    Increase the number of images to 22 (2 fixed + 20 evolving) and recompute the
    MEP. 
    Don't forget to update [[dynimage]] to `0 20*1 0` ! And you have 20 CPU cores 
    available then set [[npimage]] to 20 and run ABINIT over 20 CPU cores.
    The graph below superimposes the previous MEP (grey curve, calculated
    with 12 images) and the new one obtained by using 22 images (cyan curve). You
    can see that the global profile is almost not modified as well as the energy
    barrier.
    
    ![curve 2](paral_images_assets/curve2.png)
    
    Total energy as a function of OH distance for the path computed with 12 images
    and [[tolimg]]=0.0001 (grey curve) and the one computed with 22 images and
    [[tolimg]]=0.0001 (red curve).

    !!! tip
        The image can be obtained with `agate` or `qagate` with the following commands
            ```
            :open timages_04_MPI10o_HIST.nc # 12 images calculation
            :plot xy x="distance 1 8 dunit=A" y="etotal eunit=eV" hold=true
            :open timages_04_MPI10_22o_HIST.nc # 22 images calculation
            :plot xy x="distance 1 8 dunit=A" y="etotal eunit=eV"
            ```
        Replace `plot` with `print` to get the `gnuplot` script.
    
    The following animation is made by putting together the 22
    images obtained at the end of this calculation, from (_i_) to (_f_) and then from
    (_f_) to (_i_). It allows to visualize the MEP.

    !!! tip
        Open the `timages_04o_HIST.nc` file with `agate` or `qAgate` to produce this animation.
    
  <video id="video_string" controls autoplay loop style="width: 100%;">
  <source src="../paral_images_assets/stringvideo.mp4" type="video/mp4">
  <source src="../paral_images_assets/stringvideo.webm" type="video/webm">
  You browser does not support the video tag. Download the file [here](paral_images_assets/stringvideo.mp4).
  </video>
    
2. [[tolimg]]

    Come back to [[nimage]]=12. First you can increase [[tolimg]] to 0.001 and
    recompute the MEP. This will be much faster than in the previous case.
    
    Then you should decrease [[tolimg]] to 0.00001 and recompute the MEP. To gain
    CPU time, you can start your calculation by using the 12 images obtained at
    the end of the calculation that used [[tolimg]] = 0.0001. In your input file,
    these starting images will be specified by the keywords [[xcart]],
    [[nimage|xcart_2img]], [[nimage|xcart_3img]] ... [[nimage|xcart_12img]].
    You can copy them directly from the output file obtained at the previous
    section. The graph below superimposes the path obtained with 12 images and
    [[tolimg]]=0.001 (grey curve) and the one with 12 images and [[tolimg]]=0.0001 (cyan curve).
    
    ![image](paral_images_assets/curve3.png)
    
    Total energy as a function of OH distance for the path computed with 12 images
    and [[tolimg]]=0.0001 (cyan curve) and the one computed with 12 images and [[tolimg]]=0.001 (grey curve).
---
title: Basic parallelization in ABINIT
authors: YP, XG
---

# Tutorial on basic parallelism

## Parallelism in ABINIT, generalities and environments.

There are many situations where a sequential code is not enough, often because
it would take too much time to get a result. There are also cases where you
just want things to go as fast as your computational resources allow it.
By using more than one processor, you might also have access to more memory than
with only one processor.
To this end, it is possible to use ABINIT in
parallel, with dozens, hundreds or even thousands of processors.

This tutorial offers you a quick guided tour inside the complex world
that emerges as soon as you want to use more than one processor.
From now on, we will suppose that you are already familiar with ABINIT and that you have
gone through all four basic tutorials. If this is not the case, we strongly
advise you to do so, in order to truly benefit from this tutorial.

We strongly recommend you to acquaint yourself with some basic concepts of
[parallel computing](https://en.wikipedia.org/wiki/Parallel_computing) too.
In particular [Almdalh's law](https://en.wikipedia.org/wiki/Amdahl%27s_law),
that rationalizes the fact that, beyond some number of processors, the
inherently sequential parts will dominate parallel parts, and give a
limitation to the maximal speedup that can be achieved.

## Generalities

With the broad availability of multi-core processors, everybody now has a
parallel machine at hand. ABINIT will be able to take advantage of the
availability of several cores for most of its capabilities, be it ground-state
calculations, molecular dynamics, linear-response, many-body perturbation theory, ...

Such tightly integrated multi-core processors (or so-called SMP machines,
meaning Symmetric Multi-Processing) can be interlinked within networks, based
on Ethernet or other types of connections.
The number of cores in such composite machines can easily exceed one hundred, and
go up to several millions these days.
Most ABINIT capabilities can use efficiently several hundred computing cores.
In some cases, even more than ten thousand computing cores can be used efficiently.

Before actually starting this tutorial and the associated ones, we strongly
advise you to get familiar with your own parallel environment. It might be
relatively simple for a SMP machine, but more difficult for very powerful
machines. You will need at least to have MPI (see next section) installed on
your machine. Take some time to determine how you can launch a job in parallel
with MPI, what are the resources available and the limitations as well. 
Perhaps you will have to use a batch system 
(typically the `qsub` or `sbatch` command and an associated shell script).
Do not hesitate to
discuss with your system administrator if you feel that something is not clear to you.

We will suppose in the following that you know how to run a parallel program
and that you are familiar with the peculiarities of your system.
Please remember that, as there is no standard way of setting up a parallel
environment, we are not able to provide you with support beyond ABINIT itself.

## Characteristics of parallel environments

Different software solutions can be used to benefit from parallelism.
Most of ABINIT parallelism is based on MPI, but significant additional speedup (or a better
distribution of data, allowing to run bigger calculations) is based on OpenMP and multi-threaded libraries.
As of writing, efforts also focus on Graphical Processing Units (GPUs), with
CUDA and MAGMA. The latter will not be described in the present tutorial.

### MPI

MPI stands for Message Passing Interface. The goal of MPI, simply stated, is
to develop a widely used standard for writing message-passing programs.
As such the interface attempts to establish a practical, portable, efficient, and
flexible standard for message passing.

The main advantages of establishing a message-passing standard are portability
and ease of use. In a distributed memory communication environment in which
the higher-level routines and/or abstractions are build upon lower-level
message-passing routines, the benefits of standardization are particularly
obvious. Furthermore, the definition of a message-passing standard provides
vendors with a clearly defined base set of routines that they can implement
efficiently, or in some cases provide hardware support for, thereby enhancing
scalability (see <http://mpi-forum.org>).

At some point in its history MPI has reach a critical popularity level, and a
bunch of projects have popped-up like daisies in the grass. Now the tendency
is back to gathering and merging. For instance, Open MPI is a project
combining technologies and resources from several other projects
(FT-MPI, LA-MPI, LAM/MPI, and PACX-MPI) in order to build the best MPI library available.
Open MPI is a completely new MPI3.1-compliant implementation, offering
advantages for system and software vendors, application developers and
computer science researchers (see <https://www.open-mpi.org>)

### OpenMP

The OpenMP Application Program Interface (API) supports multi-platform
**shared-memory** parallel programming in C/C++ and Fortran on all
architectures, including Unix platforms and Windows NT platforms.
Jointly defined by a group of major computer hardware and software vendors, OpenMP is
a portable, scalable model that gives shared-memory parallel programmers a
simple and flexible interface for developing parallel applications for
platforms ranging from the desktop to the supercomputer (<http://www.openmp.org>).

OpenMP was rarely used within ABINIT versions < 8.8.x, and only for specific purposes.
Last versions > 8.8 now benefits from multi-threaded libraries speedup like MKL and fftw3.
Still not mandatory, on new architectures, multithreading shows better performances than MPI (*if and only if* an multithread version of linear
algebra library is provided)

### Scalapack

Scalapack is the parallel version of the popular LAPACK library (for linear
algebra). It can play some role in the parallelism of several parts of ABINIT,
especially the LOBPCG algorithm in ground state calculations, 
and the parallelism for the Bethe-Salpether equation. ScaLAPACK being itself based on MPI, we will not discuss
its use in ABINIT in this tutorial.

!!! warning

    Scalapack is not thread-safe in many versions.
    Combining OpenMP and Scalapack can result in unpredictable behaviours.

### Fast/slow communications

Characterizing the data-transfer efficiency between two computing cores (or
the whole set of cores) is a complex task. At a quite basic level, one has to
recognize that not only the quantity of data that can be transferred per unit
of time is important, but also the time that is needed to initialize such a
transfer (so called *latency*).

Broadly speaking, one can categorize computers following the speed of
communications. In the fast communication machines, the latency is very low
and the transfer time, once initialized, is very low too. For the parallelised
part of ABINIT, SMP machines and machines with fast interconnect 
will usually not be limited by their network characteristics, but
by the existence of residual sequential parts. The tutorials that have been
developed for ABINIT have been based on fast communication machines.

If the set of computing cores that you plan to use is not entirely linked
using a fast network, but includes some connections based e.g. on Ethernet,
then, you might not be able to benefit from the speed-up announced in the
tutorials. You have to perform some tests on your actual machine to gain
knowledge of it, and perhaps consider using multithreading.

## What parts of ABINIT are parallel?

Parallelizing a code is a very delicate and complicated task, thus do not
expect that things will systematically go faster just because you are using 
more processors. Please keep also in mind that in some situations,
parallelization is simply impossible. At the present time, the parts of ABINIT
that have been parallelized, and for which a tutorial is available, include:

* [ground state with plane waves](/tutorial/paral_gspw),
* [ground state with wavelets](/tutorial/paral_gswvl),
* [molecular dynamics](/tutorial/paral_moldyn),
* [parallelism on "images"](/tutorial/paral_images),
* [density-functional perturbation theory (DFPT)](/tutorial/paral_dfpt),
* [Many-Body Perturbation Theory](/tutorial/paral_mbt).

Note that the tutorial on [ground state with plane waves](/tutorial/paral_gspw) presents a complete overview of the
parallelism for the ground state, including up to four levels of parallelisation and, as such, is rather complex.
Of course, it is also quite powerful, and allows to use several hundreds of processors.

Albeit, the two levels based on

  * the treatment of k-points in reciprocal space;
  * the treatment of spins, for spin-polarized collinear situations [[nsppol]] = 2);

are, on the contrary, quite easy to use. Examples of such parallelism will
be given in the next sections.

## A simple example of parallelism in ABINIT

[TUTORIAL_README]

### Running a job

*Before starting, you might consider working in a different subdirectory as
for the other tutorials. Why not Work_paral?*

First one needs to copy the input file from the *\$ABI_TESTS/tutorial*
directory to your work directory, namely *tbasepar_1.abi*.

```sh
cd $ABI_TESTS/tutorial/Input
mkdir Work_paral
cd Work_paral
cp ../tbasepar_1.abi .
```

You can start immediately a sequential run with

    abinit tbasepar_1.abi >& log 2> err &

to have a reference CPU time.
On a Intel Xeon 20C 2.1 GHz, it runs in about 40 seconds.

The input file (*.abi) might possibly be modified for parallel execution, as one should avoid
unnecessary network communications. Indeed, if every node has its own temporary or
scratch directory (so not in the multicore case), you can achieve this by providing a path to a local disk
for the temporary files in the input file by using the [[tmpdata_prefix]] variable. Supposing each processor has access
to a local temporary disk space named `/scratch/user`, then you might add to the input *.abi file the following line

	tmpdata_prefix="/scratch/user/tbasepar_1"


Note that determining ahead of time the precise resources you will need for
your run will save you a lot of time if you are using a batch queue system.

Also, for parallel runs, note that the _log_ files **will not** be written exept the main log file.
You can change this behaviour by creating a file named `_LOG` to enforce the creation of all log files

```bash
touch _LOG
```

On the contrary, you can create a *_NOLOG* file if you want to avoid all log files.

### Parallelism over the k-points

The most favorable case for a parallel run is to treat the k-points
concurrently, since most calculations can be done independently for each one of them.

Actually, *tbasepar_1.abi* corresponds to the investigation of a *FCC* crystal of
lead, which requires a large number of k-points if one wants to get an
accurate description of the ground state. Examine this file. Note that the
cut-off is realistic, as well as the grid of k-points (giving 182 k points in
the irreducible Brillouin zone). 
Once done, your output files for the sequential run, launched while starting to read this section, have likely been produced.
Examine the timing in the output file (the last line gives the `Overall time`, `cpu` and `wall`), and keep note of it.

We assume you have compiled ABINIT indicating  `with_mpi="yes"` at configuration step.

On a multi-core PC, you might succeed to use two compute cores by issuing the run command for your MPI
implementation, and mention the number of processors you want to use, as well
as the abinit command:

```bash
mpirun -n 2 abinit tbasepar_1.abi >& tbasepar_1.log &
```

Depending on your particular machine, *mpirun* might have to be replaced by
*mpiexec*, and `-n` by some other option.

At variance, on a cluster, with the MPICH implementation of MPI, you have to set up a file
with the addresses of the different CPUs. Let's suppose you call it _cluster_.
For a PC bi-processor machine, this file could have only one line, like the following:

    sleepy.pcpm.ucl.ac.be:2

For a cluster of four machines, you might have something like:

    tux0
    tux1
    tux2
    tux3

Then, you have to issue the run command for your MPI implementation, and
mention the number of processors you want to use, as well as the abinit
command and the file containing the CPU addresses.

On a PC bi-processor machine, this gives the following:

```bash
mpirun -np 2 -machinefile cluster ../../src/main/abinit tbasepar_1.abi >& tbasepar_1.log &
```

Now, examine the corresponding output file. If you have kept the output from
the sequential job, you can make a diff between the two files. You will notice
that the numerical results are quite identical. You will also see that 182
k-points have been kept in the memory in the sequential case (keyword `mkmem`), while 91
k-points have been kept in the memory (per processor !) in the parallel case.

The timing can be found at the end of the file. Here is an example:

    - Proc.   0 individual time (sec): cpu=         20.0  wall=         20.1

    ================================================================================

     Calculation completed.
     Delivered    0 WARNINGs and   1 COMMENTs to log file.
    +Overall time at end (sec) : cpu=         40.1  wall=         40.3


This corresponds effectively to a speed-up of the job by a factor of two.
Let's examine it. The line beginning with `Proc. 0` corresponds to the CPU and
Wall clock timing seen by the processor number 0 (processor indexing always
starts at 0: here the other is number 1): 20.0 sec of CPU time, and nearly the same
amount of Wall clock time. The line that starts with `+Overall time`
corresponds to the sum of CPU times and Wall clock timing for all processors.
The summation is quite meaningful for the CPU time, but not so for the wall
clock time: the job was finished after 20.1 sec, and not 40.3 sec.

Now, you might try to increase the number of processors, and see whether the
CPU time is shared equally amongst the different processors, so that the Wall
clock time seen by each processor decreases. At some point (depending on your
machine, and the sequential part of ABINIT), you will not be able to decrease
further the Wall clock time seen by one processor. It is not worth to try to
use more processors. Let us define the speedup
as the time taken in a
sequential calculation divided by the time for your parallel calculation (hopefully > 1) .
 You should get a curve similar to this one:

![Speedup kpt](basepar_assets/basepar_speedup.png "Speedup for k-pt parallelisation")
_Speedup with k point parallelization_

The red curve materializes the speedup achieved, while the green one is the
$y = x$ line. The shape of the red curve will vary depending on your hardware
configuration. 

One last remark: the number of k-points need not be a multiple of the number
of processors. As an example, you might try to run the above case with 16
processors: all will treat $\lfloor 182/16 \rfloor=11$ k points, but $182-16\times11=6$ processors
will have to treat one more k point so that $6*12+10*11=182$.
The maximal speedup will only be $15.2 (=182/12)$, instead of 16.

Try to avoid leaving an empty processor as this can make abinit fail with
certain compilers. An empty processor happens, for example, if you use more processors
than the number of k point.
The extra processors do no useful work, but have to run anyway, just to confirm to abinit
once in a while that all processors are alive.

### Parallelism over the spins

The parallelization over the spins (up, down) is done along with the one over
the k-points, so it works exactly the same way. The file
*tbasepar_2.abi* in *\$ABI_TESTS/tutorial* treats a spin-polarized system
(distorted FCC Iron) with only one k-point in the Irreducible Brillouin Zone.
This is quite unphysical, and has the sole purpose to show the spin
parallelism with as few as two processors: the k-point parallelism has
precedence over the spin parallelism, so that with 2 processors, one ought
to have only one k-point to see the spin parallelism.
If needed, modify the input file, to provide a local temporary disk space.
Run this test case, in sequential, then in parallel.

While the jobs are running, read the input. Then look closely
at the output and log files in the sequential and parallel cases. They are quite similar. 
Actually, apart the mention of two processors and the speedup, there is no other
manifestation of the parallelism.

If you have more than 2 processors at hand, you might increase the value of
[[ngkpt]], so that more than one k-point is available, and see that the
k-point and spin parallelism indeed work concurrently.

### Number of computing cores to accomplish a task

Balancing efficiently the load on the processors is not always
straightforward. When using k-point- and spin-parallelism, the ideal numbers
of processors to use are those that divide the product of [[nsppol]] by
[[nkpt]] (e.g. for [[nsppol]] * [[nkpt]] = 12, it is quite efficient to use 2, 3, 4,
6 or 12 processors). ABINIT will nevertheless handle correctly other numbers
of processors, albeit slightly less efficiently, as the final time will be
determined by the processor that will have the biggest share of the work to do.

### Evidencing overhead

Beyond a certain number of processors, the efficiency of parallelism
saturates, and may even decrease. This is due to the inevitable overhead
resulting from the increasing amount of communication between the processors.
The loss of efficiency is highly dependent on the implementation and linked to
the decreasing charge on each processor too.

<!--
## Details of the implementation

### The MPI toolbox in ABINIT

The ABINIT-specific MPI routines are located in different subdirectories of
`~abinit/src` : `12_hide_mpi/`, `51_manage_mpi/`, `56_io_mpi/`, `79_seqpar_mpi/`. They include:

  * low-level communication handlers
  * header I/O helpers (hdr_io, hdr_io_netcdf);
  * wavefunction I/O helpers (Wff*);
  * a multiprocess-aware output routine (wrtout);
  * a clean exit routine (leave_new).

They are used by a wide range of routines.

You might want to have a look at the routine headers for more detailed descriptions.

### How to parallelize a routine: some hints

Here we will give you some advice on how to parallelize a subroutine of
ABINIT. Do not expect too much, and remember that you remain mostly on your
own for most decisions. Furthermore, we will suppose that you are familiar
with ABINIT internals and source code. Anyway, you can skip this section
without hesitation, as it is primarily intended for advanced developers.

First, every call to a MPI routine and every purely parallel section of your
subroutine **must** be surrounded by the following preprocessing directives
if you don't use functions provided by the `m_xmpi` module.

```fortran
#if defined HAVE_MPI
...
#endif
```

Usually, the function you will write will have as an argument a communicator.
You can then retrieve the number of processors in this communicator with `xmpi_comm_size(comm)`,
the rank of a processor with `xmpi_comm_rank(comm)`.
Example of a function

```fortran
subroutine dosomethin(arg,comm)
  !define arguments
  integer, intent(in) :: arg
  integer, intent(in) :: com,
  !define local arguments
  integer :: rank
  integer :: size
  integer, parameter :: master = 0 ! proc 0 will be our master

  !retrieve size and rank
  size = xmpi_comm_size(comm)
  rank = xmpi_comm_rank(comm)

  !do things
  call xmpi_sum(...)

  !Use not wrapped-functions
#if defined HAVE_MPI
  call mpi_comm_split(com,....)
#endif

  !Master do something
  if ( rank == master ) then
    !do things
  endif

  call wrtout(std_out,"Only master will write",'COLL')
  call wrtout(std_out,"Each proc will write in its own std_out",'PERS')
```
-->
---
authors: MT
---

# Second tutorial on the Projector Augmented-Wave (PAW) technique

## Generation of PAW atomic datasets

This tutorial aims at showing how to create your own atomic datasets for the Projector Augmented-Wave (PAW) method.

You will learn how to generate these atomic datasets and how to control their softness and transferability.
You already should know how to use ABINIT in the PAW case (see the tutorial [PAW1](/tutorial/paw1) ).

This tutorial should take about 2h00.

[TUTORIAL_README]

## 1. The PAW atomic dataset - introduction

The PAW method is based on the definition of spherical augmentation
regions of radius $r_c$ around the atoms of the system in which a basis of
atomic **partial-waves** $\phi_i$, of **pseudized partial-waves** $\tphi_i$, and of **projectors** $\tprj_i$
(dual to $\tphi_i$) have to be defined. This set of _partial-waves_ and _projectors_
functions (and some additional atomic data) are stored in a so-called **PAW dataset**.
A PAW dataset has to be generated for each atomic species in order
to reproduce atomic behavior as accurate as possible while requiring minimal
CPU and memory resources in executing ABINIT for the crystal simulations.
These two constraints are obviously conflicting.

The PAW dataset generation is the purpose of this tutorial.
It is done according the following procedure (all parameters that define a PAW dataset are in bold):

  1. Choose and define the concerned chemical species (**name** and **atomic number**).

  2. Solve the atomic all-electrons problem in a given atomic configuration.
  The atomic problem is solved within the DFT formalism, using an **exchange-correlation functional**
  and either a **Schrodinger** (default) or **scalar-relativistic** approximation.
  This _spherical problem_ is solved on a **radial grid**.
  The atomic problem is solved for a given **electronic configuration** that can be an ionized/excited one.

  3. Choose a set of electrons that will be considered as frozen around the nucleus (**core electrons**).
  The others electrons are **valence** ones and will be used in the PAW basis.
  The **core density** is then deduced from the core electrons wave functions.
  A **smooth core density** equal to the core density outside a given $r_{core}$ matching radius is computed.

  4. Choose the size of the PAW basis (**number of partial-waves and projectors**).
  Then choose the partial-waves included in the basis. The later can be atomic eigen-functions
  related to valence electrons (**bound states**) and/or **additional atomic functions**, solution
  of the wave equation for a given $l$ quantum number at arbitrary **reference energies** (unbound states).

  5. Generate pseudo partial-waves (smooth partial-waves build with a **pseudization scheme** and
  equal to partial-waves outside a given $r_c$ matching radius) and associated projector functions.
  Pseudo partial-waves are solutions of the PAW Hamiltonian deduced from the atomic Hamiltonian
  by pseudizing the effective potential (a **local pseudopotential** is built and equal to effective
  potential outside a $r_{Vloc} matching radius). Projectors and partial-waves are then
  orthogonalized with a chosen **orthogonalization scheme**.

  6. Build a **compensation charge density** used later in order to retrieve the total charge of the atom.
  This compensation charge density is located inside the PAW spheres and based on an **analytical shape function**
  (which analytic form and localization radius $r_{shape}$ can be chosen).

The user can choose between two PAW dataset generators to produce atomic files directly readable by ABINIT.
The first one is the PAW generator [[http://users.wfu.edu/natalie/papers/pwpaw/man.html|ATOMPAW]] (originally by _N. Holzwarth_) and
the second one is the [[http://www.physics.rutgers.edu/~dhv/uspp| Ultra-Soft USPP]] generator (originally written by _D. Vanderbilt_). In this tutorial, we concentrate only on `ATOMPAW`.

It is highly recommended to refer to the following papers to understand
correctly the generation of PAW atomic datasets:

1. "Projector augmented-wave method" - [[cite:Bloechl1994]]

2. "A projector Augmented Wave (PAW) code for electronic structure" - [[cite:Holzwarth2001]]

3. "From ultrasoft pseudopotentials to the projector augmented-wave method" - [[cite:Kresse1999]]

4. "Electronic structure packages: two implementations of the Projector Augmented-Wave (PAW) formalism" - [[cite:Torrent2010]]

5. "Notes for revised form of atompaw code" (by N. Holzwarth) - [[http://www.wfu.edu/%7Enatalie/papers/pwpaw/notes/atompaw/atompawEqns.pdf|PDF]]

## 2. Use of the generation code

*Before continuing, you might consider to work in a different subdirectory as
for the other tutorials. Why not `Work_paw2`?*

```sh
cd $ABI_TESTS/tutorial/Input
mkdir Work_paw2
cd Work_paw2
```

You have now to install the `ATOMPAW` code. In your internet browser, enter the following URL:

     https://users.wfu.edu/natalie/papers/pwpaw/

Then, download the last version of the `tar.gz` file, unzip and untar it.
Enter the `atompaw-4.x.y.z` and execute:

```sh
    mkdir build
    cd build
    ../configure
    make
```

if all goes well, you get the `ATOMPAW` executable at 
<span style="color:green">atompaw-4.x.y.z/build/src/atompaw</span>.  
If not, Go into the directory <span style="color:green">doc</span>, open the file <span style="color:green">atompaw-usersguide.pdf</span>, go p.3 and follow the instructions.
    
!!! Note
    On MacOS, you can use [[https://brew.sh|homebrew]] package manager and install `ATOMPAW` by typing:
    
```sh
        brew install atompaw/repo/atompaw
```

!!! Note
    In the following, we name *atompaw* the `ATOMPAW` executable.

**How to use `ATOMPAW`?**

The following process will be applied to **Nickel** in the next paragraph:

1. Edit an input file in a text editor (content of input explained [here](paw2_assets/atompaw-usersguide.pdf))
2. Run: *atompaw < inputfile*

Partial waves $\phi_i$, PS partial-waves $\tphi_i$ and projectors $\tprj_i$ are given in `wfn.i` files.
Logarithmic derivatives from atomic Hamiltonian and PAW Hamiltonian
resolutions are given in `logderiv.l` files.
A summary of the atomic _all-electron_ computation and PAW dataset properties
can be found in the `Atom_name` file (`Atom_name` is the first parameter of the input file).

Resulting PAW dataset is contained in:

- `Atom_name.XCfunc.xml` file  
  Normalized XML file according to the
    [PAW- XML specifications](https://esl.cecam.org/Paw-xml) (recommended).

- `Atom_name.XCfunc-paw.abinit` file  
  Proprietary legacy format for ABINIT


## 3. First (and basic) PAW dataset for Nickel

Our test case will be nickel; electronic configuration: $[1s^2 2s^2 2p^6 3s^2 3p^6 3d^8 4s^2 4p^0]$.

In a first stage, copy a simple input file for `ATOMPAW` in your working directory
(find it in <span style="color:green">*\$ABI_HOME/doc/tutorial/paw2/paw2_assets/Ni.atompaw.input1*</span>).
Edit this file.

{% dialog tutorial/paw2_assets/Ni.atompaw.input1 %}

This file has been built in the following way:

1. All-electron calculation parameters:
    * 1st line: define the <span style="color:green">material</span>.
```
    Ni 28
```
    * 2nd line: choose the <span style="color:green">exchange-correlation functional</span> (LDA-PW or GGA-PBE)
      and select a scalar-relativistic <span style="color:green">wave equation</span>
      (nonrelativistic or scalarrelativistic)
      and a (2000 points) logarithmic <span style="color:green">grid</span>.
```
    GGA-PBE scalarrelativistic loggrid 2000
```
2. Electronic configuration:
   _How many electronic states do we need to include in the computation?
   Besides the fully and partially occupied states, it is recommended to add all
   states that could be reached by electrons in the solid. Here, for Nickel, the
   $4p$ state is concerned. So we decide to add it in the computation._
    * 3rd line: define the <span style="color:green">electronic configuration</span>.
      A line with the maximum $n$ quantum number for each electronic shell; here `4 4 3` means `4s, 4p, 3d`.
```
    4 4 3 0 0 0
```
   * Following lines : definition of <span style="color:green">occupation numbers</span>.
     For each partially occupied shell enter the occupation number.
     _An excited configuration may be useful if the PAW dataset is intended for use in a
     context where the material is charged (such as oxides). Although, in our
     experience, the results are not highly dependent on the chosen electronic configuration._
     We choose here the $[3d^8 4s^2 4p^0]$ configuration.
     Only $3d$ and $4p$ shells are partially occupied (`3 2 8` and `4 1 0` lines).
     A `0 0 0` ends the occupation section.
```
    3 2 8
    4 1 0
    0 0 0
```

3. Selection of <span style="color:green">core and valence electrons</span>.
   _In a first approach, select only electrons from outer shells as valence. But, if particular
    thermodynamical conditions are to be simulated, it is generally needed to
    include "semi-core states" in the set of valence electrons. Semi-core states
    are generally needed with transition metal and rare-earth materials._
    Note that all wave functions designated as valence electrons will be used in the partial-wave basis.
    Core shells are designated by a $c$ and valence shells by a $v$.
    All $s$ states first, then $p$ states and finally $d$ states.
    Here:
```
    c
    c
    c
    v
    c
    c
    v
    v
```
means:
```
    1s core
    2s core
    3s core
    4s valence
    2p core
    3p core
    4p valence
    3d valence
```
4. Partial-waves basis generation:
    * A line with $l_{max}$ the <span style="color:green">maximum $l$</span> for
      the partial-waves basis. Here $l_{max}=2$.
```
    2
```
   * A line with the <span style="color:green">$r_{PAW}$</span> radius.  
     _Select it to be slightly less than half the inter-atomic distance
     in the solid (as a first choice). Here $r_{PAW}=2.3\ a.u$._
```
    2.3
```
   * Next lines: add <span style="color:green">additional _partial-waves_</span> $\phi_i$ if needed.
      Choose to have 2 partial-waves per angular momentum in the basis (this choice is not
      necessarily optimal but this is the most common one; if $r_{PAW}$ is small enough,
      1 partial-wave per $l$ may suffice).
      As a first guess, put
      all <span style="color:green">reference energies</span>
      for additional partial-waves to 0 Rydberg.
      For each angular momentum, first add "y" to add an additional partial-wave.
      Then, next line, put the value in Rydberg units.
      Repeat this for each new partial-wave and finally put "n".
      _Note : For each angular momentum, valence states already are included in
      the partial-waves basis. Here $4s$, $4p$ and $3d$ states already are in the basis._
      In the present file:
```
    y
    0.5
    n
```
means that an additional $s$\- partial-wave at $E_{ref}=0.5$ Ry as been added,
```
    y
    1.
    n
```
means that an additional $p$\- partial-wave at $E_{ref}=1.$ Ry has been added,
```
    y
    1.
    n
```
means that an additional $d$- partial-wave at $E_{ref}=1.$ Ry as been added.
Finally, partial-waves basis contains two $s$\-, two $p$\-  and two $d$\- partial-waves.
      * Next line: definition of the <span style="color:green">generation scheme for pseudo partial
        waves $\tphi_i$</span>, and of projectors $\tprj_i$.
        _We begin here with a simple scheme (i.e. "Bloechl" scheme, proposed by P. Blochl [[cite:Bloechl1994]]).
        This will probably be changed later to make the PAW dataset more efficient._
```
    bloechl
```
      * Next line: <span style="color:green">generation scheme for local pseudopotential
        $V_{loc}$</span>. In order to get PS partial-waves, the atomic potential has to be "pseudized"
        using an arbitrary pseudization scheme.
        *We choose here a "Troullier-Martins" using a wave equation at $l_{loc}=3$ and $E_{loc}=0.$ Ry.
        As a first draft, it is always recommended to put $l_{loc}=1+l_{max}$.*
```
    3 0. troulliermartins
```
      * Next two lines: `XMLOUT` makes `ATOMPAW` generate a PAW dataset in XML format;
        The next line contains options for this ABINIT file. "default" set all parameters to their default value.
```
    XMLOUT
    default

```
      * The `END` keyword ends the file.
```
    END
```

At this stage, run `ATOMPAW`. For this purpose, simply enter:

    atompaw < Ni.atompaw.input1

Lot of files are produced. We will examine some of them.
A summary of the PAW dataset generation process has been written in a file
named `Ni`.  
Open it. It should look like:

	 Completed calculations for Ni
	 Exchange-correlation type: GGA, Perdew-Burke-Ernzerhof
	  Radial integration grid is logarithmic 
	r0 =   2.2810899E-04 h =   6.3870518E-03   n =      2000 rmax =   8.0000000E+01
	 Scalar relativistic calculation
	   AEatom converged in          32  iterations
		 for nz =  28.00
		 delta  =    9.5504321957145377E-017
	  All Electron Orbital energies:         
	  n  l     occupancy       energy
	 1  0      2.0000000E+00 -6.0358607E+02
	 2  0      2.0000000E+00 -7.2163318E+01
	 3  0      2.0000000E+00 -8.1627107E+00
	 4  0      2.0000000E+00 -4.1475541E-01
	 2  1      6.0000000E+00 -6.2083048E+01
	 3  1      6.0000000E+00 -5.2469208E+00
	 4  1      0.0000000E+00 -9.0035739E-02
	 3  2      8.0000000E+00 -6.5223644E-01

	  Total energy
		 Total                    :    -3041.0743834110435     
	 Completed calculations for Ni
	 Exchange-correlation type: GGA, Perdew-Burke-Ernzerhof
	  Radial integration grid is logarithmic 
	r0 =   2.2810899E-04 h =   6.3870518E-03   n =      2000 rmax =   8.0000000E+01
	 Scalar relativistic calculation
	   SCatom converged in           1  iterations
		 for nz =  28.00
		 delta  =    8.7786021384577619E-017
	   Valence Electron Orbital energies:         
	  n  l     occupancy       energy
	 4  0      2.0000000E+00 -4.1475541E-01
	 4  1      0.0000000E+00 -9.0035739E-02
	 3  2      8.0000000E+00 -6.5223644E-01

	  Total energy
		 Total                    :    -3041.0743834029249     
		 Valence                  :    -185.18230020196870     
	  paw parameters: 
		   lmax =            2
			 rc =    2.3096984974114871     
			irc =         1445
	 Vloc: Norm-conserving Troullier-Martins with l= 3;e=   0.0000E+00
	 Projector type: Bloechl + Gram-Schmidt ortho.
	 Sinc^2 compensation charge shape zeroed at rc

	Number of basis functions     6
	 No.   n    l      Energy         Cp coeff         Occ
		1    4    0 -4.1475541E-01 -9.5091487E+00  2.0000000E+00
		2  999    0  5.0000000E-01  3.2926948E+00  0.0000000E+00
		3    4    1 -9.0035739E-02 -8.9594191E+00  0.0000000E+00
		4  999    1  1.0000000E+00  1.0610645E+01  0.0000000E+00
		5    3    2 -6.5223644E-01  9.1576184E+00  8.0000000E+00
		6  999    2  0.0000000E+00  1.3369076E+01  0.0000000E+00
	 Completed diagonalization of ovlp with info =        0
  
	 Eigenvalues of overlap operator (in the basis of projectors):
		1        7.27257365E-03
		2        2.25491432E-02
		3        1.25237568E+00
		4        1.87485118E+00
		5        1.05720648E+01
		6        2.00807906E+01
  
	  Summary of PAW energies
			Total valence energy       -185.18230536120760     
			  Smooth energy             11.667559552612433     
			  One center               -196.84986491382003     
			  Smooth kinetic            15.154868503980399     
			  Vloc energy              -2.8094614733964467     
			  Smooth exch-corr         -3.3767012052640886     
			  One-center xc            -123.07769380742522     

The generated PAW dataset (contained in <span style="color:green">Ni.GGA-PBE.xml</span> file) is a first draft.
Several parameters have to be adjusted, in order to get accurate results and efficient DFT calculations.

!!! Note
    The <span style="color:green">Ni.GGA-PBE.xml</span> file is directly usable by ABINIT.

## 4. Checking the sensitivity to some parameters

### 4.a. The radial grid

Let's try to select **700 points** in the logarithmic grid and check if any noticeable
difference in the results appears.
You just have to replace `2000` by `700` in the second line of  <span style="color:green">Ni.atompaw.input1</span> file.  
Then run:

    atompaw < Ni.atompaw.input1

again and look at the `Ni` file:

	   Summary of PAW energies
			 Total valence energy       -185.18230027710337     
			   Smooth energy             11.634042318422193     
			   One center               -196.81634259552555     
			   Smooth kinetic            15.117782033814152     
			   Vloc energy              -2.8024659321861067     
			   Smooth exch-corr         -3.3712015132649102     
			   One-center xc            -123.08319475096027     

As you see, results obtained with this new grid are very close to previous ones, expecially the `valence energy`.
We can keep the **700 points** grid.

!!! Note
	We could try to decrease the size of the grid.
	Small grids give PAW dataset with small size (in kB) and run faster in ABINIT,
	but accuracy can be affected.

!!! Note
    The final $r_{PAW}$ value (`rc = ...` in `Ni` file) changes with the
    grid; just because $r_{PAW}$ is adjusted in order to belong exactly to the radial grid.
    By looking in ATOMPAW [user's guide](paw2_assets/atompaw-usersguide.pdf), you can choose to keep it constant.

### 4.b. The relativistic approximation of the wave equation

The _scalar-relativistic_ option should give better results than _non-relativistic_ one,
but it sometimes produces difficulties for the convergence of the atomic problem
(either at the all-electron resolution step or at the PAW Hamiltonian solution step).
If convergence cannot be reached, try a non-relativistic calculation (not recommended for high Z materials).

!!! Note
    For the following, note that you always should check the `Ni` file, especially
    the values of `valence energy`. You can find the valence energy
    computed for the exact atomic problem and the valence energy computed with the
    PAW parameters. These two results should be in close agreement!

## 5. Adjusting partial-waves and projectors

Examine the AE partial-waves $\phi_i$, PS partial-waves $\tphi_i$ and projectors $\tprj_i$.
These are saved in files named `wfni`, where `i` ranges over the number of partial-waves
used, so 6 in the present example.
Each file contains 4 columns: the radius $r$ in column 1,
the AE partial-wave $\phi_i(r)$ in column 2, the PS partial-wave $\tphi_i(r)$ in
column 3, and the projector $\tprj_i(r)$ in column 4.  
Plot the 3 curves as a function of radius using a plotting tool of your choice.

Below the first $s$\- partial-wave /projector of the Ni example:

![First s partial-wave](paw2_assets/wfn1a.jpg)

  * $\phi_i$ should meet $\tphi_i$ near or after the last maximum (or minimum).
    If not, it is preferable to change the value of the matching (pseudization) radius $r_c$.

  * The maxima of $\tphi_i$ and $\tprj_i$ functions should roughly have the same order of magnitude.
    If not, you can try to get this in three ways:
    1. **Change the matching radius** for this partial-wave; but this is not always
       possible (PAW spheres should not overlap in the solid).
    2. **Change the pseudopotential scheme** (see later).
    3. If there are two (or more) partial-waves for the angular momentum $l$ under
       consideration, decreasing the magnitude of the projector is possible by
       **displacing the references energies**. Moving the energies away from each other
       generally reduces the magnitude of the projectors, but too big a difference
       between energies can lead to wrong logarithmic derivatives (see following section).

Example: plot the `wfn6` file, related to the second $d$- partial-wave:

![2nd d partial-wave](paw2_assets/wfn6a.jpg)

This partial-wave has been generated at $E_{ref}=0$ Ry and orthogonalized with the
first $d$\- partial-wave which has an eigenenergy equal to $-0.65$ Ry (see `Ni` file).
These two energies are too close and orthogonalization process produces "high" partial-waves.
Try to replace the reference energy for the additional $d$\- partial-wave.
For example, put $E_{ref}=1.$ Ry instead of $E_{ref}=0.$ Ry (line 24 of `Ni.atompaw.input1` file).

Run `ATOMPAW` again and plot `wfn6` file:

![2nd d partial-wave](paw2_assets/wfn6b.jpg)

Now the PS partial-wave and projector have the same order of magnitude!

!!! Important
    Note again that you should always check the two `Valence energy` values in `Ni` file and make
    sure they are as close as possible.
    If not, choices for projectors and/or partial-waves are certainly not judicious.

## 6. Examine the logarithmic derivatives

Examine the logarithmic derivatives, i.e., derivatives of an $l$-state
$\frac{d(log(\Psi_l(E))}{dE}$ computed for the exact atomic problem and with the PAW dataset.
They are printed in the `logderiv.l` files. Each `logderiv.l` file corresponds to an
angular momentum $l$  and contains five columns of data: the
energy, the logarithmic derivative of the $l$-state of the exact atomic problem,
 the logarithmic derivative of the pseudized problem (and two other colums not relevant for this section). In the following, when you edit a `logderiv` file, only edit the three first columns.  
In our `Ni` example, $l=0$, $1$ or $2$.

The logarithmic derivatives should have the following properties:

  * The 2 curves should be superimposed as much as possible.
    By construction, they are superimposed at the 2 energies corresponding to the 2 $l$ partial-waves.
    If the superimposition is not good enough, the reference energy for the second $l$ partial-wave should be changed.

  * Generally a discontinuity in the logarithmic derivative curve appears at $0$ Ry $\le E_0\le 4$ Ry.
    A reasonable choice is to choose the 2 reference energies so that $E_0$ is in between.

  * Too close reference energies produce "hard" projector functions
    (see section 5). But moving reference energies away
    from each other can damage accuracy of logarithmic derivatives

Here are the three logarithmic derivative curves for the current dataset:

![l=0 log derivatives](paw2_assets/log0a.jpg)

![l=1 logderivatives](paw2_assets/log1a.jpg)

![l=2 logderivatives](paw2_assets/log2a.jpg)

As you can see, except for $l=2$, exact and PAW logarithmic derivatives do not match!  
According to the previous remarks, try other values for the references
energies of the $s$\- and $p$\- additional partial-waves.  
First, edit again the  <span style="color:green">Ni.atompaw.input1</span> file and put $E_{ref}=3$ Ry for the
additional $s$\- state (line 18); run `ATOMPAW` again. Plot the `logderiv.0` file.
You should get:

![l=0 log derivatives](paw2_assets/log0b.jpg)

Then put $E_{ref}=4$ Ry for the second $p$\- state (line 21); run `ATOMPAW` again.
Plot again the `logderiv.1` file.  
You should get:

![l=1 log derivatives](paw2_assets/log1b.jpg)

Now, all PAW logarithmic derivatives match with the exact ones in a reasonable interval.

!!! Note
    It is possible to change the interval of energies used to plot logarithmic
    derivatives (default is $[-5;5]$) and also to compute them at more points
    (default is $200$). Just add the following keywords at the end of the SECOND
    LINE of the input file if you want `ATOMPAW` to output logarithmic derivatives
    for energies in [-10;10] at 500 points:
    ````
        logderivrange -10 10 500
    ````

**Additional information related to logarithmic derivatives: ghost states**

Another possible issue could be the presence of a discontinuity in the PAW
logarithmic derivative curve at an energy where the exact logarithmic derivative is continuous.
This generally shows the presence of a _ghost state_.

  * First, try to change to value of reference energies; this sometimes can make the ghost state disappear.
  * If not, it can be useful to change the pseudopotential scheme. Norm-conserving pseudopotentials are
    sometimes too attractive near $r=0$.
    - A 1st solution is to change the quantum number used to generate the norm-conserving pseudopotential.
      But this is generally not sufficient.
    - A 2nd solution is to select a `ultrasoft` pseudopotential, freeing the
      norm conservation constraint (simply replace `troulliermartins` by `ultrasoft` in the input file).
    - A 3rd solution is to select a simple `bessel` pseudopotential (replace
      `troulliermartins` by `bessel` in the input file). But, in that case, one has to
      noticeably decrease the matching radius $r_{Vloc}$ if one wants to keep reasonable
      physical results. Selecting a value of $r_{Vloc}$ between $0.6~r_{PAW}$ and $0.8~r_{PAW}$ is
      a good choice.
      To change the value of $r_{Vloc}$, one has to explicitely put all matching radii:
      $r_{PAW}$, $r_{shape}$, $r_{Vloc}$ and $r_{core}$; see [user's guide](paw2_assets/atompaw-usersguide.pdf).
   * Last solution : try to change the matching radius $r_c$ for one (or both) $l$ partial-wave(s). In some cases,
     changing $r_c$ can remove ghost states.

In most cases (changing pseudopotential or matching radius), one has to restart the procedure from step 5.

To see an example of ghost state, use the
 <span style="color:green">\$ABI_HOME/doc/tutorial/paw2_assets/Ni.ghost.atompaw.input</span> file and run it with `ATOMPAW`.
 
{% dialog tutorial/paw2_assets/Ni.ghost.atompaw.input %}

Look at the $l=1$ logarithmic derivatives (`logderiv.1` file). They look like:

![Ni l=1 log derivatives](paw2_assets/log1c.jpg)

Now, edit the <span style="color:green">Ni.ghost.atompaw.input</span> file and replace `troulliermartins` by
`ultrasoft`.  
Run `ATOMPAW` again... and look at `logderiv.1` file.
The ghost state has moved!  

Edit again the file and replace `troulliermartins` by `bessel` (line 28); then change the 17th
line `2.0 2.0 2.0 2.0` by `2.0 2.0 1.8 2.0` (decreasing the $r_{Vloc}$ radius from $2.0$ to $1.8$).  
Run `ATOMPAW`: the ghost state disappears!

Start from the original state of <span style="color:green">Ni.ghost.atompaw.input</span> file and put `1.6` for
the matching radius of $p$\- states (put `1.6` on lines 31 and 32).
Run `ATOMPAW`: the ghost state disappears!

## 7. Testing the "efficiency" of a PAW dataset

Let's use again our <span style="color:green">Ni.atompaw.input1</span> file for Nickel (with all our modifications).  
You get a file <span style="color:green">Ni.GGA-PBE-paw.xml</span> containing the PAW dataset designated for ABINIT.  

To test the efficiency of the generated PAW dataset, we finally will use `ABINIT`!  
You are about to run a DFT computation and determine the size of the **plane
wave basis** needed to reach a given accuracy. If the **cut-off energy** defining the
plane waves basis is too high (higher than `20 Hartree`), some changes have to be made in the input file.

Copy <span style="color:green">\$ABI_TESTS/tutorial/Input/tpaw2_1.abi</span> in your working directory.
Edit it, and activate the 8 datasets (uncomment the line `ndtset 8`).  

{% dialog tests/tutorial/Input/tpaw2_1.abi %}

Run 'ABINIT'. It computes the `total energy` of ferromagnetic FCC Nickel for several values of [[ecut]].  
At the end of output file, you get this:

	  ecut1     8.00000000E+00 Hartree
	  ecut2     1.00000000E+01 Hartree
	  ecut3     1.20000000E+01 Hartree
	  ecut4     1.40000000E+01 Hartree
	  ecut5     1.60000000E+01 Hartree
	  ecut6     1.80000000E+01 Hartree
	  ecut7     2.00000000E+01 Hartree
	  ecut8     2.20000000E+01 Hartree
	etotal1    -3.9299840066E+01
	etotal2    -3.9503112955E+01
	etotal3    -3.9582704516E+01
	etotal4    -3.9613343901E+01
	etotal5    -3.9622927015E+01
	etotal6    -3.9626266739E+01
	etotal7    -3.9627470087E+01
	etotal8    -3.9627833090E+01

`etotal` convergence (at 1 mHartree) is achieve for $18 \le e_{cut} \le 20$ Hartree.  
`etotal` convergence (at 0,1 mHartree) is achieve for $e_{cut} \ge 22$ Hartree.

This is not a good result for a PAW dataset; let's try to optimize it.

* 1st possibility: use `vanderbilt` **projectors** instead of `bloechl` ones.  
  Vanderbilt's projectors generally are more localized in reciprocal space than
  Bloechl's ones .
  Keyword `bloechl` has to be replaced by `vanderbilt` in the `ATOMPAW` input file
  and $r_c$ values have to be added at the end of the file (one for each PS partial-wave).
  See this input file:
  <span style="color:green">\$ABI_HOME/doc/tutorial/paw2_assets/Ni.atompaw.input.vanderbilt</span>.  

{% dialog tutorial/paw2_assets/Ni.atompaw.input.vanderbilt %}

* 2nd possibility: use `RRKJ` **pseudization scheme** for projectors.  
  Use this input file for `ATOMPAW`:
  <span style="color:green">\$ABI_HOME/doc/tutorial/paw2_assets/Ni.atompaw.input2</span>.
  As you can see `bloechl` has been changed by `custom rrkj`
  and six $r_c$ values have been added at the end of the file, each one
  corresponding to the matching radius of one PS partial-wave.
  Repeat the entire procedure (`ATOMPAW` \+ `ABINIT`)... and get a new ABINIT output file.  
  _Note: You have check again log derivatives._

{% dialog tutorial/paw2_assets/Ni.atompaw.input2 %}

      ecut1     8.00000000E+00 Hartree
      ecut2     1.00000000E+01 Hartree
      ecut3     1.20000000E+01 Hartree
      ecut4     1.40000000E+01 Hartree
      ecut5     1.60000000E+01 Hartree
      ecut6     1.80000000E+01 Hartree
      ecut7     2.00000000E+01 Hartree
      ecut8     2.20000000E+01 Hartree
    etotal1    -3.9599860476E+01
    etotal2    -3.9626919903E+01
    etotal3    -3.9627249378E+01
    etotal4    -3.9627836846E+01
    etotal5    -3.9628304332E+01
    etotal6    -3.9628429611E+01
    etotal7    -3.9628436662E+01
    etotal8    -3.9628455467E+01

`etotal` convergence (at 1 mHartree) is achieve for $12 \le e_{cut} \le 14$ Hartree.  
`etotal` convergence (at 0,1 mHartree) is achieve for $16 \le e_{cut} \le 18$ Hartree.

This is a reasonable result for a PAW dataset!

* 3rd possibility: use **enhanced polynomial** pseudization scheme for projectors.  
  Edit
  <span style="color:green">Ni.atompaw.input2</span>
  and replace `custom rrkj` by `custom polynom2 7 10`. It may sometimes improve the ecut convergence.
  

### Optional exercise

Let's go back to `Vanderbilt` projectors.  
Repeat the procedure (`ATOMPAW`\ + `ABINIT`) with the previous
<span style="color:green">\Ni.atompaw.input.vanderbilt</span> file.

{% dialog tutorial/paw2_assets/Ni.atompaw.input.vanderbilt %}

Let's try  to change the pseudization scheme for the local pseudopotential.  
Try to replace the `troulliermartins` keyword by `ultrasoft`.
Repeat the procedure (`ATOMPAW` \+ `ABINIT`).  
ABINIT can now reach convergence!

Results are below:

      ecut1     8.00000000E+00 Hartree
      ecut2     1.00000000E+01 Hartree
      ecut3     1.20000000E+01 Hartree
      ecut4     1.40000000E+01 Hartree
      ecut5     1.60000000E+01 Hartree
      ecut6     1.80000000E+01 Hartree
      ecut7     2.00000000E+01 Hartree
      ecut8     2.20000000E+01 Hartree
    etotal1    -3.9608001348E+01
    etotal2    -3.9613479343E+01
    etotal3    -3.9616615528E+01
    etotal4    -3.9620665403E+01
    etotal5    -3.9622873734E+01
    etotal6    -3.9623393021E+01
    etotal7    -3.9623440787E+01
    etotal8    -3.9623490997E+01
   
`etotal` convergence (at 1 mHartree) is achieve for $14 \le e_{cut} \le 16$ Hartree.  
`etotal` convergence (at 0,1 mHartree) is achieve for $20 \le e_{cut} \le 22$ Hartree.


!!! note
    You could have tried the `bessel` keyword instead of `ultrasoft` one.

**Summary of convergence results**

![ecut convergence](paw2_assets/ecut.jpg)

!!! Final_remarks

    * The localization of projectors in reciprocal space can (generally) be predicted by a look at `tprod.i` files.
      Such a file contains the curve of as a function of $q$ (reciprocal space variable).
      $q$ is given in $Bohr^{-1}$ units; it can be connected to ABINIT plane waves cut-off energy (in Hartree units) by:
      $e_{cut}=\frac{q_{cut}^2}{4}$. These quantities are only calculated for the bound states,
      since the Fourier transform of an extended function is not well-defined.

    * Generating projectors with Blochl's scheme often gives the guaranty to have stable calculations.
      `ATOMPAW` ends without any convergence problem and DFT calculations run without any divergence
      (but they need high plane wave cut-off). Vanderbilt projectors (and even more `custom` projectors)
      sometimes produce instabilities during the PAW dataset generation process and/or the DFT calculations
      but are more efficient.

    * In most cases, after having changed the projector generation scheme,
      one has to restart the procedure from step 5.

## 8 Testing against physical quantities

The last step is to examine carefully the physical quantities obtained with our PAW dataset.

Copy <span style="color:green">\$ABI_TESTS/tutorial/Input/tpaw2_2.abi</span> in your working directory.
Edit it, activate the 7 datasets (ubcomment the 'ndtset 7` line),
 and use
 <span style="color:green">\$ABI_HOME/doc/tutorial/paw2_assets/Ni.GGA-PBE-paw.rrkj.xml</span> 
 PAW dataset obtained from <span style="color:green">Ni.atompaw.input2 file</span>.  
Run ABINIT (this may take a while...).

{% dialog tests/tutorial/Input/tpaw2_2.abi %}

{% dialog tutorial/paw2_assets/Ni.GGA-PBE-paw.rrkj.xml %}

ABINIT computes the converged ground state of ferromagnetic FCC Nickel for several volumes around equilibrium.
Plot the `etotal` vs `acell` curve:

![etotal vs acell](paw2_assets/acell-etotal.jpg){width=70%}

From this graph and output file, you can extract some physical quantities:

    Equilibrium cell parameter:     a0 = 3.523 angstrom
    Bulk modulus:                    B = 199 GPa
    Magnetic moment at equilibrium: mu = 0.60

Compare these results with published results:

* all-electron GGA-FLAPW from [[cite:Kresse1999]]:

````
    a0 = 3.52 angstrom
     B = 200 GPa
    mu = 0.60
````

* GGA-PAW with VASP code from [[cite:Kresse1999]]:

````
    a0 = 3.52 angstrom
     B = 194 GPa
    mu = 0.61
````

* Experimental results from from [[cite:Dewaele2008]]:

````
    a0 = 3.52 angstrom
     B = 183 GPa
````

You should always compare results with all-electron ones (or other PAW computations).
 Not with experimental ones!

**Additional remark**:
It can be useful to test the sensitivity of results to some `ATOMPAW` input parameters
(see [user's guide](paw2_assets/atompaw-usersguide.pdf) for details on keywords):

  * The analytical form and the cut-off radius $r_{shape}$ of the shape function used in
    compensation charge density definition,
    By default a `sinc` function is used but a `gaussian` shape can have an influence on results.
    `Bessel` shapes are efficient and generally need a smaller cut-off radius ($r_{shape}=0.8~r_{PAW}$).
  * The matching radius $r_{core}$ used to generate the pseudo core density from atomic core density,
  * The inclusion of additional ("semi-core") states in the set of valence electrons,
  * The pseudization scheme used to get pseudopotential $Vloc(r)$.

All these parameters have to be meticulously checked, especially if the PAW
dataset is used for non-standard solid structures or thermodynamical domains.

!!! Optional_exercise

    Let's add 3s and 3p semi-core states in PAW dataset!  
    Repeat the procedure (`ATOMPAW` \+ `ABINIT`) with
    <span style="color:green">\$ABI_HOME/doc/tutorial/paw2_assets/Ni.atompaw.input.semicore</span>
    file. The execution time is a bit longer as more electrons have to be treated by `ABINIT`.  
    Look at $a_0$, $B$ or $\mu$ variation.  
    Note: this new PAW dataset has a smaller $r_{PAW}$ radius (because semi-core states are localized).
    ````
        a0 = 3.518 angstrom
        B = 194 GPa
        mu = 0.60
    ````

## 9 The Real Space Optimization (RSO) - for experienced users

In this section, an additional optimization of the atomic data is presented.
It can contribute, in some cases, to a speedup of the convergence on `ecut`.
This optimization is not essential to produce efficient PAW datasets but can be useful.  
We advise experienced users to try it.  

The idea is quite simple: when expressing the different atomic radial
functions ($\phi_i, \tphi_i, \tprj_i$) on the plane wave basis, the number of plane waves
depends on the "locality" of these radial functions in reciprocal space.

In [[cite:KingSmith1991|this paper]] a method to enforce the locality (in reciprocal space)
of projectors $\tprj_i$ is presented; the projectors expressed in reciprocal space $\tprj_i(g)$
are modified according to the following scheme:
The reciprocal space is divided in 3 regions:

* If $g \lt g_{max}$, $\tprj_i(g)$ is unchanged

* If $g \gt \gamma$, $\tprj_i(g)$ is set to zero

* If $g_{max} \le g \le \gamma$, $\tprj_i(g)$ is modified so that
  the contribution of $\tprj_i(r)$ is conserved
  with an error $W$ (as small as possible).

![RSO](paw2_assets/RSO.png){width=50%}

The above transformation of $\tprj_i(g)$ is only possible if $\tprj_i(r)$ is defined outside
the spherical augmentation region up to a radius $R_0$, with $R_0 > r_c$.
In practice we have to:

1. Impose an error $W$ ($W$ is the maximum error admitted on total energy)
2. Adjust $g_{max}$ according to $E_{cut}$ ($g_{max} \le E_{cut}$)
3. Choose $\gamma$ so that $2 g_{max} \lt \gamma \lt 3 g_{max}$

and let the `ATOMPAW` code apply the transformation to $\tprj_i$ and deduce $R_0$ radius.

You can test it now.
In your working directory, use the dataset
<span style="color:green">\$ABI_HOME/doc/tutorial/paw2_assets/Ni.atompaw.input3</span>
(Bloechl's projectors).

{% dialog tutorial/paw2_assets/Ni.atompaw.input3 %}

Replace the `XML options` (penultimate line):
````
    XMLOUT
    default
````
with:
````
    XMLOUT
    rsoptim 8. 2 0.0001
````
8., 2 and 0.0001 are the values for $g_{max}$,$\frac{\gamma}{g_{max}}$ and $W$.  

Run ATOMPAW.  
You get a new PAW dataset file for `ABINIT`.
Run `ABINIT` with it using the <span style="color:green">tpaw2_1.abi</span> file.  
Compare the results with those obtained in section 7.  

You can try several values for $g_{max}$ (keeping $\frac{\gamma}{g_{max}}$
and $W$ constant) and compare the efficiency of the atomic data;
do not forget to test physical properties again.

![RSO comparison](paw2_assets/RSOcompa.jpg)

!!! Note
	 **How to choose the RSO parameters?**  
	 $\frac{\gamma}{g_{max}} = 2$ and $0.0001 \lt W \lt 0.001$ is a good choice.
	 $g_{max}$ has to be adjusted. The lower $g_{max}$ the faster the convergence is
	 but too low $g_{max}$ can produce unphysical results.
---
authors: DJA
---

# Calculation of *U* and J using Cococcioni's approach

## How to determine *U* for DFT+*U* in ABINIT? Cococcioni's approach.

This tutorial aims to show how you can determine *U* for further DFT+*U*
calculations consistently and in a fast an easy way. You will learn how to prepare
the input files for the determination and how to use the main parameters implemented for this aim.
It is supposed that you already know how to run ABINIT with PAW  (tutorial [PAW1](/tutorial/paw1)).
Obviously, you should also read the tutorial [DFT+U](/tutorial/dftu), and likely the tutorial [PAW2](/tutorial/paw2),
to generate PAW atomic data.

This tutorial should take about 1/2 hour.

[TUTORIAL_README]

## Summary of linear response method to determine *U*

The linear response method has been introduced by several authors
[[cite:Cococcioni2002]], [[cite:Cococcioni2005]],[[cite:Dederichs1984]],[[cite:Hybertsen1989]],
[[cite:Anisimov1991]],[[cite:Pickett1998]].
It is based on the fact that *U* corresponds to the energy to localize an additional
electron on the same site: $U=E[n+1]+E[n-1]-2E[n]$ [[cite:Hybertsen1989]]. This can be reformulated
as the response to an infinitesimal change of of occupation of the orbital by
the electrons $dn$. Then *U* is the second derivative of the energy with respect
to the occupation $U=\frac{\delta^2 E}{\delta^2 n}$. The first method fixed the occupation by
cutting the hopping terms of localized orbitals. Later propositions
constrained the occupation through Lagrange multipliers [[cite:Dederichs1984]],[[cite:Anisimov1991]]. The Lagrange
multiplier $\alpha$ corresponds to a local potential that has to be applied to
increase or decrease the occupation by +/-1 electron. Note that the occupation
need not to vary by 1 electron, but the occupation shift can be infinitesimal.

It is recommended to read the following papers to understand the basic
concepts of the linear response calculations to calculate *U*:

[1] "A LDA+*U* study of selected iron compounds ", M. Cococcioni, Ph.D. thesis,
International School for Advanced Studies (SISSA), Trieste (2002)  [[cite:Cococcioni2002]]

[2] "Linear response approach to the calculation of the effective interaction
parameters in the LDA + *U* method", M. Cococcioni and S. de Gironcoli, Physical
Review B 71, 035105 (2005)  [[cite:Cococcioni2005]]

Some further reading:

[3] "Ground States of Constrained Systems: Application to Cerium Impurities",
P. H. Dederichs, S. Blugel, R. Zeller, and H. Akai, Phys. Rev. Lett. 53, 2512 (1984)   [[cite:Dederichs1984]]

[4] "Calculation of Coulomb-interaction parameters for La2CuO4 using a
constrained-density-functional approach", M. S. Hybertsen, M. Schluter, and N.
E. Christensen, Phys. Rev. B 39, 9028 (1989)   [[cite:Hybertsen1989]]

[5] "Density-functional calculation of effective Coulomb interactions in
metals", V. I. Anisimov and O. Gunnarsson, Phys. Rev. B42, 7570 (1991)  [[cite:Anisimov1991]]

[6] "Reformulation of the LDA+*U* method for a local-orbital basis", W. E.
Pickett, S. C. Erwin, and E. C. Ethridge, Phys. Rev. B58, 1201 (1998)  [[cite:Pickett1998]]

The implementation of the determination of *U* in ABINIT is briefly discussed in  [[cite:Gonze2016]].

## How to determine *U* in ABINIT

*Before continuing, you may consider to work in a different subdirectory as
for the other tutorials. Why not Work_udet?*

!!! important

    In what follows, the name of files are mentioned as if you were in this subdirectory.
    All the input files can be found in the $ABI_TESTS/tutorial/Input directory
     You can compare your results with reference output files located in
     $ABI_TESTS/tutorial/Refs directory (for the present tutorial they are named tudet*.abo).

The input file *tudet_1.abi* is an example of a file to prepare a wave function
for further processing. The corresponding output file is ../Refs/tudet_1.abo).

Copy the files *tudet_1.abi* in your work directory, and run ABINIT:

```sh
cd $ABI_TESTS/tutorial/Input
mkdir Work_udet
cd Work_udet
cp ../tudet_1.abi .

abinit  tudet_1.abi > log 2> err &
```

In the meantime, you can read the input file and see that this is a usual
DFT+U calculation, with *U*=0.

{% dialog tests/tutorial/Input/tudet_1.abi %}

This setting allows us to read the occupations of
the Fe $3d$ orbitals ([[lpawu]] 2). The cell contains 2 atoms. This is the
minimum to get reasonable response matrices. We converge the electronic
structure to a high accuracy ([[tolvrs]] 10d-12), which usually allows one to
determine occupations with a precision of 10d-10. The [[ecut]] is chosen very low,
in order to accelerate calculations.
We do not suppress the writing of the *WFK* file, because this is the input for
the calculations of U.

Once this calculation has finished, run the second one:
Copy the file *tudet_2.abi* in your work directory, and run ABINIT:

    abinit tudet_2.abi > tudet_2.log

{% dialog tests/tutorial/Input/tudet_2.abi %}

As you can see from the *tudet_2.abi* file, this run uses the *tudet_1o_WFK* as
an input (as indata_prefix = "tudet_1.o"). In the *tudet_2.abi* all the symmetry relations are specified
explicitly. In the *tudet_2.log* you can verify that none of the symmetries
connects atoms 1 with atom 2:

    symatm: atom number    1 is reached starting at atom

       1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1

       1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1

     symatm: atom number    2 is reached starting at atom

       2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2

       2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2

This is important. Otherwise the occupation numbers have no freedom to evolve
separately on the atoms surrounding the atom on which you apply the perturbation.

You can generate these symmetries, in a separate run, where you specify the
atom where the perturbation is done as a different species. From the output
you read the number of symmetries ([[nsym]]), the symmetry operations
([[symrel]]) and the non-symmorphic vectors ([[tnons]]). This is already
done here and inserted in the *tudet_2.abi* file. Note that you can alternatively
disable all symmetries with [[nsym]] = 1, or break specific symmetries by
displacing the impurity atom in the preliminary run. However, for the
determination of *U*, the positions should be the ideal positions and only the
symmetry should be reduced.

For the rest, usually it is enough to set [[macro_uj]] 1 to run the
calculation of *U*. Note also, that the [[irdwfk]] 1 and the [[tolvrs]] 1d-8
need not be set explicitly, because they are the defaults with [[macro_uj]] 1.

Once the calculation tudet_2 is converged, you can have look at the output.
You can see, that the atomic shift ([[atvshift]]) is automatically set:

             atvshift      0.00367    0.00367    0.00367    0.00367    0.00367
                           0.00367    0.00367    0.00367    0.00367    0.00367
                           0.00000    0.00000    0.00000    0.00000    0.00000
                           0.00000    0.00000    0.00000    0.00000    0.00000

This means, that all the 10 3d spin-spin orbitals on the first Fe atom where
shifted by 0.1 eV (=0.00367 Ha). On the second atom no shift was applied.
Self-consistency was reached twice: Once for a positive shift, once for the negative shift:

    grep SCF  tudet_2.abo

The lines starting with URES

     URES      ii    nat       r_max    U(J)[eV]   U_ASA[eV]   U_inf[eV]
     URES       1      2     4.69390     3.86321     3.10851     2.71778
     URES       2     16     9.38770     7.28015     5.85793     5.12160
     URES       3     54    14.08160     7.60761     6.12142     5.35197
     URES       4    128    18.77540     7.67652     6.17686     5.40045
     URES       5    250    23.46930     7.69879     6.19478     5.41611

    
contain *U* for different supercells. These values of $U$ are computed using the extrapolation
procedure proposed in [[cite:Cococcioni2005]]. In this work, it is shown that using a two atom supercell for the DFT calculation
and an extrapolation procedure allows a estimation of the value of $U$. More precise values can be obtained
using a larger supercell for the DFT calculation and an extrapolation. For simplicity, in this tutorial, we restrict to
the two atoms supercell for the DFT calculation.

 The column "nat" indicates how many atoms
were involved in the extrapolated supercell, r_max indicates the maximal distance of the
impurity atoms in that supercell. The column *U* indicates the actual *U* you
calculated and should use in your further calculations. U_ASA is an estimate
of *U* for more extended projectors and U_inf is the estimate for a projector
extended even further.

Although it is enough to set [[macro_uj]] 1, you can further tune your runs.
As a standard, the potential shift to the 1st atom treated in DFT+*U*, with a
potential shift of 0.1 eV. If you wish to determine *U* on the second atom you
put [[pawujat]] 2. To change the size of the potential shift use e.g.
[[pawujv]] 0.05 eV. Our tests show that 0.1 eV is the optimal value, but the
linear response is linear in a wide range (1-0.001 eV).

## The ujdet utility

In general the calculation of *U* with abinit as described above is sufficient.
For some post-treatment that goes beyond the standard applications, a separate
executable *ujdet* was created. The output of abinit is formatted so that you
can easily "cut" the part with the ujdet input variables: you can generate
the standard input file for the ujdet utility by typing:

    sed -n "/MARK/,/MARK/p" tudet_2.abo  > ujdet.in

Note that the input for the ujdet utility is always called *ujdet.in*

It contains the potential shifts applied vsh (there are 4 shifts: vsh1, vsh3
for non-selfconsistent calculations that allows to extract the contribution to
*U* originating from a non-interacting electron gas, and vsh2, vsh4 for positive
and negative potential shift). The same applies for the occupations occ[1-4].

We now calculate *U* for an even larger supercell: Uncomment the line scdim in *ujdet.in* and add

     scdim 6 6 6

to specify a 6 6 6 supercell or

     scdim 700 0 0

to specify the maximum total number of atoms in the supercell. Then, run *ujdet* (the executable is in the same directory as the abinit executable):

    rm ujdet.[ol]* ; ujdet > ujdet.log

    grep URES ujdet.out

     URES      ii    nat       r_max    U(J)[eV]   U_ASA[eV]   U_inf[eV]
     URES       1      2     4.69390     3.86321     3.10851     2.71778
     URES       2     16     9.38770     7.28015     5.85793     5.12160
     URES       3     54    14.08160     7.60761     6.12142     5.35197
     URES       4    128    18.77540     7.67652     6.17686     5.40045
     URES       5    250    23.46930     7.69879     6.19478     5.41611
     URES       6    432    28.16310     7.70813     6.20230     5.42268


As you can see, *U* has now been extrapolated to a supercell containing 432 atoms.

The value of *U* depends strongly on the extension of the projectors used in the
calculation. If you want to use *U* in LMTO-ASA calculations you can use the
keyword [[pawujrad]] in the *ujdet.in* file to get grips of the *U* you want to
use there. Just uncomment the line and add the ASA-radius of the specific atom e.g.

    pawujrad 2.5

Running

    rm ujdet.[ol]* ; ujdet > ujdet.log

gives now higher values in the column U_ASA than in the runs before (6.91 eV
compared to 6.20 eV): For more localized projectors the *U* value has to be bigger.
---
authors: XG, RC
---

# Third tutorial

## Crystalline silicon.

This tutorial aims at showing you how to get the following physical properties, for an insulator:

* the total energy,
* the lattice parameter,
* the band structure (actually, the Kohn-Sham band structure).

You will learn about the use of k-points, as well as the smearing of the plane-wave kinetic energy cut-off.

Visualisation tools are NOT covered in this tutorial.
Powerful visualisation procedures have been developed in the Abipy context,
relying on matplotlib. See the README of [Abipy](https://github.com/abinit/abipy)
and the [Abipy tutorials](https://github.com/abinit/abitutorials).

This tutorial should take about 1 hour.

[TUTORIAL_README]

## Computing the total energy of silicon at a fixed number of k-points

*Before beginning, you might consider working in a different subdirectory, as
for tutorial 1 or 2. Why not Work3?*

The following commands will move you to your working directory, create the *Work3* directory, and move you into that directory as you did in the
[[lesson:base1|first]] and [[lesson:base2|second]] tutorials.
Then, we copy the file *tbase3_1.abi* inside the *Work3* directory. The commands are:

```sh
cd $ABI_TESTS/tutorial/Input
mkdir Work3
cd Work3
cp ../tbase3_1.abi .
```

This is your input file:

{% dialog tests/tutorial/Input/tbase3_1.abi %}

You should edit it, read it carefully, have a look at the following **new input variables** and their explanation:

* [[rprim]]
* [[xred]] (used instead of [[xcart]])
* [[kptopt]], [[ngkpt]], [[nshiftk]], [[shiftk]], [[kptrlatt]] (not easy, take your time!)
* [[diemac]] (a different value is used for this variable compare to previous calculations where isolated molecules were considered).

Note also the following: you will work at fixed [[ecut]] (12Ha).
It is implicit that in *real life*, you should do a convergence test with respect to *ecut*.
Here, a suitable *ecut* is given to you; it corresponds to the suggested *ecut* for this pseudopotential according to the [PseudoDojo website](http://www.pseudo-dojo.org/) where the silicon pseudopotential was taken.
When we will relax the lattice parameter, it will result in a lattice parameter that is 0.2% off of the experimental value.

When you have read the input file, you can run the code, as usual:

    abinit tbase3_1.abi > log 2> err &

Then, read the output file, and note the total energy:

    etotal   -8.5187390642E+00

## Starting the convergence study with respect to k-points

There is, of course, a convergence study associated with the sampling of the Brillouin zone.
You should examine different grids, of increasing resolution. You might try the following series of grids:

    ngkpt1  2 2 2
    ngkpt2  4 4 4
    ngkpt3  6 6 6
    ngkpt4  8 8 8

However, the associated number of k-points in the irreducible Brillouin zone grows very fast.
It is

    nkpt1  2
    nkpt2 10
    nkpt3 28
    nkpt4 60

Abinit computes automatically this number of k-points, from the definition of the grid and the symmetries.
You might nevertheless define an input [[nkpt]] value in the input file, in which case the code will compare
its computed value (from the grid) with this input value.

We take this opportunity to examine the behaviour of abinit when a problem is detected.
Let us suppose that with *ngkpt 4 4 4*, one mentions *nkpt 2*.
The input file *tbase3_2.abi* is an example:

{% dialog tests/tutorial/Input/tbase3_2.abi %}

The message that you get a few dozen of lines before the end of the log file is:

```yaml
--- !BUG
src_file: m_kpts.F90
src_line: 1417
mpi_rank: 0
message: |
    The argument nkpt = 2, does not match
    the number of k points generated by kptopt, kptrlatt, shiftk,
    and the eventual symmetries, that is, nkpt= 10.
    However, note that it might be due to the user,
    if nkpt is explicitely defined in the input file.
    In this case, please check your input file.
...

  Action: contact ABINIT group (please attach the output of `abinit -b`)
```

This is a typical abinit error message.
It states what is the problem that causes the code to stop, then suggests that it might be due to an error
in the input file, namely, an erroneous value of [[nkpt]].
The expected value, [[nkpt]] 10 is mentioned before the notice that the input file might be erroneous.
Then, the file at which the problem occurred is mentioned, as well as the number of the line in that file.

As the computation of [[nkpt]] for specific grids of k-points is not an easy task,
while the even more important selection of specific economical grids
(the best ratio between the accuracy of the integration in the Brillouin zone and the number of k-points)
is more difficult, some help to the user is provided by ABINIT.

The code is able to examine automatically different k-point grids, and to propose the best grids for integration.
This is described in the [[help:abinit]], see the input variable [[prtkpt]], and the associated characterisation
of the integral accuracy, described in [[kptrlen]].

!!! tip

    The generation of lists of k-point sets is done in different test cases, in `$ABI_TESTS/v2`.
    You can directly have a look at the output files in `$ABI_TESTS/v2/Refs`,
    the output files for the tests 61 to 73.

When one begins the study of a new material, it is strongly advised to examine first the list of k-points grids,
and select (at least) three efficient ones, for the k-point convergence study.

Do not forget that the CPU time will be linearly proportional to the number of k-points to be treated:
using 10 k-points will take five times more than using 2 k-points. Even for a similar accuracy of the
Brillouin zone integration (that is to say for about the same value of [[kptrlen]]), there might be a grid that will reduce to 10 k-points in the irreducible Brillouin zone and another that will reduce to 2 k-points
in the irreducible Brillouin zone.
The latter is clearly to be preferred from a computational perspective!

## Convergence study with respect to k-points

In order to understand k-point grids, you should read [[cite:Monkhorst1976]].
Well, maybe not immediately. In the meantime, you can try the above-mentioned convergence study.

The input file *tbase3_3.abi* is an example,
while *$ABI_TESTS/tutorial/Refs/tbase3_3.abo* is a reference output file.

```sh
cd $ABI_TESTS/tutorial/Work3
cp ../tbase3_3.abi .
```

{% dialog tests/tutorial/Input/tbase3_3.abi tests/tutorial/Refs/tbase3_3.abo %}

In this output file, you should have a look at the echo of input variables.
As you know, these are preprocessed, and, in particular, [[ngkpt]] and [[shiftk]]
are used to generate the list of k-points ([[kpt]]) and their weights ([[wtk]]).
You should read the information about [[kpt]] and [[wtk]].

From the output file, here is the evolution of total energy for the different k-point grids:

    etotal1    -8.5187390642E+00
    etotal2    -8.5250179735E+00
    etotal3    -8.5251232908E+00
    etotal4    -8.5251270559E+00

The difference between dataset 3 and dataset 4 is rather small.
Even the dataset 2 gives an accuracy of about 0.0001 Ha. So, our converged value for the total energy,
at fixed [[acell]], fixed [[ecut]], is -8.8251 Ha.

## Determination of the lattice parameters

The input variable [[optcell]] governs the automatic optimisation of cell shape and volume.
For the automatic optimisation of cell volume in this cubic crystal, use:

    optcell 1
    ionmov 2
    ntime 10
    dilatmx 1.05
    ecutsm 0.5

You should read the indications about [[optcell]], [[dilatmx]] and [[ecutsm]].
In particular, while [[optcell]] is adequate for cubic crystals, for the majority of materials,
the optimal geometry must include deformations of the cell shape, not simply global rescaling,
so the most usual value of [[optcell]] to be used is 2.
Do not test all the k-point grids, only those with **nkpt** 2 and 10.

The input file *$ABI_TESTS/tutorial/Input/tbase3_4.abi* is an example,

{% dialog tests/tutorial/Input/tbase3_4.abi %}

while *$ABI_TESTS/tutorial/Refs/tbase3_4.abo* is a reference output file.

{% dialog tests/tutorial/Refs/tbase3_4.abo %}

You should obtain the following evolution of the lattice parameters:

    acell1     1.0208746777E+01  1.0208746777E+01  1.0208746777E+01 Bohr
    acell2     1.0195482058E+01  1.0195482058E+01  1.0195482058E+01 Bohr

with the following very small residual stresses:

    strten1    -2.0279878345E-08 -2.0279878345E-08 -2.0279878345E-08
                0.0000000000E+00  0.0000000000E+00  0.0000000000E+00
    strten2    -9.2829783285E-08 -9.2829783286E-08 -9.2829783286E-08
                0.0000000000E+00  0.0000000000E+00  0.0000000000E+00

The stress tensor is given in Hartree/Bohr<sup>3</sup>, and the order of the components is:

                            11  22  33
                            23  13  12

There is only a 0.13% relative difference between *acell1* and *acell2*.
So, our converged LDA value for Silicon, with the *Si.psp8* pseudopotential of the [PseudoDojo website](http://www.pseudo-dojo.org/)
is 10.195 Bohr, that is 5.395 Angstrom.
The experimental value is *5.431* Angstrom at 25 degree Celsius,
see R.W.G. Wyckoff, Crystal structures Ed. Wiley and sons, New-York (1963)
or the [NIST database](https://physics.nist.gov/cgi-bin/cuu/Value?asil|search_for=silicon).

## Computing the band structure

We fix the parameters [[acell]] to the theoretical value of 3 * 10.195,
and we fix also the grid of k-points (the 4x4x4 FCC grid, equivalent to a 8x8x8 Monkhorst-pack grid).
We will ask for 8 bands (4 valence and 4 conduction).

A band structure can be computed by solving the Kohn-Sham equation for many different k-points,
along different segments of the Brillouin zone. The potential that enters the Kohn-Sham must be
derived from a previous self-consistent calculation, and will not vary during the scan of different k-point lines.

Suppose that you want to make a L-Gamma-X-(U-)Gamma circuit, with at least 10 divisions for each segment.
The circuit will be obtained easily by the following choice of segment end points:

    L     (1/2 0 0)
    Gamma (0 0 0)
    X     (0 1/2 1/2)
    Gamma (1 1 1)

In order to enforce at least 10 divisions for each segment, one uses the input variable [[ndivsm]].
ABINIT will generate roughly the same distance between points along each segment,
despite each segment having a different length in reciprocal space.

Note:

1. the last Gamma point is in another cell of the reciprocal space than the first one,
   this choice allows to construct the X-U-Gamma segment easily;

2. the k-points are specified using reduced coordinates - in agreement with the input setting of the primitive 2-atom unit cell -
   in standard textbooks, you will often find the L, Gamma or X point given in coordinates of the conventional 8-atom cell:
   the above-mentioned circuit is then (1/2 1/2 1/2)-(0 0 0)-(1 0 0)-(1 1 1), but such (conventional) coordinates
   cannot be used with the 2-atom (non-conventional) cell.

So, you should set up in your input file, for the first dataset, a usual SCF calculation
in which you output the density ([[prtden]] 1), and, for the second dataset:

* fix [[iscf]] to -2, to make a non-self-consistent calculation,
* define [[getden]] -1, to take the output density of dataset 1,
* set [[nband]] to 8,
* set [[kptopt]] to -3, to define three segments in the brillouin Zone;
* set [[ndivsm]] to 10,
* set [[kptbounds]] to

        0.5  0.0  0.0 # L point
        0.0  0.0  0.0 # Gamma point
        0.0  0.5  0.5 # X point
        1.0  1.0  1.0 # Gamma point in another cell

* set [[enunit]] to 1, in order to have eigenenergies in eV,
* the only tolerance criterion admitted for non-self-consistent calculations is [[tolwfr]].
  You should set it to 1.0d-10 (or so), and suppress [[toldfe]].
* The [[nstep]] parameter was set to 20 to make sure convergence can be reached.  

The input file *$ABI_TESTS/tutorial/Input/tbase3_5.abi* is an example,

{% dialog tests/tutorial/Input/tbase3_5.abi %}

while *$ABI_TESTS/tutorial/Refs/tbase3_5.abo* is a reference output file.

{% dialog tests/tutorial/Refs/tbase3_5.abo %}

You should find the band structure starting at (second dataset):

    Eigenvalues (   eV  ) for nkpt=  39  k points:
    kpt#   1, nband=  8, wtk=  1.00000, kpt=  0.5000  0.0000  0.0000 (reduced coord)
    -4.83930   -2.21100    3.66138    3.66138    6.36920    8.18203    8.18203   12.44046
    kpt#   2, nband=  8, wtk=  1.00000, kpt=  0.4500  0.0000  0.0000 (reduced coord)
    -4.97880   -2.00874    3.67946    3.67946    6.39165    8.20580    8.20580   12.47444
    kpt#   3, nband=  8, wtk=  1.00000, kpt=  0.4000  0.0000  0.0000 (reduced coord)
    -5.30638   -1.49394    3.73328    3.73328    6.45364    8.26444    8.26444   12.56455
    kpt#   4, nband=  8, wtk=  1.00000, kpt=  0.3500  0.0000  0.0000 (reduced coord)
    -5.69306   -0.79729    3.82286    3.82286    6.55602    8.33970    8.33970   12.65080
     ....

One needs a graphical tool to represent all these data.
In a separate file (*_EIG*), you will find the list of k-points and the eigenenergies
(the input variable [[prteig]] is set by default to 1).

Even without a graphical tool we will have a quick look at the values at L, $\Gamma$, X and $\Gamma$ again:

    kpt#   1, nband=  8, wtk=  1.00000, kpt=  0.5000  0.0000  0.0000 (reduced coord)
    -4.83930   -2.21100    3.66138    3.66138    6.36920    8.18203    8.18203   12.44046

    kpt#  11, nband=  8, wtk=  1.00000, kpt=  0.0000  0.0000  0.0000 (reduced coord)
    -7.22396    4.87519    4.87519    4.87519    7.42159    7.42159    7.42159    8.26902

    kpt#  23, nband=  8, wtk=  1.00000, kpt=  0.0000  0.5000  0.5000 (reduced coord)
    -3.01262   -3.01262    1.97054    1.97054    5.46033    5.46033   15.02324   15.02382

    kpt#  39, nband=  8, wtk=  1.00000, kpt=  1.0000  1.0000  1.0000 (reduced coord)
    -7.22396    4.87519    4.87519    4.87519    7.42159    7.42159    7.42159    8.26902

The last $\Gamma$ is exactly equivalent to the first $\Gamma$.
It can be checked that the top of the valence band
is obtained at $\Gamma$ (=4.87519 eV). The width of the valence band is 12.1 eV, the lowest unoccupied state at X
is 0.585 eV higher than the top of the valence band, at $\Gamma$.

The Si is described as an indirect band gap material (this is correct),
with a band-gap of about 0.585 eV (this is quantitatively quite wrong: the experimental value 1.17 eV is at 25 degree Celsius).
The minimum of the conduction band is even slightly displaced with respect to X, see kpt # 21.
This underestimation of the band gap is well-known (the famous DFT band-gap problem).
In order to obtain correct band gaps, you need to go beyond the Kohn-Sham Density Functional
Theory: use the GW approximation. This is described in [the first GW tutorial](/tutorial/gw1).

For experimental data and band structure representation, see the book by M.L. Cohen and J.R. Chelikowski [[cite:Cohen1988]].

!!! important

    There is a subtlety that is worth to comment about.
    In non-self-consistent calculations, like those performed in the present band structure calculation,
    with [[iscf]] = -2, not all bands are converged within the tolerance [[tolwfr]].
    Indeed, the two upper bands (by default) have not been taken into account to
    apply this convergence criterion: they constitute a **buffer**.
    The number of such buffer bands is governed by the input variable [[nbdbuf]].

    It can happen that the highest (or two highest) band(s), if not separated by a
    gap from non-treated bands, can exhibit a very slow convergence rate.
    This buffer allows achieving convergence of *important*, non-buffer bands.
    In the present case, 6 bands have been converged with a residual better than
    [[tolwfr]], while the two upper bands are less converged (still sufficiently
    for graphical representation of the band structure).
    In order to achieve the same convergence for all 8 bands, it is advised to use [[nband]]=10  (that is, 8 + 2).

## Using AbiPy to automate the most boring steps

The |AbiPy| package provides several tools to facilitate the preparation of band structure calculations
and the analysis of the output results. First of all, one can use the |abistruct| script with
the `kpath` command to determine a high-symmetry k-path from **any file** containing structural
information (abinit input file, netcdf output files etc.).
The high-symmetry k-path follows the conventions described in [[cite:Setyawan2010]].
Let's try with:

```sh
abistruct.py kpath tbase3_5.abi

# Abinit Structure
 natom 2
 ntypat 1
 typat 1 1
 znucl 14
 xred
    0.0000000000    0.0000000000    0.0000000000
    0.2500000000    0.2500000000    0.2500000000
 acell    1.0    1.0    1.0
 rprim
    0.0000000000    5.1080000000    5.1080000000
    5.1080000000    0.0000000000    5.1080000000
    5.1080000000    5.1080000000    0.0000000000

# K-path in reduced coordinates:
# tolwfr 1e-20 iscf -2 getden ??
 ndivsm 10
 kptopt -11
 kptbounds
    +0.00000  +0.00000  +0.00000 # $\Gamma$
    +0.50000  +0.00000  +0.50000 # X
    +0.50000  +0.25000  +0.75000 # W
    +0.37500  +0.37500  +0.75000 # K
    +0.00000  +0.00000  +0.00000 # $\Gamma$
    +0.50000  +0.50000  +0.50000 # L
    +0.62500  +0.25000  +0.62500 # U
    +0.50000  +0.25000  +0.75000 # W
    +0.50000  +0.50000  +0.50000 # L
    +0.37500  +0.37500  +0.75000 # K
    +0.62500  +0.25000  +0.62500 # U
    +0.50000  +0.00000  +0.50000 # X
```

To visualize the band structure stored in the *GSR.nc* file, use the |abiopen| script and the command line:

    abiopen.py tbase3_5o_DS2_GSR.nc --expose -sns=talk

![](base3_assets/abiopen_tbase3_5o_DS2_GSR.png)

It is also possible to compare multiple GSR files with the |abicomp| script and the syntax

    abicomp.py gsr tbase3_5o_DS1_GSR.nc tbase3_5o_DS2_GSR.nc -e -sns=talk

to produce the following figures:

![](base3_assets/abicomp_tbase3_5o_GSR_files.png)

For further details about the AbiPy API and the GSR file, please consult the |GsrFileNb|.
---
authors: JW, MT
---

# Electron-positron annihilation

## Calculation of positron lifetime and momentum distribution in silicon

This tutorial aims at showing how to perform **Two-Component Density-Functional
Theory (TCDFT)** calculations in the PAW framework to obtain the following
physical properties:

  * the positron lifetime in the perfect material,
  * the lifetime of a positron localized in a vacancy,
  * the electron-positron momentum distribution.

For the description of the implementation of TCDFT in `ABINIT` see [[cite:Wiktor2015]].

The user should be familiar with the four basic tutorials of `ABINIT` and the [first PAW tutorial](/tutorial/paw1).

This tutorial should take about 2 hours.

[TUTORIAL_README]

## Computing the positron lifetime in Si lattice

*Before beginning, you might consider to work in a different subdirectory as
for the other tutorials. Why not* `Work_positron`*?*

```sh
cd $ABI_TESTS/tutorial/Input
mkdir Work_positron
cd Work_positron
cp ../tpositron_1.abi .
```

The tutorial begins with a calculation of the positron lifetime in a silicon lattice.
In a perfect material the positron is delocalized. We can assume that its
density approaches zero and that it cannot affect the electron density. We will
perform a calculation in only two steps:

  1. Calculation of the ground-state electron density without the positron.
  2. Calculation of the ground-state positron density in the presence of the electron density from step 1.

The two densities are used to calculate the positron lifetime, which is
proportional to the inverse of the overlap of the electron and positron
densities. This 2-step calculation, considering the _zero-positron density
limit_, corresponds to the conventional scheme (CONV).

In the `tpositron_1.abi` file, you will find two datasets.

{% dialog tests/tutorial/Input/tpositron_1.abi %}

The first dataset is a standard ground-state calculation. The second one introduces a positron into
the system. You can see that in this case we set:

      positron2  1      # Dataset 2 is a positronic GS calculation
      getden2    1      #   in presence of the previous electronic density

      kptopt2    0      #   Use only k=gamma point

      ixcpositron2 1    # We are using the Boronski and Nieminen parametrization

Here we set [[positron]]=1, which corresponds to a positronic ground-state
calculation, considering that the electrons are not perturbed by the presence
of the positron (_zero-positron density limit_). The electron density is read from the file resulting from
dataset 1. As we consider the positron to be completely delocalized, we only
consider the &Gamma; point in the _Brillouin_ zone. The keyword [[ixcpositron]]
selects the **electron-positron correlation functional** and **enhancement factor**.
In this calculation we use the functional parametrized by Boronski and Nieminen
[[cite:Boronski1986]], using the data provided by Arponen and Pajanne [[cite:Arponen1979]].

We can now run the calculation. In the working directory
`Work_positron`, copy the file `tpositron_1.abi`.

Then, issue:

    abinit tpositron_1.abi >& log

This calculation should only take a few seconds.

You can look at the `tpositron_1.abo` file.
We find the positron lifetime calculated in the RPA limit:

	########## Lifetime computation 2

	 # Zero-positron density limit of Arponen and Pajanne provided by Boronski & Nieminen
	   Ref.: Boronski and R.M. Nieminen, Phys. Rev. B 34, 3820 (1986)
	 # Enhancement factor of Boronski & Nieminen IN THE RPA LIMIT
	   Ref.: Boronski and R.M. Nieminen, Phys. Rev. B 34, 3820 (1986)

	 Positron lifetime                         (ps)   =  2.22879743E+02

The lifetime of 223 ps agrees well with the value of 225 ps calculated with
the same number of valence electrons in [[cite:Wiktor2015]] and
with the experimental value of about 219 ps [[cite:Panda1997]].

!!! important

    If we had not used the "zero positron density limit" approximation
    (using, for example, another value of [[ixcpositron]]), we would theoretically
    have needed a box of infinite size for the positron to completely delocalise
    itself in the crystal. This can be avoided by the use of the [[posocc]] input
    parameter. Numerically, it is equivalent to calculate the density of 1
    positron in a box of size N and that of x positron in a box of size N * x.
    Thus, we can calculate the lifetime of a positron in a primitive cell by
    setting [[posocc]] to a small value (0.0001 ...). This value must obviously be tested...


## Computing the positron lifetime in a Si monovacancy

We will now perform a positron lifetime calculation for a monovacancy in
silicon in the conventional scheme (which we applied to the perfect lattice
previously). Note that when the positron localizes inside a vacancy, the
_zero-positron density limit_ does not apply anymore. However, in some cases, the
conventional scheme proved to yield results in agreement with experiments.

For the purpose of this tutorial, we generate a defect in a cell containing
only 16 atoms. This supercell is too small to get converged results, but the
calculation is relatively fast.

{% dialog tests/tutorial/Input/tpositron_2.abi %}

You can now, issue:

    abinit tpositron_2.abi >& log

Once the calculation is finished, look at the `tpositron_2.abo` file.
Again, we look at the reported lifetime:

	########## Lifetime computation 2

	 # Zero-positron density limit of Arponen and Pajanne provided by Boronski & Nieminen
	   Ref.: Boronski and R.M. Nieminen, Phys. Rev. B 34, 3820 (1986)
	 # Enhancement factor of Boronski & Nieminen IN THE RPA LIMIT
	   Ref.: Boronski and R.M. Nieminen, Phys. Rev. B 34, 3820 (1986)

	 Positron lifetime                         (ps)   =  2.46923233E+02

We observe that when the positron localizes inside the vacancy, its lifetime
increases from 223 to 247 ps. This is because now the majority of the positron
density is localized in the vacancy region, where the electron density is
small. The overlap of the electron and positron densities is reduced, and the lifetime increased.

In the `Work_positron` directory, you will also find a `tpositron_2o_DS2_DEN_POSITRON`
file, containing the positron density. Visualizing this file (using e.g.
_cut3d_ and _XcrysDen_ or _VMD_) you can see that the positron is localized
inside the vacancy. You can see below how the positron (in red, isodensity at
30% of the maximum density) localized the silicon monovacancy looks like:

![](positron_assets/posdensity.png){width=80%}


## Performing a self-consistent electron-positron calculation for a Si vacancy

We will now perform a self-consistent calculation of the positron and electron
densities. As this calculation will take a few minutes, you can already issue, using
the `tpositron_3.abi` input file:

    abinit tpositron_3.abi >& log

{% dialog tests/tutorial/Input/tpositron_3.abi %}

This calculation is significantly longer than the previous one, because the
electron and positron steps will be repeated until the convergence criterion is reached.

In `tpositron_3.abi` we only have one dataset and we set
[[positron]] = -10 to perform an automatic calculation of electrons and positron
densities. The convergence is controlled by [[postoldfe]] = 1d-5. This means
that we will repeat the electron and positron steps until the energy
difference between them is lower than 1d-5 Ha. This value should always be
larger than [[toldfe]]. In this calculation we still use [[ixcpositron]] = 1,
which means that we are using the GGGC scheme (see [[cite:Gilgien1994]] and [[cite:Wiktor2015]]

Once the calculation is finished, look at the positron lifetime in `tpositron_3.abo`.

	########## Lifetime computation 2

	 # Zero-positron density limit of Arponen and Pajanne provided by Boronski & Nieminen
	   Ref.: Boronski and R.M. Nieminen, Phys. Rev. B 34, 3820 (1986)
	 # Enhancement factor of Boronski & Nieminen IN THE RPA LIMIT
	   Ref.: Boronski and R.M. Nieminen, Phys. Rev. B 34, 3820 (1986)

	 Positron lifetime                         (ps)   =  2.55612619E+02

Including the self-consistency increases the positron lifetime, because its
localization inside the vacancy becomes stronger when the positron and the
electron densities are allowed to relax.

## Relaxing the vacancy according to forces due to electrons and the positron

In addition to the self-consistency, the lifetime of a positron inside a
vacancy can be strongly affected by the relaxation of the atoms due to the
forces coming from both the electrons and the positron. You can already start
the relaxation (with the `tpositron_4.abi` input file) of the vacancy by issuing:

    abinit tpositron_4.abi >& log

In this calculation we switched on the atomic relaxation by setting
[[ionmov]] = 2. We need to calculate forces to be able to move the atoms, so we
set [[optforces]] = 1. In the provided `tpositron_4.abi` file, we only perform 4
relaxation steps ([[ntime]] = 4) to save time, but more steps would be needed to
converge the positron lifetime.

{% dialog tests/tutorial/Input/tpositron_4.abi %}

Look at the positron lifetime in the RPA limit after each ionic step:

	 Positron lifetime                         (ps)   =  2.55612619E+02
	 Positron lifetime                         (ps)   =  2.56978378E+02
	 Positron lifetime                         (ps)   =  2.82166606E+02
	 Positron lifetime                         (ps)   =  2.82878399E+02

As the vacancy relaxes outwards, the positron lifetime increases. 4 steps were
not enough to relax the defect completely, as the lifetime still changes.
Indeed, setting [[ntime]] to 10 delivers:

	 Positron lifetime                         (ps)   =  2.55612619E+02
	 Positron lifetime                         (ps)   =  2.56978379E+02
	 Positron lifetime                         (ps)   =  2.82166601E+02
	 Positron lifetime                         (ps)   =  2.82878398E+02
	 Positron lifetime                         (ps)   =  2.86515373E+02
	 Positron lifetime                         (ps)   =  2.86983434E+02
	 Positron lifetime                         (ps)   =  2.87266489E+02
	 Positron lifetime                         (ps)   =  2.87359897E+02
	 Positron lifetime                         (ps)   =  2.87313132E+02

Although the results at ionic steps 3 and 4 differ from each other by less
than one percent, they differ by more from the final result. The one percent
convergence is only reached at ionic step 5 and after.

Also, remember that the 16-atom supercell is not large enough to get converged
results. In Table IV of [[cite:Wiktor2015]] you can see converged
results of the positron lifetime of Si monovacancy within various methods.

## Computing the electron-positron momentum distribution (Doppler spectrum) of a Si lattice

In the last part of the tutorial we will calculate the electron-positron
momentum distribution (_Doppler spectrum_) of a silicon lattice in the conventional
scheme. This type of calculation is much more time and memory consuming than
the _lifetime_ calculation, as it is using the electron and positron
_wavefunctions_ (not only _densities_).

You can already issue:

    abinit tpositron_5.abi >& log

Now take a look at the input file `tpositron_5.abi`.

{% dialog tests/tutorial/Input/tpositron_5.abi %}

The momentum distribution calculation is activated by [[posdoppler]] = 1. You can also notice that instead
of having two datasets as in the first part of this tutorial, we now use the
automatic electron-positron loop and set [[posnstep]] = 2. This is done because
we need to have the full electron and positron wavefunctions in memory, which
is only the case when [[positron]] <= -10. Additionally, the momentum
distribution calculations require using a full k-point grid. In the input file we set:

	 kptopt 0          # Option for manual setting of k-points
	 istwfk *1         # No time-reversal symmetry optimization
	 nkpt 8            # Corresponds to a 2x2x2 grid
	 kpt               # K-point coordinates in reciprocal space:
	   0   0   0
	   0   0   0.5
	   0   0.5 0
	   0.5 0   0
	   0   0.5 0.5
	   0.5 0   0.5
	   0.5 0.5 0
	   0.5 0.5 0.5 

This grid is used in both electron and positron calculations, but only the
positron _wavefunction_ at the first point is taken in the momentum distribution
calculation, so the $\Gamma$ point should always be given first.

In the calculation of the momentum distribution, we need to include both _core_
and _valence_ electrons. The _wavefunctions_ of the core electrons are read from a
file (one per atom type), which needs to be provided. This _core WF file_ should
be named <span style="color:green">&#60;psp_file_name&#62;.corewf.xml</span>
(where `<psp_file_name>` is the name of the PAW atomic dataset file, without `.xml` suffix).
_Core WF files_ can be obtained with the `atompaw` tool
(see [the tutorial on generating PAW datasets (PAW2)](/tutorial/paw2) ) by the use of the
`prtcorewf` keyword. You will find the core wavefunction file used in this calculation in
`$ABI_PSPDIR/Si.LDA-PW-paw.abinit.corewf`.

!!! Note
	 If you use a PAW dataset in _ABINIT legacy proprietary format_ (with the `.abinit` suffix),
	 the core wavefunction file has to be named `<psp_file_name>.corewf.abinit`.
	 It also can be obtained with the `atompaw` tool by the use of the `prtcorewf` keyword.

Once the calculation is complete, you can find a `tpositron_5o_DOPPLER` file
containing the _momentum distribution_ on the FFT grid. You can use the
`$ABI_HOME/scripts/post_processing/posdopspectra.F90` tool to generate 1D
projections (_Doppler spectra_) in (001), (011) and (111) directions and to
calculate the low- and high-momentum contributions to the
momentum distribution (so called `S` and `W` parameters, see [[cite:Wiktor2015]]).

## Studying the effect of the PAW dataset completeness

The positron lifetime and momentum distribution calculations within the PAW
method are very sensitive to the number of valence electrons in the **PAW
dataset**. It is due to the fact that it is not easy to describe the positron
_wavefunction_, tending to zero at the nucleus, using the electron atomic
orbitals. The **PAW basis set** in this case needs to be more complete than only
for describing the electron _wavefunctions_.

The simplest way to make the **PAW
dataset** more complete is to include `semicore electrons`. It is also possible to
add the `partial waves` corresponding to the `semicore electrons` in the basis
used only for the positron wave function description, while keeping the
initial number of valence electrons (as done in [[cite:Wiktor2015]]).
However, this second method is less straightforward.

The previous calculations were done with only **4 valence electrons** (`3s` and `3p`).
We will now see what happens if we include the `2s` and `2p` states in the **PAW dataset**.
We use the `Si_paw_pw_12el.xml` PAW dataset which includes 8 additional valence electrons.

!!!Tip
	 To generate the new dataset we use the `atompaw` tool.  
	 To add `semicore states`, the input file is modified
	 as follows:

	 - Replace `c` by `v` for the selected orbitals in the _electronic configuration_ section
	 - Decrease the PAW augmentation radius (because semicore states are more localized)

     Don't forget to add `prtcorewf` keyword to create the core orbital file.

We can now rerun the lifetime calculation with the new atomic dataset:

{% dialog tests/tutorial/Input/tpositron_6.abi %}

    abinit tpositron_6.abi >& log

We now find the positron lifetime calculated in the RPA limit:

	########## Lifetime computation 2

	 # Zero-positron density limit of Arponen and Pajanne provided by Boronski & Nieminen
	   Ref.: Boronski and R.M. Nieminen, Phys. Rev. B 34, 3820 (1986)
	 # Enhancement factor of Boronski & Nieminen IN THE RPA LIMIT
	   Ref.: Boronski and R.M. Nieminen, Phys. Rev. B 34, 3820 (1986)

	 Positron lifetime                         (ps)   =  2.11470610E+02

This value is significantly lower than 223 ps achieved with 4 valence
electrons in the first step. **It is, therefore, very important to always test
the PAW dataset completeness for positron calculations**.

The PAW dataset completeness is even more important in the _Doppler spectra_
calculations. We will now recalculate the momentum distribution including 12
_valence electrons_ using `tpositron_7.abi`:

{% dialog tests/tutorial/Input/tpositron_7.abi %}

    abinit tpositron_7.abi >& log

Before processing the new `tpositron_7o_DOPPLER file`, you should copy files
`rho_001`, `rho_011`, `rho_111` from the fifth step to for instance `si4el_001`, `si4el_011` and `si4el_111`.
By plotting the _Doppler spectra_ in the (001) direction calculated with 4 and
12 valence electrons, you should obtain a figure like this:

![](positron_assets/doppler.png){width=60%}

The dataset with 4 valence electrons is **not complete enough** to describe the
positron **wavefunction** around the nucleus. This is reflected in the
unphysically high probability at high momenta in the spectrum.

Further explanation of the influence of the PAW dataset on the _Doppler spectra_
can be found in [[cite:Wiktor2015]]. In case you need to generate
your own dataset for momentum distribution calculations,
you can follow the [tutorial on generating PAW datasets (PAW2)](/tutorial/paw2).
---
authors: XG, RC
---

# Second (basic) tutorial

## The H<sub>2</sub> molecule, with convergence studies.

This tutorial aims at showing how to get converged values for the following physical properties:

* the bond length
* the atomisation energy

You will learn about the numerical quality of the calculations, then make
convergence studies with respect to the number of planewaves and the size of
the supercell, and finally consider the effect of the XC functional. The
problems related to the use of different pseudopotential are not examined.
You will also finish to read the [[help:abinit]].

This tutorial should take about 1 hour.

[TUTORIAL_README]

## Summary of the previous tutorial

We studied the H$_2$ molecule in a big box.
We used 10 Ha as cut-off energy, a 10x10x10 Bohr$^3$ supercell, the local-density approximation
(as well as the local-spin-density approximation) in the Perdew-Wang parametrization ([[ixc]] = -1012)
and a pseudopotential from the pseudodojo <http://www.pseudo-dojo.org/>.

At this stage, we compared our results:

* bond length: 1.486 Bohr
* atomisation energy at that bond length: 0.1704 Ha = 4.635 eV

with the experimental data (as well as theoretical data using a much more accurate technique than DFT)

  * bond length: 1.401 Bohr
  * atomisation energy: 4.747 eV

The bond length is rather bad (about 6% off), and the atomisation energy is a bit too low, 2.5% off.

## 2 The convergence in ecut (I)

**2.1.a** **Computing the bond length and corresponding atomisation energy in one run.**

*Before beginning, you might consider to work in a different subdirectory as for tutorial 1.
Why not Work2?*

Because we will compute many times the bond length and atomisation energy, it
is worth to make a single input file that will do all the associated operations.
You should try to use 2 datasets (try to combine *\$ABI_TESTS/tutorial/Input/tbase1_3.abi* with *tbase1_5.abi*).
Do not try to have the same position of the H atom as one of the H$_2$ atoms in the optimized geometry.

```sh
cd $ABI_TESTS/tutorial/Input
mkdir Work2
cd Work2
cp ../tbase2_1.abi .
```

The input file *tbase2_1.abi* is an example of file that will do the job,

{% dialog tests/tutorial/Input/tbase2_1.abi %}

while *tbase2_1.abo* is an example of output file:

{% dialog tests/tutorial/Refs/tbase2_1.abo %}

Execute the code with:

    abinit tbase2_1.abi > log &

The run should take less than one minute.

You should obtain the values:

           etotal1    -1.1182883137E+00
           etotal2    -4.7393103688E-01

and

            xcart1    -7.4307169181E-01  0.0000000000E+00  0.0000000000E+00
                       7.4307169181E-01  0.0000000000E+00  0.0000000000E+00

These are similar to those determined in [tutorial 1](/tutorial/base1),
although they have been obtained in one run.
You can also check that the residual forces are lower than `5.0d-4`.
Convergence issues are discussed in [[help:abinit#numerical-quality|section 6]] of the abinit help file, on numerical quality.
You should read it.
By the way, you have read many parts of the abinit help file!
You are missing the sections (or part of) [[help:abinit#2|2]], [[help:abinit#pseudopotential-files|5]],  [[help:abinit#numerical-quality|6]].

You are also missing the description of many input variables.
We suggest that you finish reading entirely the abinit help file now, while
the knowledge of the input variables will come in the long run.

**2.1.b** Many convergence parameters have already been identified. We will
focus only on [[ecut]] and [[acell]]. This is because

* the convergence of the SCF cycle and geometry determination are well
   under control thanks to [[toldfe]], [[toldff]] and [[tolmxf]]
   (this might not be the case for other physical properties)

* there is no k point convergence study to be done for an isolated system in a big box:
  no additional information is gained by adding a k-point beyond one

* the boxcut value (see [[boxcutmin]]) is automatically chosen larger than 2 by ABINIT, see the determination of the
  input variable [[ngfft]] by preprocessing

* we are using [[ionmov]] = 2 for the determination of the geometry.

## 3 The convergence in ecut (II)

For the check of convergence with respect to [[ecut]], you have the choice
between doing different runs of the *tbase2_1.abi* file with different values of
[[ecut]], or doing a double loop of datasets, as proposed in *$ABI_TESTS/tutorial/Input/tbase2_2.abi*.
The values of [[ecut]] have been chosen between 10 Ha and 35 Ha, by step of 5 Ha.
If you want to make a double loop, you might benefit of reading again the
[[help:abinit#loop|double-loop section]] of the abinit_help file.

**2.2.a** You have likely seen a big increase of the CPU time needed to do the
calculation. You should also look at the increase of the memory needed to do
the calculation (go back to the beginning of the output file).
The output data are as follows:

           etotal11   -1.1182883137E+00
           etotal12   -4.7393103688E-01
           etotal21   -1.1325211211E+00
           etotal22   -4.7860857539E-01
           etotal31   -1.1374371581E+00
           etotal32   -4.8027186429E-01
           etotal41   -1.1389569555E+00
           etotal42   -4.8083335144E-01
           etotal51   -1.1394234882E+00
           etotal52   -4.8101478048E-01
           etotal61   -1.1395511942E+00
           etotal62   -4.8107063412E-01

            xcart11   -7.4307169181E-01  0.0000000000E+00  0.0000000000E+00
                       7.4307169181E-01  0.0000000000E+00  0.0000000000E+00
            xcart12    0.0000000000E+00  0.0000000000E+00  0.0000000000E+00
            xcart21   -7.3687974546E-01  0.0000000000E+00  0.0000000000E+00
                       7.3687974546E-01  0.0000000000E+00  0.0000000000E+00
            xcart22    0.0000000000E+00  0.0000000000E+00  0.0000000000E+00
            xcart31   -7.3014665027E-01  0.0000000000E+00  0.0000000000E+00
                       7.3014665027E-01  0.0000000000E+00  0.0000000000E+00
            xcart32    0.0000000000E+00  0.0000000000E+00  0.0000000000E+00
            xcart41   -7.2642579309E-01  0.0000000000E+00  0.0000000000E+00
                       7.2642579309E-01  0.0000000000E+00  0.0000000000E+00
            xcart42    0.0000000000E+00  0.0000000000E+00  0.0000000000E+00
            xcart51   -7.2563260546E-01  0.0000000000E+00  0.0000000000E+00
                       7.2563260546E-01  0.0000000000E+00  0.0000000000E+00
            xcart52    0.0000000000E+00  0.0000000000E+00  0.0000000000E+00
            xcart61   -7.2554339763E-01  0.0000000000E+00  0.0000000000E+00
                       7.2554339763E-01  0.0000000000E+00  0.0000000000E+00
            xcart62    0.0000000000E+00  0.0000000000E+00  0.0000000000E+00

The corresponding atomisation energies and interatomic distances are:

| ecut (Ha)  |  atomisation energy (Ha)  | interatomic distance (Bohr)
| :--        | :--                       | :--
  10         | .1704                     |  1.486
  15         | .1753                     |  1.474
  20         | .1769                     |  1.460
  25         | .1773                     |  1.453
  30         | .1774                     |  1.451
  35         | .1774                     |  1.451

In order to obtain 0.2% relative accuracy on the bond length or atomisation
energy, one should use a kinetic cut-off energy of 25 Ha.
We will keep in mind this value for the final run.

## 4 The convergence in acell

The same technique as for [[ecut]] should be now used for the convergence in [[acell]].
We will explore [[acell]] starting from `8 8 8` to `18 18 18`, by step of `2 2 2`.
We keep [[ecut]] 10 for this study. Indeed, it is a rather general rule that there is
little cross-influence between the convergence of [[ecut]] and the convergence of [[acell]].

The file *$ABI_TESTS/tutorial/Input/tbase2_3.abi* can be used as an example.

{% dialog tests/tutorial/Input/tbase2_3.abi %}

The output results in *$ABI_TESTS/tutorial/Refs/tbase2_3.abo* are as follows:

           etotal11   -1.1305202335E+00
           etotal12   -4.8429570903E-01
           etotal21   -1.1182883137E+00
           etotal22   -4.7393103688E-01
           etotal31   -1.1165450484E+00
           etotal32   -4.7158917506E-01
           etotal41   -1.1165327748E+00
           etotal42   -4.7136118536E-01
           etotal51   -1.1167740301E+00
           etotal52   -4.7128698890E-01
           etotal61   -1.1168374331E+00
           etotal62   -4.7129589330E-01

            xcart11   -7.6471149217E-01  0.0000000000E+00  0.0000000000E+00
                       7.6471149217E-01  0.0000000000E+00  0.0000000000E+00
            xcart12    0.0000000000E+00  0.0000000000E+00  0.0000000000E+00
            xcart21   -7.4307169181E-01  0.0000000000E+00  0.0000000000E+00
                       7.4307169181E-01  0.0000000000E+00  0.0000000000E+00
            xcart22    0.0000000000E+00  0.0000000000E+00  0.0000000000E+00
            xcart31   -7.3778405090E-01  0.0000000000E+00  0.0000000000E+00
                       7.3778405090E-01  0.0000000000E+00  0.0000000000E+00
            xcart32    0.0000000000E+00  0.0000000000E+00  0.0000000000E+00
            xcart41   -7.3794243127E-01  0.0000000000E+00  0.0000000000E+00
                       7.3794243127E-01  0.0000000000E+00  0.0000000000E+00
            xcart42    0.0000000000E+00  0.0000000000E+00  0.0000000000E+00
            xcart51   -7.3742475720E-01  0.0000000000E+00  0.0000000000E+00
                       7.3742475720E-01  0.0000000000E+00  0.0000000000E+00
            xcart52    0.0000000000E+00  0.0000000000E+00  0.0000000000E+00
            xcart61   -7.3733248368E-01  0.0000000000E+00  0.0000000000E+00
                       7.3733248368E-01  0.0000000000E+00  0.0000000000E+00
            xcart62    0.0000000000E+00  0.0000000000E+00  0.0000000000E+00

The corresponding atomisation energies and interatomic distances are:

| acell (Bohr) | atomisation energy (Ha) | interatomic distance (Bohr)
| :--  | :-- | :--
8  | .1619 | 1.529
10 | .1704 | 1.486
12 | .1734 | 1.476
14 | .1738 | 1.478
16 | .1742 | 1.475
18 | .1742 | 1.475

In order to reach 0.2% convergence on the atomisation energy and interatomic distance one needs `acell 16 16 16`.
We will use `acell 16 16 16` for the final run.

For most solids the size of the unit cell will be smaller than that.
We are treating a lot of vacuum in this supercell !
So, the H$_2$ study, with this pseudopotential, turns out to be not really easy.
Of course, the number of states to be treated is minimal!
This allows to have reasonable CPU time still.

## 5 The final calculation in Local (Spin) Density Approximation

We now use the correct values of both [[ecut]] and [[acell]].
Well, you should modify the *tbase2_3.abi* file to make a calculation with `acell 16 16 16` and `ecut 25`.
You can still use the double loop feature with `udtset 1 2`
(which reduces to a single loop), to minimize the modifications to the file.

The file *$ABI_TESTS/tutorial/Input/tbase2_4.abi* can be taken as an example of input file:

{% dialog tests/tutorial/Input/tbase2_4.abi %}

while *$ABI_TESTS/tutorial/Refs/tbase2_4.abo* is as an example of output file:

{% dialog tests/tutorial/Refs/tbase2_4.abo %}

Since we are doing the calculation at a single ([[ecut]], [[acell]]) pair, the
total CPU time is not as much as for the previous determinations of optimal
values through series calculations.
However, the memory needs have still increased a bit.

The output data are:

           etotal11   -1.1369766875E+00
           etotal12   -4.7827555035E-01

            xcart11   -7.2259811794E-01  0.0000000000E+00  0.0000000000E+00
                       7.2259811794E-01  0.0000000000E+00  0.0000000000E+00
            xcart12    0.0000000000E+00  0.0000000000E+00  0.0000000000E+00

* The corresponding atomisation energy is `0.1804 Ha = 4.910 eV`
* The interatomic distance is 1.445 Bohr.
* These are our final data for the local (spin) density approximation.

Witou our choice of pseudopotential, the value of [[ixc]] was -1012, corresponding
to the Perdew-Wang [[cite:Perdew1992a]] parameterization of the LDA XC functional.
It is in principle the same as using [[ixc]] = 1.
Other expressions for the local (spin) density approximation [[ixc]]=[2, 3 ... 7] are possible.
The values 1, 2, 3 and 7 should give about the same results, since they all start
from the XC energy of the homogeneous electron gas, as determined by Quantum Monte Carlo calculations.
Other possibilities ([[ixc]] = 4, 5, 6) are older local density functionals, that could not rely on these data.

## 6 The use of the Generalized Gradient Approximation

We will use the Perdew-Burke-Ernzerhof functional proposed in [[cite:Perdew1996]]

For GGA, we use another pseudopotential than for LDA.
In principle we should redo the [[ecut]] convergence test, possibly coming to the conclusion
that another value of [[ecut]] should be use.
However, for the special case of Hydrogen, and in general pseudopotentials
with a very small core (including only the 1s orbital), pseudopotentials
issued from the LDA and from the GGA are very similar.
So, we will not redo an [[ecut]] convergence test.

!!! important

    *ecut* is often characteristic of the pseudopotentials that are used in a calculation.

Independently of the pseudopotential, an [[acell]] convergence test should not
be done again, since the vacuum is treated similarly in LDA or GGA.

So, our final values within GGA will be easily obtained by changing the pseudopotential with respect to the one used in *tbase2_4.abi*.

{% dialog tests/tutorial/Input/tbase2_5.abi %}

           etotal11   -1.1658082573E+00
           etotal12   -4.9940910146E-01

            xcart11   -7.0870975055E-01 -2.3009273766E-32 -6.6702161522E-32
                       7.0870975055E-01  2.3009273766E-32  6.6702161522E-32
            xcart12    0.0000000000E+00  0.0000000000E+00  0.0000000000E+00

* The corresponding atomisation energy is 0.1670 Ha = 4.544 eV
* The interatomic distance is 1.417 Bohr.
* These are our final data for the generalized gradient approximation.

Once more, here are the experimental data:

* bond length: 1.401 Bohr
* atomisation energy: 4.747 eV

In GGA, we are within 2% of the experimental bond length, but 5% of the experimental atomisation energy.
In LDA, we were within 3% of the experimental bond length, and about 3.5% of the experimental atomisation energy.

!!! important

    Do not forget that the typical accuracy of LDA and GGA varies with the class of materials studied.
    Usually, LDA gives too small lattice parameters, by 1...3%, while GGA gives too large lattice parameters, by 1...3% as well,
    but there might be classes of materials for which the deviation is larger.
    See e.g. [[cite:Lejaeghere2014]].
---
authors: AM, FR
---

# First tutorial on MULTIBINIT

## Build a harmonic lattice model and run a dynamics

This lesson aims at showing how to build a harmonic model by using a second-principles approach
for lattice dynamics simulations based on atomic potentials fitted on first-principles calculations.

**Before beginning, it is very important to read the reference [[cite:Wojdel2013]].**

With this lesson, you will learn to:

  * Compute the model from the DDB
  * Generate the XML for the model
  * Run a dynamics within MULTIBINIT

In this tutorial, all the knowledge about the Density Functional Theory (DFT) and Density Functional Perturbation Theory (DFPT) should have been already acquired.
In addition, the DFPT is a key feature of ABINIT for MULTIBINIT thus, in order to learn how to use the DFPT (producing the related DDB) and the associated code to merge different DDB files,
please have a look at the tutorials on [[lesson:rf1| Phonon response]], [[lesson:elastic|strain response]] and [[lesson:rf2| Mrgddb]] before to continue.
After these tutorials you should be able to compute a full DFPT calculation and a complete DDB file.
Thereby this tutorial will not provide the inputs for ABINIT, that you can find, however, in the previously cited tutorials.

The AGATE software, used to make the analysis of the results, is also required for this tutorial. You can install it on debian with:

    sudo add-apt-repository ppa:piti-diablotin/abiout
    sudo apt-get update && sudo apt-get install abiout

[TUTORIAL_README]

## 1 The Harmonic part of the lattice model

As described in [[cite:Wojdel2013]], the construction of a model starts by defining the reference structure (RS) of the system under study:

![Schema 1](lattice_model_assets/reference.png)
: Fig. 1: Example of cubic RS made by two different (black and red) atomic species.

The choice of this RS is fundamental since the model will be based on perturbations acting on it.
Once this choice is done, make sure than your system is fully relaxed and perform a single DFT calculation on this system in order to generate a DDB file.
In this tutorial, we will take as an example of a material without instabilities, the perovskite CaTiO3 in the Pnma phase.

From the RS, now we consider the perturbations: the set of atomic displacements $\boldsymbol{u}$ and lattice strain $\boldsymbol{\eta}$:

![Schema 2](lattice_model_assets/deformation.png)
: Fig. 2: Example of lattice perturbations: atomic displacements and strain.

At **the harmonic level**, we can express the potential energy as a sum of three contributions as a function of the set of perturbations ($\boldsymbol{u}$, $\boldsymbol{\eta}$):

$$\displaystyle  E^{harm}(\boldsymbol{u},\boldsymbol{\eta}) =  E^{harm}(\boldsymbol{u}) + E^{harm}(\boldsymbol{u},\boldsymbol{\eta}) + E^{harm}(\boldsymbol{\eta})$$

This calculation requires:

  * the computation of the phonon response (including short range and dipole-dipole interactions):

$$\displaystyle  E^{harm}(\boldsymbol{u}) \Longrightarrow \underbrace{\frac{\partial^2 E}{\partial
          \boldsymbol{u}^2}}_{\substack{\text{Inter-atomic}\\\text{force constants}}} ,
     \underbrace{\frac{\partial^2 E}{\partial
          {\boldsymbol{\cal{E}}}^2}}_{\text{Dielectric tensor}} ,
     \underbrace{\frac{\partial^2 E}{\partial{\boldsymbol{\cal{E}}} \partial \boldsymbol{u}}}_{\text{Effective charges}} $$

  * the computation of the strain response:
$$\displaystyle  E^{harm}(\boldsymbol{\eta}) \Longrightarrow \underbrace{\frac{\partial^2 E}{\partial
            \boldsymbol{\eta}^2}}_{\text{Elastic constants}} $$

  * the computation of the strain-phonon coupling:
$$\displaystyle E^{harm}(\boldsymbol{u},\boldsymbol{\eta}) \Longrightarrow \underbrace{\frac{\partial^2
            E}{\partial\boldsymbol{\eta}\partial\boldsymbol{u}}}_{\substack{\text{Internal strain}}} $$

We note that all the needed quantities related to the harmonic part can be computed by using the [[topic:DFPT|DFPT]] method and, once obtained, they are stored in a database (DDB) file.
In the case of an *insulator*, the dipole-dipole interaction will be recomputed directly by MULTIBINIT.
Thereby you need to provide into the DDB file the clamped-ion dielectric tensor and the Born effective charges.

!!! note

    Do not forget to include in the final DDB file the DDB obtained from the RS single DFT calculation (you can still use [[help:mrgddb | Mrgddb]]). Indeed, the DDB file is also an output of a DFT calculation and, in order to build of a model, it is important to include it in the DDB file to be used.

In this tutorial, we will take as an example of a material without lattice instabilities: the perovskite CaTiO$_3$ in its $Pnma$ phase.

*Optional exercise $\Longrightarrow$ Compute the phonon band structure with [[help:anaddb|Anaddb]]. You can download the complete DDB file (resulting from the previous calculations) here:*
{% dialog tests/tutomultibinit/Input/tmulti1_DDB %}


**Before starting, you might to consider working in a different subdirectory than for the other lessons. Why not create "Work_latticeModel"?**

The file ~abinit/tests/tutomultibinit/Input/tmulti1_1.files lists the file names and root names.
You can copy it in the **Work_latticeModel** directory and look at this file content, you should see:

      tutomulti1_1.abi
      tutomulti1_1.abo
      tmulti1_DDB

As mentioned in the guide of [[help:multibinit | MULTIBINIT]]:

   * "tutomulti1_1.abi" is the main input file
   * "tutomulti1_1.abo" is the main output
   * "tmulti1_DDB" is the DDB which contains the system definition and the list of the energy derivatives

It is now time to copy the file ~abinit/tests/tutomultibinit/Input/tmulti1_DDB and ~abinit/tests/tutomultibinit/Input/tmulti1_DDB in your **Work_latticeModel** directory.
You should read carefully the input file:

{% dialog tests/tutomultibinit/Input/tmulti1_1.abi %}

You should now run (it would take less than a second):

    multibinit < tmulti1_1.files > tmulti1_1_stdout

The resulting output file, tmulti1_1.abo, should be similar to the one below.
{% dialog tests/tutomultibinit/Refs/tmulti1_1.abo %}


The run you performed was aimed at reading the DDB file, generating the short range interatomic force constants and extract all the other informations related to the harmonic part of the model.
You can find inside the output file, the Born effective charges, the clamped-ion elastic tensor and the internal strain coupling parameters. Take some time to open and read the tmulti1_1.abo file.
If the DDB file is complete, the generation of the XML file requires only few input variables:

   * [[multibinit:prt_model]] = 1 $\Longrightarrow$ the generation of the XML file is activated, takes the time to read the possible options for [[multibinit:prt_model]].
   * [[multibinit:ngqpt]]    = 2 2 2 $\Longrightarrow$ specified the q-point mesh included in the tmulti1_DDB file
   * [[multibinit:dipdip]]   = 0 $\Longrightarrow$  it disables the computation of the dipole-dipole interaction: we don''t need to compute it to generate the XML file, MULTIBINIT will be able to regenerate the dipole-dipole interaction in a next run. We remind you that the default for this option is 1 and in the most part of your runs you will not use this option.

After this run, you should see in your directory tmulti1_1_model.xml, you can take some time to open and read this file. As you can see, it contains all the informations about the system definition (lattice parameters, atomic positions) and the data for the harmonic part of the potential.

Your XML file is now ready and you can use it as input for MULTIBINIT. To do that, copy now in your work directory the file ~abinit/tests/tutomultibinit/Input/tmulti1_2.files; you should see inside it:

      tutomulti1_2.abi
      tutomulti1_2.abo
      tmulti1_1_model.xml

Here, the DDB file is replaced by the XML file. Do not forget to copy the ~abinit/tests/tutomultibinit/Input/tutomulti1_2.abi in your directory before you run:

    multibinit < tmulti1_2.files > tmulti1_2_stdout

In tutomulti1_2.abi, [[multibinit:prt_model]] is still set to one so multibinit will write again the model XML file, which is useless at this stage, being a copy of the one read as input. Set this input variable to 0 and, in this case, MULTIBINIT will just read (and not write) the XML file.

With the two last examples, we have shown that MULTIBINIT is able to read either a DDB file or a XML as inputs for the system definition and the harmonic part of the potential.

We can now run our *first dynamics*: you can copy the files ~abinit/tests/tutomultibinit/Input/tutomulti1_3.* in your work directly and have a look them.

{% dialog tests/tutomultibinit/Input/tmulti1_3.abi %}

The simulation starts from the DDB to correctly account for the dipole-dipole interactions. You can visualize your dynamics with the agate software:

    agate tmulti1_3.abo_HIST.nc

Also try to use the effective potential from the xml file instead, in which the dipole-dipole interactions were not corrected. What do you see when you visualize the track?

* * *

This MULTIBINIT tutorial is now finished.
---
authors: XG
---

# Welcome

## Overview of the ABINIT tutorials.

These tutorials are aimed at teaching the use of ABINIT, in the
UNIX/Linux OS and its variants (MacOS, AIX etc.).
They might be used for other operating systems, but the commands have to be adapted.

At present, nearly forty tutorials are available. Each of them is at most
two hours of student work. Tutorials 1-4 cover the basics, other lectures are
more specialized. There are dependencies between tutorials.
The following schema should help you to understand these dependencies.

<center>
<map id="map1" name="map1">
<area shape="rect" coords="145  ,15 ,250 , 90 " href="base1/index.html" /> <!--2,3,4-->
<area shape="rect" coords="70  ,135 ,180 ,160 " href="spin/index.html" />
<area shape="rect" coords="70  ,165 ,129 ,184 " href="paw1/index.html" />
<area shape="rect" coords="130 ,165 ,144 ,184 " href="paw2/index.html" />
<area shape="rect" coords="145 ,165 ,180 ,184 " href="paw3/index.html" />
<area shape="rect" coords="70  ,185 ,180 ,230 " href="nuc/index.html" />
<area shape="rect" coords="70  ,235 ,124 ,257 " href="dftu/index.html" />
<area shape="rect" coords="125 ,235 ,180 ,257 " href="dmft/index.html" />
<area shape="rect" coords="70  ,258 ,125 ,280 " href="ucalc_crpa/index.html" />
<area shape="rect" coords="125 ,258 ,180 ,280 " href="udet/index.html" />
<area shape="rect" coords="70  ,285 ,180 ,310 " href="tddft/index.html" />
<area shape="rect" coords="70  ,315 ,180 ,335 " href="positron/index.html" />
<area shape="rect" coords="70  ,345 ,180 ,369 " href="ffield/index.html" />
<area shape="rect" coords="70  ,370 ,180 ,390 " href="cut3d/index.html" />
<area shape="rect" coords="70  ,395 ,180 ,420 " href="fold2bloch/index.html" />
<area shape="rect" coords="70  ,425 ,180 ,445 " href="wannier90/index.html" />
<area shape="rect" coords="70  ,450 ,180 ,475" href="../developers/developers_howto/index.html" />
<area shape="rect" coords="210  ,135 ,330 ,199 " href="rf1/index.html" />
<area shape="rect" coords="210  ,210 ,330 ,240 " href="optic/index.html" />
<area shape="rect" coords="210  ,250 ,330 ,280 " href="rf2/index.html" />
<area shape="rect" coords="210  ,300 ,330 ,325 " href="elastic/index.html" />
<area shape="rect" coords="210  ,326 ,289 ,379 " href="lattice_model/index.html" />
<area shape="rect" coords="290  ,326 ,330 ,379 " href="spin_model/index.html" />
<area shape="rect" coords="210  ,390 ,330 ,420 " href="nlo/index.html" />
<area shape="rect" coords="210  ,425 ,330 ,464 " href="eph_intro/index.html" />
<area shape="rect" coords="210  ,465 ,264 ,494 " href="eph_intro/index.html" />
<area shape="rect" coords="265  ,465 ,330 ,484 " href="eph4mob/index.html" />
<area shape="rect" coords="210  ,485 ,330 ,524 " href="eph4zpr/index.html" />
<area shape="rect" coords="355  ,400 ,440 ,440 " href="gw1/index.html" />
<area shape="rect" coords="355  ,460 ,440 ,479 " href="gw2/index.html" />
<area shape="rect" coords="355  ,480 ,440 ,520 " href="bse/index.html" />
<area shape="rect" coords="485  ,15 ,590 , 90 " href="basepar/index.html" />
<area shape="rect" coords="475  ,125 ,615 ,175 " href="paral_gspw/index.html" />
<area shape="rect" coords="475  ,180 ,615 ,235 " href="paral_moldyn/index.html" />
<area shape="rect" coords="475  ,240 ,615 ,295 " href="paral_images/index.html" />
<area shape="rect" coords="475  ,300 ,615 ,355 " href="paral_gswvl/index.html" />
<area shape="rect" coords="475  ,360 ,615 ,410 " href="paral_dfpt/index.html" />
<area shape="rect" coords="415  ,420 ,615 ,470 " href="paral_mbt/index.html" />
</map>
<img style="height: 540px; width: 720px;" alt="Schema 1" src="index_assets/tutorial_flowchart_v10.png" usemap="#map1"/>
</center>

The base tutorials are presented at the top, in orange. The blocks in red represents additional tutorials related
to ground-state features. Response-function features are explained
in the tutorials in the light brown blocks. Finally, the Many-Body Perturbation Theory
capabilities are demonstrated in the tutorials belonging to the dark brown blocks.
The right-hand side blocks gather the tutorials related to the parallelism inside ABINIT.

Visualisation tools are NOT covered in the ABINIT tutorials.
Powerful visualisation procedures have been developed in the Abipy context,
relying on matplotlib. See the README of [Abipy](https://github.com/abinit/abipy)
and the [Abipy tutorials](https://github.com/abinit/abitutorials).

Before following the tutorials, you should have read the [new user's guide](/guide/new_user),
as well as the pages
1045-1058 of [[cite:Payne1992|Rev. Mod. Phys. 64, 1045 (1992)]].
If you have more time, you should browse through the Chaps. 1 to 13, and appendices L and M of the book
[[cite:Martin2004|this book]] by R. M. Martin.
The latter reference is a must if you have not yet used another electronic structure code or a Quantum Chemistry package.

After the tutorial, you might find it useful to learn about the test cases
contained in the subdirectories of ~abinit/tests/, e.g. the directories fast,
v1, v2, ... , that provide many example input files.
You should have a look at the README files of these directories.

### Tutorials on basic concepts

* [The compilation howto tutorial](compilation) explains how to compile ABINIT from source 
  including the external libraries. 
  More advanced topics such as using **modules** in supercomputing centers,
  compiling and linking with the **intel compilers** and the **MKL library** as well as **OpenMP threads** 
  are also discussed.

**The tutorials 1-4 present the basic concepts, and form a global entity: you
should not skip any of these.**

* [The tutorial 1](base1) deals with the H2 molecule:
  get the total energy, the electronic energies, the charge density, the bond length, the atomisation energy

* [The tutorial 2](base2) deals again with the H2 molecule: convergence studies, LDA versus GGA

* [The tutorial 3](base3) deals with crystalline silicon (an insulator):
 the definition of a k-point grid, the smearing of the cut-off energy, the computation of a band structure,
 and again, convergence studies ...

* [The tutorial 4](base4) deals with crystalline aluminum (a metal), and its surface: occupation numbers,
  smearing the Fermi-Dirac distribution, the surface energy, and again, convergence studies ...

**Other tutorials present more specialized topics.**

### Miscellaneous advanced tutorials

**There is a group of tutorials that can be started without any other
prerequisite than the tutorials 1 to 4, and that you can do in any order (there are some exceptions, though):**

  * [The tutorial on spin in ABINIT](spin) presents the properties related to spin:
   spin-polarized calculations and spin-orbit coupling.

  * [The tutorial on the use of PAW (PAW1)](paw1) presents the Projector-Augmented Wave method,
    implemented in ABINIT as an alternative to norm-conserving pseudopotentials,
    with a sizeable accuracy and CPU time advantage.

  * [The tutorial on the generation of PAW atomic data files (PAW2)](paw2) presents the generation
    of atomic data for use with the PAW method. Prerequisite: PAW1.

  * [The tutorial on the validation of a PAW atomic datafile (PAW3)](paw3) demonstrates how to test
    a generated PAW dataset using ABINIT, against the ELK all-electron code, for diamond and magnesium.
    Prerequisite: PAW1 and PAW2.

  * [The tutorial on the properties of the nuclei](nuc) shows how to compute the electric field gradient.
    Prerequisite: PAW1.

  * [The tutorial on Wannier90](wannier90) deals with the Wannier90 library to obtain Maximally Localized Wannier Functions.

  * [The tutorial on polarization and finite electric field](ffield) deals with the computation
    of the polarization of an insulator (e.g. ferroelectric, or dielectric material) thanks
    to the Berry phase approach, and also presents the computation of materials properties
    in the presence of a finite electric field (also thanks to the Berry phase approach).

  * [The tutorial on electron-positron annihilation](positron) shows how to perform
    Two-Component Density-Functional Theory (TCDFT) calculations in the PAW framework
    to obtain the positron lifetime in the perfect material, the lifetime of a positron
    localized in a vacancy, the electron-positron momentum distribution wavefunctions and densities.

  * [The tutorial on cut3d](cut3d) deals with the use of the CUT3D utility to analyse wavefunctions and densities.

  * [The tutorial on the fold2bloch postprocessor](fold2bloch) deals with the use of the fold2Bloch utility
    to unfold band structures from supercell calculations.

  * [The tutorial on DFT+U](dftu) shows how to perform a DFT+U calculation using ABINIT,
    and will lead to compute the projected DOS of NiO. Prerequisite: PAW1.

  * [The tutorial on DFT+DMFT](dmft) shows how to perform a DFT+DMFT calculation on SrVO3
    using projected Wannier functions. Prerequisite: DFT+U.

  * [The tutorial on the calculation of effective interactions U and J by the cRPA method](ucalc_crpa)
    shows how to determine the U value with the constrained Random Phase Approximation
    using projected Wannier orbitals. Prerequisite: DFT+U.

  * [The tutorial on the determination of U for DFT+U](udet) shows how to determine the U value
    with the linear response method, to be used in the DFT+U approach. Prerequisite: DFT+U.

  * [The tutorial on TDDFT](tddft) deals with the computation of the excitation spectrum of finite systems,
    thanks to the Time-Dependent Density Functional Theory approach, in the Casida formalism.

  * [The "howto guide for developers"](../developers/developers_howto) introduces the new developer to the development
    of new functionalities in ABINIT.

### DFPT-related tutorials
**There is an additional group of tutorials on density-functional
perturbation theory (phonons, optics, dielectric constant, electron-phonon
interaction, elastic response, non-linear optics, Raman coefficients,
piezoelectricity ...), for which some common additional information are needed:**

  * [The tutorial DFPT 1 (RF1 - response function 1)](rf1) presents the basics of DFPT within ABINIT.
    The example given is the study of dynamical and dielectric properties of AlAs (an insulator):
    phonons at Gamma, dielectric constant, Born effective charges, LO-TO splitting, phonons in the whole Brillouin zone.
    The creation of the "Derivative Data Base" (DDB) is presented.

**The additional information given by tutorial DFPT1 opens the door to:**

  * [The tutorial DFPT 2 (RF2 - response function 2)](rf2) presents the analysis of the DDBs that have been
    introduced in the preceeding tutorial RF1. The computation of the interatomic forces and the computation
    of thermodynamical properties is an outcome of this tutorial.

  * [The tutorial on Optic](optic), the utility that allows to obtain
    the frequency dependent linear optical dielectric function and the frequency
    dependent second order nonlinear optical susceptibility, in the simple "Sum-Over-State" approximation.

**The additional information given by tutorials DFPT1 and DFPT2 (Optic not needed) opens the door to several tutorials:
the group of tutorials starting with the computation of elastic properties, followed by the two multibinit tutorials,
the tutorial on non-linear optics, and the group of electron-phonon tutorials:**

  * [The tutorial on the elastic properties](elastic) presents the computation with respect to
    the strain perturbation and its responses: elastic constants, piezoelectricity.

  * [The first tutorial on MULTIBINIT](lattice_model) presents the basic use of the MULTIBINIT application,
    how to build a harmonic model by using a second-principles approach for lattice dynamics
    simulations based on atomic potentials fitted on first-principles calculations.

  * [The second tutorial on MULTIBINIT](spin_model) presents how to build a spin model 
    and run spin dynamics in MULTIBINIT.

  * [The tutorial on static non-linear properties](nlo) presents the computation of responses beyond
    the linear order, within Density-Functional Perturbation Theory (beyond the simple Sum-Over-State approximation):
    Raman scattering efficiencies (non-resonant case), non-linear electronic susceptibility, electro-optic effect.
    Comparison with the finite field technique (combining DFPT calculations with finite difference calculations), is also provided.

  * [The introductive tutorial on the electron-phonon interaction](eph_intro) presents the electron-phonon driver 
    integrated with the ABINIT executable, discuss important technical details related to the implementation and the associated input variables. 
    The drawbacks/advantages with respect to the implementation available in ANADDB are also discussed.

  * [The tutorial on mobility calculations](eph4mob) explains how to compute phonon-limited carrier mobilities 
    in semiconductors within the relaxation time approximation (RTA)

  * [The tutorial on zero-point renormalization and temperature-dependence of electronic structure](eph4zpr) 
    explains how to obtain the electron self-energy due to phonons, compute the zero-point renormalization (ZPR) of the band gap 
    as well as temperature-dependent band gaps (or the whole electronic structure).

**Two obsolete tutorials on electron-phonon interaction are still present. 
The implementations are still available at time of writing,
but have been superseded by the new implementations, described in the above-mentioned tutorials.
Their prerequisite is also tutorials DFPT1 and DFPT2 (Optic not needed):**

  * [The obsolete tutorial on the electron-phonon interaction](eph) presents the use of the utility MRGKK and ANADDB
    to examine the electron-phonon interaction and the subsequent calculation of superconductivity temperature (for bulk systems).

  * [The obsolete tutorial on temperature dependence of the electronic structure](tdepes) presents the computation
    of the temperature dependence of the electronic structure,
    the zero-point motion effect, the lifetime/broadening of eigenenergies.

### MBPT-related tutorials
**There is another additional group of tutorials on many-body perturbation
theory (GW approximation, Bethe-Salpeter equation), to be done sequentially):**

  * [The first tutorial on GW (GW1)](gw1) deals with the computation of the quasi-particle band gap of Silicon (semiconductor),
    in the GW approximation (much better than the Kohn-Sham LDA band structure), with a plasmon-pole model.

  * [The second tutorial on GW (GW2)](gw2) deals with the computation of the quasi-particle band structure
    of Aluminum, in the GW approximation (so, much better than the Kohn-Sham LDA band structure)
    without using the plasmon-pole model.

  * [The tutorial on the Bethe-Salpeter Equation (BSE)](bse) deals with the computation
    of the macroscopic dielectric function of Silicon within the Bethe-Salpeter equation.

### Tutorials on parallelism
**Concerning parallelism, there is another set of specialized tutorials.
For each of these tutorials, you are supposed to be familiarized with the
corresponding tutorial for the sequential calculation.**

  * [An introduction on ABINIT in Parallel](basepar) should be read before going to the next tutorials about parallelism.
    One simple example of parallelism in ABINIT will be shown.

  * [Parallelism for ground-state calculations, with plane waves](paral_gspw) presents the combined
    k-point (K), plane-wave (G), band (B), spin/spinor parallelism of ABINIT (so, the "KGB" parallelism),
    for the computation of total energy, density, and ground state properties

  * [Parallelism for molecular dynamics calculations](paral_moldyn)

  * [Parallelism based on "images"](paral_images), e.g. for the determination of transitions paths
    (NEB or string method), or for PIMD, that can be activated alone, or on top of other parallelisms,
    e.g. the "KGB" parallelism for force calculations.

  * [Parallelism for ground-state calculations, with wavelets](paral_gswvl) presents the parallelism of ABINIT,
    when wavelets are used as a basis function instead of planewaves, for the computation
    of total energy, density, and ground state properties

  * [Parallelism of DFPT calculations](paral_dfpt)  you need to be familiarized with the calculation
   of response properties within ABINIT, see the tutorial [DFPT 1 (RF1)](rf1)

  * [Parallelism of Many-Body Perturbation calculations (GW)](paral_mbt) allows to speed up
    the calculation of accurate electronic structures (quasi-particle band structure, including many-body effects).

!!! note

    Note that not all features of ABINIT are covered by these tutorials.
    For a complete feature list, please see the [[topic:index|Topics page]].
    For examples on how to use these features, please see the ~abinit/tests/*
    directories and their accompanying README files.

To learn how to compile the code from source, please consult this guide:

<embed src="https://wiki.abinit.org/lib/exe/fetch.php?media=build:installing_abinit.pdf" type="application/pdf" width="100%" height="480px">
---
authors: XG
---

# Tutorial on TDDFT

## Time-Dependent Density Functional Theory, Casida's approach.

This tutorial aims at showing how to get the following physical properties, for finite systems:

  * Excitation energies
  * Associated oscillator strengths
  * Frequency-dependent polarizability and optical spectra

in the Casida approach, within Time-Dependent Density Functional Theory.

This tutorial should take about 30 minutes.

[TUTORIAL_README]

## Brief theoretical introduction

In order to perform Time-Dependent Density Functional Theory calculations (TDDFT)
of electronic excitations and oscillator strengths, in the Casida's approach,
you should first have some theoretical background.
TDDFT was first developed in the eighties, but the direct calculation of
electronic excitations was introduced much later, by Casida and co-workers.
A comprehensive description of the underlying formalism is given in

    "Time-Dependent Density Functional Response Theory of Molecular Systems:
     Theory, Computational Methods, and Functionals"
     M. E. Casida
     in Recent Development and Applications of Modern Density Functional Theory,
     edited by J.M. Seminario (Elsevier, Amsterdam, 1996), p. 391.
     <http://dx.doi.org/10.1016/S1380-7323(96)80093-8>

However this reference might be hard to get, that is why we have based the
tutorial instead on the following (also early) papers:
[[cite:Casida1998]], [[cite:Casida1998a]], and [[cite:Vasiliev1998]].

The first of these papers, [[cite:Casida1998]], will be used as main reference for our tutorial.

From these papers, you will learn that a TDDFT calculation of electronic
excitation energies start first from a usual ground-state calculation, with a
chosen XC functional. Such a calculation produces a spectrum of Kohn-Sham
electronic energies. It is widely known that differences between occupied and
unoccupied Kohn-Sham electronic energies resemble excitation energies (the
difference in energy between an excited state and the ground state), although
there is no real theoretical justification for this similarity.

These differences between Kohn-Sham electronic energies are the starting point
of Casida's approach: in the framework of TDDFT, their square give the main
contribution to the diagonal part of a matrix, whose eigenvalues will be the
square of the excitation energies. One has to add to the diagonal matrix made
from the squares of Kohn-Sham energy differences, a coupling matrix, whose
elements are four-wavefunction integrals of the Coulomb and exchange-correlation kernel.
The exchange-correlation kernel contribution will differ
in the spin-singlet and in the spin-triplet states, this being the only
difference between spin-singlet and spin-triplet states. See Eqs.(1.3) and
(1.4) of [[cite:Casida1998a]], and Eqs.(1-2) of [[cite:Vasiliev1998]].

The construction of the coupling matrix can be done on the basis of an
exchange-correlation kernel that is derived from the exchange-correlation
functional used for the ground-state, but this is not a requirement of the
theory, since such a correspondence only holds for the exact functional. In
practice, the approximation to the XC potential and the one to the XC kernel
are often different. See section III of [[cite:Casida1998]].

A big drawback of the currently known XC potentials and XC kernels is observed
when the system is infinite in at least one direction (e.g. polymers, slabs,
or solids). In this case, the addition of the coupling matrix is unable to
shift the edges of the Kohn-Sham band structure (each four-wavefunction
integral becomes too small). There is only a redistribution of the oscillator
strengths. In particular, the DFT band gap problem is NOT solved by TDDFT.
Also, the Casida's approach relies on the discreteness of the Kohn-Sham spectrum.

Thus, the TDDFT approach to electronic excitation energies in ABINIT is ONLY
valid for finite systems (atoms, molecules, clusters). Actually, only one
k-point can be used, and a "box center" must be defined, close to the center
of gravity of the system.

The Casida formalism also gives access to the oscillator strengths, needed to
obtain the frequency-dependent polarizability, and corresponding optical
spectrum. In the ABINIT implementation, the oscillators strengths are given as
a second-rank tensor, in cartesian coordinates, as well as the average over
all directions usually used for molecules and clusters. It is left to the user
to generate the polarisability spectrum, according to e.g. Eq.(1.2) of [[cite:Casida1998a]].

One can also combine the ground state total energy with the electronic
excitation energies to obtain Born-Oppenheimer potential energy curves for
excited states. This is illustrated for formaldehyde in [[cite:Casida1998a]].

Given its simplicity, and the relatively modest CPU cost of this type of
calculation, Casida's approach enjoys a wide popularity. There has been
hundreds of papers published on the basis of methodology. Still, its accuracy
might be below the expectations, as you will see. As often, test this method
to see if it suits your needs, and read the recent literature ...

## A first computation of electronic excitation energies and oscillator strengths, for N$_2$

We will now compute and analyse the excitation energies of the diatomic molecule N$_2$.
This is a rather simple system, with cylindrical symmetry,
allowing interesting understanding. Although we will suppose that you are
familiarized with quantum numbers for diatomic molecules, this should not play
an important role in the understanding of the way to use Abinit
implementation of Casida's formalism.

*Before beginning, you might consider to work in a different subdirectory as
for the other tutorials. Why not Work_tddft?*
Copy the file *ttddft_1.abi* in *Work_tddft*:

```sh
cd $ABI_TESTS/tutorial/Input
mkdir Work_tddft
cd Work_tddft
cp ../ttddft_1.abi .
```

So, issue now:

    abinit ttddft_1.abi > log &

The computation is quite fast: about 3 secs on a 2.8 GHz PC.
Let's examine the input file *ttddft_1.abi*.

{% dialog tests/tutorial/Input/ttddft_1.abi %}

There are two datasets: the first one corresponds to a typical ground-state
calculation, with only occupied bands. The density and wavefunctions are
written, for use in the second data set. The second dataset is the one where
the TDDFT calculation is done. Moreover, the non-self-consistent calculation
of the occupied eigenfunctions and corresponding eigenenergies is also
accomplished. This is obtained by setting [[iscf]] to -1.

Please, take now some time to read the information about this value of [[iscf]], and the few
input variables that acquire some meaning in this context (namely,
[[boxcenter]], [[td_mexcit]], and [[td_maxene]]). Actually, this is most of
the information that should be known to use the TDDFT in ABINIT!

You will note that we have 5 occupied bands (defined for dataset 1), and that
we add 7 unoccupied bands in the dataset 2, to obtain a total of 12 bands. The
box is not very large (6x5x5 Angstrom), the cutoff is quite reasonable, 25
Hartree), and as requested for the Casida's formalism, only one k point is
used. We are using the Perdew-Wang 92 LDA functional for both the self-
consistent and non-self-consistent calculations ([[ixc]] = -1012), as deduced
by abinit by looking at the pseudopotential file..

We can now examine the output file *ttddft_1.abo.*

{% dialog tests/tutorial/Refs/ttddft_1.abo %}

One can jump to the second dataset section, and skip a few non-interesting
information, in order to reach the following information:

     *** TDDFT : computation of excited states ***
     Splitting of  12 bands in   5 occupied bands, and   7 unoccupied bands,
     giving    35 excitations.

The matrix that is diagonalized, in the Casida's formalism, is thus a 35x35 matrix.
It will give 35 excitation energies.
Then, follows the list of excitation energies, obtained from the difference of
Kohn-Sham eigenvalues (occupied and unoccupied), for further reference. They
are ordered by increasing energy. In order to analyze the TDDFT as well as
experimental data in the next section, let us mention that the Kohn-Sham
eigenfunctions in this simulation have the following characteristics:

  * The first and fifth states are (non-degenerate) occupied $\sigma$ states (m=0), with even parity
  * The second state is a (non-degenerate) occupied $\sigma$ state (m=0), with odd parity
  * The third and fourth states are degenerate occupied $\pi$ states (m=+1,-1), with odd parity
  * The sixth and seventh states are degenerate unoccupied $\pi$ states (m=+1,-1), with even parity
  * The state 8 is a (non-degenerate) unoccupied $\sigma$ state (m=0), with even parity

Combining states 3,4 and 5 with 6, 7 and 8, give the first nine Kohn-Sham energy differences:

      Transition  (Ha)  and   (eV)   Tot. Ene. (Ha)  Aver     XX       YY       ZZ
       5->  6 3.06494E-01 8.34013E+00 -2.03684E+01 0.0000E+00 0.00E+00 0.00E+00 0.00E+00
       5->  7 3.06494E-01 8.34013E+00 -2.03684E+01 0.0000E+00 0.00E+00 0.00E+00 0.00E+00
       5->  8 3.50400E-01 9.53488E+00 -2.03245E+01 0.0000E+00 0.00E+00 0.00E+00 0.00E+00
       4->  6 3.60026E-01 9.79682E+00 -2.03149E+01 5.5534E-01 1.67E+00 0.00E+00 0.00E+00
       3->  6 3.60026E-01 9.79682E+00 -2.03149E+01 3.7821E-03 1.13E-02 0.00E+00 0.00E+00
       4->  7 3.60026E-01 9.79682E+00 -2.03149E+01 3.7822E-03 1.13E-02 0.00E+00 0.00E+00
       3->  7 3.60026E-01 9.79682E+00 -2.03149E+01 5.5534E-01 1.67E+00 0.00E+00 0.00E+00
       4->  8 4.03933E-01 1.09916E+01 -2.02710E+01 3.7976E-02 0.00E+00 1.14E-01 0.00E+00
       3->  8 4.03933E-01 1.09916E+01 -2.02710E+01 3.7976E-02 0.00E+00 0.00E+00 1.14E-01

Without the coupling matrix, these would be the excitation energies, for both
the spin-singlet and spin-triplet states. The coupling matrix modifies the
eigenenergies, by mixing different electronic excitations, and also lift some
degeneracies, e.g. the quadruplet formed by the combination of the degenerate
states 3-4 and 6-7 that gives the (four-fold degenerate) excitation energies 
with 3.60026E-01 Ha in the above table.

Indeed, concerning the spin-singlet, the following excitation energies are
obtained (see the next section of the output file):

      TDDFT singlet excitation energies (at most 20 of them are printed),
      and corresponding total energies.
      Excit#   (Ha)    and    (eV)    total energy (Ha)    major contributions
       1    3.45362E-01   9.39779E+00   -2.032952E+01    1.00(  5->  7)  0.00(  1->  7)
       2    3.45434E-01   9.39975E+00   -2.032945E+01    1.00(  5->  6)  0.00(  1->  6)
       3    3.60026E-01   9.79681E+00   -2.031486E+01    0.50(  3->  6)  0.50(  4->  7)
       4    3.68693E-01   1.00326E+01   -2.030619E+01    0.99(  5->  8)  0.00(  2-> 10)
       5    3.83765E-01   1.04428E+01   -2.029112E+01    0.50(  4->  7)  0.50(  3->  6)
       6    3.83798E-01   1.04437E+01   -2.029108E+01    0.50(  4->  6)  0.50(  3->  7)
       7    4.03285E-01   1.09740E+01   -2.027160E+01    0.99(  3->  8)  0.01(  4->  8)
       8    4.03304E-01   1.09745E+01   -2.027158E+01    0.99(  4->  8)  0.01(  3->  8)
       9    4.59051E-01   1.24914E+01   -2.021583E+01    0.91(  2->  8)  0.04(  3->  7)
       ...

The excitation energies are numbered according to increasing energies, in Ha
as well as in eV. The total energy is also given (adding excitation energy to
the ground-state energy), and finally, the two major contributions to each
of these excitations are mentioned (size of the contribution then identification).

It is seen that the first and second excitations are degenerate (numerical
inaccuracies accounts for the meV difference), and mainly comes from the first
and second Kohn-Sham energy differences (between occupied state 5 and
unoccupied states 6 and 7). This is also true for the fourth excitation, that
comes from the third Kohn-Sham energy difference (between occupied state 5 and
unoccupied state 8). The quadruplet of Kohn-Sham energy differences, that was
observed at 3.60026E-01 Ha, has been split into one doublet and two singlets,
with numbers 3 (the lowest singlet), 5-6 (the doublet) while the last singlet
is not present in the 20 lowest excitations.

The list of oscillator strength is then provided.

      Oscillator strengths :  (elements smaller than 1.e-6 are set to zero)
      Excit#   (Ha)   Average    XX        YY        ZZ         XY        XZ        YZ
       1 3.45362E-01 0.000E+00 0.000E+00 0.000E+00 0.000E+00  0.00E+00  0.00E+00  0.00E+00
       2 3.45434E-01 0.000E+00 0.000E+00 0.000E+00 0.000E+00  0.00E+00  0.00E+00  0.00E+00
       3 3.60026E-01 0.000E+00 0.000E+00 0.000E+00 0.000E+00  0.00E+00  0.00E+00  0.00E+00
       4 3.68693E-01 0.000E+00 0.000E+00 0.000E+00 0.000E+00  0.00E+00  0.00E+00  0.00E+00
       5 3.83765E-01 0.000E+00 0.000E+00 0.000E+00 0.000E+00  0.00E+00  0.00E+00  0.00E+00
       6 3.83798E-01 0.000E+00 0.000E+00 0.000E+00 0.000E+00  0.00E+00  0.00E+00  0.00E+00
       7 4.03285E-01 5.881E-02 0.000E+00 1.409E-03 1.750E-01  0.00E+00  0.00E+00 -1.57E-02
       8 4.03304E-01 5.685E-02 0.000E+00 1.692E-01 1.361E-03  0.00E+00  0.00E+00  1.52E-02
       9 4.59051E-01 8.613E-02 2.584E-01 0.000E+00 0.000E+00  0.00E+00  0.00E+00  0.00E+00
      10 4.61447E-01 0.000E+00 0.000E+00 0.000E+00 0.000E+00  0.00E+00  0.00E+00  0.00E+00
       ...

The first six transitions are forbidden, with zero oscillator strength. The
seventh and eighth transitions are allowed, with sizeable YY, YZ and ZZ components.

Next, one finds the excitation energies for the spin-triplet states:

      TDDFT triplet excitation energies (at most 20 of them are printed),
      and corresponding total energies.
      Excit#   (Ha)    and    (eV)    total energy (Ha)    major contributions
       1    2.84779E-01   7.74923E+00   -2.039010E+01    1.00(  5->  7)  0.00(  5->  6)
       2    2.84781E-01   7.74928E+00   -2.039010E+01    1.00(  5->  6)  0.00(  5->  7)
       3    2.97188E-01   8.08689E+00   -2.037770E+01    0.50(  3->  7)  0.50(  4->  6)
       4    3.30296E-01   8.98780E+00   -2.034459E+01    0.50(  4->  7)  0.50(  3->  6)
       5    3.30344E-01   8.98913E+00   -2.034454E+01    0.50(  4->  6)  0.50(  3->  7)
       6    3.43692E-01   9.35234E+00   -2.033119E+01    1.00(  5->  8)  0.00(  2-> 10)
       7    3.60026E-01   9.79681E+00   -2.031486E+01    0.50(  3->  6)  0.50(  4->  7)
       8    3.85278E-01   1.04839E+01   -2.028961E+01    0.91(  2->  7)  0.09(  3->  8)
       9    3.85310E-01   1.04848E+01   -2.028957E+01    0.91(  2->  6)  0.08(  4->  8)
       ...

Spin-triplet energies are markedly lower than the corresponding spin-singlet
energies. Also, the highest singlet derived from the Kohn-Sham quadruplet is
now the excitation number 7. The oscillator strengths also follow. At this
stage, we are in position to compare with experimental data, and try to
improve the quality of our calculation.

To summarize our results, we obtain the following five lowest-lying spin-
singlet excitation energies, with corresponding quantum numbers (that we
derive from the knowledge of the Kohn-Sham states quantum numbers):

    9.40 eV   m=+1,-1  even parity (Pi_g state)
    9.80 eV   m=0      odd parity  (Sigma_u state)
    10.03 eV  m=0      even parity (Sigma_g state)
    10.44 eV  m=+2,-2  odd parity  (Delta_u state)
    10.97 eV  m=+1,-1  odd parity  (Pi_u state)

and the following five lowest-lying spin-triplet excitations energies, with
corresponding quantum numbers:

    7.75 eV   m=+1,-1  even parity (Pi_g state)
    8.09 eV   m=0      odd parity  (Sigma_u state)
    8.99 eV   m=+2,-2  odd parity  (Delta_u state)
    9.35 eV   m=0      even parity (Sigma_g state)
    9.80 eV   m=0      odd parity  (Sigma_u state)

The quantum number related to the effect of a mirror plane, needed for $\Sigma$
states, could not be attributed on the sole basis of the knowledge of Kohn-
Sham orbitals quantum numbers.

The lowest-lying experimental spin-singlet excitation energies, see table III
of [[cite:Casida1998]], are as follows:

    9.31 eV   m=+1,-1  even parity (Pi_g state)
    9.92 eV   m=0      odd parity  (Sigma_u- state)
    10.27 eV  m=+2,-2  odd parity  (Delta_u state)

and the lowest-lying experimental spin-triplet excitations energies are:

    7.75 eV   m=0      odd parity  (Sigma_u+ state)
    8.04 eV   m=+1,-1  even parity (Pi_g state)
    8.88 eV   m=+2,-2  odd parity  (Delta_u state)
    9.67 eV   m=0      odd parity  (Sigma_u- state)

In several cases, the agreement is quite satisfactory, on the order of 0.1-0.2
eV. However, there are also noticeable discrepancies. Indeed, we have to understand, in our simulation:

  * The appearance of the spin-singlet $^1\Sigma_g$ state at 10.03 eV (Spin-singlet state 3)
  * The inversion between the spin-triplet $^3\Pi_g$ and $^3\Sigma_u$ states (Spin-triplet states 1 and 2)
  * The appearance of the spin-triplet $^3\Sigma_g$ state at 9.35 eV (Spin-triplet state 4)

Still, the agreement between these TDDFT values and the experimental values is
much better than anything that can be done on the sole basis of Kohn-Sham
energy differences, that are (for spin-singlet and -triplet):

    8.34 eV   m=+1,-1   even parity (Pi_g state)
    9.53 eV   m=0       odd parity  (Sigma_u state)
    9.80 eV   m=0(twice),+2,-2 odd parity  (Sigma_u and Delta_u states)
    10.99 eV  m=+1,-1   odd parity  (Pi_u state)

## Convergence studies

There are several parameters subject to convergence studies in this context:
the energy cut-off, the box size, and the number of unoccupied bands.

We will start with the number of unoccupied states. The only input parameter
to be changed in the input file is the value of nband2. The following results
are obtained, for nband2 = 12, 30, 60, 100 and 150 (Energies given in eV):

    Singlet 1 :  9.40   9.37   9.33   9.30   9.28
    Singlet 2 :  9.80   9.80   9.80   9.79   9.80
    Singlet 3 : 10.03   9.91   9.85   9.83   9.83
    Singlet 4 : 10.44  10.43  10.43  10.42  10.42
    Singlet 5 : 10.97  10.97  10.97  10.97  10.97
    Triplet 1 :  7.75   7.75   7.73   7.73   7.72
    Triplet 2 :  8.06   8.02   7.98   7.96   7.95
    Triplet 3 :  8.99   8.97   8.96   8.96   8.95
    Triplet 4 :  9.35   9.34   9.34   9.34   9.34
    Triplet 5 :  9.80   9.80   9.80   9.80   9.80

You might try to obtain one of these...

The computation with nband2 = 100 takes a bit more than 1 minute,
and gives a result likely converged within 0.01 eV.
Let us have a look at these data. Unfortunately, none of the above-
mentioned discrepancies with experimental data is resolved, although the
difference between the first and second spin-triplet states decreases
significantly. Although we see that at least 60 bands are needed to obtain
results converged within 0.05 eV, we will continue to rely on 12 bands to try
to understand the most important discrepancies, while keeping the CPU time to a low level.

We next try to increase the cut-off energy. Again, this is fairly easy. One
can e.g. set up a double dataset loop. The following results are obtained, for
ecut = 25, 30, 35 and 45 Ha:

    Singlet 1 :  9.40   9.37   9.36   9.36 
    Singlet 2 :  9.80   9.78   9.77   9.77 
    Singlet 3 : 10.03  10.03  10.04  10.04 
    Singlet 4 : 10.46  10.37  10.32  10.30
    Singlet 5 : 10.97  10.98  10.98  10.98
    Triplet 1 :  7.75   7.72   7.72   7.72 
    Triplet 2 :  8.09   8.06   8.06   8.06
    Triplet 3 :  8.99   8.97   8.96   8.96
    Triplet 4 :  9.35   9.35   9.35   9.35
    Triplet 5 :  9.80   9.78   9.77   9.77  

You might try to obtain one of these...

The computation with ecut=30 takes a couple of seconds
and gives a result likely converged within 0.01 eV. 
The modifications with respect to the results with ecut=25 Ha are quite small.

We finally examine the effect of the cell size. Again, this is fairly easy.
e keep 12 bands, but stick to ecut=30 Ha.
One can e.g. set up a double dataset loop. The following results are obtained,
for acell = (6 5 5), (8 7 7), (10 9 9), (12 11 11), (14 13 13), (16 15 15) and (20 19 19):

    Singlet 1 :  9.37   9.25   9.24   9.24   9.25   9.25   9.25
    Singlet 2 :  9.78   9.73   9.72   9.72   9.72   9.72   9.72
    Singlet 3 : 10.03  10.04  10.18  10.26  10.29  10.32  10.34
    Singlet 4 : 10.42  10.35  10.34  10.34  10.34  10.34  10.35
    Singlet 5 : 10.98  11.34  11.40  11.12  10.95  10.84  10.70
    Triplet 1 :  7.72   7.60   7.60   7.60   7.60   7.60   7.60
    Triplet 2 :  8.06   8.07   8.07   8.08   8.08   8.08   8.08
    Triplet 3 :  8.97   8.94   8.94   8.94   8.94   8.94   8.94
    Triplet 4 :  9.35   9.73   9.72   9.72   9.72   9.72   9.72
    Triplet 5 :  9.78   9.75  10.00  10.12  10.18  10.22  10.27

Obviously, the cell size plays an important role in the spurious appearance of
the states, that was remarked when comparing against experimental data.
Indeed, the singlet 3 and triplet 4 states energy increases strongly with the
cell size, while all other states quickly stabilize (except the still higher singlet 5 state).
Actually, the singlet 3 and 4 switch between (16 15 15) and (20 19 19),
and the triplet 4 and 5 switch between (6 5 5) and (8 7 7)  ...
The presence of the singlet 3 state (Sigma_g) in the three lowest ones,
an the triplet 4 state (Sigma_g) were two of our discrepancies.

There is one lesson to be learned from that convergence study: the
convergence of different states can be quite different. Usually, converging
the lower excited states do not require too much effort, while it is quite
difficult, especially concerning the supercell size, to converge higher states.

At this stage, we will simply stop this convergence study, and give the
results of an ABINIT calculation using ecut 30 Hartree, acell 10 9 9, and 100
bands, focusing only on those states already converged, then compare the results with other
LDA/TDLDA results (from [[cite:Casida1998]]) and experimental results:

                       present  Casida experimental
    Singlet Pi_g      :  9.20    9.05     9.31
    Singlet Sigma_u-  :  9.72    9.63     9.92
    Singlet Delta_u   : 10.33   10.22    10.27
    Triplet Sigma_u+  :  7.99    7.85     7.75
    Triplet Pi_g      :  7.59    7.54     8.04
    Triplet Delta_u   :  8.92    8.82     8.88
    Triplet Sigma_u-  :  9.72    9.63     9.67

Our calculation is based on pseudopotentials, while Casida's calculation is an
all-electron one. This fact might account for the discrepancy
between both calculations (maximal 0.15 eV - it is of course the user's responsibility to test
the influence of different pseudopotentials on his/her calculations). The
agreement with experimental data is on the order of 0.2 eV, with the exception
of the $^3\Pi_g$ state (0.45 eV). In particular, we note that LDA/TDLDA is
not able to get the correct ordering of the lowest two triplet states ... One
of our problems was intrinsic to the LDA/TDLDA approximation ...

## The choice of the exchange-correlation potential and kernel

As emphasized in [[cite:Casida1998]], choosing a different functional for the self-consistent part
(XC potential) and the generation of the coupling matrix (XC kernel) 
can give a better description of the higher-lying states. 
Indeed, a potential with a -1/r tail (unlike the LDA or GGA) like the van Leeuwen-Baerends one [[cite:VanLeeuwen1994]],
can reproduce fairly well the ionisation energy, giving a much
better description of the Rydberg states. Still, the LDA kernel works pretty well.

In order to activate this procedure, set the value of [[ixc]] in dataset 1 to the
SCF functional, and the value of ixc in dataset 2 to the XC functional to be
used for the kernel. Use pseudopotentials that agree with the SCF functional in dataset 1.
As of writing (end of 2020), the ABINIT implementation has a strong restriction on the XC kernels that can be used. They
must be of LDA-type. See the list of allowed [[ixc]] values in the description of [[iscf]]=-1.
---
authors: BAmadon
---

# Calculation of U and J using cRPA

## Using the constrained RPA to compute the U and J in the case of SrVO<sub>3</sub>.

This tutorial aims at showing how to perform a calculation of _U_ and _J_ in
Abinit using cRPA. This method is well adapted in particular to determine _U_
and _J_ as they can be used in DFT+DMFT. The implementation is described in [[cite:Amadon2014]].

It might be useful that you already know how to do PAW calculations using
ABINIT but it is not mandatory (you can follow the two tutorials on PAW in
ABINIT ([PAW1](/tutorial/paw1), [PAW2](/tutorial/paw2))).
The DFT+_U_ tutorial in ABINIT ([DFT+U](/tutorial/dftu)) might be useful to know some
basic variables about correlated orbitals.

The first GW tutorial in ABINIT ([GW](/tutorial/gw1)) is useful to learn how
to compute the screening, and how to converge the relevant parameters
(energy cutoffs and number of bands for the polarizability).

This tutorial should take two hours to complete (you should have access to more than 8 processors).

[TUTORIAL_README]

## 1 The cRPA method to compute effective interaction: summary and key parameters

The cRPA method aims at computing the effective interactions among correlated
electrons. Generally, these highly correlated materials contain rare-earth
metals or transition metals, which have partially filled _d_ or _f_ bands and
thus localized electrons. cRPA relies on the fact that screening processes can
be decomposed in two steps: Firstly, the bare Coulomb interaction is screened
by non correlated electrons to produce the effective interaction _W<sub>r</sub>_.

Secondly, correlated electrons screened this interaction to produce the fully
screening interaction _W_ [[cite:Aryasetiawan2004]]). However, the second
screening process is taken into account when one uses a method which describes
accurately the interaction among correlated electrons (such as Quantum Monte
Carlo within the DFT+DMFT method). So, to avoid a double counting of screening
by correlated electrons, the DFT+DMFT methods needs as an input the effective
interaction _W<sub>r</sub>_. The goal of this tutorial is to present the implementation
of this method using Projected Local Orbitals Wannier orbitals in ABINIT (The
implementation of cRPA in ABINIT is described in [[cite:Amadon2014]] and projected
local orbitals Wannier functions are presented in [[cite:Amadon2008]]). The
discussion about the localization of Wannier orbitals has some similarities
with the beginning on the DMFT tutorial (see [here](dmft.md#1) and [there](dmft.md#2))

Several parameters (both physical and technical) are important for the cRPA calculation:

  * **The definition of correlated orbitals.** The first part of the tutorial is similar to the DMFT tutorial
    and explains the electronic structure of SrVO<sub>3</sub> and will be used to understand the definition of
    Wannier orbitals with various extensions. Wannier functions are unitarily related to a selected
    set of Kohn Sham (KS) wavefunctions, specified in ABINIT by band index [[dmftbandi]], and [[dmftbandf]].
    Thus, as empty bands are necessary to build Wannier functions, it is required in DMFT or cRPA calculations
    that the KS Hamiltonian is correctly diagonalized: use high values for [[nnsclo]], and [[nline]]
    for cRPA and DMFT calculations and preceding DFT calculations. Another solution used in the present tutorial
    is to use a specific non self-consistent calculation to diagonalize the hamiltonian, as in _GW_ calculations.
    Concerning the localization or correlated orbitals, generally, the larger [[dmftbandf]]-[[dmftbandi]] is,
    the more localized is the radial part of the Wannier orbital. Finally, note that Wannier orbitals
    are used in DMFT and cRPA implementations but this is not the most usual choice of correlated orbitals
    in the DFT+_U_ implementation in particular in ABINIT (see [[cite:Amadon2008a]]).
    The relation between the two expressions is briefly discussed in [[cite:Geneste2017]].

  * **The constrained polarization calculation.** As we will discuss in section
    2, there are different ways to define the constrained polarizability, by suppressing in the polarization,
    some electronic transitions: As discussed in section I of [[cite:Amadon2014]], one can either suppress
    electronic transition inside a given subset of Kohn-Sham bands. In this case, one can use [[ucrpa]]=1,
    and choose the first and last bands defining the subset of bands by using variable [[ucrpa_bands]].
    One can also use an energy windows instead of a number of bands by using [[ucrpa_window]].
    Another solution is to use a weighting scheme which lowers the contribution of a given transition,
    as a function of the weight of correlated orbitals on the bands involved in the transition
    (see Eq. (5) and (7) in [[cite:Amadon2014]]). In this case, on still use the variable [[ucrpa_bands]].
    The variable will in this case be used to define Projected Local Orbitals Wannier functions that will then
    be used to define the weighting function in the polarizability. (see Eq. (5) and (7) in [[cite:Amadon2014]]).

  * **Convergence parameters** are the same as for a polarizability calculation for the screening calculation
    ([[ecuteps]], [[ecutwfn]], [[nband]]) and the same as for the exchange part of _GW_ for the effective interaction,
    ie [[ecutsigx]], [[ecutwfn]]. It is recommended, as discussed in appendix A of [[cite:Amadon2014]], to use [[pawoptosc]]=1.
    If the calculation is carried out with several atoms, use [[nsym]]=1, the calculation is not symmetrized
    over atoms in this case.

## 2 Electronic Structure of SrVO3 in LDA

*Before continuing, you might consider to work in a different subdirectory as
for the other tutorials. Why not Work_crpa?
In what follows, the name of files are mentioned as if you were in this subdirectory.
All the input files can be found in the `$ABI_TESTS/tutoparal/Input` directory.*

Copy the files *tucrpa_1.abi* from *ABI_TESTS/tutoparal/Input* to *Work_crpa* with:

```sh
cd $ABI_TESTS/tutoparal/Input
mkdir Work_crpa
cd Work_crpa
cp ../tucrpa_1.abi .
```

and run the code with:

    mpirun -n 32 abinit tucrpa_1.abi > log_1  &

This run should take some time. It is recommended that you use at least 10
processors (and 32 should be fast). It calculates the LDA ground state of
SrVO3 and compute the band structure in a second step. The variable
[[pawfatbnd]] allows to create files with "fatbands" (see description of the
variable in the list of variables): the width of the line along each k-point
path and for each band is proportional to the contribution of a given atomic
orbital on this particular Kohn Sham Wavefunction. A low cutoff and a small
number of k-points are used in order to speed up the calculation. During this
time you can take a look at the input file.

There are two datasets.
The first one is a ground state calculations with [[nnsclo]]=3 and [[nline]]=3 in order
to have well diagonalized eigenfunctions even for empty states. In practice,
you have however to check that the residue of wavefunctions is small at the
end of the calculation. In this calculation, we find 1.E-06, which is large
(1.E-10 would be better, so [[nline]] and [[nnsclo]] should be increased, but
it would take more time). When the calculation is finished, you can plot the
fatbands for Vanadium and *l=2* with

    xmgrace tucrpa_O_DS2_FATBANDS_at0001_V_is1_l0002 -par ../Input/tdmft_fatband.par

The band structure is given in eV.

![FatbandV](ucalc_crpa_assets/fatbandsV.jpeg)

and the fatbands for all Oxygen atoms and *l=1* with

    xmgrace tucrpa_O_DS2_FATBANDS_at0003_O_is1_l0001 tucrpa_O_DS2_FATBANDS_at0004_O_is1_l0001 tucrpa_O_DS2_FATBANDS_at0005_O_is1_l0001 -par ../Input/tdmft_fatband.par

![FatbandV](ucalc_crpa_assets/fatbandsO.jpeg)

In these plots, you recover the band structure of SrVO3 (see for comparison
the band structure of Fig.3 of [[cite:Amadon2008]]), and the main character of the
bands. Bands 21 to 25 are mainly _d_ and bands 12 to 20 are mainly oxygen _p_.
However, we clearly see an important hybridization. The Fermi level (at 0 eV)
is in the middle of bands 21-23.

One can easily check that bands 21-23 are mainly _d-t<sub>2g</sub>_ and bands 24-25 are
mainly _e<sub>g</sub>_: just use [[pawfatbnd]] = 2 in *tucrpa_1.abi* and relaunch the
calculations. Then the file *tucrpa_O_DS2_FATBANDS_at0001_V_is1_l2_m-2*,
*tucrpa_O_DS2_FATBANDS_at0001_V_is1_l2_m-1* and
*tucrpa_O_DS2_FATBANDS_at0001_V_is1_l2_m1* give you respectively the _xy_, _yz_ and
_xz_ fatbands (ie _d-t<sub>2g</sub>_) and *tucrpa_O_DS2_FATBANDS_at0001_V_is1_l2_m+0* and
*tucrpa_O_DS2_FATBANDS_at0001_V_is1_l2_m+2* give the _z<sup>2</sup>_ and _x<sup>2</sup>-y<sup>2</sup>_ fatbands
(ie _e<sub>g</sub>_).

So in conclusion of this study, the Kohn Sham bands which are mainly _t<sub>2g</sub>_
are the bands 21, 22 and 23.

Of course, it could have been anticipated from classical crystal field theory:
the vanadium is in the center of an octahedron of oxygen atoms, so _d_
orbitals are split in _t<sub>2g</sub>_ and _e<sub>g</sub>_. As _t<sub>2g</sub>_ orbitals are not directed
toward oxygen atoms, _t<sub>2g</sub>_-like bands are lower in energy and filled with one
electron, whereas _e<sub>g</sub>_-like bands are higher and empty.

In the next section, we will thus use the _d_ -like bands to built Wannier
functions and compute effective interactions for these orbitals.

## 3 Definition of screening and orbitals in cRPA: models, input file and log file

### 3.1. The constrained polarization calculation.

As discussed briefly in Appendix A of [[cite:Amadon2014]] as well as in section III
B of [[cite:Vaugier2012]] and Section II B of [[cite:Sakuma2013]], one can use different
schemes for the cRPA calculations. Let us discuss these different models
namely the ( _d-d_ ), ( _dp-dp_ ), ( _d-dp_ (a)), and ( _d-dp_ (b)) models.
In the notation (A-B), A and B refers respectively to some bands of A-like and
B-like character. Moreover, A refers to the definition of screening and B
refers to the definition of correlated orbitals. To clarify this definition,
we give below some examples:

  * ( ***t<sub>2g</sub>-t<sub>2g</sub>***) model (or _t<sub>2g</sub>_ model): The correlated orbitals are defined with only _t<sub>2g</sub>_-like bands (bands 21, 22 and 23). The screening inside these bands is not taken into account to built the constrained polarizability. Note that if this case (and in the ( _d-d_ ) and ( _dp-dp_ ) models), using [[ucrpa]]=1 or 2 give the same results, provided one uses [[ucrpa_bands]] equal to ([[dmftbandi]] [[dmftbandf]] ).
  * ( ***d-d*** ) model (or d model): The correlated orbitals are defined with only _d_ -like bands (bands 21 to 25). The screening inside these bands is not taken into account.
  * ( ***dp-dp*** ) model (or dp model): The correlated orbitals are defined with only _d_ -like and Op-like bands (bands 12 to 23). The screening inside these bands is not taken into account.
  * ( ***d-dp (a)***) model: In this scheme the Wannier orbitals are constructed as in the ( _dp-dp_ ) model. However, in this scheme, one only supresses the screening inside _d_ -like bands ([[ucrpa]]=1). It is coherent with the fact that the DMFT will only be applied to the d orbitals: so the screening for Op-like bands need to be taken into account.
  * ( ***d-dp (b)***) model: It is the same model as the ( _d-dp_ (a)), nevertheless, in this case, only the weighting scheme is used ([[ucrpa]]=2 in ABINIT).
Which way of computing interactions is the most relevant depends on the way
the interactions will be used. Some aspects of it are discussed in Ref.
[[cite:Vaugier2012]]. Also, for Mott insulators, and if self-consistency over
interactions is carried out, the choice of models is discussed in [[cite:Amadon2014]].

### 3.2. The input file for cRPA calculation: correlated orbitals, Wannier functions

In this section, we will present the input variables and discuss how to
extract useful information in the log file in the case of the _d-d_ model. The
input file for a typical cRPA calculation (*tucrpa_2.abi*) contains four datasets
(as usual _GW_ calculations, see the [GW tutorial](gw1.md#1a)): the
first one is a well converged LDA calculation, the second is non self-consistent calculation
to compute accurately full and empty states, the third
computes the constrained non interacting polarizability, and the fourth
computes effective interaction parameters _U_ and _J_. We discuss these four
datasets in the next four subsections.

Copy the file in your *Work_crpa* directory with:

```sh
cp ../tucrpa_2.abi .
```

The input file *tucrpa_2.abi* contains standard data to perform a LDA
calculation on SrVO<sub>3</sub>. We focus in the next subsections on some peculiar input
variables related to the fact that we perform a cRPA calculation. Before
reading the following section, launch the abinit calculation:

    abinit tucrpa_2.abi > log_2

##### 3.2.1. The first DATASET: A converged LDA calculation

The first dataset is a simple calculation of the density using LDA using 50
bands. We do not use symmetry in this calculation, so [[nsym]]=1. If symmetry
is used ([[nsym]]=0), then the calculation of _U_ and _J_ will be valid, but
the full interaction matrix described in section 3.2.4 will not be correct.

##### 3.2.2. The second DATASET: A converged LDA calculation and definition of Wannier functions

Before presenting the input variables for this dataset, we discuss two
important physical parameters relevant to this dataset.

  * Diagonalization of Kohn-Sham Hamiltonian: As in the case of DFT+DMFT or _GW_ calculation, a cRPA calculation requires that the LDA is perfectly converged and the Kohn Sham eigenstates are precisely determined, including the empty states. Indeed these empty states are necessary both to build Wannier functions and to compute the polarizability. For this reason we choose a very low value of [[tolwfr]] in the input file tucrpa_1.abi.

  * Wannier functions: Once the calculation is converged, we compute Wannier functions, as in a DFT+DMFT calculation. To do this, we only precise that we are using the DFT+DMFT implementation (usedmft=0), but only with the Wannier keywords ([[dmftbandi]] and [[dmftbandf]]). We emphasize that with respect to the discussion on models on section 3.1, [[dmftbandi]] and [[dmftbandf]] are used to define the so called A bands. We will see in dataset 2 how B bands are defined. In our case, as we are in the _d-d_ model, we choose only the _d_ -like bands as a starting point and [[dmftbandi]] and [[dmftbandf]] are thus equal to the first and last _d_ -like bands, namely 21 and 25.

```
    iscf2          -2    # Perform a non self-consistent calculation
    nbandkss2      -1    # Number of bands in KSS file (-1 means the
                         #   maximum possible)
    kssform         3    # Format of the Wavefunction file (should be 3)
    nbdbuf2         4    # The last four bands will not be perfectly
                         # diagonalized
    tolwfr2   1.0d-18    # The criterion to stop diagonalization

    # Compute Wannier functions
    usedmft2        1    # Mandatory to enable the calculation of Wannier
                         #  functions as in DMFT.
    dmftbandi2     21    # Precise the definition of Wannier functions
    dmftbandf2     25    # Precise the definition of Wannier functions
```

##### 3.2.2. The third DATASET: Compute the constrained polarizability and dielectric function

The third DATASET drives the computation of the constrained polarizability and
the dielectric function. Again, we reproduce below the input variables
peculiar to this dataset.

We add some comments here on three most important topics

  * Definition of constrained polarizability: As in the discussion on models on section 3.1, the constrained polarizability is defined thanks to B-like bands. The keywords related to this definition is [[ucrpa_bands]]. According to the value of ucrpa, [[ucrpa_bands]] defined the bands for which the transitions are neglected, or (if ucrpa=2), it defined Wannier functions that are used in the weighting scheme (see Eq. (5) and (7) of section II of [[cite:Amadon2014]]). Alternatively and only if ucrpa=1, an energy windows can be defined (with variable [[ucrpa_window]]) to exclude the transition. In our case, as we are in the _d-d_ model, we neglect transitions inside the _d_ -like bands, so we choose [[ucrpa_bands]]= 21 25.
  * Convergence of the polarizability: [[nband]] and [[ecuteps]] are the two main variables that should be converged. Note that one must study the role of these variables directly on the effective interaction parameters to determine their relevant values.
  * Frequency mesh  of the polarizability: Can be useful if one wants to plot the frequency dependence of the effective interactions (as in e.g. [[cite:Amadon2014]]).

```
optdriver3     3     # Keyword to launch the calculation of screening
gwcalctyp3     2     # The screening will be used later with gwcalctyp 2
                     # in the next dataset
getwfk3       -1     # Obtain WFK file from previous dataset
ecuteps3     5.0     # Cut-off energy of the planewave set to represent
                     #   the dielectric matrix.
                     # It is important to converge effective interactions
                     #   as a function of this parameter.

# -- Frequencies for dielectric matrix
nfreqre3       1     # Number of  real frequencies
freqremax3    10 eV  # Maximal value of frequencies
freqremin3     0 eV  # Minimal value of frequencies
nfreqim3       0     # Number of  imaginary frequencies

# -- Ucrpa: screening
ucrpa_bands3  21 25  # Define the bands corresponding to the _d_ contribution

# -- Parallelism
gwpara3        1
```

##### 3.2.3. The fourth DATASET: Compute the effective interactions

The fourth DATASET drives the computation of the effective interactions. It is
similar computationally to the computation of exchange in _GW_ calculations.
Again, we reproduce below the input variables peculiar to this dataset.

We add some comments on convergence properties

  * Convergence as the number of plane waves:  Eq. (A1) and (A2) in Appendix A of [[cite:Amadon2014]]
    show the summation over plane waves which is performed. The cutoff on plane wave which is used
    in this summation is [[ecutsigx]] because of the similarity of Eq. A2 with the Fock exchange
    (see [[theory:mbt#evaluation_gw_sigma|GW notes]] when one computes the bare interaction. [[ecutsigx]]
    is thus the main convergence parameter for this dataset. Note that as discussed in Appendix A of [[cite:Amadon2014]],
    high values of [[ecutsigx]] can be necessary because oscillator matrix elements in PAW contains
    the Fourier transform of (a product of) atomic orbitals.

  * The frequency mesh  must be the same as in the DATASET 2.

```
     optdriver4  4      # Self-Energy calculation
     gwcalctyp4  2      # activate HF or ucrpa
     getwfk4     1      # Obtain WFK file from dataset 1
     getscr4     2      # Obtain SCR file from previous dataset
     ecutsigx4  30.0    # Dimension of the G sum in Sigma_x.

     # -- Frequencies for effective interactions
     nfreqsp4    1
     freqspmax4 10 eV
     freqspmin4  0 eV
     nkptgw4     0      # number of k-point where to calculate
                        # the GW correction: all BZ is used
     mqgrid4   300      # Reduced but fine at least for SrVO3
     mqgriddg4 300      # Reduced but fine at least for SrVO3

     # -- Parallelism
     gwpara4 2          # do not change if nsppol=2
```


#####  3.2.4 The cRPA calculation: the log file (for the _d-d_ model)

We are now going to browse quickly the log file for this calculation.

  * Bare interaction of atomic orbitals.

First, at the beginning of the log file, you will find the calculation of the
bare interaction for the atomic wavefunction corresponding the angular
momentum specified by [[lpawu]], in the input file (here [[lpawu]] =2). It is
a simple fast direct integration. Note that this information is complete only
if you use XML PAW atomic data (as the one available on the JTH table
available on the ABINIT website).

```
      =======================================================================
      == Calculation of diagonal bare Coulomb interaction on ATOMIC orbitals
         (it is assumed that the wavefunction for the first reference
                 energy in PAW atomic data is an atomic eigenvalue)

     Max value of the radius in atomic data file   =    201.3994
     Max value of the mesh   in atomic data file   =    910
     PAW radius is                                 =      2.2000
     PAW value of the mesh for integration is      =    587
     Integral of atomic wavefunction until rpaw    =      0.8418

     For an atomic wfn truncated at rmax =    201.3994
         The norm of the wfn is                    =      1.0000
         The bare interaction (no renormalization) =     17.7996 eV
         The bare interaction (for a renorm. wfn ) =     17.7996 eV

     For an atomic wfn truncated at rmax =      2.2000
         The norm of the wfn is                    =      0.8418
         The bare interaction (no renormalization) =     16.0038 eV
         The bare interaction (for a renorm. wfn ) =     22.5848 eV
     =======================================================================
```

Various quantities are computed, the one which is interesting to see here, is
the bare interaction computed for a wavefunction truncated at a very large
radius (here 201.3994 au ), ie virtually infinite. We find that this bare
interaction is 17.8 eV. We will compare below this value to the bare
interaction computed on Wannier orbitals.

  * Bare Coulomb direct interaction of Wannier orbitals. In the third dataset, the code writes the Coulomb interaction matrix as:

```
          U'=U(m1,m2,m1,m2) for the bare interaction
     -      1      2      3      4      5
       1 16.041 14.898 14.885 14.898 15.715
       2 14.898 16.041 15.508 14.898 15.092
       3 14.885 15.508 16.567 15.508 15.114
       4 14.898 14.898 15.508 16.041 15.092
       5 15.715 15.092 15.114 15.092 16.567
```


 * First we see that diagonal interactions are larger than off-diagonal terms,
   which is logical, because electron interaction is larger if electrons are located in the same orbital.
 * We recover in these interaction matrix the degeneracy of _d_ orbitals in the cubic symmetry
   (we remind, as listed in [[dmatpawu]], that the order of orbitals in ABINIT are _xy_, _yz_,
   _z<sup>2<\sup>_, _xy_, _x<sup>2</sup>-y<sup>2</sup>_).
 * We note also that the interaction for _t<sub>2g</sub>_ and _e<sub>g</sub>_ orbitals are not the same.
   This effect is compared in e.g. Appendix C.1 of [[cite:Vaugier2012]] to the usual
   Slater parametrization of interaction matrices.
 * Diagonal bare interactions are smaller to the bare interaction on atomic orbitals
   (17.8 eV, as discussed above). It shows that the Wannier functions (obtained within the _d-d_ model)
   are less localized than atomic orbitals. It is logical because these Wannier functions contain indeed
   a weight on O- _p_ orbitals.
 * From this interaction matrix, one can compute the average of all elements (the famous _U_ ),
   as well as the average of diagonal elements, which is sometimes used as another definition of _U_
   (see Eq. (8) and (9) of [[cite:Amadon2014]]. The two quantities are written in the log file:

```
    Hubbard bare interaction U=1/(2l+1)**2 \sum U(m1,m2,m1,m2)=   15.3789    0.0000

(Hubbard bare interaction U=1/(2l+1) \sum U(m1,m1,m1,m1)=   16.2514    0.0000)
```

!!! important

    To speed up the calculation and if one needs only these average values of _U_
    and _J_, one can use [[nsym]] = 0 in the input file, but in this case, the
    interaction matrix does not anymore reproduce the correct symmetry. For
    several correlated atoms, the implementation is under test and it is mandatory to use [[nsym]]=1.]

  * Bare exchange interaction. The exchange interaction matrix is also written as well
    as the average of off-diagonal elements, which is _J_.

```
       Hund coupling J=U(m1,m1,m2,m2) for the bare interaction
         1      2      3      4      5
    1 16.041  0.482  0.555  0.478  0.249
    2  0.482 16.041  0.329  0.478  0.478
    3  0.555  0.329 16.567  0.400  0.555
    4  0.478  0.478  0.400 16.041  0.482
    5  0.249  0.478  0.555  0.482 16.567

    bare interaction value of J=U-1/((2l+1)(2l)) \sum_{m1,m2} (U(m1,m2,m1,m2)-U(m1,m1,m2,m2))=    0.6667    0.0000
```


  * Then, the cRPA effective interactions are given for all frequency.
     The first frequency is zero and the cRPA interactions are:

```
     U'=U(m1,m2,m1,m2) for the cRPA interaction
       1      2      3      4      5
  1  3.434  2.432  2.338  2.432  2.966
  2  2.432  3.434  2.809  2.432  2.495
  3  2.338  2.809  3.641  2.809  2.430
  4  2.432  2.432  2.809  3.434  2.495
  5  2.966  2.495  2.430  2.495  3.641

 Hubbard cRPA interaction for w =  1, U=1/(2l+1)**2 \sum U(m1,m2,m1,m2)=    2.7546   -0.0000

 (Hubbard cRPA interaction for w =   1, U=1/(2l+1) \sum U(m1,m1,m1,m1)=    3.5167   -0.0000)

 Hund coupling J=U(m1,m1,m2,m2) for the cRPA interaction
       1      2      3      4      5
  1  3.434  0.440  0.508  0.435  0.247
  2  0.440  3.434  0.315  0.436  0.442
  3  0.508  0.315  3.641  0.378  0.444
  4  0.435  0.436  0.378  3.434  0.446
  5  0.247  0.442  0.444  0.446  3.641

  cRPA interaction value of J=U-1/((2l+1)(2l)) \sum_{m1,m2} (U(m1,m2,m1,m2)-U(m1,m2,m2,m1))=    0.5997    0.0000
```

  * At the end of the calculation, the value of _U_ and _J_ as a function of frequency are gathered.

```
       -------------------------------------------------------------
                Average U and J as a function of frequency
       -------------------------------------------------------------
             omega           U(omega)            J(omega)
            0.000      2.7546   -0.0000      0.5997    0.0000
       -------------------------------------------------------------
```

## 4 Convergence studies

We give here the results of some convergence studies, than can be made by the
readers. Some are computationally expensive. It is recommanded to use at least
32 processors. Input file is provided in *tucrpa_3.abi* for the first case.

### 4.1 Cutoff in energy for the polarisability [[ecuteps]]

The number of plane waves used in the calculation of the cRPA polarizability
is determined by [[ecuteps]] (see [[theory:mbt#RPA_Fourier_space|Notes on RPA calculations]]).
The convergence can be studied simply by increasing the values of [[ecuteps]] and gives:

<center>

[[ecuteps]] (Ha)|   _U_ (eV)  |   _J_ (eV)
------------|-----------|----------
   3        |    3.22   |    0.63
   5        |    3.01   |    0.60
   7        |    2.99   |    0.59
   9        |    2.99   |    0.58

</center>

So, for [[nband]]=30, [[ecutsigx]]=30 and a 4x4x4 k-point grid, the effective
interactions are converged with a precision of 0.02 eV for [[ecuteps]]=5.

### 4.2 Number of bands [[nband]]

The RPA polarizability depends also of the number of Kohn-Sham bands as also
discussed in the [[theory:mbt#RPA_Fourier_space|Notes on RPA calculations]])

<center>

[[nband]]    |      _U_ (eV)  |   _J_ (eV)
---------|--------------|---------
 30      |       3.01   |    0.61
 50      |       2.75   |    0.60
 70      |       2.71   |    0.59
 90      |       2.70   |    0.59

</center>

So, for [[ecuteps]]=5, [[ecutsigx]]=30 and a 4x4x4 k-point grid, the effective
interactions are converged with a precision of 0.05 eV for [[nband]]=50.

### 4.3 Cutoff in energy for the calculation of the Coulomb interaction [[ecutsigx]]

[[ecutsigx]] is the input variable determining the number of plane waves used
in the calculation of the exchange part of the self-energy. The same variable
is used here to determine the number of plane waves used in the calculation of
the effective interaction. The study of the convergence gives:

<center>

[[ecutsigx]] (Ha)     |       _U_<sub>bare</sub> (eV) |   _J_<sub>bare</sub> (eV)  |    _U_ (eV)   |    _J_ (eV)
------------------|-------------------|----------------|-------------|------------
    30            |        15.38      |    0.67        |    3.22     |    0.63
    50            |        15.38      |    0.68        |    3.22     |    0.65
    70            |        15.38      |    0.69        |    3.22     |    0.66

</center>

For [[ecuteps]]=3, [[nband]]=30 and a 4 4 4 k-point grid, effective
interactions are converged with a precision of 0.02 eV for [[ecutsigx]]=30 Ha.

### 4.4 k-point

The dependence of effective interactions with the k-point grid is also important to check.

<center>

kpoint grid |    _U_<sub>bare</sub> (eV)|    _J_<sub>bare</sub> (eV) |    _U_ (eV)   |    _J_ (eV)
------------|---------------|----------------|-------------|-------------
   4 4 4    |      15.38    |      0.65      |     3.22    |     0.63
   6 6 6    |      15.36    |      0.66      |     3.18    |     0.62

</center>

For [[ecuteps]]=3, [[nband]]=30 and [[ecutsigx]]=30, effective interactions
are converged to 0.01 eV for a k-point grid of 6 6 6. The converged parameters
are thus [[ecuteps]]=5,[[ecutsigx]]=30,ngkpt=6 6 6, [[nband]]=50 with an
expected precision of 0.1 eV. We will use instead
[[ecuteps]]=5,[[ecutsigx]]=30,ngkpt=4 4 4, [[nband]]=50 to lower the
computational cost. In this case, we find values of _U_ and _J_ of 2.75 eV and
0.41 eV and _U_ is probably underestimated by 0.1 eV.

## 5 Effective interactions for different models


In this section, we compute, using the converged values of parameters, the
value of effective interactions for the models discussed in section and 3.1.
The table below gives for each model, the values of [[dmftbandi]],
[[dmftbandf]] and [[ucrpa_bands]] that must be used, and it sums up the value
of bare and effective interactions.

<center>

model                        | _d - d_ |   _t<sub>2g</sub>-t<sub>2g</sub>_  |_dp-dp_  |  _d -dp_ (a) |  _d -dp_ (b)
-----------------------------|---------|------------------------------------|---------|--------------|---------------
[[ucrpa]]                    |    1    |     1                              |    1    |     1        |     2
[[dmftbandi]]/[[dmftbandf]]  |   21/25 |    23/25                           |    12/25|    12/25     |    12/25
[[ucrpa_bands]]              |   21 25 |    23 25                           |    12 25|    21 25     |    12 25
_U_<sub>bare</sub> (eV)      |   15.4  |    15.3                            |    19.4 |    19.4      |    19.4
_U_<sub>bare diag</sub> (eV) |   16.3  |    16.0                            |    20.6 |    20.6      |    20.6
_J_<sub>bare</sub> (eV)      |   0.66  |    0.86                            |   0.96  |    0.96      |    0.96
_U_ (eV)                     |   2.8   |    2.8                             |    10.8 |    3.4       |    1.6
_U_<sub>diag</sub> (eV)      |   3.5   |    3.4                             |    12.0 |    4.4       |    2.6
_J_ (eV)                     |   0.60  |    0.76                            |     0.91|    0.87      |    0.86


</center>

Even if our calculation is not perfectly converged with respect to k-points,
the results obtained for the _d-d_, _dp-dp_, and _d-dp_ (a) models are in
agreement (within 0.1 or 0.2 eV) with results obtained in Table V of
[[cite:Amadon2014]] and references cited in this table.

To obtain the results with only the _t<sub>2g</sub>_ orbitals, one must use a specific
input file, which is tucrpa_4.abi, which uses specific keywords, peculiar to
this case (compare it with tucrpa_2.abi). In this peculiar case, the most
common definition of J has to be deduced by direct calculation from the
interaction matrices using the Slater Kanamori expression (see e.g.
[[cite:Lechermann2006]] or [[cite:Vaugier2012]]) and not using the value of _J_ computed in the code).

We now briefly comment the physics of the results.

* Bare interactions. When the number of bands used is larger, the Wannier orbitals extension is smaller.
  Thus, the interaction obtained with Wannier functions obtained from _dp_ bands are larger
  than interactions obtained with Wannier functions obtained with _d_ bands and also larger
  than interaction obtained on atomic orbitals.

* For the same Wannier functions ( _dp-dp, d-dp_ (a), _d-dp_ (b)), one can see the role of
  the screening on cRPA interactions:

      - When one supresses all transitions inside the _d_ -like and _p_ -like bands ( _dp-dp_ model), _U_ is large.
      - When only the transitions inside the _d_ -like bands are suppressed, _U_ is smaller ( _d-dp_ (a) model)
        in comparison to the _dp-dp_ model, because the screening is more efficient.
      - Lastly, when one uses the weighting scheme to compute the polarizability, _U_ is very small.
        Indeed, in this case, one can show that there are some residual transitions near the Fermi level
        (due to the hybridization between _d_ orbitals and O _p_ orbitals) which are very efficient
        to screen the interaction (A similar effect is discussed in [[cite:Amadon2014]] for UO 2 or
        [[cite:Sakuma2013]] for transition metal oxydes).

Finally, fully screened interactions (not discussed here) could also be
computed for each definition of correlated orbitals by using the default
values of [[ucrpa_bands]].

## 6 Frequency dependent interactions

In this section, we are going to compute frequency dependent effective
interactions. Just change the following input variables in order to compute
effective interaction between 0 and 30 eV. We use [[nsym]] = 0 in order to
decrease the computational cost.

     # -- Frequencies for effective interactions
     nsym        0
     nfreqsp3   60
     freqspmax3 30 eV
     freqspmin3  0 eV
     nfreqsp4   60
     freqspmax4 30 eV
     freqspmin4  0 eV

An example of input file can be found in *tucrpa_5.abi*. Note that we have
decreased some parameters to speed-up the calculations. Importantly, however,
we have increased the number of Kohn Sham bands, because calculation of
screening at high frequency involves high energy transitions which requires
high energy states (as well as semicore states). If the calculation is too
time consuming, you can reduce the number of frequencies. The following figure
has been plotted with 300 frequencies, but using 30 or 60 frequencies is
sufficient to see the main tendencies. Extract the value of _U_ for the 60 frequencies using:

    grep "U(omega)" -A 60 tucrpa_5.out > Ufreq.dat

Remove the first line, and plot the data:

![dynamical](ucalc_crpa_assets/dynamical.jpeg)

We recover in the picture the main results found in the Fig. 3 of [[cite:Aryasetiawan2006]].
Indeed, the frequency dependent effective interactions exhibits a plasmon excitation.

## 7 Conclusion

This tutorial showed how to compute effective interactions for SrVO3. It
emphasized the importance of the definition of correlated orbitals and the
definition of constrained polarizability.
---
authors: BAmadon, OGingras
---

# Tutorial on DFT+DMFT

## A DFT+DMFT calculation for SrVO<sub>3</sub>.

This tutorial aims at showing how to perform a DFT+DMFT calculation using Abinit.

You will not learn here what is DFT+DMFT. But you will learn how to do a
DFT+DMFT calculation and what are the main input variables controlling this
type of calculation.

It might be useful that you already know how to do PAW calculations using
ABINIT but it is not mandatory (you can follow the two tutorials on PAW in
ABINIT, [PAW1](/tutorial/paw1) and [PAW2](/tutorial/paw2).
Also the [DFT+U tutorial](/tutorial/dftu) in ABINIT might be useful to know some basic
variables common to DFT+_U_ and DFT+DMFT.

This tutorial should take about one hour to complete
(less if you have access to several processors).

[TUTORIAL_README]

## 1 The DFT+DMFT method: summary and key parameters

The DFT+DMFT method aims at improving the description of strongly correlated systems.
Generally, these highly correlated materials contain rare-earth
metals or transition metals, which have partially filled *d* or *f* bands and thus localized electrons.
For further information on this method, please refer
to [[cite:Georges1996]] and [[cite:Kotliar2006]]. For an introduction to Many Body
Physics (Green's function, Self-energy, imaginary time, and Matsubara frequencies),
see e.g. [[cite:Coleman2015]] and [[cite:Tremblay2017]].

Several parameters (both physical and technical) needs to be discussed for a DFT+DMFT calculation.

  * The definition of correlated orbitals. In the ABINIT DMFT implementation, it is done with the help of
    Projected Wannier orbitals (see [[cite:Amadon2008]]). The first part of the tutorial explains the importance of this choice.
    Wannier functions are unitarily related to a selected set of Kohn Sham (KS) wavefunctions, specified in ABINIT
    by band index [[dmftbandi]], and [[dmftbandf]].
    Thus, as empty bands are necessary to build Wannier functions, it is required in DMFT calculations that the KS Hamiltonian
    is correctly diagonalized: use high values for [[nnsclo]], and [[nline]] for DMFT calculations and preceding DFT calculations.
    Roughly speaking, the larger dmftbandf-dmftbandi is, the more localized is the radial part of the orbital.
    Note that this definition is different from the definition of correlated orbitals in the DFT+_U_ implementation in ABINIT
    (see [[cite:Amadon2008a]]). The relation between the two expressions is briefly discussed in [[cite:Amadon2012]].

  * The definition of the Coulomb and exchange interaction U and J are done as in DFT+_U_ through the variables [[upawu]] and [[jpawu]].     They could be computed with the cRPA method, also available in ABINIT. The value of U and J should in principle depend
    on the definition of correlated orbitals. In this tutorial, U and J will be seen as parameters, as in the DFT+_U_ approach.
    As in DFT+_U_, two double counting methods are available (see the [[dmft_dc]] input variable).

  * The choice of the double counting correction. The current default choice in ABINIT is ([[dmft_dc]] = 1)
    which corresponds to the full localized limit.

  * The method of resolution of the Anderson model. In ABINIT, it can be the Hubbard I method ([[dmft_solv]] = 2)
    the Continuous time Quantum Monte Carlo (CTQMC) method ([[dmft_solv]]=5) or the static mean field method
    ([[dmft_solv]] = 1) equivalent to usual DFT+_U_.

  * The solution of the Anderson Hamiltonian and the DMFT solution are strongly dependent over temperature.
     So the temperature [[tsmear]] is a very important physical parameter of the calculation.

  * The practical solution of the DFT+DMFT scheme is usually presented as a double loop over first the
    local Green's function, and second the electronic local density. (cf Fig. 1 in [[cite:Amadon2012]]).
    The number of iterations of both loops are respectively given in ABINIT by keywords [[dmft_iter]] and [[nstep]].
    Other useful variables are [[dmft_rslf]] = 1 and [[prtden]] = -1 (to be able to restart the calculation from the density file).
    Lastly, one linear and one logarithmic grid are used for Matsubara Frequencies indicated by [[dmft_nwli]] and [[dmft_nwlo]]
    (Typical values are 100000 and 100, but convergence should be studied).
    A large number of information are given in the log file using [[pawprtvol]] = 3.

## 2 Electronic Structure of SrVO3 in LDA

*You might create a subdirectory of the *\$ABI_TESTS/tutoparal* directory, and use it for the tutorial.
In what follows, the names of files will be mentioned as if you were in this subdirectory*

Copy the file *tdmft_1.abi* from *\$ABI_TESTS/tutoparal* in your Work
directory,

```sh
cd $ABI_TESTS/tutoparal/Input
mkdir Work_dmft
cd Work_dmft
cp ../tdmft_1.abi .
```

{% dialog tests/tutoparal/Input/tdmft_1.abi %}

Then run ABINIT with:

    mpirun -n 24 abinit  tdmft_1.abi > log_1 &

This run should take some time. It is recommended that you use at least 10
processors (and 24 should be fast). It calculates the LDA ground state of
SrVO3 and compute the band structure in a second step.

The variable [[pawfatbnd]] allows to create files with "fatbands" (see description of the
variable in the list of variables): the width of the line along each k-point
path and for each band is proportional to the contribution of a given atomic
orbital on this particular Kohn Sham Wavefunction. A low cutoff and a small
number of k-points are used in order to speed up the calculation.

During this time you can take a look at the input file. There are two datasets. The first
one is a ground state calculations with [[nnsclo]]=3 and [[nline]]=3 in order
to have well diagonalized eigenfunctions even for empty states. In practice,
you have however to check that the residue of wavefunctions is small at the
end of the calculation. In this calculation, we find 1.E-06, which is large
(1.E-10 would be better, so nnsclo and nline should be increased, but it would
take more time). When the calculation is finished, you can plot the fatbands
for Vanadium and l=2. Several possibilities are available for that purpose. We
will work with the simple |xmgrace| package
(you need to install it, if not already available on your machine).

    xmgrace tdmft_1o_DS2_FATBANDS_at0001_V_is1_l0002 -par ../Input/tdmft_fatband.par

The band structure is given in eV.

![FatbandV](dmft_assets/fatbandV.jpg)

and the fatbands for all Oxygen atoms and *l=1* with

    xmgrace tdmft_1o_DS2_FATBANDS_at0003_O_is1_l0001 tdmft_1o_DS2_FATBANDS_at0004_O_is1_l0001 tdmft_1o_DS2_FATBANDS_at0005_O_is1_l0001 -par ../Input/tdmft_fatband.par

![FatbandV](dmft_assets/fatbandO.jpg)

In these plots, you recover the band structure of SrVO3 (see for comparison
the band structure of Fig.3 of [[cite:Amadon2008]]), and the main character of
the bands. Bands 21 to 25 are mainly _d_ and bands 12 to 20 are mainly oxygen
_p_. However, we clearly see an important hybridization. The Fermi level (at 0
eV) is in the middle of bands 21-23.

One can easily check that bands 21-23 are mainly _d-t 2g_ and bands 24-25 are
mainly _e<sub>g</sub>_: just use [[pawfatbnd]] = 2 in *tdmft_1.abi* and relaunch the calculations.
Then the file *tdmft_1o_DS2_FATBANDS_at0001_V_is1_l2_m-2*,
*tdmft_1o_DS2_FATBANDS_at0001_V_is1_l2_m-1* and
*tdmft_1o_DS2_FATBANDS_at0001_V_is1_l2_m1* give you respectively the _xy_, _yz_ and
xz fatbands (ie _d-t 2g_) and *tdmft_1o_DS2_FATBANDS_at0001_V_is1_l2_m+0* and
*tdmft_1o_DS2_FATBANDS_at0001_V_is1_l2_m+2* give the _z<sup>2</sup>_ and _x<sup>2</sup>-y<sup>2</sup>_ fatbands (i.e. _e<sub>g</sub>_).

So in conclusion of this study, the Kohn Sham bands which are mainly _t<sub>2g</sub>_
are the bands 21,22 and 23.

Of course, it could have been anticipated from classical crystal field theory:
the vanadium is in the center of an octahedron of oxygen atoms, so _d_
orbitals are split in _t<sub>2g</sub>_ and _e<sub>g</sub>_. As _t<sub>2g</sub>_ orbitals are not directed
toward oxygen atoms, _t<sub>2g</sub>_-like bands are lower in energy and filled with one
electron, whereas _e<sub>g</sub>_-like bands are higher and empty.

In the next section, we will thus use the _t<sub>2g</sub>_-like bands to built Wannier
functions and do the DFT+DMFT calculation.

## 3 Electronic Structure of SrVO3: DFT+DMFT calculation


### 3.1. The input file for DMFT calculation: correlated orbitals, screened Coulomb interaction and frequency mesh

In ABINIT, correlated orbitals are defined using the projected local orbitals
Wannier functions as outlined above. The definition requires to define a given
energy window from which projected Wannier functions are constructed. We would
like in this tutorial, to apply the DMFT method on _d_ orbitals and for
simplicity on a subset of _d_ orbitals, namely _t<sub>2g</sub>_ orbitals ( _e<sub>g</sub>_ orbitals
play a minor role because they are empty). But we need to define _t<sub>2g</sub>_
orbitals. For this, we will use Wannier functions.

As we have seen in the orbitally resolved fatbands, the Kohn Sham wave
function contains a important weight of _t<sub>2g</sub>_ atomic orbitals mainly in _t
<sub>2g</sub>_-like bands but also in oxygen bands.

So, we can use only the _t<sub>2g</sub>_-like bands to define Wannier functions or also
both the _t<sub>2g</sub>_-like and _O-p_ -like bands.

The first case corresponds to the input file *tdmft_2.abi*. In this case
[[dmftbandi]] = 21 and [[dmftbandf]] = 23. As we only put the electron interaction
on _t<sub>2g</sub>_ orbitals, we have to use first [[lpawu]] = 2, but also the keyword
[[dmft_t2g]] = 1 in order to restrict the application of interaction on _t<sub>2g</sub>_ orbitals.

Notice also that before launching a DMFT calculation, the LDA should be
perfectly converged, including the empty states (check nline and nnsclo in the
input file). The input file *tdmft_2.abi* thus contains two datasets: the first
one is a well converged LDA calculation, and the second is the DFT+DMFT calculation.

Notice the other dmft variables used in the input file and check their meaning
in the input variable glossary. In particular, we are using [[dmft_solv]] = 5 for
the dmft dataset in order to use the density-density continuous time quantum
monte carlo (CTQMC) solver. (See [[cite:Gull2011]], as well as the ABINIT 2016
paper [[cite:Gonze2016]] for details about the CTQMC implementation in ABINIT.)
Note that the number of imaginary frequencies [[dmft_nwlo]] has to be set to
at least twice the value of [[dmftqmc_l]] (the discretization in imaginary
time). Here, we choose a temperature of 1200 K. For lower temperature, the
number of Matsubara frequencies should be higher.

Here we use a fast calculation, with a small value of the parameters,
especially [[dmft_nwlo]], [[dmftqmc_l]] and [[dmftqmc_n]].

Let's now discuss the value of the effective Coulomb interaction U ([[upawu]])
and J ([[jpawu]]). The values of U and J used in ABINIT in DMFT use the same
convention as in DFT+_U_ calculations in ABINIT (cf [[cite:Amadon2008a]]). However,
calculations in Ref. [[cite:Amadon2008]] use for U and J the usual convention for
_t<sub>2g</sub>_ systems as found in [[cite:Lechermann2006]], Eq. 26 (see also the appendix
in [[cite:Fresard1997]]). It corresponds to the Slater integral F4=0 and we can
show that U_abinit=U-4/3 J and J_abinit=7/6 J. So in order to use U = 4 eV and
J=0.65 eV with these latter conventions (as in [[cite:Amadon2008]]), we have to use
in ABINIT: [[upawu]] = 3.13333 eV; [[jpawu]] = 0.75833 eV and [[f4of2_sla]] = 0.

Now, you can launch the calculation:

Copy the file *../Input/tdmft_2.abi* in your Work
directory and run ABINIT:

    mpirun -n 24 abinit tdmft_2.abi > log_2

{% dialog tests/tutoparal/Input/tdmft_2.abi %}


### 3.2. The DFT+DMFT calculation: the log file

We are now going to browse quickly the log file (log_2) for this calculation.

Starting from

    =====  Start of DMFT calculation

we have first the definition of logarithmic grid for frequency, then, after:

    == Prepare data for DMFT calculation

The projection of Kohn Sham wavefunctions and (truncated) atomic orbitals are
computed (Eq.(2.1) in [[cite:Amadon2012]]) and unnormalized orbitals are built
(Eq.(2.2) in [[cite:Amadon2012]]) The occupation matrix in this orbital basis is

      ------ Symetrised Occupation

            0.10480  -0.00000  -0.00000
            0.00000   0.10480  -0.00000
           -0.00000  -0.00000   0.10480


and the Normalization of this orbital basis is

      ------ Symetrised Norm

            0.62039   0.00000   0.00000
            0.00000   0.62039   0.00000
            0.00000   0.00000   0.62039

Now, let's compare these numbers to other quantities. If the preceding LDA
calculation is converged, dmatpuopt=1 is used, and [[dmftbandi]]=1 and
[[dmftbandf]]=nband, then the above Symetrised Occupation should be exactly
equal to the occupation matrix given in the usual DFT+_U_ occupation matrix
written in the log file (with dmatpuopt=1) (see discussion in [[cite:Amadon2012]]).
In our case, we are not in this case because [[dmftbandi]]=21 so this
condition is not fulfilled. Concerning the norm if these orbitals, two factors play a role:

  * Firstly, the number of Kohn Sham function used should be infinite (cf Eq. B.4 of [[cite:Amadon2012]]),
    which is not the case here, because we take into account only bands 21-23.
    We emphasize that it is not a limitation of our approach, but just a physical choice concerning Wannier functions.
    This physical choice induces that these intermediate wave functions have a very low norm.

  * Secondly, the atomic orbitals used to do the projection are cut at the PAW radius.
    As a consequence, even if we would use a complete set of KS wavefunctions and thus the closure relation,
    the norm could not be one. In our case, it could be at most 0.84179, which is the norm of the
    truncated atomic function of _d_ orbitals of Vanadium used in this calculation.
    This number can be found in the log file by searching for ph0phiint (grep "ph0phiint(icount)= 1" log_2).
    (See also the discussion in Section B.3 of [[cite:Amadon2012]]).

Next the LDA Green's function is computed.

     =====  DFT Green Function Calculation

Then the Green's function is integrated to compute the occupation matrix.
Interestingly, the density matrix here must be equal to the density matrix
computed with the unnormalized correlated orbitals. If this is not the case,
it means that the frequency grid is not sufficiently large. In our case, we find:

            0.10481  -0.00000  -0.00000
           -0.00000   0.10481  -0.00000
           -0.00000  -0.00000   0.10481

So the error is very small (1.10E-5). As an exercise, you can decrease the
number of frequencies and see that the error becomes larger.

Then the true orthonormal Wannier functions are built and the Green's function
is computed in this basis just after:

     =====  DFT Green Function Calculation with renormalized psichi

The occupation matrix is now:

            0.16893   0.00000  -0.00000
            0.00000   0.16893   0.00000
           -0.00000   0.00000   0.16893

We see that because of the orthonormalization of the orbitals necessary to
built Wannier functions, the occupation matrix logically increases.

Then, after:

     =====  Define Interaction and self-energy

The Interaction kernel is computed from U and J, and the self energy is read
from the disk file (if it exists). Then, the Green's function is computed with
the self energy and the Fermi level is computed. Then the DMFT Loop starts.

     =====  DMFT Loop starts here

The log contains a lot of details about the calculation (especially if
[[pawprtvol]]=3). In order to have a more synthetic overview of the
calculation (this is especially useful to detect possible divergence of the
calculation), the following command extracts the evolution of the number of
electrons (LDA, LDA with Wannier functions, and DMFT number of electrons) as a
function of iterations (be careful, all numbers of electron are computed
differently as explained in the log file):

    grep -e Nb -e ITER log_2

Besides, during each DMFT calculation, there are one or more CTQMC calculations:

    Starting QMC  (Thermalization)

For the sake of efficiency, the DMFT Loop is in this calculation done only
once before doing again the DFT Loop (cf Fig. 1 of [[cite:Amadon2012]]). At the end
of the calculation, the occupation matrix is written and is:

              -- polarization spin component  1
            0.16811   0.00000  -0.00000
            0.00000   0.16811  -0.00000
           -0.00000  -0.00000   0.16811

We can see that the total number of electron is very close to one and it does
not change much as a function of iterations. As an output of the calculation,
you can find the self energy in file *tdmft_2o_DS2Self-omega_iatom0001_isppol1*
and the Green's function is file *Gtau.dat*.

### 3.3. The self energy

You can use the self-energy to compute the quasiparticle renormalization weight.
We first extract the first six Matsubara frequencies:

    head -n 8 tdmft_2o_DS2Self-omega_iatom0001_isppol1 > self.dat

Then we plot the imaginary part of the self-energy (in imaginary frequency):

    xmgrace -block self.dat -bxy 1:3

Then using |xmgrace|, you click on _Data_ , then on _Transformations_ and then
on _Regression_ and you can do a 4th order fit as:

![self](dmft_assets/self.jpg)

The slope at zero frequency obtained is 0.82. From this number, the
quasiparticle renormalisation weight can be obtained using Z=1/(1+0.82)=0.55.

### 3.4. The Green's function for correlated orbitals

The impurity (or local) Green's function for correlated orbitals is written in
the file Gtau.dat. It is plotted as a function of the imaginary time in the
interval [0, β] where β is the inverse temperature (in Hartree). You can plot
this Green's function for the six _t<sub>2g</sub>_ orbitals using e.g xmgrace

    xmgrace -nxy Gtau.dat

![Gtau](dmft_assets/Gtau.jpg)

As the six _t 2g _ orbitals are degenerated, the six Green's function must be
similar, within the stochastic noise. Moreover, this imaginary time Green's
function must be negative and the value of G(β) for the orbital i is equal to
the opposite of the number of electrons in the orbital i (-ni). Optionnally,
you can check how the Green's function can be a rough way to check for the
importance of stochastic noise. For example, change for simplicity the number
of steps for the DMFT calculation to 1:

    nstep2 1

and then use a much smaller number of steps for the Monte Carlo Solver such as

    dmftqmc_n 1.d3

save the previous Gtau.dat file:

    cp Gtau.dat Gtau.dat_save

Then relaunch the calculation. After it is completed, compare the new Green's
function and the old one with the previous value of [[dmftqmc_n]]. Using xmgrace,

    xmgrace -nxy Gtau.dat_save -nxy Gtau.dat

one obtains:

![Gtau2](dmft_assets/Gtau2.jpg)

One naturally sees that the stochastic noise is much larger in this case. This
stochastic noise can induces that the variation of physical quantities (number
of electrons, electronic density, energy) as a function of the number of
iteration is noisy. Once you have finished this comparison, copy the saved
Green's function into Gtau.dat in order to continue the tutorial with a
precise Green's function in *Gtau.dat*:

    cp Gtau.dat_save Gtau.dat

### 3.5. The local spectral function

You can now use the imaginary time Green's function (contained in file
Gtau.dat) to compute the spectral function in real frequency. Such analytical
continuation can be done on quantum Monte Carlo data using the Maximum Entropy method.

A maximum entropy code has been published recently by D. Bergeron. It can be
downloaded [here](https://www.physique.usherbrooke.ca/MaxEnt/index.php/Main_Page).
Please cite the related paper [[cite:Bergeron2016]] if you use this code in a publication.

The code has a lot of options, and definitely, the method should be understood
and the user guide should be read before any real use. It is not the goal of
this DFT+DMFT tutorial to introduce to the Maximum Entropy Method (see
[[cite:Bergeron2016]] and references therein). We give here a very quick way to
obtain a spectral function. First, you have to install this code and the
armadillo library by following the [guidelines](https://www.physique.usherbrooke.ca/MaxEnt/index.php/Download),
and then launch it on the current directory in order to generate the default
input file *OmegaMaxEnt_input_params.dat*.

    OmegaMaxEnt

Then edit the file *OmegaMaxEnt_input_params.dat*, and modify the first seven lines with:

    data file: Gtau.dat

    OPTIONAL PREPROCESSING TIME PARAMETERS

    DATA PARAMETERS
    bosonic data (yes/[no]): no
    imaginary time data (yes/[no]): yes


Then relaunch the code

    OmegaMaxEnt

and plot the spectral function:

    xmgrace OmegaMaxEnt_final_result/optimal_spectral_function_*.dat

Change the unit from Hartree to eV, and then, you have the spectral function:

![spectralfunction](dmft_assets/spectralfunction.jpg)

Even if the calculation is not well converged, you recognize in the spectral
functions the quasiparticle peak as well as Hubbard bands at -2 eV and +2.5 eV
as in Fig.4 of [[cite:Amadon2008]].

## 4 Electronic Structure of SrVO3: Choice of correlated orbitals

Previously, only the _t<sub>2g</sub>_-like bands were used in the definition of Wannier
functions. If there were no hybridization between _t<sub>2g</sub>_ orbitals and oxygen
_p_ orbitals, the Wannier functions would be pure atomic orbitals and they
would not change if the energy window was increased. But there is an important
hybridization, as a consequence, we will now built Wannier functions with a
large window, by including oxygen _p_ -like bands in the definition of Wannier
functions. Create a new input file:

    cp tdmft_2.abi tdmft_3.abi

and use [[dmftbandi]] = 12 in *tdmft_3.abi*. Now the code will built Wannier
functions with a larger window, including _O-p_ -like bands, and thus much
more localized. Launch the calculation (if
the calculation is too long, you can decide to restart the second dataset
directly from a converged LDA calculation instead of redoing the LDA
calculation for each new DMFT calculation).

    abinit tdmft_3.abi > log_3

In this case, both the occupation and the norm are larger because more states
are taken into account: you have the occupation matrix which is

      ------ Symetrised Occupation

            0.22504  -0.00000  -0.00000
           -0.00000   0.22504  -0.00000
           -0.00000  -0.00000   0.22504


and the norm is:

      ------ Symetrised Norm

            0.73746   0.00000  -0.00000
            0.00000   0.73746  -0.00000
           -0.00000  -0.00000   0.73746

Let us now compare the total number of electron and the norm with the two energy window:

<center>

Energy window:                           |   _t<sub>2g</sub>_-like bands |        _t<sub>2g</sub>_-like+ _O-p_ -like bands
-----------------------------------------|-------------------------------|---------------------------------------------------
[[dmftbandi]]/[[dmftbandf]]:                     |    21/23                      | 12/23
Norm:                                    |  0.63                         |   0.78
LDA Number of electrons (before ⊥):   |  0.63(=0.105*6)                |  1.35(=0.235*6)
LDA Number of electrons (after  ⊥):   |    1.00                       |  1.77

</center>

For the large window, as we use more Kohn Sham states, both the occupation and
the norm are larger, mainly because of the important weight of _d_ orbitals in
the oxygen bands (because of the hybridization). Concerning the norm, remind
that in any case, it cannot be larger that 0.86. So as the Norm is 0.78, it
means that by selecting bands 12-23 in the calculation, we took into account
0.78/0.86*100=90\% of the weight of the truncated atomic orbital among Kohn
Sham bands. Moreover, after orthonormalization, you can check that the
difference between LDA numbers of electrons is still large (1.02 versus 1.81),
even if the orthonormalization effect is larger on the small windows case.

At the end of the DFT+DMFT calculation, the occupation matrix is written and is

              -- polarization spin component  1
            0.29313   0.00000   0.00000
            0.00000   0.29313   0.00000
            0.00000   0.00000   0.29313

Similarly to the previous calculation, the spectral function can be plotted
using the Maximum Entropy code: we find a spectral function with an
hybridation peak at -5 eV, as described in Fig.5 of [[cite:Amadon2008]].

![spectralfunction](dmft_assets/spectralfunction2.jpg)

Resolving the lower Hubbard bands would require a more converged calculation.

As above, one can compute the renormalization weight and it gives 0.68. It
shows that with the same value of U and J, interactions have a weaker effect
for the large window Wannier functions. Indeed, the value of the screened
interaction U should be larger because the Wannier functions are more
localized (see discussion in [[cite:Amadon2008]]).

## 5 Electronic Structure of SrVO3: The internal energy

The internal energy can be obtained with

    grep -e ITER -e Internal log_3

and select the second occurrence for each iteration (the double counting
expression) which should be accurate with iscf=17 (at convergence both
expressions are equals also in DFT+DMFT). So after gathering the data:

<center>

 Iteration |     Internal Energy (Ha)
-----------|--------------------------
      1    |  -1.51857850339126E+02
      2    |  -1.51986727402835E+02
      3    |  -1.51895311846530E+02
      4    |  -1.51906820876597E+02
      5    |  -1.51891956860157E+02
      6    |  -1.51891135879190E+02
      7    |  -1.51892587329592E+02
      8    |  -1.51891607809905E+02
      9    |  -1.51891447186423E+02
     10    |  -1.51892343918492E+02

</center>

You can plot the evolution of the internal energy as a function of the iteration.

![internal](dmft_assets/internal.jpg)

You notice that the internal energy (in a DFT+DMFT calculations) does not
converge as a function of iterations, because there is a finite statistical
noise. So, as a function of iterations, first, the internal energy starts to
converge, because the modification of the energy induced by the self-
consistency cycle is larger than the statistical noise, but then the internal
energy fluctuates around a mean value. So if the statistical noise is larger
than the tolerance, the calculation will never converge. So if a given
precision on the total energy is expected, a practical solution is to increase
the number of Quantum Monte Carlo steps ([[dmftqmc_n]]) in order to lower the
statistical noise. Also another solution is to do an average over the last
values of the internal energy. 

## 6 Electronic Structure of SrVO3 in DFT+DMFT: Equilibrium volume

We focus now on the total energy. Create a new input file, *tdmft_4.abi*:

    cp tdmft_3.abi tdmft_4.abi

And use [[acell]] = 7.1605 instead of 7.2605. Relaunch the calculation and note the
internal energy (grep internal tdmft_4.abo).

Redo another calculation with [[acell]] = 7.00. Plot DMFT energies as a function of acell.

<center>

acell  |  Internal energy DMFT
-------|-------------------------
7.0000 |     -151.8908
7.1605 |     -151.8978
7.2605 |     -151.8920

</center>

You will notice
that the equilibrium volume is very weakly modified by the strong correlations is this case.

## 7 Electronic Structure of SrVO3: k-resolved Spectral function

We are going to use OmegaMaxEnt to do the direct analytical continuation of the self-energy in Matsubara frequencies to real frequencies.
(A more precise way to do the analytical continuation uses an auxiliary Green's function as mentionned in e.g. endnote 55
of Ref. [[cite:Sakuma2013a]]).
First of all, we are going to relaunch a more converged calculation using tdmft_5.abi

{% dialog tests/tutoparal/Input/tdmft_5.abi%}

Launch the calculation, it might take some time. The calculation takes in few minutes with 4 processors.

    abinit tdmft_5.abi > log_5

We are going to create a new directory for the analytical continuation.

    mkdir Spectral
  
We first extract the first Matsubara frequencies (which are not too noisy)
    
    head -n 26 tdmft_5o_DS2Selfrotformaxent0001_isppol1_iflavor0001 > Spectral/self.dat

In this directory, we launch OmegaMaxEnt just to generate the input template:

    cd Spectral
    OmegaMaxEnt

Then, you have to edit the input file *OmegaMaxEnt_input_params.dat* of OmegaMaxent and specify that the data is contained in self.dat and
that it contains a finite value a infinite frequency. So the first lines should look like this:

    data file: self.dat
    
    OPTIONAL PREPROCESSING TIME PARAMETERS
    
    DATA PARAMETERS
    bosonic data (yes/[no]):
    imaginary time data (yes/[no]):
    temperature (in energy units, k_B=1):
    finite value at infinite frequency (yes/[no]): yes


Then relaunch OmegaMaxent

    OmegaMaxEnt

You can now plot the imaginary part of the self energy in real frequencies with (be careful, this file contains in fact -2 Im$\Sigma$. If another analytical continuation tool is used, one needs to give to ABINIT -2 Im$\Sigma$ and not $\Sigma$ ):

    xmgrace OmegaMaxEnt_final_result/optimal_spectral_function.dat

Then, we need to give to ABINIT this file in order for abinit to use
it, to compute the Green's function in real frequencies and to deduce the k-resolved spectral function.
First copy this self energy in the real axis in a Self energy file and a grid file for ABINIT.

    cp OmegaMaxEnt_final_result/optimal_spectral_function.dat ../self_ra.dat
    cd ..

Create file containing the frequency grid with:

    wc -l self_ra.dat >  tdmft_5o_DS3_spectralfunction_realfrequencygrid
    cat self_ra.dat >> tdmft_5o_DS3_spectralfunction_realfrequencygrid

As in this particular case, the three self energies for the three t2g orbitals are equal,
we can do only one analytical continuation, and duplicate the results as in:

    cp self_ra.dat  tdmft_5i_DS3Self_ra-omega_iatom0001_isppol1
    cat self_ra.dat >>  tdmft_5i_DS3Self_ra-omega_iatom0001_isppol1
    cat self_ra.dat >>  tdmft_5i_DS3Self_ra-omega_iatom0001_isppol1

Now, tdmft_5i_DS3Self_ra-omega_iatom0001_isppol1 file, contains three times the self
real axis self-energy. If the three orbitals were not degenerated, one would have of course to use
a different real axis self-energy for each orbitals (and thus do an analytical continuation for each).

Copy the file containing the rotation of the self energy in the local basis (useful for non cubic cases, here
this matrix is just useless):

    cp tdmft_5o_DS2.UnitaryMatrix_for_DiagLevel_iatom0001 tdmft_5i_DS3.UnitaryMatrix_for_DiagLevel_iatom0001 

Copy the Self energy in imaginary frequency for restart also (dmft_nwlo should be the same in the input
file  tdmft_5.abi and tdmft_2.abi)

    cp tdmft_5o_DS2Self-omega_iatom0001_isppol1 tdmft_5o_DS3Self-omega_iatom0001_isppol1
    

Then modify tdmft_5.abi with

    ndtset 1
    jdtset 3
    
and relaunch the calculation.

    abinit tdmft_5.abi > log_5_dataset3


Then the spectral function is obtained in file tdmft_5o_DS3_DFTDMFT_SpectralFunction_kresolved_from_realaxisself. You can copy
it in file bands.dat:

    cp tdmft_5o_DS3_DFTDMFT_SpectralFunction_kresolved_from_realaxisself bands_dmft.dat

Extract DFT band structure from fatbands file in readable file for gnuplot (261 is the number
of k-point used to plot the band structure (it can be obtained by "grep nkpt log_5_dataset3"):

    grep " BAND" -A 261 tdmft_5o_DS3_FATBANDS_at0001_V_is1_l0001 | grep -v BAND > bands_dft.dat

And you can use a gnuplot script to plot it:

{% dialog tests/tutoparal/Input/tdmft_gnuplot%}

    gnuplot
    
    	G N U P L O T
    	Version 5.2 patchlevel 7    last modified 2019-05-29 
    
    	Copyright (C) 1986-1993, 1998, 2004, 2007-2018
    	Thomas Williams, Colin Kelley and many others
    
    	gnuplot home:     http://www.gnuplot.info
    	faq, bugs, etc:   type "help FAQ"
    	immediate help:   type "help"  (plot window: hit 'h')
    
    Terminal type is now 'qt'
    gnuplot> load "../tdmft_gnuplot"


The spectral function should thus look like this.

![internal](dmft_assets/bands.png)

The white curve is the LDA band structure, the colored plot is the DMFT spectral function.
One notes the renormalization of the bandwith as well as Hubbard bands, mainly visible a high energy  (arount 2 eV).
A  more precise description of the Hubbard band would require a more converged calculation.



## 8 Electronic Structure of SrVO3: Conclusion
  
To sum up, the important physical parameters for DFT+DMFT are the definition
of correlated orbitals, the choice of U and J (and double counting). The
important technical parameters are the frequency and time grids as well as the
number of steps for Monte Carlo, the DMFT loop and the DFT loop.

We showed in this tutorial how to compute the correlated orbital spectral functions, quasiparticle
renormalization weights, total internal energy and the k-resolved spectral function.
---
authors: VOlevano, FBruneval, MG, XG
---

# First tutorial on GW

## The quasi-particle band structure of Silicon in the GW approximation.

This tutorial aims at showing how to calculate self-energy corrections to the
DFT Kohn-Sham (KS) eigenvalues in the GW approximation.

A brief description of the formalism and of the equations implemented in the
code can be found in the [[theory:mbt|GW_notes]].
The different formulas of the GW formalism have been written in a [[pdf:gwa|pdf document]]
by Valerio Olevano who also wrote the first version of this tutorial.
For a much more consistent discussion of the theoretical aspects of the GW
method we refer the reader to the review article
[[cite:Aulbur2001|Quasiparticle calculations in solids]] by W.G Aulbur et al
also available [here](https://www.abinit.org/sites/default/files/quasiparticle_calculations_in_solids.pdf.bz2).

It is suggested to acknowledge the efforts of developers of
the GW part of ABINIT, by citing the [[cite:Gonze2005|2005 ABINIT publication]].

The user should be familiarized with the four basic tutorials of ABINIT,
see the [tutorial home page](/tutorial).
After this first tutorial on GW, you should read the [second GW tutorial](/tutorial/gw2).

This tutorial should take about 2 hours.

[TUTORIAL_README]

## 1 General example of an almost converged GW calculation

*Before beginning, you might consider to work in a different subdirectory as
for the other tutorials. Why not Work_gw1?*

At the end of [tutorial 3](/tutorial/base3), we computed the KS band
structure of silicon. In this approximation, the band dispersion as well as
the band widths are reasonable but the band gaps are qualitatively wrong.
Now we will compute the band gaps much more accurately, using the so-called
GW approximation.

We start by an example, in which we show how to perform in a single input file
the calculation of the ground state density, the Kohn Sham band structure, the
screening, and the GW corrections. We use reasonable values for the parameters
of the calculation. The discussion on the convergence tests is postponed to
the next paragraphs. We will see that GW calculations are **much more** time-consuming
than the computation of the KS eigenvalues.

So, let us run immediately this calculation, and while it is running, we will
explain what has been done.

```sh
mkdir Work_gw1
cd Work_gw1
cp $ABI_TESTS/tutorial/Input/tgw1_1.abi .
```

Then, issue:

```sh
    abinit tgw1_1.abi > log 2> err &
```

Please run this job in background because it takes about 1 minute.
In the meantime, you should read the following.

### 1.a The four steps of a GW calculation.

In order to perform a standard one-shot GW calculation one has to:

  1. Run a converged Ground State calculation to obtain the self-consistent density.

  2. Perform a non self-consistent run to compute the KS eigenvalues and the eigenfunctions
     including several empty states. Note that, unlike standard band structure calculations,
     here the KS states *must* be computed on a regular grid of **k**-points. 
     (This limitation is also present with hybrid functional calculations).

  3. Use [[optdriver]] = 3 to compute the independent-particle susceptibility $\chi^0$ on a regular grid of
     **q**-points, for at least two frequencies (usually, $\omega=0$ and a purely imaginary
     frequency - of the order of the plasmon frequency, a dozen of eV).
     The inverse dielectric matrix $\epsilon^{-1}$ is then obtained via matrix inversion and stored in an external file (SCR).
     The list of **q**-points is automatically defined by the k-mesh used to generate the KS states in the previous step.

  4. Use [[optdriver]] = 4 to compute the self-energy $\Sigma$ matrix elements for a given set of k-points in order
     to obtain the GW quasiparticle energies.
     Note that the **k**-point must belong to the k-mesh used to generate the WFK file in step 2.

The flowchart diagram of a standard one-shot run is depicted in the figure below.

![](gw1_assets/gw_flowchart.png)

The input file *tgw1_1.abi* has precisely that structure: there are four datasets.

The first dataset performs the SCF calculation to get the density. The second
dataset reads the previous density file and performs a NSCF run including
several empty states. The third dataset reads the WFK file produced in the
previous step and drives the computation of susceptibility and dielectric
matrices, producing another specialized file, *tgw1_xo_DS2_SCR* (*_SCR* for
"Screening", actually the inverse dielectric matrix $\epsilon^{-1}$). Then, in the fourth
dataset, the code calculates the quasiparticle energies for the 4th and 5th bands at the $\Gamma$ point.

So, you can edit this *tgw1_1.abi* file.

{% dialog tests/tutorial/Input/tgw1_1.abi %}

In the first half of the file, 
you will find specialized input variables for the datasets 1 to 4.

In the second half of the file,
one find the dataset-independent information, namely,
input variables describing the cell, atom types,
number, position, planewave cut-off energy, SCF convergence parameters driving
the KS band structure calculation. 

### 1.b Generating the Kohn-Sham band structure: the WFK file.

Dataset 1 drives a rather standard SCF calculation. It is worth noticing that we
use [[tolvrs]] to stop the SCF cycle because we want a well-converged KS potential
to be used in the subsequent NSCF calculation. Dataset 2 computes 100 bands and
we set [[nbdbuf]] to 20 so that only the first 80 states must be converged within [[tolwfr]].
The 20 highest energy states are simply not considered when checking the convergence.

```
    ###########
    # Dataset 1
    ############
    # SCF-GS run 
    nband1  6
    tolvrs1 1.0e-10
    
    ############
    # Dataset 2
    ############
    # Definition of parameters for the calculation of the WFK file
    nband2     100       # Number of (occ and empty) bands to be computed
    nbdbuf2     20       # Do not apply the convergence criterium to the last 20 bands (faster)
    iscf2       -2
    getden2     -1
    tolwfr2  1.0d-12     # Will stop when this tolerance is achieved 
```

!!! important

    The [[nbdbuf]] trick allows us to **save several minimization steps** because the
    last bands usually require more iterations to converge in the iterative diagonalization algorithms.
    Also note that it is a very good idea to increase significantly the value of [[nbdbuf]] when
    computing many empty states. As a rule of thumb, use 10% of [[nband]] or even more
    in complicated systems. This can really make a **huge difference** at the level of the wall time.

### 1.c Generating the screening: the SCR file.

In dataset 3, the calculation of the screening (KS susceptibility $\chi^0$ and then inverse dielectric
matrix $\epsilon^{-1}$) is performed. We need to set [[optdriver]]=3 to do that:

```
    optdriver3  3        # Screening calculation
```

The [[getwfk]] input variable is similar to other "get" input variables of ABINIT:

```
    getwfk3     -1       # Obtain WFK file from previous dataset
```

In this case, it tells the code to use the WFK file calculated in the previous dataset.

Then, three input variables describe the computation:

```
    nband3      60   # Bands used in the screening calculation
    ecuteps3    3.6  # Cut-off energy of the planewave set to represent the dielectric matrix
```

In this case, we use 60 bands to calculate the KS response function $\chi^{0}$.
The dimension of $\chi^{0}$, as well as all the other matrices ($\chi$, $\epsilon^{-1}$) is
determined by the cut-off energy [[ecuteps]] = 3.6 Hartree, which yields 169 planewaves in our case.

Finally, we define the frequencies at which the screening must be evaluated:
$\omega=0.0$ eV and the imaginary frequency $\omega= i 16.7$ eV. The latter is determined by
the input variable [[ppmfrq]]

```
    ppmfrq3    16.7 eV  # Imaginary frequency where to calculate the screening
```

The two frequencies are used to calculate the plasmon-pole model parameters.
For the non-zero frequency, it is recommended to use a value close to the
plasmon frequency for the plasmon-pole model to work well. Plasmons
frequencies are usually close to 0.5 Hartree. The parameters for the screening
calculation are not far from the ones that give converged Electron Energy Loss Function
($-\mathrm{Im} \epsilon^{-1}_{00}$) spectra, so that one can start up by using indications
from EELS calculations existing in literature.

### 1.d Computing the GW energies.

In dataset 4 the calculation of the Self-Energy matrix elements is performed.
One needs to define the driver option as well as the _WFK and _SCR files.

```
    optdriver4  4       # Self-Energy calculation
    getwfk4    -2       # Obtain WFK file from dataset 2
    getscr4    -1       # Obtain SCR file from previous dataset
```

The [[getscr]] input variable is similar to other "get" input variables of ABINIT.

Then, comes the definition of parameters needed to compute the self-energy. As
for the computation of the susceptibility and dielectric matrices, one must
define the set of bands and two sets of planewaves:

```
    nband4       80      # Bands to be used in the Self-Energy calculation
    ecutsigx4   8.0      # Dimension of the G sum in Sigma_x
                         # (the dimension in Sigma_c is controlled by npweps)
```

In this case, [[nband]] controls the number of bands used to calculate the
correlation part of the Self-Energy while [[ecutsigx]] gives the number of
planewaves used to calculate $\Sigma_x$ (the exchange part of the self-energy). The
size of the planewave set used to compute $\Sigma_c$ (the correlation part of the
self-energy) is controlled by [[ecuteps]] and cannot be larger than the value
used to generate the SCR file.
For the initial convergence studies, it is advised to set [[ecutsigx]] to a value as high
as [[ecut]] since, anyway, this parameter is not much influential on the total computational time.
Note that the exact treatment of the exchange part requires, in principle, [[ecutsigx]] = 4 * [[ecut]].

Then, come the parameters defining the **k**-points and the band indices for which
the quasiparticle energies will be computed:

```
    nkptgw4      1           # number of k-point where to calculate the GW correction
    kptgw4  0.00  0.00  0.00 # k-points
    bdgw4       4  5         # calculate GW corrections for bands from 4 to 5
```

[[nkptgw]] defines the number of **k**-points for which the GW corrections will be
computed. The **k**-point reduced coordinates are specified in [[kptgw]] while [[bdgw]] gives the
minimum/maximum band whose energies are calculated for each selected **k**-point.

!!! important

    These k-points **must belong** to the k-mesh used to generate the WFK file. Hence
    if you wish the GW correction in a particular **k**-point, you should choose a
    grid containing it. Usually this is done by taking the **k**-point grid where the
    convergence is achieved and shifting it such as at least one k-point is placed
    on the wished position in the Brillouin zone. 

There is an additional parameter, called [[zcut]], (not studied here) related to the self-energy
computation. It is meant to avoid some divergences that might occur in the
calculation due to integrable poles along the integration path.

### 1.e Examination of the output file.

Your calculation should have ended now. Let's examine the output file.
Open *tgw1_1.abo* in your preferred editor and find the section
corresponding to DATASET 3.

{% dialog tests/tutorial/Refs/tgw1_1.abo %}

After the description of the unit cell and of the pseudopotentials, you will
find the list of **k**-points used for the electrons and the grid of **q**-points (in
the Irreducible part of the Brillouin Zone) on which the susceptibility and
dielectric matrices will be computed.

```
 ==== K-mesh for the wavefunctions ====
 Number of points in the irreducible wedge :     6
 Reduced coordinates and weights :

     1)    -2.50000000E-01 -2.50000000E-01  0.00000000E+00       0.18750
     2)    -2.50000000E-01  2.50000000E-01  0.00000000E+00       0.37500
     3)     5.00000000E-01  5.00000000E-01  0.00000000E+00       0.09375
     4)    -2.50000000E-01  5.00000000E-01  2.50000000E-01       0.18750
     5)     5.00000000E-01  0.00000000E+00  0.00000000E+00       0.12500
     6)     0.00000000E+00  0.00000000E+00  0.00000000E+00       0.03125

 Together with 48 symmetry operations and time-reversal symmetry
 yields    32 points in the full Brillouin Zone.


 ==== Q-mesh for the screening function ====
 Number of points in the irreducible wedge :     6
 Reduced coordinates and weights :

     1)     0.00000000E+00  0.00000000E+00  0.00000000E+00       0.03125
     2)     5.00000000E-01  5.00000000E-01  0.00000000E+00       0.09375
     3)     5.00000000E-01  2.50000000E-01  2.50000000E-01       0.37500
     4)     0.00000000E+00  5.00000000E-01  0.00000000E+00       0.12500
     5)     5.00000000E-01 -2.50000000E-01  2.50000000E-01       0.18750
     6)     0.00000000E+00 -2.50000000E-01 -2.50000000E-01       0.18750

 Together with 48 symmetry operations and time-reversal symmetry
 yields    32 points in the full Brillouin Zone.
```

The q-mesh is the set of all the possible momentum transfers.
These points are obtained as all the possible differences among the **k**-points ( $\mathbf{q} =\mathbf{k}-\mathbf{k}'$ ) of the grid chosen to
generate the WFK file. From the last statement it is clear the importance of
choosing homogeneous **k**-point grids in order to minimize the number of **q**-points is clear.

After this section, the code prints the parameters of the FFT grid needed to
represent the wavefunctions and to compute their convolution (required for the
screening matrices). Then we have some information about the MPI distribution
of the bands and the total number of valence electrons computed by integrating
the density in the unit cell.

```
 setmesh: FFT mesh size selected  =  20x 20x 20
          total number of points  =     8000


- screening: taking advantage of time-reversal symmetry
- Maximum band index for partially occupied states nbvw = 4
- Remaining bands to be divided among processors   nbcw = 56
- Number of bands treated by each node ~56
```

With the valence density, one can obtain the classical Drude plasmon
frequency. The next lines calculate the average density of the system, and
evaluate the Wigner radius $r_s$, then compute the Drude plasmon frequency.

```
 Number of electrons calculated from density =    7.9999; Expected =    8.0000
 average of density, n =  0.029628
 r_s =    2.0048
 omega_plasma =   16.6039 [eV]
```


This is the value used by default for [[ppmfrq]]. It is in fact the second frequency
where the code calculates the dielectric matrix to adjust the plasmon-pole
model parameters.

It has been found that Drude plasma frequency is a
reasonable value where to adjust the model. The control over this parameter is
however left to the user in order to check that the result does not change
when changing [[ppmfrq]]. One has to be careful with finite systems or with systems having semicore electrons.
If the result depends much on  [[ppmfrq]], then the plasmon-pole model is
not appropriate and one should go beyond it by taking into account a full
dynamical dependence in the screening (see later, the contour-deformation
method). However, the plasmon-pole model has been found to work well for a
very large range of solid-state systems when focusing only on the real part of the GW
corrections in the band gap region.

At the end of the screening calculation, the macroscopic dielectric constant is printed:

```
      dielectric constant =  22.4176
      dielectric constant without local fields =  24.7005
```


!!! note

    Note that the convergence in the dielectric constant **does not guarantee** the
    convergence in the GW corrections and vice-versa. In fact, the dielectric constant is
    representative of only one element i.e. the head of the full dielectric
    matrix. Even if the convergence on the dielectric constant with local fields
    takes somehow into account also other non-diagonal elements. In a GW
    calculation the whole $\epsilon^{-1}$ matrix is used to build the Self-Energy operator.

The dielectric constant reported here is the so-called RPA dielectric constant
due to the electrons. Although evaluated at zero frequency, it is understood
that the ionic response is not included (this term can be computed with DFPT
and ANADDB). The RPA dielectric constant restricted to electronic effects is
also not the same as the one computed in the DFPT part of ABINIT, that
includes exchange-correlation effects.

We now enter the fourth dataset. As for dataset 3, after some general
information (origin of WFK file, header, description of unit cell, **k**-points,
**q**-points), the description of the FFT grid and jellium parameters, there is
the echo of parameters for the plasmon-pole model, and the inverse dielectric
function (the screening). The self-energy operator has been constructed, and
one can evaluate the GW energies for each state.

The final results are:

```yaml
--- !SelfEnergy_ee
kpoint     : [   0.000,    0.000,    0.000, ]
spin       : 1
KS_gap     :    2.544
QP_gap     :    3.110
Delta_QP_KS:    0.567
data: !SigmaeeData |
     Band     E0 <VxcDFT>   SigX SigC(E0)      Z dSigC/dE  Sig(E)    E-E0       E
        2   4.418 -11.332 -13.262   1.352   0.766  -0.305 -11.775  -0.443   3.975
        3   4.418 -11.332 -13.262   1.352   0.766  -0.305 -11.775  -0.443   3.975
        4   4.418 -11.332 -13.262   1.352   0.766  -0.305 -11.775  -0.443   3.975
        5   6.961 -10.028  -5.550  -4.316   0.766  -0.305  -9.904   0.124   7.085
        6   6.961 -10.028  -5.550  -4.316   0.766  -0.305  -9.904   0.124   7.085
        7   6.961 -10.028  -5.550  -4.316   0.766  -0.305  -9.904   0.124   7.085
...
```

For the desired **k**-point ($\Gamma$ point), for state 4, then state 5, one finds different information
in the SigmaeeData section:

  * E0 is the KS eigenenergy
  * VxcDFT gives the KS exchange-correlation potential expectation value
  * SigX gives the exchange contribution to the self-energy
  * SigC(E0) gives the correlation contribution to the self-energy, evaluated at the KS eigenenergy
  * Z is the renormalisation factor
  * dSigC/dE is the energy derivative of SigC with respect to the energy
  * SigC(E) gives the correlation contribution to the self-energy, evaluated at the GW energy
  * E-E0 is the difference between GW energy and KS eigenenergy
  * E is the final GW quasiparticle energy

In this case, prior to the SigmaeeData section, the direct band gap was also analyzed: KS_gap is the direct KS gap at
that particular **k**-point (and spin, in the case of spin-polarized calculations), QP_gap
is the GW one, and Delta_QP_KS is the difference. This direct gap is always
computed between the band whose number is equal to the number of electrons in
the cell divided by two (integer part, in case of spin-polarized calculation),
and the next one.
This means that the value reported by the code may be wrong if the final QP energies
obtained in the perturbative approach are not ordered by increasing energy anymore.
So it's always a good idea to check that the "gap" reported by the code corresponds
to the real QP direct gap.

!!! warning

    For a metal, these two bands do not systematically
    lie below and above the KS Fermi energy - but the concept of a direct gap is not relevant in that case.
    Moreover one should compute the Fermi energy of the QP system.

It is seen that the KS exchange-correlation potential expectation value for the
state 4 (a valence state) is rather close to the exchange self-energy
correction. For that state, the correlation correction is small, and the
difference between KS and GW energies is also small (0.128 eV). By
contrast, the exchange self-energy is much smaller than the average Kohn-Sham
potential for the state 5 (a conduction state), but the correlation correction
is much larger than for state 4. 
On the whole, the difference between Kohn-Sham and GW energies is not very large, 
but nevertheless, it is quite
important when compared with the size of the gap.

If |AbiPy| is installed on your machine, you can use the |abiopen| script
with the `--print` option to extract the results from the SIGRES.nc file
and print them to terminal:

```
abiopen.py tgw1_1o_DS4_SIGRES.nc -p

================================= Structure =================================
Full Formula (Si2)
Reduced Formula: Si
abc   :   3.839136   3.839136   3.839136
angles:  60.000000  60.000000  60.000000
Sites (2)
  #  SP       a     b     c
---  ----  ----  ----  ----
  0  Si    0     0     0
  1  Si    0.25  0.25  0.25

Abinit Spacegroup: spgid: 0, num_spatial_symmetries: 48, has_timerev: True, symmorphic: True

============================== Kohn-Sham bands ==============================
Number of electrons: 8.0, Fermi level: 4.773 (eV)
nsppol: 1, nkpt: 6, mband: 80, nspinor: 1, nspden: 1
smearing scheme: none, tsmear_eV: 0.272, occopt: 1
Direct gap:
    Energy: 2.544 (eV)
    Initial state: spin: 0, kpt: [+0.000, +0.000, +0.000], weight: 0.031, band: 3, eig: 4.418, occ: 2.000
    Final state:   spin: 0, kpt: [+0.000, +0.000, +0.000], weight: 0.031, band: 4, eig: 6.961, occ: 0.000
Fundamental gap:
    Energy: 0.710 (eV)
    Initial state: spin: 0, kpt: [+0.000, +0.000, +0.000], weight: 0.031, band: 3, eig: 4.418, occ: 2.000
    Final state:   spin: 0, kpt: [+0.500, +0.500, +0.000], weight: 0.094, band: 4, eig: 5.128, occ: 0.000
Bandwidth: 11.985 (eV)
Valence maximum located at:
    spin: 0, kpt: [+0.000, +0.000, +0.000], weight: 0.031, band: 3, eig: 4.418, occ: 2.000
Conduction minimum located at:
    spin: 0, kpt: [+0.500, +0.500, +0.000], weight: 0.094, band: 4, eig: 5.128, occ: 0.000

TIP: Call set_fermie_to_vbm() to set the Fermi level to the VBM if this is a non-magnetic semiconductor


=============================== QP direct gaps ===============================
QP_dirgap: 3.110 (eV) for K-point: [+0.000, +0.000, +0.000] $\Gamma$, spin: 0


============== QP results for each k-point and spin (all in eV) ==============
K-point: [+0.000, +0.000, +0.000] $\Gamma$, spin: 0
   band     e0    qpe  qpe_diago   vxcme  sigxme  sigcmee0  vUme    ze0
1     1  4.418  3.975      3.840 -11.332 -13.262     1.352   0.0  0.766
2     2  4.418  3.975      3.840 -11.332 -13.262     1.352   0.0  0.766
3     3  4.418  3.975      3.840 -11.332 -13.262     1.352   0.0  0.766
4     4  6.961  7.085      7.123 -10.028  -5.550    -4.316   0.0  0.766
5     5  6.961  7.085      7.123 -10.028  -5.550    -4.316   0.0  0.766
6     6  6.961  7.085      7.123 -10.028  -5.550    -4.316   0.0  0.766
```

For further details about the SIGRES.nc file and the AbiPy API see the |SigresFileNb|.

## 2 Preparing convergence studies: Kohn-Sham structure (WFK file) and screening (SCR file)

In the following sections, we will perform different convergence studies. In
order to keep the CPU time at a reasonable level, we will use fake WFK and SCR
data. We will focus on the GW correction for $\Gamma$ point to determine the values of the GW parameters needed
to reach the convergence.
Indeed, we will use a coarse **k**-point grid with one shift only, and we will not vary [[ecut]].
This is a common strategy to find the adequate specific GW parameters before the final calculations, that should be 
done with a sufficiently fine **k**-point grid, and an adequate [[ecut]], in addition to adequate specific GW parameters.

In directory *Work_gw1*, copy the file *tgw1_2.abi*:
```sh
    cp $ABI_TESTS/tutorial/Input/tgw1_2.abi .
```

Edit the *tgw1_2.abi* file, and take the time to examine it.
Then, issue:

```sh
    abinit tgw1_2.abi > tgw1_2.log 2> err &
```

After this step you will need the WFK and SCR files produced in this run for
the next runs. Move *tgw1o_DS2_WFK* to *tgw1o_DS1_WFK* and *tgw1o_DS3_SCR* to *tgw1o_DS1_SCR*.

The next sections are intended to show you how to find the converged values
of parameters that are specific of a GW calculation. The following parameters might
be used to decrease the CPU time and/or the memory requirements, in addition to the well-known k point sampling and [[ecut]].
For [[optdriver]] = 3, one needs to study the convergence with respect to [[ecuteps]] and [[nband]] simultaneously,
while for [[optdriver]] = 4, only the behaviour with respect to [[nband]] should be monitored.
As mentioned above, the global convergence with respect to [[ecut]] and to the number of k points has
to be monitored as well, but the determination of the adequate parameters can be done independently from
the determination of the adequate values for [[ecuteps]] and [[nband]]. Altogether, one has to determine the
adequate values of four parameters in GW calculations, instead of only two in ground-state calculations ([[ecut]] and the number of k points).
The adequate values of [[ecut]] and the number of k points for converged results *might* perhaps be the same as for ground-state 
calculations, but this is not always the case !

We will test the convergence with respect to [[nband]] and [[ecuteps]], simultaneously for [[optdriver]]=3 and =4.
As a side note, there are actually other technical parameters like [[ecutwfn]] or [[ecutsigx]]. 
However, for them, one can use the default value of [[ecut]].
For PAW, [[pawecutdg]] can be tuned as well.

We begin by the convergence study with respect to [[nband]], the most important parameter needed in the self-energy 
calculation, [[optdriver]] = 4.
This is because for the self-energy calculation, we will not need a double dataset loop to check
this convergence (as [[ecuteps]] is not a parameter of the [[optdriver]] = 4 calculation), and we will rely on the previously determined SCR file.

## 3 Convergence of the self-energy with respect to the number of bands

Let us check the convergence of the band gap with respect to the number of bands in the calculation of $\Sigma_c$
with a fixed screening file.  This convergence study is very important. 
*However* most of the time, the converged [[nband]] is similar for $\Sigma_c$ and for $\chi_0$ so that the same
value is taken for both. Here we will proceed carefully and converge the two occurences of [[nband]] independently.

The convergence on the number of bands to calculate $\Sigma_c$ will be
done by defining five datasets, with increasing [[nband]]:

```
    ndtset  5
    nband:  50
    nband+  50
```

In directory *Work_gw1*, copy the file *tgw1_3.abi*:
```sh
    cp $ABI_TESTS/tutorial/Input/tgw1_3.abi .
```

Edit the *tgw1_3.abi* file, and take the time to examine it.
Then, issue:

```sh
    cp tgw1_2o_DS2_WFK tgw1_3o_DS2_WFK
    cp tgw1_2o_DS3_SCR tgw1_3o_DS3_SCR
    abinit tgw1_3.abi > tgw1_3.log 2> err &
```

{% dialog tests/tutorial/Input/tgw1_3.abi %}

Edit the output file. The number of bands used for the self-energy is
mentioned in the fragments of output:

```
     SIGMA fundamental parameters:
     PLASMON POLE MODEL
     number of plane-waves for SigmaX                  283
     number of plane-waves for SigmaC and W            169
     number of plane-waves for wavefunctions           283
     number of bands                                    50
```

Gathering the GW energies for each number of bands, one gets:

```
     number of bands                                   50
        4   4.665 -11.412 -13.527   1.904   0.785  -0.273 -11.578  -0.165   4.500
        5   7.108  -9.962  -4.945  -4.344   0.793  -0.261  -9.428   0.534   7.643

     number of bands                                  100
        4   4.665 -11.412 -13.527   1.768   0.784  -0.275 -11.684  -0.271   4.394
        5   7.108  -9.962  -4.945  -4.470   0.792  -0.263  -9.528   0.434   7.542

     number of bands                                  150
        4   4.665 -11.412 -13.527   1.741   0.784  -0.275 -11.705  -0.293   4.372
        5   7.108  -9.962  -4.945  -4.494   0.792  -0.263  -9.547   0.415   7.523

     number of bands                                  200
        4   4.665 -11.412 -13.527   1.733   0.784  -0.275 -11.711  -0.299   4.366
        5   7.108  -9.962  -4.945  -4.500   0.792  -0.263  -9.553   0.410   7.518

     number of bands                                  250
        4   4.665 -11.412 -13.527   1.731   0.784  -0.275 -11.713  -0.300   4.365
        5   7.108  -9.962  -4.945  -4.502   0.792  -0.263  -9.554   0.408   7.516
```


So that [[nband]] = 100 can be considered converged within 30 meV, which is fair to compare with experimental accuracy.

With |AbiPy|, one can use the |abicomp| script provides to compare multiple SIGRES.nc files
Use the `--expose` option to visualize of the QP gaps extracted from the different netcdf files:

```text
$ abicomp.py sigres tgw1_3o_*_SIGRES.nc -e -sns

Output of robot.get_dataframe():
                       nsppol     qpgap  nspinor  nspden  nband  nkpt  \
tgw1_3o_DS1_SIGRES.nc       1  3.142871        1       1     50     3   
tgw1_3o_DS2_SIGRES.nc       1  3.148588        1       1    100     3   
tgw1_3o_DS3_SIGRES.nc       1  3.151012        1       1    150     3   
tgw1_3o_DS4_SIGRES.nc       1  3.151603        1       1    200     3   
tgw1_3o_DS5_SIGRES.nc       1  3.151485        1       1    250     3   

                       ecutwfn   ecuteps  ecutsigx  scr_nband  sigma_nband  \
tgw1_3o_DS1_SIGRES.nc      8.0  5.062893       8.0         60           50   
tgw1_3o_DS2_SIGRES.nc      8.0  5.062893       8.0         60          100   
tgw1_3o_DS3_SIGRES.nc      8.0  5.062893       8.0         60          150   
tgw1_3o_DS4_SIGRES.nc      8.0  5.062893       8.0         60          200   
tgw1_3o_DS5_SIGRES.nc      8.0  5.062893       8.0         60          250   

                       gwcalctyp  scissor_ene  
tgw1_3o_DS1_SIGRES.nc          0          0.0  
tgw1_3o_DS2_SIGRES.nc          0          0.0  
tgw1_3o_DS3_SIGRES.nc          0          0.0  
tgw1_3o_DS4_SIGRES.nc          0          0.0  
tgw1_3o_DS5_SIGRES.nc          0          0.0  
```

![](gw1_assets/abicomp_sigres_tgw1_3o.png)

Invoking the script without options will open an |ipython| terminal to interact with the AbiPy robot.
Use the `-nb` option to automatically generate a |jupyter| notebook that will open in your browser.
For further details about the API provided by SigRes Robots see the |SigresFileNb|
and the |G0W0LessonNb| for GW calculations powered by AbiPy.


## 4 Convergence of the screening with respect to the number of bands

Now, we come back to the calculation of the screening. Adequate convergence
studies will couple the change of parameters for [[optdriver]] = 3 with a
computation of the GW energy changes. One cannot rely on the convergence of
the macroscopic dielectric constant to assess the convergence of the GW energies.

As a consequence, we will define a double loop over the datasets:

    ndtset      10
    udtset      5  2

The datasets 12,22,32,42 and 52, drive the computation of the GW energies:

    # Calculation of the Self-Energy matrix elements (GW corrections)
    optdriver?2   4
    getscr?2     -1
    ecutsigx      8.0
    nband?2       100

The datasets 11,21,31,41 and 51, drive the corresponding computation of the screening:

    # Calculation of the screening (epsilon^-1 matrix)
    optdriver?1  3

In this latter series, we will have to vary the two different parameters [[ecuteps]] and [[nband]].

Let us begin with [[nband]].
This convergence study is rather important. It can be done at the same time as
the convergence study for the number of bands for the self-energy. Note that
the number of bands used to calculate both the screening and the self-energy
can be lowered by a large amount by resorting to the extrapolar technique (see
the input variable [[gwcomp]]).

Second, we check the convergence on the number of bands in the calculation of
the screening. This will be done by defining five datasets, with increasing [[nband]]:

    nband11  25
    nband21  50
    nband31  100
    nband41  150
    nband51  200


In directory *Work_gw1*, copy the file *tgw1_4.abi*:
```sh
    cp $ABI_TESTS/tutorial/Input/tgw1_4.abi .
```

Edit the *tgw1_4.abi* file, and take the time to examine it.
Then, issue:
```sh
    cp tgw1_2o_DS2_WFK tgw1_4o_DS2_WFK
    abinit tgw1_4.abi > tgw1_4.log 2> err &
```

{% dialog tests/tutorial/Input/tgw1_4.abi %}

Edit the output file. The number of bands used for the wavefunctions in the
computation of the screening is mentioned in the fragments of output:

```
     EPSILON^-1 parameters (SCR file):
     dimension of the eps^-1 matrix on file            169
     dimension of the eps^-1 matrix used               169
     number of plane-waves for wavefunctions           283
     number of bands                                    25
```


Gathering the GW energies for each number of bands, one gets:

```
  number of bands                                    25
        4   4.665 -11.412 -13.527   1.968   0.786  -0.273 -11.527  -0.115   4.550
        5   7.108  -9.962  -4.945  -4.279   0.796  -0.257  -9.375   0.587   7.696
  number of bands                                    50
        4   4.665 -11.412 -13.527   1.798   0.784  -0.275 -11.661  -0.248   4.417
        5   7.108  -9.962  -4.945  -4.446   0.789  -0.268  -9.512   0.451   7.559
  number of bands                                   100
        4   4.665 -11.412 -13.527   1.708   0.784  -0.276 -11.731  -0.318   4.347
        5   7.108  -9.962  -4.945  -4.523   0.791  -0.264  -9.571   0.391   7.499
  number of bands                                   150
        4   4.665 -11.412 -13.527   1.685   0.783  -0.276 -11.749  -0.336   4.329
        5   7.108  -9.962  -4.945  -4.542   0.790  -0.265  -9.586   0.376   7.485
  number of bands                                   200
        4   4.665 -11.412 -13.527   1.678   0.783  -0.277 -11.754  -0.341   4.324
        5   7.108  -9.962  -4.945  -4.549   0.790  -0.265  -9.592   0.370   7.479

```



So that the computation using 100 bands can be considered converged within 30 meV.
Note that the value of [[nband]] that gives a converged dielectric matrix is usually of the same order of magnitude
than the one that gives a converged $\Sigma_c$.

## 5 Convergence of the screening matrix with respect to the number of planewaves

Then, we check the convergence on the number of plane waves in the
calculation of the screening. This will be done by defining six datasets, with
increasing [[ecuteps]]:

    ecuteps:?     3.0
    ecuteps+?     1.0

In directory *Work_gw1*, get the file *tgw1_5.abi*:
```sh
    cp $ABI_TESTS/tutorial/Input/tgw1_5.abi .
```

Edit the *tgw1_5.abi* file, and take the time to examine it.
Then, issue:

```sh
    cp tgw1_2o_DS2_WFK tgw1_5o_DS2_WFK
    abinit tgw1_5.abi > tgw1_5.log 2> err &
```

{% dialog tests/tutorial/Input/tgw1_5.abi %}

Edit the output file. The number of bands used for the wavefunctions in the
computation of the screening is mentioned in the fragments of output:

```
     EPSILON^-1 parameters (SCR file):
     dimension of the eps^-1 matrix                     59
```

Gathering the GW energies for each number of bands, one gets:

```
     dimension of the eps^-1 matrix                     59
        4   4.665 -11.412 -13.527   1.899   0.786  -0.272 -11.582  -0.169   4.496
        5   7.108  -9.962  -4.945  -4.462   0.791  -0.264  -9.523   0.440   7.548
     dimension of the eps^-1 matrix                    113
        4   4.665 -11.412 -13.527   1.755   0.784  -0.275 -11.694  -0.282   4.383
        5   7.108  -9.962  -4.945  -4.513   0.791  -0.264  -9.563   0.400   7.508
     dimension of the eps^-1 matrix                    137
        4   4.665 -11.412 -13.527   1.728   0.784  -0.276 -11.715  -0.303   4.362
        5   7.108  -9.962  -4.945  -4.518   0.791  -0.264  -9.567   0.395   7.504
     dimension of the eps^-1 matrix                    169
        4   4.665 -11.412 -13.527   1.708   0.784  -0.276 -11.731  -0.318   4.347
        5   7.108  -9.962  -4.945  -4.523   0.791  -0.264  -9.571   0.391   7.499
     dimension of the eps^-1 matrix                    259
        4   4.665 -11.412 -13.527   1.696   0.784  -0.276 -11.740  -0.328   4.338
        5   7.108  -9.962  -4.945  -4.527   0.791  -0.264  -9.574   0.388   7.496
     dimension of the eps^-1 matrix                    283
        4   4.665 -11.412 -13.527   1.695   0.784  -0.276 -11.741  -0.329   4.337
        5   7.108  -9.962  -4.945  -4.527   0.791  -0.264  -9.575   0.388   7.496

```



So that ecuteps = 6.0 ([[npweps]] = 169) can be considered converged within 10 meV.

At this stage, we know that for the screening computation, we need
[[ecuteps]] = 6.0 Ha and [[nband]] = 100.

Of course, until now, we have skipped the most difficult part of the
convergence tests: the convergence in the number of **k**-points. It is as
important to check the convergence on this parameter, than on the other ones.
However, this might be very time consuming, since the CPU time scales as the
square of the number of **k**-points (roughly), and the number of k-points can
increase very rapidly from one possible grid to the next denser one. This is
why we will leave this out of the present tutorial, and consider that we
already know a sufficient **k**-point grid, for the last calculation.

As discussed in [[cite:Setten2017]], the convergence study for **k**-points
the number of bands and the cutoff energies can be decoupled
in the sense that one can start from a reasonaby coarse k-mesh to find
the converged values of [[nband]], [[ecuteps]], [[ecutsigx]] and then
fix these values and look at the convergence with respect to the BZ mesh.

## 6 Calculation of the GW corrections for the band gap at the zone center

Now we try to perform a GW calculation for a real problem: the calculation of
the GW corrections for the direct band gap of bulk silicon at the $\Gamma$ point.

In directory *Work_gw1*, get the file *tgw1_6.abi*:
```sh
    cp $ABI_TESTS/tutorial/Input/tgw1_6.abi .
```

Then, edit the *tgw1_6.abi* file, and, without examining it, comment the line

```
     ngkpt    2 2 2    # Density of k points used for the automatic tests of the tutorial
```

and uncomment the line

```
    #ngkpt    4 4 4    # Density of k points needed for a converged calculation
```

Then, issue:

```sh
    abinit tgw1_6.abi > tgw1_6.log 2> err &
```

This job lasts a couple of minutes or so. It is worth to run it before the examination of the input file.
Now, you can examine it.

{% dialog tests/tutorial/Input/tgw1_6.abi %}

We need the usual part of the input file to perform a ground state
calculation. This is done in datasets 1 and 2. At the end of dataset 2, we print out the density and wavefunction files.
We use a set of 19 **k**-points in the Irreducible Brillouin Zone.
This set of **k**-points is not shifted so it contains the $\Gamma$ point.

In dataset 3 we calculate the screening. The screening calculation is very
time-consuming. So, we have decided to decrease a bit the parameters found in
the previous convergence studies. Indeed, [[nband]] has been decreased from
100 to 50. The CPU time of this part is linear with
respect to this parameter (or more exactly, with the number of conduction
bands). Thus, the CPU time has been decreased by a factor of 2. Referring to
our previous convergence study, we see that the absolute accuracy on the GW
energies is now on the order of 0.2 eV only. This would be annoying for the absolute positioning of the band energy
as required for band-offset or ionization potential of finite systems.
However, as long as we are only interested in the gap energy that is fine enough.

Finally, in dataset 4, we calculate the self-energy matrix element at $\Gamma$, using
the previously determined parameters.

You should obtain the following results:

```yaml
--- !SelfEnergy_ee
iteration_state: {dtset: 4, }
kpoint     : [   0.000,    0.000,    0.000, ]
spin       : 1
KS_gap     :    2.564
QP_gap     :    3.196
Delta_QP_KS:    0.632
data: !SigmaeeData |
     Band     E0 <VxcDFT>   SigX SigC(E0)      Z dSigC/dE  Sig(E)    E-E0       E
        2   4.369 -11.316 -12.769   0.817   0.765  -0.308 -11.803  -0.487   3.882
        3   4.369 -11.316 -12.769   0.817   0.765  -0.308 -11.803  -0.487   3.882
        4   4.369 -11.316 -12.769   0.817   0.765  -0.308 -11.803  -0.487   3.882
        5   6.933 -10.039  -5.840  -4.010   0.765  -0.307  -9.894   0.144   7.078
        6   6.933 -10.039  -5.840  -4.010   0.765  -0.307  -9.894   0.144   7.078
        7   6.933 -10.039  -5.840  -4.010   0.765  -0.307  -9.894   0.144   7.078
...
```


So that the DFT energy gap in $\Gamma$ is about 2.564 eV, while the GW correction is
about 0.632 eV, so that the GW band gap found is 3.196 eV.

One can compare now what have been obtained to what one can get from the literature.

```
     EXP         3.40 eV   Landolt-Boernstein

     DFT (LDA)
     LDA         2.57 eV   L. Hedin, Phys. Rev. 139, A796 (1965)
     LDA         2.57 eV   M.S. Hybertsen and S. Louie, PRL 55, 1418 (1985)
     LDA (FLAPW) 2.55 eV   N. Hamada, M. Hwang and A.J. Freeman, PRB 41, 3620 (1990)
     LDA (PAW)   2.53 eV   B. Arnaud and M. Alouani, PRB 62, 4464 (2000)
     LDA         2.53 eV   present work

     GW          3.27 eV   M.S. Hybertsen and S. Louie, PRL 55, 1418 (1985)
     GW          3.35 eV   M.S. Hybertsen and S. Louie, PRB 34, 5390 (1986)
     GW          3.30 eV   R.W. Godby, M. Schlueter, L.J. Sham, PRB 37, 10159 (1988)
     GW  (FLAPW) 3.30 eV   N. Hamada, M. Hwang and A.J. Freeman, PRB 41, 3620 (1990)
     GW  (FLAPW) 3.12 eV   W. Ku and A.G. Eguiluz, PRL 89, 126401 (2002)
     GW          3.20 eV   present work
```

The values are spread over an interval of 0.2 eV. They depend on the details
of the calculations. In the case of pseudopotential calculations, they depend
of course on the pseudopotential used. However, a GW result is hardly
more accurate than 0.1 eV, in the present state of the art. But this goes also
with the other source of inaccuracy, the choice of the pseudopotential, that
can arrive up to even 0.2 eV. This can also be taken into account when
choosing the level of accuracy for the convergence parameters in the GW calculation.
As a reasonable target, the numerical sources of errors, due to insufficient [[ecuteps]], [[nband]], k point grid, 
should be kept lower than 0.02 or 0.03 eV.

## 7 How to compute GW band structures

Finally, it is possible to calculate a full GW band plot of a system via interpolation.
There are three possible techniques.

The first one is based on the use of Wannier functions to interpolate a few selected points
in the IBZ obtained using the direct GW approach [[cite:Hamann2009]].
You need to have the Wannier90 plug-in installed.
See the directory tests/wannier90, test case 03, for an example of a file where a GW calculation
is followed by the use of Wannier90.

{% dialog tests/wannier90/Input/t03.abi %}

The wannier interpolation is a very accurate method, can handle band crossings
but it may require additional work to obtain well localized wannier functions.
Another practical way follows from the
fact that the QP energies, similarly to the KS eigenvalues,
must fulfill the symmetry properties:

$$
\ee(\kpG) = \ee(\kk)
$$

and

$$
\ee(S\kk) = \ee(\kk)
$$

where $\GG$ is a reciprocal lattice vector and $S$ is a rotation of the point group of the crystal.
Therefore it's possible to employ the star-function interpolation by Shankland, Koelling and Wood
[[cite:Euwema1969]], [[cite:Koelling1986]]
in the improved version proposed by [[cite:Pickett1988]] to fit the ab-initio results.
This interpolation technique, by construction, passes through the initial points and satisfies
the basic symmetry property of the band energies.
It should be stressed, however, that this Fourier-based method can have problems in the presence of band crossings
that may cause unphysical oscillations between the ab-initio points.
To reduce this spurious effect, we suggest to interpolate the GW corrections instead of the GW energies.
The corrections, indeed, are usually smoother in k-space and the resulting fit is more stable.
A python example showing how to construct an interpolated scissor operator with AbiPy is available
[here](http://abinit.github.io/abipy/gallery/plot_qpbands_with_interpolation.html#sphx-glr-gallery-plot-qpbands-with-interpolation-py)

The third method uses the fact that the GW corrections are usually linear with the energy,
for each group of bands. This is evident when reporting on a plot the GW correction with
respect to the 0-order KS energy for each state.
One can then simply correct the KS band structure at any point, by using a GW correction for the
**k**-points where it has not been calculated explicitly, using a fit of the GW
correction at a sparse set of points.
A python example showing how to construct an energy-dependent scissor operator with AbiPy is available
[here](http://abinit.github.io/abipy/gallery/plot_qpbands_with_scissor.html#sphx-glr-gallery-plot-qpbands-with-scissor-py).

## 8 Advanced features of GW calculations

The user might switch to the [second GW tutorial](/tutorial/gw2) before
coming back to the present section.

### Calculations without using the Plasmon-Pole model

In order to circumvent the plasmon-pole model, the GW frequency convolution
has to be calculated explicitly along the real axis. This is a tough job,
since G and W have poles along the real axis. Therefore it is more convenient
to use another path of integration along the imaginary axis plus the residues
enclosed in the path.

Consequently, it is better to evaluate the screening for imaginary frequencies
(to perform the integration) and also for real frequencies (to evaluate the
contributions of the residues that may enter into the path of integration).
The number of imaginary frequencies is set by the input variable [[nfreqim]].
The regular grid of real frequencies is determined by the input variables
[[nfreqre]], which sets the number of real frequencies, and [[freqremax]],
which indicates the maximum real frequency used.

The method is particularly suited to output the spectral function (contained
in file out.sig). The grid of real frequencies used to calculate the spectral
function is set by the number of frequencies (input variable [[nfreqsp]]) and
by the maximum frequency calculated (input variable [[freqspmax]]).

### Self-consistent calculations

The details in the implementation and the justification for the approximations
retained can be found in [[cite:Bruneval2006]].
The only added input variables are [[getqps]] and [[irdqps]]. These variables
concerns the reading of the _QPS file, that contains the eigenvalues and the
unitary transform matrices of a previous quasiparticle calculation. QPS stands
for "QuasiParticle Structure".

The only modified input variables for self-consistent calculations are
[[gwcalctyp]] and [[bdgw]].
When the variable [[gwcalctyp]] is in between 0 and 9, The code calculates the
quasiparticle energies only and does not output any QPS file (as in a standard GW run).
When the variable [[gwcalctyp]] is in between 10 and 19, the code calculates
the quasiparticle energies only and outputs them in a QPS file.
When the variable [[gwcalctyp]] is in between 20 and 29, the code calculates
the quasiparticle energies and wavefunctions and outputs them in a QPS file.

For a full self-consistency calculation, the quasiparticle wavefunctions are
expanded in the basis set of the KS wavefunctions. The variable
[[bdgw]] now indicates the size of all matrices to be calculated and
diagonalized. The quasiparticle wavefunctions are consequently linear
combinations of the KS wavefunctions in between the min and max values of [[bdgw]].

A correct self-consistent calculation should consist of the following runs:

  * 1) Self-consistent KS calculation: outputs a WFK file
  * 2) Screening calculation (with KS inputs): outputs a SCR file
  * 3) Sigma calculation (with KS inputs): outputs a QPS file
  * 4) Screening calculation (with the WFK, and QPS file as an input): outputs a new SCR file
  * 5) Sigma calculation (with the WFK, QPS and the new SCR files): outputs a new QPS file
  * 6) Screening calculation (with the WFK, the new QPS file): outputs a newer SCR file
  * 7) Sigma calculation (with the WFK, the newer QPS and SCR files): outputs a newer QPS
  * ............ and so on, until the desired accuracy is reached

Note that for Hartree-Fock calculations a dummy screening is required for
initialization reasons. Therefore, a correct HF calculations should look like

  * 1) Self-consistent KS calculation: outputs a WFK file
  * 2) Screening calculation using very low convergence parameters (with KS inputs): output a **dummy** SCR file
  * 3) Sigma calculation (with KS inputs): outputs a QPS file
  * 4) Sigma calculation (with the WFK and QPS files): outputs a new QPS file
  * 5) Sigma calculation (with the WFK and the new QPS file): outputs a newer QPS file
  * ............ and so on, until the desired accuracy is reached

In the case of a self-consistent calculation, the output is slightly more complex:
**For instance, at iteration 2**

```yaml
--- !SelfEnergy_ee
iteration_state: {dtset: 3, }
kpoint     : [   0.500,    0.250,    0.000, ]
spin       : 1
KS_gap     :    3.684
QP_gap     :    5.764
Delta_QP_KS:    2.080
data: !SigmaeeData |
     Band     E_DFT   <VxcDFT>   E(N-1)  <Hhartree>   SigX  SigC[E(N-1)]    Z     dSigC/dE  Sig[E(N)]  DeltaE  E(N)_pert E(N)_diago
        1    -3.422   -10.273    -3.761     6.847   -15.232     4.034     1.000     0.000   -11.198    -0.590    -4.351    -4.351
        2    -0.574   -10.245    -0.850     9.666   -13.806     2.998     1.000     0.000   -10.807    -0.291    -1.141    -1.141
        3     2.242    -9.606     2.513    11.841   -11.452     1.931     1.000     0.000    -9.521    -0.193     2.320     2.320
        4     3.595   -10.267     4.151    13.866   -11.775     1.842     1.000     0.000    -9.933    -0.217     3.934     3.934
        5     7.279    -8.804     9.916    16.078    -4.452    -1.592     1.000     0.000    -6.044     0.119    10.034    10.035
        6    10.247    -9.143    13.462    19.395    -4.063    -1.775     1.000     0.000    -5.838     0.095    13.557    13.557
        7    11.488    -9.704    15.159    21.197    -4.061    -1.863     1.000     0.000    -5.924     0.113    15.273    15.273
        8    11.780    -9.180    15.225    20.958    -3.705    -1.893     1.000     0.000    -5.598     0.135    15.360    15.360
...
```

The columns are

  * **Band**: Index of the band
  * **E_DFT**: DFT eigenvalue
  * **VxcDFT**: Diagonal expectation value of the xc potential in between DFT bra and ket
  * **E(N-1)**: Quasiparticle energy of the previous iteration (equal to DFT for the first iteration)
  * **Hhartree**: Diagonal expectation value of the Hartree Hamiltonian (equal to E_DFT - VxcDFT for the first iteration only)
  * **SigX**: Diagonal expectation value of the exchange self-energy
  * **SigC[E(N-1)]**: Diagonal expectation value of the correlation self-energy
    (evaluated for the energy of the preceeding iteration)
  * **Z**: Quasiparticle renormalization factor Z (taken equal to 1 in methods HF, SEX, COHSEX and model GW)
  * **dSigC/dE**: Derivative of the correlation self-energy with respect to the energy
  * **Sig[E(N)]**: Total self-energy for the new quasiparticle energy
  * **DeltaE**: Energy difference with respect to the previous step
  * **E(N)_pert**: QP energy as obtained by the usual perturbative method
  * **E(N)_diago**: QP energy as obtained by the full diagonalization
---
authors: JWZ, PhG,  MVeithen,  XG, NAP
---

# Tutorial on static non-linear properties

## Electronic non-linear susceptibility, non-resonant Raman tensor, electro-optic effect.

This tutorial shows how to obtain the following non-linear physical properties, for an insulator:

  * The non-linear optical susceptibilities
  * The Raman tensor of TO and LO modes
  * The electro-optic coefficients

We will study the compound AlP. During the preliminary steps needed to compute
non-linear properties, we will also obtain several linear response properties:

  * The Born effective charges
  * The dielectric constant
  * The proper piezoelectric tensor (clamped and relaxed ions)

Finally, we will also compute the derivative of the susceptibility tensor with
respect to atomic positions (Raman tensor), using finite differences.

The user should have already completed several advanced tutorials, including:
[Response-Function 1](/tutorial/rf1), [Response-Function 2](/tutorial/rf2),
[Polarization and Finite electric fields](/tutorial/ffield), and the
[Elastic properties](/tutorial/elastic).

This tutorial should take about 1 hour and 30 minutes.

[TUTORIAL_README]

## 1 Ground-state properties of AlP and general parameters

*Before beginning, you might consider working in a different subdirectory, as for the other tutorials.
For example, create Work_NLO in \$ABI_TESTS/tutorespfn/Input*.

In order to save some time, you might immediately start running a calculation.
Copy the file *tnlo_2.abi* from *\$ABI_TESTS/tutorespfn/Input* to *Work_NLO*.
Run `abinit` on this file; it will take several minutes on a desktop PC.

In this tutorial we will assume that the ground-state properties of AlP have
been previously obtained, and that the corresponding convergence studies have
been done. Let us emphasize that, from our experience, accurate and trustable
results for third order energy derivatives usually require a extremely high
level of convergence. So, even if it is not always very apparent here, careful
convergence studies must be explicitly performed in any practical study you
attempt after this tutorial. As usual, convergence tests must be done on
the property of interest (thus it is insufficient to determine parameters giving
proper convergence only on the total energy, and to use them blindly for non-linear
properties).

We will adopt the following set of generic parameters (quite similar to those
in the
[tutorial on Polarization and finite electric field](/tutorial/ffield)):
```
acell   3*7.1391127387E+00
rprim      0.0000000000E+00  7.0710678119E-01  7.0710678119E-01
           7.0710678119E-01  0.0000000000E+00  7.0710678119E-01
           7.0710678119E-01  7.0710678119E-01  0.0000000000E+00
ecut    2.8
ecutsm  0.5
dilatmx 1.05
nband   4 (=number of occupied bands)
ngkpt   6 6 6
nshiftk 4
shiftk  0.5 0.5 0.5
        0.5 0.0 0.0
        0.0 0.5 0.0
        0.0 0.0 0.5

pseudopotentials  Pseudodojo_nc_sr_04_pw_standard_psp8/P.psp8
                  Pseudodojo_nc_sr_04_pw_standard_psp8/Al.psp8
```

In principle, the [[acell]] to be used should be the one corresponding to the
optimized structure at the [[ecut]], and [[ngkpt]] combined with [[nshiftk]]
and [[shiftk]], chosen for the calculations. Unfortunately, for the purpose of
this tutorial, in order to limit the duration of the runs, we have to work at
an unusually low cutoff of 2.8 Ha for which the optimized lattice constant is
unrealistic and equal to 7.139 Bohr (instead of the converged value of 7.251).
In what follows, the lattice constant has been arbitrarily fixed to 7.139.
Bohr. For comparison, results with [[ecut]] = 5 and 30 are also reported and, in those
cases, were obtained at the optimized lattice constants of 7.273 and 7.251 Bohr. For those
who would like to try later, convergence tests and structural optimizations
can be done using the file *\$ABI_TESTS/tutorespfn/Input/tnlo_1.abi*.

{% dialog tests/tutorespfn/Input/tnlo_1.abi %}

Before going further, you might refresh your memory concerning the other variables:
[[ecutsm]], [[dilatmx]], and [[nbdbuf]].

## 2 Linear and non-linear responses from density functional perturbation theory (DFPT)

As a theoretical support to this section of the tutorial, you might consider
reading [[cite:Veithen2005]].

In the first part of this tutorial, we will describe how to compute various
linear and non-linear responses directly connected to second-order and third-
order derivatives of the energy, using DFPT. From the $(2n+1)$ theorem,
computation of energy derivatives up to third order only requires the
knowledge of the ground-state and first-order wavefunctions, see
[[cite:Gonze1989]] and [[cite:Gonze1995]].
Our study will therefore include the following steps : (i) computation of the
ground-state wavefunctions and density; (ii) determination of
wavefunctions derivatives and construction of the related databases for second and third-
order energy derivatives, (iii) combination of the different databases and
analysis to get the physical properties of interest.

These steps are closely related to what was done for the dielectric and dynamical
properties, except that an additional database for third-order energy
derivatives will now be computed during the run. Only selected third-order
derivatives are implemented at present in ABINIT, and concern responses to electric
field and atomic displacements:

  * non-linear optical susceptibilities
    (related to a third-order derivative of the energy
    with respect to electric fields at clamped nuclei positions)

  * Raman susceptibilities (mixed third-order derivative of the energy, twice with respect
    to electric fields at clamped nuclei positions, and once with respect to atomic displacement)

  * Electro-optic coefficients (related to a third-order derivative of the energy with respect to electric fields,
    two of them being optical fields, that is, for clamped ionic positions,  and one of them being a static field,
    for which the ionic positions are allowed to relax)

The various steps are combined into a single input file.

There are two implementations available in ABINIT for the computation of third-order derivatives which includes at least one electric field perturbation (so all tensors mentioned above).
First we will present the *PEAD* method, as described in [[cite:Veithen2005]],
 then we will present the *full DFPT* method, as described in [[cite:Gonze2020]] and [[cite:Romero2020]].
In the latter case, the input file is slightly modified so the reader interested in *full DFPT* should read the *PEAD* part first.
Only the *full DFPT* implementation is available for PAW pseudopotentials.

**Responses to electric fields, atomic displacements, and strains (with PEAD)**

Let us examine the file *tnlo_2.abi*. 

{% dialog tests/tutorespfn/Input/tnlo_2.abi %}

Its purpose is to build databases for
second and third energy derivatives with respect to electric fields, atomic
displacements, and strains. You can edit it. It is made of 5 datasets. The first four data
sets are nearly the same as for a typical linear response calculation: (1)
a self-consistent calculation of the ground state in the irreducible Brillouin Zone;
(2) non self-consistent calculation of the ground state
to get the wavefunctions over the full Brillouin Zone; (3) Computation of the derivatives of the
wavefunctions with respect to k points, using DFPT; (4)
second derivatives of the energy and related first-order wavefunctions
with respect to electric field, atomic displacements, and strains; and
finally (5) the third derivative calculations. Some
specific features must however be explicitly specified in order to prepare for the
non-linear response step (dataset 5). First, it is mandatory to specify:
```
nbdbuf  0
nband   4 (= number of valence bands)
```
so that only filled bands are treated.
Also, in dataset 4, it is required to impose [[prtden]], and [[prepanl]]
```
prtden4    1
prepanl4   1
```
The purpose for [[prtden]] is to obtain the first order densities (in
addition to the first order wavefunctions). The purposes
of [[prepanl]]  are (i) to constrain [[kptopt]] = 2 even in the
computation of phonons where ABINIT usually take advantages of symmetry
irrespective of kptopt and (ii) compute the electric field derivatives in the
3 directions of space, irrespective of the symmetry.

Note also that while the strain perturbation is not used in dataset 5, the information
it provides is necessary to obtain some of the relaxed ion properties that
will be examined later.

The input to dataset 5 trigger the various third-order derivatives to be
computed. This section includes the following:
```
   getwfk5    2
  get1den5    4
   get1wf5    4
   kptopt5    2
optdriver5    5
  d3e_pert1_elfd5    1
  d3e_pert1_phon5    1
 d3e_pert1_atpol5    1 2
   d3e_pert1_dir5    1 1 1
  d3e_pert2_elfd5    1
   d3e_pert2_dir5    1 1 1
  d3e_pert3_elfd5    1
   d3e_pert3_dir5    1 1 1
```
The first three lines retrieve the ground state and first-order densities and wavefunctions.
[[optdriver]] 5 triggers the 3rd-order energy calculation. Finally, the `d3e` input
variables determine the 3 perturbations of the 3rd order derivatives, and their
directions. Notice that we compute three derivatives with respect to electric field:
[[d3e_pert1_elfd]], [[d3e_pert2_elfd]], and [[d3e_pert3_elfd]]. These will be combined
to give the necessary data for the nonlinear optical susceptibility. We also
include [[d3e_pert1_phon]], for both atoms in the unit cell ([[d3e_pert1_atpol]]). When combined
later with the electric field perturbations 2 and 3, this will yield the necessary
information for the Raman tensor. Finally, for all three perturbation classes, we compute
the perturbations in all three spatial directions.

If it was not done at the beginning of this tutorial, you can now make the
run. You can have a quick look to the output file to verify that everything is
OK. It contains the values of second and third energy derivatives. It has
however no immediate interest since the information is not presented in a very
convenient format. The relevant information is in fact also stored in the
database files (DDB) generated independently for second and third energy
derivatives at the end of run steps 4 and 5. Keep these databases, as they will be
used later for a global and convenient analysis of the results using ANADDB.

**Responses to electric fields, atomic displacements, and strains (with full DFPT)**

If not already done, the reader should read the part on the *PEAD* method.

Contrary to *PEAD*, with *full DFPT* the electric field is treated analytically,
the same way it is done for the computation of second-order derivatives of the energy (see [Response-Function 1](/tutorial/rf1)).
One needs first order wavefunctions derivatives with respect to k-points (dataset 3 of *tnlo_2.abi*).
to compute second order energy derivatives with an electric field perturbation (dataset 4 of *tnlo_2.abi*).
It turns out that third-order derivatives of the energy, in this context, needs *second* derivatives of wavefunctions with respect to electric fields and k-points.
To compute the latter one also needs *second* derivatives of wavefunctions with respect to k-points only, so in total we need two additional sets of wavefunctions derivatives.

Now you can compare the files *tnlo_2.abi* and *tnlo_6.abi*.

{% dialog tests/tutorespfn/Input/tnlo_6.abi %}

The first four datasets are identical, so we produce ground state quantities, first-order WF derivatives and second derivatives of the energy exactly the same way.
Before computing the non-linear terms, there are two additional datasets to compute the needed second-order WF derivatives, see dataset 5 and 6.
They are very similar to dataset 3 (computation of d/dk WF derivatives), except for the keywords activating second-order WF derivatives computation ([[rf2_dkdk]] and [[rf2_dkde]])
and the reading of the needed files:
```
rf2_dkdk5    1
  getddk5    3
 prepanl5    1
```
and
```
rf2_dkde6    1
  getddk6    3
 getdkdk6    5
 get1den6    4
getdelfd6    4
 prepanl6    1
```
The use of [[prepanl]] here makes use of crystal symmetries to compute only directions that will be needed for non-linear terms.
The latter are computed in dataset 7. Compared to *tnlo_2.abi*, it contains three additional lines:
```
getddk7     3
getdkde7    6
usepead7    0
```
The first two arguments specify which files to read to get d/dk and d/dkde WF derivatives.
[[usepead]] is the keyword controlling which implementation to use, which has to be 0 to use *full DFPT*.

If not already done, you can now run the code with *tnlo_6.abi*.
Even with a very small cutoff of 2.8 Ha, the non-linear susceptibility tensor differs only by few percents comparing *PEAD* and *full DFPT*,
and the first-order change in the electronic dielectric susceptibility tensor differs by less than one percent.
These differences come from convergence effects which can be reduced increasing the cutoff energy and most importantly the number of k-points.

!!! note
    With the same set of parameters, the *full DFPT* method takes obviously more CPU times than *PEAD*, 
    as we have to compute second-order WF derivatives.
    However, it is expected that the *full DFPT* method converges faster with the number of k-points,
    so one could actually save CPU time and memory using *full DFPT* to get a converged result.


As in the *PEAD* case, the code produced DDB files that can be used for the rest of this tutorial.

**Merge of the DDB.**

At this stage, using either the *PEAD* or *full DFPT* method, all the relevant energy derivatives have been obtained and are
stored in individual databases.
In the following we use the data produced with *PEAD*, but one can use *full DFPT* instead and compare.

The individual databases must be combined with the
[[help:mrgddb|MRGDDB]] merge
utility in order to get a complete database *tnlo_3.ddb.out*. Explicitly, you
should merge the files *tnlo_2o_DS4_DDB* and *tnlo_2o_DS5_DDB*.
An input file that can be piped into MRGDDB is provided as *tnlo_3.abi*. 

{% dialog tests/tutorespfn/Input/tnlo_3.abi %}
You
can use it to perform the merge via *\$ABI_HOME/src/98_main/mrgddb < tnlo_3.abi*.

**Analysis of the DDB.**

We are now ready for the analysis of the results using [[help:anaddb|ANADDB]]. You can copy
the files *\$ABI_TESTS/tutorespfn/Input/tnlo_4.abi* and
*\$ABI_TESTS/tutorespfn/Input/tnlo_4.files* to *Work-NLO*. 

{% dialog tests/tutorespfn/Input/tnlo_4.files tests/tutorespfn/Input/tnlo_4.abi %}

You already used
ANADDB previously. It is located in the same directory as *abinit* and *mrgddb*.
You might copy it, or make an alias. The present input is in
very similar to the one you have used for the analysis of dynamical
and dielectric responses except that some new flags need to be activated.

For the strain perturbation you need to specify [[anaddb:elaflag]], [[anaddb:piezoflag]], and [[anaddb:instrflag]]:
```
  elaflag 3
piezoflag  3
instrflag  1
```
For the non-linear responses you need
```
 nlflag  1
ramansr 1
 alphon  1
 prtmbm  1
```
[[anaddb:nlflag]]=1 activates the non-linear response.

[[anaddb:ramansr]] = 1 will impose the sum rule on the first-order change of the
electronic dielectric susceptibility under atomic displacement, hereafter
referred to as $\frac{d \chi}{d \tau}$. It is a condition of invariance of $\chi$ under
translation of the whole crystal, similar to the acoustic sum rules for
phonons at Gamma or the charge neutrality sum rule on the effective charge $Z^*$.

[[anaddb:prtmbm]]=1 will allow to get the mode by mode phonon contributions of
the ions to the electro-optic coefficients.

[[anaddb:alphon]]=1 will allow to get the previous mode by mode contribution
when aligning the eigenvectors with the cartesian axis of coordinates (in the
input, the principal axis should always be aligned with z for a convenient
analysis of the results).

Finally, the second list of phonon, specified with [[anaddb:nph2l]] and
[[anaddb:qph2l]], must also be explicitly considered to obtain the Raman
efficiencies of longitudinal modes (in a way similar to the computation of
frequencies of longitudinal mode frequencies at Gamma):
```
# Wave vector list no. 2
#***********************
   nph2l  1
   qph2l  1.0 0.0 0.0 0.0
```
You can now run the code `anaddb` as
*\$ABI_HOME/src/98_main/anaddb < tnlo_4.files*.
The results are in the file tnlo_4.abo.
Various interesting physical properties are now directly accessible in this
output in meaningful units. You can go through the file and
identify the results mentioned below. Note that the order in which they are
given below is not the same than the order in which they appear in the
tnlo_4.abo. You will have to jump between different sections of tnlo_4.abo to find them.

For comparison, we report in parenthesis (...) the values obtained with ecut =
5, and for nonlinear responses in brackets [...] the results from [[cite:Veithen2005]].

  * Born effective charge of Al:
```
Z*_Al = 2.207969 (2.233928)
```
  * Optical phonon frequencies at Gamma :
```
w_TO (cm^-1) = 463.2713 (417.4934)
w_LO (cm^-1) = 534.6882 (493.4411)
```
  * Linear optical dielectric constant :
```
Electronic dielectric tensor = 6.12216718 (6.10643103)
```
  * Static dielectric constant :
```
Relaxed ion dielectric tensor = 8.15521897 (8.53019279)
```
Some other quantities, such as the piezoelectric coefficients, are related to the
strain response, and are more extensively discussed in the tutorial on the strain perturbation.

  * Proper piezoelectric coefficients :
```
clamped ion (C/m^2) = -0.58864089 (-0.58710170)
relaxed ion (C/m^2) =  0.26741085 ( 0.11459002)
```
Finally, different quantities are related to non-linear responses.

  * Nonlinear optical susceptibility :
They are directly provided in the output in pm/V. As you can see the value
computed here at ecut = 2.8 is far from the well converged result.
```
d_36 (pm/V)  = 8.934453 (10.174996) [21]
```
  * Electro-optic coefficients:
As we asked for mode by mode decomposition the output provides individual
contributions. We report below a summary of the results. They concern the
clamped r_63 coefficient.
```
Electronic EO constant (pm/V): -0.953493194  (-1.091488899)
Full Ionic EO constant (pm/V):  0.536131045  ( 0.662692165)
     Total EO constant (pm/V): -0.417362150  (-0.428796933)
```

  * Raman properties

The code directly report the Raman susceptibilities $\alpha$ for both transverse (TO)
and longitudinal (LO) optic modes at Gamma:
```
alpha(TO) = 0.004315217 (0.004379774)
alpha(LO) = 0.006863765 (0.007243040)
```

The basic quantity to derive the Raman susceptibilities are the $\partial\chi/\partial\tau$ that
are also reported separately:
```
d chi_23/d tau_1 (Bohr^-1, Al)  = -0.043617186 (-0.043054192)
```

In *cubic semiconductors* it is common to report the Raman polarizability of
optical phonon modes at Gamma by the following expression [[cite:Veithen2005]]:
$$
        a = \Omega_0 \partial\chi/\partial\tau = \sqrt{\mu \Omega_0}\alpha
$$
where $\Omega_0$ is the primitive unit cell volume (reported as `ucvol` in for
example *tnlo_2.abo*),
$\mu$ the reduced mass of
the system, so here $1/\mu = 1/m_{Al} + 1/m_{P}$, and $\alpha$ the Raman susceptibility
tensor. The Raman polarizability $a$ has units of length$^2$. Using alpha(TO) and
alpha(LO) from above, along with $\mu$ and $\Omega_0$, all in atomic units, and then
converting length (Bohr) to Angstroms, we find:
```
a(TO) (Unit: Ang^2)= 3.1424 (3.2794) [4.30]
a(LO) (Unit: Ang^2)= 4.9983 (5.4234) [7.46]
```
## 3 Finite difference calculation of the Raman tensor

For comparison with the DFPT calculation, we can compute $\frac{d \chi}{d \tau}$ for the Al
nucleus from finite differences. In practice, this is achieved by computing
the linear optical susceptibility for 3 different positions of the Al nucleus.
This is done with the file *\$ABI_TESTS/tutorespfn/Input/tnlo_5.abi*. 

{% dialog tests/tutorespfn/Input/tnlo_5.abi %}

This file
uses again the unrealistically low cutoff energy [[ecut]] of 2.8 Ha.
The calculation takes about 2 or 3 minutes on a standard desktop PC.
To run this calculation, copy
*\$ABI_TESTS/tutorespfn/Input/tnlo_5.abi* to your working directory and run
with `abinit`. If you have time,
modify the cutoff to [[ecut]] = 5 Ha, in order to obtain
more realistic results.  This run will take about twice as along as the
2.8 Ha version.

The input file *tnlo_5.abi* contains 12 datasets, arranged in a double loop.
```
ndtset 12
udtset 3 4
```
Input variable [[ndtset]] indicates 12 total sets, as usual, while
the [[udtset]] variable indicates an outer loop of 3 sets, and for each
of these, an inner loop of four sets.

The outer loop is over three sets of atomic positions, set by [[xcart]].
The first is the equilibrium, and the second two have aluminum shifted by
+0.01 and -0.01 Bohr along x:
```
# tau = 0.0 equilibrium
xcart1?    2.5240575146E+00  2.5240575146E+00  2.5240575146E+00
           0.0000000000E+00  0.0000000000E+00  0.0000000000E+00

# tau = 0.01: aluminum shifted along x by 0.01 Bohr
xcart2?    2.5240575146E+00  2.5240575146E+00  2.5240575146E+00
           0.0100000000E+00  0.0000000000E+00  0.0000000000E+00

# tau = -0.01: aluminum shifted along x by -0.01 Bohr
xcart3?    2.5240575146E+00  2.5240575146E+00  2.5240575146E+00
          -0.0100000000E+00  0.0000000000E+00  0.0000000000E+00
```
Then for each of three sets of positions, a standard four-set DFPT
calculation of the electric field response at clamped ion positions
is executed: (1) ground state wavefunctions in the irreducible Brillouin
zone; (2) ground state wavefunctions in the full Brillouin zone; (3)
response to change in k vectors; and (4) response to the electric field,
yielding the dielectric tensor.

After running the calculation, the following is available in the output
file *tnlo_5.abo*. Quoted here are the results at [[ecut]] of 2.8.

For tau = 0:
```
Dielectric tensor, in cartesian coordinates,
   j1       j2             matrix element
dir pert dir pert     real part    imaginary part

 1    4   1    4         6.1221671827        -0.0000000000
 1    4   2    4        -0.0000000000        -0.0000000000
 1    4   3    4        -0.0000000000        -0.0000000000

 2    4   1    4        -0.0000000000        -0.0000000000
 2    4   2    4         6.1221671827        -0.0000000000
 2    4   3    4        -0.0000000000        -0.0000000000

 3    4   1    4        -0.0000000000        -0.0000000000
 3    4   2    4        -0.0000000000        -0.0000000000
 3    4   3    4         6.1221671827        -0.0000000000
```
For tau = +0.01 :
```
Dielectric tensor, in cartesian coordinates,
   j1       j2             matrix element
dir pert dir pert     real part    imaginary part

 1    4   1    4         6.1223310904        -0.0000000000
 1    4   2    4        -0.0000000000        -0.0000000000
 1    4   3    4        -0.0000000000        -0.0000000000

 2    4   1    4        -0.0000000000        -0.0000000000
 2    4   2    4         6.1222249248        -0.0000000000
 2    4   3    4        -0.0055063835        -0.0000000000

 3    4   1    4        -0.0000000000        -0.0000000000
 3    4   2    4        -0.0055063835        -0.0000000000
 3    4   3    4         6.1222249248        -0.0000000000
```
For tau = -0.01 :
```
Dielectric tensor, in cartesian coordinates,
   j1       j2             matrix element
dir pert dir pert     real part    imaginary part

 1    4   1    4         6.1223310830        -0.0000000000
 1    4   2    4        -0.0000000000        -0.0000000000
 1    4   3    4        -0.0000000000        -0.0000000000

 2    4   1    4        -0.0000000000        -0.0000000000
 2    4   2    4         6.1222249159        -0.0000000000
 2    4   3    4         0.0055063810        -0.0000000000

 3    4   1    4        -0.0000000000        -0.0000000000
 3    4   2    4         0.0055063810        -0.0000000000
 3    4   3    4         6.1222249159        -0.0000000000
```
You can extract the value of dchi_23/dtau_1 for Al from the dielectric tensor
(hereafter called eps) above using the following finite-difference formula [unit of bohr^-1] :
```
dchi_23/dtau_1= (1/4 pi) (eps_23[tau=+0.01] - eps_23[tau=-0.01])/2*tau
              = (1/4 pi) (-0.0055063835 - 0.0055063810)/(0.02)
              = -0.0438184
```
This value is close to that obtained at [[ecut]]=2.8 from DFPT (-0.043617186).
When convergence is reached (beware, the k point convergence is extremely
slow, much slower than for other properties), both approaches yield the same result.
You might therefore ask which approach is the most convenient
and should be used in practice.

As a guide, we note that the finite-difference approach give results
very similar to the DFPT ones for a similar cutoff and k-point grid. It is
however more tedious because individual atomic displacement must be
successively considered (which becomes cumbersome for complex crystals)
and the results must then
be converted into appropriate units with risk of error of manipulations.

The DFPT approach is the most convenient and avoids considerable operator intervention.
Everything is reported together (not only $d\chi / d\tau$ but also the full Raman
polarizability tensors) and in appropriate units. It should therefore be
considered as the better choice.

## 4 Plotting the Raman Spectrum

The output of an ANADDB analysis, for example *tnlo_4.abo* as performed here, can be
used to plot a simulated Raman spectrum, including both peak positions and intensities,
which can be compared to experiment. Many details of the process are outlined in [[cite:Caracas2006]].

A post-processing script, written in Python, is available in the ABINIT package:
see *$ABI_HOME/scripts/post_processing/Raman_spec.py*. If you have not installed the whole ABINIT package, but only are using
compiled executables and the documentation on the web, you should make the extra effort of downloading the full package, as mentioned in the ABINIT install notes.

This program reads an input file
that sets the ANADDB file to read, the output file base name, and various processing
parameters. To continue, we suggest copying this script into your working directory, or making
a link to it.

Running *python Raman_spec.py --help* gives an outline of the input file format,
but don't be afraid to open and read the Raman_spec.py file itself for further details on the
file input.
As a start, here is a minimal input file to Raman_spec.py for the tnlo_4.abo run:
```
# filename from anaddb run that created raman tensors
filename tnlo_4.abo

# base name for Raman_spec.py output files
outname AlP.out

# temperature in Kelvin for spectrum (default is zero K)
temp 298.0

# number frequencies (default is 1000)
n_freq 400

# min and max frequencies 
# (default is 0.95 and 1.05 of bands found; default unit is cm^-1, user can instead specify Ha or Hz)
min_freq 200.0
max_freq 800.0

# Lorentzian broadening to apply
# (default is zero; default unit is cm^-1, user can instead specify Ha or Hz)
spread 1.0

# calculation type: 1 is powder
calctype 1
```
You can copy this into an editor and save as for example *AlP.input*. Then execute
```
python Raman_spec.py AlP.input
```
Once complete, examine the various output files produced, which will all be named starting with AlP.out. They are all human readable
ASCII files and well-documented. For example, to visualize the powder spectrum of the TO mode
predicted by your ANADDB run, plot the first two
columns of AlP.out_spec, which give the frequencies and intensities of the powder-averaged Raman spectrum.

The resulting powder-average spectra, plotted here with Gnuplot, is shown below. For the cubic structure calculated here,
the resulting spectra contains a single Raman TO mode corresponding to an XY polarization.

![](nlo_assets/AlP-Raman-ecut-2.8.png)

Finally, if one includes a calculation of the frequency-dependent dielectric tensor during the ANADDB calculation
(see [[anaddb:dieflag]]),
this program extracts that dielectric tensor and prints it to its own file.
---
authors: SPesant, MCote, XG, BAmadon
---

# Tutorial on DFT+U

## The projected density of states of NiO.

This tutorial aims at showing how to perform a DFT+U calculation using Abinit (see also [[cite:Amadon2008a]])

You will learn what is a DFT+U calculation and what are the main input
variables controlling this type of calculation.

It is supposed that you already know how to do PAW calculations using ABINIT.
Please follow the two tutorials on PAW in ABINIT ([PAW1](/tutorial/paw1), [PAW2](/tutorial/paw2)), if this is not the case.

This tutorial should take about 1 hour to complete.

[TUTORIAL_README]

## 0 Short summary of the DFT+U method

The standard Local Density Approximation (LDA), where the exchange and
correlation energy is fit to homogeneous electron gas results, is a functional
that works well for a vast number of compounds. But, for some crystals, the
interactions between electrons are so important that they cannot be
represented by the LDA alone. Generally, these highly correlated materials
contain rare-earth metals or transition metals, which have partially filled *d*
or *f* bands and thus localized electrons.

The LDA tends to delocalize electrons over the crystal, and each electron
feels an average of the Coulombic potential. For highly correlated materials,
the large Coulombic repulsion between localized electrons might not be well
represented by a functional such as the LDA. A way to avoid this problem is to
add a Hubbard-like, localised term, to the LDA density functional. This
approach is known as DFT+U (formerly referred to as LDA+U). In the actual implementation, we
separate localized d or f electrons, on which the Hubbard term will act, from
the delocalized ones (*s* and *p* electrons). The latter are correctly described
by the usual LDA calculation. In order to avoid the double counting of the
correlation part for localized electrons (already included in the LDA,
although in an average manner), another term - called the double-counting
correction - is subtracted from the Hamiltonian.

In Abinit, two double-counting corrections are currently implemented:

-The Full localized limit (FLL) [[cite:Liechtenstein1995]] ([[usepawu]]=1)

-The Around Mean Field (AMF) [[cite:Czyzyk1994]]  ([[usepawu]]=2)

For some systems, the result might depend on the choice of the double-counting method.
However, the two methods generally give similar results.

## 1 Ground state calculation of NiO using LDA

*Before continuing, you might consider to work in a different subdirectory as
for the other tutorials. Why not Work_dftu?
In what follows, the names of files will be mentioned as if you were in this subdirectory.*

Copy the file *tdftu_1.abi* from *\$ABI_TESTS/tutorial/Input* to your *Work_dftu* directory with:

```sh
cd $ABI_TESTS/tutorial/Input
mkdir Work_dftu
cd Work_dftu
cp ../tdftu_1.abi .
```

{% dialog tests/tutorial/Input/tdftu_1.abi %}

Now run the code as usual.
The job should take less than 20 seconds on a laptop. It calculates the LDA
ground state of the NiO crystal. A low cutoff and a small number of k-points
are used in order to speed up the calculation. During this time you can take a
look at the input file.

The NiO crystallizes in the rocksalt structure, with one Ni and one O atom in
the primitive cell (the crystallographic primitive cell). However, NiO is
known to exhibit an antiferromagnetic ordering at low temperature (along the
<111> direction). From the electronic point of view, the true unit cell has
two Ni and two O atoms: the local magnetic moment around the first Ni atom
will have a sign opposite to the one of the other Ni atom.

You should take some time to examine the values used for the input variables
[[xred]], [[rprim]] (note the last line!), [[typat]], [[spinat]], [[nsppol]],
and [[nspden]], that define this antiferromagnetic ordering along the <111>
direction (of a conventional cubic cell).

If you take a look at the output file (tdftu_1.out), you can see the
integrated total density in the PAW spheres (see the [PAW1](/tutorial/paw1)
and [PAW2](/tutorial/paw2) tutorials on PAW formalism). This value roughly
estimates the magnetic moment of NiO:

     Integrated electronic and magnetization densities in atomic spheres:
     ---------------------------------------------------------------------
     Radius=ratsph(iatom), smearing ratsm=  0.0000. Diff(up-dn)=approximate z local magnetic moment.
     Atom    Radius    up_density   dn_density  Total(up+dn)  Diff(up-dn)
        1   1.81432     8.564383     7.188016     15.752398     1.376367
        2   1.81432     7.188016     8.564383     15.752398    -1.376367
        3   1.41465     2.260902     2.260902      4.521804    -0.000000
        4   1.41465     2.260902     2.260902      4.521804     0.000000
     

The atoms in the output file, are listed as in the [[typat]] variable (the
first two are nickel atoms and the last two are oxygen atoms). The results
indicate that spins are located in each nickel atom of the doubled primitive
cell. Fortunately, the LDA succeeds to give an antiferromagnetic ground state
for the NiO. But the result does not agree with the experimental data.

The magnetic moment (the difference between up and down spin on the nickel atom)
range around 1.6-1.9 according to experiments  ([[cite:Cheetham1983]],[[cite:Neubeck1999]],[[cite:Sawatzky1984]],
[[cite:Hufner1984]])
Also, as the Fermi level is at 0.33748 Ha (see the *tdftu_1.abo* file), one
can see (on the *tdftu_1.o_EIG* file that contains eigenvalues for the three k-point of this calculation) that the band gap obtained between the last (24th) occupied band (0.31537 Ha, at k
point 3) and the first (25th) unoccupied band (0.35671 Ha, at kpoint 3) is
approximately 1.1 eV which is lower than the measured value of 4.0-4.3 eV
(This value could be modified using well-converged parameters but would still
be much lower than what is expected). A easier and graphical way to evaluate the gap would be to plot the density
of states (see last section of this tutorial).

Making abstraction of the effect of insufficiently convergence parameters, the
reason for the discrepancy between the DFT-LDA data and the experiments is
first the fact the DFT is a theory for the ground state and second, the lack
of correlation of the LDA. Alone, the homogeneous electron gas cannot
correctly represent the interactions among $d$ electrons of the Ni atom. That is
why we want to improve our functional, and be able to manage the strong correlation in NiO.

## 2 DFT+U with the FLL double-counting

As seen previously, the LDA does not gives good results for the magnetization
and band gap compared to experiments.
At this stage, we will try to improve the correspondence between calculation
and experimental data. First, we will use the DFT(LDA)+U with the Full
localized limit (FLL) double-counting method.

FLL and AMF double-counting expressions are given in the papers listed above,
and use the adequate number of electrons for each spin. For the Hubbard term,
the rotationally invariant interaction is used.

!!! note

    It is important to notice that in order to use DFT+U in Abinit, you must
    employ PAW pseudopotentials.

You should run abinit with the *tdftu_2.abi* input file. This calculation takes
also less than 20 seconds on a laptop.
During the calculation, you can take a look at the input file.

{% dialog tests/tutorial/Input/tdftu_2.abi %}

Some variable describing the DFT+U parameters have been added to the previous file. All
other parameters were kept constant from the preceding calculation. First, you
must set the variable [[usepawu]] to one (for the FLL method) and two (for the
AMF method) in order to enable the DFT+U calculation. Then, with [[lpawu]] you
give for each atomic species ([[znucl]]) the values of angular momentum (l) for
which the DFT+U correction will be applied. The choices are 1 for *p*-orbitals, 2 for *d*-orbitals
and 3 for *f*-orbitals. You cannot treat s orbitals with DFT+U in the
present version of ABINIT. Also, if you do not want to apply DFT+U correction
on a species, you can set the variable to -1. For the case of NiO, we put
[[lpawu]] to 2 for Ni and -1 for O.


!!! note

    The current implementation applies DFT+U correction only inside atomic sphere. To check if this
    approximation is realistic, relaunch the calculation with [[pawprtvol]] equal to three.
    Then search for ph0phiint in the log file:

        pawpuxinit: icount, ph0phiint(icount)= 1  0.90467

    This line indicates that the norm of atomic wavefunctions inside atomic sphere is 0.90, rather close
    to one. In the case of nickel, the approximation is thus realistic. The case where the norm is too small
    (close to 0.5) is discussed in [[cite:Geneste2017]].

Finally, as described in the article cited above for FLL and AMF, we must
define the screened Coulomb interaction between electrons that are treated in
DFT+U, with the help of the variable [[upawu]] and the screened exchange
interaction, with [[jpawu]]. Note that you can choose the energy unit by
indicating at the end of the line the unit abbreviation (e.g. eV or Ha). For
NiO, we will use variables that are generally accepted for this type of compound:

    upawu  8.0 0.0 eV
    jpawu  0.8 0.0 eV


You can take a look at the result of the calculation. The magnetic moment is now:


     Integrated electronic and magnetization densities in atomic spheres:
     ---------------------------------------------------------------------
     Radius=ratsph(iatom), smearing ratsm=  0.0000. Diff(up-dn)=approximate z local magnetic moment.
     Atom    Radius    up_density   dn_density  Total(up+dn)  Diff(up-dn)
        1   1.81432     8.749919     6.987384     15.737302     1.762535
        2   1.81432     6.987384     8.749919     15.737302    -1.762535
        3   1.41465     2.290397     2.290397      4.580793    -0.000000
        4   1.41465     2.290397     2.290397      4.580793    -0.000000


NiO is found antiferromagnetic, with a moment that is in reasonable agreement
with experimental results. Moreover, the system is a large gap insulator with
about 5.3 eV band gap (the 24th band at k point 3 has an eigenenergy of
0.26699 Ha, much lower than the eigenenergy of the 25th band at k point 1,
namely 0.46243 Ha, see the *tdftu_2.o_EIG* file). This number is very approximative, since the very rough
sampling of k points is not really appropriate to evaluate a band gap, still
one obtains the right physics.

A word of caution is in order here. It is NOT the case that one obtain
systematically a good result with the DFT+U method at the first trial. Indeed,
due to the nature of the modification of the energy functional, the landscape
of this energy functional might present numerous local minima (see for examples
[[cite:Jomard2008]] or [[cite:Dorado2009]]).

Unlike DFT+U, for the simple LDA (without U), in the non-spin-polarized case,
there is usually only one minimum, that is the global minimum. So, if it
converges, the self-consistency algorithm always find the same solution,
namely, the global minimum. This is already not true in the case of spin-
polarized calculations (where there might be several stable solutions of the
SCF cycles, like ferromagnetic and ferromagnetic), but usually, there are not
many local minima, and the use of the [[spinat]] input variables allows one to
adequately select the global physical characteristics of the sought solution.

By contrast, with the U, the [[spinat]] input variable is too primitive, and
one needs to be able to initialize a spin-density matrix on each atomic site
where a U is present, in order to guide the SCF algorithm.

The fact that [[spinat]] works for NiO comes from the relative simplicity of this system.

## 3 Initialization of the density matrix

*You should begin by running the tdftu_3.abi file before continuing.*

In order to help the DFT+U find the ground state, you can define the initial
density matrix for correlated orbitals with [[dmatpawu]]. For $d$ orbitals, this variable
must contains $5\times5$ square matrices. There should be one square matrix per nsppol and atom.
So in our case, there are 2 square matrices.
Also, to enable this
feature, [[usedmatpu]] must be set to a non-zero value (default is 0). When
positive, the density matrix is kept to the [[dmatpawu]] value for the
[[usedmatpu]] value steps. For our calculation(tdftu_3.abi) , [[usedmatpu]] is 5,
thus the spin-density matrix is kept constant for 5 SCF steps.
Let's examinates the input dmatpawu

{% dialog tests/tutorial/Input/tdftu_3.abi %}

To understand the density matrix used in the variable [[dmatpawu]] in this input file, have a look
to the section on this variable [[dmatpawu]]. This section show the order to orbitals in the density matrix. With
the help of this section, one can understand that the density matrix corresponds to all orbitals filled except
$e_g$ orbitals for one spin.

In the log file (not the usual output file), you will find for each step, the
calculated density matrix, followed by the imposed density matrix. After the
first 5 SCF steps, the initial density matrix is no longer imposed. Here is a
section of the log file, in which the imposed occupation matrices are echoed:

    -------------------------------------------------------------------------

    Occupation matrix for correlated orbitals is kept constant
    and equal to dmatpawu from input file !
    ----------------------------------------------------------

    == Atom   1 == Imposed occupation matrix for spin 1 ==
         0.90036    0.00000   -0.00003    0.00000    0.00000
         0.00000    0.90036   -0.00001    0.00000    0.00002
        -0.00003   -0.00001    0.91309   -0.00001    0.00000
         0.00000    0.00000   -0.00001    0.90036   -0.00002
         0.00000    0.00002    0.00000   -0.00002    0.91309

    == Atom   1 == Imposed occupation matrix for spin 2 ==
         0.89677   -0.00001    0.00011   -0.00001    0.00000
        -0.00001    0.89677    0.00006    0.00001   -0.00010
         0.00011    0.00006    0.11580    0.00006    0.00000
        -0.00001    0.00001    0.00006    0.89677    0.00010
         0.00000   -0.00010    0.00000    0.00010    0.11580

    == Atom   2 == Imposed occupation matrix for spin 1 ==
         0.89677   -0.00001    0.00011   -0.00001    0.00000
        -0.00001    0.89677    0.00006    0.00001   -0.00010
         0.00011    0.00006    0.11580    0.00006    0.00000
        -0.00001    0.00001    0.00006    0.89677    0.00010
         0.00000   -0.00010    0.00000    0.00010    0.11580

    == Atom   2 == Imposed occupation matrix for spin 2 ==
         0.90036    0.00000   -0.00003    0.00000    0.00000
         0.00000    0.90036   -0.00001    0.00000    0.00002
        -0.00003   -0.00001    0.91309   -0.00001    0.00000
         0.00000    0.00000   -0.00001    0.90036   -0.00002
         0.00000    0.00002    0.00000   -0.00002    0.91309

Generally, the DFT+U functional meets the problem of multiple local minima,
much more than the usual LDA or GGA functionals. One often gets trapped in a
local minimum. Trying different starting points might be important...

## 4 AMF double-counting method

Now we will use the other implementation for the double-counting term in DFT+U
(in Abinit), known as AMF. As the FLL method, this method uses the number of
electrons for each spin independently and the complete interactions $U(m_1,m_2,m_3,m_4)$ and $J(m_1,m_2,m_3,m_4)$.

As in the preceding run, we will start with a fixed density matrix for d
orbitals. You might now start your calculation, with the *tdftu_4.abi*, or skip the calculation, and rely on the reference file
provided in the *\$ABI_TESTS/tutorial/Refs* directory. Examine the *tdftu_4.abi* file.

{% dialog tests/tutorial/Input/tdftu_4.abi %}

The only difference in the input file compared to *tdftu_3.abi* is the
value of [[usepawu]] = 2. We obtain a band gap of 4.75 eV. The value of the
band gap with AMF and FLL is different. However, we have to remember that
these results are not well converged. By contrast, the magnetization,

      Integrated electronic and magnetization densities in atomic spheres:
      ---------------------------------------------------------------------
      Radius=ratsph(iatom), smearing ratsm=  0.0000. Diff(up-dn)=approximate z local magnetic moment.
      Atom    Radius    up_density   dn_density  Total(up+dn)  Diff(up-dn)
         1   1.81432     8.675718     6.993823     15.669541     1.681895
         2   1.81432     6.993823     8.675718     15.669541    -1.681895
         3   1.41465     2.288681     2.288681      4.577361    -0.000000
         4   1.41465     2.288681     2.288681      4.577361     0.000000


is very similar to the DFT+U FLL. 
For other systems, the difference can be more important. FLL is designed
to work well for systems in which occupations of orbitals are 0 or 1 for each
spin. The AMF should be used when orbital occupations are near the average occupancies.

## 5 Projected density of states in DFT+U

Using [[prtdos]] 3, you can now compute the projected d and f density of states.
For more information about projected density of states, for more details see the [PAW1](/tutorial/paw1) tutorial.
---
authors: JB,JBieder
---

# Parallelism for molecular dynamics

## How to perform Molecular Dynamics calculations using parallelism

This tutorial aims at showing how to perform molecular dynamics with ABINIT
using a parallel computer.
You will learn how to launch molecular dynamics calculation and what are the
main input variables that govern convergence and numerical efficiency.

You are supposed to know already some basics of parallelism in ABINIT,
explained in the tutorial
[A first introduction to ABINIT in parallel](/tutorial/basepar), and
[ground state with plane waves](/tutorial/paral_gspw).

This tutorial should take about 1.5 hour to be done and requires to have at
least a 200 CPU core parallel computer.

[TUTORIAL_README]

!!! tip
    
    In this tutorial, most of the images and plots are easily obtained using the post-processing tool
    [qAgate](https://github.com/piti-diablotin/qAgate) or [agate](https://github.com/piti-diablotin/agate), 
    its core engine.
    Any post-process tool will work though !!

## Summary of the molecular dynamics method

The basic idea underlying Ab Initio Molecular Dynamics (AIMD) is to compute
the forces acting on the nuclei from electronic structure calculations that
are performed as the molecular dynamics trajectory is generated.

An AIMD calculation assumes only that the system is composed of nuclei and
electrons, that the Born -Oppenheimer approximation is valid, and that the
dynamics of the nuclei can be treated classically on the ground-state
electronic surface. It allows both equilibrium thermodynamic and dynamical
properties of a system at finite temperature to be computed. For example
melting temperatures, phase transitions, atomic vibrations, structure
factor... but also XANES or IR spectrum can be obtained with this technique.

AIMD deals with supercells of hundred to thousand of atoms (usually, the
larger, the better!). In addition Molecular Dynamics simulations can be
performed for days, weeks or even months! They are therefore very time
consuming and can not be done without the help of high speed and massively
parallel computing.

In the following, when "run ABINIT over *nn* CPU cores" appears, you have to use
a specific command line according to the operating system and architecture of
the computer you are using. This can be for instance: `mpirun -n nn abinit input.abi`
or the use of a specific submission file.

For this tutorial, one needs a working directory.
Why not `Work_paral_moldyn`? e.g. with the commands:

```sh
mkdir -p $ABI_TESTS/tutoparal/Input/Work_paral_moldyn
cd $ABI_TESTS/tutoparal/Input/Work_paral_moldyn
```

## Performing molecular dynamics with ABINIT

There are different algorithms to do molecular dynamics. See the input
variable [[ionmov]], with values 1, 6, 7, 8, 9, 12, 13 and 14. [[dtion]]
controls the ion time step in atomic units of time (one atomic time unit is
2.418884 x 10<sup>-17</sup> seconds, which is the value of Planck's constant in Ha.s).
The default value is 100. You should try several values for [[dtion]] in order
to establish the stable and efficient choice. For example this value should
decrease at high pressure.

Except for the isothermal/isenthalpic ([[ionmov]] 13) ensemble the input
variable [[optcell]] must be 0 (default value).
You have also to define the maximal number of timesteps of the molecular dynamics.

Usually you can set the input variable [[ntime]] to a large value, 5000, since
there is no *end* to a molecular dynamics simulation. You can always stop or
restart the calculation at your convenience by using the input variable [[restartxf]]=-1.

The input file `tmoldyn_01.abi` is an example of a file that contains data for a
molecular dynamics simulation using the isokinetic ensemble [[ionmov]]=12 for fcc aluminum.

{% dialog tests/tutoparal/Input/tmoldyn_01.abi %}

Open the `tmoldyn_01.abi` file and look at it carefully. The unit cell is defined at
the end. It is a 2x2x2 fcc supercell containing 32 atoms of Al. [[ionmov]] is
set to 12 for the isokinetic ensemble, and since [[ntime]] is set to 50,
ABINIT will carry on 50 time steps of molecular dynamics. The calculation will
be performed for a temperature of 3000 K, see the key variable [[mdtemp]]. It
gives the initial and final temperatures of the simulation (in Kelvin). The
temperature will change linearly from the initial temperature `mdtemp(1)` at
`itime=1` to the final temperature `mdtemp(2)` at the end of the [[ntime]]
timesteps. Here the temperature will stay constant during the whole simulation.

!!! note

    Note that we use the same temperature for the ions and the electrons:
    [[occopt]] has been set to 3 for a Fermi-Dirac smearing and [[tsmear]] has
    been set to 3000 Kelvin. Nothing prevents you to use different electronic and
    ionic temperature, you just have to know why you are doing so!
    With such a temperature, the number of electronic bands [[nband]] is set to 80.

!!! note
    For AIMD, [[nsym]] is set to 1 to remove the use of symmetries !

Molecular dynamics simulations are always large calculations, dealing with
supercells of hundreds to thousands of atoms. Therefore they are always
performed in parallel. In `tmoldyn_01.in`, [[paral_kgb]] has been set to 1 to
activate the parallelisation over **k**-points, G-vectors and bands. 
The three following keywords give the number of processors for each level of
parallelisation. Since we have only 1 **k**-point in the simulation ([[ngkpt]]
has been set to 1 1 1) [[npkpt]] is set to 1,  [[npband]] to 2 with [[bandpp]]=40 (1 unique block),
and [[npfft]] is kept to 1.

Then run the calculation in parallel over 2 CPU cores. You can change the
distribution of processors over the level of parallelisation to try to find
the most efficient one. 
Since molecular dynamics can last for weeks, it is crucial to find the appropriate
distribution to reduce the computational time at the maximum. 
You can use [[timopt]] to get information on time repartition during the simulation.

Now look at the output file. For each iteration you will see the coordinates, the forces, the
velocities and the kinetic and the total energy.

In addition, ABINIT should have generated a `_HIST.nc` file, which contains the
whole history of the molecular dynamics simulation: atomic positions,
velocities, primitive translations, stress tensor, energies... at each time
step. This file will be used to restart the calculation if you want to perform
more time steps or to extract the necessary informations to make use of the
molecular dynamics simulation. 
In `tmoldyn_01.abi` add the keyword [[restartxf]] and set it to -1. 
Run the calculation again, in the same directory. Look at
the new output file. The number of each time step are indicated over the total
number of steps:

    Iteration: (  1/100) Internal Cycle: (1/1)

Since we already performed 50 steps of molecular dynamics, the total number of
time steps are now 100. So the first 50 iterations are from the previous
calculation. You can check that by comparing `tmoldyn_01.abo` and
`tmoldyn_01.abo0001`. There is only one `_HIST.nc` file and it contains the history of
the two calculations.

Now we can calculate and plot several quantities.

We need for that the `diag_moldyn.py` python script.
You can find it in the `$ABI_HOME/doc/tutorial/paral_moldyn_assets`
directory (link [here](paral_moldyn_assets/diag_moldyn.py)).
Run the script as:

    python diag_moldyn.py

!!! warning
    The above script `diag_moldyn.py` uses `python2` and **does not work** with `python3`

You can read (on standard output) the average value and the standard deviation
of the total energy, the temperature and the pressure. You have also generated
several files which contain pressures, energies, stresses, positions and
temperatures. You can plot this files to observe the behavior of the
quantities during the molecular dynamics. Note that 100 time steps is far from
being sufficient to equilibrate physical quantities as pressure. 2000 or 3000
are more common numbers to reach this goal but it would exceed the time
allocated for this tutorial.

!!! tip
    You can use `agate` or `qAgate` to open and visualize the `_HIST.nc` file. In the console, the averages 
    and the standard deviations of several quantities like 
    temperature pressure stresses are automatically displayed.

    You can plot the evolution of pressure with respect to the time step by issuing `:plot P`.
    Replace `:plot` by `:print` or `:data` to save the image or write an ASCII file.

## The convergence on **k**-points and supercell size

In the previous section you have learned to perform molecular dynamics with
abinit. We used a fcc supercell of 32 atoms with only 1 **k**-point. In this
section we will make convergence studies with respect to these parameters.

### Computing the convergence in **k**-points

The files `tmoldyn_02.abi` and `tmoldyn_03.abi` are input files for 2x2x2 and 3x3x3
**k**-points grid respectively (4 and 14 **k**-points in the irreducible Brillouin zone).

{% dialog tests/tutoparal/Input/tmoldyn_02.abi tests/tutoparal/Input/tmoldyn_03.abi %}

Since the parallelisation is the most efficient over the **k**-point level
you should always put [[nkpt]] to the largest possible value before increasing
[[npfft]] and [[npband]]. We have followed this rule in the input files.

If you used the `diag_moldyn.py` script, change the name of the previous file 
PRESS to PRESS01 to save it. 
Run now ABINIT in parallel over 8 CPU cores with `tmoldyn_02.abi` and over 28 CPU
cores with `tmoldyn_03.abi`. 
At the end of each calculation use the `diag_moldyn.py` script and save the results in PRESS02 and PRESS03. 
You can now plot the pressures in term of the **k**-points grids and compare the average values:

!!! tip
    Alternatively, use `agate` or `qagate` to directly produce the following picture
    ```
    :open tmoldyn_01o_HIST.nc
    :plot P hold=true
    :open tmoldyn_02o_HIST.nc
    :plot P 
    :open tmoldyn_03o_HIST.nc
    :plot P  output Kpoints
    ```

![Kpoints](paral_moldyn_assets/Kpoints.png)

As said previously our simulations are too short to be completely convincing
but you can see that you need at least a 2x2x2 **k**-points grid for a 32 atoms
cell. If you have some time, increase ntime to 300 and run again ABINIT.

### Computing the convergence in cell size

We also have to check if our cell is sufficiently large to give reliable
physical quantities. In the previous section we used a 2x2x2 fcc supercell.
`tmoldyn_04.abi` is an input file for a 3x3x3 fcc supercell and therefore
contains 108 atoms. [[nband]] and [[acell]] has been scaled accordingly to
take into account the new size of the cell.

Run now ABINIT in parallel over 8 CPU cores and then `diag_moldyn.py`
(note that the output file is very big, and no reference has been provided for comparison).
Save the pressure to PRESS04.
`tmoldyn_05.abi` has the same cell but a 2x2x2 **k**-points grid (note that the
output file is very big, and no reference has been provided for comparison).
Run it over the adequate number of cores and save the pressure to PRESS05.
Plot now PRESS04 and PRESS05 and compare the average values. 
You will see that for this size of cell, one **k**-point is sufficient.

!!! tip
    Alternatively, use `agate` or `qagate` to directly produce the following picture
    ```
    :open tmoldyn_04o_HIST.nc
    :plot P  hold=true
    :open tmoldyn_05o_HIST.nc
    :plot P
    ```

![108atoms](paral_moldyn_assets/108atoms.png)

We are now going to increase again the cell size. With a 4x4x4 fcc cell, the
file `tmoldyn_06.in` has 256 atoms. Of course, [[nband]] and [[acell]] have been
scaled. This calculation should last for 50 min over 32 CPU cores (note that
the output file is very big, and no reference has been provided for
comparison). Run it and save the pressure to PRESS06. Plot now PRESS02,
PRESS04 and PRESS06, remove the first steps and compare the pressure average values:

!!! tip
    Alternatively, use `agate` or `qagate` to directly produce the following picture
    ```
    :open tmoldyn_02o_HIST.nc
    :plot P  hold=true
    :open tmoldyn_05o_HIST.nc
    :plot P
    :open tmoldyn_06o_HIST.nc
    :plot P  
    ```

![cells](paral_moldyn_assets/cell.png)

You can see that even if the pressure was converged in term of **k**-points, the 32
atoms supercell was not sufficient to give a reliable pressure. A 3x3x3
supercell with only 1 **k**-point seems to be the adequate size. You see also
that with a bigger cell, the pressure fluctuations are considerably reduced.

Note that here we made the convergence studies using the pressure as a
criteria. The results can depend on the physical quantity you are looking at,
pressure, temperature, energy, or dynamical matrices by observing the
displacement fluctuations.... Always check if your cell is large enough and
give the corresponding uncertainty.
Also, to reduce the time necessary to do this tutorial we set the value of
[[ecut]] to 3 Ha. This is too small, for Al, it should be closer to 8 Ha.

Finally, here and for simplicity, we just changed the value of [[ngkpt]] to
change the grid of **k**-points. By default, ABINIT uses a [[shiftk]] so the 
one **k**-point generated with a 1x1x1 gid is *not* $\Gamma$!.
To get the $\Gamma$ point only, and decrease the execution time by half, set
[[shiftk]] to `0 0 0` and check the value [[kpt]] in the output file.

## Compute the melting temperature of Al at a given pressure

As an example of what can be done in molecular dynamics, we are going to
calculate the melting temperature of aluminum using the so-called Heat Until
it Melts (HUM) method. In this method the solid phase is heated gradually
until melting occurs. Let's start with a temperature of 4500 K.
To work fast, we use a 32 atoms supercell and 1 **k**-point 
(note that the output file is very big,
and no reference has been provided for comparison).

Run ABINIT in parallel over 2 CPU cores and then `diag_moldyn.py`.
Save the pressure to PRESS71. Plot the atomic positions, you see that at this
temperature, the cell is solid. 

!!! tip
    Alternatively, use `agate` or `qagate` to directly produce the following pictures.
    The positions can be obtains directly with `:plot positions yz` where yz can be replaced by
    xy or xz to change the plan. Note that the atoms on the border are not displayed at contrario
    to the animations.

![xcart](paral_moldyn_assets/xcart1.png)

  <video id="video_md1" controls autoplay loop style="width: 100%;">
  <source src="../paral_moldyn_assets/4500.mp4" type="video/mp4">
  You browser does not support the video tag. Download the file [here](paral_moldyn_assets/4500.mp4).
  </video>

Increase the temperature to 5000 K (do not
forget to also change the electronic temperature with tsmear) and run ABINIT.
Save the pressure and again, look at the positions. The cell is still solid.
Set the temperature to 5500 K. Look at the positions: the ions are moving
across the cell and do not come back to their equilibrium positions. The cell
has melted and is now liquid. Plot the pressures for the three simulations:

![xcart](paral_moldyn_assets/xcart2.png)
 <video id="video_md2" controls autoplay loop style="width: 100%;">
 <source src="../paral_moldyn_assets/5500.mp4" type="video/mp4">
 You browser does not support the video tag. Download the file [here](paral_moldyn_assets/5500.mp4).
 </video>

!!! tip
    To run the three temperatures in 1 run, use [[ndtset]] 3 and specify for each `dtset` the values for [[mdtemp]]
    and [[tsmear]].
    An example of file is given with `tmoldyn_07.abi`. 

{% dialog tests/tutoparal/Input/tmoldyn_07.abi %}

!!! tip
    
    To produce the following plot, use `agate` or `qagate` with the following commands
    ```
    :open tmoldyn_07o_DS1_HIST.nc
    :plot P hold=true
    :open tmoldyn_07o_DS2_HIST.nc
    :plot P 
    :open tmoldyn_07o_DS3_HIST.nc
    :plot P 
    ```

![melting](paral_moldyn_assets/melting.png)

You can clearly observe a discontinuous change in pressure due to the volume
difference between the solid and liquid phases. This give a melting
temperature of 5250 K at 132 GPa. 
One has to be very careful. Indeed, in addition to
the crude parameters we used ([[ecut]], [[natom]]...), the HUM method has some
intrinsic drawbacks. In HUM the crystal is heated homogeneously, the melting
initiates in the bulk and this results in an overheating effect.
---
authors: GZ, MT, EB, MJV
---

# Tutorial about the spin

## Properties related to spin (spin polarized calculations, ferro- ferri- magnetic materials, and spin-orbit coupling).

This tutorial aims at showing how to get the following physical properties:

* the total magnetization of a ferromagnetic (or ferrimagnetic) material
* the estimation of the atom magnetic moment
* analyse the total density of states per spin direction
* analyse the density of states per atom and per spin direction
* effect of spin-orbit coupling for a non magnetic system
* non-collinear magnetism (not yet)
* spin-orbit coupling and magnetocristalline anisotropy (not yet)

You will learn to use features of ABINIT which deal with spin.

This tutorial should take about 1.5 hour.

[TUTORIAL_README]

## 1 A ferromagnetic material: *bcc* Fe

*Before beginning, you might consider to work in a different subdirectory, as
for the other tutorials. Why not Work_spin?*

The file *tspin_x.abi* in *\$ABI_TESTS/tutorial/Input* lists the input files for the spin tutorials.
You can copy the first one in the *Work_spin* directory with:

```sh
cd $ABI_TESTS/tutorial/Input
mkdir Work_spin
cd Work_spin
cp ../tspin_1.abi .
```

{% dialog tests/tutorial/Input/tspin_1.abi %}

You can now run the calculation with:

```sh
abinit tspin_1.abi > log > err &
```

In the mean time the calculation is done, have a look at the input file, and read it carefully.
Because we are going to perform magnetic calculations, there are two new types of variables related to magnetism:

* [[nsppol]]
* [[spinat]]

You will work with a low [[ecut]] (=18Ha) and a small k-point grid (defined by [[ngkpt]]) of 4x4x4
Monkhorst-Pack grid. It is implicit that in *real life*, you should do a
convergence test with respect to both [[ecut]] and [[ngkpt]] parameters and this is even more
important with magnetism that involves low energy differences and small magnetization density values.

This basic first example will compare two cases, 
one that do not take into account magnetism and one that take into account it 
(done through two datasets in the input, the dataset 1 does not include magnetism and dataset 2 includes it).
We now look at the output file. 
In the magnetic case (dataset 2), the electronic density is split into two parts,
the "Spin-up" and the "Spin-down" parts to which correspond different Kohn-Sham
potentials and different sets of eigenvalues whose occupations are given by the
Fermi-Dirac function (without the ubiquitous factor 2).

For the first k-point, for instance, we get:

```
(no magnetism)
   2.00000    2.00000    2.00000    2.00000    2.00000    1.99999    1.99999    1.28101   1.28101    0.35284

(magnetic case)
(SPIN UP)
   1.00000    1.00000    1.00000    1.00000    1.00000    1.00000    1.00000    0.97826   0.97826    0.68642
(SPIN DOWN)
   1.00000    1.00000    1.00000    1.00000    1.00000    0.99995    0.99995    0.05851   0.05851    0.01699
```

We note that the occupations of the magnetic case are different for up and down spin channels, which
means that the eigenvalues are shifted 
(which is in turn due to a shift of the exchange-correlation potential and therefore of the effective potential).
You can have a look at the output file to compare spin-up and down eigenvalues:

```
(up channel:)
  -3.09143   -1.74675   -1.74675   -1.74418    0.25777    0.36032    0.36032    0.46350   0.46350    0.49374
(dn channel:)
  -3.04218   -1.69171   -1.69171   -1.68906    0.27331    0.40348    0.40348    0.52935   0.52935    0.54215
```

Now you can move down in the output file and look at the following section:
````
 Integrated electronic and magnetization densities in atomic spheres:
 ---------------------------------------------------------------------
 Radius=ratsph(iatom), smearing ratsm=  0.0000. Diff(up-dn)=approximate z local magnetic moment.
 Atom    Radius    up_density   dn_density  Total(up+dn)  Diff(up-dn)
    1   2.00000     7.658786     6.158329     13.817115     1.500458
 ---------------------------------------------------------------------
  Sum:              7.658786     6.158329     13.817115     1.500458
 Total magnetization (from the atomic spheres):             1.500458
 Total magnetization (exact up - dn):                       1.571040
======================================================================
````

In the last line, it is reported the total magnetization of the whole unit cell 
(in unit of $\mu_B$ - Bohr's magneton), which correponds to the difference between the
integrated up and down densities, here we have 1.571040 $\mu_B$.
It is also reported the same difference but as integrated into spheres around each atom 
(here we have only one atom), which gives here 1.500458 $\mu_B$ on iron atom.
The integrated magnetization in atomic spheres is an approximation, one should always check
that the sum of the integrated atomic magnetization "from the atomic sphere" is very close to the 
exact up-dn unit cell magnetization.
In the case of dataset 1 without magnetism, only the integrated electronic density is reported since there is
no distinction between up and down spin channels.

You can look at the total energy of dataset 1 and 2:

```
           etotal1    -1.2342819713E+02
           etotal2    -1.2343141307E+02
```

The energy of the magnetic calculation is lower than the non-magnetic one,
which is expected for a magnetic crystal (the system gains energy through the extra degrees of freedom given by the spins).
Finally, you can also remark that the stress tensor is affected by the presence of magnetism.
This would also be true for the forces, but we don't remark it here because we are in high symmetry symmetry structure (all the forces are zeroed by symmetry), however this would be apparent for a less symmetric material.


It is interesting to consider in more detail the distribution of eigenvalues
for each spin channel, which is best done by looking at the respective densities of state (DOS).
To this end we have set [[prtdos]] = 1 in the input file that will print the up and down DOS (as soon as [[nsppol]] = 2).
The DOS data can be found in the files *tspin_1o_DS1_DOS* and *tspin_1o_DS2_DOS*
for the non-magnetic and the magnetic cases respectively, which can be used with a plotting software.
Traditionally, in order to enhance visibility, the DOS of minority spin electrons is done using negative values.
If we compare the DOS of the magnetized system (the Fermi energy is highlighted by a vertical dashed line):

![](spin_assets/bccfe_mag_dos2.jpg)

and the non-magnetized system:

![](spin_assets/bccfe_nonmag_dos2.jpg)

We observe that the up and down DOS channels have been "shifted" with respect to each other, which is a footprint of the presence of non-zero total magnetization in the crystal (either ferro- or ferri-magnetic).
The integrated DOS yields the number of electrons for each spin direction, 
and we see that the magnetization arises from the fact that there are more up than down electrons at the Fermi level.

The magnetization is from the up channels because we initialized [[spinat]] with positive values.
We could initialize it to a negative value to have negative magnetization.
[[spinat]] serves two purposes: it is a way to initially break the spin symmetry (up/down), 
and also to start with an initial magnetic moment, 
ideally close enough to the final DFT one but sometimes a final non-zero magnetic moment on the atoms is obtained by initializing [[spinat]] to larger values than the formal one 
(in spin DFT there can be several local minima for the total energy and depending on your starting point you can end up in different local minima).

Note that [[spinat]] has three components (i.e. for x, y  and z directions) for each atom 
but in the absence of spin-orbit coupling (collinear calculation) 
there is no relation between the direction of magnetization and the crystal axes.
In these collinear magnetism calculations, only the z component of the spins is read to define the amplitude of the atomic magnetic moment 
(treated as a scalar value, i.e. only sign and amplitude matter).

The self-consistent loop is affecting both the density (like in the non-magnetic case) as
well as the spin-magnetization. 
For this reason, it might be more difficult to reach convergence in the magnetic cases than in 
the non-magnetic cases.
Not only starting with a large enough magnetic moment might help to converge toward the correct
magnetic ground sate, 
but also modified (tighter) convergence parameters might be needed. 
For example, in the case of Cobalt, in order to obtain the correct (non-zero) magnetic moment, 
a rather dense k-point sampling in the Brillouin zone must be used (e.g. 16x16x16), with a 
rather small value of [[tsmear]]. 
The convergence of a magnetic calculation a smaller value of [[tolrde]] (e.g. 0.001 instead of the default 0.005),
a larger value of [[nline]] (e.g. 6 to 12 instead of the default 4) 
and/or reducing the mixing parameters diemix and mostly diemixmag.


## 2 An antiferromagnetic example: *fcc* Fe

Well sort of....

Actually, fcc Fe, displays many complicated structures, in particular spin spirals.
A spiral is characterized by a direction along an axis, an angle of
the magnetization with respect to this axis and a step after which the magnetization comes full circled.
A very simple particular case is when the angle is 90°, the axis is <100> and
the step is the unit cell side: spin directions alternate between
planes perpendicular to the <100> axis yielding a "spiral stairway":

![](spin_assets/fcc_fe_conv.jpg)

For instance, if the atom at [x,y,0] possesses an "up" magnetization, the atom
at [x+1/2,y,1/2] would possess a down magnetization etc...
To describe such a structure, a unit cell with two atoms is sufficient, [0,0,0] and
[1/2,0,1/2].
The two atoms will be given opposite magnetization with the help of the variable [[spinat]].

Copy the file *$ABI_TESTS/tutorial/Input/tspin_2.abi* in *Work_spin*.

{% dialog tests/tutorial/Input/tspin_2.abi %}

This is your input file.

You can run the calculation, then you should edit the *tspin_2.in* file, and briefly
look at the two changes with respect to the file *tspin_1.abi*: the
unit cell basis vectors [[rprim]], and the new [[spinat]].

Note also that we use now [[nsppol]] = 1 and [[nspden]] = 2: this combination of values
is only valid when performing a strictly antiferromagnetic (AFM) calculation: 
nspden = 2 means that we have 2 independent components for the charge density 
while nsppol = 1 means that we have 1 independent component for the wave-functions.
In that case, ABINIT uses the so-called Shubnikov symmetries, to perform
calculations twice faster than with [[nsppol]] = 2 and [[nspden]] = 2 
(spin up and down channels are equivalent by symmetry in the case of perfect AFM cases). 
The symmetry of the crystal is not the full fcc symmetry anymore, since the
symmetry must now preserve the magnetization of each atom.  
ABINIT is nevertheless able to detect such symmetry belonging to the Shubnikov groups
and correctly finds that the cell is primitive, which would not be the case
if we had the same vector [[spinat]] on each atom (FM case).

If we now run this AFM calculation, its computation time is
approximately 10-20 seconds on a recent CPU.
If we look at the eigenvalues and occupations, they are again filled with a
factor 2, which comes from the symmetry considerations aforementioned, and
not from the "usual" spin degeneracy: the potential for spin-up is equal to
the potential for spin-down, shifted by the antiferromagnetic translation
vector. Eigenenergies are identical for spin-up and spin-down, but
wavefunctions are shifted one with respect to the other.

```
 kpt#   1, nband= 20, wtk=  0.25000, kpt=  0.1250  0.1250  0.2500 (reduced coord)
  -2.75606   -2.71227   -1.51270   -1.50656   -1.50211   -1.46164   -1.45614   -1.45485
   0.25613    0.34136    0.38202    0.41368    0.41915    0.46180    0.48400    0.50628
   0.52100    0.54004    0.56337    0.64500
      occupation numbers for kpt#   1
   2.00000    2.00000    2.00000    2.00000    2.00000    2.00000    2.00000    2.00000
   2.00000    2.00000    2.00000    2.00000    2.00000    2.00000    2.00000    2.00000
   1.95186    0.00000    0.00000    0.00000
```

The total magnetization being zero we can question how do we know we have magnetic order?
Indeed, the DOS will not be useful since we have as many up and down electrons.

We can however look at the integrated magnetization around each atom to have an
indication of the magnetic moment carried by each atom:

```
 Integrated electronic and magnetization densities in atomic spheres:
 ---------------------------------------------------------------------
 Radius=ratsph(iatom), smearing ratsm=  0.0000. Diff(up-dn)=approximate z local magnetic moment.
 Atom    Radius    up_density   dn_density  Total(up+dn)  Diff(up-dn)
    1   2.00000     7.748236     6.575029     14.323265     1.173206
    2   2.00000     6.575029     7.748236     14.323265    -1.173206
 ---------------------------------------------------------------------
  Sum:             14.323265    14.323265     28.646531    -0.000000
 Total magnetization (from the atomic spheres):            -0.000000
 Total magnetization (exact up - dn):                      -0.000000
================================================================================
```

which gives (dependent on the radius used to project the charge density):

    magnetization of atom 1= 1.173206
    magnetization of atom 2=-1.173206

Another way to get integrated atom magnetization can be done by using the utility 
*cut3d* which yields an interpolation of the magnetization at any point in space. 
*cut3d* is one of the executables of the ABINIT package and is installed together with abinit.
For the moment cut3d is interactive, and we will use it through a very primitive script
(written in Python) to perform a rough estimate of the magnetization on each atom.
You can have a look at the [magnetization.py program](spin_assets/magnetization.py), and note
(or believe) that it does perform an integration of the magnetization in a cube of
side acell/2 around each atom; if applicable, you might consider adjusting the
value of the "CUT3D" string in the Python script.

Copy it in your *Work_spin* directory. If you run the program, by typing

```
python magnetization.py
```

you will see the result:

```
For atom 0 magnetic moment 1.2336144871839712
For atom 1 magnetic moment -1.2336216848949257
```

which also show that the magnetizations of the two atoms are opposite.
The values of the atom magnetization is a bit different than the ones from the
ABINIT output because it uses a slightly different integration paramters.

## 3 Another look at *fcc* Fe

Instead of treating fcc Fe directly as an AFM material, we will
not make any hypotheses on its magnetic structure, and run the calculation
like the one for bcc Fe, anticipating only that the two spin directions are going to be different.
We will not even assume that the initial spins are of the same magnitude.

You can copy the file *$ABI_TESTS/tutorial/Input/tspin_3.abi* to *Work_spin*.

{% dialog tests/tutorial/Input/tspin_3.abi %}

You can run the calculation with this is input file and look at it to understand its contents.

Note the values of [[spinat]]. In this job, we will again characterize the magnetic structure.
We are going to use atom and angular momentum projected densities of state.
These are DOS weighted by the projection of the wave functions
on angular momentum channels (i.e. the spherical harmonics) centered on each atom of the system.
Note that these DOS are computed with the tetrahedron method, which is rather
time consuming and produces more accurate but less smooth DOS than the smearing method. 
The CPU time is strongly dependent on the number of k-points, and we use here only a reduced set.
(This will take about 40 seconds on a modern computer)

To specify this calculation we need new variables, in addition to [[prtdos]] set now to 3:

* [[natsph]]
* [[iatsph]]
* [[ratsph]]

This will specify the atoms around which the calculation will be performed, and the radius of the sphere.
We specifically select a new dataset for each atom, a non self-consistent
calculation being run to generate the projected density of states.
First, we note that the value of the total energy is -2.4972993185E+02 Ha,
which shows that we have attained essentially the same state as in tspin2 (etotal=-2.4972993198E+02).

The density of states will be in the files *tspin_3o_DS2_DOS_AT0001* for the
first atom, and *tspin_3o_DS3_DOS_AT0002* for the second atom.
We can extract the density of d states, which carries most of the magnetic
moment and whose integral up to the Fermi level will yield an estimate of the
magnetization on each atom.
We note the Fermi level (echoed in the file *tspin_3o_DS1_DOS*):

    Fermi energy :      0.52472131

If we have a look at the integrated site-projected density of states, we can
compute the total moment on each atom. To this end, one can open the file
*tspin_3o_DS3_DOS_AT0002*, which contains information pertaining to atom 2. This
file is self-documented, and describes the line content, for spin up and spin down:

```
# energy(Ha)  l=0   l=1   l=2   l=3   l=4    (integral=>)  l=0   l=1   l=2 l=3   l=4
```

If we look for the lines containing  an energy of "0.52450", we find

up :
  0.52450  0.3714  1.0151  13.6555  0.1144  0.0683   1.29  3.30  <font color="red">2.49</font>  0.05  0.02

down :
  0.52450  0.1597  1.6758   9.8137  0.0872  0.0750   1.29  3.32  <font color="red">3.69</font>  0.04  0.01

There are apparently changes in the DOS for all projected orbitals,
but only the d orbital projection has sizeable differences. 
This is confirmed by looking at the integrated DOS which is different only for the
d-channel. 
The difference between up and down is 1.20, in rough agreement
(regarding our very crude methods of integration) with the previous
calculation. 
Using a calculation with the same number of k-points for the
projected DOS, we can plot the up-down integrated dos difference for the d-channel of each atom:

![](spin_assets/energy_diff_fccfe.jpg)

Note that there is some scatter in this graph, due to the finite number of digits
of the integrated DOS given in the file *tspin_3o_DS3_DOS_AT0001* and *tspin_3o_DS3_DOS_AT0002*.
If we now look at the up and down DOS for each atom, we can see that the
corner atom and the face atom possess opposite magnetizations, which roughly
cancel each other. 
The DOS computed with the tetrahedron method is not as smooth as by the smearing method, 
and a running average allows for a better view.

## 4 Ferrimagnetic (not yet)

Some materials can display a particular form of ferromagnetism, which also can
be viewed as non compensated antiferromagnetism, called ferrimagnetism.
Some atoms possess up spin and other possess down spin, but the total spin magnetization is non zero.
This happens generally for system with different type of atoms with different magnetic moment, and sometimes
in rather complicated structures such as magnetite.

## 5 The spin-orbit coupling (SOC)

For heavy atoms a relativistic description of the electronic structure becomes
necessary, and this can be accomplished through the relativistic DFT approach.

### 5.1 Norm-conserving pseudo-potentials

For atoms, the Dirac equation is solved and the 2(2l+1) l-channel
degeneracy is lifted according to the eigenvalues of the $L+S$ operator
(l+1/2 and l-1/2 of degeneracy 2l+2 and 2l).
After pseudization, the associated wave functions can be recovered by adding to usual pseudo-potential projectors a
spin-orbit term of the generic form $v(r).|l,s\rangle L.S \langle l,s|$.
Not all potentials include this additional term,  if you use the pseudopotentials from pseudodojo, you have
to choose those generated in a fully relativistic option (with "FR" in the name, "SR" means semi-relativistic).

In a plane wave calculation, the wavefunctions will be two-component
spinors, that is they will have a spin-up and a spin-down component, and these
components will be coupled. 
This means the size of the Hamiltonian matrix is quadrupled and the CPU time will be enlarged.

We will consider here a heavier atom than Iron: *Tantalum*.

You can copy the file *$ABI_TESTS/tutorial/Input/tspin_5.abi* in *Work_spin* and run the calculation.

{% dialog tests/tutorial/Input/tspin_5.abi %}

The input file contains one new variable:

  * [[nspinor]]

Have a look at it. You should also look at [[so_psp]]; it is not set explicitly here,
because the SO information is directly read from the pseudopotential file.
One could force a non-SO calculation by setting [[so_psp]] to 0 even if the pseudo is "FR".

In this run, we check that we recover the splitting of the atomic levels by
performing a calculation in a big box. 
Two calculations are launched with and without SOC.

We can easily follow the symmetry of the different levels of the non-SOC calculation:

```
 kpt#   1, nband= 26, wtk=  1.00000, kpt=  0.0000  0.0000  0.0000 (reduced coord)
  -2.60792   -1.43380   -1.43380   -1.43380   -0.14851   -0.09142   -0.09142   -0.09142
  -0.09033   -0.09033    0.02657    0.02657    0.02657    0.05604    0.14930    0.14930
   0.14930    0.15321    0.15321    0.15321    0.16211    0.16211    0.27952    0.27952
   0.27952    0.31238
```

That is, the symmetry: s, p, s, d
After application of the SOC, we now have to consider twice as many levels:

```
 kpt#   1, nband= 26, wtk=  1.00000, kpt=  0.0000  0.0000  0.0000 (reduced coord)
  -2.59704   -2.59704   -1.65268   -1.65268   -1.32544   -1.32544   -1.32544   -1.32544
  -0.14663   -0.14663   -0.09760   -0.09760   -0.09760   -0.09760   -0.07881   -0.07881
  -0.07805   -0.07805   -0.07805   -0.07805    0.01044    0.01044    0.03597    0.03597
   0.03597    0.03597
```

The levels are not perfectly degenerate, due to the finite size of the simulation box,
and in particular the cubic shape, which gives a small crystal field splitting of the d orbitals
between $e_g$ and $t_{2g}$ states.
We can nevetheless compute the splitting of the levels, and we obtain, for e.g. the p-channel: 1.65268-1.32544=0.32724 Ha

If we now consider the
[NIST table](https://www.nist.gov/pml/atomic-reference-data-electronic-structure-calculations-tantalum)
of atomic data, we obtain:

    5p splitting, table: 1.681344-1.359740 = 0.321604 Ha
    5d splitting, table: 0.153395-0.131684 = 0.021711 Ha

We obtain a reasonable agreement. A better agreement could be obtained by improving the convergence parameters.

### 5.2 Projector Augmented-Wave

Within the Projector Augmented-Wave (PAW) method, the usual (pseudo-)Hamiltonian can be expressed as:

$$
H  =  K + V_{eff} + \Sigma_{ij} D_{ij}  |p_i \rangle \langle p_j|
$$

If the two following conditions are satisfied:

(1) the local PAW basis is complete enough;
(2) the electronic density is mainly contained in the PAW augmentation regions,

it can be shown that a very good approximation of the PAW Hamiltonian --
including spin-orbit coupling -- is:

$$
H  \simeq  K + V_{eff} + \Sigma (D_{ij}+D^{SO}_{ij})  |p_i \rangle \langle p_j|
$$

where $D^{SO}_{ij}$ is the projection of the ($L.S$) operator into the PAW augmentation regions.
As an immediate consequence, we have the possibility to use the standard $p_i$ PAW projectors;
in other words, it is possible to use the standard PAW datasets (pseudopotentials) to perform
calculations including SOC.
But, of course, it is still necessary to express the wave-functions as two
components spinors (spin-up and a spin-down components).
Let's have a look at the following keyword:

  * [[pawspnorb]]

This activates the spin-orbit coupling within PAW (forcing [[nspinor]]=2).

Now the practice:
We consider Bismuth, the PAW dataset file contains the 5d, 6s and 6p electrons in the valence.

You can copy the file *$ABI_TESTS/tutorial/Input/tspin_6.abi* in *Work_spin*
(one Bismuth atom in a large cell) and run the calculation. 
It takes about 10 seconds on a recent computer.

{% dialog tests/tutorial/Input/tspin_6.abi %}

Two datasets are executed: the first without SOC, the second one using  [[pawspnorb]]=1.

The resulting eigenvalues are:

Without SOC:
```
 Eigenvalues (hartree) for nkpt=   1  k points:
 kpt#   1, nband= 12, wtk=  1.00000, kpt=  0.0000  0.0000  0.0000 (reduced coord)
5d  -0.86985   -0.86985   -0.86879   -0.86879   -0.86879   
6s  -0.43169   
6p  -0.05487   -0.05486   -0.05486   
...
```

With SOC:
```
 kpt#   1, nband= 24, wtk=  1.00000, kpt=  0.0000  0.0000  0.0000 (reduced coord)
5d  -0.91133   -0.91133   -0.91133   -0.91133   -0.80022   -0.80022   -0.80022   -0.80022   -0.79954   -0.79954   
6s  -0.41639   -0.41639   
6p  -0.09817   -0.09817   -0.02546   -0.02546   -0.02546   -0.02546        
...
```

Again, the levels are not perfectly degenerate, due to the finite size and non spherical
shape of the simulation box.
We can compute the splitting of the levels, and we obtain:

    5d-channel: 0.91133-0.80022 = 0.11111 Ha
    6p-channel: 0.09817-0.02546 = 0.07271 Ha

If we now consider the
[NIST table](https://www.nist.gov/pml/atomic-reference-data-electronic-structure-calculations-bismuth)
of atomic data, we obtain:

    5d-channel: 1.063136-0.952668 = 0.11047 Ha
    6p-channel: 0.228107-0.156444 = 0.07166 Ha

A perfect agreement even with a small simulation cell and very small values of plane-wave cut-offs.
This comes from the generation of the PAW dataset, where the SOC is calculated very accurately
and for an atomic reference. 
The exchange correlation functional has little impact on large SOC
splittings, which are mainly a kinetic energy effect.

## 6 Rotation of the magnetization and spin-orbit coupling (coming soon)

The most spectacular manifestation of the SOC is the energy
associated with a rotation of the magnetisation with respect to the crystal axis.
It is at the origin of the magneto crystalline anisotropy (MCA) of paramount technological importance.

## 7 Summary

The table below can help to know how to handle the different magnetic calculation cases in ABINIT:

|  case         | msppol | nspinor | nspden |
| ------------- | ------ | ------- | ------ |
| non-magnetic  | 1      | 1       | 1      |
| collinear FM  | 2      | 1       | 2      |
| collinear AFM | 1      | 1       | 2      |
| non-collinear | 1      | 2       | 4      |

Note that non-collinear magnetism can be done with or without SOC.
With SOC do not use time reversal symmetry ([[kptopt]] = 4), however as the symmetries
in the presence of SOC it is not (yet fully) implemented please remove all symmetries when running
non-collinear magnetism with SOC (i.e. [[kptopt]]=3 and [[nsym]]=1).
The non-collinear magnetism + SOC cases can be much more difficult to converge (SCF) and might require 
to strongly reduce the [[diemix]] and mostly the [[diemixmag]] mixing flags; you might have to increase [[nline]] and play with negative values of [[nnsclo]].
DFT+U correction is also necessary in most of the magnetic calculations if you use regular LDA and GGA functionals (i.e. not hybrid fucntionals).


* * *
GZ would like to thank B. Siberchicot for useful comments.
---
authors: MG, GB
---

# An overview of the EPH code

This page provides a quick introduction to the new electron-phonon driver integrated with the ABINIT executable.
We discuss important technical details related to the implementation and the associated input variables.
The drawbacks/advantages with respect to the implementation available in ANADDB are also discussed.

## Why a new EPH code?

First of all, let's try to answer the question:
*why did you decide to implement a new code for electron-phonon calculations?*

As you may know, e-ph calculations have been available through the ANADDB executable for a long time.
This ANADDB-based implementation is essentially a post-processing of the e-ph matrix elements
computed by the DFPT code. This approach presents advantages as well as drawbacks.
On the one hand, most of the work required to compute e-ph matrix elements is implemented directly by the DFPT routines.
This means that e-ph calculations with advanced features such as PAW, SOC, non-collinear magnetism *etc*
are readily available in the ANADDB version once support in the DFPT part is implemented.

On the other hand, this post-processing approach implies that the number of $\kk/\qq$-points in the e-ph matrix elements
is **automatically fixed at the level of the DFPT run**.
In other words, if you want to compute phonon-limited mobilities with e.g. a 90×90×90 $\kk$- and $\qq$-mesh,
you need to perform DFPT calculations with the same sampling thus rendering the computation almost impossible.
In principle, it is possible to use tricks such as a linear interpolation of the e-ph matrix elements
to densify the sampling, but in order to get a decent interpolation one usually needs initial BZ meshes
that are significantly denser than the ones needed to converge the DFPT part alone.

As a matter of fact, electrons, phonons and e-ph properties **present completely different convergence rates**.
In silicon, for instance, a 9×9×9 mesh both for phonons and electrons is enough to converge
the electron density and the vibrational spectrum [[cite:Petretto2018]].
On the contrary, the phonon-limited mobility of Si requires a 45×45×45 $\kk$-grid and a 90×90×90 $\qq$-grid
to reach a 5% relative error [[cite:Brunin2020b]].
Roughly speaking, an explicit computation of phonons in Si with a 90×90×90 $\qq$-mesh
requires around 20000 × 3 × [[natom]] DFPT calculations
so you can easily get an idea of the cost of a fully ab-initio evaluation of the e-ph matrix elements
for all these $\qq$-points.

The EPH code bypasses this bottleneck by **interpolating the DFPT potentials** in $\qq$-space
while Bloch states are computed non-self-consistently on arbitrarily dense $\kk$-meshes.
As a net result, the three problems (electrons, phonons and electron-phonon) are now
**partly decoupled** and can be converged separately.
Keep in mind, however, that the fact that one can easily densify the $\qq$-sampling in the EPH code does not
mean that one can use under-converged values for the ground-state (GS) and DFPT parts.
Indeed, the quality of the interpolation depends on the initial $\kk$- and $\qq$-meshes.
The take-home message is that one should always converge carefully both electronic and vibrational properties
before moving to EPH computations.

For further information about the difference between EPH and ANADDB, see also [[cite:Gonze2019]].
Further details about the EPH implementation are available in [[cite:Brunin2020b]].

## EPH workflow

A typical EPH workflow with arrows denoting dependencies between the different steps
is schematically represented in the figure below:

![](eph_intro_assets/eph_workflow.png){: style="height:400px;width:400px"}

The brown boxes are standard DFPT calculations done with relatively coarse $\kk$- and $\qq$-meshes (for instance, 9×9×9 in Si).
Each DFPT run produces a (partial) DDB file with a portion of the full dynamical matrix
as well as POT files with the first-order derivative of the KS potential (referred to as the DFPT potential below).
The partial POT files are merged with the **mrgdv** utility to produce a
single **DVDB file** (Derivatives of V($\rr$) DataBase).
As usual, the partial DDB files are merged with **mrgddb** (see [the second tutorial on DFPT](/tutorial/rf2)).

The EPH driver (blue box) receives in input the total DDB and the DVDB as well as a GS WFK file that is usually
produced with a different $\kk$-mesh (in some cases, even with a different number of bands
when high-energy empty states are needed).
These ingredients are then used to compute (interpolate) e-ph matrix elements and the associated physical properties.
<!--
The $\kk$-mesh in the WFK file must be commensurate with the $\qq$-mesh in the DVDB file
-->

The EPH calculation is activated by using [[optdriver]] = 7 while [[eph_task]] defines the physical properties to be computed.
To read the external files, one specifies the filepath with the three variables:
[[getwfk_filepath]], [[getddb_filepath]] and [[getdvdb_filepath]].

Internally, the code starts by reading the DDB file to construct the interatomic
force constants (IFCs) in the real-space supercell ($\RR$-space).
<!-- Other external files (WFK, DVDB) may be read depending on the value of [[eph_task]]. -->
Then, the EPH code computes phonon bands and DOS.
Finally a specialized routine is invoked depending on the value of [[eph_task]].

The following physical properties can be computed:

* Imaginary part of ph-e self-energy in metals (**eph_task 1**) that gives access to:

    * Phonon linewidths induced by e-ph coupling
    * Eliashberg function
    * Superconducting properties within the isotropic Migdal-Eliashberg formalism

<!--
    * transport properties in metals with the LOVA approximation.
-->

* Real and imaginary parts of the e-ph self-energy (**eph_task 4**) that gives access to:

    * Zero-point renormalization of the band gap
    * QP corrections due to e-ph scattering as a function of T
    * Spectral function $A(\ww)$ and Eliashberg functions

* Imaginary part of the e-ph self-energy at the KS energy (**eph_task -4**) that gives access to:

    * Phonon-limited carrier mobility, electrical conductivity and Seebeck coefficient
    * Phonon-limited carrier mean free path and relaxation times (or scattering rates)
    * All the calculations above can be done as a function of temperature and doping, for nonpolar and polar materials.

<!--
Other more advanced options are available...

Crystalline symmetries are used throughout the code in order to reduce the number
of $\kk$- and $\qq$-points that must be explicitly treated.
To achieve good parallel efficiently, the most CPU demanding parts are parallelized with MPI employing
a distribution schemes over $\qq$-points, perturbations
and bands (the band level is available only when computing the full self-energy).
[[istwfk]] [[kptopt]]
Features available in ANADDB that are not yet supported by EPH
-->

At the time of writing ( |today|), the following features are **not yet supported** by EPH:

* PAW calculations
* Spin-orbit coupling
* Non-collinear magnetism ([[nspinor]] = 2 and [[nspden]] = 4)
* Non-local part of the pseudopotential applied with [[useylm]] = 1

In this introduction, we focus on the parts that are common to the different sub-drivers i.e.:

1. Computation of vibrational properties via Fourier interpolation of the dynamical matrix.
2. Fourier interpolation of the DFPT potentials.

The use of the different sub-drivers is discussed in more detail in the specialized lessons:

* [Phonon-limited mobilities](/tutorial/eph4mob)
* [ZPR and T-dependent band structures](/tutorial/eph4zpr)

<!--
* [Isotropic superconductivity in metals](tutorial/eph4isotc)
-->

## Phonon bands and DOS with EPH

Since phonon frequencies and displacements are needed for e-ph calculations, it is not surprising
that some of the ANADDB features related to the treatment of the dynamical matrix
are integrated in the EPH code as well.
In many cases, the variables names are the same in EPH and ANADDB especially for important variables
such as [[dipdip]], [[asr]], and [[chneut]].
There are however important differences with respect to the ANADDB input file.
More specifically, in EPH the name of the DDB file is specified by
[[getddb_filepath]] whereas the DFPT $\qq$-mesh associated to the DDB file is given by [[ddb_ngqpt]].
**These two variables are mandatory** when performing EPH calculations.

!!! important

    In Abinit9 the default values of [[dipdip]], [[asr]], and [[chneut]] have been changed.
    In particular, the acoustic sum rule and the charge neutrality of the Born effecive charges
    are enforced by default.
    It is responsability of the user to check whether the breaking of these sum rules (always
    present due to numerical inaccuracies) is reasonable.
    By the same token, make sure that no vibrational instabilty is present before
    embarking on big EPH calculations.
    If the spectrum presents instabilities around $\Gamma$ due to a Fourier interpolation
    done with coarse $\qq$-sampling, you may try to use [[rifcsph]].

### Variables for phonon DOS

By default, the EPH code computes the phonon DOS and the atom-projected PHDOS by interpolating
the IFCs on the *dense* $\qq$-mesh specified by [[ph_ngqpt]].
The default $\qq$-grid is 20×20×20. You may want to increase this value for more accurate results.
The step of the (linear) frequency mesh is governed by [[ph_wstep]].
The linear tetrahedron method by [[cite:Bloechl1994]] is used by default.
The Gaussian method can be activated via [[ph_intmeth]] with [[ph_smear]] defining
the Gaussian smearing (in Hartree units by default).
The final results are stored in the **PHDOS.nc** file (same format at the one produced by ANADDB).
The computation of the PHDOS can be disabled by setting [[prtphdos]] = 0 to make the calculation a bit faster.

To summarize: to compute the PHDOS with the tetrahedron method and a 40x40x40 $\Gamma$-centered $\qq$-mesh,
one should use

```sh
prtphdos 1  # 0 to disable this part.
ph_ngqpt 40 40 40
```

To visualize the results with |AbiPy|, execute

```sh
abiopen.py out_PHDOS.nc -e
```

### Variables for phonon band structures

The computation of the phonon band structure is activated automatically provided the input file
defines the high-symmetry $\qq$-path in terms of [[ph_nqpath]] vertices listed in the [[ph_qpath]] array.
The [[ph_ndivsm]] variable defines the number of divisions used to sample the smallest segment of the path
so that the number of points in each segment is proportional to the length of the segment.
The computation of the phonon band structure can be deactivated by setting [[prtphbands]] = 0.
The final results are stored in the **PHBST.nc** file (same format at the one produced by ANADDB).

For instance, the section for the phonon band structure could look like

```sh
prtphbands 1
ph_nqpath 5
ph_qpath 0.0 0.0 0.0 # Gamma
         1/2 0.0 1/2  # X
         1/2 1/4 3/4  # W
         3/8 3/8 3/4  # K
         0.0 0.0 0.0  # Gamma
```

To obtain the list of high-symmetry q-points, one can use the |abistruct| script provided by |AbiPy|:

```sh
abistruct.py kpath in_DDB
```

and the phonon bands can be visualized using

```sh
abiopen.py out_PHBST.nc -e
```


## Electron-phonon matrix elements

The e-ph matrix elements $\gkq$ are defined by

\begin{equation}
\gkq = \langle \psi_{m\kk+\qq}|\Delta_\qnu V^\KS|\psi_{n\kk} \rangle,
\label{eq:elphon_mel}
\end{equation}

where $\psi_{n\kk}$ is the KS Bloch state and $\Delta_\qnu V^\KS$ is the first-order variation of the
self-consistent KS potential induced by the phonon mode $\qnu$.
The scattering potential can be expressed as:

\begin{equation}
    \Delta_{\qq\nu} V^\KS(\rr) = e^{i\qq\cdot\rr} \Delta_{\qq\nu} v^\KS(\rr).
\label{eq:delta_vks}
\end{equation}

where $\Delta_{\qq\nu} v^\KS(\rr)$ is a lattice periodic function [[cite:Giustino2017]].
Note that ABINIT computes the response to the **atomic perturbation** defined by
the three variables [[qpt]], [[rfdir]] and [[rfatpol]] when [[rfphon]] is set to 1.
The connection between the phonon representation and the atomic perturbation employed by the DFPT code
is given by:

\begin{equation}
    \Delta_{\qq\nu} v^\KS(\rr) =
    \dfrac{1}{\sqrt{2\wqnu}} \sum_{\kappa\alpha}
    \dfrac{{e}_{\kappa\alpha,\nu}(\qq)}{\sqrt{M_\kappa}}\,\partial_{\kappa\alpha,\qq} v^\KS(\rr),
\end{equation}

where ${e}_{\kappa\alpha,\nu}(\qq)$ is the $\alpha$-th Cartesian component of the phonon eigenvector
for the atom $\kappa$ in the unit cell, $M_\kappa$ its atomic mass and
$\partial_{\kappa\alpha,\qq} v^\KS(\rr)$ is the first-order derivative of the KS potential
that can be obtained with DFPT by solving self-consistently a system of Sternheimer equations for a given
$(\kappa\alpha, \qq)$ perturbation [[cite:Gonze1997]] [[cite:Baroni2001]].

The DVDB file stores $\partial_{\kappa\alpha,\qq} v^\KS(\rr)$
for all the $\qq$-points in the IBZ and all the irreducible atomic perturbations.
More rigorously, we should say that the DVDB file stores the local part of the DFPT potential
(variation of the Hartree + XC + local part of the pseudo)
but this is a rather technical point discussed in more detail in [[cite:Brunin2020b]] that is not relevant
for the present discussion so we do not elaborate more on this.

!!! important

    The number of irreducible atomic perturbations depends on the $\qq$-point and the symmetries of the system.
    Actually only a particular subset of the point group of the unperturbed crystal can be
    exploited at the DFPT level.
    Fortunately, you don't have to worry about these technical details as symmetries are fully supported in EPH.
    Just run the DFPT calculation for the irreducible perturbations in the IBZ as usual.
    Also the computation of the WFK can be limited to the IBZ.
    The EPH code will employ symmetries to reconstruct the different quantities at runtime.

## Fourier interpolation of the DFPT potentials

The EPH code employs the Fourier interpolation proposed in [[cite:Eiguren2008]]
to obtain the scattering potentials at arbitrary $\qq$-points.
In this interpolation technique, one uses the DFPT potential stored in the DVDB file
to compute the $\qq \rightarrow \RR$ Fourier transform

\begin{equation}
	\label{eq:dfpt_pot_realspace}
    W_{\kappa\alpha}(\rr,\RR) = \dfrac{1}{N_\qq} \sum_\qq e^{-i\qq\cdot(\RR - \rr)}\,
    \partial_{\kappa\alpha\qq}{v^{\text{scf}}}(\rr),
\end{equation}

where the sum is over the $N_\qq$ BZ $\qq$-points belonging to the ab-initio [[ddb_ngqpt]] grid.
<!--
and $\partial_{\kappa\alpha\qq}{v^{\text{scf}}}$ represents the (lattice-periodic) first order derivative
of the local part of the KS potential associated to atom $\kappa$ along the Cartesian direction $\alpha$.
-->
Once $W_{\kappa\alpha}(\rr,\RR)$ is known, it is possible to interpolate the potential
at an arbitrary point $\tilde{\qq}$ using the inverse transform

\begin{equation}
	\label{eq:dfpt_pot_interpolation}
    \partial v^{scf}_{\tilde\qq\kappa\alpha}(\rr) \approx \sum_\RR e^{i\tilde{\qq}\cdot(\RR - \rr)} W_{\kappa\alpha}(\rr,\RR).
\end{equation}

where the sum is over the lattice vectors inside the Born-von Karman supercell.
The algorithm used to define the $\RR$ points of the supercell with the
corresponding weights is specified by [[dvdb_rspace_cell]].

The accuracy of the interpolation depends on the localization of $W_{\kappa\alpha}$ in $\RR$-space.
This means that the Born-von Karman supercell corresponding to the [[ddb_ngqpt]] grid should be large
enough to capture the spatial decay of $W_{\kappa\alpha}(\rr,\RR)$ as a function of $\RR$.
As a consequence, [[ddb_ngqpt]] should be subject to convergence studies.

In metals, $W_{\kappa\alpha}$ is expected to be short-ranged provided one ignores possible Kohn anomalies.
On the contrary, a special numerical treatment is needed in semiconductors and insulators due to the presence of
long-ranged (LR) **dipolar** and **quadrupolar** fields in $\RR$-space.
These LR terms determine a non-analytic behaviour of the scattering potentials
in the long-wavelength limit $\qq \rightarrow 0$ [[cite:Vogl1976]].
To handle the LR part, the EPH code uses an approach that is similar in spirit to the one employed
for the Fourier interpolation of the dynamical matrix [[cite:Gonze1997]].

The idea is relatively simple.
One subtracts the LR part from the DFPT potentials before computing Eq. \eqref{eq:dfpt_pot_realspace}
thus making the real-space representation amenable to Fourier interpolation.
The non-analytical part
<!-- (Eq.\eqref{eq:v1_long_range}) -->
is then restored back to Eq. \eqref{eq:dfpt_pot_interpolation} when interpolating the potential at $\tilde{\qq}$.

<!--
the long-range part associated to the displacement of atom $\kappa$ along the cartesian direction $\alpha$ can be modeled with
-->
In polar materials, the leading term is given by the dipolar field [[cite:Verdi2015]], [[cite:Sjakste2015]]:

\begin{equation}
   \label{eq:v1_long_range}
    V^{\mathcal{L}}_{\kappa\alpha\qq}(\rr) = i \dfrac{4\pi}{\Omega} \sum_{\GG \neq -\qq}
    \dfrac{(\qG)_\beta\cdot {\bm Z}^*_{\kappa\alpha,\beta}\,
    e^{i (\qG) \cdot (\rr - {\bm{\tau}}_{\kappa})}} {(\qG) \cdot {\bm{\varepsilon}}^\infty \cdot (\qG)},
\end{equation}

where ${\bm{\tau}}_\kappa$ is the atom position, $\Omega$ is the volume of the unit cell,
$\bm{Z}^*$ and ${\bm{\varepsilon}}^\infty$ are the Born effective charge tensor
and the dielectric tensor, respectively, and summation over the Cartesian directions $\beta$ is implied.
This term is present in polar materials (non-zero $\bm{Z}^*$) and
diverges as $1/q$ for $\qq \rightarrow 0$ hence the singularity is integrable in 3D systems.
$\bm{Z}^*$ and ${\bm{\varepsilon}}^\infty$ are read automatically from the DDB file (if present) so we
**strongly recommend** to compute these quantities with DFPT in order to prepare an EPH calculation
in semiconductors.

In non-polar materials, the Born effective charges are zero but the scattering potentials are still non-analytic
due to presence of jump discontinuities.
As discussed in [[cite:Brunin2020]], the non-analytic behaviour can be fully captured by using the more general expression:
<!--
In this case the leading term is associated to the dynamical quadrupoles [[cite:Royo2019]].
The expression for the LR model including both dipole and quadrupole terms reads:
-->

\begin{equation}
    V^{\mathcal{L}}_{\kappa\alpha,\qq}(\rr) =
    %\partial_{\kappa\alpha,\qq} v^{\Lc}(\rr) =
    \frac{4\pi}{\Omega} \sum_{\GG\neq\mathbf{-q}}
    \frac{ i(\qG)_\beta
    Z^*_{\kappa\alpha,\beta}
    -
    (\qG)_\beta
    ({Z^*_{\kappa\alpha,\beta}\cal{Q}}v^{\text{Hxc},\cal{E}_{\gamma}}(\rr)- \frac{1}{2}Q_{\kappa\alpha}^{\beta\gamma})(\qG)_\gamma
    }
    {(\qG) \cdot {\bm{\varepsilon}}^\infty \cdot (\qG)}
    e^{i (\qG) \cdot (\rr - {\bm{\tau}}_{\kappa})}
\label{eq:LRpot}
\end{equation}

!!! important
	The computation of the dynamical quadrupoles tensor within the DFPT framework will be made available in a future release,
	together with a specific tutorial. Once it is computed and stored in the DDB,
	the EPH code reads it automatically and uses it for the LR model.

<!--
TODO: Discuss more the integration with the DFPT part.
-->

In the implementation, each Fourier component is multiplied by the
Gaussian filter $e^{-\frac{|\qG|^2}{4\alpha}}$
in order to damp the short range components that are not relevant for the definition of the LR model.
The $\alpha$ parameter can be specified via the [[dvdb_qdamp]] input variable.
The default value worked well in the majority of the systems investigated so far yet
this parameter should be subject to convergence tests.
For testing purposes, it is possible to deactivate the treatment of the LR part by setting [[dvdb_add_lr]] to 0.
<!--
It can also be set to -1 if one is interested only in the short-range part of the e-ph scattering potentials.
-->

### On the importance of the initial DFPT mesh

At this point it is worth commenting about the importance of the initial DFPT $\qq$-mesh.
The Fourier interpolation implicitly assumes that the signal in $\RR$-space decays quickly hence
the quality of the *interpolated* phonon frequencies and of the *interpolated* DFPT potentials
between the ab-initio points depends on the spacing of the initial $\qq$-mesh that
in turns defines the size of the Born-von-Karman supercell.
In other words, the denser the DFPT mesh, the larger the real-space supercell and the better the interpolation,
especially for $\qq$-points close to $\Gamma$.
<!--
In semiconductors the atomic displacement induces dynamical dipoles and quadrupoles at the level of the density
that will generate long-range scattering potentials.
These potentials affect the behaviour of the e-ph matrix elements for $\qq \rightarrow 0$ and the
$\qq$-mesh must be dense enough to capture the full strenght of the coupling.
A more detailed discussion can be found in [[cite:Brunin2020]], [[cite:Verdi2015]] and [[cite:Sjakste2015]].
-->

From a more practical point of view, this implies that one should always **monitor the convergence of the
physical properties with respect to the initial DFPT $\qq$-mesh**.
The LR model implemented in ABINIT facilitates the convergence as the non-analytic behaviour for
$\qq \rightarrow 0$ is properly described yet the Fourier interpolation can introduce oscillations
between the *ab-initio* $\qq$-points and these oscillations may affect the quality of the
physical results [[cite:Brunin2020]].

## Tricks to accelerate the computation and reduce the memory requirements

Each sub-driver implements specialized techniques to accelerate the calculation and reduce the memory requirements.
Here we focus on the techniques that are common to the different EPH sub-drivers.
Additional tricks specific to the particular value of [[eph_task]] are discussed in more detail in the associated lessons.

First of all, note that the memory requirements for the $W_{\kappa\alpha}(\rr,\RR)$ array
scales as [[nfft]] × product([[ddb_ngqpt]]).
This quantity should be multiplied by 3 * [[natom]] if each MPI process stores all the perturbations in memory.
The MPI parallelism over perturbations (see [[eph_np_pqbks]]) allows one to decrease this prefactor from
*3 × natom* to *3 × natom / nprocs_pert*.

Also, the total number of $\rr$-points ([[nfft]]) plays an important role both at the level of memory
as well as the level of the wall-time.
To optimize this part, one can decrease the value of [[boxcutmin]]
to a value smaller than 2 e.g. 1.5 or the more aggressive 1.1.
Note that one is not obliged to run the GS/DFPT part with the same [[boxcutmin]].
The EPH code will automatically interpolate the DFPT potentials if the input FFT mesh computed from
[[ecut]] and [[boxcutmin]] differs from the one found in the DVDB file.
Just to clarify, you can change the value of [[boxcutmin]] in the EPH part but not [[ecut]].
<!--
An exact representation of densities/potentials in $\GG$-space is obtained with [[boxcutmin]] = 2,
but we found that using a value of 1.1 does not significantly affect the result
while allowing one to decrease the cost of the calculation and the memory by a factor ~8.
-->

A significant fraction of the wall-time in EPH is spent for performing the FFTs required
to apply the first-order Hamiltonian $H^1$.
The use of single precision in the FFT routines allows one to decrease the computational cost without loosing precision.
This trick is activated by setting [[mixprec]] = 1.
Note that this feature is available only if the code is linked with FFTW3 or intel-MKL.

!!! important

    The [[boxcutmin]] and [[mixprec]] tricks **are not activated by default**
    because users are supposed to perform preliminary tests
    to make sure the quality of the results is not affected by these options.


By default, the EPH code stores the KS wavefunctions in a single precision array although
the majority of the calculations are done with double precision arithmetic (except for the FFT when [[mixprec]] is used).
The precision used for the internal buffer can be specified at configure time with:

```sh
enable_gw_dpc=“yes”      # Store wavefunctions in double precision buffers.
```

The default value if "no" i.e. single precision and we suggest not to change this option unless you have a good reason to do so.

We terminate the discussion with another trick that is not directly related to the EPH code but
to the DFPT computation.
Since the EPH code does not need the first order change of the wavefunctions (1WF files)
we suggest to avoid the output of these files by using [[prtwf]] = -1 in the DFPT part
as these files are quite large and the overall space on disk scales as **nqpt × 3 × natom**.
When [[prtwf]] is set to -1, the DFPT code writes the 1WF file only if the DFPT SCF cycle is not converged
so that one can still restart from these wavefunctions if needed
(restarting a DFPT run from the first order wavefunctions is more effective than restarting from the first order density).


## Star-function interpolation of the KS eigenvalues

As mentioned in the introduction, in the EPH implementation Bloch states are computed
non-self-consistently on arbitrarily dense $\kk$-meshes without having to resort to interpolation schemes.
The advantage of such approach is that calculations can be easily automated.
The drawback is that the computational cost of the NSCF quickly increases with the density
of the $\kk$-mesh and [[nband]] hence for "big calculations" the cost
of the NSCF part may be even greater than the e-ph computation itself.

There are however physical properties whose computation does not require the knowledge
of the KS states for each $\kk$-point in the IBZ.
For instance, the computation of mobilities in semiconductors require the knowledge of the KS states whose energy
is slight above (below) the CMB (VBM), let's say ~0.2 eV.
In metals, only states close to the Fermi level are needed to compute superconducting properties with the standard formalism.
In other words, several e-ph calculations in which delta functions are involved require extremely dense BZ meshes
to converge but as a matter of fact only a **relatively small fraction of the BZ compatible with energy and crystalline-momentum
conservation is nededed**.

At this point a question naturally arises: can we avoid the NSCF computation of $\kk$-points that
are supposed to give negligible contribution to the final physical results?
The answer is yes provided we are able to predict in some easy way and with reasonable accuracy the
KS eigenvalues $\ee_\nk$ **without actually solving the KS equations**.

The aproach used in the EPH code is based on the star-function interpolation by Shankland-Koelling-Wood (SKW)
with the improvements described in [[cite:Pickett1988]].
In this method, the single-particle energies are expressed in terms of the (symmetrized) Fourier sum

\begin{equation}
\label{eq:skw_expansion}
  \enk = \sum_\RR c_{n\RR} S_\RR(\kk).
\end{equation}

where the star function, $S_\RR(\kk)$, is defined by

\begin{equation}
\label{eq:star_function}
S_\RR(\kk) = \dfrac{1}{N}\sum_\mcO e^{i\kk\cdot \mcO \RR},
\end{equation}

$\RR$ is a lattice vector and the sum is over the $N$ rotations of the crystallographic point group.
By construction, the expansion in Eq.\eqref{eq:skw_expansion} fulfills the basic symmetry properties
of the single-particle energies:

\begin{align}
\label{eq:eigen_properties}
  \enk = \epsilon_{n\kG}, \\
  \enk = \epsilon_{n\mcO\kk}.
\end{align}

In principle, the expansion coefficients in Eq.\eqref{eq:skw_expansion} can be uniquely determined
by using a number of star functions equal to the number of *ab initio* $\kk$-points
but this usually leads to sharp oscillations between the input eigenvalues.
To avoid this problem, one uses more star functions than *ab initio* $\kk$-points and constrains the
fit so that the interpolant function passes through the input energies and a roughness function is minimized [[cite:Pickett1988]].

This [[einterp]] variable activates the interpolation of the electronic eigenvalues.
The user can specify the number of star functions per
*ab initio* $\kk$-point and an optional Fourier filtering as proposed in [[cite:Uehara2000]].

<!--
%In this case, rcut is given by einterp(2) * Rmax where Rmax is the maximum length of the lattice vectors included in the star expansion
 [[einterp]] can be used to interpolate KS eigenvalues at
 the end of the ground state calculation (\href{https://docs.abinit.org/tests/v8/Input/t04.in}{{\texttt{v8\#42}}})
 or to interpolate GW energies (\href{https://docs.abinit.org/tests/libxc/Input/t42.in}{{\texttt{libxc\#42}}}) when {\bf optdriver} = 4.

In this case, one can employ the Python interface provided by \ABIPY to
automate the procedure.
An example can be found in this
\href{https://nbviewer.jupyter.org/github/abinit/abitutorials/blob/master/abitutorials/g0w0/lesson_g0w0.ipynb}{jupyter notebook}.

%einterp consists of 4 entries. The first element specificies the interpolation method.
%
%1 → Star-function interpolation (Shankland-Koelling-Wood Fourier interpolation scheme, see [Pickett1988]
%2 → B-spline interpolation.
%The meaning of the other entries depend on the interpolation technique selected.
%In the case of star-function interpolation:
%
%einterp(2): Number of star-functions per {\it ab initio} k-point
%einterp(3): If non-zero, activate Fourier filtering according to Eq 9 of [Uehara2000]. In this case, rcut is given by einterp(2) * Rmax where Rmax is the maximum length of the lattice vectors included in the star expansion
%einterp(4): Used if einterp(2) /= 0. It defines rsigma in Eq 9

%It is worth noting that the QP energies must fulfill the symmetry properties of the point group of the crystal:
%and
%where G is a reciprocal lattice vector and is an operation of the point group.
%Therefore it is possible to employ the star-function interpolation by Shankland, Koelling and Wood in the improved version proposed by Pickett to fit the {\it ab initio} results. This interpolation technique, by construction, passes through the initial points and satisfies the basic symmetry property of the band energies.
%It should be stressed, however, that this Fourier-based method can have problems in the presence of band crossings that may cause unphysical oscillations between the {\it ab initio} points.
-->
Without entering into details that will be discussed in the other specialized lessons,
one can use the SKW algorithm to find the relevant $\kk$-points, perform an *ab-initio* NSCF run
for these wavevectors only in order to produce a WFK file that can be used by the EPH code.
The entire procedure is performed in an automatic way inside ABINIT but before running big EPH calculations,
we strongly recommend to check whether the SKW interpolation gives reasonable results.
A typical test would be:

1. Compute a WFK file on a reasonably dense IBZ (let's call the file **IBZ_WFK**)

2. Perform a NSCF band structure calculation along a high-symmetry $\kk$-path covering the most important
   regions of the BZ (e.g. band edges in semiconductors).
   This step produces another WFK file that will be used as reference to check the interpolation
   (let's call it **KPATH_WKF**)

3. Use the *abitk* executable in *src/98_main* to interpolate the KS energies using the eigenvalues
   in **IBZ_WFK** and compare the results with the ab-initio band structure stored in **KPATH_WFK**.

The syntax is:

```sh
abitk skw_compare IBZ_WFK KPATH_WFK [--lpratio 5] [--rcut 0.0] [--rsigma 0]
```

where optional arguments are placed between square brackets and the default value is indicated.
If you prefer, it is possible to replace WFK files with GSR.nc file as in

```sh
abitk skw_compare IBZ_GSR.nc KPATH_GSR.nc
```

as only KS energies are needed for the SKW interpolation.

To compare the bands with AbiPy, use the |abicomp| script with the `ebands` command:

```sh
abicomp.py ebands abinitio_EBANDS.nc skw_EBANDS.nc -p combiplot
```

If the fit is not satisfactory, you may want to try one of the following options (in order of importance):

1. Increase the ab-initio mesh in **IBZ_WFK**.
2. Increase the value of *lpratio*
3. Play with *rcut* and *rsigma* to damp the oscillations in the interpolant

Note that it is sometimes difficult to get completely rid of spurious oscillations
or artifacts in the SKW interpolation
especially in the presence of **degeneracies or band crossing/anti-crossing**,
Remember, however, that achieving perfect agreement between the SKW interpolation and the ab-initio results
is not crucial since the SKW bands are only used to find those $\kk$-points that are sufficiently
close to the band edges (Fermi level).
All these wavevectors will be recomputed afterwards with KS-DFT and possible oscillations
or artifacts will disappear in the ab-initio results.

In a nutshell, you need to **make sure that the SKW bands are reasonably close** to the ab-initio results
especially in the region around the band edges for semiconductors or around the Fermi level for metals.
Small deviations between SKW and ab-initio bands can always be accounted for by increasing the value
of [[sigma_erange]] used for generating the KERANGE.nc file.

Examples of input files to compute WFK files with the KERANGE are given in the last section of
the [mobility tutorial](/tutorial/eph4mob#how-to-compute-only-the-k-points-close-to-the-band-edges).

<!--
TODO: Recheck the code, perhaps I can use the ab-initio band edge if its greater/smaller than the SKW one.
The most important thing is that SKW reproduces the position of the band edges as these values are then used
that the position of the SKW band edge is consistent
-->
---
authors: FBruneval, XG
---

# Second tutorial on GW

## Treatment of metals.

This tutorial aims at showing how to obtain self-energy corrections to the DFT
Kohn-Sham eigenvalues within the GW approximation, in the metallic case,
without the use of a plasmon-pole model.
The band width and Fermi energy of Aluminum will be computed.

The user may read the papers

  * F. Bruneval, N. Vast, and L. Reining, Phys. Rev. B **74** , 045102 (2006) [[cite:Bruneval2006]],
for some information and results about the GW treatment of Aluminum. He will
also find there an analysis of the effect of self-consistency on
quasiparticles in solids (not present in this tutorial, however available in
Abinit). The description of the contour deformation technique that bypasses
the use of a plasmon-pole model to calculate the frequency convolution of G
and W can be found in

  * S. Lebegue, S. Arnaud, M. Alouani, P. Bloechl, Phys. Rev. B **67**, 155208 (2003) [[cite:Lebegue2003]],

with the relevant formulas.

A brief description of the equations implemented in the code can be found in the [[theory:mbt|GW notes]]
Also, it is suggested to acknowledge the efforts of developers of the GW part of ABINIT,
by citing the [[cite:Gonze2005|2005 ABINIT publication]].

The user should be familiarized with the four basic tutorials of ABINIT, see the
[tutorial index](/tutorial/) as well as the [first GW tutorial](/tutorial/gw1).

Visualisation tools are NOT covered in this tutorial.
Powerful visualisation procedures have been developed in the Abipy context,
relying on matplotlib. See the README file of [Abipy](https://github.com/abinit/abipy)
and the [Abipy tutorials](https://github.com/abinit/abitutorials).

This tutorial should take about one hour to be completed (also including the
reading of [[cite:Bruneval2006]] and [[cite:Lebegue2003]].

[TUTORIAL_README]

## The preliminary Kohn-Sham band structure calculation

*Before beginning, you might consider to work in a different subdirectory as
for the other tutorials. Why not Work_gw2?*

In [basic tutorial 4](/tutorial/base4), we have computed different properties of
Aluminum within DFT(LDA). Unlike for silicon, in this approximation, there is
no outstanding problem in the computed band structure. Nevertheless, as you
will see, the agreement of the band structure with experiment can be improved
significantly if one uses the GW approximation.

In the directory *Work_gw2*, copy the file *tgw2_1.abi* located in *\$ABI_TESTS/tutorial/Input*.

```sh
    mkdir Work_gw2
    cd Work_gw2
    cp $ABI_TESTS/tutorial/Input/tgw2_1.abi .
```

Then issue:

    abinit tgw2_1.abi > tgw2_1.log 2> err &

{% dialog tests/tutorial/Input/tgw2_1.abi %}

This run generates the WFK file for the subsequent GW computation and also
provides the band width of Aluminum. Note that the simple Fermi-Dirac smearing
functional is used ([[occopt]] = 3), with a large smearing ([[tsmear]] = 0.05 Ha).
The **k**-point grid is quite rough, an unshifted 4x4x4 Monkhorst-Pack grid
(64 **k**-points in the full Brillouin Zone, folding to 8 **k**-points in the Irreducible
wedge, [[ngkpt]] = 4 4 4). Converged results would need a 4x4x4 grid with 4
shifts (256 **k**-points in the full Brillouin zone). This grid contains the $\Gamma$
point, at which the valence band structure reaches its minimum.

The output file presents the Fermi energy

```
 Fermi (or HOMO) energy (eV) =   6.76526   Average Vxc (eV)=  -9.88844
```

as well as the lowest energy, at the $\Gamma$ point

```
 Eigenvalues (   eV  ) for nkpt=   8  k points:
 kpt#   1, nband=  6, wtk=  0.01563, kpt=  0.0000  0.0000  0.0000 (reduced coord)
  -4.22031   19.66151   19.66151   19.66151   21.09462   21.09462
```


So, the occupied band width is 10.98 eV within PBE.
This is to be compared to the experimental value of 10.6 eV (see references in [[cite:Bruneval2006]]).

## Calculation of the screening file

Let's run the calculation of the screening file immediately.
So, copy the file *tgw2_2.abi*. Also, copy the WFK file (*tgw2_1o_WFK*) to *tgw2_2i_WFK*. Then
run the calculation (it should take about 3 seconds).

```sh
    cp $ABI_TESTS/tutorial/Input/tgw2_2.abi .
    cp tgw2_1o_WFK tgw2_2i_WFK
    abinit tgw2_2.abi >& log &
```


{% dialog tests/tutorial/Input/tgw2_2.abi %}

We now have to consider starting a GW calculation. However, unlike in the case
of Silicon in the previous GW tutorial, where we were focussing on quantities
close to the Fermi energy (spanning a range of a few eV), here we need to
consider a much wider range of energy: the bottom of the valence band lies
around -11 eV below the Fermi level. Unfortunately, this energy is of the same
order of magnitude as the plasmon excitations. With a rough evaluation, the
classical plasma frequency for a homogeneous electron gas with a density equal
to the average valence density of Aluminum is 15.77 eV. Hence, using
plasmon-pole models may be not really appropriate.

In what follows, we will compute the GW band structure without a plasmon-pole
model, by performing the numerical frequency convolution. In
practice, it is convenient to extend all the functions of frequency to the
full complex plane. And then, making use of the residue theorem, the
integration path can be deformed: one transforms an integral along the real
axis into an integral along the imaginary axis plus residues enclosed in the
new contour of integration. The method is extensively described in [[cite:Lebegue2003]].

Examine the input file *tgw2_2.abi*.

{% dialog tests/tutorial/Input/tgw2_2.abi %}

The ten first lines contain the important information.
There, you find some input variables that you are already
familiarized with, like [[optdriver]], [[ecuteps]], but also new
input variables: [[gwcalctyp]], [[nfreqim]], [[nfreqre]], and [[freqremax]].
The purpose of this run is simply to generate the screening matrices. Unlike
for the plasmon-pole models, one needs to compute these at many different
frequencies. This is the purpose of the new input variables. The main variable
[[gwcalctyp]] is set to 2 in order to specify a _non_ plasmon-pole model
calculation. Note that the number of frequencies along the imaginary axis
governed by [[nfreqim]] can be chosen quite small, since all functions are
smooth in this direction. In contrast, the number of frequencies needed along
the real axis set with the variable [[nfreqre]] is usually larger.

## Finding the Fermi energy and the bottom of the valence band

Let's run the calculation of the band width immediately and
then we'll study of the input file. So, get the file *tgw2_3.abi*.

```sh
    cp $ABI_TESTS/tutorial/Input/tgw2_3.abi .
    cp tgw2_1o_WFK tgw2_3i_WFK
    cp tgw2_2o_SCR tgw2_3i_SCR
    abinit tgw2_3.abi >& log &
```

The computation of the GW quasiparticle energy at the $\Gamma$ point of Aluminum
does not differ from the one of quasiparticle in Silicon. However, the
determination of the Fermi energy raises a completely new problem: one should
sample the whole Brillouin Zone to get new energies (quasiparticle energies) and
then determine the Fermi energy. This is actually the first step towards a self-consistency!

Examine the input file *tgw2_3.abi*:

{% dialog tests/tutorial/Input/tgw2_3.abi %}

The first thirty lines contain the important information.
There, you find some input variables with values that you are
already familiarized with, like [[optdriver]], [[ecutsigx]], [[ecutwfn]].
Then, comes the input variable [[gwcalctyp]] = 12. The value _x2_ corresponds to
a contour integration. The value _1x_ corresponds to a self-consistent
calculation with update of the energies only. Then, one finds the list of k
points and bands for which a quasiparticle correction will be computed:
[[nkptgw]], [[kptgw]], and [[bdgw]]. The number and list of **k**-points is simply
the same as [[nkpt]] and [[kpt]]. One might have specified less **k**-points,
though (only those needing an update). The list of band ranges [[bdgw]] has
been generated on the basis of the DFT(LDA) eigenenergies. We considered only the
bands in the vicinity of the Fermi level: bands much below or much above are
likely to remain much below or much above the Fermi region. In the present run, we
are just interested in the states that may cross the Fermi level, when going
from DFT to GW. Of course, it would have been easier to select an homogeneous range
for the whole Brillouin zone, e.g. from 1 to 5, but this would have been more time-consuming.

In the output file, one finds the quasiparticle energy at $\Gamma$, for the lowest band:

```yaml
--- !SelfEnergy_ee
iteration_state: {dtset: 1, }
kpoint     : [   0.000,    0.000,    0.000, ]
spin       : 1
KS_gap     :    0.000
QP_gap     :    0.000
Delta_QP_KS:    0.000
data: !SigmaeeData |
     Band     E_DFT   <VxcDFT>   E(N-1)  <Hhartree>   SigX  SigC[E(N-1)]    Z     dSigC/dE  Sig[E(N)]  DeltaE  E(N)_pert E(N)_diago
        1    -4.220    -9.458    -4.220     5.238   -15.616     5.931     0.903    -0.107    -9.663    -0.206    -4.426    -4.448
        2    19.662    -9.585    19.662    29.246    -2.718    -7.097     0.802    -0.246    -9.770    -0.185    19.477    19.430
        3    19.662    -9.585    19.662    29.246    -2.718    -7.098     0.802    -0.247    -9.770    -0.185    19.476    19.430
        4    19.662    -9.585    19.662    29.246    -2.718    -7.098     0.802    -0.247    -9.770    -0.185    19.476    19.431
        5    21.095    -9.387    21.095    30.482    -2.461    -7.390     0.713    -0.402    -9.718    -0.331    20.764    20.631
        6    21.095    -9.387    21.095    30.482    -2.461    -7.390     0.713    -0.402    -9.718    -0.331    20.764    20.631
        7    21.095    -9.387    21.095    30.482    -2.461    -7.390     0.713    -0.403    -9.718    -0.331    20.764    20.631
...
```

(the last column is the relevant quantity). The updated Fermi energy is also mentioned:

```text
 New Fermi energy :        2.425933E-01 Ha ,    6.601299E+00 eV
```


The last information is not printed in case of [[gwcalctyp]] lower than 10.

Combining the quasiparticle energy at $\Gamma$ and the Fermi energy, gives the
band width, 11.05 eV.
Remember the experimental value is around 10.6 eV.

## Computing a GW spectral function, and the plasmon satellite of Aluminum

The access to the non-plasmon-pole-model self-energy (real and imaginary part)
has additional benefit, e.g. an accurate spectral function can be computed,
see [[cite:Lebegue2003]]. You may be interested to see the plasmon satellite of
Aluminum, which can be accounted for within the GW approximation.

Remember that the spectral function is proportional to (with some multiplicative matrix elements) the spectrum which
is measured in photoemission spectroscopy (PES). In PES, a photon impinges the
sample and extracts an electron from the material. The difference of energy
between the incoming photon and the obtained electron gives the binding energy
of the electron in the solid, or in other words the quasiparticle energy or
the band structure. In simple metals, an additional process can take place
easily: the impinging photon can *extract an electron together with a global
charge oscillation in the sample*. The extracted electron will have a kinetic
energy lower than in the direct process, because a part of the energy has gone
to the plasmon. The electron will appear to have a larger binding energy...
You will see that the spectral function of Aluminum consists of a main peak
which corresponds to the quasiparticle excitation and some additional peaks
which correspond to quasiparticle and plasmon excitations together.

Let's run this calculation immediately:

```sh
    cp $ABI_TESTS/tutorial/Input/tgw2_4.abi .
    cp tgw2_1o_WFK tgw2_4i_WFK
    cp tgw2_2o_SCR tgw2_4i_SCR
    abinit tgw2_4.abi >& log &
```

{% dialog tests/tutorial/Input/tgw2_4.abi %}

Compared to the previous file (*tgw2_3.abi*), the input file contains two
additional keywords: [[nfreqsp]], and [[freqspmax]]. Also, the computation of
the GW self-energy is done only at the $\Gamma$ point.

The spectral function is written in the file *tgw2_4o_SIG*. It is a simple text
file. It contains, as a function of the frequency (eV), the real part of the
self-energy, the imaginary part of the self-energy, and the spectral function.
You can visualize it using your preferred software.
For instance, start |gnuplot| and issue

```gnuplot
p 'tgw2_4o_SIG' u 1:4 w l
```

You should be able to distinguish the main quasiparticle peak located at the
GW energy (-3.7 eV) and some additional features in the vicinity of the GW
eigenvalue minus a plasmon energy (-3.7 eV - 15.8 eV = -19.5 eV).

!!! tip

    If |AbiPy| is installed on your machine, you can use the |abiopen| script
    with the `--expose` option to visualize the results:

        abiopen.py tgw2_4o_SIGRES.nc -e -sns

    ![](gw2_assets/al_spectra_b0_gamma.png)

Another file, *tgw2_4o_GW*, is worth to mention: it contains information to be
used for the subsequent calculation of excitonic effects within the Bethe-Salpeter Equation with ABINIT or with other codes from
the [ETSF software page](http://www.etsf.eu/resources/software/codes).
---
authors: XG
---

# Second tutorial on DFPT:

## Phonon band structures, thermodynamical properties.

In this tutorial you will learn how to post-process the raw data of the Abinit DFPT calculations to
get the following physical properties of periodic solids:

* Interatomic forces constants
* Phonon band structures
* Thermodynamical properties

Visualisation tools are NOT covered in this tutorial.
Powerful visualisation procedures have been developed in the Abipy context,
relying on matplotlib. See the README of [Abipy](https://github.com/abinit/abipy)
and the [Abipy tutorials](https://github.com/abinit/abitutorials).

This tutorial should take about 1 hour.

[TUTORIAL_README]

## 1 Generation of a derivative database

*Before beginning, you might consider to work in a different subdirectory as
for the other tutorials. Why not create Work_rf2 in \$ABI_TESTS/tutorespfn/Input?*

Then copy the file *trf2_1.abi* from  \$ABI_TESTS/tutorespfn/Input* to *Work_rf2*:

```sh
cd $ABI_TESTS/tutorespfn/Input
mkdir Work_rf2
cd Work_rf2
cp ../trf2_1.abi .
```

This tutorial starts by the generation of a database, that might be quite time-consuming.
We suggest you to start immediately this computation with

    abinit trf2_1.abi >& log &

It takes about 1-2 minutes to be completed on a PC 2.8 GHz.

In order to do interatomic force constant (IFC) calculations, and to compute
associated phonon band structure and thermodynamical properties, you should
first have some theoretical background.
Let us assume that you have read the litterature relative to the [first tutorial on DFPT](/tutorial/rf1).
You might find additional material, related to the present section, in
[[cite:Gonze1997a]] -especially section IX-, [[cite:Lee1995]] and [[cite:Baroni2001]].
If you haven't read parts of these references, we strongly advise you take the time to read them now.

In short, the idea is that, in order to compute properties for which the
phonon frequencies are needed in the full Brillouin zone, one can use an
elaborate Fourier interpolation, so that only few dynamical matrices need to
be computed directly. Others will be computed by interpolation.
A schematic representation of the different steps required to compute the
dynamical matrix in the IBZ and post-process the results with anaddb is given below.

![](rf2_assets/ph_dde_workflow.png)

Let us have a look at the input file *trf2_1.abi*.

{% dialog tests/tutorespfn/Input/trf2_1.abi %}

The calculation is done for AlAs, the same crystalline material as for the first tutorial on DFPT.
Many input parameters are also quite similar, both at the level of the description
of the unit cell and for the choice of cut-off energy and k point grid.

Still, this input file is rather complex: in one single run, one produces the
Derivative Databases (DDBs) needed for the rest of this tutorial.
So, it starts with a ground-state calculation (dataset 1), followed by the
computation of the response to the d/dk perturbation (dataset 2), and the
response to electric fields, and phonons at Gamma (dataset 3). Datasets 4 to
10 generate the dynamical matrices at 7 q wavevectors, other than Gamma. At
present (v8.6), one can only compute one q point per dataset, that is why so many datasets are needed.

Also, the values of these q wavevectors are not determined automatically.
They must correspond to the q wavevectors needed by the ANADDB utility (see later),
that is, they should form a reduced set of symmetry-inequivalent wavevectors,
corresponding to a regularly spaced grid. In principle, they might not include
the Gamma point, but it is **recommended** to have it in the set, in order for the
Fourier interpolation not to introduce errors at that important point.

!!! tip

    In order to minimize the number of preliminary non-self-consistent calculations,
    it is advised to take a q point mesh that is adjusted to the k point mesh used
    for the electronic structure: all q wavevectors should connect two k point
    wavevectors from this grid.

Such a set of q wavevectors can be generated straightforwardly by running a GS
calculation with [[kptopt]] = 1, [[nshiftk]] = 1, [[shiftk]] = 0 0 0 (to include
gamma) and taking the output kpt set file as this qpt set. One might set
[[nstep]] = 1 and [[nline]] = 1, so only one iteration runs, or even
[[nstep]] = 0 and [[prtvol]] = -1, so no real DFT calculation is done.

The input file *\$ABI_TESTS/tutorespfn/Input/trf2_2.abi* is precisely an input
file that can be used to generate such a set of k points.
Copy it in the present *Work_rf2* directly, as well as the accompanying *trf2_2.files*.
Examine these files, then run this calculation (it is very rapid - it won't hurt the trf2_1 job).
The following k point set is obtained:

           kpt    0.00000000E+00  0.00000000E+00  0.00000000E+00
                  2.50000000E-01  0.00000000E+00  0.00000000E+00
                  5.00000000E-01  0.00000000E+00  0.00000000E+00
                  2.50000000E-01  2.50000000E-01  0.00000000E+00
                  5.00000000E-01  2.50000000E-01  0.00000000E+00
                 -2.50000000E-01  2.50000000E-01  0.00000000E+00
                  5.00000000E-01  5.00000000E-01  0.00000000E+00
                 -2.50000000E-01  5.00000000E-01  2.50000000E-01

It is, as promised, the same as the q point set in the *trf2_1.abi file*.

Now, it might be worth to examine in some detail one of the Derivative
Database that has been created by the trf2_1 run.
We suppose that the file *trf2_1o_DS3_DDB* has already been created.
It corresponds to the third dataset, namely the response to q = 0 and electric field.
Open this file, and read the [[help:respfn#ddb|6.5 section]] of the respfn help file.
Examine the *trf2_1o_DS3_DDB* file carefully.

Seven other similar files will be generated by the trf2_1 run, containing the
same header, but a different 2DTE block. It will be the duty of the MRGDDB
utility, next section, to gather all these information and merge them into a single DDB file.

Now, there might be two possibilities: either the trf2_1 run is finished, and
you can continue the tutorial with the section 2 about the MRGDDB utility, or the run is not finished.
In the latter case, instead of waiting for trf2_1 to be finished, we suggest
you to pursue with section 3. You will use as DDB file the one that can be
found in *\$ABI_TESTS/tutorespfn/Refs*, with the name [[tests/tutorespfn/Refs/trf2_3.ddb.abo|trf2_3.ddb.abo]],
instead of the one that would result from the section 2.
Copy this file to the present directory, then go to
section section 3 of this tutorial. You might come back to section 2 afterwards.

## 2 Manipulation of the derivative databases (the MRGDDB utility)

The use of the MRGDDB utility is described in its [[help:mrgddb|help file]].
Please, read it carefully now.

Use MRGDDB to create the merge DDB from the eight DDB's corresponding to
datasets 3 to 10 of the trf2_1 job, containing the dynamical matrices for the
8 q points, as well as the response to the electric field (dielectric tensor
and Born effective charges). Name the new DDB *trf2_3.ddb.abo*.

!!! note

    Including also the DDB from dataset 1 won't hurt
    (it contains the forces and stresses), but is not needed for the computation
    of phonon band structure, interatomic force constants, and thermodynamical properties.

File *\$ABI_TESTS/tutorespfn/Input/trf2_3.abi* is an example of input file for MRGDDB.

{% dialog tests/tutorespfn/Input/trf2_3.abi %}

You can copy it in the *Work_rf2* directory, and run the merge as follows:

    mrgddb < trf2_3.abi

Note the chevron in the call.
## 3 Analysis of the derivative databases

An introduction to the use of the ANADDB utility is described in its [[help:anaddb|help file]].
Please, read it carefully.

This ANADDB utility is able to perform many different tasks, each governed by
a selected set of input variables, with also some input variables common to
many of the different tasks. The list of tasks to be done in one run is
governed by different flags.
Here is the list of flags:

  * [[anaddb:dieflag]]
  * [[anaddb:elaflag]]
  * [[anaddb:elphflag]]
  * [[anaddb:ifcflag]]
  * [[anaddb:instrflag]]
  * [[anaddb:nlflag]]
  * [[anaddb:piezoflag]]
  * [[anaddb:polflag]]
  * [[anaddb:thmflag]]

Please, take some time to read the description of each of these flags.
Note that some of these flags might be required to allow to run another task.
In this tutorial, we will focus on the flags [[anaddb:ifcflag]] and [[anaddb:thmflag]].

## 4 The computation of interatomic force constants

You can copy the files *trf2_4.abi* and *trf2_4.files* from *\$ABI_TESTS/tutorespfn/Input* to the *Work_rf2* directory.
Open the file *trf2_4.abi*. Note that anaddb use the old format to start calculations which means it needs a files file. Also note that [[anaddb:ifcflag]] is activated.

{% dialog tests/tutorespfn/Input/trf2_4.files tests/tutorespfn/Input/trf2_4.abi %}

Related input variables can be split in three groups.
The first group of variables define the grid of q wavevectors:

  * [[anaddb:brav]]
  * [[anaddb:ngqpt]]
  * [[anaddb:nqshft]]
  * [[anaddb:q1shft]]

Unfortunately, the names of input variables and their meaning is not exactly
the same as the names used to generate the k points in ABINIT.
This is a shame, a remnant of history.
Please read carefully the documentation that describes these input variables.

The second group of variables allows to impose the acoustic sum rule on the
dynamical matrices and the charge neutrality on Born effective charges before proceeding with the analysis:

  * [[anaddb:asr]]
  * [[anaddb:chneut]]

Please, read carefully the explanation for these input variables.

Finally, a third group of variables is related specifically to the analysis of the IFC:

  * [[anaddb:dipdip]]
  * [[anaddb:ifcana]]
  * [[anaddb:ifcout]]
  * [[anaddb:natifc]]
  * [[anaddb:atifc]]

Here also, spend some time to read the associated documentation.

Now, you should issue:

    anaddb < trf2_4.files > trf2_4.log

It will last only a few seconds.

The file *trf2_4.abo* contains the list of interatomic force constants, as well as some analysis.

{% dialog tests/tutorespfn/Refs/trf2_4.abo %}

Open this file and find the following paragraph:

     Analysis of interatomic force constants

     Are given : column(1-3), the total force constant
           then  column(4-6), the Ewald part
           then  column(7-9), the short-range part
     Column 1, 4 and 7 are related to the displacement
           of the generic atom along x,
     column 2, 5 and 8 are related to the displacement
           of the generic atom along y,
     column 3, 6 and 9 are related to the displacement
           of the generic atom along z.

The interatomic force constants are output for the nuclei specified by the
input variable [[anaddb:atifc]]. Here, only atom 1 is considered. The IFCs
with respect to the other nuclei is given, by order of increasing distance.
For each pair of nuclei involving atom 1, there is first the output of the
IFCs in cartesian coordinates, as well as their decomposition into an Ewald
and a short-range part, then, the analysis with respect to a local system of
coordinate. The latter is chosen such that it diagonalizes the IFC tensor, in
case of the self-force constant, and in the other cases, the first vector is
the vector joining the two nuclei, in order to decompose the IFC into a
longitudinal and a transverse component.

## 5 Computation of phonon band structures with efficient interpolation

You can copy the files trf2_5.abi and trf2_5.files from $ABI_TESTS/tutorespfn/Input to the Work_rf2 directory.
Then open *trf2_5.abi*.

{% dialog tests/tutorespfn/Input/trf2_5.files tests/tutorespfn/Input/trf2_5.abi %}

Note that [[anaddb:ifcflag]] is again activated.
Indeed, in order to compute a phonon band structure using the Fourier
interpolation, the IFCs are required. This is why the two first groups of
variables, needed to generate the IFCs are still defined. The third group of
variables is now restricted to [[anaddb:dipdip]] only.

Then, come the input variables needed to define the list of q wavevectors in the band structure:

* [[anaddb:eivec]]: flag to turn on the analysis of phonon eigenvectors
* [[anaddb:nph1l]]: number of q-points for phonon interpolation
* [[anaddb:qph1l]]: list of q-points for phonon interpolation
* [[anaddb:nph2l]]: number of q-directions for LO-TO correction
* [[anaddb:qph2l]]: list of q-directions for LO-TO correction

Now, you should issue:

    anaddb < trf2_5.files > trf2_5.log

It will last only a few seconds.

The file *trf2_5.abo* contains the list of eigenvalues, for all the needed
q-wavevectors. You can iopen it, and have a look at the different sections of
the file. Note that the interatomic force constants are computed (they are
needed for the Fourier interpolation), but not printed.

{% dialog tests/tutorespfn/Refs/trf2_5.abo %}

Please, open also the other output file, named *trf2_5_B2EPS.freq*.
It contains the frequencies, in a format suitable for graphical output, using the program
*band2eps* (the latter should be more documented, and will not be described in the present tutorial).

You can copy the files *trf2_6.abi* and *trf2_6.files* to the *Work_rf2* directory. Note that, like anaddb, band2eps use the old format using the files file. Then, issue

    band2eps < trf2_6.files > trf2_6.log

{% dialog tests/tutorespfn/Input/trf2_6.files tests/tutorespfn/Input/trf2_6.abi %}

The file *trf2_6.abo.eps* has been produced. It is an .eps file (eps stand for
Encapsulated PostScript). You can use the program ghostview to vizualize it.
The command to issue will depend on the way you have configured your machine,
but the following might perhaps do the work:

    gv trf2_6.abo.eps

You should see a nice phonon band structure for AlAs. Well, not so nice, after
all, because there are two strange dips for the highest phonon band, at the Gamma point.
This is due to the lack of LO-TO splitting for the ANADDB treatment of the first list of vector.
The correct phonon band structure is:

![](rf2_assets/trf2_6.abo.png)

You can correct the LO-TO splitting by the following little hack.

Open the file *trf2_5_B2EPS.freq*, and note that the value of the frequency, in
the sixth column, has a discontinuity exactly for the Gamma point (the three
first columns give the k point coordinates), that is, at lines 1 and 31:

     0.0000000000E+00  0.0000000000E+00  0.0000000000E+00  0.1568561346E-02  0.1568561346E-02  0.1568561346E-02

Replace these values (sixth column, line 1 and 31) by the correct value,
including the LO-TO splitting, that you can find in the file *trf2_5.abo*, at
the end, second list of vector. That is, the lines 1 and 31 should now read:

     0.000000E+00  0.000000E+00  0.000000E+00  1.568561E-03  1.568561E-03  1.730570E-03

Now, run *band2eps* again. Your phonon band structure should be perfect!

It can be compared with the AlAs phonon band structure published in [[cite:Giannozzi1991]].

Of course, one should make a convergence study, on the k and q point grids
(separately!), as well as on the energy cut-off, and also test LDA and GGA...
But this is left to the user! You can have a look at the paper [[cite:Petretto2018]]
for a careful analysis of phonon dispersion convergence with Abinit.

### Plotting phonon bands with AbiPy

If |AbiPy| is installed on your machine, you can use the |abiopen| script
with the `--expose` option to visualize the phonon band structure stored in the *PHBST.nc* file
produced by *anaddb*.
For instance:

```sh
abiopen.py trf2_5_PHBST.nc --expose --seaborn=talk
```

produces the following plot without LO-TO splitting:

![](rf2_assets/abiopen_trf2_5_PHBST.png)
:   (left) Phonon bands without LO-TO splitting
    (right) Plot with band connection estimated from the overlap of the eigenvectors at adjacent q-points

Alternatively, we can start from the DDB file and use the |abiview| script.
In this case, AbiPy will generate the anaddb input file
with all the variables required to handle the plotting of the LO-TO splitting,
invoke anaddb for us and finally plot the results.
All of this with just two lines:

```sh
# Copy the tutorial output file to have the correct file extension (DDB)
# otherwise abiview does not know how to handle our file.
cp trf2_3.ddb.abo trf2_3_DDB

abiview.py ddb trf2_3_DDB -sns=talk
```

![](rf2_assets/abiview_ddb_trf2_3_DDB.png)

We can also compare our results with the phonon band structure available on the |materials_project|.

First of all, let's find the materials project identifier associated to this particular phase of AlAs.
Of course, one could use the materials project web interface but we can also do it
from the shell by just passing our Abinit input file to the |abistruct| script:

```shell
abistruct.py mp_match trf2_1.abi

# Found 1 structures in Materials Project database (use `verbose` to get further info)

######################### abivars input for mp-2172 #########################
# Full Formula (Al1 As1)
# Reduced Formula: AlAs
# abc   :   4.054377   4.054377   4.054377
# angles:  60.000000  60.000000  60.000000
#
# Spglib space group info (magnetic symmetries are not taken into account).
# Spacegroup: F-43m (216), Hall: F -4 2 3, Abinit spg_number: None
# Crystal_system: cubic, Lattice_type: cubic, Point_group: -43m
#
#   Idx  Symbol    Reduced_Coords              Wyck      EqIdx
# -----  --------  --------------------------  ------  -------
#     0  Al        +0.00000 +0.00000 +0.00000  a             0
#     1  As        +0.25000 +0.25000 +0.25000  d             1

 natom 2
 ntypat 2
 typat 1 2
 znucl 13 33
 xred
    0.0000000000    0.0000000000    0.0000000000
    0.2500000000    0.2500000000    0.2500000000
 acell    1.0    1.0    1.0
 rprim
    6.6351943530    0.0000000000    3.8308312587
    2.2117314510    6.2557212277    3.8308312587
    0.0000000000    0.0000000000    7.6616624984
```

AbiPy found one entry in the MP database that matches the structure given in our input file
and has generated the corresponding input file.
Now we know that this phase of AlAs corresponds to `mp-2172` and we can
look at the phonon band structure computed by [[cite:Petretto2018a]] at
<https://materialsproject.org/materials/mp-2172/>

!!! tip

    For further information on the AbiPy API, please consult the |DdbFileNb| .
    To learn how to automate DFPT calculations with Python, see
    [this jupyter notebook](https://nbviewer.jupyter.org/github/abinit/abitutorials/blob/master/abitutorials/dfpt/lesson_dfpt.ipynb).


## 6 Thermodynamical properties

We will give only a very short example of the use of ANADDB to compute
thermodynamical properties. This is because this part of ANADDB is likely the
farthest from a clean, stable, usage. By exploring the input variables, the
user should be able to produce figures and data like the ones for SiO2 quartz
and stishovite, published in [[cite:Lee1995]].

You can copy the files *trf2_7.abi* from *\$ABI_TESTS/tutorespfn/Input* to *Work_rf2*
and have a look at them.
The same DDB as for trf2_4 and trf2_5 is used, namely *trf2_3.ddb.abo*.

{% dialog tests/tutorespfn/Input/trf2_7.files tests/tutorespfn/Input/trf2_7.abi %}

The following additional input variables are present:

  * [[anaddb:thmflag]]
  * [[anaddb:ng2qpt]]
  * [[anaddb:ngrids]]
  * [[anaddb:q2shft]]
  * [[anaddb:nchan]]
  * [[anaddb:nwchan]]
  * [[anaddb:thmtol]]
  * [[anaddb:ntemper]]
  * [[anaddb:temperinc]]
  * [[anaddb:tempermin]]

Examine the input file, the input variables, then run anaddb as usual.
Then, open the output file. You should be able to find the crucial section:

     # At  T     F(J/mol-c)     E(J/mol-c)     S(J/(mol-c.K)) C(J/(mol-c.K)) Omega_mean(cm-1)
     # (A mol-c is the abbreviation of a mole-cell, that is, the
     #  number of Avogadro times the atoms in a unit cell)
      2.000E+01  8.1406018E+03  8.1484316E+03  3.9149240E-01  1.4057917E+00  7.2615609E+01
      4.000E+01  8.1084535E+03  8.2384509E+03  3.2499352E+00  7.8730812E+00  9.4376064E+01
      6.000E+01  8.0007856E+03  8.4587550E+03  7.6328229E+00  1.3972961E+01  1.1313071E+02
      8.000E+01  7.8007781E+03  8.7924243E+03  1.2395577E+01  1.9312368E+01  1.3603493E+02
      1.000E+02  7.5044659E+03  9.2281056E+03  1.7236397E+01  2.4166544E+01  1.5713727E+02
      1.200E+02  7.1116611E+03  9.7549590E+03  2.2027483E+01  2.8405448E+01  1.7346361E+02
      1.400E+02  6.6242892E+03  1.0359674E+04  2.6681323E+01  3.1951237E+01  1.8536427E+02
      1.600E+02  6.0456925E+03  1.1028647E+04  3.1143464E+01  3.4844495E+01  1.9397760E+02
      1.800E+02  5.3800094E+03  1.1749746E+04  3.5387425E+01  3.7181668E+01  2.0029176E+02
      2.000E+02  4.6317003E+03  1.2512909E+04  3.9406045E+01  3.9067753E+01  2.0500949E+02


There, one finds, the phonon free energy, the phonon internal energy, the
phonon entropy and the phonon heat capacity.
The atomic temperature factors can also be computed.
An example is presented in [[test:v5_22]]

!!! important

    Do not forget that we are working in the harmonic approximation; beyond some
    temperature, anharmonic effects will have a sizeable contributions.
---
authors: T. Rangel
---

# Tutorial on the use of Wannier90 library

## The Wannier90 interface tutorial

This tutorial aims at showing how to use the Wannier90 interface to compute
Maximally Localized Wannier Functions (MLWFs).

You will learn how to get MLWFs with ABINIT and Wannier90 and what are the
basic variables to govern the numerical efficiency.

This tutorial should take about 1 hour and it is important to note that the examples in this tutorial
are not converged, they are just examples to show how to use the code.

[TUTORIAL_README]

## Summary of Wannier90 in ABINIT

Wannier90 is a code that computes MLWFs (see [www.wannier.org](http://www.wannier.org) ).
Wannier90 uses the methodology introduced by N. Marzari and D. Vanderbilt in 1997 and it is
highly recommended to read the following papers to understand its basics:
[[cite:Marzari1997]] and [[cite:Souza2002a]].

Wannier functions (WFs) can be obtained from Bloch states by means of the formulas 1-3 of [[cite:Souza2002a]].
As you may note there is a freedom of choice in the Bloch orbital's phase which is reflected
in the shape and the spatial extent of the WF.
This means that for different phases there will be WFs with different spatial localizations.

To obtain the MLWFs we minimize the spread of the WF with respect to the choice of phase.
This is done by using a steepest-descent algorithm, see section D of [[cite:Marzari1997]].
After a ground state calculation the Wannier90 code will obtain the MLWFs
requiring just two ingredients:

* The overlaps $M_{mn} = \langle u_{mk} | u_{nk+b} \rangle$ between the cell periodic part
  of the Bloch states $|u_{nk}\rangle$. See Eq. 25 of [[cite:Marzari1997]]).

* As a starting guess the projection, $A_{mn} = \langle \psi_{mk} | g_{n} \rangle$,
  of the Bloch states $|\psi_{nk} \rangle$ onto trial
  localized orbitals $|g_{n}\rangle$ (See section D of [[cite:Souza2002a]])

What ABINIT does is to take the Bloch functions from a ground state calculation
and compute these two ingredients. Then, Wannier90 is executed. Wannier90 is
included as a library in ABINIT and the process is automatic, so that in a
single run you can do both the ground state calculation and the computation of MLWFs.

## A first example: silicon

Before starting make sure that you compiled abinit enabling Wannier90.
You may have to recompile the code with

```
configure --with-config-file=myconf.ac9
```

where *myconf.ac9* defines:

```sh
#Install prefix of the PSML I/O library (e.g.
#                          /usr/local).
with_wannier90="/usr/local"

```

Now we will compute a set of MLWFs for silicon.
We are going to extract the Wannier functions corresponding to the four valence states of silicon.

*Before beginning, you might consider to work in a different sub-directory as
for the other tutorials. Why not Work_w90?*
Then copy the files *tw90_1.abi* and *wannier90.win* from
the *$ABI_TESTS/tutoplugs/Input* directory to *Work_w90*:

    cd $ABI_TESTS/tutoplugs/Input
    mkdir Work_w90
    cd Work_w90
    cp ../tw90_1.abi .

Wannier90 also uses a secondary input file called *wannier90.win*.
Therefore, you must include this file in the folder:

    cp ../wannier90.win .

Now you are ready to run abinit. Please type in:

    abinit  tw90_1.abi > log 2> err &

Let's examine the input file *tw90_1.abi*, while the calculation is running.

{% dialog tests/tutoplugs/Input/tw90_1.abi %}

The input file should look familiar to you. It is indeed the primitive cell of silicon.
It has two data sets: first a SCF calculation and then a NSCF calculation which
will call the Wannier90 library. The only new input variable is [[prtwant]]
which has to be set to 2 in order to use the Wannier90 utility.

Now lets look at the second input file *wannier90.win*.

{% dialog tests/tutoplugs/Input/wannier90.win %}

This is a mandatory input file required by the Wannier90 library.
There are many variables that can be defined inside this file.
In our case we used **num_wann** and **num_iter**.
These variables are used in the minimization of the spread
to obtain the MLWF. In particular, **num_wann** defines the number of Wannier
functions to extract while **num_iter** sets the maximum number of iterations. There
are also variables to govern the disentanglement procedure outlined in [[cite:Souza2002a]]
which are not used in this simple case. The complete list of input variables
can be found in the Wannier90 user guide (see [www.wannier.org](http://www.wannier.org)).

We can now examine the log file. After the convergence of the SCF cycle is
reached. We can see that the Wannier90 library is called. You will find the following lines:

      Calculation of overlap and call to Wannier90 library
      to obtain Maximally Localized Wannier functions
      - wannier90.win is a mandatory secondary input
      - wannier90.wout is the output for the library
      - wannier90.amn contains projections
      - wannier90random.amn contains random projections
      - wannier90.mmn contains the overlap
      - wannier90.eig contains the eigenvalues

This is an explanation of the input and output files for the Wannier90
library. As you can see many new files were created. The input files for
Wannier90 which were created by ABINIT are:

**wannier90random.amn**
:   Contains a list of projections to be used as a starting guess of the WF.
    This is the $A_{mn}$ matrix which was mentioned before in this tutorial.

**wannier90.eig**
:   Contains a list of eigenvalues for each k-point and band.

**wannier90.mmn**
:   Contains the overlaps between the cell periodic part of the Bloch states.
    This is the M_mn matrix mentioned before in this tutorial.

**UNK**
:   Files containing the wavefunction in real space for every k-point.
    Once these files were computed by ABINIT the Wannier90 library was used.
    The output files of Wannier90 are:

**wannier90.wout**
:   This is the main output file of the library.
    You should read it carefully to see the details of the calculation.

**wannier90.chk**
:   This file is required to restart a calculation is case you use Wannier90 in standalone mode.
    In our case it is not used.

To obtain information about the steepest-descent minimization just issue:

    grep CONV tw90_1o_DS2_w90.wout

You will obtain a table of the following form:

     +--------------------------------------------------------------------+<-- CONV
     | Iter  Delta Spread     RMS Gradient      Spread (Ang^2)      Time  |<-- CONV
     +--------------------------------------------------------------------+<-- CONV
          0     0.438E+02     0.0000000000       43.7939618280       0.08  <-- CONV
          1    -0.946E+01    10.5484513508       34.3387915333       0.09  <-- CONV


You can verify that the final spread you get is around 4.0 Å$^2$.

Similarly to obtain information about the disentanglement procedure (not used in this example)
just type:

    grep DIS wannier90.wout

You will obtain a table of the following form:

     +---------------------------------------------------------------------+<-- DIS
     |  Iter     Omega_I(i-1)      Omega_I(i)      Delta (frac.)    Time   |<-- DIS
     +---------------------------------------------------------------------+<-- DIS

!!! tip

    If |AbiPy| is installed on your machine, you can use the |abiopen| script
    with the `wout` command and the `--expose` option to visualize the iterations

        abiopen.py tw90_1o_DS2_w90.wout --expose -sns

    ![](wannier90_assets/abiopen_tw90_1o_DS2_w90.png)


### Visualize the Wannier functions

You can see the Wannier functions in |xcrysden| format. Just type:

    xcrysden --xsf tw90_1o_DS2_w90_00001.xsf

To see the isosurface click on: Tools->Data Grid -> OK
And modify the isovalue, put, for instance, 0.3 and check the option "Render +/- isovalue" then click on OK
Alternatively, one can read the xsf file with |vesta|.
MacOsx users can use the command line:

    open tw90_1o_DS2_w90_00003.xsf -a vesta

to invoke vesta directly from the terminal:

![](wannier90_assets/tw90_1o_DS2_w90_00001.png)


!!! important

    * It is important to set [[istwfk]] equal to 1 for every k-point avoiding using symmetries.
      The reason is that the formalism used to extract the MLWF assumes that you have a uniform grid of k-points.
      See section IV of [[cite:Marzari1997]].

    * The number of Wannier functions to extract should be minor or equal to the number of bands.
      If _nband > nwan_ then the disentanglement routines will be called.

    * The number of k-points should be equal to ngkpt(1)*ngkpt(2)*ngkpt(3).
      This is achieved by using the input variables [[kptopt]]= 3, [[ngkpt]] and [[nshiftk]]= 1.

    * The prefix of all wannier90 files in this sample case is _wannier90_.
      Other possible prefixes are w90_ and **abo** __w90_ , where **abo** is the fourth line on your .file file.
      To setup the prefix, ABINIT will first look for a file named **abo**
      __w90.win_ if it is not found then it will look for _w90.win_ and finally for _wannier90.win_.

## The PAW case

Before starting it is assumed that you have already completed the tutorials [PAW1](/tutorial/paw1) and [PAW2](/tutorial/paw2).

For silicon, we just have to add the variable [[pawecutdg]] and the PAW Atomic Data is included in the pseudopotential file.
An example has already been prepared.

Just copy the file  *tw90_2.abi* into *Work_w90*:

    cp ../tw90_2.abi .

We are going to reuse the wannier90.win of the previous example.
Now, just run abinit again

    abinit tw90_2.abi > log 2> err &

As it is expected, the results should be similar than those of the PW case.

!!! important

    For the PAW case the UNK files are not those of the real
    wavefunctions. The contribution inside the spheres is not computed, however,
    they can be used to plot the Wannier functions.

## Defining the initial projections

Up to now we have obtained the MLWF for the four valence bands of silicon. It
is important to note that for valence states the MLWF can be obtained starting
from a random initial position. However, for conduction states we have to give
a very accurate starting guess to get the MLWF.

We are going to extract the $sp^3$ hybrid orbitals of Silane SiH$_4$. You can start
by copying from the tests/tutoplugs directory the following files:

    cp ../tw90_3.abi .
    cp ../tw90_3o_DS2_w90.win .

Now run abinit

    abinit  tw90_3.abi > log 2> err &

While it is running, we can start to examine the input files.
Open the main input file *tw90_3.abi*. The file is divided into three datasets.

{% dialog tests/tutoplugs/Input/tw90_3.abi %}

First a SCF calculation is done. What follows is a NSCF calculation including
more bands. Finally, in the third dataset we just read the wavefunction from
the previous one and the Wannier90 library is called. **w90iniprj** is a keyword
used to indicate that the initial projections will be given in the **.win** file.

!!! note

    You may notice that the **.win** file is now called *tw90_3o_DS2_w90.win*.
    It has the following form: *prefix_dataset_w90.win*, where the prefix is taken from
    the third line of your .file file. and dataset is the dataset number
    at which you call Wannier90 (dataset 2 in this example).

Now open the **.win** file. The initial projections will be the $sp^3$ hybrid
orbitals centered in the position of the silicon atom.
This is written explicitly as:

    begin projections
    Si:sp3
    end projections

There is an enormous freedom of choice for the initial projections. For
instance, you can define the centers in Cartesian coordinates or rotate the
axis. Please refer to the Wannier90 user guide and see the part related to
projections (see [www.wannier90.org](http://www.wannier.org)).

## More on Wannier90 + ABINIT

Now we will redo the silicon case but defining different initial projections.

This calculation will be more time consuming, so you can start by running the
calculation while reading:

    cp ../tw90_4.abi .
    cp ../tw90_4o_DS3_w90.win .
    abinit  tw90_4.abi > log 2> err &

**Initial projections:**

In this example we extract sp$^3$ orbitals centered on the silicon atoms. But you
could also extract bonding and anti-bonding orbitals by uncommenting and
commenting the required lines as it is indicated in *tw90_4o_DS3_w90.win*.

You can see that we are using **r=4** in the initial projections block. This
is to indicate that the radial part will be a Gaussian function whose width
can be controlled by the value of the variable **zona**. The main advantage over
radial functions in the form of hydrogenic orbitals is that the time to write
the .amn file will be reduced.

**Interpolated band structure**

We are going to run Wannier90 in standalone mode. Just comment out the following
lines of the **.win** file:

    postproc_setup = .true.   !used to write .nnkp file at first run
    num_iter = 100

And uncomment the following two lines:

    !restart = plot
    !bands_plot = .true.

Now run Wannier90:

    $ABI_HOME/fallbacks/exports/bin/wannier90.x-abinit tw90_4o_DS3_w90

The interpolated band structure is in *tw90_4o_DS3_w90_band.dat*

To plot the bands, open |gnuplot| in the terminal and type:

```gnuplot
load "tw90_4o_DS3_w90_band.gnu"
```

![](wannier90_assets/load_tw90_4o_DS3_w90_band.png)

---
authors: MG
---

# Superconducting properties within the isotropic Eliashberg formalism

This tutorial explains how to compute the phonon linewidths induced by the electron-phonon (e-ph) interaction
in metallic systems and how to use the Eliashberg spectral function $\alpha^2F$ and the McMillan equation
to estimate the superconducting critical temperature $T_c$ within the **isotropic Eliashberg formalism**.

We start by presenting the basic equations implemented in the code and their connection with the ABINIT variables.
Then we discuss how to run isotropic $T_c$-calculations and how to perform typical convergence studies
for MgB$_2$, a well-known phonon-mediated superconductor with $T_c$ = 39 K.
For a more complete theoretical introduction, see [[cite:Giustino2017]] and references therein.

It is assumed the user has already completed the two tutorials [RF1](/tutorial/rf1) and [RF2](/tutorial/rf2),
and that he/she is familiar with the calculation of ground state and vibrational properties **in metals**.
The user should have read the [fourth lesson on Al](/tutorial/base4) as well
as the [introduction page for the EPH code](tutorial/eph_intro) before running these examples.

This lesson should take about 1.5 hour.

## Formalism and connection with the implementation

Due to the interaction with electrons, vibrational energies get renormalized and phonons
acquire a **finite lifetime**.
These many-body effects are described by the phonon-electron self-energy:

$$
\Pi_{\qq\nu}(\ww, T) = \dfrac{2}{N_\kk} \sum_{mn\kk} |g_{mn\nu}(\kk, \qq)|^2
\dfrac{f_\nk - f_{m\kq}}{\ee_{\nk} - \ee_{m\kq} -\ww - i\eta^+}
$$

where only contributions up to the second order in the e-ph vertex $g$ have been included.
The sum over the electron wavevector $\kk$ is performed over the first BZ, $\eta$ is a positive real infinitesimal
and $\gkkp$ are the e-ph matrix element discussed in the [EPH introduction](eph_intro).
The self-energy depends on the temperature via the Fermi-Dirac distribution function $f(\ee, T)$
and the factor two accounts for spin degeneracy (henceforth we assume a non-magnetic system
with scalar wavefunctions i.e. [[nsppol]] == 1 and [[nspinor]] == 1
thus the electron energies $\ee$ and the e-ph matrix elements $g$ do not depend on the spin).

The real part of $\Pi_\qnu$ gives the correction to the vibrational energies due to e-ph interaction
while the **phonon linewidth** $\gamma_{\qq\nu}$ (full width at half maximum) is twice
the imaginary part $\Pi^{''}_\qnu$ evaluated at the "bare" phonon frequency $\ww_\qnu$ as computed with DFPT:

$$
\gamma_{\qq\nu}(T) = 2\, \Pi^{''}_\qnu(\ww=\ww_\qnu, T).
$$

Using the [Sokhotski–Plemelj theorem](https://en.wikipedia.org/wiki/Sokhotski%E2%80%93Plemelj_theorem):

$$
\lim_{\eta \rightarrow 0^+} \dfrac{1}{x \pm i\eta} = \mp i\pi\delta(x) + \mathcal{P}(\dfrac{1}{x})
$$

the imaginary part of $\Pi$ can we rewritten as:

$$
\Pi^{''}_{\qq\nu}(\ww_\qnu, T) = \dfrac{2\pi}{N_\kk} \sum_{mn\kk} |g_{mn\nu}(\kk, \qq)|^2
(f_\nk - f_{m\kq})\,\delta(\ee_{\kk n} - \ee_{m\kpq} -\ww_\qnu)
$$

where the delta function enforces energy conservation in the scattering process.
Because phonon energies are typically small compared to electronic energies,
the energy difference $\ee_{\kk n} - \ee_{m\kpq}$ is also small hence it is possible to
approximate the difference in the occupation factors with:

$$
f_\nk - f_{m\kq} \approx f'(\ee_\nk) (\ee_{\kk n} - \ee_{m\kpq}) = -f'(\ee_\nk) \ww_\qnu
$$

At low T, the derivative of the Fermi-Dirac occupation function is strongly peaked around
the Fermi level thus we can approximate it with:

$$ f'(\ee_\nk) \approx -\delta(\ee_\nk - \ee_F) $$

By neglecting $\ww_\qnu$ in the argument of the delta, we obtain the
so-called **double-delta approximation** (DDA) for the phonon linewidth [[cite:Allen1972]]:

\begin{equation}\label{eq:DDA_gamma}
\gamma_{\qq\nu} = \dfrac{4\pi \ww_{\qq\nu}}{N_\kk} \sum_{mn\kk}
|g_{mn\nu}(\kk, \qq)|^2 \delta(\ee_{\kpq m} -\ee_F) \delta(\ee_{\kk n} -\ee_F)
\end{equation}

For a given phonon wavevector $\qq$, the DDA restricts the BZ integration to
electronic transitions between $\kk$ and $\kq$ states on the Fermi surface (FS)
hence very dense $\kk$-meshes are needed to converge $\gamma_{\qq\nu}$.
The convergence rate is expected to be slower than that required by the electron DOS per spin at $\ee_F$:

$$
g(\ee_F) = \dfrac{1}{N_\kk} \sum_{n\kk} \delta(\ee_{n\kk} - \ee_F)
$$

in which a single Dirac delta is involved.

!!! note

    The DDA breaks down at small $\qq$ where the phonon frequency cannot be neglected.
    Note also that the DDA predicts the linewidths to not depend on T.

    It is also worth stressing that there is another important contribution to the phonon lifetimes induced by
    **non-harmonic** terms in the Taylor expansion of the Born-Oppenheimer energy surface around the equilibrium point.
    Within the framework of many-body perturbation theory, these non-harmonic terms lead
    to **phonon-phonon scattering processes** that can give a substantial contribution to the lifetimes,
    especially at "high" T.
    In the rest of the tutorial, however, non-harmonic terms will be ignored and we will be mainly focusing
    on the computation of the imaginary part of the phonon self-energy $\Pi$ in the harmonic approximation.


At the level of the implementation, the [[eph_intmeth]] input variable
selects the technique for integrating the double delta over the FS:
[[eph_intmeth]] == 2 (default) activates the **optimized tetrahedron** scheme [[cite:Kawamura2014]]
as implemented in the [libtetrabz library](http://libtetrabz.osdn.jp/en/_build/html/index.html)
while [[eph_intmeth]] == 1 replaces the Dirac distribution with a **Gaussian** function of finite width.
In the later case, one can choose between a constant broadening $\sigma$
specified in Hartree by [[eph_fsmear]] or an adaptive Gaussian scheme (activated if [[eph_fsmear]] < 0)
in which a state-dependent $\sigma_\nk$ is automatically computed from
the electron group velocities $v_\nk$ [[cite:Li2015]].
Note that the adaptive Gaussian scheme is internally used by the code also when the optimized tetrahedron method
is employed since the DDA is ill-defined for $\qq = \Gamma$.

<!--

!!! important

    The tetrahedron method is more accurate and does not require any broadening parameter.
    Note, however, that in the present implementation the computational cost of the double
    delta with the tetrahedron method quickly increases with the size of the $\kk$-mesh
    so the adaptive Gaussian scheme may represent a valid alternative, especially
    when dense $\kk$-meshes are needed.
-->

The value of the Fermi level $\ee_F$ is automatically computed from the KS eigenvalues stored
in the input WFK file according to the two input variables [[occopt]] and [[tsmear]].
These variables are **usually equal** to the ones used for the GS/DFPT calculations
although it is possible to change the value of $\ee_F$ during an EPH calculation using
three (mutually exclusive) input variables: [[eph_fermie]], [[eph_extrael]] and [[eph_doping]].
This may be useful if one wants to study the effect of doping within the **rigid band approximation**.

<!--
As concerns the computation of the electron DOS, we have [[prtdos]] [[dosdeltae]] [[tsmear]]
-->

The sum over bands in Eq.\ref{eq:DDA_gamma} is restricted to the Bloch states within a
symmetric energy window of thickness [[eph_fsewin]] around $\ee_F$.
This value should be chosen according to the FS integration scheme [[eph_intmeth]] and the value of
[[eph_fsmear]] (if standard Gaussian).
Additional details are given in the tutorial.
The $\kk$-mesh for electrons is defined by the input variables [[ngkpt]], [[nshiftk]] and [[shiftk]].
These parameters **must be equal** to the ones used to generate the input WFK file passed to the EPH code.

The code computes $\gamma_{\qq\nu}$ for all the $\qq$-point in the IBZ associated
to the [[eph_ngqpt_fine]] $\qq$-mesh and the DFPT potentials are interpolated starting
from the *ab-initio* [[ddb_ngqpt]] $\qq$-mesh associated to the input DVDB/DDB files.
If [[eph_ngqpt_fine]] is not specified, [[eph_ngqpt_fine]] == [[ddb_ngqpt]] is assumed
and no interpolation of the scattering potentials in $\qq$-space is performed.

Once the phonon linewidths $\gamma_{\qq\nu}$ are known in the IBZ, ABINIT computes
the isotropic Eliashberg function defined by:

$$
\alpha^2F(\ww) =
\dfrac{1}{N_\qq\, N_F} \sum_{\qq\nu} \dfrac{\gamma_{\qq\nu}}{\ww_{\qq\nu}} \delta(\ww - \ww_{\qq \nu})
$$

where $N_F$ is the density of states (DOS) per spin at the Fermi level.
The Eliashberg function can be equivalently expressed as:

$$
\alpha^2F(\ww) =
\dfrac{1}{N_\qq N_\kk\, N_F} \sum_{\kk\qq mn \nu}
|g_{mn\nu}(\kk, \qq)|^2 \delta(\ee_{\kpq m} -\ee_F) \delta(\ee_{\kk n} -\ee_F) \delta(\ww - \ww_{\qq \nu}).
$$

From a physical perspective, $\alpha^2F(\ww)$ gives the strength by which a phonon
of energy $\hbar\ww$ scatters electronic states on the FS (remember that ABINIT uses atomic units hence $\hbar = 1$).
This quantity is accessible in experiments and experience has shown that
$\alpha^2F(\ww)$ is qualitatively similar to the phonon DOS $F(\ww)$:

$$
F(\ww) = \dfrac{1}{N_\qq} \sum_{\qq\nu} \delta(\ww - \ww_{\qq \nu}).
$$

This is not surprising as the equation for $\alpha^2F(\ww)$ resembles the one for $F(\ww)$,
except for the weighting factor $\frac{\gamma_{\qq\nu}}{\ww_{\qq\nu}}$.
This also explains the $\alpha^2F$ notation: the Eliashberg function can be seen
as the phonon DOS times the positive frequency-dependent prefactor $\alpha^2(\ww)$.

!!! warning

    Converging $\alpha^2F(\ww)$ usually requires very fine $\kk$- and $\qq$-meshes.
    This is especially true if the FS significantly deviates from the free electron picture (spherical FS)
    and/or the e-ph coupling is strongly anisotropic as in MgB$_2$.
    The $\kk$-mesh affects the quality of the individual $\gamma_\qnu$ linewidths
    whereas dense $\qq$-meshes may be needed to resolve the fine detauls and/or
    sample regions in $\qq$-space where the coupling is strong.

The technique used to compute $\alpha^2F(\ww)$ is defined by [[ph_intmeth]] (note the **ph_** prefix instead of **eph_**).
Both the Gaussian ([[ph_intmeth]] = 1 with [[ph_smear]] smearing) and
the linear tetrahedron method by [[cite:Blochl1994]] ([[ph_intmeth]] = 2, default) are implemented.
The $\alpha^2F(\ww)$ function is evaluated on a linear mesh of step [[ph_wstep]] covering the entire
range of phonon frequencies.
The total e-ph coupling strength $\lambda$ (a dimensionless measure of the average strength of the e-ph coupling)
is defined as the first inverse moment of $\alpha^2F(\ww)$:

$$
\lambda = \int \dfrac{\alpha^2F(\ww)}{\ww}\dd\ww = \sum_{\qq\nu} \lambda_{\qq\nu}
$$

where we have introduced the mode-dependent contributions:

$$
\lambda_{\qq\nu} = \dfrac{\gamma_{\qq\nu}}{\pi N_F \ww_{\qq\nu}^2}
$$

!!! warning

    Due to the $1/\omega$ factor, "low-energy" modes are expected to contribute more to $\lambda$
    than "high-energy modes".
    On the other hand, $\alpha^2F(\ww)$ should go to zero as $\ww^2$ for $\ww \rightarrow 0$
    so that the integrand function is finite at the acoustic limit.
    Obviously, if the system is not dynamically stable ("negative" frequencies in the BZ)
    $\lambda$ will explode but this does not mean you have found a room-temperature superconductor!

In principle, $T_c$ can be obtained by solving the isotropic Eliashberg equations for the superconducting
gap (see e.g. [[cite:Margine2013]]) but many applications prefer to bypass the explicit solution
and **estimate** $T_c$ using the semi-empirical McMillan equation [[cite:McMillan1968]]
in the improved version proposed by [[cite:Allen1975]]:

$$
T_c = \dfrac{\ww_{log}}{1.2} \exp \Biggl [ \dfrac{-1.04 (1 + \lambda)}{\lambda ( 1 - 0.62 \mu^*) - \mu^*} \Biggr ]
$$

where $\mu^*$ describes the screened electron-electron interaction and
$\ww_{\text{log}}$ is the *logarithmic* average of the phonon frequencies defined by:

$$
\ww_{\text{log}} = \exp \Biggl [ \dfrac{2}{\lambda} \int \dfrac{\alpha^2F(\ww)}{\ww}\log(\ww)\dd\ww \Biggr ]
$$

In pratical applications, $\mu^*$ is treated as an **external parameter**,
typically between 0.1 and 0.2, that is adjusted to reproduce experimental results.
The default value of [[eph_mustar]] is 0.1.

Before concluding this brief theoretical introduction, we would like to
stress that the present formalism assumes a **single superconducting gap**
with a **weak dependence** on $\kk$ so that it is possible to average the anisotropic equations over the FS.
This approximation becomes valid in the so-called dirty-limit (metals with impurities)
as impurities tend to smear out the anisotropy of the superconducting gap.
A more detailed discussion about isotropic/anisotropic formulations can be found in [[cite:Margine2013]].

<!--
TODO
Transport properties in the normal metallic state
Allen obtained an approximation relating the conductivity of metals
to the transport spectral function α2Ftr, which is a variant of the Eliashberg spectral function α2F.
lowest-order variational approximation (LOVA)
[[cite:Allen1976b]], [[cite:Allen1978]]
[[eph_transport]]
-->


## Getting started

[TUTORIAL_README]

Before beginning, you might consider to work in a different subdirectory as for the other tutorials.
Why not create *Work_eph4isotc* in $ABI_TESTS/tutorespfn/Input?

```sh
cd $ABI_TESTS/tutorespfn/Input
mkdir Work_eph4isotc
cd Work_eph4isotc
```

In this lesson we prefer to focus on e-ph calculations and the associated convergence studies.
For this reason, we rely on **pre-computed** DEN.nc, DDB and DFPT potentials
to bypass both the GS and the DFPT part.
The DEN.nc file will be used to perform NSCF computations on **arbitrarily dense** $\kk$-meshes while the
DFPT POT.nc files will be merged with the *mrgdv* utility to produce the DVDB database required by the EPH code.

Note that these files **are not shipped** with the official ABINIT tarball as they are relatively large.
In order to run the examples of this tutorial, you need to download the files from
[this github repository](https://github.com/abinit/MgB2_eph4isotc).
If git is installed on your machine, you can easily fetch the entire repository with:

```sh
git clone https://github.com/abinit/MgB2_eph4isotc.git
```

Alternatively, use *wget*:

```sh
wget https://github.com/abinit/MgB2_eph4isotc/archive/master.zip
```

or *curl*:

```sh
curl -L https://github.com/abinit/MgB2_eph4isotc/archive/master.zip -o master.zip
```

or simply copy the tarball by clicking the "download button" in the github web page,
then unzip the archive and rename the directory with:

```sh
unzip master.zip
mv MgB2_eph4isotc-master MgB2_eph4isotc
```

!!! warning

    The directory with the precomputed files **must be located inside _Work_eph4isotc_**
    and must be named `MgB2_eph4isotc`.
    The reason is that all the input files and examples of this tutorial read data from external files
    specified in terms of **relative paths**.


The |AbiPy| script used to perform the GS + DFPT steps is available
[here](https://github.com/abinit/MgB2_eph4isotc/blob/main/run_mgb2_phonons.py).
In order to facilitate comparison with previous studies,
we use norm-conserving pseudopotentials with a cutoff energy [[ecut]] of 38 Ha
and the experimental parameters for hexagonal MgB$_2$ (a = 5.8317 and c/a= 1.1416).
All the calculations are performed with a 12x12x12 [[ngkpt]] Gamma-centered $\kk$-grid for electrons (**too coarse**),
and the Marzari smearing ([[occopt]] = 4) with [[tsmear]] = 0.02 Ha.
The DFPT computations is done for 12 irreducible $\qq$-points corresponding
to a $\Gamma$-centered 4x4x4 $\qq$-mesh (again, **too coarse** as we will see in the next sections).

Please keep in mind that several parameters have been tuned in order to reach a reasonable **compromise between accuracy
and computational cost** so do not expect the results obtained at the end of the lesson to be fully converged.
It is clear that, in real life, one should start from convergence studies for
lattice parameters and vibrational properties as a function of the $\kk$-mesh and [[tsmear]]
before embarking on EPH calculations.

## Merging the DFPT potentials

To merge the DFPT potential files, copy the first input file in *Work_eph4isotc* with:

```sh
cp $ABI_TESTS/tutorespfn/Input/teph4isotc_1.abi .
```

and execute the *mrgdv* tool using:

```sh
mrgdv < teph4isotc_1.abi
```

The first line in *teph4isotc_1.abi* specifies the name of the output DVDB file, followed by the
number of partial DFPT POT files and the full list of files we want to merge:

{% dialog tests/tutorespfn/Input/teph4isotc_1.abi %}

This step produces the **teph4isotc_1_DVDB** file that will be used in the next examples.
Executing:

```sh
mrgdv info teph4isotc_1_DVDB
```

shows that all the independent atomic perturbations are available:

```md
 The list of irreducible perturbations for this q vector is:
    1)  idir= 1, ipert=   1, type=independent, found=Yes
    2)  idir= 2, ipert=   1, type=symmetric, found=No
    3)  idir= 3, ipert=   1, type=independent, found=Yes
    4)  idir= 1, ipert=   2, type=independent, found=Yes
    5)  idir= 2, ipert=   2, type=symmetric, found=No
    6)  idir= 3, ipert=   2, type=independent, found=Yes
    7)  idir= 1, ipert=   3, type=symmetric, found=No
    8)  idir= 2, ipert=   3, type=symmetric, found=No
    9)  idir= 3, ipert=   3, type=symmetric, found=No

 All the independent perturbations are available
 Done
```

## Analyzing electronic and vibrational properties

Before proceeding with e-ph calculations, it is worth spending some time to analyze
the structural, electronic and vibrational properties of MgB$_2$ in more detail.
We will be using the |AbiPy| scripts to post-process the data stored in the precomputed netcdf files.

To visualize the crystalline structure with e.g. |vesta|,
use the |abiview| script and the `structure` command.

```md
abiview.py structure MgB2_eph4isotc/flow_mgb2_phonons/w0/t0/run.abi -a vesta

Full Formula (Mg1 B2)
Reduced Formula: MgB2
abc   :   3.086000   3.086000   3.523000
angles:  90.000000  90.000000 120.000000
Sites (3)
  #  SP           a         b    c
---  ----  --------  --------  ---
  0  Mg    0         0         0
  1  B     0.333333  0.666667  0.5
  2  B     0.666667  0.333333  0.5

Visualizing structure with: vesta
Writing data to: .xsf with fmt: xsf
Executing MacOSx open command: open -a vesta --args   /Users/gmatteo/git_repos/abinit_rmms/_build/tests/.xsf
```

!!! tip

    Other graphical applications are supported, see `abiview.py structure --help`.
    Note that AbiPy assumes that these tools are **already installed and the binaries
    can be found in $PATH**.
    The same AbiPy command can be used with ABINIT input files as well as output files
    in netcdf format e.g. GSR.nc.

MgB$_2$ crystallizes in the so-called
[AlB$_2$ prototype structure](http://aflow.org/prototype-encyclopedia/AB2_hP3_191_a_d)
with Boron atoms forming graphite-like (honeycomb) layers separated by layers of Mg atoms.
This structure may be regarded as that of completely intercalated graphite with C replaced by B.
Since MgB$_2$ is formally **isoelectronic to graphite**,
its band dispersion is expected to show some similarity to that
of graphite and graphite intercalation compounds.

![](eph4isotc_assets/mgb2_structure.png)

Now use the |abiopen| script to plot the electronic bands stored in the GSR file produced
by the NSCF calculation (the second task of the first Work i.e. *w0/t1*):

```sh
abiopen.py MgB2_eph4isotc/flow_mgb2_phonons/w0/t1/outdata/out_GSR.nc -e
```

This command produces the following figures:

![](eph4isotc_assets/MgB2_ebands.png)

The electronic properties of MgB$_2$ are intensively discussed in literature, see e.g. [[cite:Mazin2003]].
The dispersion is quite similar to that of graphite with three bonding $\sigma$
bands corresponding to in-plane $sp^2$ hybridization in the boron layer and two $\pi$ bands
(bonding $\pi$ and anti-bonding $\pi^*$) formed by B $p_z$ orbitals.
The lowest band in the band plot is a fully occupied $\sigma$ band.
The other two $\sigma$ bands are incompletely filled and correspond to
the relatively flat states located slightly above $\ee_F$ along the $\Gamma-A$ segment.
Ab-initio calculations showed that these states contribute the most to the e-ph coupling.
<!-- due to a strong coupling with the $E_{2g}$ vibrational modes. -->
The other two bands crossing at the K point at around 1eV are $\pi-\pi^*$ bands.

To compute the so-called fatbands, one can use perforn a NSCF calculation with a $\kk$-path and [[prtdos]] 3.
This is left as optional exercise.
Fatbands plots produced with AbiPy starting from the *FATBANDS.nc* file are available
[here](https://abinit.github.io/abipy/gallery/plot_efatbands.html).

!!! tip

    The high-symmetry $\kk$-path has been automatically computed by AbiPy when generating the Flow.
    To get the explicit list of high-symmetry $\kk$-point, use the |abistruct| script
    with the `kpath` command:

    ```sh
    abistruct.py kpath mgb2_DEN.nc

    ...

    # K-path in reduced coordinates:
     ndivsm 10
     kptopt -11
     kptbounds
        +0.00000  +0.00000  +0.00000  # $\Gamma$
        +0.50000  +0.00000  +0.00000  # M
        +0.33333  +0.33333  +0.00000  # K
        +0.00000  +0.00000  +0.00000  # $\Gamma$
        +0.00000  +0.00000  +0.50000  # A
        +0.50000  +0.00000  +0.50000  # L
        +0.33333  +0.33333  +0.50000  # H
        +0.00000  +0.00000  +0.50000  # A
        +0.50000  +0.00000  +0.50000  # L
        +0.50000  +0.00000  +0.00000  # M
        +0.33333  +0.33333  +0.00000  # K
        +0.33333  +0.33333  +0.50000  # H
    ```

The value of the DOS at the Fermi level plays a very important role when discussing superconducting properties.
Actually this is one of the quantities that should be subject to convergence studies with respect to the $\kk$-grid
before embarking on DFPT/EPH calculations.
To compute the DOS with the tetrahedron method, use the `ebands_edos` command of *abitk*

```sh
abitk ebands_edos MgB2_eph4isotc/flow_mgb2_phonons/w0/t0/outdata/out_GSR.nc
```

to read the eigenvalues in the IBZ from a `GSR.n` file (`WFK.nc` files are supported as well).
This command produces a text file named `out_GSR.nc_EDOS` that can be visualized with:

```sh
abiopen.py out_GSR.nc_EDOS -e
```

![](eph4isotc_assets/MgB2_matplotlib_fs.png)

<!--
Nesting factor
Yet visualizing the FS is rather useful when discussing e-ph properties in metals.

\begin{equation}
    N(\qq) = \sum_{mn\kk} \delta(\ee_{\kpq m}) \delta(\ee_{\kk n})
\end{equation}
-->

Note that the phonon linewidths have a *geometrical contribution* due to Fermi surface since $\gamma_\qnu$
is expected to be large in correspondence of $\qq$ wave vectors connecting two portions of the FS.
Strictly speaking this is true only if the e-ph matrix elements are constant.
In real materials the amplitude of $g_{mn\nu}(\kk, \qq)$ is not constant
and this may enhance/suppress the value $\gamma_\qnu$ for particular modes.
Yet visualizing the FS is rather useful when discussing e-ph properties in metals.
For this reason, it is useful to have a look at the FS with an external

Graphical tools for the visualization of the FS usually require an external file with
electronic energies in the full BZ whereas *ab-initio* codes usually take advantage of symmetries
to compute $\ee_\nk$ in the IBZ only.
To produce a BXSF that can be used to visualize the FS with e.g. |xcrysden|,
use the `ebands_bxsf` command of the *abitk* utility located in *src/98_main*
and provide a *GSR.nc* (*WFK.nc*) file with energies computed on a $\kk$-mesh in the IBZ:

```sh
abitk ebands_bxsf MgB2_eph4isotc/flow_mgb2_phonons/w0/t0/outdata/out_GSR.nc
```

This command reconstructs the KS energy in the full BZ by symmetry and produces
the `out_GSR.nc_BXSF` file that can be opened with |xcrysden| using:

```sh
xcrysden --bxsf out_GSR.nc_BXSF
```

![](eph4isotc_assets/MgB2_matplotlib_fs.png)

Other tools such as |fermisurfer| or |pyprocar| can read data in the BXSF format as well.
For a minimalistic matplotlib-based approach, use can use the |abiview| script with the **fs** command:

```sh
abiview.py fs MgB2_eph4isotc/flow_mgb2_phonons/w0/t0/outdata/out_GSR.nc
```

to visualize the FS in the **unit cell** of the reciprocal lattice.


![](eph4isotc_assets/MgB2_matplotlib_fs.png)

!!! tip

    The BXSF file can be produced at the end of the GS calculation
    or inside the EPH code by setting [[prtfsurf]] to 1 but *abitk* is quite handy if you already
    have a file and you don't want to write a full Abinit input file and rerun the calculation
    from scratch.


We now focus on the **vibrational properties** of MgB$_2$.
In principle, one can compute phonon frequencies either with *anaddb* or with the EPH code.
However, for many applications, it is much easier
to automate the entire process by invoking *anaddb* directly from the |abiview| script.
To compute the phonon band structure using the DDB file produced on the 4x4x4 $\qq$-mesh, use:

```sh
abiview.py ddb MgB2_eph4isotc/flow_mgb2_phonons/w1/outdata/out_DDB
```

that produces the following figures:

![](eph4isotc_assets/MgB2_phbands.png)

The Projected Phonon DOS (right panel) shows that the low-energy vibrations (below 40 meV) are mainly of Mg character
whereas the higher energy modes mainly involve motions of B atoms.
Of particular interest for us is the two-fold degenerate band located at ~ 60 meV along the $\Gamma-A$ segment.
These are the so-called $E_{2g}$ vibrational modes that couple strongly with the two $\sigma$ bands crossing $\ee_F$.

!!! tip

    A more detailed analysis reveals that $E_{2g}$ involve in-plane stretching of the B-B bonds.
    To visualize the phonon modes, one can use the |phononwebsite| web app by H. Miranda.

    ```sh
    abiview.py ddb MgB2_eph4isotc/flow_mgb2_phonons/w1/outdata/out_DDB

    Opening URL: http://henriquemiranda.github.io/phononwebsite/phonon.html?json=http://localhost:8000/__abinb_workdir__/Mg1B2136lmnep.json
    Using default browser, if the webpage is not displayed correctly
    try to change browser either via command line options or directly in the shell with e.g:

         export BROWSER=firefox

    Press Ctrl+C to terminate the HTTP server

    If the page does not work, upload the json file directly.
    ```

<!--
In metals, the interatomic force-constants are short-ranged yet this does not guarantee that
a Fourier interpolation done starting from a 4x4x4 $\qq$-mesh can capture all the fine details.
-->
The vibrational spectrum just obtained looks reasonable: no vibrational instability is observed and
the acoustic modes tend to zero linearly for $|\qq| \rightarrow 0$
(note that the acoustic sum-rule is enforced by default via [[asr]] = 1).
Yet this does not mean that our results are fully converged.
This is clearly seen if we compare the phonon bands computed with a 4x4x4 and a 6x6x6 ab-initio $\qq$-mesh.
Also in this case, we can automate the process by using the `ddb` command of the |abicomp| script that takes
in input an arbitrary list of DDB files, calls *anaddb* for all the DDB files and finally compare the results:

```sh
abicomp.py ddb \
   MgB2_eph4isotc/flow_mgb2_phonons/w1/outdata/out_DDB \
   MgB2_eph4isotc/666q_DDB -e
```

![](eph4isotc_assets/abicomp_phbands.png)

The figure reveals that the vibrational spectrum interpolated from the 4x4x4 $\qq$-mesh
underestimates the maximum phonon frequency.
Other differences are visible above 40 meV, especially in the  $E_{2g}$ modes.

<!--
Note also that phonon frequencies in metals are also quite sensitive to the $\kk$-mesh and the smearing [[tsmear]].
In real life one should perform an accurate convergence study...
-->

!!! note

    The *666q_DDB* file was produced with the same AbiPy script by just changing the value
    of the *ngqpt* variable.
    The reason why we do not provide files obtained with a 6x6x6 $\qq$-sampling is that the size of the
    git repository including all the DFPT potentials is around 200 Mb.
    For this reason, we continue using the 4x4x4 DDB file but we should take
    this into account when comparing our results with previous works.

## Our first computation of the isotropic Tc

For our first example, we use a relatively simple input file that allows us to introduce
the most important variables and the organization of the results.
Copy *teph4isotc_2.abi* in the working directory and run the code using:

```sh
abinit teph4isotc_2.abi > log 2> err
```

!!! tip

    If you prefer, you can run it in parallel with e.g two MPI processes using:

    ```sh
    mpirun -n 2 abinit teph4isotc_2.abi > log 2> err
    ```

    without having to introduce any input variable for the MPI parallelization
    as the EPH code can automatically distribute the workload over k-points and spins.
    Further details concerning the MPI version are given in the
    [last section of the tutorial](#notes-on-the-mpi-parallelism)

We now discuss the meaning of the different variables in more detail.

{% dialog tests/tutorespfn/Input/teph4isotc_2.abi %}

To activate the computation of $\gamma_{\qq\nu}$ in metals, we use [[optdriver]] = 7 and [[eph_task]] = 1.
The location of the external DDB, DVDB and WFK files is specified via
[[getddb_filepath]] [[getdvdb_filepath]] [[getwfk_filepath]], respectively.

```sh
getddb_filepath  "MgB2_eph4isotc/flow_mgb2_phonons/w1/outdata/out_DDB"

getwfk_filepath  "MgB2_eph4isotc/flow_mgb2_phonons/w0/t0/outdata/out_WFK.nc"

getdvdb_filepath "teph4isotc_1_DVDB"
```

The DDB and the WFK files are taken from the git repository while for the DVDB we use
the file produced by *mrgdv* in the previous section (remember what we said about
the use of **relative paths** in the input files).
Note also the use of the new input variable [[structure]] (added in Abinit v9)
with the **abifile** prefix

```sh
structure "abifile:MgB2_eph4isotc/flow_mgb2_phonons/w0/t0/outdata/out_DEN.nc"
```

to read the crystalline structure from an external file
so that we do avoid repeating the unit cell in every input file.

Next, we have the variables defining the coarse and the fine $\qq$-mesh
([[ddb_ngqpt]] and [[eph_ngqpt_fine]], respectively):

```sh
ddb_ngqpt 4 4 4        # The ab-initio q-mesh (DDB, DVDB)
eph_ngqpt_fine 6 6 6   # Activate interpolation of DFPT potentials
                       # gamma_{q,nu} are computed of this fine grid.
dipdip 0               # No treatment of the dipole-dipole part. OK for metals
```

[[dipdip]] is set to zero as we are dealing with a metal and the inter-atomic force
constants are short-ranged provided we ignore possible Kohn-anomalies [[cite:Kohn1959]].

The $\kk$-point integration is performed with the Gaussian smearing
([[eph_intmeth]] == 1 and 0.1 eV for [[eph_fsmear]]).

```sh
eph_intmeth 1       # Gaussian method for double-delta integration.
eph_fsmear 0.1 eV   # Constant broadening in Gaussian function.
eph_fsewin 0.3 eV   # Energy window for wavefunctions.
```

!!! note

    In order to accelerate the calculation, we have decreased [[eph_fsewin]]
    from its default value of 1.0 eV to 0.3 eV.
    This is possible when the Gaussian method is used since
    states whose energy is 3-4 standard deviation from the Fermi level give
    negligible contribution to the double delta integral.
    In other words, for the Gaussian technique, an optimal value of [[eph_fsewin]]
    can be deduced from [[eph_fsmear]]
    Unfortunately, this is not possible when the tetrahedron method is used and, to some extent,
    also when the adaptive broadening is used.
    Hopefully, the next versions of the code will provide ...
    0.04 Ha ~ 1 eV

Then we activate two tricks to make the calculation faster:

```sh
mixprec 1
boxcutmin 1.1
```

As explained in the documentation, using [[mixprec]] = 1 and 2 > [[boxcutmin]] >= 1.1
should not have significant effects on the final results yet these are not the default values
as users are supposed to compare the results with/without these tricks (especially [[boxcutmin]])
before running production calculations.

<!--

TODO: discuss ph_nqpath and ph_qpath
In this particular example, there's no difference between a calculation done with *dipdip* 0 or 1 as our
DDB does not contain $\ee^\infty$ and the Born-effective charges.
Finally, we have the specification of the $\kk$-mesh must in terms of [[ngkpt]], [[nshiftk]] and [[shiftk]]
that must be equal to the one used to generate the input WFK file:

```sh
ngkpt   12 12 12
nshiftk 1
shiftk  0.0 0.0 0.0
```

```sh
eph_task 0

# phonon DOS with q-mesh (default tetrahedron method)
ph_ngqpt
#ph_intmeth

# phonon bands along high-symmetry q-path
ph_nqpath
ph_qpath
ph_ndivsm

# Compute phonon bands along this q-path.
ph_nqpath 8
ph_qpath
    0.0 0.0 0.0   # Gamma
    1/3 1/3 0.0   # K
    0.5 0.0 0.0   # M
    0.0 0.0 0.0   # Gamma
    0.0 0.0 0.5   # A
    1/3 1/3 0.5   # H
    0.5 0.0 0.5   # L
    0.0 0.0 0.5   # A
```

The phonon DOS will be computed using the (dense) [[ph_ngqpt]] $\qq$-mesh with the [[ph_intmeth]] integration scheme.
The high-symmetry $\qq$-path for the phonon band structure is specified with:
[[ph_nqpath]], [[ph_qpath]], and [[ph_ndivsm]].

!!! important

    [[dipdip]] can be set to zero as we are dealing with a metal and therefore the IFCs are short-ranged.
    Note that the default value of [[dipdip]] is designed for polar semiconductors so we recommend
    to override the default behaviour when performing calculations with [[eph_task]] = 1.

Remember to discuss k-mesh and [[tsmear]] at the DFPT level.
-->

### Output files

We now discuss in more detail the main output file produced by the EPH run.

{% dialog tests/tutorespfn/Refs/teph4isotc_2.abo %}

After the standard section with info on the unit cell and the pseudopotentials,
we find info on the electronic DOS:

```sh
 Linear tetrahedron method.
 Mesh step:  10.0 (meV) with npts: 2757
 From emin:  -5.1 to emax:  22.5 (eV)
 Number of k-points in the IBZ: 133
 Fermi level:   7.63392108E+00 (eV)
 Total electron DOS at Fermi level in states/eV:   7.36154553E-01
 Total number of electrons at eF:    8.0

- Writing electron DOS to file: teph4isotc_2o_DS1_EDOS
```

Then the code outputs some basic quantities concerning the Fermi surface
and the method for the BZ integration of the double delta:

```md
 ==== Fermi surface info ====
 FS integration done with adaptive gaussian method
 Total number of k-points in the full mesh: 1728
 For spin: 1
    Number of BZ k-points close to the Fermi surface: 291 [ 16.8 %]
    Maximum number of bands crossing the Fermi level: 3
    min band: 3
    Max band: 5
```

Then, for each $\qq$-point in the IBZ, the code outputs
the values of $\ww_\qnu$, $\gamma_\qnu$ and $\lambda_\qnu$:

```md
 q-point =    0.000000E+00    0.000000E+00    0.000000E+00
 Mode number    Frequency (Ha)  Linewidth (Ha)  Lambda(q,n)
    1        0.000000E+00    0.000000E+00    0.000000E+00
    2        0.000000E+00    0.000000E+00    0.000000E+00
    3        0.000000E+00    0.000000E+00    0.000000E+00
    4        1.444402E-03    4.898804E-07    3.731164E-03
    5        1.444402E-03    4.898804E-07    3.731164E-03
    6        1.674636E-03    2.955247E-15    1.674492E-11
    7        2.426616E-03    1.025439E-03    2.767185E+00
    8        2.426616E-03    1.025439E-03    2.767185E+00
    9        3.127043E-03    3.508418E-08    5.701304E-05
```

The frequencies of the acoustic modes at $\Gamma$ are zero (as they should be)
since [[asr]] is automatically set to 1 so the acoustic rule is **automatically enforced**.
Also, the linewidths of the acoustic modes are zero.
In this case, however, the code uses a rather simple heuristic rule:

```sh
abiview.py ddb_asr MgB2_eph4isotc/flow_mgb2_phonons/w1/outdata/out_DDB
```

Finally, we have the value of the isotropic $\lambda$:

```sh
 lambda=   0.4286
```

The calculation has produced the following output files:

```sh
$ ls teph4isotc_2o_DS1*

teph4isotc_2o_DS1_A2F.nc       teph4isotc_2o_DS1_NOINTP_A2FW    teph4isotc_2o_DS1_PHDOS.nc
teph4isotc_2o_DS1_A2FW         teph4isotc_2o_DS1_NOINTP_PH_A2FW teph4isotc_2o_DS1_PHGAMMA
teph4isotc_2o_DS1_EBANDS.agr   teph4isotc_2o_DS1_PHBANDS.agr    teph4isotc_2o_DS1_PH_A2FW
teph4isotc_2o_DS1_EDOS         teph4isotc_2o_DS1_PHBST.nc
```

The files without the `.nc` extension are text files that can be plotted with e.g. |gnuplot| or |xmgrace|.
More specifically,

Apple
:   Pomaceous fruit of plants of the genus Malus in the family Rosaceae.

The *A2F.nc* netcdf file stores all the results of the calculation in a format that can
can be visualized with |abiopen|:

```sh
abiopen.py teph4isotc_2o_DS1_A2F.nc -e
```

that produces the following plot:

![](eph4isotc_assets/abiopen_teph4isotc_2o_DS1_A2F.png)

Our first results should be interpreted with a critical eye due to coarse sampling used,
yet our $\alpha^2F(\ww)$ already shows features that are observed in other (more accurate) calculations
reported in the literature: a broad peak at ~70 meV that gives the most important contribution to the total $\lambda$
and a second smaller peak at ~70 meV.
The sharp delta-like peaks are an artifact of the linear interpolation and will hopefully disappear
if denser $\qq$-meshes are used.
Our initial values for $\lambda$ and $\ww_{log}$ are:

```md
a2f(w) on the [4 4 4] q-mesh (ddb_ngqpt|eph_ngqpt)
Isotropic lambda: 0.39, omega_log: 0.067 (eV), 780.042 (K)k
```

If we compare our results for $\lambda$ with those reported in [[cite:Margine2013]],

![](eph4isotc_assets/MgB2_Margine_table.png)

we see that our first calculation underestimates $\lambda$ by almost a factor two.
Note, however, that all the previous studies used much denser samplings and all of them
use at least a 6x6x6 $\qq$-mesh for phonons.
It is clear that we need to densify the BZ sampling to get more reliable results yet
our results are not that bad considering that the calculation took less than XXX minutes in sequential.

<!--

??? note "Exercise"

     Since our WFK is defined on a 12x12x12 $\kk$-mesh it is very easy to activate the interpolation
     of the DFPT potentials to go from a 4x4x4 to a 12x12x12 $\qq$-mesh while keeping the same WFK file.
     Use [[eph_ngqpt_fine]] in the previous input file to densify the $\qq$-mesh for phonons and compare the
     results with those obtained with a 6x6x6 sampling.
     Use compare the results obtained without [[mixprec]] 1 and [[boxcutmin]] 1.1.
     You may want to run the calculation in parallel with mpirun.

-->

## Using the tetrahedron method

In this section, we repeat the calculation done in **teph4isotc_2.abi**
but now with the **optimized tetrahedron** scheme by [[cite:Kawamura2014]].
This test will show that (i) phonon linewidths are very sensitive to the $\kk$-mesh
and the integration scheme and that (ii) one has to test different values of [[eph_fsewin]]
to find the optimizal one.

The reason is that the optimized tetrahedron scheme constructs a third-order interpolation function
with 20 $\kk$-points so states that are relatively far from the Fermi level contributes to the integrand.
If [[eph_fsewin]] is too small, part of the weigt is lost.
To find an optimal value, we can use:

```md
ndtset 4
eph_intmeth 2   # Tetra
eph_fsewin: 0.4 eV
eph_fsewin+ 0.4 eV
```

We do not provide an input file to perform such test as one can easily change **teph4isotc_2.abi**.
and the run the calculation in parallel.
To compare the results, we use the `a2f` command of the |abicomp| script and pass the list of `A2F.c` files:

```sh
abicomp.py a2f teph4isotc_2o_DS*_A2F.nc -e
```

![](eph4isotc_assets/abicomp_phbands.png)


## Preparing the convergence study wrt the k-mesh

<!--
Our goal is to perform calculations of $\gamma_\qnu$ and $\lambda_\qnu$ with different $\kk/\qq$-meshes
in order to analyze the convergence behaviour of $\alpha^2F(\ww)$ and $\lambda$.
Due to the limited amount of time available for a tutorial, we won't be able to fully reproduce published results
also because we are still using a 4x4x4 **ab-initio** $\qq$-mesh that, as already discussed, is not enough
to describe vibrational properties.
By the same token, we should not expect the Fourier interpolation of the scattering potentials to work perfectly
when such a coarse **ab-initio** sampling is used.
In a nutshell, we have to make the best of what we have at hand while minimizing the computational cost.

Since we already have a WFK file on a 12x12x12 $\kk$-mesh, it makes sense to compute electrons
on a 24x24x24 $\kk$-mesh so that we can compare the following configurations:
-->

<!--
TODO: Discuss energy window, sigma_erange in connection with the FS window and the integration scheme

As already mentioned in the introduction, the integration of the double delta requires very dense $\kk$-meshes
for electrons in order to obtain accurate results for $\gamma_\qnu$.
By the same token, converging the fine details of $\alpha^2F(\ww)$ and $\lambda$ requires
sampling enough phonon $\qq$-wavectors in the IBZ.
For this reason, convergence studies are performed by densifying both the mesh for electrons and for phonons.
Additional convergence studies wrt [[eph_fsmear]] and [[ph_smear]] are required if Gaussian techniques are used.
Remember that in the EPH code the $\qq$-mesh can be changed at will thanks to the Fourier interpolation
of the dynamical matrix/DFPT potentials whereas Bloch states must be computed explicitly.

Before running calculations with difference meshes, it is a good idea
to sketch the convergence study we want to perform.
Let's assume we want to test the following configurations:

A 6x6x6 $\qq$-mesh (without interpolation) and the following $\kk$-meshes:

    12x12x12, 18x18x18, 24x24x24 (36x36x36)

A 12x12x12 $\qq$-mesh (with interpolation) and the following $\kk$-meshes:

    12x12x12, 24x24x24, (36x36x36)

The pattern at this point should clear: we need to perform NSCF computations
for four different Gamma-centered $\kk$-meshes: 12x12x12, 18x18x18, 24x24x24, and (48x48x48).
The computational cost of the NSC run increases quickly with the size of the $\kk$-mesh but as already
stressed we only needed electron states in a appropriate energy window around the Fermi level.
As the $\kk$-mesh must be a multiple of the $\qq$-mesh, we need to generate **different WFK files**
to prepare our convergence studies.
-->

The NSCF computation of the WFK becomes quite CPU-consuming and memory-demanding if dense $\kk$-meshes are needed.
Fortunately, we can optimize this part since the computation of $\gamma_\qnu$
requires the knowledge of Bloch states inside a relatively small energy window around $\ee_F$.
Similarly to what is done in the [eph4mob tutorial](eph4mob), we can therefore take advantage of
the star-function SKW interpolation to find the $\kk$ wavevectors whose energy is inside
the [[sigma_erange]] energy window **around the Fermi level**.
The choice of an optimal window is discussed afterwards.

First of all, we **strongly recommend** to test whether the SKW interpolation can reproduce
the *ab-initio* results with reasonable accuracy
by comparing the *ab-initio* band structure with the interpolated one by
using *abitk* with the `skw_compare` command:

```sh
abitk skw_compare \
    MgB2_eph4isotc/flow_mgb2_phonons/w0/t0/outdata/out_GSR.nc \
    MgB2_eph4isotc/flow_mgb2_phonons/w0/t1/outdata/out_GSR.nc  --is-metal
```

The fist argument is a *GSR.nc* (*WFK.nc*) file with the energies in the IBZ
that will be used to build the star-function interpolant.
The quality of the interpolant obviously depends on the density of this $\kk$-mesh.
The second file contains the *ab-initio* eigenvalues along a $\kk$-path computed with a standard NSCF run.
In this example, we are using precomputed GSR.nc files with a 12x12x12 $\kk$-mesh for the SKW interpolation.

!!! Note

    Note the use of the `--is-metal` option else *abitk* will complain that it cannot compute the gaps
    as the code by default assumes a semiconductor.

The *skw_compare* command has produced two netcdf files (*abinitio_EBANDS.nc* and *skw_EBANDS.nc*)
that we can be compared with the |abicomp| script and the `ebands` command:

```sh
abicomp.py ebands abinitio_EBANDS.nc skw_EBANDS.nc -p combiplot
```

to produce the following plot:

![](eph4isotc_assets/skw_vs_abinitio_ebands.png)

The figure shows that the SKW interpolant nicely reproduces the *ab-initio* dispersion around $\ee_F$.
Discrepancies between the *ab-initio* results and the interpolated values are visible
in correspondence of **band-crossings**.
This is expected since SKW is a Fourier-based approach and band-crossing leads to a non-analytic behaviour
of the signal that cannot be reproduced with a finite number of Fourier components.
Fortunately, these band crossings are relatively far from the Fermi level hence they do not enter into play
in the computation of superconducting properties.

!!! tip

    Should the fit be problematic, use the command line options:

    ```sh
    abitk skw_compare IBZ_WFK KPATH_WFK [--lpratio 5] [--rcut 0.0] [--rsigma 0]
    ```

    to improve the interpolation and take note of the SKW parameters
    as the same values should be used when calling ABINIT with the [[einterp]] input variable.
    The worst case scenario of band crossings and oscillations close to the Fermi level
    can be handled by enlarging the [[sigma_erange]] energy window.

At this point, we are confident that the SKW interpolation is OK and we can use it to locate
the $\kk$-wavevectors of a much denser $\kk$-mesh whose energy is inside the [[sigma_erange]] window
around the Fermi level.
This is done in the *teph4isotc_4.abi* input file:

{% dialog tests/tutorespfn/Input/teph4isotc_3.abi %}

The most important section of the input file is reproduced below:

```sh
optdriver 8
wfk_task "wfk_kpts_erange"
getwfk_filepath  "MgB2_eph4isotc/flow_mgb2_phonons/w0/t0/outdata/out_WFK.nc"

# Define fine k-mesh for the SKW interpolation
sigma_ngkpt   24 24 24
sigma_nshiftk 1
sigma_shiftk  0 0 0

sigma_erange -0.3 -0.3 eV  # Select kpts in the fine mesh within this energy window.
einterp 1 5 0 0            # Parameters for star-function interpolation (default values)
```

The first part ([[optdriver]] and [[wfk_task]]) activates the computation of the KERANGE.nc file
with *ab-initio* energies in the IBZ taken from [[getwfk_filepath]].
The three variables [[sigma_ngkpt]], [[sigma_nshiftk]] and [[sigma_shiftk]] define the final dense $\kk$-mesh.
Finally, [[sigma_erange]] defines the energy window around the Fermi level
while [[einterp]] lists the parameters passed to the SKW routines
In this example, we are using the default values.
You may need to change [[einterp]] if you had to use different options for the SKW interpolation
when using `abitk skw_compare`.

!!! important

    When dealing with metals, both entries in [[sigma_erange]] must be **negative**
    so that the energy window is refered to $\ee_F$ and not to the CBM/VBM used in semiconductors.
    Remember to use a value for the window that is reasonably large in order to account
    for possible oscillations and/or inaccuracies of the SKW interpolation around $\ee_F$.

{% dialog tests/tutorespfn/Refs/teph4isotc_3.abo %}
{% dialog tests/tutorespfn/Refs/teph4isotc_4.abo %}

Once we have the *KERANGE.nc* file, we can use it to perform a NSCF calculation to generate
a customized WFK file on the dense $\kk$-mesh:
This is done in the *teph4isotc_4.abi* input file:

{% dialog tests/tutorespfn/Input/teph4isotc_4.abi %}

to perform a NSCF calculation with [[kptopt]] 0 to produce a new WFK file on the dense $\kk$-mesh.

```sh
iscf  -2
tolwfr 1e-18
kptopt 0                        # Important!
```

with the [[getkerange_filepath]]:

!!! note

    Note how we use [[getden_filepath]] and the syntax:

    ```sh
    getkerange_filepath "teph4isotc_3o_DS1_KERANGE.nc"

    # Read DEN file to initialize the NSCF run.
    getden_filepath "MgB2_eph4isotc/flow_mgb2_phonons/w0/t0/outdata/out_DEN.nc"

    # Init GS wavefunctions from this file (optional).
    getwfk_filepath "MgB2_eph4isotc/flow_mgb2_phonons/w0/t0/outdata/out_WFK.nc"
    ```

    to start the NSCF run from the **precomputed** GS DEN file, initialize the trial wafefunctions
    from [[getwfk_filepath]] to accelerate a bit the calculation.

TODO: Discussion about energy range and integration scheme.

## Convergence study wrt to the k/q-mesh

At this point, we can use the WFK file to perform EPH calculations with denser $\kk$-meshes.
We will be using settings similar to the ones used in **teph4isotc_2.abi** except for
the use of [[eph_ngqpt_fine]], [[getwfk_filepath]] and [[ngkpt]]:

{% dialog tests/tutorespfn/Input/teph4isotc_5.abi %}

<!--
## A more accurate calculation

If you want to reproduce the results, you can find another github repository here
with the DFPT computation performed with a 24x24x24 $\kk$-mesh and a 6x6x6 $\qq$-mesh for phonons.
Download the repository as explained at the beginning of the lesson, then merge the partial POT files
and compute the WFK file with a NSCF run staring from the DEN.nc file.

You should get:

The EPH calculation on 48 CPUs takes ~ minutes and less than XXX Gb of memory.
-->

## Notes on the MPI parallelism

EPH calculations performed with [[eph_task]] = 1 support 4 different levels of MPI parallelism and
the number of MPI processes for each level can be specified via the [[eph_np_pqbks]] input variable.
Note that we wrote 4 MPI levels instead of 5 because, for isotropic $T_c$ calculations,
the **band parallelism is not supported**, only atomic perturbations, $\qq$-points, $\kk$-points and spins (if any).

The [[eph_np_pqbks]] variable is **optional** in the sense that whatever number of MPI processes you use,
the EPH code will be able to parallelize the calculation by distributing over $\kk$-points and spins (if any).
This distribution, however, may not be optimal, especially if you have lot of CPUs available
and/or you need to decrease the memory requirements per CPU.

To decrease memory, we suggest to activate the parallelism over perturbations
(up to 3 x [[natom]]) to make the memory for $W(\rr, \RR, \text{3 x natom})$ scale.
If memory is not of concern, activate the $\qq$-point parallelism for better efficiency.

!!! important

    Note that the code is not able to distribute the memory for the wavefunctions although
    only the states in the [[eph_fsewin]] energy window are read and stored in memory.
---
authors: MC, SP
---

# Tutorial on analysis tools

## How to use cut3d

This tutorial covers some of the features available in the program cut3d. The
help file of this program can be view at [[help:cut3d]].

The cut3d program can produce output that can be read by several other programs.
At present, at least XCrySDen ([www.xcrysden.org](http://www.xcrysden.org/), freely available)
is known to support this output.

The cut3d program can be used to treat data in a density file (*_DEN*), a
potential file (*_POT*) or a wavefunction file (*_WFK*). In this tutorial will we
see how to use this program to obtain the value of the density on a line.

This tutorial should take about 15 minutes.

## 1 Obtaining the density and wavefunctions files for Silicon

We will first start by a simple example: viewing Silicon density information.
Before using the cut3d program, we first have to generate the density and
the wavefunctions files by running the Abinit program. For this simple case,
we will use the last input file used in [tutorial 3](/tutorial/base3)
(*tbase3_5.in*). That input file contains two datasets, one to obtain a
self-consistent density and the other to obtain a selected number of k-point wavefunctions.
If you do not have the output of this case, you will need to
rerun it, at this stage, this should be fairly easy for you!

The output files that are of interest for us now are:

    tbase3_5o_DS1_DEN
    tbase3_5o_DS1_WFK
    tbase3_5o_DS2_WFK

!!! important

    To ask Abinit to output the density, you have to set the input
    parameter [[prtden]] to 1 in the input file as it was done in the first dataset in this example.

**NOTE:** In DS1, the self-consistent dataset, we have a good density (file:
*tbase3_5o_DS1_DEN*) which is physically relevant.
Careful, if you run a non-self-consistent job, you may obtain a density but it will not be physical!

**NOTE:** The wavefunctions in DS1 (file: *tbase3_5o_DS1_WFK*) are those of the
Monkhorst and Pack grid which are good for self-consistent calculations but
often not really inspiring physically. For this reason, we often have to do a
non-self-consistent run to get relevant wavefunctions as it is done in this
case. The DS2 (file: *tbase3_5o_DS2_WFK*) contains the interesting wavefunctions
that we want to analyse.

## 2 Getting the density on a given line

Now we are ready to use the cut3d program. First we must make sure that we
have compiled the program. If you have run the command  *make* in the Abinit
main directory, and everything went well, then you have obtained the abinit
program and all the accompanying program, in particular *cut3d*. You can
obtain only the cut3d program by issuing the command *make cut3d* in the Abinit main directory.

Assuming that we are in the directory *Tutorial/Work_tools*, and the *cut3d*
program can be accessed by simply typing its name, now write the command:

    cut3d

The *cut3d* program will ask you several questions in order to determine what
you want him to do. In this tutorial, the text coming from the *cut3d* program are
in black and the input that you have to type in are in  red.

After some text about the version and the licence, you will see the question:

    What is the name of the 3D function (density, potential or wavef) file ?

You can enter the density file:

    tbase3_5o_DS1_DEN

You will then see:

     => Your 3D function file is : tbase3_5o_DS1_DEN

     Does this file contain formatted 3D ASCII data (=0)
     or unformatted binary header + 3D data (=1) ?

Your answer is 1 as generally all output of Abinit are in unformatted binary
form. After you have pressed enter, cut3d prints out a lot of information
contained in this file that explain how that density was obtained. At this
point, you can double check that this is really the file that you want to
analyse. Then you will have the choices:

     What is your choice ? Type:
     1 => point (interpolation of data for a single point)
     2 => line (interpolation of data along a line)
     3 => plane (interpolation of data in a plane)
     4 => volume (interpolation of data in a volume)
     5 => 3D formatted data (output the bare 3D data - one column)
     6 => 3D indexed data (bare 3D data, preceeded by 3D index)
     7 => 3D Molekel formatted data
     8 => 3D data with coordinates (tecplot ASCII format)
     9 => output .xsf file for XCrysDen
     10 => output .dx file for OpenDx
     11 => compute atomic charge using the Hirshfeld method
     12 => NetCDF file
     13 => exit

As you can see, there are many options available! At the moment, we will do
something simple which is getting the density along a given line.

Choose option 2. You will be given the following choices:

    Type 1) for a line between two cartesian-defined points
      or 2) for a line between two crystallographic-defined points
      or 3) for a line defined by its direction

Select option 1. You will get the message:

    Type the first point coordinates (Bohrs):
       -> X-dir Y-dir Z-dir:

We will ask for the density along the [111] direction starting from the origin
and extending to two unit cell. For the first point, enter 0 0 0.
You will get the message:

    Type the second point coordinates (Bohrs):
       -> X-dir Y-dir Z-dir:

Looking at the input file, we see that the cubic side has a length of 10.217 bohr.
To get 2 unit cells, you will enter 20.434 20.434 20.434.
You will then be asked for the line resolution.

    Enter line resolution:

This is how many points are calculated for the line. Typically, you should
enter a large number, say 5000. You will then be asked for the file name.

    Enter the name of an output file:

Enter something meaningful, si_den_111.dat.

    More analysis of the 3D file ? (1=default=yes,2=no)

Enter 2 to finish with cut3d. If you do a list of the files in your working
directory, you should see the *si_den_111.dat* file. You can look at this
file, you will see that this is a simple two columns file. You can visualize
it with your favorite plotting software (ex: xmgrace, gnuplot, ...). If you
open this file with xmgrace, you will obtain the following graph:

![](cut3d_assets/si_den_111.png)

We can learn quite a lot from this graph. Looking at the input file, we see
that we have a silicon atom at the origin and another 1/4 along the diagonal.
We note also that there is almost no density at the origin. By doing the line
in the [111] direction, we have crossed the first silicon-silicon bond which
we can see as a large density peak indicated by the red arrows.

By the way, cut3d works with the so-called trilinear interpolation of the
density. That is, the FFT grid, on which the density is defined, in real
space, leads to a partitioning of the space in small rectangular
parallelepipeds, and in each such parallelepiped, the eight values of the
density on the corner are used to define an interpolation that is linear in
each the dimensions of the cube. More explicitly, let us chose one of the
corner as origin, and define reduced coordinates, varying from 0 to 1 inside
the parallelepiped, the density inside the parallelepiped is given by

     density(dx,dy,dz)= density(0,0,0) +
        c100 * dx + c010 * dy + c001 * dz +
        c110 * dx * dy + c101 * dx * dz +
        c011 * dy * dz + c111 * dx * dy * dz

where the 7 coefficients c100, c010, c001, c110, c101, c011, c111 are to be
determined from the knowledge of the density at the 7 other corners.

This trilinear interpolation guarantees that the overall function of space is
continuous, but do not guarantee that the derivatives change continuously.
This is seen in the above figure, as the two abrupt changes of slopes, close
to the maximum. These are completely spurious features, that can be avoided is
a finer FFT grid is used to determine the density. Actually, the density
profile along the bond, with a fine FFT grid, behave quite smoothly, and reach
its maximum at the mid point between the two atoms, in contrast to the figure
presented previously.

!!! note

    An important point to remember when doing visualization of density
    obtained with a pseudopotential calculation is that the core charge is not
    included! This is the reason why we use pseudopotential and the charge we get
    is only the valence charge. It is possible to include some of the core charge
    by doing calculations using pseudos with partial core charge. The fact that
    the core charge is missing means that there is no charge at the nucleus of our
    atoms. This means that we are getting low density at the nucleus center and we
    have to remember that this is not a physical effect! The location of the
    silicon atoms have been indicating by blue arrows.
---
authors: MG, MS
---

# Parallelism for many-body calculations

## G<sub>0</sub>W<sub>0</sub> corrections in &alpha;-quartz SiO<sub>2</sub>.

This tutorial aims at showing how to perform parallel calculations with the GW
part of ABINIT. We will discuss the approaches used to parallelize the
different steps of a typical G<sub>0</sub>W<sub>0</sub> calculation, and how to setup the parameters
of the run in order to achieve good speedup. α-quartz SiO<sub>2</sub> is used as test case.

It is supposed that you have some knowledge about UNIX/Linux, and you know how to submit MPI jobs.
You are supposed to know already some basics of parallelism in ABINIT,
explained in the tutorial [A first introduction to ABINIT in parallel](basepar.md).

In the following, when "run ABINIT over nn CPU cores" appears, you have to use
a specific command line according to the operating system and architecture of
the computer you are using. This can be for instance:

    mpirun -n nn abinit abinit.abi

or the use of a specific submission file.

This tutorial should take about 1.5 hour and requires a modern 
computer cluster of 20 CPU cores or more.

[TUTORIAL_README]

## 1 Generating the WFK file in parallel

Before beginning, you should create a working directory in *\$ABI_TESTS/tutoparal/Input*
whose name might be *Work_mbt*.

```sh
cd $ABI_TESTS/tutoparal/Input/
mkdir Work_mbt && cd Work_mbt
```

The input files necessary to run the examples related to this tutorial are
located in the directory *\$ABI_TESTS/tutoparal/Input* .
We will do most of the actions of this tutorial in this working directory.

Note that the pseudopotentials needed for running the tutorial (Si.psp8 and O.psp8)
are located in the directory *\$ABI_PSPDIR/Pseudodojo_nc_sr_04_pw_standard_psp8*.

In the [first GW tutorial](/tutorial/gw1), we have learned how to
generate the WFK file with the sequential version of the code.
Now we will perform a similar calculation taking advantage of the k-point parallelism
implemented in the ground-state part.

First of all, copy all input files *tmbt_\*.abi* in the working directory *Work_mbt*:

    cd Work_mbt
    cp ../tmbt_*.abi .

Now open the input file *\$ABI_TESTS/tutoparal/Input/tmbt_1.abi* in your
preferred editor, and look at its structure.

{% dialog tests/tutoparal/Input/tmbt_1.abi %}

The first dataset performs a rather standard SCF calculation to obtain the
ground-state density. The second dataset reads the density file and calculates
the Kohn-Sham band structure including many empty states:

    # DATASET 2 : WFK generation
    iscf2      -2      # NSCF
    getden2    -1      # Read previous density
    tolwfr2    1d-12   # Stopping criterion for the NSCF cycle.
    nband2      160    # Number of (occ and empty) bands computed in the NSCF cycle.
    nbdbuf2     10     # A large buffer helps to reduce the number of NSCF steps.

We have already encountered these variables in the [first GW tutorial](/tutorial/gw1)
so their meaning should be familiar to you.
The only thing worth stressing is that this calculation solves the NSCF cycle
with the conjugate-gradient method ([[paral_kgb]] == 0)

The NSCF cycle is executed in parallel using the standard parallelism over
k-points and spin in which the ([[nkpt]] x [[nsppol]]) blocks of bands are
distributed among the nodes. This test uses an unshifted 4x4x3 grid (48 k
points in the full Brillouin Zone, folding to 9 k-points in the irreducible
wedge) hence the theoretical maximum speedup is 9.

Now run ABINIT over nn CPU cores using (here, nn=9)

    mpirun -n 9 abinit tmbt_1.abi > tmbt_1.log 2> err &

but keep in mind that, to avoid idle processors, the number of CPUs (nn) should
divide 9. At the end of the run, the code will produce the file *tmbt_1o_DS2_WFK*
needed for the subsequent GW calculations.

With three cores, the wall clock time is around 1.5 minutes.

    >>> tail tmbt_1.out

    -
    - Proc.   0 individual time (sec): cpu=        209.0  wall=        209.0

    ================================================================================

     Calculation completed.
    .Delivered    0 WARNINGs and   5 COMMENTs to log file.
    +Overall time at end (sec) : cpu=        626.9  wall=        626.9


A reference output file is given in *\$ABI_TESTS/tutoparal/Refs*, under the name *tmbt_1.abo*.

Note that 150 bands are not enough to obtain converged GW results, you might
increase the number of bands in proportion to your computing resources.

## 2 Computing the screening in parallel using the Adler-Wiser expression

In this part of the tutorial, we will compute the RPA polarizability with the
Adler-Wiser approach. The basic equations are discussed in this
[[theory:mbt#5-the-rpa-polarizability-in-fourier-space|section]] of the GW notes.

First copy the file *tmbt_2.abi* in the working directory, then create a
symbolic link pointing to the WFK file we have generated in the previous step:

    >>> ln -s tmbt_1o_DS2_WFK tmbt_2i_WFK

Now open the input file *\$ABI_TESTS/tutoparal/Input/tmbt_2.abi* so that we can
discuss its structure.

{% dialog tests/tutoparal/Input/tmbt_2.abi %}

The set of parameters controlling the screening computation is summarized below:

    optdriver   3   # Screening run
    irdwfk      1   # Read input WFK file
    symchi      1   # Use symmetries to speedup the BZ integration
    awtr        1   # Take advantage of time-reversal. Mandatory when gwpara=2 is used.
    gwpara      2   # Parallelization over bands
    ecutwfn     24  # Cutoff for the wavefunctions.
    ecuteps     8   # Cutoff for the polarizability.
    nband       50  # Number of bands in the RPA expression (24 occupied bands)
    inclvkb     2   # Correct treatment of the optical limit.

Most of the variables have been already discussed in the
[first GW tutorial](/tutorial/gw1). The only variables that
deserve some additional explanation are [[gwpara]] and [[awtr]].

[[gwpara]] selects the parallel algorithm used to compute the screening. Two
different approaches are implemented:

  * **gwpara** =1 -> Trivial parallelization over the k-points in the full Brillouin
  * **gwpara** =2 -> Parallelization over bands with memory distribution

Each method presents advantages and drawbacks that are discussed in the
documentation of the variable. In this tutorial, we will be focusing on
**gwpara** =2 since this is the algorithm with the best MPI-scalability and,
mostly important, it is the only one that allows for a significant reduction
of the memory requirement. By the way, this is the default value, so you do not
need to mention it explicitly in your input file.

The option [[awtr]] = 1 specifies that the system presents time reversal
symmetry so that it is possible to halve the number of transitions that have
to be calculated explicitly (only resonant transitions are needed). Note that
[[awtr]] = 1 is MANDATORY when [[gwpara]] = 2 is used. By the way, [[awtr]] = 1 
is also the default value,  so you do not
need to mention it explicitly in your input file.

Before running the calculation in parallel, it is worth discussing some
important technical details of the implementation. For our purposes, it
suffices to say that, when [[gwpara]] = 2 is used in the screening part, the
code distributes the wavefunctions such that each processing unit owns the
FULL set of occupied bands while the empty states are distributed among the
nodes. The parallel computation of the inverse dielectric matrix is done in
three different steps that can be schematically described as follows:

**Step 1.** Each node computes the partial contribution to the RPA polarizability:

$$ \sum_{c,\nu} = \sum^\text{occ}_\nu \sum^\text{mystop}_\text{c=mystart} $$

**Step 2.** The partial results are collected on each node.

**Step 3.** The master node performs the matrix inversion to obtain the inverse dielectric matrix and writes the final result on file.

Both the first and second step of the algorithm are expected to scale well
with the number of processors. Step 3, on the contrary, is performed in
sequential thus it will have a detrimental effect on the overall scaling,
especially in the case of large screening matrices (large [[npweps]] or large
number of frequency points ω).

Note that the maximum number of CPUs that can be used is dictated by the
number of empty states used to compute the polarizability. Most importantly, a
balanced distribution of the computing time is obtained when the number of
processors divides the number of conduction states.

The main limitation of the present implementation is represented by the
storage of the polarizability. This matrix, indeed, is not distributed hence
each node must have enough memory to store in memory a table whose size is
given by ( **npweps**<sup>2</sup> x **nomega** x 16 bytes) where **nomega** is the total
number of frequencies computed.

Tests performed at the Barcelona Supercomputing Center (see figures below)
have revealed that the first and the second part of the MPI algorithm have a
very good scaling. The routines cchi0 and cchi0q0 where the RPA expression is
computed (step 1 and 2) scales almost linearly up to 512 processors. The
degradation of the total speedup observed for large number of processors is
mainly due to the portions of the computation that are not parallelized,
namely the reading of the WFK file and the matrix inversion (qloop).

![](paral_mbt_assets/screening_speedup.png)

![](paral_mbt_assets/screening_rel_times.png)

At this point, the most important technical details of the implementation have
been covered, and we can finally run ABINIT over nn CPU cores using

    (mpirun ...) abinit tmbt_2.abi > tmbt_2.log 2> err &

Run the input file *tmb_2.abi* using different number of processors and keep
track of the time for each processor number so that we can test the
scalability of the implementation. The performance analysis reported in the
figures above was obtained with PAW using ZnO as tests case, but you should
observe a similar behavior also in SiO<sub>2</sub>.

Now let us have a look at the output results. Since this tutorial mainly
focuses on how to run efficient MPI computations, we will not perform any
converge study for SiO<sub>2</sub>. Most of the parameters used in the input files are
already close to converge, only the k-point sampling and the number of empty
states should be increased. You might modify the input files to perform the
standard converge tests following the procedure described in the [first GW tutorial](/tutorial/gw1).

In the main output file, there is a section reporting how the bands are
distributed among the nodes. For a sequential calculation, we have

     screening : taking advantage of time-reversal symmetry
     Maximum band index for partially occupied states nbvw =    24
     Remaining bands to be divided among processors   nbcw =    26
     Number of bands treated by each node ~   26

The value reported in the last line will decrease when the computation is done with more processors.

The memory allocated for the wavefunctions scales with the number of
processors. You can use the grep utility to extract this information from the
log file. For a calculation in sequential, we have:

    >>> grep "Memory needed" tmbt_2.log

      Memory needed for storing ug=         29.5 [Mb]
      Memory needed for storing ur=        180.2 [Mb]

_ug_ denotes the internal buffer used to store the Fourier components of the
orbitals whose size scales linearly with [[npwwfn]]. _ur_ is the array storing
the orbitals on the real space FFT mesh. Keep in mind that the size of _ur_
scales linearly with the total number of points in the FFT box, number that is
usually much larger than the number of planewaves ([[npwwfn]]). The number of
FFT divisions used in the GW code can be extracted from the main output file using

    >>> grep setmesh tmbt_2.out  -A 1
     setmesh: FFT mesh size selected  =  27x 27x 36
              total number of points  =    26244

As discussed in this
[[theory:mbt#6-notes-on-the-calculation-of-the-oscillator-matrix-elements|section]] of the GW notes,
the Fast Fourier Transform represents one of the most CPU
intensive part of the execution. For this reason the code provides the input
variable [[fftgw]] that can be used to decrease the number of FFT points for
better efficiency. The second digit of the input variable [[gwmem]], instead,
governs the storage of the real space orbitals and can used to avoid the
storage of the costly array _ur_ at the price of an increase in computational time.

#### **2.d Manual parallelization over q-points.**

The computational effort required by the screening computation scales linearly
with the number of q-points.
As explained in this
[[theory:mbt#5-the-rpa-polarizability-in-fourier-space|section]]
of the GW notes, the code exploits the symmetries of the screening function so
that only the irreducible Brillouin zone (IBZ) has to be calculated
explicitly. On the other hand, a large number of q-points might be needed to
achieve converged results. Typical examples are GW calculations in metals or
optical properties within the Bethe-Salpeter formalism.

If enough processing units are available, the linear factor due to the q-point
sampling can be trivially absorbed by splitting the calculation of the
q-points into several independent runs using the variables [[nqptdm]] and
[[qptdm]]. The results can then be gathered in a unique binary file by means
of the **mrgscr** utility (see also the automatic tests [[test:v3_87]], [[test:v3_88]] and [[test:v3_89]]).

## 3 Computing the screening in parallel using the Hilbert transform method

As discussed in the [[theory:mbt#RPA_Fourier_space|GW_notes]]
the algorithm based on the Adler-Wiser expression is not optimal when many
frequencies are wanted. In this paragraph, we therefore discuss how to use the
Hilbert transform method to calculate the RPA polarizability on a dense
frequency mesh. The equations implemented in the code are documented in
[[theory:mbt#hilbert_transform|in this section]].


As usual, we have to copy the file *tmbt_3.abi* in the working directory,
and then create a symbolic link pointing to the WFK file.

    >>> ln -s tmbt_1o_DS2_WFK tmbt_3i_WFK

The input file is *\$ABI_TESTS/tutoparal/Input/tmbt_3.abi*.
Open it so that we can have a look at its structure.

{% dialog tests/tutoparal/Input/tmbt_3.abi %}

A snapshot of the most important parameters governing the algorithm is reported below.

    gwcalctyp   2    # Contour-deformation technique.
    spmeth      1    # Enable the spectral method.
    nomegasf  100    # Number of points for the spectral function.
    gwpara      2    # Parallelization over bands
    awtr        1    # Take advantage of time-reversal. Mandatory when gwpara=2 is used.
    freqremax  40 eV # Frequency mesh for the polarizability
    nfreqre    20
    nfreqim     5

The input file is similar to the one we used for the Adler-Wiser calculation.
The input variable [[spmeth]] enables the spectral method. [[nomegasf]]
defines the number of ω′ points in the linear mesh used for the spectral
function i.e. the number of ω′ in the
[[theory:mbt#hilbert_transform|equation]] for the spectral function.

As discussed in the [[theory:mbt#hilbert_transform|GW notes] for the spectral function.
Hilbert transform method is much more memory demanding that the Adler-Wiser
approach, mainly because of the large value of [[nomegasf]] that is usually
needed to converge the results. Fortunately, the particular distribution of
the data employed in [[gwpara]] = 2 turns out to be well suited for the
calculation of the spectral function since each processor has to store and
treat only a subset of the entire range of transition energies. The algorithm
therefore presents good MPI-scalability since the number of ω′ frequencies
that have to be stored and considered in the Hilbert transform decreases with
the number of processors.

Now run ABINIT over nn CPU cores using

    (mpirun ...) abinit tmbt_3.abi > tmbt_3.log 2> err

and test the scaling by varying the number of processors. Keep in mind that,
also in this case, the distribution of the computing work is well balanced
when the number of CPUs divides the number of conduction states.

The memory needed to store the spectral function is reported in the log file:

    >>> grep "sf_chi0q0" tmbt_3.log
     memory required by sf_chi0q0:           1.0036 [Gb]

Note how the size of this array decreases when more processors are used.

The figure below shows the electron energy loss function (EELF) of SiO<sub>2</sub>
calculated using the Adler-Wiser and the Hilbert transform method. You might
try to reproduce these results (the EELF is reported in the file *tmbt_3o_EELF*,
a much denser k-sampling is required to achieve convergence).

![](paral_mbt_assets/comp-AW-spect.png)

## 4 Computing the one-shot GW corrections in parallel

In this last paragraph, we discuss how to calculate G<sub>0</sub>W<sub>0</sub> corrections in
parallel with [[gwpara]] = 2. The basic equations used to compute the self-energy matrix elements are discussed in
[[theory:mbt#evaluation_gw_sigma|this part]] of the GW notes.

Before running the calculation, copy the file *tmbt_4.abi* in the working
directory. Then create two symbolic links for the SCR and the WFK file:

    ln -s tmbt_1o_DS2_WFK tmbt_4i_WFK
    ln -s tmbt_2o_SCR     tmbt_4i_SCR

Now open the input file *\$ABI_TESTS/tutoparal/Input/tmbt_4.abi*.

{% dialog tests/tutoparal/Input/tmbt_4.abi %}

The most important parameters of the calculation are reported below:

```
optdriver   4            # Sigma run.
irdwfk      1
irdscr      1
gwcalctyp   0 ppmodel 1  # G<sub>0</sub>W<sub>0</sub> calculation with the plasmon-pole approximation.
#gwcalctyp  2            # Uncomment this line to use the contour-deformation technique but remember to change the SCR file!
gwpara      2            # Parallelization over bands.
symsigma    1            # To enable the symmetrization of the self-energy matrix elements.
ecutwfn    24            # Cutoff for the wavefunctions.
ecuteps     8            # Cutoff in the correlation part.
ecutsigx   20            # Cutoff in the exchange part.
nband       50           # Number of bands for the correlation part.
```

For our purposes, it suffices to say that this input file defines a standard
one-shot calculation with the plasmon-pole model approximation. We refer to
the documentation and to the [first GW tutorial](/tutorial/gw1)
for a more complete description of the meaning of these variables.

Also in this case, we use [[gwpara]] = 2 to perform the calculation in parallel.
Note, however, that the distribution of the orbitals employed in the self-
energy part significantly differs from the one used to compute the screening.
In what follows, we briefly describe the two-step procedure used to distribute
the wavefunctions:

1. Each node reads and stores in memory the states where the QP corrections are computed
   (the list of states specified by [[kptgw]] and [[bdgw]]).

2. The [[nband]] bands are distributed using the following partition scheme:

   ![](paral_mbt_assets/band_distribution_sigma.png)

   where we have assumed a calculation done with four nodes (the index in the box
   denotes the MPI rank).

By virtue of the particular distribution adopted, the computation of the
correlation part is expected to scale well with the number CPUs. The maximum
number of processors that can be used is limited by [[nband]]. Note, however,
that only a subset of processors will receive the occupied states when the
bands are distributed in step 2. As a consequence, the theoretical maximum
speedup that can be obtained in the exchange part is limited by the
availability of the occupied states on the different MPI nodes involved in the run.

The best-case scenario is when the QP corrections are wanted for all the
occupied states. In this case, indeed, each node can compute part of the self-
energy and almost linear scaling should be reached. The worst-case scenario is
when the quasiparticle corrections are wanted only for a few states (e.g. band
gap calculations) and NCPU >> Nvalence. In this case, indeed, only Nvalence
processors will participate to the calculation of the exchange part.

To summarize: The MPI computation of the correlation part is efficient when
the number of processors divides **nband**. Optimal scaling in the exchange
part is obtained only when each node possesses the full set of occupied states.

The two figures below show the speedup of the sigma part as function of the
number of processors. The self-energy is calculated for 5 quasiparticle states
using nband=1024 (205 occupied states). Note that this setup is close to the
worst-case scenario. The computation of the self-energy matrix elements
(csigme) scales well up to 64 processors. For large number number of CPUs, the
scaling departs from the linear behavior due to the unbalanced distribution of
the occupied bands. The non-scalable parts of the implementation
(init1, rdkss) limit the total speedup due to Amdhal's law.

![](paral_mbt_assets/sigma_analysis.png)

The implementation presents good memory scalability since the largest arrays
are distributed. Only the size of the screening does not scale with the number
of nodes. By default each CPU stores in memory the entire screening matrix for
all the q-points and frequencies in order to optimize the computation. In the
case of large matrices, however, it possible to opt for an out-of-core
solution in which only a single q-point is stored in memory and the data is
read from the external SCR file (slower but less memory demanding). This
option is controlled by the first digit of [[gwmem]].

Now that we know how distribute the load efficiently, we can finally run the
calculation using

    (mpirun ...) abinit tmbt_4.abi > tmbt_4.log 2> err &

Keep track of the time for each processor number so that we can test the
scalability of the self-energy part.

Please note that the results of these tests are not converged. A well
converged calculation would require a 6x6x6 k-mesh to sample the full BZ, and
a cutoff energy of 10 Ha for the screening matrix. The QP results converge
extremely slowly with respect to the number of empty states. To converge the
QP gaps within 0.1 eV accuracy, we had to include 1200 bands in the screening
and 800 states in the calculation of the self-energy.

The comparison between the LDA band structure and the G<sub>0</sub>W<sub>0</sub> energy bands of
α-quartz SiO<sub>2</sub> is reported in the figure below. The direct gap at Γ is opened
up significantly from the LDA value of 6.1 eV to about 9.4 eV when the one-
shot G<sub>0</sub>W<sub>0</sub> method is used. You are invited to reproduce this result (take into
account that this calculation has been performed at the theoretical LDA
parameters, while the experimental structure is used in all the input files of
this tutorial).

![](paral_mbt_assets/SiO2_KSG0W0_PPM1.png)

## 5 Basic rules for efficient parallel calculations

1. Remember that "Anything that can possibly go wrong, does". 
   So, when writing your input file, try to "Keep It Short and Simple".

2. Do one thing and do it well:
  Avoid using different values of [[optdriver]] in the same input file. Each
  runlevel employs different approaches to distribute memory and CPU time, hence
  it is almost impossible to find the number of processors that will produce a
  balanced run in each dataset.

3. Prime number theorem:
  Convergence studies should be executed in parallel only when the parameters
  that are tested do not interfere with the MPI algorithm. For example, the
  convergence study in the number of bands in the screening should be done in
  separated input files when [[gwpara]]=2 is used.

4. Less is more:
  Split big calculations into smaller runs whenever possible. For example,
  screening calculations can be split over q-points. The calculation of the
  self-energy can be easily split over [[kptgw]] and [[bdgw]].

5. Look before you leap:
  Use the converge tests to estimate how the CPU-time and the memory
  requirements depend on the parameter that is tested. Having an estimate of the
  computing resources is very helpful when one has to launch the final
  calculation with converged parameters.
---
authors: MVer
---

# Electron-phonon tutorial

## Electron-Phonon interaction and superconducting properties of Al.

This tutorial demonstrates how to obtain the following physical properties, for a metal:

  * the phonon linewidths (inverse lifetimes) due to the electron-phonon interaction
  * the Eliashberg spectral function
  * the electron-phonon coupling strength
  * the McMillan critical temperature
  * the resistivity and electronic part of the thermal conductivity

Here you will learn to use the electron-phonon coupling part of the ANADDB
utility. This implies a preliminary calculation of the electron-phonon matrix
elements and phonon frequencies and eigenvectors, from a standard ABINIT
phonon calculation, which will be reviewed succinctly.

Note that this tutorial covers a legacy version of the electron-phonon calculation.
A new, more efficient workflow is presented in Tutorials [Introduction to the new EPH workflow](/tutorial/eph_intro).
The old workflow is still useful because it is compatible with spinors (spin orbit coupling), PAW, and
therefore also DFT+U, but it will be deprecated at some time in the future.

Visualisation tools are NOT covered in this tutorial.
Powerful visualisation procedures have been developed in the Abipy context,
relying on matplotlib. See the README of [Abipy](https://github.com/abinit/abipy)
and the [Abipy tutorials](https://github.com/abinit/abitutorials).

This tutorial should take about 1 hour.

[TUTORIAL_README]

## 1 Calculation of the ground state and phonon structure of fcc Al

*Before beginning, you might consider making a different subdirectory to work
in. Why not create Work_eph in \$ABI_TESTS/tutorespfn/Input?*

It is presumed that the user has already followed the Tutorials [RF1](/tutorial/rf1) and [RF2](/tutorial/rf2),
and understands the calculation of ground state and response (phonon using density-functional
perturbation theory (DFPT)) properties with ABINIT.

Copy the file *teph_1.abi* to your working directory. It contains in particular file names and root names 
for the first run (GS+perturbations).

```sh
cd $ABI_TESTS/tutorespfn/Input
mkdir Work_eph
cd Work_eph
cp ../teph_1.abi .
```

{% dialog tests/tutorespfn/Input/teph_1.abi %}

You can immediately start this run - the input files will be examined later...

    abinit teph_1.abi > log 2> err &

### Dataset Structure and Flow

The *teph_1.abi* file contains a number of datasets (DS). These will perform the
ground state (DS 1), then the phonon perturbations (DS 2-4), for a cell of FCC
aluminium. The DDK perturbation (DS 5) is also calculated, and will be used to
obtain the Fermi velocities for the transport calculations in Section 6.

Once these are done, abinit calculates the wave functions on the full grid of
k-points (using [[kptopt]]=3) in DS 6: these will be used to calculate the
electron-phonon matrix elements. In a full calculation the density of k-points
should be increased significantly here and for the following datasets. For DS
1-5 only the normal convergence of the phonon frequencies should be ensured.
In DS 7-10 only the matrix elements are calculated, for the electron-phonon
coupling and for the DDK (position/momentum matrix elements), on the dense and
complete grid of k-points from DS 6. Note that the separation of the matrix
element calculation is new from version 7.6.

The important variable for electron-phonon coupling calculations is [[prtgkk]]
This prints out files suffixed GKK, which contain the electron-phonon matrix
elements, and are calculated by DS 7-9. The matrix elements only depend on the self-consistent perturbed
density (files suffixed 1DENx), which we get from DS 2-4 (linked by variable
[[get1den]]). The GKK can therefore be calculated on arbitrarily dense k-point
meshes chosen in DS 6. Even better, only a single SCF step is needed, since no self-consistency
is required. To enforce the calculation of all the matrix elements on all
k-points, symmetries are disabled when [[prtgkk]] is set to 1, so be sure *not*
to use it during the normal self-consistent phonon runs DS 2-4. Again this is
very different from versions before 7.6.

### Convergence

The calculation is done using minimal values of a number of parameters, in
order to make it tractable in a time appropriate for a tutorial. The results
will be completely unconverged, but by the end of the tutorial you should know
how to run a full electron phonon calculation, and be able to improve the
convergence on your own.

Edit the file *teph_1.abi*. We now examine several variables. The kinetic energy
cutoff [[ecut]] is a bit low, and the number of k-points (determined by
[[ngkpt]]) is much too low. Electron-phonon calculations require a very
precise determination of the Fermi surface of the metal. This implies a very
dense k-point mesh, and the convergence of the grid must be checked. In our
case, for Al, we will use a (non-shifted) 6x6x6 k-point grid, but a converged
calculation needs more than 16x16x16 points. This will be re-considered in
section 5. The q-point grid will be 2x2x2, and is selected by the [[ngqpt]], 
qshift, nqshift, and [[qptopt]] variables. It must be a sub-grid of the full
k-point grid, and must contain the Γ point. For the phonon and GKK datasets,
the q-point is selected by the variable [[iqpt]]: this guarantees that there are
no errors in transcription of q coordinates, and simplifies workflows. The only
unknown ahead of time is the total number of q (and hence datasets) in the irreducible
wedge. One way to check this is to set iqpt to a very large value: the error message
gives you the size of the irreducible set.

The value of [[acell]] is fixed to a rounded value from experiment.
It, too, should be converged to get physical results (see [Tutorial 3](/tutorial/base3)).

Note that the value of 1.0E-14 for [[tolwfr]] is tight, and should be even
lower (down to 1.0E-20 or even 1.0E-22) for accurate results. This is because the wavefunctions
will be used later explicitly in the matrix elements for ANADDB, as opposed to
only energy values or densities, which are averages of the wavefunctions and
eigenenergies over k-points and bands. Electron-phonon quantities are delicate
sums of a few of these small matrix elements (those near the Fermi surface),
so each matrix element must be accurate. You can however set [[prtwf]] to 0 in
the phonon calculations, and avoid saving huge perturbed wavefunction files to
disk (you only need to keep the ground state wave functions, with prtwf1 1).

### Execution

Run the first input (a few seconds on a recent PC), and you should obtain a value of

     etotal -2.3076512079E+00 Ha

for the energy at the end of DATASET 1. The following datasets calculate the
second order energy variations under atomic displacement in the three reduced
directions of the fcc unit cell. This is done for three different phonons,
Gamma, (1/2,0,0), and X=(1/2,1/2,0), which generate the 2x2x2 q-point grid
(take care with the reduced coordinates of the reciprocal space points! They
are not along cartesian directions, but along the reciprocal space lattice
vectors.). The whole calculation follows the same lines as [Tutorial RF1](/tutorial/rf1).
As an example, DATASET 3 calculates the perturbed
wavefunctions at k+q, for k in the ground state k-point mesh, and q=(1/2,0,0).
Then, DATASET 3 calculates

     2DTE 6.9410188336E-01 Ha

for the second-order energy variation for movement of the (unique) atom along
the first reduced direction for q=(1/2,0,0). The main differences with [Tutorial RF1](/tutorial/rf1)
are that Given we are dealing with a metal, no
perturbation wrt electric fields is considered ; However, if you want to do
transport calculations, you need the ddk calculation anyway, to get the
electron band velocities. This is added in dataset 5. In the standard case,
ABINIT uses symmetry operations and non-stationary expressions to calculate a
minimal number of 2DTE for different mixed second derivatives of the total
energy. In our case we use the first derivatives, and they must all be calculated explicitly.

You are now the proud owner of 9 first-order matrix element files (suffixed
\_GKKx, and corresponding copies in netcdf format \_GKKx.nc), 
corresponding to the three directional perturbations of the atom at
each of the three q-points. The \_GKK files contain the matrix elements of the
electron-phonon interaction, which we will extract and use in the following.
Besides the \_GKK files there are the \_DDB files for each perturbation which
contain the 2DTE for the different phonons wavevectors q.

## 2 Merging of the 2DTE DDB files using MRGDDB

You can copy the following content to a file *teph_2.abi* within your working directory:

    teph_2.ddb.out
    Total ddb for Al FCC system
    3
    teph_1o_DS2_DDB
    teph_1o_DS3_DDB
    teph_1o_DS4_DDB

or use

{% dialog tests/tutorespfn/Input/teph_2.abi %}

This is your input file for the [[help:mrgddb|MRGDDB]] utility, which will
take the different \_DDB files and merge them into a single one which ANADDB
will use to determine the phonon frequencies and eigenvectors. *teph_2.abi*
contains the name of the final file, a comment line, then the number of *_DDB*
files to be merged and their names.
*mrgddb* is run with the command

     mrgddb < teph_2.abi

It runs in a few seconds.

## 3 Extraction and merging of the electron-phonon matrix elements using MRGGKK

A merge similar to that in the last section must be carried out for the
electron-phonon matrix elements. This is done using the MRGGKK utility, and
its input file is *\$ABI_TESTS/tutorespfn/Input/teph_3.abi*, shown below

    teph_3o_GKK.bin   # Name of output file
    0                    # binary (0) or ascii (1) output
    teph_1o_DS1_WFK   # GS wavefunction file
    0  9 9               # number of 1WF files, of GKK files, and of perturbations in the GKK files
    teph_1o_DS2_GKK1  # names of the 1WF then the (eventual) GKK files
    teph_1o_DS2_GKK2
    ...

or use

{% dialog tests/tutorespfn/Input/teph_3.abi %}

The matrix element sections of all the \_GKK files will be extracted and
concatenated into one (binary) file, here named *teph_3o_GKK.bin*. The following
lines in *teph_3.abi* give the output format (0 for binary or 1 for ascii), then
the name of the ground state wavefunction file. The fourth line contains 3
integers, which give the number of \_1WF files (which can also be used to
salvage the GKK), the number of \_GKK files, and the number of perturbations in
the \_GKK files. Thus, MRGGKK functions very much like [help:mrgddb|MRGDDB], and can merge \_GKK
files which already contain several perturbations (q-points or atomic
displacements). Finally, the names of the different \_1WF and \_GKK files are listed.

MRGGKK will run on this example in a few seconds. In more general cases, the
runtime will depend on the size of the system, and for a large number of bands
or k-points can extend up to 20 minutes or more.

## 4 Basic ANADDB calculation of electron-phonon quantities

The general theory of electron-phonon coupling and Eliashberg
superconductivity is reviewed in [[cite:Allen1983a|Theory of Superconducting Tc]],
by P.B. Allen and B. Mitrovic.
The first implementations similar to that in ABINIT are those in [[cite:Savrasov1996]] and [[cite:Liu1996]].

File *\$ABI_TESTS/tutorespfn/Input/teph_4.abi* contains the input needed by
ANADDB to carry out the calculation of the electron-phonon quantities.

{% dialog tests/tutorespfn/Input/teph_4.abi %}

ANADDB has file path variables, just like ABINIT, which tells it where to find the input,
ddb, and gkk files, and what to name the output, thermodynamical output, and
electron phonon output files. 

The new variables are at the head of the file:

    # turn on calculation of the electron-phonon quantities
    elphflag 1

    # Path in reciprocal space along which the phonon linewidths
    #  and band structure will be calculated
    nqpath 7
    qpath
     0.0 0.0 0.0
     1/2 1/2 0.0
     1   1   1
     1/2 1/2 1/2
     1/2 1/2 0.0
     1/2 3/4 1/4
     1/2 1/2 1/2

    # Coulomb pseudopotential parameter
    mustar 0.136

[[anaddb:elphflag]] is a flag to turn on the calculation of the electron-
phonon quantities. The first quantities which will be calculated are the
phonon linewidths along a path in reciprocal space (exactly like the band
structure in [the tutorial 3](/tutorial/base3). The path is specified by the
variable [[anaddb:qpath]] giving the apexes of the path in reciprocal space,
which are usually special points of high symmetry. The number of points is
given by [[anaddb:nqpath]]. Note that qpath can be used in normal phonon band
structure calculations as well, provided that [[anaddb:qph1l]] is omitted from
the input file (the latter overrides qpath). The phonon linewidths are printed
to a file suffixed \_LWD. The phonon band structure is shown below, and is already
stable with a 6x6x6 k-point grid.

![Phonon band structure of Al](eph_assets/Al_phonon_BS.png)

The phonon linewidths are proportional to the electron phonon coupling, and
still depend on the phonon wavevector q. The other electron-phonon
calculations which are presently implemented in ANADDB, in particular for
superconductivity, determine isotropic quantities, which are averaged over the
Fermi surface and summed over q-points. Integrating the coupling over
reciprocal space, but keeping the resolution in the phonon mode's energy, one
calculates the Eliashberg spectral function $\alpha^2F$. The $\alpha^2F$ function is similar
to the density of states of the phonons, but is weighted according to the
coupling of the phonons to the electrons. It is output to a file with suffix
\_A2F, which is ready to be represented using any graphical software (Xmgrace,
matlab, OpenDX...). The first inverse moment of $\alpha^2F$ gives the global coupling
strength, or mass renormalization factor, $\lambda$. From $\lambda$, using the [[cite:McMillan1968|McMillan formula]]
as modified by [[cite:Allen1975|Allen and Dynes]]
ANADDB calculates the critical temperature for superconductivity.
The formula contains an adjustable
parameter $\mu^{\star}$ which approximates the effect of Coulomb interactions, and is
given by the input variable [[anaddb:mustar]]. For Al with the k-point grid
given and a value of $\mu^{\star}$=0.136 the ANADDB output file shows the following values

     mka2f: lambda <omega^2> =     1.179090E-07
     mka2f: lambda <omega^3> =     1.575413E-10
     mka2f: lambda <omega^4> =     2.212559E-13
     mka2f: lambda <omega^5> =     3.213830E-16
     mka2f: isotropic lambda =     8.542685E-02
     mka2f: omegalog  =     1.038371E-03 (Ha)     3.278912E+02 (Kelvin)
     mka2f: input mustar =     1.360000E-01
    -mka2f: MacMillan Tc =     2.645390E+05 (Ha)     8.353470E+10 (Kelvin)

As could be expected, this is a very bad estimation of the experimental value of 1.2
K. The coupling strength is severely underestimated (experiment gives 0.44),
and the logarithmic average frequency is a bit too high (converged value of 270 K in [[cite:Savrasov1996|Savrasov]]).
The resulting critical temperature from McMillan's formula is unphysical, and you
should always regard it critically: if $\lambda$ is close to or lower than the chosen value of
$\mu^{\star}$, $T_c$ will diverge. 
Aluminum is a good case in which things can be improved easily,
because its Fermi surface is large and isotropic and the coupling is weak.

## 5 Convergence tests of the integration techniques

In section 4, we used the default method for integration on the Fermi surface,
which employs a smearing of the DOS and attributes Gaussian weights to each
k-point as a function of its distance from the Fermi surface. Another
efficient method of integration in k-space is the tetrahedron method, which is
also implemented in ANADDB, and can be used by setting [[anaddb:telphint]] =
0. In this case the k-point grid must be specified explicitly in the input,
repeating the variable [[anaddb:kptrlatt]] from the ABINIT output, so that
ANADDB can re-construct the different tetrahedra which fill the reciprocal
unit cell. In the Gaussian case, the width of the smearing can be controlled
using the input variable [[anaddb:elphsmear]].

To test our calculations, they should be re-done with a denser k-point grid
and a denser q-point grid, until the results ($\alpha^2F$ or $\lambda$) are converged. The
value of [[anaddb:elphsmear]] should also be checked, to make sure that it
does not affect results. Normally, the limit for a very small
[[anaddb:elphsmear]] and a very dense k-point grid is the same as the value
obtained with the tetrahedron method (which usually converges with a sparser k-point grid).

Edit input file *\$ABI_TESTS/tutorespfn/Input/teph_5.abi* and you will see the
main difference with teph_4.abi is the choice of the tetrahedron integration method.

{% dialog tests/tutorespfn/Input/teph_5.abi %}

Already at this level, you can see the increased accuracy: the isotropic $\lambda$
values (around 0.50) and the MacMillan $T_c$ (2.2 Kelvin) are much more realistic.

If you are patient, save the output \_LWD and \_A2F files and run the
full tutorial again with a denser k-point grid (say, 12x12x12) and you will be able
to observe the differences in convergence. For Al the $\alpha^2F$ function and related
quantities converge for a minimal k-point grid of about 16x16x16.

## 6 Transport quantities within Boltzmann theory

The electron-phonon interaction is also responsible for the resistivity of
normal metals and related phenomena. Even in a perfect crystal, interaction
with phonons will limit electron life times (and vice versa). This can be
calculated fairly simply using the Boltzmann theory of transport with first
order scattering by phonons (see, e.g., [[cite:Ziman1960|"Electrons and Phonons" by Ziman]]).

The additional ingredient needed to calculate transport quantities (electrical
resistivity, heat conductivity limited by electron-phonon coupling) is the
Fermi velocity, i.e. the group velocity of a wavepacket of electrons placed at
the Fermi surface. This is the "true" velocity the charge will move at, once
you have displaced the Fermi sphere a little bit in k space (see, e.g.
[[cite:Ashcroft1976|Ashcroft and Mermin]]). The velocity can be related simply to a
commutator of the position, which is also used for dielectric response, using
a DDK calculation (see [the first DFPT tutorial (DFPT1)](/tutorial/rf1).
The phonon calculation at Gamma need not include the electric field (this is a metal after all, so the effect on the
phonons should be negligible), but we need an additional dataset to calculate
the 3 DDK files along the 3 primitive directions of the unit cell. To be more
precise, just as for the el-ph matrix elements, we do not need the perturbed
wavefunctions, only the perturbed eigenvalues. Calculating the DDK derivatives
with [[prtgkk]] set to 1 will output files named \_GKKxx (xx=3 *natom* + 1 to
3 *natom* + 3) containing the matrix elements of the ddk perturbation (these are
basically the first part of the normal DDK files for E field perturbation,
without the wave function coefficients).

The ANADDB input must specify where the ddk files are, so ANADDB can
calculate the Fermi velocities. The variable ddk_filepath points to a 
small file listing the 3 DDK files to be used, whose contents in our case are:

    teph_1_DS5_GKK4
    teph_1_DS5_GKK5
    teph_1_DS5_GKK6

The abinit input file *teph_1.abi* already obtained the DDK files from the
additional dataset, DS5, with the following lines of *teph_1.abi*:

    tolwfr5 1.0d-14
    iqpt5 1
    rfphon5 0
    rfelfd5 2
    prtwf5   0

You can copy the additional .ddk file from the tests/tutorespfn/Inputs directory, and
run ANADDB. The input for *teph_6.abi* has added to *teph_5.abi* the following 2 lines:

    ifltransport 1
    ep_keepbands 1

see

{% dialog tests/tutorespfn/Input/teph_6.abi %}

and has produced a number of additional files:

  * *_A2F_TR* contain the equivalent Eliashberg spectral functions with Fermi speed factors (how many phonons do we have at a given energy, how much do they couple with the electrons, and how fast are these electrons going). Integrating with appropriate functions of the phonon energy, one gets:
  * the resistivity as a function of temperature (teph_6.out_ep_RHO and figure)
  * the thermal conductivity as a function of temperature (teph_6.out_ep_WTH) but ONLY the electronic contribution. You are still missing the phonon-phonon interactions, which are the limiting factor in the thermal conductivity beyond a few 100 K. For metals at even higher temperature the electrons will often dominate again as they contain more degrees of freedom.

![Resistivity of Al as a function of T](eph_assets/teph_6_RHO.png)

The high T behavior is necessarily linear if you include only first order e-p
coupling and neglect the variation of the GKK off of the Fermi surface. The
inset shows the low T behavior, which is not a simple polynomial (with textbook
models it should be T$^3$ or T$^5$ - see [[cite:Ashcroft1976|Ashcroft and Mermin]]). See the [[cite:Savrasov1996|Savrasov paper]]
for reference values in simple metals using well converged k- and q- point grids, here we are missing a factor of 2, which is not
bad given how crude the grids are.

Finally, note that the \_RHO and \_WTH files contain a series of tensor
components, for the resistivity tensor (2 1 = y x or the current response
along y when you apply an electric field along x). In many systems the tensor
should be diagonal by symmetry, and the value of off-diagonal terms gives an
estimate of the symmetrization error (tiny). Similarly the difference in the diagonal terms
is due to numerical convergence: here is is a few per-mil, visible in the figure 
between green and red lines (yy and xx components).
---
authors: MS, MT
---

# Third tutorial on the Projector Augmented-Wave (PAW) technique

## Testing PAW datasets against an all-electron code

This tutorial will demonstrate how to test a generated PAW dataset against an
all-electron code. We will be comparing results with the open `Elk` FP-LAPW code
(a branch of the `EXCITING` code) available under GPLv3.

You will learn how to compare calculations of the _equilibrium lattice_
parameter, the _Bulk modulus_ and the _band structure_ between `ABINIT` PAW results
and those from the `Elk` code.
It is assumed you already know how to use `ABINIT` in the PAW case. The tutorial
assumes no previous experience with the `Elk` code, but it is strongly advised
that the users familiarise themselves a bit with this code before attempting
to do similar comparisons with their own datasets.

This tutorial should take about 3h-4h.

[TUTORIAL_README]

## 1. Introduction - the quantities to be compared

When comparing results between all-electron and pseudopotential codes, it is
usually **impossible to compare total energies**. This is because the total energy
of an all-electron code includes the contribution from the _kinetic energy_ of
the _core orbitals_, while in the pseudopotential approach, the only information
that is retained is the density distribution of the frozen core. This is
typically so even in a PAW implementation.

Differences in total energies should be comparable, but calculating these to a
given accuracy is usually a long and cumbersome process. However, some things
can be calculated with relative ease. These include structural properties -
such as the equilibrium lattice parameter(s) and the bulk modulus - as well as
orbital energies, i.e. the band structure for a simple bulk system.

!!! Note
    We are here aiming to compare the results under similar numerical
    conditions. That does not necessarily mean that the calculations can be
    compared with experimental results, nor that the results of the calculations
    individually represent the absolutely most converged values for a given
    system. However, to ensure that the numerical precision is equivalent, we must
    take care to:

    * Use the same (scalar-relativistic) exchange-correlation functional.
    * Match the `Elk` muffin-tin radii and the PAW cutoff radii.
    * Use a k-point grid of similar quality.
    * Use a similar cutoff for the plane wave expansion.
    * Freeze the core in the `Elk` code (whenever possible), to match the frozen PAW core in `ABINIT`.
    * Use a similar atomic on-site radial grid.

We will use **Carbon**, in the **diamond structure**, as an example of a simple solid
with a band gap, and we will use **Magnesium** as an example of a metallic solid.
Naturally, it is important to keep things as simple as possible when
benchmarking PAW datasets, and there is a problem when the element has no
naturally occurring pure solid phase. For elements which are molecular in
their pure state (like Oxygen, Nitrogen and so forth), or occur only in
compound solids, one solution is to compare results on a larger range of
solids where the other constituents have already been well tested. For
instance, for oxygen, one could compare results for ZnO, MgO and MnO, provided
that one has already satisfied oneself that the datasets for Zn, Mg, and Mn in
their pure forms are good.

One could also compare results for molecules, and we encourage you to do this
if you have the time. However, doing this consistently in `ABINIT` requires a
supercell approach and would make this tutorial very long, so we shall not do
it here. We will now discuss the prerequisites for this tutorial.

## 2. Prerequisites, code components and scripts

It is assumed that you are already familiar with the contents and procedures
in tutorials [PAW1](/tutorial/paw1) and [PAW2](/tutorial/paw2), and so
have some familiarity with input files for  `ATOMPAW` , and the issues in creating
PAW datasets. To exactly reproduce the results in this tutorial, you will
need:

  * The `ATOMPAW` code for generating PAW datasets (see tutorial [PAW2](/tutorial/paw2), section **2.**).
  
```sh
	   cd atompaw-4.x.y.z
	   mkdir build
	   cd build
	   ../configure
	   make
```
  
  * the `Elk` code (this tutorial was designed with v1.2.15),
    available [here](https://sourceforge.net/projects/elk/files/).
    We will use the `Elk` code itself, as well as its `eos` (equation-of-state) utility,
    for calculating equilibrium lattice parameters.

  * Auxiliary `bash` and `python` _scripts_ for the comparison of band structures, available in the directory
    *\$ABI_HOME/doc/tutorial/paw3_assets/scripts/*. For the `python` scripts, `python3` is required.
    There are also various *gnuplot* scripts there.

You will of course also need a working copy of `ABINIT`. Please make sure that
the above components are downloaded and working on your system before
continuing this tutorial. The tutorial also makes extensive use of |gnuplot|, so
please also ensure that a recent and working version is installed on your system.

!!! Note
    By the time that you are doing this tutorial there will probably be
    newer versions of all these programs available. It is of course better to use
    the latest versions, and we simply state the versions of codes used when this
    tutorial was written so that specific numerical results can be reproduced if necessary.

## 3. Carbon (diamond)

### 3.1. Carbon - a simple datatset

Make a _working directory_ for the `ATOMPAW` generation (you could call it
`C_atompaw`) and copy the file: [C_simple.input](paw3_assets/inputs/C_simple.input) to it.

{% dialog tutorial/paw3_assets/inputs/C_simple.input %}

Then go there and run `ATOMPAW` by typing (assuming that you have set things up so that you
can run atompaw by just typing `atompaw`):

    atompaw < C_simple.input

The code should run, and if you do an `ls`, the contents of the directory will be something like:

     C                        density     logderiv.2  potSC1   tprod.2  wfn2    wfnSC1
     C.LDA-PW-paw.xml         dummy       NC          rvf      vloc     wfn3
     C.LDA-PW-paw.xml.corewf  fort.24     OCCWFN      rVx      wfn001   wfn4
     compare.abinit           logderiv.0  potAE0      tau      wfn002   wfn5
     C_simple.input           logderiv.1  potential   tprod.1  wfn1     wfnAE0


There is a lot of output, so it is useful to work with a graphical overview.
Copy the `gnuplot` _script_ [plot_C_all.p](paw3_assets/scripts/plot_C_all.p) to your folder.
Open a new terminal window by typing `xterm &`, and run `gnuplot` in the new
terminal window. At the `gnuplot` command prompt type:

    gnuplot> load 'plot_C_all.p'

You should get a plot that looks like this:

![Plot of atompaw outputs](paw3_assets/images/plot_C_all_1.png)

You can now keep the `gnuplot` terminal and plot window open as you work, and if
you change the `ATOMPAW` input file and re-run it, you can update the plot by
retyping the `load..` command. The `gnuplot` window plots the essential
information from the `ATOMPAW` outputs, the logarithmic derivatives, (the
derivatives of the dataset are green), the wavefunctions and projectors for
each momentum channel (the full wavefunction is in red, the PW part is green,
and the projector is blue) as well as the local potential. Finally, it shows the Fourier transform
of the projector products (the x-axis is in units of Ha).

The inputs directory also contains scripts for plotting these graphs
individually, and you are encouraged to test and modify them. We can look
inside the `C_simple.input` file:

{% dialog tutorial/paw3_assets/inputs/C_simple.input %}

Here we see that the current dataset is very simple, it has no basis states
beyond the $2s$ and $2p$ occupied valence states in carbon. It is thus not
expected to produce very good results, since there is almost no flexibility in
the PAW dataset. Note that the `scalarrelativistic` option is turned on. While
this is not strictly necessary for such a light atom, we must always ensure to
have this turned on if we intend to compare with results from the `Elk` code.

We will now run basic convergence tests in `ABINIT` for this dataset. The
dataset file for `ABINIT` has already been generated (it is the `C.LDA-PW-paw.xml`
file in the current directory). Make a new subdirectory for the
test in the current directory (you could call it `abinit_test` for instance), go
there and copy the file: [ab_C_test.abi](paw3_assets/inputs/ab_C_test.abi) into
it. This `ABINIT` input file contains several datasets which increment the `ecut`
input variable, and perform ground state and band structure calculations for
each value of `ecut`. This is thus the internal `ABINIT` convergence study. Any
dataset is expected to converge to a result sooner or later, but that does not
mean that the final result is accurate, unless the dataset is good. The goal
is of course to generate a dataset which both converges quickly and is very accurate.  

{% dialog tutorial/paw3_assets/inputs/ab_C_test.abi %}

The `ab_C_test.abi` file contains:

    pseudos="C.LDA-PW-paw.xml"

    pp_dirpath="../"
    outdata_prefix="outputs/ab_C_test_o"
    tmpdata_prefix="outputs/ab_C_test"

So it expects the newly generated dataset to be in the directory above.
Also, to keep things tidy, it assumes the outputs will be put in a subdirectory
called `outputs/`. Make sure to create it before you start the `ABINIT` run by writing:

    mkdir outputs

!!! important
    You may have to change the path to reach the Psps_for_tests repository. For this, modify the varaible `pp_dirpath` in the input file. 

You can now run the `ABINIT` tests (maybe even in a separate new `xterm` window), by executing:

    abinit ab_C_test.abi >& log_C_test

There are 18 double-index datasets in total, with the first index running from
1 to 9 and the second from 1 to 2. You can check on the progress of the
calculation by issuing `ls outputs/`. When the `.._o_DS92..` files appear, the
calculation should be just about finished. While the calculation runs you
might want to take a look in the input file. Note the lines pertaining to the
increment in `ecut` (around line 29):

     ...
     #Cutoff variables
     ecut:? 5.0
     ecut+?  5.0
     `pawecutdg` 110.0
     ecutsm 0.5
     ...

ecut is increased in increments of 5 Ha from an initial value of 5, to a final
ecut of 45 Ha. Note that `pawecutdg` is kept fixed, at a value high enough to be
expected to be good for the final value of `ecut`. In principle, a convergence
study of `pawecutdg` should be performed as well, once a good value of `ecut` has been found.

We can now check the basic convergence attributes of the dataset. The
convergence of the total energy is easily checked by issuing some `grep` commands:

     grep 'etotal' ab_C_test.abo

This should give you an output similar to this (though not the text to the left):

                                         etotal   (ecut)
     etotal11 -1.0972441353E+01
     etotal21 -1.1443217299E+01      - 470.78 mHa (10 Ha)
     etotal31 -1.1507234681E+01      -  64.02 mHa (15 Ha)
     etotal41 -1.1517560274E+01      -  10.33 mHa (20 Ha)
     etotal51 -1.1518066800E+01      -   0.51 mHa (25 Ha)
     etotal61 -1.1518201844E+01      -   0.14 mHa (30 Ha)
     etotal71 -1.1518427967E+01      -   0.23 mHa (35 Ha)
     etotal81 -1.1518542165E+01      -   0.11 mHa (40 Ha)
     etotal91 -1.1518570692E+01      -   0.03 mHa (45 Ha)

Your values might differ slightly in the last decimals. The calculation of
diamond with the current PAW Carbon dataset converged to a precision of the
total energy below 1 mHa for a cutoff of about 25 Ha (this is not particularly
good for a PAW dataset). Also, the convergence is a bit jumpy after an `ecut` of
about 25 Ha, which is an indication of a) that the number of projectors per
angular momentum channel is low, and  b) that other parameters apart from ecut
dominate convergence beyond this point.

If we turn to the _band structure_, we can use the script
[comp_bands_abinit2abinit.py](paw3_assets/scripts/comp_bands_abinit2abinit.py)
to check the convergence of the band structure. Copy the script to the
directory where the `ABINIT` input file is and issue:

     python comp_bands_abinit2abinit.py outputs/ab_C_test_o_DS12_EIG outputs/ab_C_test_o_DS92_EIG eV

This will print a long series of columns and at the end you will see:

     ...
     #        nvals:   280
     # average diff:     1.758809  eV
     # minimum diff:    -4.437905  eV
     # maximum diff:     1.089000  eV
     #
     # NOTE: `ABINIT` values are read in fixed format with five decimal
     #       places. For low values, four or three decimal figures
     #       may be the highest precision you can get.

This provides you with some statistics of the difference in the band energies.
Specifically this is the average difference between a the band structure
calculated at an `ecut` of 5 Ha (in dataset 12) and another at an `ecut` of 45 Ha (in dataset 92).

The differences between these datasets are naturally very large, about 1.8 eV
on average, because the band-structure of the first dataset is far from
converged. The columns output before the statistics are arranged so that if
you pipe the output to a data file:

     python comp_bands_abinit2abinit.py outputs/ab_C_test_o_DS12_EIG outputs/ab_C_test_o_DS92_EIG eV > bands_5Ha_vs_45Ha.dat

you can plot the two band structures in `gnuplot` directly, by entering:

     gnuplot> plot 'bands_5Ha_vs_45Ha.dat' u 1:2 w lp title '5 Ha', 'bands_5Ha_vs_45Ha.dat' u 1:3 w lp title '45 Ha'

This should furnish you with a graph that looks something like this:

![Comparison of bands 5 vs. 45 Ha](paw3_assets/images/plot_C_band_1.png)

Not surprisingly, the band structures are very different. However, a search
through the datasets of increasing index (i.e. DS22, DS32, DS42, ...) yields
that for dataset 42, i.e with an `ecut` of 20 Ha, we are already converged to a
level of 0.01 eV. Issuing the command:

     python comp_bands_abinit2abinit.py outputs/ab_C_test_o_DS42_EIG outputs/ab_C_test_o_DS92_EIG eV > bands_20Ha_vs_45Ha.dat

and plotting this with:

     gnuplot> plot 'bands_20Ha_vs_45Ha.dat' u 1:2 w lp title '20 Ha', 'bands_20Ha_vs_45Ha.dat' u 1:3 w lp title '45 Ha'

Should give you a plot similar to this:

![Comparison of bands 20 vs. 45 Ha](paw3_assets/images/plot_C_band_2.png)

You can convince yourself by zooming in that the band structures are very
similar. The statistics at the end of the bands_20Ha_vs_45Ha.dat  file shows
that we are converged within abinit:

    ...
    #        nvals:   280
    # average diff:     0.003808  eV
    # minimum diff:    -0.008980  eV
    # maximum diff:     0.000272  eV
    ...


### 3.2. Carbon - calculating the equilibrium lattice parameter

That we have converged the dataset on its own does of course not mean that the
dataset is good, i.e. that it reproduces the same results as an all-electron
calculation. To independently verify that the dataset is good, we need to
calculate the equilibrium lattice parameter (and the _Bulk modulus_) and compare
this and the band structure with an `Elk` calculation.

First, we will need to calculate the total energy of diamond in `ABINIT` for a
number of lattice parameters around the minimum of the total energy. There is
example input file for doing this at:
[ab_C_equi.abi](paw3_assets/inputs/ab_C_equi.abi).
The new input file has ten datasets which increment the lattice parameter, alatt,
from 6.1 to 7.0 Bohr in steps of 0.1 Bohr. A look in the input file will tell
you that `ecut` is set to 25 Hartrees. Copy these to your abinit_test directory and run:

{% dialog tutorial/paw3_assets/inputs/ab_C_equi.abi %}

    abinit ab_C_equi.abi >& log_C_equi

The run should be done fairly quickly, and when it's done we can check on the
volume and the total energy by using "grep"

     grep 'volume' log_C_equi

and

     grep 'etotal' log_C_equi

The outputs should be something like this:

     ...
     Unit cell volume ucvol=  5.6745250E+01 bohr^3
     Unit cell volume ucvol=  5.9582000E+01 bohr^3
     Unit cell volume ucvol=  6.2511750E+01 bohr^3
     Unit cell volume ucvol=  6.5536000E+01 bohr^3
     Unit cell volume ucvol=  6.8656250E+01 bohr^3
     Unit cell volume ucvol=  7.1874000E+01 bohr^3
     Unit cell volume ucvol=  7.5190750E+01 bohr^3
     Unit cell volume ucvol=  7.8608000E+01 bohr^3
     Unit cell volume ucvol=  8.2127250E+01 bohr^3
     Unit cell volume ucvol=  8.5750000E+01 bohr^3
     ...  
           etotal1    -1.1461991605E+01
           etotal2    -1.1480508012E+01
           etotal3    -1.1494820912E+01
           etotal4    -1.1505365819E+01
           etotal5    -1.1512537944E+01
           etotal6    -1.1516696065E+01   <-
           etotal7    -1.1518166172E+01   <- minimum around here
           etotal8    -1.1517244861E+01   <-
           etotal9    -1.1514202226E+01
           etotal10   -1.1509284702E+01
     ...


If we examine the `etotal` values, the total energy does indeed go to a
minimum, and we also see that given the magnitude of the variations of the
total energy, an `ecut` of 25 Ha should be more than sufficient. We will now
extract the equilibrium volume and bulk modulus by using the eos bundled with
elk. This requires us to put the above data in an `eos.in` file. Create such a
file with your favorite editor and enter the following five lines and then the
data you just extracted:

     "C - Diamond"                : cname - name of material
     2                            : natoms - number of atoms
     1                            : etype - equation of state fit type
     50.0 95.0 100                : vplt1, vplt2, nvplt - start, end and #pts for fit
     10                            : nevpt - number of supplied points
     5.6745250E+01  -1.1461991605E+01
     5.9582000E+01  -1.1480508012E+01
     6.2511750E+01  -1.1494820912E+01
     6.5536000E+01  -1.1505365819E+01
     6.8656250E+01  -1.1512537944E+01
     7.1874000E+01  -1.1516696065E+01
     7.5190750E+01  -1.1518166172E+01
     7.8608000E+01  -1.1517244861E+01
     8.2127250E+01  -1.1514202226E+01
     8.5750000E+01  -1.1509284702E+01

When you run `eos` (the executable should be located in `src/eos/` in the
directory where `Elk` was compiled), it will produce several  `.OUT` files. The
file `PARAM.OUT` contains the information we need:

     C - Diamond

     Universal EOS
     Vinet P et al., J. Phys.: Condens. Matter 1, p1941 (1989)

     (Default units are atomic: Hartree, Bohr etc.)

     V0                =            75.50730327    
     E0                =           -11.51817590    
     B0                =           0.1564766690E-01
     B0'               =            3.685291965    

     B0 (GPa)          =            460.3701770  


This tells us the equilibrium volume and bulk modulus. The volume of our
diamond FCC lattice depends on the lattice parameter as: $\frac{a^3}{4}$. If we want to
convert the volume to a lattice parameter, we have to multiply by four and
then take the third root, so:

    alatt = (4*75.50730327)^(1/3) = 6.7094 Bohr (3.5505 Å)

at equilibrium for this dataset.

### 3.3. Carbon - the all-electron calculation

In order to estimate whether these values are good or not, we need independent
verification, and this will be provided by the all-electron `Elk` code. There is
an `Elk` input file matching our `ABINIT` diamond calculation at
[elk_C_diamond.in](paw3_assets/inputs/elk_C_diamond.in). You need
to copy this file to a directory set up for the `Elk` run (why not call it
`C_elk`), and it needs to be renamed to `elk.in`, which is the required input
name for an `Elk` calculation. We are now ready to run the `Elk` code for the first time.

If we take a look in the `elk.in` file, at the beginning we will see the lines:

     ! Carbon, diamond structure (FCC)

     ! The tasks keyword defines what will be done by the code:
     ! 0 - Perform ground-state calculation from scratch
     ! 1 - Restart GS calc. from STATE.OUT file
     ! 20 - Calculate band structure as defined by plot1d
     tasks
     0
     20

     ! Set core-valence cutoff energy
     ecvcut
      -6.0

     ! Construct atomic species file 'C.in'
     species
      6  : atomic number
      'C'
      'carbon'
      21894.16673    : atomic mass
      1.300000000    : muffin-tin radius
      4              : number of occ. states
      1   0   1   2  : 1s
      2   0   1   2  : 2s
      2   1   1   1  : 2p m=1
      2   1   2   1  : 2p m=2
    ...

{% dialog tutorial/paw3_assets/inputs/elk_C_diamond.in %}


Any text after an exclamation mark (or a colon on the lines defining data) is
a comment. The keyword `tasks` defines what the code should do. In this case
it is set to calculate the ground state for the given structure and to
calculate a band structure. The block `ecvcut` sets the core-valence cutoff
energy. The next input block, `species` defines the parameters for the
generation of an atomic species file (it will be given the name `C.in`). As a
first step, we need to generate this file, but we will need to modify it
before we perform the main calculation. Therefore, you should run the code
briefly (by just running the executable in your directory) and then kill it
after a few seconds (using `Ctrl+C` for instance ), as soon as it has generated the `C.in` file.

If you look in your directory after the code has been killed you will probably
see a lot of `.OUT` files with uppercase names. These are the `Elk` output files.
You should also see a `C.in` file. When you open it, you should see:

     'C'                                        : spsymb
     'carbon'                                   : spname
      -6.00000                                  : spzn
       39910624.45                              : spmass
       0.816497E-06    1.3000   38.0877   300    : rminsp, rmt, rmaxsp, nrmt
       4                                        : nstsp
       1   0   1   2.00000    T                 : nsp, lsp, ksp, occsp, spcore
       2   0   1   2.00000    F
       2   1   1   1.00000    F
       2   1   2   1.00000    F
       1                                        : apword
        0.1500   0  F                           : apwe0, apwdm, apwve
       0                                        : nlx
       3                                        : nlorb
       0   2                                    : lorbl, lorbord
        0.1500   0  F                           : lorbe0, lorbdm, lorbve
        0.1500   1  F
       1   2                                    : lorbl, lorbord
        0.1500   0  F                           : lorbe0, lorbdm, lorbve
        0.1500   1  F
       0   2                                    : lorbl, lorbord
        0.1500   0  F                           : lorbe0, lorbdm, lorbve
       -0.5012   0  T

The first four lines contain information pertaining to the symbol, name,
charge and mass of the atom. The fifth line holds data concerning the
numerical grid: the distance of the first grid point from the origin, the
muffin-tin radius, the maximum radius for the on-site atomic calculation, and
the number of grid points. The subsequent lines contain data about the
occupied states (the ones ending with "T" or "F"), and after that there is
information pertaining to the FP-LAPW on-site basis functions.

The first important thing to check here is whether all the orbitals that we
have included as valence states in the PAW dataset are treated as valence in
this species file. We do this by checking that there is an "F" after the
corresponding states in the occupation list:

     ...
        1   0   1   2.00000    T                 : spn, spl, spk, spocc, spcore
        2   0   1   2.00000    F
        2   1   1   1.00000    F
        2   1   2   1.00000    F
     ...

The first two numbers are the $n$, $l$ quantum numbers of the atomic state, so we
see that the $2s$ states, and the $2p$ states are set to valence as in the PAW dataset.

!!! Note
    This might not be the case in general, the version of `Elk` we use is
    modified to accept an adjustment of the _cutoff energy_ for determining whether
    a state should be treated as core or valence. This is what is set by the line:
    ````
    ...
    ecvcut
    -6.0 : core-valence cutoff energy
    ...
    ````

in the `elk.in` file. If you find too few or too many states are included as
valence for another atomic species, this value needs to be adjusted downwards or upwards.

The second thing we need to check is whether the number of grid points and the
_muffin-tin_ radius that we use in the `Elk` calculation is roughly equivalent to
the PAW one. If you have a look in the PAW dataset we generated before, i.e.
in the `C.LDA-PW-paw.xml` file, there is the line:

	   ...
	   <radial_grid eq="r=a*(exp(d*i)-1)" a=" 2.1888410558886799E-03" d=" 1.3133046335332079E-02" istart="0" iend="  800" id="log1">
	<values>
	   ...

These define the PAW grids used for wavefunctions, densities and potentials.
To approximately match the intensity of the grids, we should modify the fifth
line in the `C.in` file:

	   ...
		 0.816497E-06    1.3000   38.0877   300    : sprmin, rmt, sprmax, nrmt
	   ...
	   to:
	   ...
		 0.816497E-06    1.3000   38.0877   500    : sprmin, rmt, sprmax, nrmt
	   ...

You now need to comment out the species generation input block in the `elk.in` file:

	   ...
	   ! Construct atomic species file 'C.in'
	   !species
	   ! 6  : atomic number
	   ! 'C'
	   ! 'carbon'
	   ! 21894.16673    : atomic mass
	   ! 1.300000000    : muffin-tin radius
	   ! 4              : number of occ. states
	   ! 1   0   1   2  : 1s
	   ! 2   0   1   2  : 2s
	   ! 2   1   1   1  : 2p m=1
	   ! 2   1   2   1  : 2p m=2
	   ...

!!! Note
    This is very important! If you do not comment these lines the species
    file `C.in` will be regenerated when you run `Elk` and your modifications will be lost.

Now it is time to start `Elk` again. The code will now run and produce a lot of
`.OUT` files. There is rarely anything output to screen, unless it's an error
message, so to track the progress of the `Elk` calculation you can use the `tail` command:

     tail -f INFO.OUT

You get out of `tail` by pressing `CRTL+C`. While the calculation is running,
you might want to familiarise yourself with the different input blocks in the
`elk.in` file. When the `Elk` run has finished, there will be a `BAND.OUT` file in
your run directory. We can now do an analogous band structure comparison to
before, by using the python script [comp_bands_abinit2elk.py](paw3_assets/scripts/comp_bands_abinit2elk.py)
(you should copy this to your current directory). If your previous abinit
calculation is in the subdirectory `path/abinit_test` above you write:

     python comp_bands_abinit2elk.py path/abinit_test/outputs/ab_C_test_o_DS42_EIG BAND.OUT eV

This will get you the ending lines:

     ...
     #        nvals:   280
     # average diff:    12.393189  eV
     # minimum diff:   -12.668010  eV
     # maximum diff:   -12.290953  eV
     ...

So it looks like there is a huge difference! However, there is something we
have forgotten. Pipe the data to a file by writing:

    python comp_bands_abinit2elk.py path/abinit_test/outputs/ab_C_test_o_DS42_EIG BAND.OUT eV > bands.dat

and plot it in `gnuplot` with:

    gnuplot> plot 'bands.dat' u 1:2 w lp title 'ABINIT', 'bands.dat' u 1:3 w lp title 'Elk'

You should get a graph like this:

![Comparison of `ABINIT` and elk bands without shift](paw3_assets/images/band_abinit_elk_I.png)

As you can see, the band structures look alike but differ by an absolute
shift, which is normal, because in a periodic system there is no unique vacuum
energy, and band energies are always defined up to an arbitrary constant
shift. This shift depends on the numerical details, and will be different for
different codes using different numerical approaches. (Note in the `Elk` input
file that the keyword `xctype` controls the type - LDA or GGA - of the
exchange-correlation functional.)

However, if we decide upon a reference pont, like the valence band maximum
(VBM), or a point nearby, and align the two band plots at that point, there
will still be differences. By comparing with the plot we just made, we see
that the VBM is at the ninth k-point from the left, on band four. The script
we used previously can accomodate a shift, by issuing the command:

    python comp_bands_abinit2elk.py path/abinit_test/outputs/ab_C_test_o_DS42_EIG BAND.OUT align 9 4 eV

So that if the keyword `align` is present followed by the k-point index and
band number, we order the script to align at that point. Naturally, that will
make the positions of that particular point fit perfectly, but if we look at
the end of the output:

     ...
     # AVERAGES FOR OCCUPIED STATES:
     #        nvals:   106
     # average diff:     0.021871  eV
     # minimum diff:    -0.042755  eV
     # maximum diff:     0.064097  eV
     #
     # AVERAGES FOR UNOCCUPIED STATES:
     #        nvals:   174
     # average diff:     0.047221  eV
     # minimum diff:    -0.287747  eV
     # maximum diff:     0.089309  eV

     ...

we can tell that this is not true for the rest of the points. Since the script
assumes alignment at the VBM, it now separates its statistics for occupied and
unoccupied bands. The uppermost unoccupied bands can fit badly, depending on
what precision was asked of `ABINIT` (especially, if `nbdbuf` is used).

The fit is quite bad in general, an average of about 0.025 eV difference for
occupied states, and about 0.05 eV difference for unoccupied states. If you
plot the ouput as before, by piping the above to a `bands.dat` file and
executing the same `gnuplot` command, you should get the plot below.

![Comparison of `ABINIT` and elk bands with shift](paw3_assets/images/band_abinit_elk_II.png)

On the scale of the band plot there is a small - but visible - difference
between the two. Note that the deviations are usually larger away from the
high-symmetry points, which is why it's important to choose some points away
from these as well when making these comparisons. However, it is difficult to
conclude visually from the band structure that this is a bad dataset without
using the statistics output by the _script_, and without some sense of what
precision can be expected.

As we are now creating our "gold standard" with an `Elk` calculation, we also
need to calculate the equilibrium lattice parameter and Bulk modulus of
diamond with the `Elk` code. Unfortunately, `Elk` does not use datasets, so the
various lattice parameters we used in our `ABINIT` structural search will have
to be put in one by one by hand and the code run for each. The lattice
parameters in the `ABINIT` run were from 6.1 to 7.0 in increments of 0.1, so
that makes ten runs in total. To perform the first, simply edit the `elk.in`
file and change the keyword (at line 57):

     ...
     scale
     6.7403 : lattice parameter in Bohr
     ...

to:

     ...
     scale
     6.1 : lattice parameter in Bohr
     ...

!!! Note
    You also have to change the keyword `frozencr` to ".false." because, at
    the time of writing, there is an error in the calculation of the total energy
    for frozen core-states. This means that the Elk input file must have the keyword (at line 65 ):
    ````
    ...
    frozencr
    .false.
    ...
    ````
    when you are determining parameters which depend on the total energy. (It can
    safely be set to ".true." for band structure calculations however.) The
    difference in the lattice parameters when using frozen versus unfrozen core
    states in an all-electron calculation is expected to be of the order of 0.005 Bohr.

Finally, you don't need to calculate the band structure for each run, so you
might wand to change the `tasks` keyword section (at line 7):

     ...
     tasks
      0
      20
     ...

to just

     ...
     tasks
      0
     ...

After you've done these modifications, run `Elk` again. After the run has
finished, look in the `TOTENERGY.OUT` and the `LATTICE.OUT` files to get the
converged total energy and the volume. Write these down or save them in a safe
place, edit the `elk.in` file again, and so forth until you've calculated all
ten energies corresponding to the ten lattice parameter values. In the end you
should get a list which you can put in an `eos.in` file:

     "C - Diamond (Elk)"          : cname - name of material
     2                            : natoms - number of atoms
     1                            : etype - equation of state fit type
     50.0 95.0 100                : vplt1, vplt2, nvplt - start, end and #pts for fit
     10                           : nevpt - number of supplied points
     56.74525000  -75.5758514889
     59.58200000  -75.5933438506
     62.51175000  -75.6067015802
     65.53600000  -75.6163263051
     68.65625000  -75.6226219775
     71.87400000  -75.6259766731
     75.19075000  -75.6266976640
     78.60800000  -75.6250734731
     82.12725000  -75.6213749855
     85.75000000  -75.6158453243

(Your values might be slightly different in the last few decimals depending on
your system.) By running the `eos` utility as before we get:

     V0                =            74.47144624
     B0 (GPa)          =            469.7040543

     alatt = (4*74.47144624)^(1/3) = 6.6785 Bohr (3.5341 Å)

So we see that the initial, primitive, `ABINIT` dataset is about 9 GPa off for
the _Bulk modulus_ and about 0.035 Bohr away from the correct value for the
lattice parameter. In principle, these should be about an order of magnitude
better, so let us see if we can make it so.

### 3.4. Carbon - improving the dataset

Now that you know the target values, is up to you to experiment and see if you
can improve this dataset. The techniques are well documented in tutorial
[PAW2](/tutorial/paw2). Here's a brief summary of main points to be concerned about:

  * Use the keyword series `custom rrkj ...`, or `custom polynom ...`, or `custom polynom2 ...`,
    if you want to have maximum control over the convergence properties of the projectors.
  * Check the logarithmic derivatives very carefully for the presence of ghost states.
  * A dataset intended for ground-state calculations needs, as a rule of thumb, at least two projectors
    per angular momentum channel. This is because only the occupied states need to be reproduced very accurately.
    If you need to perform calculations which involve the Fock operator or unoccupied states
    - like in GW calculations for instance - you will probably need at least three projectors.
    You might also want to add extra projectors in completely unoccupied $l$-channels.

We will now benchmark a more optimized atomic dataset for carbon.
Try and check the convergence properties, equilibrium lattice parameter, bulk modulus,
and bands for the input file below:

     C 6                                     ! Atomic name and number
     LDA-PW scalarrelativistic loggrid 801 logderivrange -10 40 1000 ! XC approx., SE type, gridtype, # pts, logderiv
     2 2 0 0 0 0                             ! maximum n for each l: 2s,2p,0d,0f..
     2 1 2                                   ! Partially filled shell: 2p^2
     0 0 0                                   ! Stop marker
     c                                       ! 1s - core
     v                                       ! 2s - valence
     v                                       ! 2p - valence
     1                                       ! l_max treated = 1
     1.3                                     ! core radius r_c
     y                                       ! Add unocc. s-state
     12.2                                    !  reference energy
     n                                       ! no more unoccupied s-states
     y                                       ! Add unocc. p-state
     6.9                                     !  reference energy
     n                                       ! no more unoccupied p-states
     custom polynom2 7 11 vanderbiltortho sinc   ! more complicated scheme for projectors
     3 0 ultrasoft                           ! localisation scheme
     1.3                                     ! Core radius for occ. 2s state
     1.3                                     ! Core radius for unoocc. 2s state
     1.3                                     ! Core radius for occ. 2p state
     1.3                                     ! Core radius for unocc. 2p state
     XMLOUT                                       ! Run atompaw2abinit converter
     prtcorewf noxcnhat nospline noptim      ! `ABINIT` conversion options
     0                                       ! Stop marker

Generate an atomic data file from this (you can replace the items in the old
input file if you want, or make a new directory for this study). You might
want to try and modify the gnuplot scripts so that they work correctly for
this dataset. (The `wfn*` files are ordered just like the core radius list at
the end, so now their meaning and the numbering of some other files have
changed.) There is an example of the modifications in the plot script
[plot_C_all_II.p](paw3_assets/scripts/plot_C_all_II.p), which you
can download and run in gnuplot. You should get a plot like this:

![Plot info for second Carbon dataset](paw3_assets/images/plot_C_all_II.png)

Note the much better fit of the logarithmic derivatives, and the change in the
shape of the projector functions (in blue in the `wfn` plots), due to the more
complicated scheme used to optimise them.

Generate the dataset like before and run the `ABINIT` `ecut` testing datasets in
the `ab_C_test.abi` `ABINIT` input file again. You should get an `etotal`
convergence like this (again, the values to the left are just there to help):

                                       etotal   (ecut)
      etotal11 -1.0785137440E+01
      etotal21 -1.1489028406E+01   - 703.89 mHa (10 Ha)
      etotal31 -1.1522398057E+01   -  33.37 mHa (15 Ha)
      etotal41 -1.1523369210E+01   -   0.97 mHa (20 Ha)
      etotal51 -1.1523467540E+01   -   0.10 mHa (25 Ha)
      etotal61 -1.1523515348E+01   -   0.05 mHa (30 Ha)
      etotal71 -1.1523525440E+01   -   0.01 mHa (35 Ha)
      etotal81 -1.1523552361E+01   -   0.03 mHa (40 Ha)
      etotal91 -1.1523572404E+01   -   0.02 mHa (45 Ha)
 

This dataset already seems to be converged to about 1 mHa at an `ecut` of 15 Ha,
so it is much more efficient. A comparison of bands (in units of eV) between
datasets 32 and 92 gives:

     ...
     #        nvals:   280
     # average diff:     0.004312  eV
     # minimum diff:    -0.013878  eV
     # maximum diff:     0.000544  eV
     ...

Which also shows a much faster convergence than before. Is the dataset
accurate enough? Well, if you run the `ABINIT` equilibrium parameter input file
in `ab_C_equi.abi`, you should get data for an `eos.in` file:

     "C - Diamond (second PAW dataset)" : cname - name of material
     2                                  : natoms - number of atoms
     1                                  : etype - equation of state fit type
     50.0 95.0 100                      : vplt1, vplt2, nvplt - start, end and #pts for fit
     10                                 : nevpt - number of supplied points
     5.6745250E+01  -1.1471957529E+01
     5.9582000E+01  -1.1489599125E+01
     6.2511750E+01  -1.1503102315E+01
     6.5536000E+01  -1.1512899444E+01
     6.8656250E+01  -1.1519381720E+01
     7.1874000E+01  -1.1522903996E+01
     7.5190750E+01  -1.1523789014E+01
     7.8608000E+01  -1.1522330567E+01
     8.2127250E+01  -1.1518796247E+01
     8.5750000E+01  -1.1513430193E+01
     

And when fed to `eos`, this gives us the equilibrium data:

     V0                =            74.71100799
     B0 (GPa)          =            465.7949258

     alatt = (4*74.71100799)^(1/3) = 6.6857 Bohr (3.5379 Å)

For comparison, we list all previous values again:

     Equilibrium        Bulk modulus       lattice
     volume, V0         B0                parameter
     75.5073            460.37            3.5505 Å   (1st primitive PAW dataset)
     74.7110	        465.79            3.5379 Å   (2nd better PAW dataset)
     74.4714            469.70            3.5341 Å   (Elk all-electron)

It is obvious that the second dataset is much better than the first one.
A comparison of the most converged values for the bands using the command:

    python comp_bands_abinit2elk.py ab_C_test_o_DS92_EIG BAND.OUT align 9 4 eV

(This assumes that you have all the files you need in the current directory.)
As before, the extra command parameters on the end mean "align the 9-th
k-point on the fourth band and convert values to eV". This will align the band
structures at the valence band maximum. The statistics printed out at the end
should be something like this:

     ...
     # AVERAGES FOR OCCUPIED STATES:
     #        nvals:   106
     # average diff:     0.013523  eV
     # minimum diff:    -0.001229  eV
     # maximum diff:     0.040695  eV
     #
     # AVERAGES FOR UNOCCUPIED STATES:
     #        nvals:   174
     # average diff:     0.016247  eV
     # minimum diff:    -0.013699  eV
     # maximum diff:     0.117064  eV 
     ...

Which shows a precision, on average, of slightly better than 0.01 eV for both
the four occupied and the four lowest unoccupied bands. As before, you can
pipe this output to a file and plot the bands for visual inspection.

This is a better dataset, but probably by no means the best possible. It is
likely that one can construct a dataset for carbon that has even better
convergence properties, and is even more accurate. You are encouraged to
experiment and try to make a better one.

## 4. Magnesium - dealing with the Fermi energy of a metallic system

There is added complication if the system is metallic, and that is the
treatment of the smearing used in order to eliminated the sharp peaks in the
_density of states_ (DOS) near the Fermi energy. The DOS is technically
integrated over in any ground-state calculation, and for a metal this
requires, in principle, an infinite k-point grid in order to resolve the Fermi surface.

In practice, a smearing function is used so that a usually quite large - but
finite - number of k-points will be sufficient. This smearing function has a
certain spread controlled by a smearing parameter, and the optimum value of
this parameter depends on the k-point grid used. As the k-point grid becomes
denser, the optimum spread becomes smaller, and all values converge toward
their ideal counterparts in the limit of no smearing and an infinitely dense grid.

The problem is that, in `ABINIT`, finding the optimum smearing parameter takes a
(potentially time consuming) convergence study. However, we are in luck. The
`Elk` code has an option for automatically determining the smearing parameter.
Thus we should use the `Elk` code first, set a relatively dense k-mesh, and
calculate the equilibrium bulk modulus, lattice parameter and band structure.
Then we make sure to match the automatically determined smearing width, and
most importantly, make sure that we match the smearing function used between
the `Elk` and the `ABINIT` calculation.

### 4.1. Magnesium - The all-electron calculation

There is an `Elk` input file prepared at: [elk_Mg_band.in](paw3_assets/inputs/elk_Mg_band.in),
we suggest you copy it into a subdirectory dedicated to the Mg `Elk` calculation (why not `Mg_elk`?), rename
it to `elk.in` and take a look inside the input file.

{% dialog tutorial/paw3_assets/inputs/elk_Mg_band.in %}


There will be sections familiar from before, defining the lattice vectors,
structure, etc. (Mg has a 2-atom hexagonal unit cell.) Then there are a couple
of new lines for the metallic case:

     ...
     ! Metallic options
     stype
      0          : Smearing type 0 - Gaussian
     autoswidth
      .true.     : Automatic determination of swidth
     ...

When you run `Elk` with this file, it will start a ground-state run (this might
take some time due to the dense k-point mesh), all the while automatically
determining the smearing width. At the end of the calculation the final value
of `swidth` will have been determined, and can be easily extracted with a `grep`:

     grep ' smearing' INFO.OUT

this should furnish you with a list:

     Automatic determination of smearing width
     New smearing width :   0.1000000000E-02
     New smearing width :   0.4116175210E-02
     New smearing width :   0.4056329377E-02
     New smearing width :   0.4060342507E-02
     New smearing width :   0.4078123678E-02
     New smearing width :   0.4097424577E-02
     New smearing width :   0.4105841081E-02
     New smearing width :   0.4109614663E-02
     New smearing width :   0.4109651379E-02
     New smearing width :   0.4109804149E-02
     New smearing width :   0.4109807004E-02
     New smearing width :   0.4109805701E-02
     New smearing width :   0.4109804616E-02
     New smearing width :   0.4109804468E-02
     New smearing width :   0.4109804441E-02
     New smearing width :   0.4109804423E-02


where the last value is the one we seek, i.e. the smearing at convergence.
Since this `Elk` file will also calculate the band structure, you will have a
`BAND.OUT` file at the end of this calculation to compare your `ABINIT` band
structure to. There is one more thing we need to check, and that is the Fermi energy:

     grep 'Fermi  ' INFO.OUT

     Fermi                       :     0.116185305134    
     Fermi                       :     0.115496524671    
     Fermi                       :     0.122186414492    
     Fermi                       :     0.128341839155    
     Fermi                       :     0.132281493053    
     Fermi                       :     0.133819140456    
     Fermi                       :     0.134308473303    
     Fermi                       :     0.134328785350    
     Fermi                       :     0.134347853104    
     Fermi                       :     0.134347939064    
     Fermi                       :     0.134347635069    
     Fermi                       :     0.134347477436    
     Fermi                       :     0.134347453635    
     Fermi                       :     0.134347448126    
     Fermi                       :     0.134347446032    
     Fermi                       :     0.134347446149    


The last one is the Fermi energy at convergence. We will need this later when
we compare band structures to align the band plots at the Fermi energy.

Now it's time to calculate the equilibrium lattice parameters. There is a
prepared file at: [elk_Mg_equi.in](paw3_assets/inputs/elk_Mg_equi.in).

{% dialog tutorial/paw3_assets/inputs/elk_Mg_equi.in %}

As before copy this to your directory rename it to `elk.in`. The layout of this file looks pretty much
like the one before, except the band structure keywords are missing, and now
switdth is fixed to the value we extracted before:

     ...
     ! Metallic options
     stype
      0          : Smearing type 0 - Gaussian
     swidth
      0.4109804423E-02     : Smearing width
     ...

To calculate the equilibrium lattice parameters, we are going to use the bulk
modulus, which is a quantity defined with respect to a scaling of the entire
cell (as opposed to _Young's modulus_, for instance, which is defined with
respect to linear scaling along the lattice vectors). There is a handy `scale`
keyword for `Elk`, which will accomplish this for us. If we look at the region
where the lattice is defined:

     ...
     ! Define lattice vectors
     ! Magnesium has an hexagonal native structure
     ! with a=b=3.20927 Å c=5.21033 Å  alpha=90 beta=90 gamma=60
     ! (experimental, at 25 degrees Celsius)

     ! Scale factor to be applied to all lattice vectors
     scale
      1.00

      with
           6.0646414   0.0000000   0.0000000
           3.0323207   5.2521335   0.0000000
           0.0000000   0.0000000   9.8460968
     ...

We will here also need to perform several calculations (like we did for the
diamond case) and we need to change the value of the `scale` keyword for each
one. A good set of values would be: 0.94, 0.96, 0.98, 1.0, 1.02 1.04 and 1.06,
i.e. a change of scale in steps of 2% with seven values in total spaced around
the experimental equilibrium lattice structure.

After each run, as before, you should collect the value of the unit cell
volume and the total energy. After seven runs you should have a set of numbers
which you can put in an `eos.in` file (depending on the system, your actual
values may differ slightly from these):

     "Mg - bulk metallic"
      2                      : natoms - number of atoms
      1                      : etype - equation of state fit type
      260.0 374.0 100        : vplt1, vplt2, nvplt - start, end and #pts for fit
      7                      : nevpt - number of supplied points
      260.4884939  -399.042203252
      277.4716924  -399.044435776
      295.1774734  -399.044943459
      313.6208908  -399.043962316
      332.8169982  -399.041812744
      352.7808497  -399.038751783
      373.5274988  -399.034932869

Upon using the eos utility you will get standard type of outputs in `PARAM.OUT`:

     Mg - bulk metallic

     Universal EOS
     Vinet P et al., J. Phys.: Condens. Matter 1, p1941 (1989)

     (Default units are atomic: Hartree, Bohr etc.)

      V0                =            291.6029247    
      E0                =           -399.0449584    
      B0                =           0.1364455738E-02
      B0'               =            4.304295809    

      B0 (GPa)          =            40.14366703    
 
Now we have to translate this in terms of the lattice parameters. The
equilibrium scale factor is given by:
$scale = (\frac{V_0}{V_1})^{\frac{1}{3}} = (\frac{291.6029247}{313.6208908})^{\frac{1}{3}} = 0.9760280459$

Where $V_1$ is the volume with scale set to 1.0. Multiplying all basis vectors
with this scale factor, we have that:

     Equilibrium        Bulk modulus       lattice
     volume, V0         B0                parameters
     291.6029           40.1437            a = b = 3.1323 Å  c = 5.0854 Å

Now we have all the information needed to proceed with the `ABINIT` calculation.

### 4.2. Magnesium - The `ABINIT` calculation

As usual, it's best to prepare a separate subdirectory for the atomic data and
the `ABINIT` test. We will assume that the subdirectories have been created as:

     mkdir Mg_atompaw
     mkdir Mg_atompaw/abinit_test
     mkdir Mg_atompaw/abinit_test/outputs

and that your current directory is `./Mg_atompaw`. For the Mg `ATOMPAW` input,
use the file  [Mg.input](paw3_assets/inputs/Mg.input).

{% dialog tutorial/paw3_assets/inputs/Mg.input %}

Note that there are not really many projectors in this dataset, only two
per angular momentum channel. It should be possible to make this much better
adding extra projectors, and maybe even unoccupied $d$-states. If you run
atompaw with this, you can have a look with the bundled `plot_MG_all.p` file
and others like it to get a feel for the quality of this dataset.

Generate the `ABINIT` dataset file, and make sure it's given as:
`./Mg_atompaw/Mg_LDA-PW-paw.xml`, then go to the subdirectory for the `ABINIT` test,
and copy these files to it: [ab_Mg_test.abi](paw3_assets/inputs/ab_Mg_test.abi),
and [ab_Mg_equi.abi](paw3_assets/inputs/ab_Mg_equi.abi). 
The file for testing the convergence has already been set up so that the smearing
strategy is equivalent to the `Elk` one, as evidenced by the lines:

{% dialog tutorial/paw3_assets/inputs/ab_Mg_test.abi %}
{% dialog tutorial/paw3_assets/inputs/ab_Mg_equi.abi %}

     ...
     # Parameters for metals
     tsmear 0.4109804423E-02
     occopt 7
     ...

inside it. The `occopt 7` input variable corresponds exactly to the Gaussian
smearing which is the default for the `Elk` code. (In fact it is the 0th order
Methfessel-Paxton expression [[cite:Methfessel1989]], for other
possibilities compare the entries for the keyword `stype` in the `Elk` manual
and the entries for `occopt` in ABINIT).

Now run the test input file (if your computer has several cores, you might
want to take advantage of that and run `ABINIT` in parallel). The test suite can
take some time to complete, because of the dense _k-point mesh sampling_. Make
sure you pipe the screen to a log file: `log_Mg_test`

When the run is finished, we can check the convergence properties as before,
and we that an `ecut` of 15 Ha is definitely enough. The interesting thing will
now be to compare the band structures. First we need to check the Fermi energy
of the `ABINIT` calculation, if you do a `grep`:

     grep ' Fermi' log_Mg_test

you will see a long list of Fermi energies, one for each iteration, finally
converging towards one number:

     ...
     newocc : new Fermi energy is       0.137605 , with nelect=     20.000000
     newocc : new Fermi energy is       0.137605 , with nelect=     20.000000
     newocc : new Fermi energy is       0.137605 , with nelect=     20.000000


The last one of these is the final _Fermi energy_ of the `ABINIT` calculation. The
`abinit2elk` band comparison _script_ can now be given the _Fermi energies_ of the
two different calculations and align band structures there. Copy the
`BAND.OUT` file from the `Elk` calculation to the current directory, as well as
the band comparison script `comp_bands_abinit2elk.py`. This script can also be
used to align the bands at different Fermi energies. However, in the
`BAND.OUT` file from `Elk`, the bands are already shifted so that the Fermi
energy is at zero, so it is only the alignment of the `ABINIT` file that is required:

     python comp_bands_abinit2elk.py ./outputs/ab_Mg_test_o_DS32_EIG BAND.OUT Fermi 0.137605 0.0 eV

Issuing this command will provide the final lines:

     ...
     #        nvals:   940
     # average diff:     0.029652  eV
     # minimum diff:    -0.036252  eV
     # maximum diff:     0.215973  eV
     ...

Which means that we are on average accurate to about 0.03 eV. If you pipe the
output to a file `bands_abinit_elk.dat`, and go into gnuplot and use the script `plot_Mg_bands.p`:

     gnuplot> load 'plot_Mg_bands.p'
     
You should get a plot that looks something like this:

![Comparison of Mg `ABINIT` and Elk bands alignet at Fermi level](paw3_assets/images/band_abinit_elk_III.png)

As we can see, the bands should fit quite well. Finally, for the structural, a
run of the `ab_Mg_equi.abi` file gives us all the information we need for the
creation of an `eos.in` file:

     "Mg - bulk metallic (ABINIT)"
      2                      : natoms - number of atoms
      1                      : etype - equation of state fit type
      260.0 380.0 100        : vplt1, vplt2, nvplt - start, end and #pts for fit
      7                      : nevpt - number of supplied points
      2.6048849E+02  -1.2697509579E+02
      2.7747169E+02  -1.2697742149E+02
      2.9517747E+02  -1.2697801365E+02
      3.1362089E+02  -1.2697716194E+02
      3.3281700E+02  -1.2697512888E+02
      3.5278085E+02  -1.2697212368E+02
      3.7352750E+02  -1.2696833809E+02


When the eos utility is run, we get the equilibrium volume and Bulk modulus:

     ...
     V0                =            293.0281803
     ...
     B0 (GPa)          =            39.19118222

Converting this to lattice parameters as before, we can compare this with the `Elk` run:

     Equilibrium        Bulk modulus       lattice
     volume, V0         B0                parameters
     291.6029           40.1437        a = b = 3.1323 Å  c = 5.0854 Å  (Elk)
     293.0282           39.1912        a = b = 3.1374 Å  c = 5.0937 Å  (ABINIT)


Which is very close.

Again, this is a decent dataset for ground-state calculations, but it can
probably be made even better. You are encouraged to try and do this.

## 5. PAW datasets for GW calculations

There are a number of issues to consider when making datasets for GW
calculations, here is a list of a few:

  * Care needs to be taken so that the logarithmic derivatives match for much higher energies
    than for ground-state calculations. They should at least match well up to the energy of
    the unoccupied states used in the calculation. The easiest way of ensuring this is increasing
    the number of projectors per state.

  * The on-site basis needs to be of higher quality to minimise truncation error due to the finite number
    of on-site basis functions (projectors). Again, this requires more projectors per angular momentum channel.

  * As a rule of thumb, a PAW dataset for GW should have at least three projectors per state, if not more.

  * A particularly sensitive thing is the quality of the expansion of the pseudised plane-wave part in terms of the on-site basis.
     This can be checked by using the density of states (DOS), as described in the 
     [first PAW tutorial](/tutorial/paw1).
---
authors: JWZ, XG
---

# Tutorial on properties at the nuclei

## Observables near the atomic nuclei.

The purpose of this tutorial is to show how to compute several observables of
interest in M&ouml;ssbauer, NMR, and NQR spectroscopy, namely:

  * the electric field gradient,
  * the isomer shift

This tutorial should take about 1 hour.

[TUTORIAL_README]

## Electric field gradient

Various spectroscopies, including nuclear magnetic resonance and nuclear
quadrupole resonance (NMR and NQR), as well as M&ouml;ssbauer spectroscopy, show
spectral features arising from the electric field gradient at the nuclear
sites. Note that the electric field gradient (EFG) considered here arises from
the distribution of charge within the solid, not due to any external electric fields.

The way that the EFG is observed in spectroscopic experiments is through its
coupling to the nuclear electric quadrupole moment. The physics of this
coupling is described in various texts, for example [[cite:Slichter1978]].
Abinit computes the field gradient at each site, and then reports the gradient and its
coupling based on input values of the nuclear quadrupole moments.

The electric field and its gradient at each nuclear site arises from the
distribution of charge, both electronic and ionic, in the solid. The gradient
especially is quite sensitive to the details of the distribution at short
range, and so it is necessary to use the PAW formalism to compute the gradient
accurately. For charge density $n({\mathbf r})$, the potential $V$ is given
by
$$ V({\mathbf r})=\int \frac{n({\mathbf r'})}{ |\mathbf{r}-\mathbf{r'}| } d{\mathbf r'} $$
and the electric field gradient is
$$ V_{ij} = -\frac{\partial^2}{\partial x_i\partial x_j}V({\mathbf r}). $$
The gradient is computed at each nuclear site, for each source of charge arising
from the PAW decomposition (see [the tutorial PAW1](/tutorial/paw1) ).
This is done in the code as follows [[cite:Profeta2003]],[[cite:Zwanziger2008]]:

  * Valence space described by planewaves: expression for gradient is Fourier-transformed at each nuclear site.
  * Ion cores: gradient is computed by an Ewald sum method
  * On-site PAW contributions: moments of densities are integrated in real space around each atom, weighted by the gradient operator

The code reports each contribution separately if requested.

The electric field gradient computation is performed at the end of a ground-state calculation,
and takes almost no additional time.
The tutorial file is for stishovite, a polymorph of SiO$_2$. In addition to typical ground state
variables, only two additional variables are added:

    prtefg  2
    quadmom 0.0 -0.02558

{% dialog tests/tutorial/Input/tnuc_1.abi %}

The first variable instructs Abinit to compute and print the electric field
gradient, and the second gives the quadrupole moments of the nuclei, in
barns, one for each type of atom.
A standard source for quadrupole moments is [[cite:Pyykko2008]].
Here we are considering silicon and oxygen, and in
particular Si-29, which has zero quadrupole moment, and O-17, the only stable
isotope of oxygen with a non-zero quadrupole moment.

After running the file *tnuc_1.abi* through Abinit, you can find the following
near the end of the output file:

Electric Field Gradient Calculation 

    Atom   1, typat   1: Cq =      0.000000 MHz     eta =      0.000000
     
          efg eigval :     -0.152323
    -         eigvec :      0.000000     0.000000    -1.000000
          efg eigval :     -0.054274
    -         eigvec :      0.707107    -0.707107    -0.000000
          efg eigval :      0.206597
    -         eigvec :      0.707107     0.707107     0.000000
     
          total efg :      0.076161     0.130436     0.000000
          total efg :      0.130436     0.076161     0.000000
          total efg :      0.000000     0.000000    -0.152323
 
This fragment gives the gradient at the first atom, which was silicon. Note
that the gradient is not zero, but the coupling is---that's because the
quadrupole moment of Si-29 is zero, so although there's a gradient there's
nothing in the nucleus for it to couple to.

Atom 3 is an oxygen atom, and its entry in the output is:

    Atom   3, typat   2: Cq =      6.615041 MHz     eta =      0.140313
     
          efg eigval :     -1.100599
    -         eigvec :      0.707107    -0.707107     0.000000
          efg eigval :      0.473085
    -         eigvec :     -0.000000    -0.000000    -1.000000
          efg eigval :      0.627514
    -         eigvec :      0.707107     0.707107    -0.000000
     
          total efg :     -0.236543     0.864057    -0.000000
          total efg :      0.864057    -0.236543    -0.000000
          total efg :     -0.000000    -0.000000     0.473085
     
     
          efg_el :     -0.036290    -0.075078    -0.000000
          efg_el :     -0.075078    -0.036290    -0.000000
          efg_el :     -0.000000    -0.000000     0.072579
     
          efg_ion :     -0.016807     0.291185    -0.000000
          efg_ion :      0.291185    -0.016807    -0.000000
          efg_ion :     -0.000000    -0.000000     0.033615
     
          efg_paw :     -0.183446     0.647950     0.000000
          efg_paw :      0.647950    -0.183446     0.000000
          efg_paw :      0.000000     0.000000     0.366891
 
Now we see the electric field gradient coupling, in frequency units, along
with the asymmetry of the coupling tensor, and, finally, the three
contributions to the total. Note that the valence part, efg_el, is 
small, while the ionic part and the on-site PAW part are larger. In fact, the
PAW part is largest; this is why these calculations give very poor results
with norm-conserving pseudopotentials, and need the full accuracy of PAW to capture
the behavior near the nucleus.
Experimentally, the nuclear quadrupole coupling for O-17 in stishovite is
reported as $6.5\pm 0.1$ MHz, with asymmetry $0.125\pm 0.05$ [[cite:Xianyuxue1994]].
It is not uncommon for PAW-based EFG calculations to give coupling values a few percent
too large; often this can be improved by using PAW datasets with smaller PAW
radii, at the expense of more expensive calculations [[cite:Zwanziger2016]]. 

## Fermi contact interaction

The Fermi contact interaction arises from overlap of the electronic wavefunctions
with the atomic nucleus, and is an observable for example in
M&ouml;ssbauer spectroscopy [[cite:Greenwood1971]]. In M&ouml;ssbauer spectra,
the isomer shift $\delta$ is expressed in (SI) velocity units as
$$ \delta = \frac{2\pi}{3}\frac{c}{E_\gamma}\frac{Z e^2}{ 4\pi\epsilon_0} ( |\Psi (R)_A|^2 - |\Psi (R)_S|^2 )\Delta\langle r^2\rangle  $$
where $\Psi(R)$ is the electronic
wavefunction at nuclear site $R$, for the absorber (A) and source (S) respectively;
$c$ is the speed of light, $E_\gamma$ is the nuclear transition energy, and $Z$ the atomic number;
and $\Delta\langle r^2\rangle$ the change in the nuclear size squared. All these quantities
are assumed known in the M&ouml;ssbauer spectrum of interest, except $|\Psi(R)|^2$, the
Fermi contact term.

Abinit computes the Fermi contact term in the PAW formalism by using as observable
$\delta(R)$, that is, the Dirac delta function at the nuclear site [[cite:Zwanziger2009]].
Like the electric field gradient computation, the Fermi contact calculation
is performed at the end of a ground-
state calculation, and takes almost no time. There is a tutorial file for
SnO$_2$, which, like stishovite studied above, has the rutile structure.
In addition to typical ground state
variables, only one additional variable is needed:

    prtfc  1

{% dialog tests/tutorial/Input/tnuc_2.abi %}

After running this file, inspect the output and look for the phrase
"Fermi-contact Term Calculation". There you'll find the FC output for
each atom; in this case, the Sn atoms, [[typat]] 1, yield a contact term
of 71.6428 (density in atomic units, $a^{-3}_0$).

To interpret M&ouml;ssbauer spectra you need really both a source and
an absorber; in the tutorial we provide also a file for $\alpha$-Sn (grey
tin, which is non-metallic).

{% dialog tests/tutorial/Input/tnuc_3.abi %}

If you run this file, you should find a contact term of 102.0748.

To check your results, you can use experimental data for the isomer shift $\delta$
for known compounds to compute $\Delta\langle r^2\rangle$ in the above equation
(see [[cite:Zwanziger2009]]). Using our results above together with standard
tin M&ouml;ssbauer parameters of $E_\gamma = 23.875$ keV and an experimental shift
of 2.2 mm/sec for $\alpha$-Sn relative to SnO$_2$, we find
$\Delta\langle r^2\rangle = 5.67\times 10^{-3}\mathrm{fm}^2$, in decent agreement
with other calculations of 6--7$\times 10^{-3}\mathrm{fm}^2$ [[cite:Svane1987]], [[cite:Svane1997]].
---
authors: SS, XG
---

# Tutorial on fold2bloch

## Unfolding the electronic structure of a lattice of Hydrogen atoms.

Supercells are often used in electronic structure calculations in order to
model compound alloys, defects, etc. The band structure obtained directly from
these calculations is hard to interpret due to the Brillouin zone folding as a
result of its reduced size for the supercell, compared to that for the
unperturbed host unit cell.
The unfolding technique used in Abinit is the one presented in [[cite:Rubel2014]].

This tutorial aims at demonstrating how to unfold the band structure of a
supercell and present it in the basis of conventional Bloch wave vectors
inherent to the unperturbed unit cell. We will construct a supercell of 6
hydrogen atoms, perform self-consistent cycle and plot the corresponding band
structure. At the end, we will recover the familiar dispersion relation using
the *fold2Bloch* utility. See also the [[help:fold2bloch]].

This tutorial should take about 1 hour.

[TUTORIAL_README]

## Creating a Hydrogen supercell structure

Let's begin with the simplest structure: a lattice of hydrogen atoms. It is a
convenient starting point since the hybridization between $s$-orbitals results
in a well known dispersion relation for the energy eigenvalues as a function
of the wave vector $k$:

$$E(k)=E_0 - 2A \cos(kb)$$

Here $b$ is the lattice spacing and $A$ reflects the strength of
hybridization between adjacent $s$-orbitals. For more details please refer to
the Feynman's Lectures on Physics (vol 3, chapter 13).

Our model structure will be a cubic lattice of hydrogen atoms spaced 3 Bohr apart.
The primitive cell contains only one atom. Here we chose to represent
the same structure with a 6 atom supercell. Such a large cell is redundant in
this case, since there is nothing that disturbs the original symmetry.
However, we made this choice on purpose in order to observe the zone folding.

![](fold2bloch_assets/Fig1.png)
:   Fig. 1: Primitive unit cell (a) and 1x2x3 supercell (b) that represent a cubic
    lattice of hydrogen atoms.

Since we are interested in the band structure, we need to select a path in the
reciprocal space which will be used for plotting. The supercell was expanded
in Y and Z directions. Accordingly, the reciprocal space shrinks along the
same directions. We select those directions for the band structure plot as
they will be affected by the zone folding.

![](fold2bloch_assets/Fig2.png)
:   Fig. 2: Brillouin zone of the supercell.

*Before beginning, you might consider to work in a different subdirectory as
for the other tutorials. Why not Work_fold2Bloch?*

In order to use the *fold2Bloch*, you need to first generate a wave function file (*WFK* file).

For this purpose, copy the file
tests/tutorial/Input/tfold2bloch_1.abi in the directory ~abinit/tests/tutorial/Input/Work_fold2Bloch.

```sh
cd $ABI_TESTS/tutorial/Input
mkdir Work_fold2Bloch
cd Work_fold2Bloch
cp ../tfold2bloch_1.abi .
```

{% dialog tests/tutorial/Input/tfold2bloch_1.abi %}

The input file has two datasets,
the first to generate the *WFK* file, and the second to draw the band structure.
Now you are ready to run Abinit. Issue the following:

    abinit tfold2bloch_1.abi > tfold1bloch_1.log &

This will generate a self consistent charge density for the 6 Hydrogen atom
supercell structure, and the wave function file, *tfold2bloch_1o_DS1_WFK*, which is
needed for unfolding with *fold2Bloch*.

## Folded band structure

Before we proceed with the unfolding, let's plot the "standard" band structure
of a supercell. We will be looking for signatures of the zone folding. In
order to do this, there are several possibilities. You might e.g. rely on Abipy (see below), or on Matlab.
This tutorial was initally illustrated using Matlab, for which 
you will need the following scripts 

1.  energy_eig-abinit.sh
2.  plot_band.m

that are located at $ABI_HOME/doc/tutorial/fold2Bloch_assets/ .
Copy them to your work directory.

Execute the *energy_eig-abinit.sh* script

    ./energy_eig-abinit.sh tfold2bloch_1o_DS2_EIG

This will generate an output file: *tfold2bloch_1o_DS2_EIG.dat*

Edit *plot_band.m* file and point to the newly created *tfold2bloch_1o_DS2_EIG.dat* file.

    data=load('tfold2bloch_1o_DS2_EIG.dat');

Then, run the *plot_band.m* script in MatLab

    plot_band.m

This will plot the band structure of the 6 atom Hydrogen supercell created.

Lastly, compare the image obtained to the band structure image below.

![](fold2bloch_assets/Fig3.png)
:   Fig. 3: 6 atom hydrogen supercell band structure plot

Here you can see that the band structure does not look like a cosine function
along directions Y-Gamma and Z-Gamma. The band structure is folded according
to the multiplicity along those directions used when constructing the supercell (Fig. 1b).

!!! tip

    If |AbiPy| is installed on your machine, you can use the |abiopen| script
    with the `--expose` option to visualize the band dispersion:

        abiopen.py tfold2bloch_1o_DS2_GSR.nc --expose

![](fold2bloch_assets/abiopen_tfold2bloch_1o_DS2_GSR.png)


## Bloch spectral weights with fold2Bloch

Next step is to execute *fold2Bloch* using the wave function file from the 2nd
dataset, and multiplicity in the corresponding directions, used when
constructing the super-cell (x:y:z), included as part of the command line
arguments. For this example the multiplicity used was (1:2:3)

Execute the following command:

    fold2Bloch tfold2bloch_1o_DS2_WFK 1:2:3

You should see the following:

```
           ***********************
           ** Fold2Bloch V 1.1  **
           **Build  Mar 16, 2015**
           ***********************
 Reading eigenvalues from: tfold2bloch_1o_DS2_WFK , with iomode: IO_MODE_MPI
 wfk_read_eigenvalues completed. cpu:  0.03 [s] , wall:  0.04 [s] <<< TIME
- Creating netcdf file WITHOUT MPI-IO support: tfold2bloch_FOLD2BLOCH.nc
         2% Processing K point:    0.000000   -0.500000    0.000000
         4% Processing K point:    0.000000   -0.450000    0.000000
         7% Processing K point:    0.000000   -0.400000    0.000000
...
        95% Processing K point:    0.000000    0.000000    0.400000
        97% Processing K point:    0.000000    0.000000    0.450000
       100% Processing K point:    0.000000    0.000000    0.500000
      Number of K points processed:          42
      Data was written to: tfold2bloch.f2b
      Data format: KX, KY, KZ, Eigenvalue(Ha), Weight
```

That output tells us which K-point was processed, total number of K-points
processed, output file, and the format that the data is written in.

Now take a look at the *tfold2Bloch.f2b*. The first few lines should be as follows:

```
   0.000000  -0.250000   0.000000  -0.308342   0.589037
   0.000000  -0.250000   0.333333  -0.308342   0.000000
   0.000000  -0.250000  -0.333333  -0.308342   0.000000
   0.000000   0.250000   0.000000  -0.308342   0.410963
   0.000000   0.250000   0.333333  -0.308342   0.000000
   0.000000   0.250000  -0.333333  -0.308342   0.000000
   0.000000  -0.250000   0.000000  -0.308342   0.410963
   0.000000  -0.250000   0.333333  -0.308342   0.000000
   0.000000  -0.250000  -0.333333  -0.308342   0.000000
   0.000000   0.250000   0.000000  -0.308342   0.589037
   0.000000   0.250000   0.333333  -0.308342   0.000000
   0.000000   0.250000  -0.333333  -0.308342   0.000000
   0.000000  -0.250000   0.000000  -0.083709   0.000000
   0.000000  -0.250000   0.333333  -0.083709   0.593925
   0.000000  -0.250000  -0.333333  -0.083709   0.025041
   0.000000   0.250000   0.000000  -0.083709   0.000000
   0.000000   0.250000   0.333333  -0.083709   0.133575
   0.000000   0.250000  -0.333333  -0.083709   0.247460
   0.000000  -0.250000   0.000000  -0.083709   0.000000
   0.000000  -0.250000   0.333333  -0.083709   0.193298
   0.000000  -0.250000  -0.333333  -0.083709   0.218686
   0.000000   0.250000   0.000000  -0.083709   0.000000
   0.000000   0.250000   0.333333  -0.083709   0.564322
   0.000000   0.250000  -0.333333  -0.083709   0.023694
   0.000000  -0.225000   0.000000  -0.332652   1.000000
   0.000000  -0.225000   0.333333  -0.332652   0.000000
    ...
```

Let's take a moment to analyse the output. Columns 1-3 correspond to kx, ky
and kz of the unfolded bands; the 4th column is the energy eigenvalue in [Ha]
and the 5th column corresponds to a spectral weight of the k-point after
unfolding. Do not confuse it with k-point weight, which represents its
multiplicity in the Brillouin zone. Since our supercell contains 6 unit cells
(1 x 2 x 3), each eigenvalue in the solution contains information about 6 Bloch
wave vectors, i.e., unfolded into 6 k-points. The relative contribution of
these k-points is determined by the spectral weight.

Lines 1-6 represent unfolding of the 1st eigenvalue of -0.31796 Ha. This
eigenvalue corresponds to the Bloch wave vectors of (0 ±1/2 0) as they have a
non-zero weight. The weights total 1 for normalization. This eigenvalue is
degenerate, so lines 7-12 look very similar. When a disorder is present (in a
form of defects, distortions, impurities) individual eigenstates may not
express an exclusive Bloch character any longer. This can have some
interesting consequences for transport or optical properties, which are not
apparent from the folded band structure.

!!! note

    the whole output is much bigger than the sample provided above. For the
    band structure visualization proceed to the next step.

## Unfolded band structure

Lets visualize the unfolded band structure. It is different from a regular
band structure plot, though. Now we have one additional dimension -- the Bloch spectral weight.
There are several alternative visualization strategies. Here
we use the scatter plot with the point size proportional to the spectral weight.
The following MatLab script will help you build a graph for any
*fold2Bloch* output: [ubs_dots.m](fold2bloch_assets/ubs_dots.m)

Make sure the following parameters in *ubs_dots.m* are set as follows:

```matlab
KPATH = [0 1/2 0; ...
        0 0 0; ...
        0 0 1/2];

finpt='tfold2bloch_1o.f2b';
```

and:

```matlab
G = [0.3333333 0.0000000 0.0000000;
   0.000000  0.1666667 0.0000000;
   0.000000  0.000000  0.1111111]; % Reciprocal latt. vect. [Bohr^-1] from *.out
```

Reciprocal lattice vector information must match that in *tfold2bloch_1.abo*:

    Real(R)+Recip(G) space primitive vectors, cartesian coordinates (Bohr,Bohr^-1):
     R(1)=  3.0000000  0.0000000  0.0000000  G(1)=  0.3333333  0.0000000  0.0000000
     R(2)=  0.0000000  6.0000000  0.0000000  G(2)=  0.0000000  0.1666667  0.0000000
     R(3)=  0.0000000  0.0000000  9.0000000  G(3)=  0.0000000  0.0000000  0.1111111
     Unit cell volume ucvol=  1.6200000E+02 bohr^3
     Angles (23,13,12)=  9.00000000E+01  9.00000000E+01  9.00000000E+01 degrees

For graphing any other *fold2Bloch* output, make sure that the "%%Init Parameters"
are set accordingly to size of the supercell constructed.

After running the script you should see the following graph:

![](fold2bloch_assets/H6.png)

As you can see the unfolded band structure perfectly reproduces the
anticipated dispersion relation $E(k)=E_0 - 2A \cos(kb)$. We can even
estimate the magnitude of the hopping matrix element between adjacent atoms
$V_{ssG} = -A$. The band width is $4A = (-1) - (-12) = 11$ eV which results in $V_{ssG} = -2.75$ eV.

To analyze the results with AbiPy use:

    abiopen.py tfold2bloch_FOLD2BLOCH.nc

to open the file inside |ipython| and then type, at the request of ipython:

```ipython
# Plot unfolded bands along the path defined by kbounds.
# 'In [x]' are ipython prompts, do no type them, type only 'kbounds ...' then 'klabels ...' then 'abifile...'
In [1] kbounds = [0, 1/2, 0, 0, 0, 0, 0, 0, 1/2]
In [2] klabels = ["Y", r"$\Gamma$", "X"]
In [3] abifile.plot_unfolded(kbounds, klabels, title="Unfolded bands")
```

![](fold2bloch_assets/spectralw_unfolded.png)

See also
[this example](http://abinit.github.io/abipy/gallery/plot_fold2bloch.html#sphx-glr-gallery-plot-fold2bloch-py).
---
authors: DC, MT
---

# Parallelism for the ground state using wavelets

## Boron cluster, alkane molecule...

This tutorial explains how to run the calculation of an isolated system using a
wavelet basis-set on a parallel computer using MPI. You will learn the
different characteristics of a parallel run using the wavelet basis-set and
test the speed-up on a small **boron cluster of 14 atoms** followed by a test on a
bigger **alkane molecule**.

This tutorial should take about 90 minutes and requires you have several CPU
cores (up to 64 if possible).

You are supposed to know already some basics of parallelism in ABINIT,
explained in the tutorial [A first introduction to ABINIT in parallel](/tutorial/basepar).  

The tutorial will be more profitable if you have already performed calculations
using the wavelet formalism (see the [[topic:Wavelets|topic page on wavelets]]
and the [[usewvl]] keyword).

!!! Important
       To use a wavelet basis set `ABINIT` should have been compiled with the `bigdft` library.
       To do this, download the [bigdft fallback](https://www.abinit.org/fallbacks) and use
       the `--with-bigdft`, `BIGDFT_LIBS`, `BIGDFT_FCFLAGS`, etc. flags during the
       `configure` step.
 
[TUTORIAL_README]

## 1 Wavelets variables and parallelism

The parallelism with the wavelet formalism can be used for two purposes: to
**reduce the memory** load per node, or to reduce the overall **computation time**.

The MPI parallelization in the wavelet mode relies on the **orbital distribution**
scheme, in which the orbitals of the system under investigation are
distributed over the assigned MPI processes. This scheme reaches its limit
when the number of MPI processes is equal to the number of orbitals in the
simulation. To distribute the orbitals uniformly, the number of processors
must be a factor (divisor) of the number of orbitals. If this is not the case,
the distribution is not optimal, but the code tries to balance the load over
the processors. For example, if we have 5 orbitals and 4 processors, the
orbitals will have the distribution: `2/1/1/1`.

There are no specific input variables to use the parallelism in the wavelet
mode as the only parallelisation level is on orbitals. So running ABINIT with
an `mpirun` command is enough (this command differs according to the local MPI
implementation) such as:

       mpirun -n Nproc abinit < infile.abi >& logfile

For further understanding of the wavelet mode, or for citation purposes, one
may read [[cite:Genovese2008]]

## 2 Speed-up calculation for a boron cluster

We propose here to determine the speed-up in the calculation of the total
energy of a cluster made of 14 boron atoms.

![Boron14](paral_gswvl_assets/boron.jpg){width=30%}

Open the file `tgswvl_1.abi`. It
contains first the definition of the wavelet basis-set. One may want to test
the precision of the calculation by varying the [[wvl_hgrid]] and
[[wvl_crmult]] variables. This is not the purpose of this tutorial, so we will
use the given values (0.45 Bohr and 5).

{% dialog tests/tutoparal/Input/tgswvl_1.abi  %}

Run ABINIT with 3 processors. The overall time is printed at the end of the
output file (and of the log):

    Proc.   0 individual time (sec): cpu=         36.0  wall=         36.0

Read the output file to find the number of orbitals in the calculation (given
by the keyword [[nband]]). With the distribution scheme of the wavelet mode,
the best distribution over processors will be obtained for, 1, 3, 7 and 21
processors. Create four different directories (with the number of processors
for instance) and run four times ABINIT with the same input file, varying the
number of processors in {1, 3, 7, 21}.

    abinit tgswvl_1.abi >& log

The speed-up is the ratio between the time with one processor and the
time of a run with N processors.

Assuming that the directories are called {01, 03, 07, 21}, one can grep the
over-all time of a run and plot it with the [gnuplot](http://www.gnuplot.info) graphical tool.  
Just issue:

    gnuplot

and, in `gnuplot` command line, type:

    plot "< grep 'individual time' */*.abo | tr '/' ' '" u 1:(ttt/$11) w lp t "Boron cluster", x t "Ideal speed-up"

where `ttt` represents the time on one processor (replace `ttt` by this time in the
command line above).  

![Speedup for the Boron14 cluster](paral_gswvl_assets/speedup-B14.png){width=50%}

The efficiency (in percent) of the parallelization process is the ratio between the speed-up and the
number of processors. One can plot it (using [gnuplot](http://www.gnuplot.info)) with:

    plot "< grep 'individual time' */*.abo | tr '/' ' '" u 1:(ttt/$11/$1*100) w lp t "Boron cluster"

![Efficiency for the Boron14 cluster](paral_gswvl_assets/efficiency-B14.png){width=50%}

The first conclusion is that the efficiency is not so good when one use one
orbital per processor. This is a general rule with the wavelet mode: due to
the implementation, a good balance between speed and efficiency is obtained
for two orbitals per processor. One can also see that the efficiency generally
decreases with the number of processors.

This system is rather small and the amount of time spent in the overhead (read
the input file, initialise arrays...) is impacting the performance. Let's see
how to focus on the calculation parts.

## 3 Time partition

The wavelet mode is generating a `wvl_timings.yml` file at each run (warning: it will
erase any existing copy). This is a text file in [YAML format](https://en.wikipedia.org/wiki/YAML)
that can be read directly. There are three sections, giving the time of the initialisation
process (section`INITS`), the time of the SCF loop itself (section `WFN OPT`),
and the time for the post-processing (section `POSTPRC`).

!!! Note
	 The `wvl_timings.yaml` file is only created if [[timopt]]
	 is set to **10** in the input file.

Let's have a closer look to the SCF section. We can extract the following data
(the actual figures will vary between runs and number of processors):

    == WFN OPT:
     #  Class                    % , Time (s),   Max, Min Load (relative)
     Flib LowLevel         : [  6.8,      1.7,  1.16,  0.84]
     Communications        : [ 33.3,      8.1,  1.16,  0.85]
     BLAS-LAPACK           : [  0.1, 3.63E-02,  1.09,  0.91]
     PS Computation        : [  5.4,      1.3,  1.03,  0.89]
     Potential             : [  7.3,      1.8,  1.17,  0.58]
     Convolutions          : [ 42.4,      10.,  1.10,  0.88]
     Linear Algebra        : [  0.1, 3.61E-02,  1.03,  0.98]
     Other                 : [  1.3,     0.33,  1.23,  0.77]
     Initialization        : [  0.1, 2.64E-02,  1.35,  0.81]
     Total                 : [ 96.9,      24.,  1.00,  1.00]

	 # Category                  % ,  Time (s),   Max, Min Load (relative)
	  Allreduce, Large Size: [ 22.7,       5.5,  1.23,  0.88]
		Class              : Communications, Allreduce operations
	  Rho_comput           : [ 16.9,       4.1,  1.19,  0.79]
		Class              : Convolutions, OpenCL ported
	  Precondition         : [ 13.0,       3.2,  1.01,  0.99]
		Class              : Convolutions, OpenCL ported
	  ApplyLocPotKin       : [ 12.6,       3.1,  1.08,  0.89]
		Class              : Convolutions, OpenCL ported
	  Exchange-Correlation : [  7.3,       1.8,  1.17,  0.58]
		Class              : Potential, construct local XC potential
	  PSolver Computation  : [  5.4,       1.3,  1.03,  0.89]
		Class              : PS Computation, 3D SG_FFT and related operations
	  Init to Zero         : [  4.1,       1.0,  1.17,  0.78]
		Class              : Flib LowLevel, Memset of storage space
	  Allreduce, Small Size: [  3.4,      0.84,  1.89,  0.26]
		Class              : Communications, Allreduce operations
	  Pot_commun           : [  3.2,      0.79,  1.08,  0.93]
		Class              : Communications, AllGathrv grid
	  PSolver Communication: [  2.4,      0.59,  1.21,  0.96]
		Class              : Communications, MPI_ALLTOALL and MPI_ALLGATHERV
	  Array allocations    : [  2.3,      0.55,  1.16,  0.89]
		Class              : Flib LowLevel, Heap storage allocation
	  Un-TransComm         : [  1.1,      0.27,  1.07,  0.92]
		Class              : Communications, ALLtoALLV
	  Diis                 : [  0.7,      0.16,  1.24,  0.75]
		Class              : Other, Other
	  ApplyProj            : [  0.5,      0.11,  1.10,  0.89]
		Class              : Other, RMA pattern
	  Rho_commun           : [  0.4,  9.42E-02,  6.63,  0.02]
		Class              : Communications, AllReduce grid
	  Vector copy          : [  0.4,  8.59E-02,  1.56,  0.69]
		Class              : Flib LowLevel, Memory copy of arrays
	  Un-TransSwitch       : [  0.2,  5.99E-02,  1.65,  0.59]
		Class              : Other, RMA pattern
	  Blas GEMM            : [  0.1,  3.62E-02,  1.09,  0.91]
		Class              : BLAS-LAPACK, Matrix-Matrix multiplications
	  Chol_comput          : [  0.1,  3.58E-02,  1.03,  0.98]
		Class              : Linear Algebra, ALLReduce orbs
	  CrtLocPot            : [  0.1,  2.64E-02,  1.35,  0.81]
		Class              : Initialization, Miscellaneous
	  Routine Profiling    : [  0.1,  1.32E-02,  1.07,  0.90]
		Class              : Flib LowLevel, Profiling performances

With the total time of this `WFN OPT` section, one can compute the speed-up and the
efficiency of the wavelet mode more accurately:

      N processors   Time (s)   Speed-up  Efficiency (%)
           3           40.        2.1        69.2
           7           24.        3.5        49.4
          21           20.        4.2        19.8

With the percentages of the `wvl_timings.yaml` file, one can see that, for this
example, the time is mostly spent in communications, the precondionner,
the computation of the density and the application of the local part of the Hamiltonian.  
Let's categorise the time information:

  * The communication time (all entries of class `Communications`).
  * The time spent doing convolutions (all entries of class `Convolutions`).
  * The linear algebra part (all entries of class `Linear Algebra` and `BLAS-LAPACK`).
  * The other entries are in miscellaneous categories.

The summations are given in the file, on top of the section. One obtains the percentage
per category during the SCF loop:

     CLASS            PERCENT    TIME(sec)
     Convolutions      42.4        10.0
     Communications    33.3         8.1
     Potential          7.3         1.8
     Flib Lowlevel      6.8         1.7
     PS Computation     5.4         1.3
     Linear Algebra     0.2         0.1
     Other              4.6         1.8

You can analyse all the `wvl_timings.yaml` that have been generated for the different
number of processors and see the evolution of the different categories.

## 4 Orbital parallelism and array parallelism

If the number of processors is not a divisor of the number of orbitals, there
will be some processors with fewer orbitals than others. This is not the best
distribution from an orbital point of view. But, the wavelet mode also
distributes the scalar arrays like density and potentials by z-planes in real
space. So some parts of the code may become more efficient when used with a
bigger number of processors, like the Poisson Solver part for instance.

Run the boron example with {2, 4, 14, 15} processors and plot the speed-up.  

One can also look at the standard output to the **load balancing of orbitals**
and the **load balancing of the Poisson Solver** (with 15 processors):

With 15 processors the repartition of the 21 orbitals is the following:

     Processes from 0  to  9 treat 2 orbitals
     Processes from 10 to 10 treat 1 orbitals
     Processes from 11 to 14 treat 0 orbitals

With 15 processors, we can read in `log` file:

    [...]
	 Poisson Kernel Initialization:
	   MPI tasks                           :  15
	 Poisson Kernel Creation:
	   Boundary Conditions                 : Free
	   Memory Requirements per MPI task:
		 Density (MB)                      :  1.33
		 Kernel (MB)                       :  1.40
		 Full Grid Arrays (MB)             :  18.18
		 Load Balancing of calculations:
		   Density:
			 MPI tasks 0- 14               : 100%
		   Kernel:
			 MPI tasks 0- 13               : 100%
			 MPI task 14                   :  50%
		   Complete LB per task            : 1/3 LB_density + 2/3 LB_kernel
    [...]

As expected, one can see that:

* The load balancing per orbital is bad (4 processors are doing nothing)

* The load balancing of the scalar arrays distribution is not so good since
  the last processor will have a reduced array.
  
It is thus useless to run this job at 15 processors;
14 will give the same run time (since the load balancing will be better).

## 5 Speed-up calculation on a 65-atom alkane

Let's do the same with a bigger molecule and a finer grid. Open the file
`tgswvl_2.abi`. It contains the definition of an alkane chain of 65 atoms,
providing 64 orbitals.

![Boron14](paral_gswvl_assets/alkane.jpg){width=60%}

{% dialog tests/tutoparal/Input/tgswvl_2.abi %}

Run this input file with {1, 2, 4, 8, 16, 24, 32, 48, 64} processors.
The run with one processor should take less than one hour. If
the time is short, one can reduce [[wvl_hgrid]] in the input file to 0.45.

_Time measurements for a run over several processors of a $C_{21}H_{44}$ alkane chain_
![Speedup for the C21H44 alkane chain](paral_gswvl_assets/speedup-C21.png)
![Efficiency for the C21H44 alkane chain](paral_gswvl_assets/efficiency-C21.png)
![Time repartition for the C21H44 alkane chain](paral_gswvl_assets/time-C21.png)

As we obtained previously, the efficiency is generally lowered when the number
of processors is not a divisor of the number of orbitals (namely here 24 and 48).

## 6 Conclusion

With the wavelet mode, it is possible to efficiently decrease the run time by
increasing the number of processors. The efficiency is limited by the increase
of amount of time spent in the communications. The efficiency increases with
the quality of the calculation: the more accurate the calculations are (finer
_hgrid_...), the more efficient the code parallelization will be.
---
authors: XG, RC
---

# Fourth tutorial

## Aluminum, the bulk and the surface.

This tutorial aims at showing how to get the following physical properties for a metal and for a surface:

* the total energy
* the lattice parameter
* the relaxation of surface atoms
* the surface energy

You will learn about the smearing of the Brillouin zone integration, and also a bit about preconditioning the SCF cycle.

This tutorial should take about 1 hour and 30 minutes.

[TUTORIAL_README]

## Total energy and lattice parameters at fixed smearing and k-point grid

*Before beginning, you might consider to work in a different subdirectory, as for tutorials 1, 2 or 3.
Why not Work4?*


The following commands will move you to your working directory, create the *Work4* directory, and move you into that directory as you did in the previous tutorials.
Then, we copy the file *tbase4_1.abi* inside the *Work4* directory. The commands are:

```sh
cd $ABI_TESTS/tutorial/Input
mkdir Work4
cd Work4
cp ../tbase4_1.abi .
```

*tbase4_1.abi* is our input file. You should edit it and read it carefully,

{% dialog tests/tutorial/Input/tbase4_1.abi %}

and then a look at the following new input variables:

* [[occopt]]
* [[tsmear]]

Note also the following:

1. We will work at fixed [[ecut]] (6Ha). It is implicit that in *real research application*,
   you should do a convergence test with respect to *ecut*.
   Here, a suitable *ecut* is given to you in order to save time.
   It will give a lattice parameter that is 0.2% off of the experimental value.
   Note that this is the *softest* pseudopotential of those that we have used until now: the *01h.pspgth* for H needed 30 Ha
   (it was rather hard), the *Si.psp8* for Si needed 12 Ha. See the end of this page for a
   discussion of *soft* and *hard* pseudopotentials.

2. The input variable [[diemac]] has been suppressed.
   Aluminum is a metal, and the default value for this input variable is tailored for that case.

When you have read the input file, you can run the code, as usual (it will take a few seconds).

  abinit tbase4_1.abi > log 2> err &

Then, give a quick look at the output file.
You should note that the Fermi energy and occupation numbers have been computed automatically:

    Fermi (or HOMO) energy (hartree) =   0.27151   Average Vxc (hartree)=  -0.36713
    Eigenvalues (hartree) for nkpt=   2  k points:
    kpt#   1, nband=  3, wtk=  0.75000, kpt= -0.2500  0.5000  0.0000 (reduced coord)
      0.09836    0.25743    0.42131
        occupation numbers for kpt#   1
      2.00003    1.33305    0.00015
    prteigrs : prtvol=0 or 1, do not print more k-points.

You should also note that the components of the total energy include an entropy term:

    --- !EnergyTerms
    iteration_state     : {dtset: 1, itime: 3, icycle: 1, }
    comment             : Components of total free energy in Hartree
    kinetic             :  8.68009594268178E-01
    hartree             :  3.75144741427686E-03
    xc                  : -1.11506134985146E+00
    Ewald energy        : -2.71387012800927E+00
    psp_core            :  1.56870175692757E-02
    local_psp           :  1.66222476058238E-01
    non_local_psp       :  4.25215770913582E-01
    internal            : -2.35004517163717E+00
    '-kT*entropy'       : -7.99850001032776E-03
    total_energy        : -2.35804367164750E+00
    total_energy_eV     : -6.41656315078440E+01
    band_energy         :  3.72511439902163E-01
    ...

## The convergence study with respect to k-points

There is of course a convergence study associated to the sampling of the Brillouin zone.
You should examine different grids, of increasing resolution.
You might try the following series of grids:

    ngkpt1  2 2 2
    ngkpt2  4 4 4
    ngkpt3  6 6 6
    ngkpt4  8 8 8

with the associated [[nkpt]]:

    nkpt1  2
    nkpt2 10
    nkpt3 28
    nkpt4 60

The input file *tbase4_2.abi* is an example:

{% dialog tests/tutorial/Input/tbase4_2.abi %}

while *tbase4_2.abo* is a reference output file:

{% dialog tests/tutorial/Refs/tbase4_2.abo %}

The run might take a few seconds on a modern PC.

You will see that, **for the particular value** [[tsmear]] = 0.05 Ha, the lattice parameter
is already converged with [[nkpt]] = 10:

  acell1     7.6023827082E+00  7.6023827082E+00  7.6023827082E+00 Bohr
  acell2     7.5627822506E+00  7.5627822506E+00  7.5627822506E+00 Bohr
  acell3     7.5543007304E+00  7.5543007304E+00  7.5543007304E+00 Bohr
  acell4     7.5529744581E+00  7.5529744581E+00  7.5529744581E+00 Bohr

Note that there is usually a **strong** cross-convergence effect between the number of
k-points and the value of the broadening, [[tsmear]].
The right procedure is: for each value of *tsmear*, convergence with respect to the number of k-points,
then compare the k-point converged values for different values of *tsmear*.

In what follows, we will restrict ourselves to the grids with [[nkpt]] = 2, 10 and 28.

<a id="3"></a>
## The convergence study with respect to both number of k-points and broadening factor

The theoretical convergence rate as a function of [[tsmear]] heading to 0, in the case of [[occopt]] = 4, is cubic.
We rely on this value of [[occopt]] for this tutorial. Still, it might not be always robust,
as this value might yield difficulties to find univocally the Fermi energy.
A slightly worse convergence rate (quadratic) is obtained with [[occopt]] = 7, which is actually 
the recommended value for metallic systems.

Such convergence rates are obtained in the hypothesis of infinitely dense k-point grid.
We will check the evolution of [[acell]] as a function of [[tsmear]], for the following values
of *tsmear*: 0.01, 0.02, 0.03 and 0.04.

Use the double-loop capability of the multi-dataset mode,
with the *tsmear* changes in the **inner** loop. This will saves CPU time, as the wavefunctions
of the previous dataset will be excellent (no transfer to different k-points).

The input file *tbase4_3.abi* is an example:

{% dialog tests/tutorial/Input/tbase4_3.abi %}

while *tbase4_3.abo* is the reference output file.

{% dialog tests/tutorial/Refs/tbase4_3.abo %}

From the output file, here is the evolution of [[acell]]:

    acell11    7.6022357792E+00  7.6022357792E+00  7.6022357792E+00 Bohr
    acell12    7.6022341271E+00  7.6022341271E+00  7.6022341271E+00 Bohr
    acell13    7.6022341214E+00  7.6022341214E+00  7.6022341214E+00 Bohr
    acell14    7.6022357148E+00  7.6022357148E+00  7.6022357148E+00 Bohr
    acell21    7.5604102145E+00  7.5604102145E+00  7.5604102145E+00 Bohr
    acell22    7.5605496029E+00  7.5605496029E+00  7.5605496029E+00 Bohr
    acell23    7.5565044147E+00  7.5565044147E+00  7.5565044147E+00 Bohr
    acell24    7.5593333886E+00  7.5593333886E+00  7.5593333886E+00 Bohr
    acell31    7.5483073963E+00  7.5483073963E+00  7.5483073963E+00 Bohr
    acell32    7.5482393302E+00  7.5482393302E+00  7.5482393302E+00 Bohr
    acell33    7.5497784006E+00  7.5497784006E+00  7.5497784006E+00 Bohr
    acell34    7.5521340033E+00  7.5521340033E+00  7.5521340033E+00 Bohr

These data should be analyzed properly. For [[tsmear]] = 0.01, the converged value,
contained in *acell31*, must be compared to *acell11* and *acell21*:
between *acell21* and *acell31*, the difference is below 0.2%.
*acell31* can be considered to be converged with respect to the number of k-points, at fixed **tsmear**.
This **tsmear** being the lowest one, it is usually the most difficult to converge,
and the values acell31,32,33 and 34 are indeed well-converged with respect to the k-point number.
The use of the largest **tsmear** = 0.04, giving **acell34**, induces only a small error in the lattice parameter.
For that particular value of *tsmear*, one can use the second k-point grid, giving *acell24*.

!!! summary

    So to summarize: we can choose to work with a 10 k-point grid in the irreducible Brillouin
    zone, and the associated [[tsmear]] = 0.04, with less than 0.1% error on the lattice parameter.
    Note that this error due to the Brillouin zone sampling could add to the error
    due to the choice of [[ecut]] (that was mentioned previously to be on the order of 0.2%).

In what follows, we will stick to these values of [[ecut]] and [[tsmear]] and try to use k-point grids with a similar resolution.

Our final value for the aluminum lattice parameter, in the LDA, using the *Al.psp8* pseudopotential,
is thus 7.5593 Bohr, which corresponds to 4.0002 Angstrom. The experimental value at 25 Celsius is 4.04958 Angstrom, hence our theoretical value has an error of 1.2%. We caution that converged parameters should be used to properly assess the accuracy of a pseudopotential and functional.

The associated total energy and accuracy can be deduced from:

    etotal11   -2.3516656074E+00
    etotal12   -2.3532597160E+00
    etotal13   -2.3548538247E+00
    etotal14   -2.3564479440E+00
    etotal21   -2.3568282638E+00
    etotal22   -2.3574128355E+00
    etotal23   -2.3576771874E+00
    etotal24   -2.3578584768E+00
    etotal31   -2.3582092001E+00
    etotal32   -2.3581800122E+00
    etotal33   -2.3581917663E+00
    etotal34   -2.3582884106E+00

**etotal** 24 is -2.3578584768E+00 Ha, with an accuracy of 0.0005 Ha.

!!! tip

    To analyze the convergence of the total energy, one can use the |abicomp| script
    provide by |AbiPy| and the `gsr` command that will start an interactive |ipython| session
    so that we can interact directly with the AbiPy object.
    To load all the GSR files produced by calculation, use the command

        abicomp.py gsr tbase4_3o*GSR.nc

    then, inside the ipython terminal, execute the `plot_convergence` method of the `GsrRobot`:

    ```ipython
    In [1]: robot.plot_convergence("energy", sortby="nkpt", hue="tsmear")
    ```

    to produce this plot with the total energy in eV for different values of nkpt grouped by tsmear:

    ![](base4_assets/abicomp_tbase4_3o.png)

## Surface energy of aluminum (100): changing the orientation of the unit cell

In order to study the Aluminum (100) surface, we will have to set up a supercell representing a slab.
This supercell should be chosen as to be compatible with the primitive surface unit cell.
The corresponding directions are `[-1 1 0]` and `[1 1 0]`. The direction perpendicular to the surface is `[0 0 1]`.
There is no primitive cell of bulk aluminum based on these vectors, but a doubled cell.
We will first compute the total energy associated with this doubled cell.
This is not strictly needed, but it is a valuable intermediate step towards the study of the surface.

You might start from *tbase4_3.abi*. You have to change [[rprim]]. Still, try to keep [[acell]]
at the values of bulk aluminum that were determined previously.
But it is not all: the most difficult part in the passage to this doubled cell is the definition of the k-point grid.
Of course, one could just take a homogeneous simple cubic grid of k-points, but this will not
correspond exactly to the k-point grid used in the primitive cell in *tbase4_3.abi*.
This would not be a big problem, but you would miss some error cancellation.

The answer to this problem is given in the input file *$ABI_TESTS/tutorial/Input/tbase4_4.abi*.

{% dialog tests/tutorial/Input/tbase4_4.abi %}

The procedure to do the exact translation of the k-point grid will not be explained here (sorry for this).
If you do not see how to do it, just use homogeneous simple cubic grids, with about the same resolution
as for the primitive cell case. There is a simple rule to estimate **roughly** whether two
grids for different cells have the same resolution: simply multiply the linear dimensions of the k-point grids,
by the number of sublattices, by the number of atoms in the cell.
For example, the corresponding product for the usual 10 k-point grid is `4x4x4 x 4 x 1 = 256`.
In the file *tbase4_4.in*, one has `4x4x4 x 2 x 2 = 256`.
The grids of k-points should not be too anisotropic for this rough estimation to be valid.

Note also the input variables [[rprim]] and [[chkprim]] in this input file.

Now run *tbase4_4.abi* (the reference file is *$ABI_TESTS/tutorial/Refs/tbase4_4.abo*).
You should find the following total energy:

    etotal     -4.7164794308E+00

It is not exactly twice the total energy for the primitive cell, mentioned above, but the difference is less than 0.001 Ha.
It is due to the different FFT grids used in the two runs, and affect the exchange-correlation energy.
These grids are always homogeneous primitive 3D grids, so that changing the orientation of the lattice
will give mutually incompatible lattices. Increasing the size of the FFT grid would improve the agreement.

## Surface energy: a (3 aluminum layer + 1 vacuum layer) slab calculation

We will first compute the total energy associated with only three layers of aluminum,
separated by only one layer of vacuum.
This is kind of a minimal slab:

* one surface layer
* one "bulk" layer
* one surface layer
* one vacuum layer
* ...

It is convenient to take the vacuum region as having a multiple of the width of the aluminum layers, but this is not mandatory.
The supercell to use is the double of the previous cell (that had two layers of Aluminum atoms along the `[0 0 1]` direction).
Of course, the relaxation of the surface might give an important contribution to the total energy.

You should start from *tbase4_4.abi*.
You have to modify [[rprim]] (double the cell along `[0 0 1]`), the atomic positions, as well as the k-point mesh.
For the latter, it is supposed that the electrons cannot propagate from one slab to its image in the `[0 0 1]` direction,
so that the $k_z$ component of the special k-points can be taken 0: only one layer of k-points is needed along the z-direction.
You should also allow the relaxation of atomic positions, but not the relaxation of lattice parameters
(the lattice parameters along x or y must be considered fixed to the bulk value, while, for the z direction,
there is no interest to allow the vacuum region to collapse!

The input file *tbase4_5.abi* is an example,

{% dialog tests/tutorial/Input/tbase4_5.abi %}

while *tbase4_5.abo* is the reference output file.

{% dialog tests/tutorial/Refs/tbase4_5.abo %}

The run will take a few second on a modern PC.

The total energy after the first SCF cycle, when the atomic positions are equal to their starting values, is:

    ETOT  5  -7.0427135007667

The total energy of three aluminum atoms in the bulk,
(from section 4.3, etotal24 multiplied by three) is -7.0735754304 Ha.
Comparing the non-relaxed slab energy and the bulk energy, one obtains
the non-relaxed surface energy, per surface unit cell (there are two surfaces in our simulation cell!),
namely 0.01543 Ha = 0.420 eV.

The total energy after the Broyden relaxation is:

    etotal     -7.0429806856E+00

The relaxed surface energy, per surface unit cell, is obtained by comparing the bulk energy and the
relaxed slab energy, and gives 0.015297 Ha = 0.416 eV.
It seems that the relaxation energy is very small, compared to the surface energy, but we need to do the convergence studies.

## Surface energy: increasing the number of vacuum layers

One should now increase the number of vacuum layers: 2 and 3 layers instead of only 1.
It is preferable to define atomic positions in Cartesian coordinates.
The same coordinates will work for both 2 and 3 vacuum layers, while this is not the case for reduced coordinates,
as the cell size increases.

The input file *tbase4_6.abi* is an example input file,

{% dialog tests/tutorial/Input/tbase4_6.abi %}

while *tbase4_6.abo* is the reference output file.

{% dialog tests/tutorial/Refs/tbase4_6.abo %}

The run is on the order of of few seconds on a modern PC.

In the Broyden step 0 of the first dataset, you will notice the WARNING:

    scprqt:  WARNING -
     nstep=    6 was not enough SCF cycles to converge;
     maximum force difference=  6.859E-05 exceeds toldff=  5.000E-05

The input variable [[nstep]] was intentionally set to the rather low value of 6, to warn you about
possible convergence difficulties.
The SCF convergence might indeed get more and more difficult with cell size.
This is because the default preconditioner (see the notice of the input variable [[dielng]]) is not very good
for the metal+vacuum case.
For the interpretation of the present run, this is not critical, as the convergence criterion
was close of being fulfilled, but one should keep this in mind, as you will see.

For the 2 vacuum layer case, one has the non-relaxed total energy:

    ETOT  6  -7.0350152828531

giving the unrelaxed surface energy 0.0193 Ha = 0.525 eV;
and for the relaxed case:

    etotal1    -7.0358659542E+00

(this one is converged to the required level) giving the relaxed surface energy 0.0189 Ha = 0.514 eV

Note that the difference between unrelaxed and relaxed case is a bit larger than in the case of one vacuum layer.
This is because there was some interaction between slabs of different supercells.

For the 3 vacuum layer case, the self-consistency is slightly more difficult than with 2 vacuum layers:
the Broyden step 0 is not sufficiently converged (one might set [[nstep]] to a larger value, but the best
is to change the preconditioner, as described below)...
However, for the Broyden steps number 2 and beyond, because one takes advantage of the previous wavefunctions,
a sufficient convergence is reached.
The total energy, in the relaxed case, is:

    etotal2    -7.0371360761E+00

giving the relaxed surface energy `0.0182 Ha = 0.495 eV`.
There is a rather small 0.019 eV difference with the 2 vacuum layer case.

For the next run, we will keep the 2 vacuum layer case, and we know that the accuracy
of the coming calculation cannot be better than 0.019 eV. One might investigate the 4 vacuum layer case,
but this is not worth, in the present tutorial.

## Surface energy: increasing the number of aluminum layers

One should now increase the number of aluminum layers, while keeping 2 vacuum layers.
We will consider 4 and 5 aluminum layers. This is rather straightforward to set up, but will also change the preconditioner.
One could use an effective dielectric constant of about 3 or 5, with a rather small mixing coefficient, on the order of 0.2.
However, there is also another possibility, using an estimation of the dielectric matrix governed by [[iprcel]]=45.
For comparison with the previous treatment of SCF, one can recompute the result with 3 aluminum layers.

The input file *tbase4_7.abi* is an example, while

{% dialog tests/tutorial/Input/tbase4_7.abi %}

*tbase4_7.abo* is a reference output file.

{% dialog tests/tutorial/Refs/tbase4_7.abo %}

This run might take about one minute, and is the longest of the four basic tutorials. You should start it now.

You will notice that the SCF convergence is rather satisfactory, for all the cases (3, 4 or 5 metal layers).

For the 3 aluminum layer case, one has the non-relaxed total energy:

    ETOT  6  -7.0350153035193

(this quantity is converged, unlike in test 4.6) giving the unrelaxed surface energy 0.0193 Ha = 0.525 eV;
and for the relaxed case:

    etotal1    -7.0358683757E+00

(by contrast the difference with test 4.6 is less than 1 microHa) giving
the relaxed surface energy 0.0189 Ha = 0.514 eV.

For the 4 aluminum layer case, one has the non-relaxed total energy:

    ETOT  6  -9.3958299123967

giving the unrelaxed surface energy 0.0178 Ha = 0.484 eV; and for the relaxed case:

    etotal2    -9.3978596458E+00

giving the relaxed surface energy 0.0168 Ha = 0.457 eV.

For the 5 aluminum layer case, one has the non-relaxed total energy:

    ETOT  6  -11.754755842794

giving the unrelaxed surface energy 0.0173 Ha = 0.471 eV; and for the relaxed case:

    etotal3    -1.1755343136E+01

giving the relaxed surface energy 0.0170 Ha = 0.463 eV.

The relative difference in the surface energy of the 4 and 5 layer cases is on the order of 1.2%.

In the framework of this tutorial, we will not pursue this investigation, which is a simple application
of the concepts already explored.

Just for your information, and as an additional warning, when the work accomplished
until now is completed with 6 and 7 layers without relaxation
(see *\$ABI_TESTS/tutorial/Input/tbase4_8.abi* and *\$ABI_TESTS/tutorial/Refs/tbase4_8.abo* where 5, 6 and 7 layers are treated),
this non-relaxed energy surface energy behaves as follows:

number of aluminum layers | surface energy
--- | ---
3   | 0.525 eV
4   | 0.484 eV
5   | 0.471 eV
6   | 0.419 eV
7   | 0.426 eV

So, the surface energy convergence is rather difficult to reach. Our values, with a `4x4x1` grid,
a smearing of 0.04 Ha, a kinetic energy cut-off of 6 Ha, the *Al.psp8* pseudopotential,
still oscillate between 0.42 eV and 0.53 eV.
Increasing the k-point sampling might decrease slightly the oscillations, but note that this effect
is intrinsic to the computation of properties of a metallic surface: the electrons are confined inside the slab potential,
with sub-bands in the direction normal to the surface, and the Fermi energy oscillates with the width of the slab.
This effect might be understood based on a comparison with the behaviour of a jellium slab.
An error on the order of 0.019 eV is due to the thin vacuum layer.
Other sources of errors might have to be rechecked, seeing the kind of accuracy that is needed.

Experimental data give a surface energy around 0.55 eV (sorry, the reference is to be provided).

## Soft and hard pseudopotentials

In the context of a plane-wave basis, a *soft* pseudopotential means that a low [[ecut]]
will be required to obtain convergence whereas a *hard* pseudopotential implies that a high [[ecut]] will be needed.
It can be understood by considering the pseudo-wave-functions of that atom.
A *hard* pseudopotential has pseudo-wave-functions that have sharp features in real space which require many plane-waves to describe.

On the other hand, a *soft* pseudopotential has rather smooth pseudo-wave-functions that need
fewer plane-waves to describe accurately than the pseudo-wave-functions of *hard* pseudopotentials.
This designation is somewhat qualitative, and it is relative to other pseudopotentials.
In other words, a pseudopotential can be *soft* when compared to a certain pseudopotential but *hard* with respect to another.

In general, pseudopotentials describing light elements, those of the 2nd line of the periodic table,
and pseudopotentials that include semi-core states are considered *hard* as they have strongly peaked pseudo-wave-functions
that require a large [[ecut]].
This discussion is valid for norm-conserving pseudopotentials. With PAW pseudopotentials,
we are able to keep pseudo-wave-function smooth which means that they will require lower [[ecut]]
than their norm-conserving counterpart which is one of their main benefits.
---
authors: xuhe, NH
---

# Spin model in MULTIBINIT

## Build a spin model and run spin dynamics in MULTIBINIT

This lesson aims at showing how to build a spin model and run a spin dynamics calculation.

**Before beginning, it is important to know the theory of spin dynamics, which can be found in the literature (e.g.[[cite: Evans2014]], [[cite: Eriksson2017]] ).**

With this lesson, you will learn to:

  * Run a spin dynamics calculation with MULTIBINIT
  * Determine the critical temperature for a magnetic phase transition
  * Calculate spin canting angles for systems with Dzyaloshinskii-Moriya interaction

The TB2J python package, which can be used to generate a spin model, can be found on the website at [https://github.com/mailhexu/TB2J](https://github.com/mailhexu/TB2J). The online documenation can be found at [https://tb2j.readthedocs.io](https://tb2j.readthedocs.io/en/latest/).



*Before beginning, you might consider to work in a subdirectory for this tutorial. Why not Work_spindyn?*

[TUTORIAL_README]

## 1 Heisenberg Model formalism

The spin model, as implemented in MULTIBINIT, is defined as a classical Heisenberg model. In the current version of MULTIBINIT, we consider the following interactions: exchange interaction, single ion anisotropy (SIA), Dzyaloshinski-Moriya (DM) interaction, external magnetic fields. The total energy then reads as

$$E = E^{exc}+E^{SIA} + E^{DM}+E^{ext}.$$

The exchange energy $E^{exc}$ can be written as

$$E^{exc} =- \sum_{i\neq j} J_{ij} \vec{S_i}\cdot{\vec{S_j}}$$

A few conventions used in MULTIBINIT should be noted:

- all the $\vec{S}$ are normalized to 1.
- Both $J_{ij}$ and $J_{ji}$ are in the Hamiltonian.
- There is a minus sign, which means that the interaction is ferromagnetic for $J_{ij} >0$.

As the sites $i$ and $j$ defined in the model are in a finite cell (often the primitive cell), there are interactions between sites in the same cell and between sites in two different cells. The position vector of a site $j'$ in a different cell than site $i$ is denoted as $\vec{r}_{j'}=\vec{r}_j+\vec{R}$ with $\vec{R}$ being a combination of lattice vectors. For a site $j$ in the same cell as site $i$ the lattice vector $\vec{R}$ is $(0,0,0)$. Due to translation symmetry we can choose the lattice vector for site $i$ to be $\vec{R}=\vec{0}$. Hence, we denote the Heisenberg coefficients as $J_{ij}(\vec{R})$ and drop the prime for the sites in different cells.

The SIA term can be written as

$$E^{SIA}=-k_u \sum_i (\vec{S_i}\cdot \vec{e})^2,$$

where $k_u$ and $\vec{e}$ are the amplitude and direction of the single ion anisotropy.

The DM term can be written as

$$ E^{DM} = \sum_{i\neq j} \vec{D}_{ij}\cdot \vec{S_i}\times{\vec{S_j}},$$

where $\vec{D_{ij}}$ is the amplitude of the DM interaction.

The external magnetic field term can be written as

$$E^{ext}=- \sum_i   m_{i}   \vec{S_i}\cdot \vec{H},$$

where $m_i$ denotes the magnetic moment of site $i$, and $\vec{H}$ is the magnetic field.



## 2. Build spin model file

One way to calculate the Heisenberg model parameters is to use the spin force theorem (see [[cite:Liechtenstein1983]], [[cite:Katsnelson2000]]), for which one perturbs the system by rotating  localized spins. In ABINIT, the Hamiltonian uses plane waves as a basis set, thus the localized spin is not directly accessible. We can construct localized Wannier functions and rewrite the Hamiltonian in the Wannier basis. Then, the exchange parameters can be calculated from this Hamiltonian ( [[cite:Korotin2015]] ). 

For building the Wannier function Hamiltonian from ABINIT, see the tutorial [wannier90](/tutorial/wannier90). Other DFT codes interfaced with [Wannier90](http://www.wannier.org) , can also be used. Then, the  [TB2J](https://github.com/mailhexu/TB2J) package can be used to calculate the Heisenberg model parameters and generate the input model for MULTIBINIT. The data will be stored in a xml (.xml) or a netcdf (.nc) file which is used as input for the MULTIBINIT calculation. For the tutorial, this file is provided. Please read the [TB2J tutorial](https://tb2j.readthedocs.io/en/latest/) to see how to create your own xml/netcdf file. 

## 3. Run spin dynamics

### Basic: how to use MULTIBINIT to run spin dynamics

Once we have the spin model xml file, we can run a spin dynamics calculation with MULTIBINIT. Example input files can be found at ~abinit/tests/tutomultibinit/Input/tmulti5_1.* .  There are three files:

* "tmulti5_1.files" is the "files" file, which gives the names of the input and output files for  MULTIBINIT.
* "tmulti5_1.abi" is the main input file containing the parameters for the spin dynamics simulation.
* "tmulti5_1.xml" is the file containing the Heisenberg model parameters. 

You can copy these three files into a directory (e.g. Work_spindyn).

In tmulti5_1.files, three file names are given:

```
tmulti5_1.abi
tmulti5_1.abo
tmulti5_1.xml
```

which gives the input, output and xml file names. The file tmulti5_1.xml contains the $J_{ij}$ values for a simple toy system which has a cubic lattice and one atom per unit cell. Its critical temperature is around 600K.

In tmulti5_1.abi, the variables for running a spin dynamics calculation are given:

```
prt_model = 0
ncell =   16 16 16              ! number of unit cells in supercell

spin_dynamics = 1               ! switch on spin dynamics
spin_init_state = 2             ! ferromagnetic initial state

spin_temperature = 600          ! temperature of spin (Kelvin)
spin_ntime_pre = 10000          ! time steps for thermolization
spin_ntime = 20000              ! time steps for measurement
spin_nctime = 100               ! Number of time steps between two writes
                                ! into netcdf
spin_dt = 1e-16 s               ! Time step (seconds)
spin_projection_qpoint= 0.0 0.0 0.0       ! Wave vector for summation of spin in each
                                ! sublattice.

spin_write_traj = 0             ! do not write spin trajectory to netcdf file
```

To run spin dynamics with MULTIBINIT

```
cd Work_spindyn
multibinit --F03 < tmulti5_1.files > tmulti5_1.txt 
```

Note that the .files file will be deprecated in the next version of ABINIT and MULTIBINIT. Then only two files are required. The following variables in the input file can be used to specify the spin potential file and the prefix of the output files. 

```
spin_pot_fname = "tmulti5_1.xml"
outdata_prefix = "tmulti5_1.abo"
```

To run the spin dynamics without using the files file, 

```
multibinit tmulti5_1.abi --F03 > tmulti5_1.txt 
```

After the calculation is done, you will find an output file named tmulti5_1.abo and a netcdf file tmulti5_1.abo_spinhist.nc.

In the .abo file, you can find the lines below, which give a overview of the evolution of the system with time:

```
Beginning spin dynamic steps :
==================================================================
    Iteration          time(s)         Avg_Mst/Ms      ETOT(Ha/uc)
------------------------------------------------------------------
Thermalization run:
-           100      9.90000E-15      6.52530E-01     -2.13916E-03
-           200      1.99000E-14      5.58122E-01     -1.82158E-03
-           300      2.99000E-14      5.28037E-01     -1.78953E-03
  .....
Measurement run:
-           100      9.90000E-15      4.45356E-01     -1.70973E-03
-           200      1.99000E-14      4.23270E-01     -1.68122E-03
-           300      2.99000E-14      4.07006E-01     -1.61948E-03
  .....
```

Here, the Avg_mst ($||<m_i e^{2\pi \vec{q}\cdot\vec{R_i}}>||$) means the average staggered magnetic moment, Ms is the saturated magnetic moment.
If all the spins for the wave-vector ($\vec{q}$) [[multibinit:spin_projection_qpoint]] are aligned ,
the value for Avg_Mst/Ms is 1.0. it deviates from 1.0 due to thermal fluctuations.
The last column states the total energy of the system per unit cell.

There are two stages, the warming up and measurement, in the whole spin dynamics process.
During the thermalization the spins evolve towards the equilibrium state for the temperature defined in the input file.
During the measurement stage, the steps are sampled for the calculation of the observables.

At the end of the run, there is a summary of the calculation

```
Summary of spin dynamics
   At the end of the run, the average spin at each sublattice is
      Sublattice       <M_i>(x)  <M_i>(y)  <M_i>(z)  ||<M_i>||
-        0001          -0.31575   0.08664   0.24544   0.40921

```

For structures with more than one magnetic site in the unit cell (sublattices), a separate line will be printed for each sublattice. This allows us to see how the spins in the different sublattices are aligned to each other.

The following observables are printed, which are:

```
# Temperature              Cv             chi        BinderU4             Mst
    600.00000     8.26236E+03     6.40552E-02     0.66303E+00     4.19057E-01
```

* Cv: volume specific heat:

  $C_v=(<E^2>-<E>^2)/(k_B^2 T^2)$ .

  $<E>$ means average of energy per unit cell during the observation time. At zero temperature $C_v=0$.  $C_v$ is in atomic unit.

* chi ($\chi$): the isothermal susceptibility:

  $\chi=\frac{\partial <m>}{\partial H}= (<m^2>-<m>^2)/(k_B T)$ .

  $<m>$ is the average of the total staggered magnetic moment.  At zero temperature, $\chi$ is not well defined.

* BinderU4: The Binder cumulant, which is

  $U_4=1-\frac{<m^4>}{3<m^2>^2}$.

  In a 3D Heisenberg system, $U_4$ goes to 2/3 when $T<T_C$ and goes to 4/9 when $T >T_C$ .

* Mst: The staggered magnetic moment, which is defined as:

  $M=\sum_i m_i \exp(i \vec{q}\cdot{\vec{R_i}})$.

  Here Mst is normalized to the saturated magnetic moment so the maximum of M is 1.

In the netcdf file, the trajectories of the spins can be found. They can be further analyzed using post-processing tools.

We are now coming back to the values chosen for the input variables in the tmulti5_1.abi file. It is essential to choose these values such that the results of the calculation are meaningful. Therefore, we recommend a convergence study concerning the following parameters:

* time step ([[multibinit: spin_dt]]):

Typical time steps are about $10^{-15}$ to $10^{-17}$ s.
An optimal time step can be determined by trying several values and comparing the results (equilibrium magnetic order, moments, etc) to a calculation with a small time step (e.g. $10^{-17}$ s).
At this stage, a small box and a temperature close to zero can be used.

* supercell size ([[multibinit:ncell]])

  Due to the periodic boundary condition, the spins between periods could be correlated with each other, which can lead to an artificial increase in, e.g., the phase transition temperature. Also, certain quantities cannot be sampled using a small box. Hence, it is required to test if the quantity of interest is converged with the supercell size.

  For anti-ferromagnetic structures, or more generally, structures with non-zero wave vector, the box size should allow the spins to fit to the q-vector, i.e. ($\vec{q}\cdot \vec{n}$) should be integers. For some structures, it is not easy or sometimes impossible to find such $\vec{n}$. In these cases, a large box is usually required.

* Thermalization time([[multibinit: spin_ntime_pre]])

    The thermalization time should at least allow the spins to relax to the equilibrium state. To see how much time is needed for the system to get to the equilibrium state, we can plot the magnetic moment as a function of time. It should be noted that the relaxation to the equilibrium state usually takes much longer near the phase transition temperature. Therefore, it is important to test the relaxation time.

* Measurement time ([[multibinit: spin_ntime]])

   In order to calculate some observables, longer times (e.g. 10 times the relaxation time) are required so enough samples can be generated.

### A real world example: $LaFeO_3$

A most common usage of spin dynamics is to calculate the magnetic quantities (e.g. magnetic moments, susceptibility, specific heat ) as a function of temperature and determine the critical  temperature where a phase transition from one magnetic phase to another occurs.

By setting [[multibinit:spin_var_temperature]] to 1 and specifying the starting temperature, final temperature, and the number of steps, a series of calculations is carried out. (See e.g. ~abinit/tests/tutomultibinit/Input/tmulti5_2.* )

(*Note that some of the parameters in the input file are set to "bad" values. Let's try to tune them to make a meaningful calculation.* )

```
dynamics =  0                   ! Disable molecular dynamics
ncell =   6 6 6                 ! Size of supercell (Is this too small?)
spin_dynamics=1                 ! Run spin dynamics
spin_ntime_pre = 10000          ! Thermolization steps (Is this enough?)
spin_ntime = 10000              ! Measurement steps. (Is this enough?)
spin_nctime = 100               ! Number of time steps between two writes
                                ! into netcdf
spin_dt = 1e-16 s               ! Time step (Is this too large?)
spin_init_state = 2             ! Ferromagnetic initial state (problematic?)
spin_projection_qpoint = 0.0 0.0 0.0       ! Wave vector of spin order

spin_var_temperature = 1        ! Variable temperature calculation
spin_temperature_start = 0      ! Starting temperature
spin_temperature_end = 500      ! Final temperature (Smaller than Neel temp.?)
spin_temperature_nstep = 6      ! Number of temperature steps (Is this enough?)

spin_sia_add = 1              ! add a single ion anistropy (SIA) term?
spin_sia_k1amp = 1e-6         ! amplitude of SIA (in Ha), how large should be used?
spin_sia_k1dir = 0.0 0.0 1.0  ! direction of SIA

spin_calc_thermo_obs = 1      ! calculate thermodynamics related observables

```

Note that you are now running several calculations for different temperatures, so this might take a minute or two. After the run, the trajectories for each temperature will be written into the \*\_T0001_spin_hist.nc to \*\_T0006_spin_hist.nc files if spin_temperature_step=6.

There are several ways to find the critical temperature. The most natural way is to use the M-T curve. However, there are some difficulties because the change of the magnetic moment is not abrupt at the critical temperature, and its value is sensitive to the box size. The specific heat and the magnetic susceptibility diverge at $T_c$ and are therefore more reliable to determine the critical temperature. The specific heat has the additional advantage that we do not need to know the magnetic order to calculate it. Another option is to calculate the Binder cumulant, defined as $U_4= 1.0- \frac{<m^4>}{3 <m^2> }$, which is less sensitive to the box size and also changes abruptly at $T_c$.

These quantities can be found at the end of the various T calculation in the output file:

```
 Summary of various T run:
# Temperature              Cv             chi        BinderU4             Mst
      0.00000     0.00000E+00     1.46625E-13     0.66667E+00     1.00000E+00
    100.00000     8.52440E+02     1.01185E+00     0.66666E+00     9.57752E-01
    200.00000     7.65946E+02     1.49460E+00     0.66664E+00     9.12555E-01
    300.00000     9.90440E+02     2.63298E+00     0.66660E+00     8.65566E-01
....
```

They can also be found in the \*.varT file so it's easy to plot the observables as functions of temperature. The average magnetization of each sublattice is also in this file.

If the input parameters are well tuned you will obtain the curves for the different quantities like the following. From the result we can see the Neel temperature is about 800 K (The experimental $T_N$ of LaFeO3 is about 740 K).



![tmulti5_2](spin_model_assets/tmulti5_2.png)



### Example with DMI: 1D canted AFM-chain

We now study a system with Dzyaloshinskii-Moriya interaction (DMI). The system is a simple 1D chain with a unit cell consisting of two sites A and B, as shown below. The exchange between A and B is $J= 5$ meV, and the DMI $\vec{D}= (0, 0, 2)$ meV. The arrow from A to B means $D_{AB}$, where $D_{AB}=-D_{BA}$.

The input files can be found in ~abinit/tests/tutomultibinit/Input/tmulti5_3.*



![canting_DMI](spin_model_assets/canting_DMI.png)

In this system, the exchange favors a collinear spin alignment, while the DMI favors the spins to be perpendicular to their neighbors. Usually, the DMI is much smaller than the exchange interaction, thus the system has a canted AFM spin alignment. We can run spin dynamics at zero temperature to get the ground state and calculate the canting angle.

```
prt_model = 0
dynamics =  0                   ! disable molecular dynamics

ncell =   4 1 1                 ! size of supercell.
spin_dynamics= 2                ! enable spin dynamics. Depondt-Mertens algorithm.
spin_ntime_pre =100000          ! warming up steps.
spin_ntime =100000              ! number of steps.
spin_nctime=1000                ! number of  time steps between two nc file write
spin_dt=1e-16 s                 ! time step.
spin_init_state = 1             ! start from random
spin_temperature = 1e-9         ! spin temperature. It is usually better to avoid 0.

spin_sia_add = 1                ! add a single ion anistropy (SIA) term.
spin_sia_k1amp = 1e-4 eV        ! amplitude of SIA, 0.1 meV. +: easy axis, -: hard axis
spin_sia_k1dir = 1.0 0.0 0.0    ! direction of SIA, easy axis along x.
```

Here, we add a relatively small (0.1 meV) single ion anisotropy term so that the easy axis is along x. Note that the DMI $\vec{D}$ is along the z-axis, therefore it lowers the energy if the spins have x and y components, while in the xy plane, the energy is isotropic. Hence, a SIA along x can be useful to break this symmetry. We can try to rotate the SIA direction in the xy plane to see if the result changes, and also try a SIA z-direction to see what happens.

At the end of spin dynamics, we can find the following output, which is the last snapshot of the spins.

It shows that the the spins have anti-parallel alignment along the easy axis (x), with a canting towards the y-axis. The canting angle is about arctan(0.187/0.982).

```
    At the end of the run, the average spin at each sublattice is
      Sublattice       <M_i>(x)  <M_i>(y)  <M_i>(z)  ||<M_i>||
-        0001          -0.98217  -0.18802  -0.00000   1.00000
-        0002           0.98207  -0.18851  -0.00000   1.00000
```





### Tips:

* Anti-ferromagnetic/ spin spiral structure.

  In the example above, the magnetic structure is anti-ferromagnetic,  where the unit cell is a multiple of the spin period. Sometimes the unit cell used does not contain the full period of spin, e.g. in a simple cubic AFM lattice with only one atom in the primitive cell.  We can use the magnetic wave vector to calculate the staggered magnetic moment. This is also useful for spin spiral structures, etc.

  ```
  spin_projection_qpoint = 0.5 0.5 0.5
  ```
  






---
authors: DRH
---

# Tutorial on elastic properties

## Elastic and piezoelectric properties.

This tutorial shows how to calculate physical properties related to strain, for
an insulator and a metal:

  * the rigid-atom elastic tensor
  * the rigid-atom piezoelectric tensor (insulators only)
  * the internal strain tensor
  * the atomic relaxation corrections to the elastic and piezoelectric tensor

You should complete tutorials [RF1](/tutorial/rf1) and [RF2](/tutorial/rf2)
to introduce the density-functional perturbation theory (DFPT) features of
ABINIT before starting this tutorial. You will learn to use additional DFPT
features of ABINIT, and to use relevant parts of the associated codes Mrgddb and Anaddb.

Visualisation tools are NOT covered in this tutorial.
Powerful visualisation procedures have been developed in the Abipy context,
relying on matplotlib. See the README of [Abipy](https://github.com/abinit/abipy)
and the [Abipy tutorials](https://github.com/abinit/abitutorials).

This tutorial should take about two hours.

[TUTORIAL_README]

## 1 The ground-state geometry of (hypothetical) wurtzite AlP

*Before beginning, you might consider working in a different subdirectory as for the other tutorials.
Why not create Work_elast in \$ABI_TESTS/tutorespfn/Input?*
You should also copy the file *telast_1.abi* from *\$ABI_TESTS/tutorespfn/Input* to *Work_elast*.

```sh
cd $ABI_TESTS/tutorespfn/Input
mkdir Work_elast
cd Work_elast
cp ../telast_1.abi .
```

You may wish to start the calculation (less than
one minute on a standard 3GHz machine) before you read the following.

    abinit telast_1.abi > telast_1.log &

Then, you should open your input file *telast_1.abi* with an editor and examine it as you read this discussion.

{% dialog tests/tutorespfn/Input/telast_1.abi %}

The hypothetical wurtzite structure for AlP retains the tetrahedral
coordination of the atoms of the actual zincblende structure of AlP, but has a hexagonal lattice.
It was chosen for this tutorial because the atomic
positions are not completely determined by symmetry.
Both the atomic positions and the lattice constants should be optimized before beginning DFPT
calculations, especially those related to strain properties.
While GS structural optimization was treated in tutorials 1-3, we are introducing a few
new features here, and you should look at the following new input variables which will be discussed below:

  * [[getxred]]
  * [[iatfix]]
  * [[natfix]]
  * [[strfact]]

There are two datasets specified in *telast_1.abi*. First, let us examine the
common input data. We specify a starting guess for [[acell]], and give an
accurate decimal specification for [[rprim]]. The definition of the atom
types and atoms follows [tutorial DFPT1](/tutorial/rf1). The reduced atomic
positions [[xred]] are a starting approximation, and will be replaced by our
converged results in the remaining input files, as will [[acell]].

We will work with a fixed plane wave cutoff [[ecut]] (=6 Ha), but introduce
[[ecutsm]] (0.5 Ha) as in [tutorial 3](/tutorial/base3) to smear the cutoff,
which produces smoothly varying stresses as the lattice parameters are
optimized. We will keep the same value of [[ecutsm]] for the DFPT calculations
as well, since changing it from the optimization run value could reintroduce
non-zero forces and stresses. For the k-point grid, we must explicitly specify
[[shiftk]] since the default value results in a grid shifted so as to break
hexagonal symmetry. The RF strain calculations check this, and will exit with
an error message if the grid does not have the proper symmetry.
The self-consistency procedures follow [tutorial RF1](/tutorial/rf1).

Dataset 1 optimizes the atomic positions keeping the lattice parameters fixed,
setting [[ionmov]]=2 as in [tutorial 1](/tutorial/base1). The optimization
steps proceed until the maximum force component on any atom is less than
[[tolmxf]]. It is always advised to relax the forces before beginning the
lattice parameter optimization. Dataset 2 optimizes the lattice parameters
with [[optcell]]=2 as in [tutorial 3](/tutorial/base3). However, tutorial 3
treats cubic Si, and the atom positions in reduced coordinates remained
fixed. In the present, more general case, the reduced atomic coordinates must
be reoptimized as the lattice parameters are optimized. Note that it is
necessary to include [[getxred]] = -1 so that the second dataset is
initialized with the relaxed coordinates. Coordinate and lattice parameter
optimizations actually take place simultaneously, with the computed stresses
at each step acting as forces on the lattice parameters. We have introduced
[[strfact]] which scales the stresses so that they may be compared with the
same [[tolmxf]] convergence test that is applied to the forces. The default
value of 100 is probably a good choice for many systems, but you should be
aware of what is happening.

From the hexagonal symmetry, we know that the positions of the atoms in the
a-b basal plane are fixed. However, a uniform translation along the c axis of
all the atoms leaves the structure invariant. Only the relative displacement
of the Al and As planes along the c axis is physically relevant. We will fix
the Al positions to be at reduced c-axis coordinates 0 and 1/2 (these are
related by symmetry) by introducing [[natfix]] and [[iatfix]] to constrain the
structural optimization. This is really just for cosmetic purposes, since
letting them all slide an arbitrary amount (as they otherwise would) won't
change any results. However, you probably wouldn't want to publish the results
that way, so we may as well develop good habits.

Now we shall examine the results of the structural optimization run. As
always, we should first examine the log file to make sure the run has
terminated cleanly. There are a number of warnings, but none of them are
apparently serious. Next, let us edit the output file, *telast_1.abo*.

{% dialog tests/tutorespfn/Refs/telast_1.abo %}

The first thing to look for is to see whether Abinit recognized the symmetry of the
system. In setting up a new data file, it's easy to make mistakes, so this is a valuable check.
We see

    DATASET    1 : space group P6_3 m c (#186); Bravais hP (primitive hexag.)

which is correct. Next, we confirm that the structural optimization converged.
The following lines tell us that things are OK.
From dataset 1 :

    At Broyd/MD step   4, gradients are converged :
    max grad (force/stress) = 2.7264E-08 < tolmxf= 1.0000E-06 ha/bohr (free atoms)

and from dataset 2 :

    At Broyd/MD step  14, gradients are converged :
    max grad (force/stress) = 5.3969E-07 < tolmxf= 1.0000E-06 ha/bohr (free atoms)

We can also confirm that the stresses are relaxed:

    Cartesian components of stress tensor (hartree/bohr^3)
     sigma(1 1)=  1.12504637E-09  sigma(3 2)=  0.00000000E+00
     sigma(2 2)=  1.12504637E-09  sigma(3 1)=  0.00000000E+00
     sigma(3 3)= -2.49308610E-09  sigma(2 1)=  0.00000000E+00

Now would be a good time to copy *telast_2.abi* into your
working directory, since we will use the present output to start the next run.
Locate the optimized lattice parameters and reduced atomic coordinates near
the end of *telast_1.abo*:

     acell2    7.2488954246E+00  7.2488954246E+00  1.1879499870E+01 Bohr

     xred2     3.3333333333E-01  6.6666666667E-01  0.0000000000E+00
               6.6666666667E-01  3.3333333333E-01  5.0000000000E-01
               3.3333333333E-01  6.6666666667E-01  3.7517446813E-01
               6.6666666667E-01  3.3333333333E-01  8.7517446813E-01

With your editor, copy and paste these into *telast_2.abi* at the indicated
places in the "Common input data" area. Be sure to change acell2 and xred2 to
acell and xred since these common values will apply to all datasets in the next set of calculations.

## 2 Response-function calculations of several second derivatives of the total energy

We will now compute second derivatives of the total energy (2DTE's) with
respect to all the perturbations we need to compute elastic and piezoelectric
properties. You may want to review the first paragraphs of the [[help:respfn]]
which you studied in [tutorial RF1](/tutorial/rf1).
We will introduce only one new input variable for the strain perturbation,

  * [[rfstrs]]

The treatment of strain as a perturbation has some subtle aspects. It would be
a good idea to read  Metric tensor formulation of strain in density-functional
perturbation theory, by D. R. Hamann, Xifan Wu, Karin M. Rabe, and David Vanderbilt [[cite:Hamann2005]]
especially Sec. II and Sec. IV. We will do all the RF calculations you learned in [tutorial RF1](/tutorial/rf1) together
with strain, so you should review the variables

  * [[rfphon]]
  * [[rfatpol]]
  * [[rfdir]]
  * [[rfelfd]]

If not yet done (see end of previous section), copy *telast_2.abi* into *Work_elast* and start the
calculation while you read (less than 2 minutes on a standard 3GHz machine). We now
assume that you know which command to use to launch ABINIT.
Look at *telast_2.abi* in your editor to follow the discussion.

{% dialog tests/tutorespfn/Input/telast_2.abi %}

This has been set up as a self-contained calculation with three datasets. The
first is simply a GS run to obtain the GS wave functions we will need for the
DFPT calculations. We have removed the convergence test from the common input
data to remind ourselves that different tests are needed for different
datasets. We set a tight limit on the convergence of the self-consistent
potential with [[tolvrs]]. Since we have specified [[nband]]=8, all the bands
are occupied and the potential test also assures us that all the wave
functions are well converged. This issue will come up again in the section on
metals. We could have used the output wave functions *telast_1o_DS2_WFK* as
input for our RF calculations and skipped dataset 1, but redoing the GS
calculation takes relatively little time for this simple system.

Dataset 2 involves the calculation of the derivatives of the wave functions
with respect to the Brillouin-zone wave vector, the so-called ddk wave
functions. Recall that these are auxiliary quantities needed to compute the
response to the [[lesson:rf1#5|electric field perturbation]] and
introduced in [tutorial RF1](/tutorial/rf1). It would be a good idea to review the relevant
parts of [[help:respfn#1|section 1]] of the respfn_help file.

Examining this section of *telast_2.abi*, note that electric
field as well as strain are uniform perturbations, so that they are defined only for
[[qpt]] = 0 0 0. [[rfelfd]] = 2 specifies that we want the ddk calculation to be
performed, which requires [[iscf]] = -3. The ddk wave functions will be used to
calculate both the piezoelectric tensor and the Born effective charges, and in
general we need them for **k** derivatives in all three (reduced) directions,
[[rfdir]] = 1 1 1. Since there is no potential self-consistency in the ddk
calculations, we must specify convergence in terms of the wave function
residuals using [[tolwfr]].

Finally, dataset 3 performs the actual calculations of the needed 2DTE's for
the elastic and piezoelectric tensors. Setting [[rfphon]] = 1 turns on the
atomic displacement perturbation, which we need for all atoms ([[rfatpol]] = 1
4) and all directions ([[rfdir]] = 1 1 1). Abinit will calculate first-order
wave functions for each atom and direction in turn, and use those to calculate
2DTE's with respect to all pairs of atomic displacements and with respect to
one atomic displacement and one component of electric field. These quantities,
the interatomic force constants (at $\Gamma$) and the Born effective charges will
be used later to compute the atomic relaxation contribution to the elastic and
piezoelectric tensor.

First-order wave functions for the strain perturbation are computed next.
Setting [[rfstrs]] = 3 specifies that we want both uniaxial and shear strains
to be treated, and [[rfdir]] = 1 1 1 cycles through strains xx, yy, and zz for
uniaxial and yz, xz, and xy for shear. We note that while other perturbations
in Abinit are treated in reduced coordinates, strain is better dealt with in
Cartesian coordinates for reasons discussed in the reference cited above.
These wave functions are used to compute three types of 2DTE's. Derivatives
with respect to two strain components give us the so-called rigid-ion elastic
tensor. Derivatives with respect to one strain and one electric field
component give us the rigid-ion piezoelectric tensor. Finally, derivatives
with respect to one strain and one atomic displacement yield the internal-strain 
force-response tensor, an intermediate quantity that will be necessary
to compute the atomic relaxation corrections to the rigid-ion quantities. As
in [tutorial DFPT1](/tutorial/rf1), we specify convergence in terms of the residual of the
potential (here the first-order potential) using [[tolvrs]].

Your run should have completed by now. Abinit should have created quite a few files,
among which:

  * telast_2.abo (main output file)
  * telast_2o_DS1_DDB (first derivatives of the energy from GS calculation)
  * telast_2o_DS3_DDB (second derivatives from the RF calculation)
  * telast_2o_DS1_WFK (GS wave functions)
  * telast_2o_DS2_1WF* (ddk wave functions)
  * telast_2o_DS3_1WF* (RF first-order wave functions from various perturbations)

The derivative database DDB files are ascii and
readable, but primarily for subsequent analysis by anaddb which we will
undertake in the next section. Finally, the various wave function binary files
are primarily of use for subsequent calculations, where they could cut the
number of needed iterations in, for example, convergence testing. 
File names have been generated according to the following convention.
After the "root" name (which is by default taken from the name of the .abi file),
follows the number of the dataset producing the file.
Finally, the first-order wave function 1WF files have a final "pertcase"
number described in [[help:respfn#1|section 1]] of the [[help:respfn|respfn_help file]].
While *telast_2.abi* specifies all atomic displacements, only the 
symmetry-inequivalent perturbations are treated, so the "pertcase" list is incomplete.
All cases specified in the input data are treated for the strain perturbation.

First, take a look at the end of the \*log file to make sure the run
has completed without error. You might wish to take a look at the WARNING's,
but they all appear to be harmless. Next, edit your *telast_2.abo* file.
Searching backwards for ETOT you will find

         iter   2DEtotal(Ha)        deltaE(Ha) residm    vres2
    -ETOT  1   2.6864034257676     -1.394E+01 7.628E+01 3.415E+02
     ETOT  2   1.5949143903393     -1.091E+00 3.669E-02 4.465E+00
     ETOT  3   1.5767914745018     -1.812E-02 5.026E-05 3.449E-01
     ETOT  4   1.5758067303228     -9.847E-04 9.600E-07 6.237E-03
     ETOT  5   1.5757929703648     -1.376E-05 6.251E-08 3.303E-05
     ETOT  6   1.5757928924185     -7.795E-08 1.128E-09 3.010E-07
     ETOT  7   1.5757928914135     -1.005E-09 9.367E-11 6.841E-09
     ETOT  8   1.5757928913731     -4.046E-11 1.281E-12 1.312E-10
     ETOT  9   1.5757928913723     -7.105E-13 1.065E-13 8.070E-12

     At SCF step    9       vres2   =  8.07E-12 < tolvrs=  1.00E-10 =>converged.

Abinit is solving a set of Schroedinger-like equations for the first-order
wave functions, and these functions minimize a variational expression for the
2DTE (technically, they are called self-consistent Sternheimer equations).
The  energy  convergence looks similar to that of GS calculations.  The fact
that *vres2*, the residual of the self-consistent first-order potential, has
reached [[tolvrs]] well within [[nstep]] (40) iterations indicates that the
2DTE calculation for this perturbation (xy strain) has converged. It would
pay to examine a few more cases for different perturbations (unless you have
looked through all the warnings in the log).

Another convergence item to examine in your .abo file is

     Seventeen components of 2nd-order total energy (hartree) are
     1,2,3: 0th-order hamiltonian combined with 1st-order wavefunctions
         kin0=   1.67341861E+01 eigvalue=  -4.11015159E-01  local=  -3.22857213E+00
     4,5,6,7: 1st-order hamiltonian combined with 1st and 0th-order wfs
     loc psp =  -9.55499839E+00  Hartree=   4.67777679E+00     xc=  -7.47227780E-01
         kin1=  -2.31525775E+01
     8,9,10: eventually, occupation + non-local contributions
        edocc=   0.00000000E+00     enl0=   6.39144174E-01   enl1=  -6.46481544E-03
     1-10 gives the relaxation energy (to be shifted if some occ is /=2.0)
       erelax=  -1.50497487E+01
     11,12,13 Non-relaxation  contributions : frozen-wavefunctions and Ewald
      fr.hart=  -1.37404595E-01   fr.kin=   1.27503317E+01 fr.loc=   4.84873306E-01
     14,15,16 Non-relaxation  contributions : frozen-wavefunctions and Ewald
      fr.nonl=   4.27603780E-01    fr.xc=  -3.16560587E-02  Ewald=   3.13179353E+00
     17 Non-relaxation  contributions : pseudopotential core energy
      pspcore=   0.00000000E+00
     Resulting in :
     2DEtotal=    0.1575792891E+01 Ha. Also 2DEtotal=    0.428795052510E+02 eV
        (2DErelax=   -1.5049748718E+01 Ha. 2DEnonrelax=    1.6625541610E+01 Ha)
        (  non-var. 2DEtotal :    1.5757929293E+00 Ha)

This detailed breakdown of the contributions to 2DTE is probably of limited
interest, but you should compare "2DEtotal" and "non-var. 2DEtotal" from the
last three lines. While the first-order wave function for the present
perturbation minimizes a variational expression for the second derivative
with respect to this perturbation as we just saw, the various 2DTE given as
elastic tensors, etc. in the output and in the DDB file are all computed using
non-variational expressions. Using the non-variational expressions, mixed
second derivatives with respect to the present perturbation and all other
perturbations of interest can be computed directly from the present 
first-order wave functions. The disadvantage is that the non-variational result
has errors which are linearly proportional to convergence errors in the GS and
first-order wave functions. Since errors in the variational 2DEtotal are
second-order in wave-function convergence errors, comparing this to the non-variational
result for the diagonal second derivative will give an idea of the
accuracy of the latter and perhaps indicate the need for tighter convergence
tolerances for both the GS and RF wave functions.
This is discussed in X. Gonze and C. Lee, Phys. Rev. B 55, 10355 (1997) [[cite:Gonze1997a]], Sec. II.
For an atomic-displacement perturbation, the corresponding breakdown of the 2DTE is headed
"Thirteen components."

Now let us take a look at the results we want, the various 2DTE's. They begin by

     ==> Compute Derivative Database <==

      2nd-order matrix (non-cartesian coordinates, masses not included,
       asr not included )
      cartesian coordinates for strain terms (1/ucvol factor
       for elastic tensor components not included)
         j1       j2             matrix element
      dir pert dir pert     real part     imaginary part
 
       1    1   1    1         5.7740048299         0.0000000000
       1    1   2    1        -2.8870024150         0.0000000000
       1    1   3    1         0.0000000000         0.0000000000
                .....

These are the "raw" 2DTE's, in reduced coordinates for atom-displacement and
electric-field perturbations, but cartesian coordinates for strain
perturbations. The same results with the same organization appear in the file
*telast_2_DS3_DDB* which will be used later as input for automated analysis and
converted to more useful notation and units by anaddb. A breakout of various
types of 2DTE's follows (all converted to Cartesian coordinates and in atomic units):

      Dynamical matrix, in cartesian coordinates,
       if specified in the inputs, asr has been imposed
         j1       j2             matrix element
      dir pert dir pert     real part    imaginary part

       1    1   1    1         0.1098837504         0.0000000000
       1    1   2    1         0.0000000000         0.0000000000
       1    1   3    1         0.0000000000         0.0000000000
                .....

This contains the interatomic force constant data that will be used later to
include atomic relaxation effects. "asr" refers to the acoustic sum rule,
which basically is a way of making sure that forces sum to zero when an atom is displaced.

      Effective charges, in cartesian coordinates,
      (from phonon response)
       if specified in the inputs, asr has been imposed
         j1       j2             matrix element
      dir pert dir pert     real part    imaginary part

       1    6   1    1         2.0670263917         0.0000000000
       2    6   1    1         0.0000000000         0.0000000000
       3    6   1    1         0.0000000000         0.0000000000
                .....

The Born effective charges will be used to find the atomic relaxation
contributions of the piezoelectric tensor.

      Rigid-atom elastic tensor , in cartesian coordinates,
         j1       j2             matrix element
      dir pert dir pert     real part    imaginary part

       1    7   1    7         0.0078004875         0.0000000000
       1    7   2    7         0.0019706468         0.0000000000
       1    7   3    7         0.0011850679         0.0000000000
                .....

The rigid-atom elastic tensor is the 2DTE with respect to a pair of strains.
We recall that "pert" = natom+3 and natom+4 for unaxial and shear strains, respectively.

      Internal strain coupling parameters, in cartesian coordinates,
       zero average net force deriv. has been imposed
         j1       j2             matrix element
      dir pert dir pert     real part    imaginary part

       1    1   1    7         0.1464077350         0.0000000000
       1    1   2    7        -0.1464077350         0.0000000000
       1    1   3    7         0.0000000000         0.0000000000
                .....

These 2DTE's with respect to one strain and one atomic displacement are needed
for atomic relaxation corrections to both the elastic tensor and piezoelectric
tensor. While this set of parameters is of limited direct interest, it should
be examined in cases when you think that high symmetry may eliminate the need
for these corrections. You are probably wrong, and any non-zero term indicates a correction.

      Rigid-atom proper piezoelectric tensor, in cartesian coordinates,
      (from strain response)
         j1       j2             matrix element
      dir pert dir pert     real part    imaginary part

       1    6   1    7         0.0000000000         0.0000000000
       1    6   2    7         0.0000000000         0.0000000000
       1    6   3    7        -0.0000000000         0.0000000000
       1    6   1    8         0.0000000000         0.0000000000
       1    6   2    8         0.0070195611         0.0000000000
       1    6   3    8         0.0000000000         0.0000000000

Finally, we have the piezoelectric tensor, the 2DTE with respect to one strain
and one uniform electric field component. (Yes, there are non-zero elements.)

## 3 ANADDB calculation of atom-relaxation effects

In this section, we will run the program anaddb, which analyzes DDB files
generated in prior RF calculations. You should copy *telast_3.abi* 
in your *Work_elast* directory. You should now go to the [[help:anaddb|anaddb help file]]
introduction. The bulk of the material in this help file is contained in the
description of the variables. You should read the descriptions of

  * [[anaddb:elaflag]],
  * [[anaddb:piezoflag]],
  * [[anaddb:instrflag]],
  * [[anaddb:chneut]].

For the theory underlying the incorporation of atom-relaxation corrections, it
is recommended you see  X. Wu, D. Vanderbilt, and D. R. Hamann [[cite:Wu2005]].

Anaddb can do lots of other things, such as calculate the frequency-dependent
dielectric tensor, interpolate the phonon spectrum to make nice phonon
dispersion plots, calculate Raman spectra, etc., but we are focusing on the
minimum needed for the elastic and piezoelectric constants at zero electric field.

We also mention that [[help:mrgddb|mrgddb]]
is another utility program that can be used to combine DDB files generated in
several different datasets or in different runs into a single DDB file that
can be analyzed by anaddb. One particular usage would be to combine the DDB
file produced by the GS run, which contains first-derivative information such
as stresses and forces with the RF DDB. It is anticipated that anaddb in a
future release will implement the finite-stress corrections to the elastic
tensor discussed in [notes by A. R. Oganov](/theory/documents/elasticity-oganov.pdf) .

Now would be a good time to edit *telast_3.abi* and observe that it is very
simple, consisting of nothing more than the four variables listed above set to
appropriate values.

{% dialog tests/tutorespfn/Input/telast_3.abi %}

A *telast_3.files* file is also needed for anaddb. Copy it from the tests/tutorespfn/Input directory.

{% dialog tests/tutorespfn/Input/telast_3.files %}

The first two lines specify the .abi and .abo files, the third line specifies the DDB file, and the
last lines are dummy names which would be used in connection with other
capabilities of anaddb. Now you should run the calculation:

    anaddb <telast_3.files >& telast_3.log

This calculation should only take a few seconds. You should edit the log file,
go to the end, and make sure the calculation terminated without error. Next,
examine *telast_3.abo*. After some header information, we come to tables giving
the "force-response" and "displacement-response" internal strain tensors.
These represent, respectively, the force on each atom and the displacement of
each atom in response to a unit strain of the specified type. These numbers
are of limited interest to us, but represent important intermediate quantities
in the treatment of atomic relaxation (see the X. Wu [[cite:Wu2005]] paper cited above).

Next, we come to the elastic tensor output:

     Elastic Tensor (clamped ion) (unit:10^2GP):

       2.2949823   0.5797842   0.3486590  -0.0000000  -0.0000000   0.0000001
       0.5797842   2.2949822   0.3486590  -0.0000000   0.0000000   0.0000001
       0.3486589   0.3486589   2.4696020  -0.0000000   0.0000000   0.0000001
       0.0000000   0.0000000   0.0000000   0.5821756   0.0000000   0.0000000
      -0.0000000   0.0000000   0.0000000   0.0000000   0.5821756   0.0000000
       0.0000000   0.0000000   0.0000000   0.0000000   0.0000000   0.8575990

     Elastic Tensor (relaxed ion) (unit:10^2GP):
      (at fixed electric field boundary condition)

       1.8986339   0.7520966   0.5652395  -0.0000000  -0.0000000   0.0000001
       0.7520966   1.8986339   0.5652395  -0.0000000   0.0000000   0.0000001
       0.5652395   0.5652395   2.0508555  -0.0000000   0.0000000   0.0000001
       0.0000000   0.0000000   0.0000000   0.4487908   0.0000000   0.0000000
       0.0000000  -0.0000000  -0.0000000   0.0000000   0.4487909  -0.0000000
       0.0000000  -0.0000000   0.0000000   0.0000000  -0.0000000   0.5732684


While not labeled, the rows and columns 1-6 here represent xx, yy, zz, yz, xz,
xy strains and stresses in the conventional Voigt notation.
The clamped-ion results were calculated in the telast_2 RF run, and are simply
converted to standard GPa units by anaddb (the terms "clamped ion," "clamped
atom," and "rigid atom" used in various places are interchangeable, similarly for "relaxed.")
The relaxed-ion result was calculated by anaddb by combining 2DTE's for
internal strain and interatomic force constants which are stored in the input
DDB file. Comparing the clamped and relaxed results, we see that all the
diagonal elastic constants have decreased in value.
This is plausible, since allowing the internal degrees of freedom to relax
should make a material less stiff. These tensors should be symmetric, and
certain tensor elements should be zero or identical by symmetry.
It's a good idea to check these properties against a standard text such as  J.
F. Nye, Physical Properties of Crystals (Oxford U. P., Oxford 1985) [[cite:Nye1985]].
Departures from expected symmetries (there are a few in the last decimal place
here) are due to either convergence errors or, if large, incorrectly specified
geometry (however, see the final comments on symmetry  below).

Later, we will obtain the (3,3) component of the Elastic Tensor (clamped ion)
from finite differences. Its value is 246.96020 GPa (note the unit:10^2GP indication above).

Next in *telast_3.abo we find the piezoelectric tensor results:

     Proper piezoelectric constants (clamped ion) (unit:c/m^2)

          0.00000000      0.00000000      0.36031593
          0.00000000      0.00000000      0.36031593
         -0.00000000     -0.00000000     -0.69614902
          0.00000000      0.40162256      0.00000000
          0.40162251      0.00000000      0.00000000
          0.00000000      0.00000000     -0.00000001

     ddb_piezo : WARNING -
      Acoustic sum rule violation met : the eigenvalues of accoustic mode
      are too large at Gamma point
      Increase cutoff energy or k-points sampling.
      The three eigenvalues are:   -1.016026E-04   -1.389885E-05   -2.054240E-05

     Proper piezoelectric constants (relaxed ion) (unit:c/m^2)

          0.00000000     -0.00000000     -0.12745531
         -0.00000000      0.00000000     -0.12745533
          0.00000000      0.00000000      0.24692958
          0.00000000     -0.15011587      0.00000000
         -0.15011568      0.00000000      0.00000000
         -0.00000000      0.00000000     -0.00000002

The 3 columns here represent x, y, and z electric polarization, and the 6 rows
the Voigt strains. The clamped-ion result was calculated in the telast_2 RF
run, and is simply scaled to conventional units by anaddb. The ion relaxation
contributions are based on 2DTE's for internal strain, interatomic force
constants, and Born effective charges, and typically constitute much larger
corrections to the piezoelectric tensor than to the elastic tensor. Once
again, symmetries should be checked (The slight discrepancies seen here can
be removed by setting tolvrs3 = 1.0d-18 in *telast_2.abi*). One should be aware
that the piezoelectric tensor is identically zero in any material which has a center of symmetry.

There is also a WARNING message. For the purpose of a tutorial, the [[ecut]]
has not been selected large enough. Changing [[ecut]] from 6 Ha to 15 Ha will eliminate this problem.

Since we are dealing with a hypothetical material, there is no experimental
data with which to compare our results. In the next section, we will calculate
a few of these numbers by a finite-difference method to gain confidence in the RF approach.

## 4 Finite-difference calculation of elastic and piezoelectric constants

You should copy *telast_4.abi* into your *Work_elast* directory.

{% dialog tests/tutorespfn/Input/telast_4.abi %}

Editing *telast_4.abi*, you will see that it has four datasets, the first two
with the c-axis contracted 0.01% and the second two with it expanded 0.01%,
which we specified by changing the third row of [[rprim]]. The common data is
essentially the same as telast_2.abi, and the relaxed [[acell]] values and
[[xred]] from telast_1.abo have already been included. Datasets 1 and 3 do the
self-consistent convergence of the GS wave functions for the strained lattices
and compute the stress. Datasets 2 and 4 introduce a new variable.

  * [[berryopt]]

Electric polarization in solids is a subtle topic which has been
rigorously resolved thirty years ago. It is now understood to be a bulk property, and to be
quantitatively described by a Berry phase formulation introduced by R. D.
King-Smith and D. Vanderbilt, Phys. Ref. B 47, 1651(1993) [[cite:Kingsmith1993]]. It can be
calculated in a GS calculation by integrating the gradient with respect to
**k** of the GS wave functions over the Brillouin zone. In GS calculations,
the gradients are approximated by finite-difference expressions constructed
from neighboring points in the **k** mesh. These are closely related to the
ddk wave functions used in RF calculations in section 2 and introduced in
[[lesson:rf1#5|tutorial DFPT1, section 5]]. We will use [[berryopt]] = -1,
which utilizes an improved coding of the calculation, and must specify
[[rfdir]] = 1 1 1 so that the cartesian components of the polarization are computed.

Now, run the *telast_4.abi* calculation, which should only take a minute or two, and
edit *telast_4.abo*. To calculate the elastic constants, we need to find the
stresses  sigma(1 1) and sigma(3 3). We see that each of the four datasets
have stress results, but that there are slight differences between those from,
for example dataset 1 and dataset 2, which should be identical. Despite our
tight limit, this is still a convergence issue. Look at the following convergence results,

    Dataset 1:
     At SCF step   13       vres2   =  6.68E-21 < tolvrs=  1.00E-18 =>converged.

    Dataset 2:
     At SCF step    1       vres2   =  5.31E-22 < tolvrs=  1.00E-18 =>converged.

Since dataset 2 has better convergence, we will use it.  Coherently, 
we will use also the dataset 4 results, choosing those in GPa units,

    -Cartesian components of stress tensor (GPa)         [Pressure=  9.1716E-03 GPa]
    - sigma(1 1)= -1.39848817E-03  sigma(3 2)=  0.00000000E+00
    - sigma(2 2)= -1.39848817E-03  sigma(3 1)=  0.00000000E+00
    - sigma(3 3)= -2.47179326E-02  sigma(2 1)=  0.00000000E+00

    -Cartesian components of stress tensor (GPa)         [Pressure= -1.1941E-02 GPa]
    - sigma(1 1)=  5.57418718E-03  sigma(3 2)=  0.00000000E+00
    - sigma(2 2)=  5.57418718E-03  sigma(3 1)=  0.00000000E+00
    - sigma(3 3)=  2.46737283E-02  sigma(2 1)=  0.00000000E+00

Let us now compute the numerical derivative of sigma(3 3) and compare to our
RF result. Recalling that our dimensionless strains were ±0.0001, we find
246.9583 GPa. This compares very well with the value 246.9602 GPa, the 3,3
element of the Rigid-ion elastic tensor we found from our anaddb calculation
in section 3. (Recall that our strains and stresses were both 3,3 or z,z or Voigt
3.) Similarly, the numerical derivative of  sigma(3 1) is 34.8634 GPa
compared to 34.8658 GPa, the 3,1 elastic-tensor element computed above.

The good agreement we found from this simple numerical differentiation
required that we had accurately relaxed the lattice so that the stress of the
unstrained structure was very small. Similar numerical-derivative comparisons
for systems with finite stress are more complicated, as discussed in
[notes by A. R. Oganov](/theory/documents/elasticity-oganov.pdf).
Numerical-derivative comparisons for the relaxed-ion results are extremely challenging
since they require relaxing atomic forces to exceedingly small limits.

Now let us examine the electric polarizations found in datasets 2 and 4,
focusing on the C/m^2 results,

           Polarization    -3.695942387E-15 C/m^2
           Polarization    -8.071946282E-16 C/m^2
           Polarization    -3.244653656E-01 C/m^2

           Polarization     5.396151684E-16 C/m^2
           Polarization     9.218443808E-18 C/m^2
           Polarization    -3.246052331E-01 C/m^2

While not labeled as such, these are the Cartesian x, y, and z components,
respectively, and the x and y components are zero within numerical accuracy as
they must be from symmetry. Numerical differentiation of the z component
yields -0.699337 C/m$^2$. This is to be compared with the z,3 element of our
rigid-ion piezoelectric tensor from section 3, -0.696149 C/m$^2$, and the two
results do not compare as well as we might wish.

What is wrong? There are two possibilities. The first is that the RF
calculation produces the proper piezoelectric tensor, while numerical
differentiation of the polarization produces the improper piezoelectric
tensor. This is a subtle point, for which you are referred to  D. Vanderbilt,
J. Phys. Chem. Solids 61, 147 (2000) [[cite:Vanderbilt2000]]. The improper-to-proper transformation
only effects certain tensor elements, however, and for our particular
combination of crystal symmetry and choice of strain there is no correction.
The second possibility is the subject of the next section.

## 5 Alternative DFPT calculation of some piezoelectric constants

Our GS calculation of the polarization in section 4 used, in effect, a finite-
difference approximation to ddk wave functions, while our RF calculations in
section 2 used analytic results based on the RF approach. Since the **k** grid
determined by [[ngkpt]] = 4 4 4 and [[nshiftk]] = 1 is rather coarse, this is
a probable source of discrepancy. Since this issue was noted previously in
connection with the calculation of Born effective charges by  Na Sai, K. M.
Rabe, and D. Vanderbilt, Phys. Rev. B 66, 104108 (2002) [[cite:Sai2002]], Abinit has
incorporated the ability to use finite-difference ddk wave functions from GS
calculations in RF calculations of electric-field-related 2DTE's. 

Copy
*telast_5.abi* into *Work_elast*, and edit *telast_5.abi*.

{% dialog tests/tutorespfn/Input/telast_5.abi %}

You should compare this with our previous RF data, *telast_2.abi*, and note that
dataset1 and the Common data (after entering relaxed structural results) are
essentially identical. Dataset 2 has been replaced by a non-self-consistent GS
calculation with [[berryopt]] = -2 specified to perform the finite-difference
ddk wave function calculation. (The finite-difference first-order wave
functions are implicit but not actually calculated in the GS polarization
calculation.) We have restricted [[rfdir]] to 0 0 1 since we are only
interested in the 3,3 piezoelectric constant. Now compare dataset 3 with that
in *telast_2.abi*. Can you figure out what we have dropped and why? Run the
*telast_5.abi* calculation, which will only take about a minute with our simplifications.

Now edit *telast_5.abo*, looking for the piezoelectric tensor,

      Rigid-atom proper piezoelectric tensor, in cartesian coordinates,
      (from strain response)
         j1       j2             matrix element
      dir pert dir pert     real part    imaginary part

       3    6   3    7        -0.0122230317         0.0000000000

{% dialog tests/tutorespfn/Refs/telast_5.abo %}

We immediately see a problem -- this output, like most of the .out file, is in
atomic units, while we computed our numerical derivative in conventional C/m$^2$
units. While you might think to simply run anaddb to do the conversion as
before, its present version is not happy with such an incomplete DDB file as
telast_5 has generated and will not produce the desired result. While it
should be left as an exercise to the student to dig the conversion factor out
of the literature, or better yet out of the source code, we will cheat and
tell you that 1 a.u.=57.2147606 C/m$^2$. Thus the new RF result for the 3,3 rigid-
ion piezoelectric constant is -0.699338  C/m$^2$ compared to the result found in
section 4 by a completely-GS finite difference calculation, -0.699337 C/m$^2$.
The agreement is now excellent!

The fully RF calculation in section 2 in fact will converge much more rapidly
with **k** sample than the partial-finite-difference method introduced here.
Is it worthwhile to have learned how to do this? We believe that is always
pays to have alternative ways to test results, and besides, this didn't take
much time. (Have you found the conversion factor on your own yet?)

A final word about AlP calculations: in this study, we have not used converged parameters for the sake of speed.
It is always the duty of the user to check the precision of his numerical calculations with respect to the
parameters of the calculation.


## 6 Response-function calculation of the elastic constants of Al metal

For metals, the existence of partially occupied bands is a complicating
feature for RF as well as GS calculations.
Now would be a good time to review [tutorial 4](/tutorial/base4) which dealt in detail with the interplay between
**k**-sample convergence and Fermi-surface broadening, especially [[lesson:base4#3|section 3 of tutorial 4]].
You should copy *telast_6.abi* into *Work_elast*, and begin your run
while you read on, since it involves a convergence study with multiple datasets and may take about two minutes.

{% dialog tests/tutorespfn/Input/telast_6.abi %}

While the run is in progress, edit *telast_6.abi*. As in *tbase4_3.abi*, we will set
[[udtset]] to specify a double loop.  In the present case, however, the outer
loop will be over 3 successively larger meshes of **k** points, while the
inner loop will be successively

  1. GS self-consistent runs with optimization of acell.
  2. GS density-generating run for the next step.
  3. Non-self-consistent GS run to converge unoccupied or slightly-occupied bands.
  4. RF run for symmetry-inequivalent elastic constants.

In Section 1, we did a separate GS structural optimization run and
transferred the results by hand to RF run section 2.  Because we are doing a
convergence test here, we have combined these steps, and use [[getcell]] to
transfer the optimized coordinates from the first dataset of the inner loop
forward to the rest.  If we were doing a more complicated structure with
internal coordinates that were also optimized, we would need to use both this
and [[getxred]] to transfer these, as in telast_1.abi.

The specific data for inner-loop dataset 1 is very similar to that for *telast_1.abi*.
Inner-loop dataset 2 is a bit of a hack.  We need the density
for inner-loop dataset 3, and while we could set [[prtden]] = 1 in dataset 1,
this would produce a separate density file for every step in the structural
optimization, and it isn't clear how to automatically pick out the last one.
So, dataset 2 picks up the wave functions from dataset 1 (only one file of
these is produced, for the optimized structure), does one more iteration with
fixed geometry, and writes a density file.

Inner-loop dataset 3 is a non-self-consistent run whose purpose is to ensure
that all the wave functions specified by [[nband]] are well converged. For
metals, we have to specify enough bands to make sure that the Fermi surface is
properly calculated.  Bands above the  Fermi level which have small occupancy
or near-zero occupancy if their energies exceed the Fermi energy by more than
a few times [[tsmear]], will have very little effect on the self-consistent
potential, so the [[tolvrs]] test in dataset 1 doesn't ensure their
convergence.  Using [[tolwfr]] in inner-loop dataset 3 does.  Partially-
occupied or unoccupied bands up to [[nband]]   play a different role in
constructing the first-order wave functions than do the many unoccupied bands
beyond [[nband]] which aren't explicitly treated in Abinit, as discussed in
S. de Gironcoli, Phys. Rev. B 51, 6773 (1995) [[cite:DeGironcoli1995]].  By setting [[nband]] exactly
equal to the number of occupied bands for RF calculations for semiconductors
and insulators, we avoid having to deal with the issue of converging
unoccupied bands.  Could we avoid the extra steps by simply using [[tolwfr]]
instead of [[tolvrs]] in dataset 1?  Perhaps, but experience has shown that
this does not necessarily lead to as well-converged a potential, and it is not
recommended.  These same considerations apply to phonon calculations for
metals, or in particular to [[qpt]]= 0 0 0 phonon calculations for the
interatomic force constants needed to find atom-relaxation contributions to
the elastic constants for non-trivial structures as in section 2 and section 3.

The data specific to the elastic-tensor RF calculation in inner-loop dataset 4
should by now be familiar.  We take advantage of the fact that for cubic
symmetry the only symmetry-inequivalent elastic constants are C$_{11}$, C$_{12}$, and
C$_{44}$.  Abinit, unfortunately, does not do this analysis automatically, so we
specify [[rfdir]] = 1 0 0 to avoid duplicate calculations.  (Note that if atom
relaxation is to be taken into account  for a more complex structure, the full
set of directions must be used.)

When the telast_6 calculations finish, first look at *telast_6.log* as usual to
make sure they have run to completion without error. Next, it would be a good
idea to look at the band occupancies occ?? (where ?? is a dual-loop dataset
index) reported at the end (following  `==END DATASET(S)==`).  The highest band,
the fourth in this case, should have zero or very small occupation, or you
need to increase [[nband]] or decrease [[tsmear]] .  Now, use your newly
perfected knowledge of the Abinit perturbation indexing conventions to scan
through *telast_6.abo* and find C$_{11}$ , C$_{12}$ , and C$_{44}$ for each of the three
**k** -sample choices, which will be  under the " Rigid-atom elastic tensor"
heading.  Also find the lattice constants for each case, whose convergence you
studied in [tutorial 4](/tutorial/base4).
You should be able to cut-and-paste these into a table like the following,

                C_11        C_12        C_44        acell
    ngkpt=3*6   0.003844    0.002294    0.001377    7.585323
    ngkpt=3*8   0.004409    0.002088    0.001355    7.583261 
    ngkpt=3*10  0.004392    0.002092    0.001354    7.583710

We can immediately see that the lattice constant converges considerably more
rapidly with **k** sample than the elastic constants.   For [[ngkpt]] =3*6,
acell is converged to 0.02%, while the C's have up to 15% error.  For [[ngkpt]]
=3*8, the C's are converged to better than 0.5%, even for the largest,
C$_{11}$, which should be acceptable.

As in [tutorial 4](/tutorial/base4), the [[ngkpt]] convergence is controlled by [[tsmear]].  The
smaller the broadening, the denser the **k** sample that is needed to get a
smooth variation of occupancy, and presumably stress, with strain.   While we
will not explore [[tsmear]] convergence in this tutorial, you may wish to do so
on your own.  

Also, even more than for the lattice parameter, the type of smearing
function plays an important role. The preferred smearing, [[occopt]]=7,
apparently performs worse than the the standard Fermi-Dirac broadening
[[occopt]]=3 that we have used above. Indeed, with [[occopt]]=7
and the same [[tsmear]]=0.02, one obtains:


                C_11        C_12        C_44        acell
    ngkpt=3*6   0.003113    0.002598    0.001575    7.543949
    ngkpt=3*8   0.004447    0.001948    0.001441    7.542962
    ngkpt=3*10  0.004781    0.001816    0.001233    7.544365
    ngkpt=3*12  0.004257    0.002108    0.001244    7.545547
    ngkpt=3*14  0.004056    0.002210    0.001300    7.545783
    ngkpt=3*16  0.004230    0.002120    0.001316    7.545453
    ngkpt=3*20  0.004271    0.002100    0.001311    7.545363


The reasons that this supposedly superior smoothing function performs poorly in this context
has not been investigated. Of course, [[tsmear]] has a different meaning
in both cases, and perhaps the value of 0.02Ha (=6315 Kelvin) for [[occopt]]=3 might yield a large error
anyhow (see the predicted [[acell]]).

The main thing to be learned is that checking
convergence with respect to all relevant parameters is **always** the user's
responsibility.   Simple systems that include the main physical features of a
complex system of interest will usually suffice for this testing.  Don't get
caught publishing a result that another researcher refutes on convergence
grounds, and do not blame such a mistake on Abinit!

Now we make a comparison with experiment.  Using the above computed values, converting the C's to standard
units (Ha/Bohr$^3$ = 2.94210119E+04 GPa) and using zero-temperature extrapolated
experimental results from P. M. Sutton, Phys. Rev. 91, 816 (1953) [[cite:Sutton1953]], we find

                                      C_11(GPa)  C_12(GPa)  C_44(GPa)
    Calculated (T=6315K)                129.2      61.5       39.8 
    Calculated (T=0K, Gaussian 0.02Ha)  125.6      61.8       38.6
    Experiment (T=0K)                   123.0      70.8       30.9


Is this good agreement?  The numerical values are not yet definitive,
as one should do more convergence studies anyhow. This being said, there isn't much literature on DFT calculations of
full sets of elastic constants.  Many calculations of the bulk modulus
(K=(C$_{11}$+2C$_{12}$ )/3 in the cubic case) typically are within 10% of experiment
for the LDA.  Running telast_6 with ixc=11, the Perdew-Burke-Enzerhof GGA,
increases the calculated C's by 1-2%, and wouldn't be expected to make a large
difference for a nearly-free-electron metal.

### Comment on symmetry

It is important to bear in mind that the way a tensor like the elastic tensor
appears is a function of the frame used. Thus for the aluminum fcc case
considered above, the nonzero elements are _C$_{11}$_, _C$_{12}$_, and _C$_{44}$_,
_provided that the crystal axes are aligned with the laboratory frame._
For an arbitrary alignment of the crystal axes, many more _C$_{ij}$_ elements will be
non-zero, and this can be confusing.

It is easy to see why this happens if you
imagine actually measuring the elastic tensor elements. If you start with the
conventional cubic cell, and apply pressure to one face, you can measure _C$_{11}$_.
 But if you turn the cell to some random angle, you'll measure a response
that is a mixture of _C$_{11}$_ and _C$_{12}$_.

Within ABINIT, if the aluminum fcc
cell is described using [[angdeg]] and [[acell]], then an axis of the
primitive cell will be aligned along the laboratory _z_ axis but this will not
lead to a (conventional) cell alignment with the laboratory frame. The
resulting elastic tensor will be correct but will appear to be more
complicated than in the illustration above. It can be rotated back to a simple
frame by hand (bearing in mind that all four indices of the fourth-rank
elastic tensor have to be rotated!) but it's easier to start with a more
conventional alignment of the unit cell.

If you use a standard text like
Bradley and Cracknell, The Mathematical Theory of Symmetry in Solids, Oxford [[cite:Bradley1972]]
you can find the standard primitive cell descriptions for the Bravais lattice
types and these are aligned as much as possible with a standard laboratory frame.
---
authors: XG
plotly: true
---

# Parallelism in the DFPT formalism

## Atomic displacements, homogeneous electric fields, strain.

This tutorial aims at showing how to use the parallelism for all the properties
that are computed on the basis of the density-functional perturbation theory
(DFPT) part of ABINIT, i.e.:

  * responses to atomic displacements (to compute phonon frequencies, band structures, ... )
  * responses to homogeneous electric fields (dielectric constant, Born effective charges, Infra-red characteristics )
  * responses to strain (elastic constants, piezoelectric constants ...)

Such computations are realized when one of the input variables [[rfphon]],
[[rfelfd]] or [[rfstrs]] are non-zero, which activates [[optdriver]]=1.
You are supposed to be well-familiarized with such calculations before starting
the present tutorial. See the input variables described in [[varset:dfpt]] and
the tutorial [Response-Function 1](/tutorial/rf1) and subsequent tutorials.

You will learn about the basic implementation of parallelism for DFPT
calculations, then will execute a very simple, quick calculation for one
dynamical matrix for FCC aluminum, whose scaling is limited to a few dozen
computing cores, then you will execute a calculation whose scaling is much
better, but that would take a few hours in sequential, using a provided input file.

For the last section of that part, you would be better off having access
to more than 100 computing cores, although you might also change the input
parameters to adjust to the machine you have at hand. For the other parts of
the tutorial, a 16-computing-core machine is recommended, in order to perform
the scalability measurements.

You are supposed to know already some basics of parallelism in ABINIT,
explained in the tutorial [A first introduction to ABINIT in parallel](/tutorial/basepar).

This tutorial should take less than two hours to be done if a powerful parallel
computer is available.

[TUTORIAL_README]

## 1 The structure of the parallelism for the DFPT part of ABINIT

**1.1.** Let us recall first the basic structure of typical DFPT calculations summarized in box 1.

![Schema 1](paral_dfpt_assets/DFPT_Structure.png)

The step 1 is done in a preliminary ground-state calculation (either by an
independent run, or by computing them using an earlier dataset before the DFPT calculation).
The parallelisation of this step is examined in a separate tutorial.

The step 2 and step 3 are the time-consuming DFPT steps, to which the present
tutorial is dedicated, and for which the implementation of the parallelism
will be explained. They generate different files, and in particular, one (or several) DDB file(s).
As explained in related tutorials (see e.g.
[Response-Function 1](/tutorial/rf1)), several perturbations are usually treated in
one dataset (hence the "Do for each perturbation"-loop in step 2 of this
Schema). For example, in one dataset, although one considers only one phonon
wavevector, all the primitive atomic displacements for this wavevector (as
determined by the symmetries) can be treated in turn.

The step 4 refers to MRGDDB and ANADDB post-processing of the DDB generated by
steps 2 and 3. These are not time-consuming sections. No parallelism is needed.

**1.2.** The equations to be considered for the computation of the first-order
wavefunctions and potentials (step 2 of box 1), which is the really time-consuming part
of any DFPT calculation, are presented in box 2.

![Schema 2](paral_dfpt_assets/DFPT_Equations.png)

The parallelism currently implemented for the DFPT part of ABINIT is based on
the parallel distribution of the most important arrays: the set of first-order
and ground-state wavefunctions.
While each such wavefunction is stored completely in the memory linked to one
computing core (there is no splitting of different parts of one wavefunction
between different computing cores - neither in real space nor in reciprocal
space), different such wavefunctions might be distributed over different
computing cores. This easily achieves a combined k point, spin and band index
parallelism, as explained in box 3.

![Schema 3](paral_dfpt_assets/1WFs.png)

**1.3.** The parallelism over k points, band index and spin for the set of
wavefunctions is implemented in all relevant steps, except for the reading
(initialisation) of the ground-state wavefunctions (from an external file).
Thus, the most CPU time-consuming parts of the DFPT computations are parallelized.
The following are not parallelized:

  * input of the set of ground-state wavefunctions
  * computing the first-order change of potential from the first-order density,
    and similar operations that do not depend on the bands or k points.

In the case of small systems, the maximum achievable speed-up will be limited
by the input of the set of ground-state wavefunctions. For medium to large
systems, the maximum achievable speed-up will be determined by the operations
that do not depend on the k point and band indices.
Finally, the distribution of the set of ground-state wavefunctions to the
computing cores that need them is partly parallelized, as no parallelism over
bands can be exploited.

*Before continuing you might work in a different subdirectory as for the other
tutorials. Why not work_paral_dfpt?*

All the input files can be found in the *\$ABI_TESTS/tutoparal/Input* directory.
You might have to adapt them to the path of the directory in which you have decided to perform your runs.
You can compare your results with reference output files located in *\$ABI_TESTS/tutoparal/Refs*.

In the following, when "(mpirun ...) abinit" appears, you have to use a
specific command line according to the operating system and architecture of
the computer you are using. This can be for instance: mpirun -np 16 abinit

## 2 Computation of one dynamical matrix (q =0.25 -0.125 0.125) for FCC aluminum

We start by treating the case of a small systems, namely FCC aluminum, for
which there is only one atom per unit cell. Of course, many k points are needed, since this is a metal.

**2.1.** The first step is the pre-computation of the ground state
wavefunctions. This is driven by the files *tdfpt_01.abi*.
You should edit it and examine it.

{% dialog tests/tutoparal/Input/tdfpt_01.abi %}

One relies on a k-point grid of 8x8x8 x 4 shifts (=2048 k points), and 5 bands.
The k-point grid sampling is well converged, actually.
For this ground-state calculation, symmetries can be used to reduce
drastically the number of k points: there are 60 k points in the irreducible
Brillouin zone (this cannot be deduced from the examination of the input file, though).
In order to treat properly the phonon calculation, the number of bands is larger than the
default value, that would have given [[nband]]=3. Indeed, several of the unoccupied bands 
plays a role in the response calculations in the case of etallic occupations.
For example, the acoustic sum rule might be largely violated when too few unoccopied 
bands are treated. 

This calculation is very fast, actually.
You can launch it:

    mpirun -n 4  abinit tdfpt_01.abi > log &

A reference output file is available in *\$ABI_TESTS/tutoparal/Refs*, under
the name *tdfpt_01.abo*. It was obtained using 4 computing cores, and took a few seconds.

**2.2.** The second step is the DFPT calculation, see the file *tdfpt_02.abi*

{% dialog tests/tutoparal/Input/tdfpt_02.abi %}

There are three perturbations (three atomic displacements). For the two first
perturbations, no symmetry can be used, while for the third, two symmetries
can be used to reduce the number of k points to 1024. Hence, for the perfectly
scalable sections of the code, the maximum speed up is 5120 (=1024 k points *
5 bands), if you have access to 5120 computing cores. However, the sequential
parts of the code dominate at a much, much lower value. Indeed, the sequential
parts is actually a few percents of the code on one processor, depending on
the machine you run. The speed-up might saturate beyond 8...16 (depending on
the machine). Note that the number of processors that you use for this second step
is independent of the number of processors that you used for the first step.
The only relevant information from the first step is the *_WFK file.

First copy the output of the ground-state calculation so that it can be used
as the input of the DFPT calculation:

    cp tdfpt_01.o_WFK tdfpt_02.i_WFK

(A _WFQ file is not needed, as all GS wavefunctions at k+q are present in the GW wavefuction at k).
Then, you can launch the calculation:

    mpirun -n 4 abinit tdfpt_02.abi > tdfpt_02.log &

A reference output file is given in *\$ABI_TESTS/tutoparal/Refs*, under the name
*tdfpt_02.abo*. Edit it, and examine some information.
The calculation has been made with four computing cores:

```
-   mpi_nproc: 4, omp_nthreads: -1 (-1 if OMP is not activated)
```

The wall clock time is less than 50 seconds :

```
-
- Proc.   0 individual time (sec): cpu=         28.8  wall=         28.9

================================================================================

 Calculation completed.
.Delivered   0 WARNINGs and   0 COMMENTs to log file.
+Overall time at end (sec) : cpu=        115.4  wall=        115.8
```

The major result is the phonon frequencies:

      Phonon wavevector (reduced coordinates) :  0.25000 -0.12500  0.12500
     Phonon energies in Hartree :
       6.521506E-04  7.483301E-04  1.099648E-03
     Phonon frequencies in cm-1    :
    -  1.431305E+02  1.642395E+02  2.413447E+02

**2.3.** Because this test case is quite fast, you should play a bit with it.
In particular, run it several times, with an increasing number of computing
cores (let us say, up to 40 computing cores, at which stage you should have
obtained a saturation of the speed-up).
You should be able to obtain a decent speedup up to 8 processors, then the gain becomes more and more marginal.
Note however that the result is independent (to an exquisite accuracy) of the number of computing cores that is used

Let us explain the timing section. It is present a bit before the end of the output file:

    -
    - For major independent code sections, cpu and wall times (sec),
    -  as well as % of the time and number of calls for node 0-
    -<BEGIN_TIMER mpi_nprocs = 4, omp_nthreads = 1, mpi_rank = 0>
    - cpu_time =           28.8, wall_time =           28.9
    -
    - routine                        cpu     %       wall     %      number of calls  Gflops    Speedup Efficacity
    -                                                                  (-1=no count)
    - fourwf%(pot)                  14.392  12.5     14.443  12.5         170048      -1.00        1.00       1.00
    - nonlop(apply)                  2.787   2.4      2.802   2.4         125248      -1.00        0.99       0.99
    - nonlop(forces)                 2.609   2.3      2.620   2.3          64000      -1.00        1.00       1.00
    - fourwf%(G->r)                  2.044   1.8      2.052   1.8          47124      -1.00        1.00       1.00
    - dfpt_vtorho-kpt loop           1.174   1.0      1.177   1.0             21      -1.00        1.00       1.00
    - getgh1c_setup                  1.111   1.0      1.114   1.0           8960      -1.00        1.00       1.00
    - mkffnl                         1.087   0.9      1.092   0.9          20992      -1.00        1.00       1.00
    - projbd                         1.023   0.9      1.036   0.9         286336      -1.00        0.99       0.99
    - dfpt_vtowfk(contrib)           0.806   0.7      0.805   0.7             -1      -1.00        1.00       1.00
    - others (120)                  -2.787  -2.4     -2.825  -2.4             -1      -1.00        0.99       0.99
    -<END_TIMER>
    -
    - subtotal                      24.246  21.0     24.317  21.0                                  1.00       1.00

    - For major independent code sections, cpu and wall times (sec),
    - as well as % of the total time and number of calls
    
    -<BEGIN_TIMER mpi_nprocs = 4, omp_nthreads = 1, mpi_rank = world>
    - cpu_time =         115.3, wall_time =         115.6
    -
    - routine                         cpu     %       wall     %      number of calls Gflops    Speedup Efficacity
    -                                                                  (-1=no count)
    - fourwf%(pot)                  51.717  44.8     51.909  44.9         679543      -1.00        1.00       1.00
    - dfpt_vtorho:MPI               12.267  10.6     12.292  10.6             84      -1.00        1.00       1.00
    - nonlop(apply)                 10.092   8.8     10.149   8.8         500343      -1.00        0.99       0.99
    - nonlop(forces)                 9.520   8.3      9.562   8.3         256000      -1.00        1.00       1.00
    - fourwf%(G->r)                  7.367   6.4      7.398   6.4         187320      -1.00        1.00       1.00
    - dfpt_vtorho-kpt loop           4.182   3.6      4.193   3.6             84      -1.00        1.00       1.00
    - getgh1c_setup                  3.955   3.4      3.967   3.4          35840      -1.00        1.00       1.00
    - mkffnl                         3.911   3.4      3.929   3.4          83968      -1.00        1.00       1.00
    - projbd                         3.735   3.2      3.780   3.3        1144046      -1.00        0.99       0.99
    - dfpt_vtowfk(contrib)           2.754   2.4      2.748   2.4             -4      -1.00        1.00       1.00
    - getghc-other                   1.861   1.6      1.790   1.5             -4      -1.00        1.04       1.04
    - pspini                         0.861   0.7      0.865   0.7              4      -1.00        0.99       0.99
    - newkpt(excl. rwwf   )          0.754   0.7      0.757   0.7             -4      -1.00        1.00       1.00
    - others (116)                 -14.132 -12.3    -14.220 -12.3             -1      -1.00        0.99       0.99
    -<END_TIMER>

    - subtotal                      98.845  85.7     99.120  85.7                                  1.00       1.00

It is made of two groups of data. The first one corresponds to the analysis of
the timing for the computing core (node) number 0. The second one is the sum
over all computing cores of the data of the first group. Note that there is a
factor of four between these two groups, reflecting that the load balance is good.

Let us examine the second group of data in more detail. It corresponds to a
decomposition of the most time-consuming parts of the code. Note that the
subtotal is 85.7 percent, thus the statistics is not very accurate, as it should be close to 100%.
Actually, as of ABINIT v9, there is must be a bug in the timing decomposition, since, 
e.g. there is a negative time announced for the "others" subroutines.

Anyhow, several of the most time-consuming parts are directly related to application of the Hamiltonian to wavefunctions,
namely, fourwf%(pot) (application of the local potential, which implies two Fourier transforms), nonlop(apply) 
(application of the non-local part of the Hamiltonian), nonlop(forces) (computation of the non-local part of the
interatomic forces), fourwf%(G->r) (fourier transform needed to build the first-order density). Also, quite noticeable
is dfpt_vtorho:MPI , synchronisation of the MPI parallelism. 

A study of the speed-up brought by the k-point parallelism for this simple test case
gives the following behaviour, between 1 and 40 cores:

<div id="plotly_plot" style="width:90%;height:450px;"></div>
<script>
$(function() {
    Plotly.newPlot(document.getElementById('plotly_plot'), 
        [{ x: [1, 2, 4, 6, 8, 12, 16, 24, 32, 40], y: [1, 2, 4, 6, 8, 12, 16, 24, 32, 40], name: 'Ideal'},
         { x: [1, 2, 4, 6, 8, 12, 16, 24, 32, 40], y: [1, 1.92, 3.75, 5.50, 7.17, 9.34, 11.1, 14.4, 17.8, 19.7], name: 'Observed' }], 
        { title: "Parallel speed-up", 
          xaxis: {title:'Number of cores'} }
    );
});
</script>

At 8 processors, one gets a speed-up of 7.17, which is quite decent (about 90% efficiency),
but 16 processors, the speed-up is only 11.1 (about 70% efficiency), and the efficiency is below 50% at 40 processors,
for a speed-up of 19.7 ..

## 3 Computation of one perturbation for a slab of 29 atoms of barium titanate

**3.1.** This test, with 29 atoms, is slower, but highlights other aspects 
of the DFPT parallelism than the Al FCC case. 
It consists in the computation of one perturbation at [[qpt]] 0.0 0.25
0.0 for a 29 atom slab of barium titanate, artificially terminated by a double
TiO<sub>2</sub> layer on each face, with a reasonable k-point sampling of the Brillouin zone.

The symmetry of the system and perturbation will allow to decrease this
sampling to one quarter of the Brillouin zone. E.g. with the k-point sampling
[[ngkpt]] 4 4 1, there will be actually 4 k-points in the irreducible Brillouin
zone for the Ground state calculations. For the DFPT case, only one (binary) symmetry
will survive, so that, after the calculation of the frozen-wavefunction part
(for which no symmetry is used), the self-consistent part will be done with 8
k points in the corresponding irreducible Brillouin zone. Beyond 8 cores, the parallelisation
will be done also on the bands. This will prove to be much more dependent
on the computer architecture than the (low-communication) parallelism over k-points.

With the sampling 8 8 1, there will be 32 k points in the irreducible Brillouin zone for the DFPT
case. This will allow potentially to use efficiently a larger number of processors, provided the
computer architecture and network is good enough.

There are 116 occupied bands. For the ground state calculation, 4 additional conduction
bands will be explicitly treated, which will allow better SCF stability thanks to [[iprcel]] 45.
Note that the value of [[ecut]] that is used in the
present tutorial is too low to obtain physical results (it should be around 40 Hartree).
Also, only one atomic displacement is considered, so that the phonon frequencies
delivered at the end of the run are meaningless.

As in the previous case, a preparatory ground-state calculation is needed.
We use the input variable [[autoparal]]=1 . It does not delivers the best repartition of
processors among [[npkpt]], [[npband]] and [[npfft]], but achieves a decent repartition, usually within a factor of two.
With 24 processors, it selects [[npkpt]]=4 (optimal), [[npband]]=3 and [[npfft]]=2, while [[npband]]=6 and [[npband]]=1 would do better.
For information, the speeup going from 24 cores to 64 cores is 1.76, not quite the increase of number of processor (2.76).
Anyhow, the topics of the tutorial is not the GS calculation.

The input files are provided, in the directory *\$ABI_TESTS/tutoparal/Input*.
The preparatory step is driven by *tdfpt_03.abi*. The real
(=DFPT) test case is driven by *tdfpt_04.abi*. The
reference output files are present in *\$ABI_TESTS/tutoparal/Refs*:
*tdfpt_03_MPI24.abo* and *tdfpt_04_MPI24.abo*. The naming convention is such that the
number of cores used to run them is added after the name of the test: the
*tdfpt_03.abi* files are run with 24 cores.
The preparatory step takes about 3 minutes, and the DFPT step takes about
3 minutes as well.

{% dialog tests/tutoparal/Input/tdfpt_03.abi tests/tutoparal/Input/tdfpt_04.abi %}

You can run now these test cases. For tdfpt_03, with [[autoparal]]=1, 
you will be able to run on different numbers of processors compatible with [[nkpt]]=4,
[[nband]]=120 and [[ngfft]]=[30 30 192], detected by ABINIT. Alternatively, you might decide to explicitly 
define [[npkpt]], [[npband]] and [[npfft]].
At variance, for tdfpt_04, no adaptation of the input file is
needed to be able to run on an arbitrary number of processors.
To launch the ground-state computation, type:

    mpirun -n 24 abinit tdfpt_03.abi > log &

then copy the output of the ground-state calculation so that it can be used as
the input of the DFPT calculation:

    mv tdfpt_03o_WFK.nc tdfpt_04i_WFK.nc

and launch the calculation:

    mpirun -n 24 abinit tdfpt_04.abi > log &

Now, examine the obtained output file for test 04, especially the timing.

In the reference file *\$ABI_TESTS/tutoparal/Refs/tdfpt_04_MPI24.abo*,
with 24 computing cores, the timing section delivers:

    - For major independent code sections, cpu and wall times (sec),
    -  as well as % of the time and number of calls for node 0-
    -<BEGIN_TIMER mpi_nprocs = 24, omp_nthreads = 1, mpi_rank = 0>
    - cpu_time =          159.9, wall_time =          160.0
    -
    - routine                        cpu     %       wall     %      number of calls  Gflops    Speedup Efficacity
    -                                                                  (-1=no count)
    - projbd                        46.305   1.2     46.345   1.2          11520      -1.00        1.00       1.00
    - nonlop(apply)                 42.180   1.1     42.183   1.1           5760      -1.00        1.00       1.00
    - dfpt_vtorho:MPI               25.087   0.7     25.085   0.7             30      -1.00        1.00       1.00
    - fourwf%(pot)                  22.435   0.6     22.436   0.6           6930      -1.00        1.00       1.00
    - nonlop(forces)                 5.485   0.1      5.486   0.1           4563      -1.00        1.00       1.00
    - fourwf%(G->r)                  4.445   0.1      4.446   0.1           2340      -1.00        1.00       1.00
    - pspini                         2.311   0.1      2.311   0.1              1      -1.00        1.00       1.00

    <...>

    - For major independent code sections, cpu and wall times (sec),
    - as well as % of the total time and number of calls

    -<BEGIN_TIMER mpi_nprocs = 24, omp_nthreads = 1, mpi_rank = world>
    - cpu_time =        3826.4, wall_time =        3828.0
    -
    - routine                         cpu     %       wall     %      number of calls Gflops    Speedup Efficacity
    -                                                                  (-1=no count)
    - projbd                      1238.539  32.4   1239.573  32.4         278400      -1.00        1.00       1.00
    - nonlop(apply)               1012.783  26.5   1012.982  26.5         139200      -1.00        1.00       1.00
    - fourwf%(pot)                 544.047  14.2    544.201  14.2         167040      -1.00        1.00       1.00
    - dfpt_vtorho:MPI              450.196  11.8    450.170  11.8            720      -1.00        1.00       1.00
    - nonlop(forces)               131.081   3.4    131.129   3.4         108576      -1.00        1.00       1.00
    - fourwf%(G->r)                107.885   2.8    107.930   2.8          55680      -1.00        1.00       1.00
    - pspini                        54.945   1.4     54.943   1.4             24      -1.00        1.00       1.00

    <...>

    - others (99)                  -98.669  -2.6    -99.848  -2.6             -1      -1.00        0.99       0.99
    -<END_TIMER>

    - subtotal                    3569.873  93.3   3570.325  93.3                                  1.00       1.00

You will notice that the run took about 160 seconds (wall clock time)..
The sum of the major independent code sections is reasonably
close to 100%. You might now explore the behaviour of the CPU time for
different numbers of compute cores (consider values below and above 24
processors). Some time-consuming routines will benefit from the parallelism, some other will not.

The kpoint + band parallelism will efficiently work for many important sections
of the code: projbd, fourwf%(pot), nonlop(apply), fourwf%(G->r).
In this test, the product nkpt (the effective number of k points for the current
perturbation) times nband is 8*120=960. Of course, the total speed-up will
saturate well below this value, as there are some non-parallelized sections of the code.

In the above-mentioned list, the kpoint+band parallelism cannot be exploited
(or is badly exploited) in several sections of the code : "dfpt_vtorho:MPI",
about 12 percents of the total time of the run on 24 processors, "pspini", about 1.4 percent. 
This amounts to about 1/8 of the total.
However, the scalability of the band parallelisation is rather poor, and effective saturation
in this case already happens at 16 processor.

A study of the speed-up brought by the combined k-point and band parallelism for this test case
on a 2 AMD EPYC 7502 machine (2 CPUS, each with 32 cores)
gives the following behaviour, between 1 and 40 cores:

<div id="plotly_plot2" style="width:90%;height:450px;"></div>
<script>
$(function() {
    Plotly.newPlot(document.getElementById('plotly_plot2'), 
        [{ x: [1, 2, 4, 6, 8, 12, 16, 24, 32, 40], y: [1, 2, 4, 6, 8, 12, 16, 24, 32, 40], name: 'Ideal'},
         { x: [1, 2, 4, 6, 8, 12, 16, 24, 32, 40], y: [1, 1.97, 3.93, 3.86, 7.67, 7.53, 14.77, 16.44, 15.47, 14.37], name: 'Observed' }],
        { title: "Parallel speed-up", 
          xaxis: {title:'Number of cores'} }
    );
}); 
</script>

The additional band parallelism is very efficient when running with 16 cores, bringing 14.77 speed-up,
while using only the k point parallelism, with 6 cores, gives 7.67 speed-up. However, the behaviour
is disappointing beyond 16 cores, or even for a number of processors which is not a multiple of 8.

Such a behaviour might be different on your machine.

**3.2.** A better parallelism can be seen if the number of k-points is brought
back to a converged value (8x8x1). Again, 
Try this if you have more than 100 processors at hand.

Set in your input files *tdfpt_03.abi* and *tdfpt_04.abo* :

       ngkpt 8 8 1    ! This should replace ngkpt 4 4 1

and launch again the preliminary step, then the DFPT step. Then, you can
practice the DFPT calculation by varying the number of computing cores. For
the latter, you could even consider varying the number of self-consistent
iterations to see the initialisation effects (small value of nstep), or target
a value giving converged results ([[nstep]] 50 instead of [[nstep]] 18). The energy
cut-off might also be increased (e.g. [[ecut]] 40 Hartree gives a much better
value). Indeed, with a large value of k points, and large value of nstep, you
might be able to obtain a speed-up of more than one hundred for the DFPT
calculation, when compared to a sequential run (see below). Keep track of the
time for each computing core number, to observe the scaling.

On a machine with a good communication network, the following results were observed in 2011.
The Wall clock timing decreases from

    - Proc.   0 individual time (sec): cpu=       2977.3  wall=       2977.3

with 16 processors to

    - Proc.   0 individual time (sec): cpu=        513.2  wall=        513.2

with 128 processors.

The next figure presents the speed-up of a typical calculation with increasing
number of computing cores (also, the efficiency of the calculation).

![Schema 1](paral_dfpt_assets/Speedup.jpeg)

Beyond 300 computing cores, the sequential parts of the code start to dominate.
With more realistic computing parameters ([[ecut]] 40), they dominate only beyond 600 processors.

However, on the same (recent, but with slow connection beyond 32 cores) computer than for the [[ngkpt]] 4 4 1 case,
the saturation sets in already beyond 16 cores, with the following behaviour (the reference is taken with respect to the timing at 4 processors):

<div id="plotly_plot3" style="width:90%;height:450px;"></div>
<script>
$(function() {
    Plotly.newPlot(document.getElementById('plotly_plot3'),
        [{ x: [1,4, 8, 12, 16, 24, 32, 40], y: [1, 4, 8, 12, 16, 24, 32, 40], name: 'Ideal'},
         { x: [4, 8, 12, 16, 24, 32, 40], y: [4, 7.86, 10.46, 14.92, 13.82, 16.15, 15.27], name: 'Observed' }],
        { title: "Parallel speed-up",
          xaxis: {title:'Number of cores'} }
    );
});
</script>

Thus, it is very important that you gain some understanding of the scaling of your typical runs
for your particular computer, and that you know the parameters (especially [[nkpt]]) of your calculations. 
Up to 4 or 8 cores, the ABINIT scaling will usually be very good, if
k-point parallelism is possible. In the range between 10 and 100 cores, the speed-up might still be good,
but this will depend on details.

This last example is the end of the present tutorial. 
The basics of the current implementation of the parallelism for the DFPT part of ABINIT have been explained,
then you have explored two test cases: one for a small cell
materials, with lots of k points, and another one, medium-size, in which the k
point and band parallelism must be used. It might reveal efficient, but this will depend on the detail of your calculation
and your computer architecture.
---
authors: JWZ, PhG, MVeithen, XG
---

# Tutorial on polarization and finite electric fields

## Polarization, and responses to finite electric fields for AlP.

This tutorial describes how to obtain the following physical properties, for an insulator:

  * The polarization.
  * The Born effective charge (by finite differences of polarization)
  * The Born effective charge (by finite differences of forces)
  * The dielectric constant
  * The proper piezoelectric tensor (clamped and relaxed ions)

The case of the linear responses (for the Born effective charge, dielectric constant,
piezoelectric tensor) is treated independently in other tutorials
([Response-Function 1](/tutorial/rf1), [Elastic properties](/tutorial/elastic)),
using Density-Functional Perturbation Theory.
You will learn here how to obtain these quantities using finite
difference techniques within ABINIT. To that end, we will describe how to
compute the polarization, in the Berry phase formulation, and how to perform
finite electric field calculations.

The basic theory for the Berry phase computation of the polarization was proposed
by R. D. King-Smith and D. Vanderbilt in [[cite:Kingsmith1993]].
The longer excellent paper by  D. Vanderbilt and R. D. King-Smith ([[cite:Vanderbilt1993]])
clarifies many aspects of this theory.
Good overviews of this subject may be found in the review article
[[cite:Resta1994]] and book [[cite:Vanderbilt2018]].

In order to gain the theoretical background needed to perform a calculation
with a finite electric field, you should consider reading the following papers:
[[cite:Souza2002]], [[cite:Nunes2001]] and
[[https://www.abinit.org/sites/default/files/PhD-M.Veithen.pdf|M. Veithen PhD thesis]].
Finally, the extension to the PAW formalism specifically in ABINIT is
discussed in [[cite:Gonze2009]] and [[cite:Zwanziger2012]].

This tutorial should take about 1 hour and 30 minutes.

[TUTORIAL_README]

## 1 Ground-state properties of AlP and general parameters

*Before beginning, you might consider working in a different subdirectory, as for the other tutorials.
For example, create Work_ffield in \$ABI_TESTS/tutorespfn/Input*

In this tutorial we will assume that the ground-state properties of AlP have
been previously obtained, and that the corresponding convergence studies have been done.
We will adopt the following set of generic parameters:
```
acell   3*7.2728565836E+00
ecut    5    
ecutsm  0.5
dilatmx 1.05
nband   4 (=number of occupied bands)
ngkpt   6 6 6
nshiftk 4
shiftk  0.5 0.5 0.5
        0.5 0.0 0.0
        0.0 0.5 0.0
        0.0 0.0 0.5

pseudopotentials  Pseudodojo_nc_sr_04_pw_standard_psp8/P.psp8
                  Pseudodojo_nc_sr_04_pw_standard_psp8/Al.psp8
```

In principle, the [[acell]] to be used should be the one corresponding to the
optimized structure at the [[ecut]], and [[ngkpt]] combined with [[nshiftk]]
and [[shiftk]], chosen for the calculations.

For the purpose of this tutorial, in order to limit the duration of the runs,
we are working at a low cutoff of 5 Ha, for which the optimized lattice constant is
equal to $7.27\times 2/\sqrt{2}=10.29~\mathrm{Bohr}$. Nonetheless this value is close to
that obtained with a highly converged geometry optimization, of 10.25~Bohr.
As always, if you adapt this tutorial to your own research, it is critical to perform
full convergence studies.
Before going
further, you might refresh your memory concerning the other variables:
[[ecutsm]], [[dilatmx]], [[ngkpt]], [[nshiftk]], and [[shiftk]].

Note as well that the pseudopotentials used here are freely available
from [[http://www.pseudo-dojo.org|Pseudo Dojo]]. The ones chosen here for P and Al
use the Perdew-Wang parameterization of the local density approximation (LDA); this is
done to facilitate comparison of the results of this tutorial with those of 
[Non-linear properties](/tutorial/nlo).

## 2 Berry phase calculation of polarization in zero field

In this section, you will learn how to perform a Berry phase calculation of
the polarization. As a practical problem we will try to compute the Born
effective charges from finite difference of the polarization (under finite
atomic displacements), for AlP.

You can now copy the file *tffield_1.abi* to *Work_ffield*,
```sh
cd $ABI_TESTS/tutorespfn/Input
mkdir Work_ffield
cd Work_ffield
cp ../tffield_1.abi .
```

{% dialog tests/tutorespfn/Input/tffield_1.abi %}

Note that two pseudopotentials are mentioned in this input file: one for the
phosphorus atom, and one for the aluminum atom. The first listed, for
P in this case, will define the first type of atom. The second listed, for Al,
will define the second type of atom. This might be the first time that you
encounter this situation (more than one type of atom) in the tutorials, in
contrast with the four "basic" tutorials.  Because of the use of more than one
type of atom, the following input variables must be present:

  * [[ntypat]]
  * [[typat]]

You can start the calculation. It should take about 90 seconds on a typical
desktop machine.  In the meantime, examine the *tffield_1.abi* file. It
includes three computations (see the section labelled as *atomic positions*)
corresponding to, first, the reference optimized structure ($\tau=0$), and then
to the structure with the Al atom displaced from 0.01 bohr to the  right and to
the left (referred to as $\tau = +0.01$ and $\tau =-0.01$). These  are typical
for the amplitude of atomic displacement in this kind of finite difference
computation. Notice also that the displacements are given using [[xcart]], that
is, explicitly in Cartesian directions in atomic units, rather than the
primitive cell axes (which would use [[xred]]). This makes the correspondence
with the polarization output in Cartesian directions much simpler to
understand.

There are two implementations of the Berry phase within ABINIT. One
is triggered by positive values of [[berryopt]] and was implemented by Na Sai.
The other one is triggered by negative values of [[berryopt]] and was implemented
by Marek Veithen. Both are suitable to compute the polarization, however, here we will
focus on the implementation of Marek Veithen for two reasons. First, the
results are directly provided in Cartesian coordinates at the end of the run
(while the implementation of Na Sai reports them in reduced coordinates).
Second, the implementation of Marek Veithen is the one to be used for the
finite electric field calculation as described in the next section. Finally,
note also that Veithen's implementation works with [[kptopt]] = 1 or 2 while
Na Sai implementation is restricted to [[kptopt]] = 2, which is less convenient.

The input file is typical for a self-consistent ground state calculation. In addition to the
usual variables, for the Berry phase calculation we simply need
to include [[berryopt]] and [[rfdir]]:
```
berryopt      -1
rfdir          1 1 1
```

Make the run, then open the output file and look for the occurrence "Berry".
The output reports values of the Berry phase for individual k-point strings.

```
 Computing the polarization (Berry phase) for reciprocal vector:
  0.16667  0.00000  0.00000 (in reduced coordinates)
 -0.01620  0.01620  0.01620 (in cartesian coordinates - atomic units)
 Number of strings:   144
 Number of k points in string:    6

 Summary of the results
 Electronic Berry phase     2.206976733E-03
            Ionic phase    -7.500000000E-01
            Total phase    -7.477930233E-01
    Remapping in [-1,1]    -7.477930233E-01

           Polarization    -1.632453164E-02 (a.u. of charge)/bohr^2
           Polarization    -9.340041842E-01 C/m^2
```
The "Remapping in [-1,1]" is there to avoid the quantum of polarization. As
discussed in [[cite:Djani2012]], the indeterminacy of the quantum phase,
directly related to the quantum of polarization, can lead to spurious effects
(see Fig.  2 of the above-mentioned paper). By remapping on the [-1,1]
interval, any indeterminacy is removed. However, removing such a quantum of
polarization between two calculations might give the false impression that one
is on the same polarization branch in the two calculations, while actually the
branch is made different by this remapping. Cross-checking the polarization
results by computing the Born effective charge, further multiplied by the
displacements between the two geometries is an excellent way to estimate the
amplitude of the polarization.

Other subtleties of Berry phases, explained in [[cite:Vanderbilt1993]], also
apply.  First, note that neither the electronic Berry phase nor the ionic phase
vanish in this highly symmetric case, contrary to intuition. Even though AlP
does not have inversion symmetry, it does have tetrahedral symmetry, which
would be enough to make an ordinary vector vanish. But a lattice-valued vector
does not have to vanish: the lattice just has to transform into itself under
the tetrahedral point group. The ionic phase corresponds actually to a
lattice-valued vector (-3/4 -3/4 -3/4). Concerning the electronic phase, it
does not exactly vanish, unless the sampling of k points becomes continuous. 

If you go further in the file you will find the final results in cartesian
coordinates. You can collect them for the different values of $\tau$.

$\tau = 0$
```
 Polarization in cartesian coordinates (a.u.):
     Total: -0.282749182E-01  -0.282749182E-01  -0.282749182E-01

 Polarization in cartesian coordinates (C/m^2):
     Total: -0.161774270E+01  -0.161774270E+01  -0.161774270E+01
```
$\tau = +0.01$
```
 Polarization in cartesian coordinates (a.u.):
     Total: -0.281920467E-01  -0.282749119E-01  -0.282749119E-01

 Polarization in cartesian coordinates (C/m^2):
     Total: -0.161300123E+01  -0.161774234E+01  -0.161774234E+01
```
$\tau = -0.01$
```
 Polarization in cartesian coordinates (a.u.):
     Total: -0.283577762E-01  -0.282749119E-01  -0.282749119E-01

 Polarization in cartesian coordinates (C/m^2):
     Total: -0.162248340E+01  -0.161774234E+01  -0.161774234E+01
```
From the previous data, we can extract the Born effective charge of Al. Values
to be used are those in a.u., in order to find the charge in electron units. It
corresponds to (the volume of the primitive unit cell must be specified in atomic units too):
$$ Z^* = \Omega_0  \frac{P(\tau = +0.01) - P(\tau = -0.01)}{2\tau} $$
$$=  272.02 \frac{ (-2.8192\times 10^{-2}) - (-2.8358\times 10^{-2})}{0.02} $$
$$ = 2.258$$

For comparison, the same calculation using Density-Functional Perturbation Theory
(DFPT) can be done by using the file *\$ABI_TESTS/tutorespfn/Input/tffield_2.abi*.

{% dialog tests/tutorespfn/Input/tffield_2.abi %}

Actually, the file *tffield_2.abi*
not only leads to the computation of the Born effective charges, but also the
computation of the piezoelectric constants (see later).
You can review how to use DFPT in the
[tutorial Response-function 1](/tutorial/rf1) and 
[tutorial Response-function 2](/tutorial/rf2) tutorials. 

!!! note
    An interesting feature of *tffield_2.abi* is the use of `berryopt2 -2` in
    the second data set. This input variable causes the computation of the DDK
    wavefunctions using a finite difference formula, rather than the DFPT approach
    triggered by [[rfddk]]. Although not strictly required in the present DFPT calculation,
    the finite difference approach is necessary in the various
    Berry's phase computations of polarization, in order to maintain phase coherency
    between wavefunctions at neighboring k points. Therefore in the present tutorial we use
    the finite difference approach, in order to compare the results of the Berry's phase
    computation to those of DFPT more accurately.

!!! warning
    The use of kpoint overlaps in Berry's phase calculations is necessary, but causes the
    results to converge *much* more slowly with kpoint mesh density than other types of
    calculations. It is critical in production work using Berry's phase methods to check
    carefully the convergence with respect to kpoint mesh density.

Go ahead and run the input file,
and have a look at the output file, to identify the
place where the Born effective charge is written (search for the phrase
"Effective charges"). The value we get from DFPT is 2.254,
in surprisingly good agreement with the above-mentioned value of 2.258.  This
level of agreement is fortuitous for unconverged calculations. Both
methods (finite-difference and DFPT) will tend to the same value for better
converged calculations.

The DDB file generated by *\$ABI_TESTS/tutorespfn/Input/tffield_2.abi* can be used as input to
anaddb for further processing, using the input file *tffield_3.abi* and the *tffield_3.files* file.

{% dialog tests/tutorespfn/Input/tffield_3.files tests/tutorespfn/Input/tffield_3.abi %}

!!! note
    While the `abinit` program itself takes its input file as an
    argument, the `anaddb` post-processing program depends in general on multiple
    input files, and therefore it is more convenient to pipe in a file whose
    contents are just the names of the files that `anaddb` should ultimately use.  In
    the present case, the piped-in file *tffield_3.files* is written such that the
    DDB file is named *tffield_2o_DS3_DDB* (this is defined in the third line of
    *tffield_3.files*). In the event of a mismatch, you can either edit
    *tffield_3.files* to match the DDB you have, or change the name of the DDB
    file.

The DFPT calculation yields the following
piezoelectric constants, as found in
*tffield_3.abo*:
```
 Proper piezoelectric constants (clamped ion) (unit:c/m^2)

      0.00000000     -0.00000000      0.00000000
      0.00000000      0.00000000      0.00000000
     -0.00000000     -0.00000000     -0.00000000
     -0.64263948      0.00000000      0.00000000
      0.00000000     -0.64263948      0.00000000
      0.00000000      0.00000000     -0.64263948
 ....
 Proper piezoelectric constants (relaxed ion) (unit:c/m^2)

      0.00000000      0.00000000     -0.00000000
      0.00000000     -0.00000000     -0.00000000
      0.00000000     -0.00000000     -0.00000000
      0.13114427      0.00000000     -0.00000000
      0.00000000      0.13114427     -0.00000000
     -0.00000000     -0.00000000      0.13114427
```
{% dialog tests/tutorespfn/Refs/tffield_3.abo %}

The piezoelectric constants here are the change in polarization as a function
of strain [[cite:Wu2005]].  The rows are the strain directions using Voigt
notation (directions 1-6) while the columns are the polarization directions. In the
$\bar{4}3m$ crystal class of AlP, the only non-zero piezoelectric elements
are those associated with shear strain (Voigt notation strains $e_4$, $e_5$,
and $e_6$) [[cite:Nye1985]].

The relaxed ion values, where the ionic relaxation largely suppresses the electronic
piezoelectricity, will be more difficult to converge than the clamped ion result.

Because the Berry phase approach computes polarization, it can also be used to compute the
piezoelectric constants from finite difference of polarization with respect to strains.
This can be done considering clamped ions or relaxed ions configurations.
For this purpose, have a look at the files
*tffield_4.abi* (clamped ions) and *tffield_5.abi* (relaxed ions).

{% dialog tests/tutorespfn/Input/tffield_4.abi tests/tutorespfn/Input/tffield_5.abi %}

In these input files the finite strain is applied by multiplying the $e_4$ (Voigt notation)
strain tensor by the (dimensionless) unit cell vectors:
$$ 
   \left[\begin{matrix}
   1 & 0 & 0 \\\
   0 & 1 & e_4/2 \\\
   0 & e_4/2 & 1 \\\
   \end{matrix}\right]
   \left[\begin{matrix}
   0 & 1/\sqrt{2} & 1/\sqrt{2} \\\
   1/\sqrt{2} & 0 & 1/\sqrt{2} \\\
   1/\sqrt{2} & 1/\sqrt{2} & 0 \\\
   \end{matrix}\right]
=
   \left[\begin{matrix}
   0 & 1/\sqrt{2} & 1/\sqrt{2} \\\
   (1+e_4/2)/\sqrt{2} & e_4/2\sqrt{2} & 1/\sqrt{2} \\\
   (1+e_4/2)/\sqrt{2} & 1/\sqrt{2} & e_4/2\sqrt{2} \\\
   \end{matrix}\right]
$$
Don't forget that in the input file, vectors are read in as rows of numbers, not columns!

Notice how in the relaxed ion case, the input file includes [[ionmov]] = 2 and [[optcell]] = 0, in
order to relax the ion positions at fixed cell geometry. These calculations
should give the following final results (obtained by taking finite difference
expressions of the strains for different electric fields): 
$-0.6427~C/m^2$ for the clamped ion case, and $0.1310~C/m^2$ for the relaxed ion case.

For example, the clamped ion piezoelectric constant was obtained from *tffield_4.abo*:
```
== DATASET  2 ==========================================================
....
 Polarization in cartesian coordinates (C/m^2):
     Total: -0.162420887E+01  -0.162587046E+01  -0.162587046E+01
....    
== DATASET  3 ==========================================================
     ...
 Polarization in cartesian coordinates (C/m^2):
     Total: -0.161135421E+01  -0.160969264E+01  -0.160969264E+01
```
The difference between -0.162420887E+01 (obtained at strain +0.01) and
-0.161135421E+01 (obtained at train -0.01) gives the finite difference -0.0128546,
which, divided by 0.02 (the total change in strain) gives -0.6427, as noted above.

## 3 Finite electric field calculations

In this section, you will learn how to perform a calculation with a finite electric field.

You can now copy the file *\$ABI_TESTS/tutorespfn/Input/tffield_6.abi* to *Work_ffield*.

{% dialog tests/tutorespfn/Input/tffield_6.abi %}

You can start the run immediately.
It performs a finite field calculation at clamped atomic positions. You can look at this input file to
identify the features specific to the finite field calculation.

As general parameters, one has to specify [[nband]], [[nbdbuf]] and [[kptopt]]:

            nband          4
            nbdbuf         0
            kptopt         1

As a first step (dataset 11), the code must perform a Berry phase calculation
in zero electric field. For that purpose, 
it is necessary to set the values of [[berryopt]] and [[rfdir]]:

            berryopt11     -1
            rfdir11        1 1 1

!!! warning

    You cannot use berryopt to +1 to initiate a finite field calculation.
    You must begin with berryopt -1.

After that, there are different steps corresponding to various values of the
electric field, as set by [[efield]]. For those steps, it is important to take
care of the following parameters, as shown here for example for dataset 21:

            berryopt21     4
            efield21       0.0001  0.0001  0.0001
            getwfk21       11

The electric field is applied here along the [111] direction. It must be
incremented step by step (it is not possible to go to high field directly). At
each step, the wavefunctions of the previous step must be used. When the field
gets large, it is important to check that it does not significantly exceed
the critical field for Zener breakthrough (the drop of potential over the
Born-von Karmann supercell must be smaller than the gap). In practice, the
number of k-point must be enlarged to reach convergence. However, at the same
time, the critical field becomes smaller. In practice, reasonable fields can
still be reached for k-point grids providing a reasonable degree of
convergence. A compromise must however be found.

As these calculations are quite long, the input file has been limited to a small
number of
small fields. Three cases have been selected: $E = 0$, $E = +0.0001$ and $E = -0.0001$.
If you have time later, you can perform
more exhaustive calculations over a larger set of fields.

Various quantities can be extracted from the finite
field calculation at clamped ions using finite difference techniques: the Born
effective charge $Z^*$ can be extracted from the difference in forces at 
different electric fields, the optical dielectric
constant can be deduced from the polarizations at different fields, 
and the clamped ion
piezoelectric tensor can be deduced from the stress tensor at different fields. 
As an illustration
we will focus here on the computation of $Z^*$.

Examine the output file. For each field, the file contains various
quantities that can be collected. For the present purpose, we can look at the
evolution of the forces with the field and extract the following results from
the output file:

$E=0$
```
cartesian forces (hartree/bohr) at end:
   1     -0.00000000000000    -0.00000000000000    -0.00000000000000
   2     -0.00000000000000    -0.00000000000000    -0.00000000000000
```
$E = +0.0001$
```
 cartesian forces (hartree/bohr) at end:
    1     -0.00022532204220    -0.00022532204220    -0.00022532204220
    2      0.00022532204220     0.00022532204220     0.00022532204220
```
$E = -0.0001$
```
 cartesian forces (hartree/bohr) at end:
    1      0.00022548273033     0.00022548273033     0.00022548273033
    2     -0.00022548273033    -0.00022548273033    -0.00022548273033
```
In a finite electric field, the force on atom $A$ in direction $i$ can be written as:
$$
F_{A,i} = Z^*_{A,ii}E + \Omega_0 \frac{d\chi}{d\tau} E^2
$$

The value for positive and negative fields above are nearly the same, showing
that the quadratic term is almost negligible. This clearly appears in the
figure below where the field dependence of the force for a larger range of
electric fields is plotted.

![](ffield_assets/tffield_6_for.png)

We can therefore extract with good accuracy the Born effective charge as:

$$
Z^*_{\mathrm Al}    = \frac{F_{\mathrm Al}(E=+0.0001) - F_{\mathrm Al}(E=-0.0001)}{2\times 0.0001}
= \frac{(2.2532\times 10^{-4}) - (-2.2548\times 10^{-4})}{0.0002}
= 2.254.
$$

This value is similar to the value reported from DFPT. If you do calculations
for more electric fields, fitting them with the general expression of the
force above (including the $E^2$ term), you can find the $d\chi/d\tau$ term.
From the given input file *tffield_6.abi*, using all the fields,
you should find $d\chi/d\tau$ for Al of  = -0.0295. 

Going back to the output file, you can also look at the evolution of the
polarization with the field.

$E = 0$
```
 Polarization in cartesian coordinates (a.u.):
     Total: -0.282749182E-01  -0.282749182E-01  -0.282749182E-01
```
$E = +0.0001$
```
 Polarization in cartesian coordinates (a.u.):
     Total: -0.282310128E-01  -0.282310128E-01  -0.282310128E-01
```
$E = -0.0001$
```
 Polarization in cartesian coordinates (a.u.):
     Total: -0.283187730E-01  -0.283187730E-01  -0.283187730E-01
```

In a finite electric field, the polarization in terms of the linear and
quadratic susceptibilities is, in SI units,

$$
P_i = \epsilon_0\chi^{(1)}_{ij} E_j + \epsilon_0\chi^{(2)}_{ijk}  E_jE_k
$$

or, in atomic units:

$$
P_i = \frac{1}{4\pi}\chi^{(1)}_{ij} E_j + \frac{1}{4\pi}\chi^{(2)}_{ijk}  E_jE_k,
$$

as $4\pi\epsilon_0 = 1$ in atomic units.

The change of polarization for positive and negative fields above are nearly
the same, showing again that the quadratic term is almost negligible. This
clearly appears in the figure below where the field dependence of the
polarization for a larger range of electric fields is plotted.

![](ffield_assets/tffield_6_pol.png)

We can therefore extract the linear optical dielectric susceptibility:

$$
\chi_{11}^{(1)} = 4\pi\frac{P_1(E=+0.0001) - P_1(E=-0.0001)}{2\times 0.0001}
= 4\pi\frac{(-2.82310\times 10^{-2}) - (-2.83188\times 10^{-2})}{0.0002}
= 5.5166.
$$

The relative permittivity, also known as the dielectric constant, is

$$
\epsilon_{11}/\epsilon_0 = 1+ \chi^{(1)}_{11} = 6.5166.
$$

This value is a bit over the value of 6.463 obtained by DFPT from *tffield_3.abi*. 
Typically, finite field calculations converge with
the density of the k point grid more slowly than DFPT calculations.

If you do calculations for more electric fields, fitting them with the general
expression of the polarization above (including the $E^2$ term) you can find the non-
linear optical susceptibility $\chi^{(2)}/4\pi$ (in atomic units). 
For *tffield_6.abi* you should find $\chi^{(2)}/4\pi = 2.5427$, so 
in SI units $\chi^{(2)} = 62.14~\mathrm{pm/V}$ and $d_{36} = 15.54~\mathrm{pm/V}$. 
The relationship between the $\chi^{(2)}_{ijk}$ tensor and the $d_{ij}$ tensor (the
quantity reported by `abinit` in a nonlinear optics DFPT computation) involves
a variety of symmetries and is explained in detail in the book [[cite:Boyd2020]].

Looking at the evolution of the stress with electric field (see graphic below), you should also be
able to extract the piezoelectric constants. You can try to do it as an
exercise. As the calculation here was at clamped ions, you will get the
clamped ion proper piezoelectric constants. You should
obtain -0.6365 C/m$^2$. The relationships between the various response functions under
conditions of strain, stress, field, and so forth are discussed in depth in
[[cite:Wu2005]].

![](ffield_assets/tffield_6_stress.png)

You can modify the previous input file to perform a finite field 
calculation combined with ion relaxation, similarly to how
*tffield_5.abi* was modified from *tffield_4.abi*, giving access to the
the relaxed ion proper piezoelectric constant.
From the output of this run, analyzing the results in the same way as before,
you should obtain 0.1311 C/m$^2$ for the relaxed ion piezoelectric constant.
---
authors: XG, RC
---

# First tutorial on DFPT

## Dynamical and dielectric properties of AlAs

In this tutorial you will learn how to get the following physical properties (of an insulator) from density-functional perturbation theory (DFPT):

  * the phonon frequencies and eigenvectors at $\Gamma$
  * the dielectric constant
  * the Born effective charges
  * the LO-TO splitting
  * the phonon frequencies and eigenvectors at other q-points in the Brillouin Zone

In order to learn the use of the associated codes *mrgddb* and *anaddb*,
to produce phonon band structures and the associated
thermodynamical properties, please consult the [second tutorial on DFPT](/tutorial/rf2).

This tutorial should take about 2 hours.

[TUTORIAL_README]

## 1 The ground-state geometry of AlAs

*Before beginning, you might consider to work in a different subdirectory as for the other tutorials.
Why not create Work_rf1 in \$ABI_TESTS/tutorespfn/Input?*

```sh
cd $ABI_TESTS/tutorespfn/Input
mkdir Work_rf1
cd Work_rf1
cp ../trf1_1.abi .
```

!!! important

    The reference directory that contains the example files for the
    tutorial is no more `$ABI_TESTS/tutorial`(as for the basic tutorials and the
    specialized, non-DFPT ones), but `$ABI_TESTS/tutorespfn`.
    This will be the case for all the DFPT based part of the tutorial.

Note that two pseudopotentials are mentioned in the input file: one
for the Aluminum atom, and one for the Arsenic atom.
The first listed in *trf1_1.abi* (for Al) will define the first type of atom of the input file
(see input variables [[typat]] and [[ntypat]]) and the second (for As) will define the second type of atom. 
It might be the first time that you encounter this situation (more than one type of atoms) in the
tutorials, at variance with the first four basic tutorials.
!!! warning
    To access the pseudopotential, the input file expect you to define the variable ABI_PSPDIR in your environment.

You can copy the file *\$ABI_TESTS/tutorespfn/Input/trf1_1.abi* in *Work_rf1*.
This is your input file. You should read it carefully.

{% dialog tests/tutorespfn/Input/trf1_1.abi %}

It drives a single self-consistent calculation of the total energy of AlAs to generate the corresponding
self-consistent charge density and wavefunctions, that will be used for the DFPT calculations.

Note that the value of [[tolvrs]] is rather stringent.
This is because the wavefunctions determined by the present run will be used later as starting
point of the DFPT calculation.
The number of steps, [[nstep]], in this example file has been set to 25. You should always choose a large enough value of [[nstep]] to reach your [[tolvrs]] target.

!!! danger

    Do not follow blindly all examples of the tutorials: always check the convergence of your calculations while in production!

You will work at fixed [[ecut]] (3Ha) and k-point grid, defined by [[kptrlatt]] (the 8x8x8 Monkhorst-Pack grid).
In *real life* you should do a convergence test with respect to both parameters.
We postpone the discussion of the accuracy of these choices and
the choice of pseudopotential to the end of the
[fifth section of this tutorial](#5-dfpt-calculation-of-the-effect-of-an-homogeneous-electric-field).
They give acceptable but not very accurate results such that the running time is reasonable for a tutorial.

You should make the run (a few seconds):

    abinit trf1_1.abi > log 2> err

The resulting main output file, *trf1_1.abo*, should be similar to the one below:

{% dialog tests/tutorespfn/Refs/trf1_1.abo %}

This output file is not very long, so you can quickly read it entirely.
Note that one obtains the following value for the energy, in the final echo section:

     etotal   -9.7626837450E+00
     etotal   -9.7658722915E+00

However, we will rely later, for the purpose of doing finite differences, on a more accurate (more digits) value of this
total energy, that can be found about a dozen of lines before this final echo:

    total_energy        : -9.76587229147669E+00

The output file also mentions that the forces on both atoms vanish.

The run that you just made will be considered as defining a ground-state
configuration, on top of which responses to perturbations will be computed.
The main output of this ground-state run is the wavefunction file *trf1_1o_WFK*,
that you can already rename as *trf1_2i_WFK* to use it as input wave function for the next runs.

## 2 Frozen-phonon calculation of a second derivative of the total energy

We will now compute the second derivative of the total energy with
respect to an atomic displacement by different means.
For that purpose, you must first read [[help:respfn#intro|sections 0 and the first paragraph of section 1]]
of the *respfn help* file (the auxiliary help file, that deals specifically with the DFPT features).
We will explain later, in more detail, the signification of the different
input parameters introduced in section 1 of the *respfn help* file.

For the time being, in order to be able to perform a direct comparison with the result of
a DFPT calculation, we choose as a perturbation the displacement of the Al
atom along the first axis of the reduced coordinates.

You can copy the file *\$ABI_TESTS/tutorespfn/Input/trf1_2.abi* in *Work_rf1*.
This is your input file. You should open it and briefly look at the two
changes with respect to *trf1_1.abi*:
the change of [[xred]], and the reading of the wavefunction file, using the [[irdwfk]] input variable. 
!!! warning
    You need to copy trf1_1o_WFK to trf1_2i_WFK so Abinit can find the wavefunction during the calculation.

Then, you can make the run, following the same command as before, with a different files file, referring to *trf1_2.abi*.
The symmetry is lowered with respect to the ground-state geometry, so that the number of k-points
increases a lot, and of course, the CPU time.

{% dialog tests/tutorespfn/Input/trf1_2.abi tests/tutorespfn/Refs/trf1_2.abo %}

From this run, it is possible to get the values of the total energy, and the
value of the gradient of the total energy (dE) with respect to change of reduced coordinate (dt):

	 rms dE/dt=  3.5517E-03; max dE/dt=  5.0080E-03; dE/dt below (all hartree)
	    1       0.005007986445      0.002526333145      0.002526333145
	    2      -0.005007927934     -0.002526305645     -0.002526305645
            ...
	total_energy        : -9.76586978750721E+00

The change of reduced coordinate ([[xred]]) of the Al atom along the first axis was
rather small (1/1000 = 0.001), and we can make an estimate of the second derivative of
the total energy with respect to the reduced coordinate thanks to finite-difference formulas.

We start first from the total-energy difference.
The total energy is symmetric with respect to that perturbation, so that it has no linear term.
The difference between the ground-state value (-9.76587229147669E+00 Hartree) of
the previous run, and the perturbed value (-9.76586978750721E+00 Hartree) of
the present one, is thus one half of the square of the coordinate change
(0.001) times the second derivative of total energy (2DTE).
From these number, the 2DTE is 5.00793896 Hartree.

Alternatively, we can start from the reduced gradients. The value of the
reduced gradient with respect to a displacement of the Al atom along the first
reduced axis is 0.005007986445 Ha. At first order, this quantity is the
product of the 2DTE by the reduced coordinate difference. The estimate of the
2DTE is thus 5.007986445 Ha. The agreement with the other estimate is rather good (4.10^-5 Hartree).

However, it is possible to do much better, thanks to the use of a higher-order finite-difference formula.
For this purpose, one can perform another calculation, in which the change of
reduced coordinate along the first axis is 0.002, instead of 0.001.
The doubling of the perturbation allows for a rather
simple higher-order estimation, as we will see later.
The results of this calculation are as follows:

     rms dE/dt=  7.1249E-03; max dE/dt=  1.0016E-02; dE/dt below (all hartree)
	1       0.010016404892      0.005097557910      0.005097557910
    	2      -0.010016285027     -0.005097505086     -0.005097505086
        ...
     total_energy        : -9.76586227537498E+00

From these data, taking into account that the perturbation was twice stronger,
the same procedure as above leads to the values 5.008050855 Hartree (from
finite difference of energy) and 5.008202446 Hartree (from finite difference
of forces, the value 0.010016404892 has to be multiplied by 1000/2).

The combination of these data with the previous estimate can be done thanks to an
higher-order finite-difference formula, in which the difference of estimations
(the largest perturbation minus the smallest one) is divided by three, and
then subtracted from the smallest estimation.

As far as the total-energy estimation is concerned, the difference is 0.000111895 Ha, which divided by
three and subtracted from 5.00793896 Hartree, gives 5.007901661 Hartree.
The same higher-order procedure for force estimates gives 5.00791444 Hartree. So,
the agreement between total-energy estimate and force estimate of the 2DTE can
be observed up to the 6th digit, inclusive.

Before comparing this result with the 2DTE directly computed from the DFPT
capabilities of ABINIT, a last comment is in order. One can observe that the
action-reaction law is fulfilled only approximately by the system. Indeed, the
force created on the second atom, should be exactly equal in magnitude to the
force on the first atom. The values of dE/dt, mentioned above show a small,
but non-negligible difference between the two atoms. As an example, for the
doubled perturbation, there is a difference in the absolute values of the
first component of the reduced force, 0.010016404892 and -0.010016285027.

Actually, the forces should cancel each other exactly if the translation
symmetry is perfect. This is not the case, but the breaking of this symmetry
can be shown to arise **only** from the presence of the exchange-correlation
grid of points. This grid does not move when atoms are displaced, and so there
is a very small variation of the total energy when the system is moved as a
whole. It is easy to restore the action-reaction law, by subtracting from
every force component the mean of the forces on all atoms. This is actually
done when the gradient with respect to reduced coordinates are transformed
into forces, and specified in cartesian coordinates, as can be seen in the
output file for the small displacement:

     cartesian forces (hartree/bohr) at end:
	1     -0.00001684430130    -0.00094404759278    -0.00094404759278
    	2      0.00001684430130     0.00094404759278     0.00094404759278


This effect will be seen also at the level of 2DTE. The so-called *acoustic sum rule*,
which imposes that the frequency of three modes (called acoustic modes)
tends to zero with vanishing wavevector, will also be slightly broken. In this
case, it will also be rather easy to reimpose the acoustic sum rule. In any
case, taking a finer XC grid will allow one to reduce this effect.

## 3 DFPT calculation of a second derivative of the total energy

We now compute the second derivative of the total energy with respect to the
same atomic displacement through the DFPT capabilities of ABINIT.

You can copy the file *\$ABI_TESTS/tutorespfn/Input/trf1_3.abi* in *Work_rf1*.
This is your input file. You should examine it. The changes with respect to
*trf1_1.abi* are all gathered in the first part of this file, before

```
#######################################################################
#Common input variables
```

Accordingly, you should get familiarized with the new input variables:
[[rfphon]], [[rfatpol]], [[rfdir]]. Then, pay attention to the special use of
the [[kptopt]] input variable. It will be explained in more detail later.

!!! warning
    You need to copy trf1_1o_WFK to trf1_3i_WFK so Abinit can find the wavefunction during the calculation.


{% dialog tests/tutorespfn/Input/trf1_3.abi %}

When you have understood the purpose of the input variable values specified
before the "Common input variables" section, you can make the code run, as usual.

Then, we need to analyze the different output files. For that purpose, you should read
the content of the [[help:respfn#output|section 6]] of the respfn help file.
Read it quickly, as we will come back to the most important points hereafter.

ABINIT has created several different files:

  * *trf1_3.log* (the log file)
  * *trf1_3.abo* (the output file), possibly also trf1_3o_OUT.nc, an abridged netCDF version
  * *trf1_3o_1WF1* (the 1st-order wavefunction file)
  * *trf1_3o_DEN1* (the 1st-order density file)
  * *trf1_3o_POT1* (the 1st-order potential file)
  * *trf1_3o_DDB* (the derivative database), possibly also *trf1_3o_DDB.nc*, its netCDF version

Let us have a look at the output file. You can follow the description provided
in the [[help:respfn#output|section 6.2]] of the respfn help file.

{% dialog tests/tutorespfn/Refs/trf1_3.abo %}

You should be able to find the place where the iterations
for the minimisation (with respect to the unique perturbation) take place:

          iter   2DEtotal(Ha)       deltaE(Ha) residm    vres2
    -ETOT  1   6.5139692863477     -1.464E+01 1.148E-02 1.945E+02
     ETOT  2   5.0217046300208     -1.492E+00 9.268E-04 2.029E+00
     ETOT  3   5.0082169144442     -1.349E-02 5.342E-06 5.671E-02
     ETOT  4   5.0079142425429     -3.027E-04 1.607E-07 2.092E-03
     ETOT  5   5.0079045457133     -9.697E-06 5.596E-09 3.120E-05
     ETOT  6   5.0079044210095     -1.247E-07 9.980E-11 2.323E-07
     ETOT  7   5.0079044201247     -8.848E-10 8.647E-13 2.761E-09

From these data, you can see that the 2DTE determined by the DFPT technique is
in excellent agreement with the higher-order finite-difference values for the
2DTE, determined in the previous section: 5.007939 Hartree from the energy
differences, and 5.007914 Hartree from the force differences.

Now, you can read the remaining of the [[help:respfn#output|section 6.2]] of the respfn help file.
Then, you should also open the *trf1_3o_DDB* file, and read the
corresponding [[help:respfn#ddb|section 6.5]] of the respfn help file.

Finally, the excellent agreement between the finite-difference formula and the
DFPT approach calls for some accuracy considerations. These can be found in
[[help:respfn#numerical-quality|section 7]] of the respfn help file.

!!! tip

    With |AbiPy|, one can easily visualize the convergence of the DFPT cycle with the |abiopen| script
    and the syntax:

        abiopen.py trf1_3.abo --expose -sns=talk

    ![](rf1_assets/abiopen_trf1_3.out.png)

## 4 DFPT calculation of the dynamical matrix at $\Gamma$

We are now in the position to compute the full dynamical matrix at the $\Gamma$ point (q=0).
You can copy the file *\$ABI_TESTS/tutorespfn/Input/trf1_4.abi* in *Work_rf1*.
This is your input file.

As for test rf1_3, the changes with respect to *trf1_1.abi* are
all gathered in the first part of this file. Moreover, the changes with
respect to *trf1_3.abi* concern only the input variables [[rfatpol]], and [[rfdir]].
Namely, all the atoms will be displaced, in all the directions.

!!! warning
    You need to copy trf1_1o_WFK to trf1_4i_WFK so Abinit can find the wavefunction during the calculation.

{% dialog tests/tutorespfn/Input/trf1_4.abi%}

There are six perturbations to consider.
So, one might think that the CPU time will raise accordingly.
This is not true, as ABINIT is able to determine which perturbations are the symmetric of another perturbation,
see [[help:respfn#symmetries|section 3]] of the respfn help file.

Now, you can make the run. You open the file *trf1_4.abo*, and notice that the
response to two perturbations were computed explicitly, while the response to
the other four could be deduced from the two first by using the symmetries.

     The list of irreducible perturbations for this q vector is:
        1)    idir= 1    ipert=   1
        2)    idir= 1    ipert=   2

Nothing mysterious: one of the two irreducible perturbations is for the Al
atom, placed in a rather symmetric local site, and the other perturbation is for the As atom.

The phonon frequencies, obtained by diagonalizing the dynamical matrix (where
the atomic masses have been taken into account, see [[amu]]), are given as follows:

      Phonon wavevector (reduced coordinates) :  0.00000  0.00000  0.00000
     Phonon energies in Hartree :
       2.559712E-06  2.559712E-06  2.559713E-06  1.568567E-03  1.568567E-03
       1.568567E-03
     Phonon frequencies in cm-1    :
    -  5.617917E-01  5.617918E-01  5.617921E-01  3.442606E+02  3.442606E+02
    -  3.442606E+02


!!! tip

    You might wonder about the dash sign present in the first column of the two
    lines giving the frequencies in cm$^{-1}$. The first column of the main ABINIT
    output files is always dedicated to signs needed to automatically treat the
    comparison with respect to reference files. Except if you become an ABINIT
    developer, you should ignore these signs. In the present case, they should not
    be interpreted as a minus sign for the floating numbers that follow them...

There are a good and a bad news about this result. The good news is that
there are indeed three acoustic modes, with frequency rather close to zero
(less than 1 cm$^{-1}$, which is rather good!). The bad news comes when the three
other frequencies are compared with experimental results, or other theoretical
results. Indeed, in the present run, one obtains three degenerate modes, while
there should be a (2+1) splitting. This can be seen in the paper Ab initio
calculation of phonon dispersions in semiconductors [[cite:Giannozzi1991]], especially Fig. 2.

Actually, we have forgotten to take into account the coupling between atomic
displacements and the homogeneous electric field, that exists in the case of
polar insulators, for so-called "Longitudinal Optic (LO) modes". A splitting
appears between these modes and the "Transverse Optic (TO) modes". This
splitting (Lyddane-Sachs-Teller LO-TO splitting) is presented in simple terms
in standard textbooks, and should not be forgotten in doing Ab initio
calculations of phonon frequencies.

Thus we have now to treat correctly the homogeneous electric field type perturbation.

<a id="5"></a>
## 5 DFPT calculation of the effect of an homogeneous electric field

The treatment of the homogeneous electric field perturbation is formally much
more complex than the treatment of atomic displacements.
This is primarily because the change of potential associated with an homogeneous electric field
is not periodic, and thus does not satisfy the Born-von Karman periodic boundary conditions.

For the purpose of the present tutorial, one should read the section II.C of
the above-mentioned paper [[cite:Giannozzi1991]].
The reader will find in  [[cite:Gonze1997]] and [[cite:Gonze1997a]]
more detailed information about this perturbation, closely related to the ABINIT implementation.
There is also an extensive discussion of the Born
effective charges by [[cite:Ghosez1998]].

In order to compute the response of solids to an homogeneous electric field in ABINIT,
the remaining sections of the respfn help file
should be read. These sections also present the information needed to compute
phonons with non-zero q wavevector, which will be the subject of the next
section of the present tutorial. The sections to be read are:

  * the part of [[help:respfn#1|section 1]] that had not yet been read
  * [[help:respfn#2|section 2]]
  * [[help:respfn#4|section 4]]
  * and, for completeness, [[help:respfn#5|section 5]]

You are now in the position to compute the full dynamical matrix at $\Gamma$ (q=0),
including the coupling with an homogeneous electric field.
You can copy *\$ABI_TESTS/tutorespfn/Input/trf1_5.abi* in *Work_rf1*.
This is your input file.

{% dialog tests/tutorespfn/Input/trf1_5.abi tests/tutorespfn/Refs/trf1_5.abo %}

As for the other DFPT tests, the changes with respect to the *trf1_1.abi* are all gathered
in the first part of this file.
Unlike the other tests, however, the multi-dataset mode was used, computing from scratch
the ground-state properties, then computing the effect of the ddk perturbation, then the effect of all
other perturbations (electric field as well as atomic displacements).

The analysis of the output file is even more cumbersome than the previous ones.
Let us skip the first dataset. In the dataset 2 section, one perturbation is correctly selected:

     ==>  initialize data related to q vector <==

     The list of irreducible perturbations for this q vector is:
        1)    idir= 1    ipert=   3

    ================================================================================

    --------------------------------------------------------------------------------
     Perturbation wavevector (in red.coord.)   0.000000  0.000000  0.000000
     Perturbation : derivative vs k along direction   1


The analysis of the output for this particular perturbation is not
particularly interesting, except for the f-sum rule ratio

     dfpt_looppert : ek2=    1.6833336546E+01
              f-sum rule ratio=    1.0028274804E+00

that should be close to 1, and becomes closer to it when [[ecut]] is
increased, and the sampling of k points is improved. (In the present status of
ABINIT, the f-rule ratio is not computed correctly when [[ecutsm]]/=0)

In the third dataset section, three irreducible perturbations are considered:

     ==>  initialize data related to q vector <==

     The list of irreducible perturbations for this q vector is:
        1)    idir= 1    ipert=   1
        2)    idir= 1    ipert=   2
        3)    idir= 1    ipert=   4

Much later, the dielectric tensor is given:

      Dielectric tensor, in cartesian coordinates,
         j1       j2             matrix element
      dir pert dir pert     real part    imaginary part

       1    4   1    4         9.7501435881        -0.0000000000
       1    4   2    4         0.0000000000        -0.0000000000
       1    4   3    4         0.0000000000        -0.0000000000
    
       2    4   1    4         0.0000000000        -0.0000000000
       2    4   2    4         9.7501435881        -0.0000000000
       2    4   3    4         0.0000000000        -0.0000000000
    
       3    4   1    4         0.0000000000        -0.0000000000
       3    4   2    4         0.0000000000        -0.0000000000
       3    4   3    4         9.7501435881        -0.0000000000


It is diagonal and isotropic, and corresponds to a dielectric constant of 9.7501435881.

Then, the Born effective charges are given, either computed from the
derivative of the wavefunctions with respect to the electric field, or
computed from the derivative of the wavefunctions with respect to an atomic
displacement, as explained in section II of [[cite:Gonze1997a]]:

      Effective charges, in cartesian coordinates,
      (from electric field response)
      ...


and

      Effective charges, in cartesian coordinates,
      (from phonon response)
      ...

Namely, the Born effective charge of the Al atom is 2.105, and the one of the
As atom is -2.127. The charge neutrality sum rule is not fulfilled exactly.
When [[ecut]] is increased, and the sampling of k points is improved, the sum
of the two charges goes closer to zero.

Finally, the phonon frequencies are computed:

      Phonon wavevector (reduced coordinates) :  0.00000  0.00000  0.00000
     Phonon energies in Hartree :
       2.559710E-06  2.559710E-06  2.559711E-06  1.568567E-03  1.568567E-03
       1.568567E-03
     Phonon frequencies in cm-1    :
    -  5.617914E-01  5.617914E-01  5.617917E-01  3.442606E+02  3.442606E+02
    -  3.442606E+02
    
      Phonon at Gamma, with non-analyticity in the
      direction (cartesian coordinates)  1.00000  0.00000  0.00000
     Phonon energies in Hartree :
       2.559710E-06  2.559710E-06  4.044009E-06  1.568567E-03  1.568567E-03
       1.729799E-03
     Phonon frequencies in cm-1    :
    -  5.617914E-01  5.617914E-01  8.875575E-01  3.442606E+02  3.442606E+02
    -  3.796470E+02
    
      Phonon at Gamma, with non-analyticity in the
      direction (cartesian coordinates)  0.00000  1.00000  0.00000
     Phonon energies in Hartree :
       2.559710E-06  2.559711E-06  4.044009E-06  1.568567E-03  1.568567E-03
       1.729799E-03
     Phonon frequencies in cm-1    :
    -  5.617914E-01  5.617917E-01  8.875573E-01  3.442606E+02  3.442606E+02
    -  3.796470E+02
    
      Phonon at Gamma, with non-analyticity in the
      direction (cartesian coordinates)  0.00000  0.00000  1.00000
     Phonon energies in Hartree :
       2.559710E-06  2.559711E-06  4.044008E-06  1.568567E-03  1.568567E-03
       1.729799E-03
     Phonon frequencies in cm-1    :
    -  5.617914E-01  5.617917E-01  8.875573E-01  3.442606E+02  3.442606E+02
    -  3.796470E+02


There are four sections. In the first one, any effect of the homogeneous electric field is simply discarded,
while the three next sections the electric field is considered along the three cartesian coordinates.

In the present material, the directionality of the electric field has no
influence. We note that there are still three acoustic mode, below 1 cm$^{-1}$,
while the optic modes have the correct degeneracies: two TO modes at 344.3
cm$^{-1}$, and one LO mode at 379.6 cm$^{-1}$.

These values can be compared to experimental (361 cm$^{-1}$, 402 cm$^{-1}$) as well
as theoretical (363 cm$^{-1}$, 400 cm$^{-1}$) values (again [[cite:Giannozzi1991]]).
Most of the discrepancy comes from the too low value of
[[ecut]]. Using ABINIT with [[ecut]]=6 Hartree gives (358.8 cm$^{-1}$, 389.8 cm$^{-1}$).
The remaining of the discrepancy may come partly from the
pseudopotentials, that are particularly soft.

The comparison of Born effective charges is also interesting. After imposition
of the neutrality sum rule, the Al Born effective charge is 2.116. The value
from Gianozzi et al is 2.17, the experimental value is 2.18.
Increasing [[ecut]] to 6 Hartree in ABINIT gives 2.168.

For the dielectric tensor, it is more delicate. The value from Gianozzi et al
is 9.2, while the experimental value is 8.2 . The agreement is not very good,
a fact that can be attributed to the DFT lack of polarization-dependence [[cite:Gonze1995a]].
Still, the agreement of our calculation with the theoretical result is not very good. With
[[ecut]] = 3 Hartree, we have 9.75. Changing it to 6 Hartree gives 10.40 . A
better k point sampling (8x8x8), with [[ecut]] = 6 Hartree, reduces the value to 9.89.
Changing pseudopotentials finally improves the agreement: with the
much harder *al.psp8* and *as.psp8* pseudopotentials with adequate
[[ecut]] = 20 Hartree and 8x8x8 Monkhorst-Pack sampling, we reach a value of 9.30. 
Note that we need to change [[ixc]]=-1012 and consider [[nband]]=9, since there is 3 electrons for Al and 15 electrons for As moving in these pseudopotential.
This information can be found by searching zion in the .abo of any file using the pseudopotentials. 
This illustrates that the dielectric tensor is a much more sensitive quantity than the others.

## 6 DFPT calculation of phonon frequencies at non-zero q

The computation of phonon frequencies at non-zero q is actually simpler than the one at $\Gamma$.
One must distinguish two cases. Either the q wavevector connects k points that belong
to the same grid, or the wavevector q is general.
In any case, the computation within the reciprocal space DFPT formalism is more
efficient than the real space frozen-phonon technique since the use of supercells is
completely avoided with DFPT. For an explanation of this fact, see for example section IV of [[cite:Gonze1997]].

You can copy the file *\$ABI_TESTS/tutorespfn/Input/trf1_6.abi* in *Work_rf1*.
This is your input file.

{% dialog tests/tutorespfn/Input/trf1_6.abi tests/tutorespfn/Refs/trf1_6.abo %}

As for the other RF tests, the changes with respect to *trf1_1.abi* are
all gathered in the first part of this file.
The multi-dataset mode is used, computing from scratch the ground-state wave functions, then computing different
dynamical matrices with DFPT.
The run is about 1 minutes on a 2.8 GHz machine.
In the mean time, you might read more of the ABINIT documentation
(why not the [[help:mrgddb|mrgddb_help]] and the [[help:anaddb|anaddb_help]]).

The results of this simulation can be compared to those provided in [[cite:Giannozzi1991]].
The agreement is rather good, despite the low cut-off energy, and different pseudopotentials.

At X, they get 95 cm$^{-1}$, 216 cm$^{-1}$, 337 cm$^{-1}$ and 393 cm$^{-1}$, while we get
92.8 cm$^{-1}$, 204.6 cm$^{-1}$, 313.8 cm$^{-1}$ and 375.9 cm$^{-1}$.
With [[ecut]]=6 Hartree, we get 89.7 cm$^{-1}$, 212.3 cm$^{-1}$, 328.5 cm$^{-1}$ and 385.8 cm$^{-1}$.

At L, they get 71 cm$^{-1}$, 212 cm$^{-1}$, 352 cm$^{-1}$ and 372 cm$^{-1}$, while we get
69.2 cm$^{-1}$, 202.5 cm$^{-1}$, 332.6 cm$^{-1}$ and 352.3 cm$^{-1}$. With [[ecut]]=6 Hartree, we
get 68.1 cm$^{-1}$, 208.5 cm$^{-1}$, 346.7 cm$^{-1}$ and 362.6 cm$^{-1}$.

At q=(0.1 0 0), we get 31.7 cm$^{-1}$, 63.6 cm$^{-1}$, 342.0 cm$^{-1}$ and 379.7 cm$^{-1}$. The
acoustic modes tends (nearly-)linearly to zero, while the optic modes are
close to their values at $\Gamma$ : 344.3 cm$^{-1}$ and 379.6 cm$^{-1}$.

* * *

!!! note
    This ABINIT tutorial is now finished.
    You are advised to go through the [second tutorial on DFPT](/tutorial/rf2) to
    make some post-processing analysis (phonon dispersions, thermodynamical properties, *etc*)

---
authors: MT, FJ
---

# First tutorial on the Projector Augmented-Wave (PAW) method

## Projector Augmented-Wave approach, how to use it?

This tutorial aims at showing how to perform a calculation within the **Projector Augmented-Wave** (PAW) method.


You will learn how to launch a PAW calculation and what are the main input
variables that govern convergence and numerical efficiency.
You are supposed to know how to use ABINIT with _Norm-Conserving PseudoPotentials_ (NCPP).

This tutorial should take about 1.5 hour.

[TUTORIAL_README]

## 1. Summary of the PAW method

The **Projector Augmented-Wave** approach has been introduced by Peter Blochl in 1994:

> The Projector Augmented-Wave method is an extension of
  augmented wave methods and the pseudopotential approach, which combines their
  traditions into a unified electronic structure method".
  It is based on a linear and invertible transformation (the PAW transformation)
  that connects the "true" wavefunctions $\Psi$ with "auxiliary" (or "pseudo") soft
  wavefunctions $\tPsi$:

\begin{equation} \label{eq:PAW_true_wavefunction}
|\Psi\rangle = |\tPsi\rangle + \sum_i \sum_a \Bigl(|\phi_i^a\rangle - |\tphi_i^a\rangle \Bigr)
\,\langle \tprj_i^a|\tPsi\rangle = |\tPsi\rangle + \sum_a |\Psi^1_a\rangle - |\tPsi_a^1\rangle
\end{equation}

This relation is based on the definition of _augmentation regions_
(atomic spheres of radius $r_c$), around the atoms in which the partial
waves $|\phi_i\rangle$ form a basis of atomic wavefunctions; $|\tphi_i\rangle$ are *pseudized*
partial waves (obtained from $|\phi_i\rangle$), and $|\tprj_i\rangle$ are dual functions of
the $|\tphi_i\rangle$ called projectors.
It is therefore possible to write every quantity depending on $\Psi_n$ (density,
energy, Hamiltonian) as a function of $\tPsi_n$ and to find $\tPsi_n$ by solving self-consistent equations.

The PAW method has two main advantages:

1. From $\tPsi$, it is always possible to obtain the true *all electron* wavefunction $\Psi$,
2. The convergence is comparable to an _UltraSoft PseudoPotential_ (USPP) one.

From a practical point of view, a PAW calculation is
rather similar to a Norm-Conserving PseudoPotential one.
Most noticeably, one will have to use a special atomic data file (**PAW dataset**) that contains the
$\phi_i$, $\tphi_i$ and $\tprj_i$ and that plays the same role as a pseudopotential file.

!!! tip

    It is highly recommended to read the following papers to understand correctly
    the basic concepts of the PAW method: [[cite:Bloechl1994]] and [[cite:Kresse1999]].
    The implementation of the PAW method in ABINIT is detailed in [[cite:Torrent2008]],
    describing specific notations and formulations

## 2. Using PAW with ABINIT

*Before continuing, you might consider to work in a different subdirectory as
for the other tutorials. Why not Work_paw1?
In what follows, the name of files are mentioned as if you were in this subdirectory.
All the input files can be found in the `$ABI_TESTS/tutorial/Input` directory.*

!!! important

    You can compare your results with reference output files located in
    `$ABI_TESTS/tutorial/Refs` and `$ABI_TESTS/tutorial/Refs/tpaw1_addons`
    directories (for the present tutorial they are named `tpaw1_*.abo`).

The input file *tpaw1_1.abi* is an example of a file to be used to compute
the total energy of diamond at the experimental volume (within the
_LDA exchange-correlation functional_). Copy the *tpaw1_1.abi* file in your work directory,

```sh
cd $ABI_TESTS/tutorial/Input
mkdir Work_paw1
cd Work_paw1
cp ../tpaw1_1.abi .
```

and execute ABINIT:

    abinit tpaw1_1.abi >& log

The code should run very quickly.
In the meantime, you can read the input file and see that there is no PAW input variable.

{% dialog tests/tutorial/Input/tpaw1_1.abi %}

Now, open the *tpaw1_1.abi* file and change the line with pseudopotential information:
replace the *PseudosTM_pwteter/6c.pspnc* file with *Pseudodojo_paw_pw_standard/C.xml*.
Run the code again:

    abinit tpaw1_1.abi >& log

Your run should stop almost immediately!
The input file, indeed, is missing the mandatory argument [[pawecutdg]]!!

Add the line:

    pawecutdg 50

to *tpaw1_1.abi* and run it again. Now the code completes successfully.

!!! note

    The time needed for the PAW run is greater than the time needed for
    the Norm-Conserving PseudoPotential run; indeed, at constant value of plane
    wave cut-off energy [[ecut]] PAW requires more computational resources:

    * the *on-site* contributions have to be computed,
    * the nonlocal contribution of the PAW dataset uses 2 projectors per angular momentum,
      while the nonlocal contribution of the Present Norm-Conserving Pseudopotential uses only one.

    However, as the plane wave cut-off energy required by PAW is much smaller than
    the cut-off needed for the Norm-Conserving PseudoPotential (see next section),
    **a PAW calculation will actually require less CPU time**.

Let's open the output file (*tpaw1_1.abo*) and have a look inside (remember:
you can compare with a reference file in *$ABI_TESTS/tutorial/Refs/*).
Compared to an output file for a Norm-Conserving PseudoPotential run, an
output file for PAW contains the following specific topics:

* At the beginning of the file,
  some specific default PAW input variables ([[ngfftdg]], [[pawecutdg]], and
  [[useylm]]), mentioned in the section:

```
    -outvars: echo values of preprocessed input variables --------
```

* The use of two FFT grids, mentioned in:

```
Coarse grid specifications (used for wave-functions):

getcut: wavevector=  0.0000  0.0000  0.0000  ngfft=  18  18  18
        ecut(hartree)=     15.000   => boxcut(ratio)=   2.17276

Fine grid specifications (used for densities):

getcut: wavevector=  0.0000  0.0000  0.0000  ngfft=  32  32  32
        ecut(hartree)=     50.000   => boxcut(ratio)=   2.10918
```


* A specific description of the PAW dataset (you might follow the tutorial [PAW2](/tutorial/paw2),
devoted to the building of the PAW atomic data, for a complete understanding of the file):

```
--- Pseudopotential description ------------------------------------------------
- pspini: atom type   1  psp file is /Users/torrentm/WORK/ABINIT/GIT/beauty/tests//Psps_for_tests/Pseudodojo_paw_pw_standard/C.xml
- pspatm: opening atomic psp file    /Users/torrentm/WORK/ABINIT/GIT/beauty/tests//Psps_for_tests/Pseudodojo_paw_pw_standard/C.xml
- pspatm : Reading pseudopotential header in XML form from /Users/torrentm/WORK/ABINIT/GIT/beauty/tests//Psps_for_tests/Pseudodojo_paw_pw_standard/C.xml
 Pseudopotential format is: paw10
 basis_size (lnmax)=  4 (lmn_size=  8), orbitals=   0   0   1   1
 Spheres core radius: rc_sph= 1.50736703
 1 radial meshes are used:
  - mesh 1: r(i)=AA*[exp(BB*(i-1))-1], size=2001 , AA= 0.94549E-03 BB= 0.56729E-02
 Shapefunction is SIN type: shapef(r)=[sin(pi*r/rshp)/(pi*r/rshp)]**2
 Radius for shape functions =  1.30052589
 mmax= 2001
 Radial grid used for partial waves is grid 1
 Radial grid used for projectors is grid 1
 Radial grid used for (t)core density is grid 1
 Radial grid used for Vloc is grid 1
 Radial grid used for pseudo valence density is grid 1
 Mesh size for Vloc has been set to 1756 to avoid numerical noise.
 Compensation charge density is not taken into account in XC energy/potential
 pspatm: atomic psp has been read  and splines computed
```

* After the SCF cycle section:
  The value of the integrated compensation charge evaluated by two different
  numerical methodologies:   
  1. computed in the _augmentation regions_
     on the "spherical" grid,  
  2. computed in the whole simulation cell on the "FFT" grid...  
  A discussion on these two values will be done in a forthcoming section.

```
PAW TEST:
==== Compensation charge inside spheres ============
The following values must be close to each other ...
 Compensation charge over spherical meshes =      0.263866295499286
 Compensation charge over fine fft grid    =      0.263862122639226
```


* Information related the non-local term (pseudopotential intensity $D_{ij}$)
  and the spherical density matrix (augmentation wave occupancies $\rho_{ij}$):

```
==== Results concerning PAW augmentation regions ====

Total pseudopotential strength Dij (hartree):
Atom #  1
  ...
Atom #  2
  ...

Augmentation waves occupancies Rhoij:
Atom #  1
 ...
Atom #  2
 ...
```

* At the end of the file we find the decomposition of the total energy both
  by direct calculation and double counting calculation:

```
--- !EnergyTerms
iteration_state     : {dtset: 1, }
comment             : Components of total free energy in Hartree
kinetic             :  6.90447470595323E+00
hartree             :  9.62706609299964E-01
xc                  : -4.29580260772849E+00
Ewald energy        : -1.27864121210521E+01
psp_core            :  9.19814512188249E-01
local_psp           : -4.66849481780168E+00
spherical_terms     :  1.43754510318459E+00
total_energy        : -1.15261686159562E+01
total_energy_eV     : -3.13642998643870E+02
...

--- !EnergyTermsDC
iteration_state     : {dtset: 1, }
comment             : '"Double-counting" decomposition of free energy'
band_energy         :  3.08710945092562E-01
Ewald energy        : -1.27864121210521E+01
psp_core            :  9.19814512188249E-01
xc_dc               : -7.38241276339915E-01
spherical_terms     :  7.69958781192718E-01
total_energy_dc     : -1.15261691589185E+01
total_energy_dc_eV  : -3.13643013418624E+02
...
```


!!! Note
    The PAW total energy is not the equal to the one obtained in the Norm-Conserving PseudoPotential case.
    This is due to the arbitrary modification of the energy reference during the pseudopotential
    construction.  
    In PAW, comparing total energies computed with different atomic datasets is more meaningful: most of
    the parts of the energy are calculated exactly, and in general you should be
    able to compare numbers for (valence) *energies* between different PAW potentials or different codes.

## 3. Convergence with respect to the plane-wave basis cut-off

As in the usual Norm-Conserving PseudoPotential case, the critical convergence parameter is the cut-off
energy defining the size of the plane-wave basis.

###3.a. Norm-Conserving PseudoPotential case##

The input file *tpaw1_2.abi* contains data to be used to compute the convergence in [[ecut]]
for diamond (at experimental volume). There are 9 datasets, with increasing [[ecut]] values
from 8 Ha to 24 Ha.
You might use the *tpaw1_2.abi* file (with a standard Norm-Conserving
PseudoPotential), and run:

    abinit tpaw1_1.abi >& log

You should obtain the following _total energy_ values (see *tpaw1_2.abo*):

	etotal1    -1.1628880677E+01
	etotal2    -1.1828052470E+01
	etotal3    -1.1921833945E+01
	etotal4    -1.1976374633E+01
	etotal5    -1.2017601960E+01
	etotal6    -1.2046855404E+01
	etotal7    -1.2062173253E+01
	etotal8    -1.2069642342E+01
	etotal9    -1.2073328672E+01

You can check that the etotal convergence (at the 1 mHartree level) is not
achieved for $e_{cut} = 14$ Hartree.

###3.b. Projector Augmented-Wave case###

Use the same input file as in section **3.a**.  
Again, modify the last line of *tpaw1_2.abi*, replacing the *PseudosTM_pwteter/6c.pspnc* file
by *Pseudodojo_paw_pw_standard/C.xml*.  
Run the code again and open the ABINIT output file (_.abo_). You should obtain the values:

	etotal1    -1.1404460615E+01
	etotal2    -1.1496598029E+01
	etotal3    -1.1518754947E+01
	etotal4    -1.1524981521E+01
	etotal5    -1.1526736707E+01
	etotal6    -1.1527011746E+01
	etotal7    -1.1527027274E+01
	etotal8    -1.1527104066E+01
	etotal9    -1.1527236307E+01

 
You can check that:

The _etotal_ convergence (at 1 mHartree) is achieved for _$14 \le e_{cut} \le 16$ Hartree_
   (_etotal5_ is within 1 mHartree of the final value);

With the same input parameters, for diamond, **a PAW calculation needs a lower cutoff,
compared to a calculation with NCPPs**.

## 4. Convergence with respect to the double grid FFT cut-off

In a NCPP calculation, the _plane wave_ density grid should be (at least) twice bigger
than the wavefunctions grid, in each direction.
In a PAW calculation, the _plane wave_ density grid is tunable
thanks to the input variable [[pawecutdg]] (PAW: ECUT for Double Grid).  
This is mainly needed to allow the mapping of densities and potentials, located
in the augmentation regions (spheres), onto the global FFT grid.  
The number of points of the Fourier grid located in the spheres must be large
enough to preserve a minimal accuracy. It is determined from the cut-off energy
[[pawecutdg]].  
One of the most sensitive objects affected by this "grid
transfer" is the compensation charge density; its integral over the
augmentation regions (on spherical grids) must cancel with its integral over
the whole simulation cell (on the FFT grid).

Use now the input file *tpaw1_3.abi*.
The only difference with the *tpaw1_2.abi* file is that [[ecut]] is fixed to 12
Ha, while [[pawecutdg]] runs from 12 to 39 Ha.

{% dialog tests/tutorial/Input/tpaw1_3.abi %}

Launch ABINIT with these files; you should obtain the values (file *tpaw1_3.abo*):

	etotal1    -1.1518201683E+01
	etotal2    -1.1518333032E+01
	etotal3    -1.1518666700E+01
	etotal4    -1.1518802859E+01
	etotal5    -1.1518785111E+01
	etotal6    -1.1518739739E+01
	etotal7    -1.1518716669E+01
	etotal8    -1.1518723268E+01
	etotal9    -1.1518738376E+01
	etotal10   -1.1518752752E+01

We see that the variation of the energy with respect to the [[pawecutdg]] parameter is well
below the 1 mHa level.  
In principle, it should be sufficient to choose
pawecutdg = 12 Ha in order to obtain an energy change lower than 1 mHa.
In practice, it is better to keep a security margin. Here, for pawecutdg = 24 Ha
(5th dataset), the energy change is lower than 0.001 mHa: this choice will be more than enough.

!!! note

    Note the steps in the convergence. They are due to sudden
    changes in the grid size (see the output values for  [[ngfftdg]]) which do not
    occur for each increase of [[pawecutdg]]. To avoid troubles due to these
    steps, it is better to choose a value of [[pawecutdg]] slightly higher.

The convergence of the compensation charge has a similar behaviour; it is
possible to check it in the output file, just after the SCF cycle by looking at:

```
PAW TEST:
==== Compensation charge inside spheres ============
 The following values must be close to each other ...
 Compensation charge over spherical meshes =      0.252499599273249
 Compensation charge over fine fft grid    =      0.252497392737764
```

The two values of the integrated compensation charge density must be close to each other.
Note that, for numerical reasons, they cannot be exactly the same (integration
over a radial grid does not use the same scheme as integration over a FFT grid).

_Additional test_:
We want now to check the convergence with respect to [[ecut]] with a fixed value [[pawecutdg]] = 24 Ha.
Let's modify *tpaw1_2.abi* file, setting pawecutdg to 24 Ha, and let's launch ABINIT again.  
You should obtain the values:

	etotal1    -1.1628880677E+01
	etotal2    -1.1828052470E+01
	etotal3    -1.1921833945E+01
	etotal4    -1.1976374633E+01
	etotal5    -1.2017601960E+01
	etotal6    -1.2046855404E+01
	etotal7    -1.2062173253E+01
	etotal8    -1.2069642342E+01
	etotal9    -1.2073328672E+01

You can check again that the _etotal_ convergence (at the 1 mHartree level)
is achieved for $14 \le e_{cut} \le 16$ Hartree.

!!! Note

    Although [[pawecutdg]] should always be checked, in practice, a common use it
    to put it bigger than [[ecut]] and keep it constant during all calculations.
    Increasing [[pawecutdg]] slightly changes the CPU execution time, but above
    all it is memory-consuming.
    Note that, if [[ecut]] is already high, there is no need for a high [[pawecutdg]].

!!! Important

    When testing [[ecut]] convergency, [[pawecutdg]] has to remain
    constant to obtain consistent results.

## 5. Plotting PAW contributions to the Density of States (DOS)

We now use the input file *tpaw1_4.abi* file.
ABINIT is used to compute the Density Of State (DOS)
(see the [[prtdos]] keyword in the input file).
Also note that more k-points are used in order to increase the accuracy of the DOS.
[[ecut]] is set to 12 Ha, while [[pawecutdg]] is 24 Ha.

{% dialog tests/tutorial/Input/tpaw1_4.abi %}

Launch the code with these files; you should obtain the *tpaw1_4.abo* and the DOS file (*tpaw1_4o_DOS*):

    abinit tpaw1_4.abi >& log

You can plot the DOS file; for this purpose, use a graphical tool
and plot column 3 with respect to column 2.  
Example: if you use the |xmgrace| tool, launch:

    xmgrace -block tpaw1_4o_DOS -bxy 1:2

At this stage, you have a usual Density of State plot; nothing specific to PAW.

Now, edit the *tpaw1_4.abi* file, comment the "prtdos 1" line, and uncomment (or add):

    prtdos 3
    pawprtdos 1
    natsph 1 iatsph 1
    ratsph 1.51

[[prtdos]]=3 requires the output of the projected DOS;
[[natsph]]=1, [[iatsph]]=1 select the first carbon atom as
the center of projection, and [[ratsph]]=1.51 sets the radius of the projection area to 1.51
atomic units (this is exactly the radius of the PAW augmentation regions:
generally the best choice).  
[[pawprtdos]]=1 is specific to PAW.
With this option, ABINIT should compute all the contributions to the projected DOS.

Let us remember that:

\begin{equation*}
|\Psi\rangle = |\tPsi\rangle + \sum_i \sum_a \Bigl(|\phi_i^a\rangle - |\tphi_i^a\rangle \Bigr)
\,\langle \tprj_i^a|\tPsi\rangle
\end{equation*}

Within PAW, the total projected DOS has 3 contributions:

1. The smooth plane-waves (PW) contribution (from $|\tPsi\rangle$),
2. The all-electron on-site (AE) contribution (from $\langle \tprj^a_i|\tPsi\rangle |\phi_i^a\rangle$),
3. The pseudo on-site (PS) contribution (from $\langle \tprj^a_i|\tPsi\rangle |\tphi_i^a\rangle$).

Execute ABINIT again (with the modified input file).
You get a new DOS file, named *tpaw1_4o_DOS_AT0001*.
You can edit it and look inside; it contains the 3 PAW contributions
(mentioned above) for each angular momentum.
_In the diamond case, only $l = 0$ and $l = 1$ momenta are to be considered_.

Now, plot the file, using the 7th, 12th and 17th columns with respect to the
2nd one; it plots the 3 PAW contributions for $l = 0$ (the total DOS is the sum of
the three contributions).
If you use the |xmgrace| tool, launch:

    xmgrace -block tpaw1_4o_DOS_AT0001 -bxy 1:7 -bxy 1:12 -bxy 1:17

You should get this:

![Projected DOS - 4 proj](paw1_assets/DOS-4proj.jpg)

As you can see, the smooth PW contribution and the PS on-site contribution are close. At basis completeness,
they should cancel; we could approximate the DOS by the AE on-site part taken alone.  
That's exactly the purpose of the [[pawprtdos]] = 2 option: in that case, only the AE
on-site contribution is computed and given as a good approximation of the
total projected DOS. The main advantage of this option is that the computing time is
greatly reduced (the DOS is instantaneously computed).

However, as you will see in the next section, this approximation is only valid when:

1. The $\tphi_i$ basis is complete enough
2. The electronic density is mainly contained in the sphere defined by [[ratsph]].

## 6. Testing the completeness of the PAW partial wave basis

In the previous section we used a "standard" PAW dataset, with 2
_partial waves_ per angular momentum. It is generally the best compromise
between the completeness of the partial wave basis and the efficiency of the
PAW dataset (the more _partial waves_ you have, the longer the CPU time used by ABINIT is).

Let's have a look at the *<span style="color:green">\$ABI_PSPDIR/Pseudodojo_paw_pw_standard/C.xml</span>* file.
The tag <span style="color:green"><valence_states</span> has 4
<span style="color:green"><state</span> lines. This indicates the number of partial
waves (4) and their $l$ angular momentum. In the present file, there are two $l = 0$
partial waves and two $l = 1$ partial waves.

Now, let's open the *<span style="color:green">\$ABI_PSPDIR/C_paw_pw_2proj.xml</span>*
and *<span style="color:green">\$ABI_PSPDIR/C_paw_pw_6proj.xml</span>* PAW dataset files.
The first dataset is based on only **one** _partial wave_ per $l$; the second one is based on
 **three** _partial waves_ per $l$.
The completeness of the partial wave basis increases when you use
*<span style="color:green">C_paw_pw_2proj.xml</span>*,
*<span style="color:green">Pseudodojo_paw_pw_standard/C.xml</span>*
and *<span style="color:green">C_paw_pw_6proj.xml</span>*.

Now, let us plot the DOS using the two new PAW datasets.

1. Save the existing *tpaw1_4o_DOS_AT0001* file, naming it f.i. *tpaw1_4o_4proj_DOS_AT0001*.
2. Open the *tpaw1_4.abi* file and modify it in order to use the
   *<span style="color:green">C_paw_pw_2proj.xml</span>* PAW dataset.
3. Run ABINIT.
4. Save the new *tpaw1_4o_DOS_AT0001* file, naming it f.i. *tpaw1_4o_2proj_DOS_AT0001*.
5. Open the *tpaw1_4.abi* file and modify it in order to use the
   *<span style="color:green">C_paw_pw_2proj.xml</span>* PAW dataset.
6. Run ABINIT again.
7. Save the new *tpaw1_4o_DOS_AT0001* file, naming it f.i. *tpaw1_4o_6proj_DOS_AT0001*.

Then, plot the contributions to the projected DOS for the two new DOS files.
You should get:

![Projected DOS - 2 proj](paw1_assets/DOS-2proj.jpg)

![Projected DOS - 6 proj](paw1_assets/DOS-6proj.jpg)

Adding the DOS obtained in the previous section to the comparison, you
immediately see that the superposition of the plane wave part DOS (PW) and the PS
on-site DOS depends on the completeness of the partial wave basis!

Now, you can have a look at the 3 output files (one for each PAW dataset).  
A way to estimate the completeness of the partial wave basis is to compare
derivatives of total energy; if you look at the stress tensor:

```
For the 2 `partial-wave` basis:   2.85307799E-04  2.85307799E-04 2.85307799E-04  0.  0.  0.
For the 4 `partial-wave` basis:   4.97872484E-04  4.97872484E-04 4.97872484E-04  0.  0.  0.
For the 6 `partial-wave` basis:   5.38392693E-04  5.38392693E-04 5.38392693E-04  0.  0.  0.
```

The *2 partial-wave* basis is clearly not complete; the *4 partial-wave basis* results are correct.
Such a test is useful to estimate the precision we can expect on the stress tensor
(at least due to the partial wave basis completeness).

You can compare other results in the 3 output files: total energy, eigenvalues, occupations...

!!! Note
    The CPU time increases with the size of the partial wave basis. This is why it is
    not recommended to use systematically high-precision PAW datasets.  
    If you want to learn how to generate PAW datasets with different *partial wave* basis,
    you might follow the [tutorial on generating PAW datasets (PAW2)](/tutorial/paw2).

## 7. Checking the validity of PAW results

The validity of our computation has to be checked
by comparison, on known structures, with known results.  
In the case of diamond, lots of computations and experimental results exist.

!!! important
    The validity of PAW calculations (**completeness of plane wave basis and
    partial wave basis**) should always be checked by comparison
    with **all-electrons** computations or with other existing PAW results;
    it should not be done by comparison with experimental results.
    As the PAW method has the same accuracy than all-electron methods, results should be very close.

In the case of diamond, all-electron results can be found f.i. in [[cite:Holzwarth1997]].
All-electron equilibrium parameters for diamond (within _Local Density Approximation_) obtained
with the FP-LAPW WIEN2K code are:

    a0 = 3.54 angstrom
    B = 470 GPa

Experiments give:

    a0 = 3.56 angstrom
    B = 443 GPa

Let's test with ABINIT.
We use now the input file *tpaw1_5.abi* file and we run
ABINIT to compute values of _etotal_ for several cell parameters
around 3.54 angstrom, using the standard PAW dataset.

{% dialog tests/tutorial/Input/tpaw1_5.abi %}

    abinit tpaw1_1.abi >& log

From the *tpaw1_5.abo* file, you can extract the 7 values of _acell_ and 7 values
of _etotal_, then put them into a file and plot it with a graphical tool.
You should get:

![diamond: etotal vs acell](paw1_assets/etotal-acell.png){ width=60% }

From this curve, you can extract the cell values of $a_0$ and $B$
(with the method of your choice, for example by a Birch-Murnhagan spline fit).
You get:

    a0 = 3.53 angstrom B = 469.5 GPa

These results are in excellent agreement with FP-LAPW ones!

## 8. Additional comments about PAW in ABINIT

### 8.a. Overlap of PAW augmentation regions###

In principle, the PAW formalism is only valid for non-overlapping augmentation spherical
regions. But, in usual cases, a small overlap between spheres is acceptable.
By default, ABINIT checks that the distances between atoms are large enough to
avoid overlap; a "small" voluminal overlap of 5% is accepted by default. This
value can be tuned with the [[pawovlp]] input keyword.
The overlap check can even be by-passed with [[pawovlp]]=-1 (not recommended!).

!!! warning

    While a small overlap can be acceptable for the
    augmentation regions, an overlap of the compensation charge densities has to
    be avoided. The compensation charge density is defined by a radius (named
    $r_{shape}$ in the PAW dataset) and an analytical shape function. The overlap
    related to the compensation charge radius is checked by ABINIT and a WARNING
    is eventually printed.

Also note that you can control the compensation charge radius and shape
function while generating the PAW dataset (see [tutorial on generating PAW datasets (PAW2)](/tutorial/paw2)).


### 8.b. Mixing scheme for the Self-Consistent cycle; decomposition of the total energy###

The use of an efficient **mixing scheme** in the self-consistent loop is a crucial
point to minimize the number of steps to achieve convergence.
This mixing can be done on the potential or on the density.  
By default, in a Norm-Conserving PseudoPotential calculation, the mixing is done on the potential;
but, for technical reasons, this choice is not optimal for PAW calculations.
Thus, by default, the mixing is done on the density when PAW is activated.  

The mixing scheme can be controlled by the [[iscf]] variable (see the different options of this input variable).
To compare both schemes, you can edit the *tpaw1_1.abi* file and try [[iscf]] = 7
or 17 and compare the behaviour of the SC cycle in both cases; as you can see, the
final _total energy_ is the same but the way to reach it is completely different.

Now, have a look at the end of the file and focus on the `Components of total
free energy`; the total energy is decomposed according to two different schemes (`direct` and `double counting`);
at very high convergence of the SCF cycle the potential/density
residual is very small and these two values should be the same.  
But it has been observed that
the converged value was reached more rapidly by the `direct` energy, when the
mixing is on the potential, and by the `double counting` energy when the mixing
is on the density. Thus, by default, in the output file is to print the direct
energy when the mixing is on the potential, and the double counting energy
when the mixing is on the density.

Also note that PAW _partial waves occupancies_ $\rho_{ij}$ also are
mixed during the SC cycle; by default, the mixing is done in the same way as the density.

### 8.c. PAW+U for correlated materials###

If the system under study contains strongly correlated electrons, the `DFT+U`
method can be useful. It is controlled by the  [[usepawu]], [[lpawu]],
[[upawu]] and [[jpawu]] input keywords.
Note that the formalism implemented in ABINIT is approximate, i.e. it is only valid if:

1. The $\tphi^a_i$ basis is complete enough;
2. The electronic density is mainly contained in the PAW sphere.

The approximation done here is the same as the one explained in the 5th
section of this tutorial: considering that smooth PW contributions and PS
on-site contributions are closely related, only the AE on-site contribution is
computed; it is indeed a very good approximation.

Converging a _Self-Consistent Cycle_, or ensuring the global minimum is reached,
with PAW+U is sometimes difficult. Using [[usedmatpu]] and [[dmatpawu]] can help.
See [tutorial on DFT+U](/tutorial/dftu).

### 8.d. Printing volume for PAW###

If you want to get more detailed output concerning the PAW computation, you
can use the [[pawprtvol]] input keyword.
It is particularly useful to print details about _pseudopotential strength_
$D_{ij}$ or _partial waves occupancies_ $\rho_{ij}$.

### 8.e. Additional PAW input variables###

Looking at the [[varset:paw|PAW variable set]], you can find the description
of additional input keywords related to PAW.
They are to be used when tuning the computation, in order to gain accuracy or save CPU time.  

See also descriptions of these variables and input file examples in the [[topic:PAW|PAW topic]] page.

!!! Warning

    In a standard computation, these variables should not be modified!
---
authors: MG
---

# Zero-point renormalization of the band gap and temperature-dependent band gaps

This tutorial explains how to compute the electron self-energy due to phonons, obtain the zero-point
renormalization (ZPR) of the band gap and temperature-dependent band energies within the harmonic approximation.
We start with a very brief overview of the many-body formalism in the context of the electron-phonon (e-ph) interaction.
Then we discuss how to evaluate the e-ph self-energy and perform typical convergence studies
using the polar semiconductor MgO as an example.
Further details concerning the implementation are given in [[cite:Gonze2019]] and [[cite:Romero2020]].
Additional examples are provided in this
[jupyter notebook](https://nbviewer.jupyter.org/github/abinit/abitutorials/blob/master/abitutorials/sigeph/lesson_sigeph.ipynb)
that explains how to use Abipy to automate the calculations and post-process the results for Diamond.

It is assumed the user has already completed the two tutorials [RF1](/tutorial/rf1) and [RF2](/tutorial/rf2),
and that he/she is familiar with the calculation of ground state (GS) and response properties
in particular phonons, Born effective charges and the high-frequency dielectric tensor.
The user should have read the [introduction tutorial for the EPH code](/tutorial/eph_intro)
before running these examples.

This lesson should take about 1.5 hour.

## Formalism

The electron-phonon self-energy, $\Sigma^{\text{e-ph}}$, describes the renormalization of
charged electronic excitations due to the interaction with phonons.
This term should be added to the electron-electron (e-e) self-energy $\Sigma^{\text{e-e}}$
that encodes many-body effects induced by the Coulomb interaction beyond the classical electrostatic Hartree potential.
The e-e contribution can be estimated using, for instance, the [$GW$ approximation](/tutorial/gw1) but
in this tutorial we are mainly interested in $\Sigma^{\text{e-ph}}$ and its temperature dependence.

In semiconductors and insulators, indeed, most of the temperature dependence of the electronic properties
at "low" $T$ originates from the **e-ph interaction**
and the **thermal expansion** of the unit cell (topic that, however, won't be discussed in this lesson).
Corrections due to $\Sigma^{\text{e-e}}$ are obviously important as it is well known that KS gaps computed
with LDA/GGA are systematically underestimated compared to experimental data.
Nevertheless, the temperature dependence of $\Sigma^{\text{e-e}}$
is rather small as long as $kT$ is smaller than the fundamental gap (let's say $3 kT < E_{\text{g}}$).

In state-of-the-art *ab-initio* perturbative methods, the e-ph coupling is described
within DFT by expanding the KS effective potential up to the **second order in the atomic displacement**,
and the vibrational properties are computed with DFPT [[cite:Gonze1997]], [[cite:Baroni2001]].
Note that anharmonic effects that may become relevant at "high" $T$ are not included in the present formalism.

The e-ph self-energy consists of two terms: the **frequency-dependent Fan-Migdal** (FM) self-energy
and the **static and Hermitian Debye-Waller** (DW) part (see e.g. [[cite:Giustino2017]] and references therein):

$$ \Sigma^\eph(\ww, T) = \Sigma^\FM(\ww, T) + \Sigma^{\DW}(T). $$

The diagonal matrix elements of the FM self-energy in the KS basis set are given by

\begin{equation}
\begin{split}
    \Sigma^\FM_{n\kk}(\omega, T) =
                & \sum_{m,\nu} \int_\BZ \frac{d\qq}{\Omega_\BZ} |\gkq|^2 \\
                & \times \left[
                    \frac{n_\qnu(T) + f_{m\kk+\qq}(\ef,T)}
                         {\omega - \emkq  + \wqnu + i \eta} \right.\\
                & \left. +
                    \frac{n_\qnu(T) + 1 - f_{m\kk+\qq}(\ef,T)}
                         {\omega - \emkq  - \wqnu + i \eta} \right] ,
\end{split}
\label{eq:fan_selfen}
\end{equation}

where $f_{m\kk+\qq}(\ef,T)$ and $n_\qnu(T)$ are the Fermi-Dirac and Bose-Einstein occupation functions
with $T$ the physical temperature and $\ef$ the Fermi level that in turns depends on $T$ and the number of electrons per unit cell.
The integration is performed over the $\qq$-points in the Brillouin zone (BZ) of volume $\Omega_\BZ$ and $\eta$
is a positive real infinitesimal.

!!! important

    From a mathematical point of view, one should take the limit $\eta \rightarrow 0^+$.
    At the level of the implementation, the infinitesimal $\eta$ is replaced by a (small)
    finite value given by the [[zcut]] variable that should be subject to convergence studies.
    More specifically, one should monitor the convergence of the physical properties of interest
    for [[zcut]] $\rightarrow 0^+$ and number of $\qq$-points $\rightarrow \infty$.
    Convergence studies should start from values of [[zcut]] that are comparable to the typical phonon frequency
    of the system (usually 0.01 eV or smaller).
    Note that the default value for [[zcut]] is 0.1 eV.
    This value is reasonable for $GW$ calculations but 0.1 eV is, very likely, too large when
    computing $\Sigma^{\text{e-ph}}$.

The **static DW term** involves the second order derivative of the KS potential with respect to the nuclear displacements.
State-of-the-art implementations approximate the DW contribution with

\begin{equation}
\label{eq:dw_selfen}
\Sigma_{n\kk}^{\DW}(T) = \sum_{\qq\nu m} (2 n_{\qq\nu}(T) + 1) \dfrac{g_{mn\nu}^{2,DW}(\kk, \qq)}{\ee_{n\kk} - \ee_{m\kk}},
\end{equation}

where $g_{mn\nu}^{2,\DW}(\kk,\qq)$ is an effective matrix element that, within the **rigid-ion approximation**,
can be expressed in terms of the standard first-order $\gkq$ matrix elements by exploiting the invariance
of the QP energies under infinitesimal translation [[cite:Giustino2017]].

At the level of the implementation, the number of bands in the two sums is defined by [[nband]]
while the $\qq$-mesh for the integration is specified by [[eph_ngqpt_fine]]
(or [[ddb_ngqpt]] if the DFPT potentials are not interpolated).
The list of temperatures (in Kelvin) is initialized from [[tmesh]].

For the sake of simplicity, we will omit the $T$-dependence in the next equations.
Keep in mind, however, that all the expressions in which $\Sigma$ is involved
have an additional dependence on the physical temperature $T$.

!!! important

    The EPH code takes advantage of time-reversal and spatial symmetries to reduce the BZ integration
    to an appropriate irreducible wedge, $\text{IBZ}_\kk$, defined by the little group of the $\kk$-point i.e.
    the set of point group operations of the crystal that leave the $\kk$-point invariant
    within a reciprocal lattice vector $\GG$.
    Calculations at high-symmetry $\kk$-points such as $\Gamma$ are therefore much faster as there
    are more symmetries that can be exploited (smaller $\text{IBZ}_k$).

    This symmetrization procedure is activated by default and can be disabled by setting [[symsigma]]
    to 0 for testing purposes.
    Note that when [[symsigma]] is set to 1, the code performs a final average of the QP results
    within each degenerate subspace.
    As a consequence, **accidental degeneracies won't be removed** when [[symsigma]] is set to 1.

Note that both the FM and the DW term converge slowly with the $\qq$-sampling.
Moreover, accurate computation of the real part require the inclusion of a large number of empty states.

In order to accelerate the convergence with [[nband]], the EPH code can replace the contributions
given by the high-energy states above a certain band index $M$ with the solution
of a **non-self-consistent Sternheimer equation** in which only the first $M$ states are required.
The methodology, proposed in [[cite:Gonze2011]], is based on a **quasi-static approximation**
in which the phonon frequencies in the denominator of Eq. (\ref{eq:fan_selfen}) are neglected and
the frequency dependence of $\Sigma$ is approximated with the value computed at $\omega = \enk$.
This approximation is justified when the bands above $M$ are **sufficiently high in energy** with respect
to the $n\kk$ states that must be corrected.
The Sternheimer approach requires the specification of [[eph_stern]] and [[getpot_filepath]].
The parameter $M$ corresponds to the [[nband]] input variable that, obviously,
**cannot be larger than the number of bands stored in the WFK file**
(the code will abort if this condition is not fulfilled).

### Quasi-particle corrections due to the e-ph coupling

Strictly speaking, the quasi-particle (QP) excitations are defined by the solution(s)
in the complex plane of the equation $$ z = \ee_\nk + \Sigma_\nk^{\text{e-ph}}(z) $$
provided the non-diagonal components of the e-ph self-energy can be neglected.
In practice, the problem is usually simplified by seeking **approximated solutions along the real axis**
following two different approaches:

1. **on-the-mass-shell**
2. **linearized QP equation**.

In the on-the-mass-shell approximation, the QP energy is simply given by the real part
of the self-energy evaluated at the bare KS eigenvalue:

$$ \ee^\QP_\nk = \ee_\nk + \Re\, \Sigma_\nk^{\text{e-ph}}(\ee_\nk). $$

<!--
This approach is equivalent to a (thermal average of) Rayleigh-Schrodinger perturbation theory.
-->
In the linearized QP equation, on the contrary, the self-energy is Taylor-expanded around
the bare KS eigenvalue and the QP correction is obtained using

$$ \ee^\QP_\nk = \ee_\nk + Z_\nk\,\Re  \Sigma^\text{e-ph}_\nk(\ee_\nk) $$

with the renormalization factor $Z_\nk$ given by

$$
  Z_\nk = \left(1 - \Re\left[ \frac{\partial\Sigma^\text{e-ph}_{\nk}}{\partial\ee}\right]\Bigg|_{\ee=\ee_\nk} \right)^{-1}.
$$

Note that $Z_\nk$ is approximately equal to the area under the QP peak in the spectral function:

$$
A_\nk(\ww) =
-\dfrac{1}{\pi} | \Im G_\nk(\omega) | =
-\dfrac{1}{\pi} \dfrac{\Im \Sigma_\nk(\ww)} {(\ww - \ee_\nk - \Re \Sigma_\nk(\ww)) ^ 2 + \Im \Sigma_\nk(\ww) ^ 2}
$$

Since $A_\nk(\ww)$ integrates to 1,
values of $Z_\nk$ in the [0.7, 1] range usually indicate the presence of a well-defined QP excitation
that may be accompanied by some **background** and, possibly, additional **satellites**.
Values of $Z_\nk$ greater than one are clearly unphysical and signal the breakdown of the linearized QP equation.
The interpretation of these results requires
a careful analysis of $A_\nk(\ww)$ and/or additional convergence tests.

!!! important

    Both approaches are implemented in ABINIT although it should be noted that, according to recent works,
    the on-the-mass-shell approach provides results that are closer to those obtained
    with more advanced techniques based on the cumulant expansion [[cite:Nery2018]].

The ZPR of the excitation energy is defined as the difference between the QP energy evaluated at $T$ = 0
and the bare KS energy.
In a similar way, one defines the ZPR of the gap as the difference between the QP band gap at $T$ = 0
and the KS gap.

It is worth to stress that the EPH code can compute QP corrections only for $\nk$ states
that are present in the input WFK file (a similar requirement is present in the $GW$ code as well).
As a consequence, the $\kk$-mesh ([[ngkpt]], [[nshiftk]], [[shiftk]]) for the WFK file
should be chosen carefully especially if the band edges are not located at high-symmetry $\kk$-points.

Note that there are different approaches one can use to specify the set of $\nk$ states in $\Sigma_{\nk}$.
Each approach has pros and cons.
The most direct way consists in specifying the $\kk$-points and the band range
using the three variables: [[nkptgw]], [[kptgw]], [[bdgw]]
For instance, in order to compute the correction for the VBM/CBM at $\Gamma$ in silicon
(non-magnetic semiconductor with 8 valence electrons per unit cell), one would use:

```sh
nkptgw 1
kptgw  0 0 0  # [3, nkptgw] array
bdgw   4 5    # [2, nkptgw] array giving the initial and the last band index
              # for each nkptgw k-point
```

as the index of the valence band is given by 8 / 2 = 4.
Obviously, this input file can only provide the ZPR of the direct gap as Si has an indirect fundamental gap.
This the most flexible approach but it requires the specification of three variables and, obviously, one should
know the positions of the CBM/VBM.
Alternatively, one can use [[gw_qprange]] or [[sigma_erange]].
<!--
although *sigma_erange* is usually employed for transport calculations with [[eph_task]] = -4.
Note that [[gw_qprange]] is mainly used to compute all the corrections for the occupied states plus
some conduction states
-->

!!! important

    When [[symsigma]] is set to 1 (default), the code may decide to enlarge the initial value of [[bdgw]]
    so that all **degenerate states** for that particular $\kk$-point are included in the calculation.

## Typical workflow for ZPR

A typical workflow for ZPR calculations involves the following steps 
(see the [introductory e-ph tutorial](/tutorial/eph_intro)):

1. **GS calculation** to obtain the WFK and the DEN file.
   The $\kk$-mesh should be dense enough to converge both electronic and vibrational properties.
   Remember to set [[prtpot]] to 1 to produce the file with the KS potential required by the Sternheimer method.

2. **DFPT calculations** for all the IBZ $\qq$-points corresponding to the *ab-initio* [[ddb_ngqpt]] mesh
   that will be used to perform the Fourier interpolation of the dynamical matrix and of the DFPT potentials.
   In the simplest case, one uses a $\qq$-mesh that is equal to the GS $\kk$-mesh (sub-meshes are also fine)
   and the DFPT calculations can directly start from the WFK produced in step #1.
   Remember to compute $\bm{\epsilon}^{\infty}$, $\bm{Z}^*$ (polar materials) and the dynamical quadrupoles
   $\bm{Q}^*$ as these quantities are needed for an accurate interpolation of phonon frequencies and DFPT potentials.

3. **NSCF computation** of a WFK file on a much denser $\kk$-mesh containing the wavevectors
   where phonon-induced QP corrections are wanted. The NSCF run uses the DEN file produced in step #1.
   Remember to compute **enough empty states** so that it is possible to perform
   convergence studies w.r.t. [[nband]] afterwards.

4. **Merge the partial DDB and POT files** with *mrgddb* and *mrgdvdb*, respectively.

5. Start from the DDB/DVDB files produced in step #4 and the WFK file obtained in step #3
   to **perform ZPR calculations** with [[eph_task]] 4.

## Getting started

[TUTORIAL_README]

Before beginning, you might consider to work in a different subdirectory as for the other tutorials. 
Why not create Work_eph4zpr in $ABI_TESTS/tutorespfn/Input?

```sh
cd $ABI_TESTS/tutorespfn/Input
mkdir Work_eph4zpr
cd Work_eph4zpr
```

In this tutorial, we prefer to focus on the use of the EPH code hence
we will be using **pre-computed** DDB and DFPT POT files to bypass the DFPT part.
We also provide a DEN.nc file to initialize the NSCF calculations
and a POT file with the GS KS potential required to solve the Sternheimer equation.

If *git* is installed on your machine, one can easily fetch the entire repository (23 MB) with:

```sh
git clone https://github.com/abinit/MgO_eph_zpr.git
```

Alternatively, use *wget*:

```sh
wget https://github.com/abinit/MgO_eph_zpr/archive/master.zip
```

or *curl*:

```sh
curl -L https://github.com/abinit/MgO_eph_zpr/archive/master.zip -o master.zip
```

or simply copy the tarball by clicking the "download button" available in the github web page,
unzip the file and rename the directory with:

```sh
unzip master.zip
mv MgO_eph_zpr-master MgO_eph_zpr
```

!!! warning

    The directory with the precomputed files must be located in the same working directory
    in which you will be executing the tutorial and must be named `MgO_eph_zpr`.

The |AbiPy| script used to executed the DFPT part is available
[here](https://github.com/abinit/MgO_eph_zpr/blob/master/run_zpr_mgo.py).
Note that several parameters have been tuned to reach a reasonable **compromise between accuracy
and computational cost** so do not expect the results obtained at the end of the lesson to be fully converged.
More specifically, we use norm-conserving pseudopotentials with a cutoff energy [[ecut]]
of 30 Ha (too low, it should be ~50 Ha).
The DFPT computations is done for the set of irreducible $\qq$-points corresponding
to a $\Gamma$-centered 4x4x4 $\qq$ mesh (again, too coarse).
$\bm{Z}^*$ and $\bm{\ee}^\infty$ are also computed with the same underconverged settings.

Since AbiPy does not support multiple datasets, each directory corresponds to a single calculation.
In particular, all the DFPT tasks (atomic perturbations, DDK, electric field perturbation)
can be found inside the `w1` directory while `w0/t0/outdata` contains the GS results.
and `w0/t1/outdata` the GSR file with energie on an high-symmetry $\kk$-path.

## How to extract useful info from the output files

Since this is the first tutorial that uses precomputed output files,
it is worth explaining how to use the terminal and command line utilities such as *ncdump*, *abitk* and |AbiPy| scripts
to extract useful information from the pre-computed files before moving to EPH calculations.

First of all, most of the netcdf files produced by ABINIT store the input file in string format
in the **input_string** netcdf variable.
This variable may be useful if you need to know the input variables used to produce that particular output file.
To print the value of **input_string** inside the terminal, use the *ncdump* utility and the syntax:

```sh
ncdump -v input_string MgO_eph_zpr/flow_zpr_mgo/w0/t0/outdata/out_DEN.nc

input_string = "jdtset 1 nband 12 ecut 35.0 ngkpt 4 4 4 nshiftk 1 shiftk 0 0 0 tolvrs 1e-12 nstep 150 iomode 3 prtpot 1 diemac 9.0 nbdbuf 4 paral_kgb 0 natom 2 ntypat 2 typat 2 1 znucl 8 12 xred 0.0000000000 0.0000000000 0.0000000000 0.5000000000 0.5000000000 0.5000000000 acell 1.0 1.0 1.0 rprim 0.0000000000 4.0182361526 4.0182361526 4.0182361526 0.0000000000 4.0182361526 4.0182361526 4.0182361526 0.0000000000" ;
```

In all the examples of this tutorial, we will be using the new [[structure]]
input variable (added in version 9) to initialize the unit cell from an external file so that
we don't need to repeat the unit cell over and over again in all the input files.
The syntax is :

```sh
structure "abifile:MgO_eph_zpr/flow_zpr_mgo/w0/t0/outdata/out_DEN.nc"
```

where the `abifile` prefix tells ABINIT that the lattice parameters and atomic positions
should be extracted from an ABINIT binary file e.g. HIST.nc, DEN.nc, GSR.nc, etc.
(other formats are supported as well, see the documentation).

To print the crystalline structure to terminal, use the *abitk* Fortran executable
shipped with the ABINIT package and the `crystal_print` command:

```md
abitk crystal_print MgO_eph_zpr/flow_zpr_mgo/w0/t0/outdata/out_DEN.nc

 ==== Info on the Cryst% object ====
 Real(R)+Recip(G) space primitive vectors, cartesian coordinates (Bohr,Bohr^-1):
 R(1)=  0.0000000  4.0182362  4.0182362  G(1)= -0.1244327  0.1244327  0.1244327
 R(2)=  4.0182362  0.0000000  4.0182362  G(2)=  0.1244327 -0.1244327  0.1244327
 R(3)=  4.0182362  4.0182362  0.0000000  G(3)=  0.1244327  0.1244327 -0.1244327
 Unit cell volume ucvol=  1.2975866E+02 bohr^3
 Angles (23,13,12)=  6.00000000E+01  6.00000000E+01  6.00000000E+01 degrees
 Time-reversal symmetry is present
 Reduced atomic positions [iatom, xred, symbol]:
    1)    0.0000000  0.0000000  0.0000000  Mg
    2)    0.5000000  0.5000000  0.5000000   O
```

This is the crystalline structure that we will be using in the forthcoming examples.
<!--
To print the same info in a format that we can directly reuse in the ABINIT input file, use:
$ abitk crystal_abivars flow_zpr_mgo/w0/t0/outdata/out_DEN.nc
-->

Since we want to compute the renormalization of the band gap due to phonons, it is also useful
to have a look at the KS gaps obtained from the GS run.
The command is:

```md
abitk ebands_gaps MgO_eph_zpr/flow_zpr_mgo/w0/t0/outdata/out_DEN.nc

 Direct band gap semiconductor
 Fundamental gap:     4.479 (eV)
   VBM:     4.490 (eV) at k: [ 0.0000E+00,  0.0000E+00,  0.0000E+00]
   CBM:     8.969 (eV) at k: [ 0.0000E+00,  0.0000E+00,  0.0000E+00]
 Direct gap:         4.479 (eV) at k: [ 0.0000E+00,  0.0000E+00,  0.0000E+00]
```

The same *abitk* command can be used with all netcdf files containing KS energies e.g *GSR.nc*, *WFK.nc*.

!!! warning

    Our values for the gaps are consistent with the results for MgO given on the
    [materialsproject](https://materialsproject.org/materials/mp-1265/).
    Remember, however, that the values and the positions of the gaps may vary
    (in somes cases even significantly) depending on the $\kk$-sampling.

    In this case, *abitk* reports the gaps computed from a $\kk$-mesh as the DEN file can only be produced
    by a SCF calculation.
    The results for MgO are correct simply because the CBM/VBM are at the $\Gamma$ point and **this point
    belongs to the GS $\kk$-mesh**.
    Other systems (e.g. Si) may have the CBM/VBM at wavevectors that are not easily captured with a homogeneous mesh.
    **The most reliable approach to find the location of the CBM/VBM is to perform a band structure calculation
    on a high-symmetry $\kk$-path.**

*abitk* is handy if you need to call Fortran routines from the terminal to perform basic tasks
but Fortran is not the best language when it comes to post-processing and data analysis.
This kind of operation, indeed, is much easier to implement using a high-level language such as python.
To plot the band structure using the GS eigenvalues stored in the GSR.nc file,
use the |abiopen| script provided by AbiPy with the `-e` option:

```sh
abiopen.py MgO_eph_zpr/flow_zpr_mgo/w0/t1/outdata/out_GSR.nc -e
```

![](eph4zpr_assets/MgO_ebands.png)

The figure confirms that the gap is direct at the $\Gamma$ point.
The VBM is three-fold degenerate when SOC is not included.


## How to merge partial DDB files with mrgddb

First of all, let's merge the partial DDB files with the command

```sh
mrgddb < teph4zpr_1.abi
```

and the following input file:

{% dialog tests/tutorespfn/Input/teph4zpr_1.abi %}

that lists the **relative paths** of the **partial DDB files** in the
`MgO_eph_zpr` directory.

Since we are dealing with a polar material, it is worth checking whether our final DDB contains
Born effective charges and the electronic dielectric tensor.
Instead of running *anaddb* or *abinit* and then checking the output file,
we can simply use |abiopen| with the `-p` option:

```sh
abiopen.py MgO_eph_zpr/flow_zpr_mgo/w1//outdata/out_DDB -p

================================== DDB Info ==================================

Number of q-points in DDB: 8
guessed_ngqpt: [4 4 4] (guess for the q-mesh divisions made by AbiPy)
ecut = 35.000000, ecutsm = 0.000000, nkpt = 36, nsym = 48, usepaw = 0
nsppol 1, nspinor 1, nspden 1, ixc = 11, occopt = 1, tsmear = 0.010000

Has total energy: False, Has forces: False
Has stress tensor: False

Has (at least one) atomic pertubation: True
Has (at least one diagonal) electric-field perturbation: True
Has (at least one) Born effective charge: True
Has (all) strain terms: False
Has (all) internal strain terms: False
Has (all) piezoelectric terms: False
```

We can also invoke *anaddb* directly from python to have a quick look at the phonon dispersion:

```text
abiview.py ddb MgO_eph_zpr/flow_zpr_mgo/w1//outdata/out_DDB

Computing phonon bands and DOS from DDB file with
nqsmall = 10, ndivsm = 20;
asr = 2, chneut = 1, dipdip = 1, lo_to_splitting = automatic, dos_method = tetra
```

that produces the following figures:

![](eph4zpr_assets/abiview.png)

The results seem reasonable: the acoustic modes go to zero linearly for $\qq \rightarrow 0$
as we are dealing with a 3D system, no instability is present
and the phonon dispersion shows the LO-TO splitting typical of polar materials.

Note, however, that the acoustic sum-rule is automatically enforced by the code so
it is always a good idea to compare the results with/without [[asr]] as this
is an indirect indicator of the convergence/reliability of our calculations.
We can automate the process with the *ddb_asr* command of |abiview|:

```text
abiview.py ddb_asr MgO_eph_zpr/flow_zpr_mgo/w1//outdata/out_DDB
```

that produces the following figure:

![](eph4zpr_assets/mgo_phbands_asr.png)

!!! important

    This clearly indicates that the breaking of the acoustic sum-rule is not negligible.
    In this case, the breaking is mainly due the too low cutoff energy employed in our calculations.
    In real life, one should stop here and redo the DFPT calculation with a larger [[ecut]]
    and possibly a denser $\qq$-mesh but since the goal of this lesson is to teach you how
    to run ZPR calculations, **we ignore this serious problem and continue with the other examples**.

    PS: If you want to compute the phonons bands with/without [[dipdip]], use:

    ```text
    abiview.py ddb_dipdip MgO_eph_zpr/flow_zpr_mgo/w1//outdata/out_DDB
    ```

## How to merge partial POT files with mrgdv

Now we can merge the DFPT potential with the *mrgdv* tool using the command.

```sh
mrgdv < teph4zpr_2.abi
```

and the following input file:

{% dialog tests/tutorespfn/Input/teph4zpr_2.abi %}

!!! tip

    The number at the end of the POT file corresponds to the (*idir*, *ipert*) pertubation for that
    particular $\qq$-point. The *pertcase* index is computed as:

    ```fortran
    pertcase = idir + ipert
    ```

    where *idir* gives the direction ([1, 2, 3]) and *ipert* specifies the perturbation type:

    - *ipert* in [1, ..., *natom*] corresponds to atomic perturbations (reduced directions)
    - *ipert* = *natom* + 1 corresponds d/dk  (reduced directions)
    - *ipert* = *natom* + 2 corresponds the electric field
    - *ipert* = *natom* + 3 corresponds the uniaxial stress (Cartesian directions)
    - *ipert* = *natom* + 4 corresponds the shear stress.   (Cartesian directions)

    All DFPT POT files with 1 <= pertcase <= 3 x [[natom]] therefore correspond to atomic pertubations
    for a given $\qq$-point.

    The value of *pertcase* and *qpt* are reported in the ABINIT header.
    To print the header to terminal, use *abitk* with the *hdr_print* command

    ```sh
     abitk hdr_print MgO_eph_zpr/flow_zpr_mgo/w1/t11/outdata/out_POT4.nc

     ===============================================================================
     ECHO of part of the ABINIT file header

     First record :
    .codvsn,headform,fform = 9.2.0      80  111

     Second record :
     bantot,intxc,ixc,natom  =   480     0    11     2
     ngfft(1:3),nkpt         =    32    32    32    40
     nspden,nspinor          =     1     1
     nsppol,nsym,npsp,ntypat =     1    48     2     2
     occopt,pertcase,usepaw  =     1     4     0
     ecut,ecutdg,ecutsm      =  3.5000000000E+01  3.5000000000E+01  0.0000000000E+00
     ecut_eff                =  3.5000000000E+01
     qptn(1:3)               =  5.0000000000E-01  0.0000000000E+00  0.0000000000E+00
     rprimd(1:3,1)           =  0.0000000000E+00  4.0182361526E+00  4.0182361526E+00
     rprimd(1:3,2)           =  4.0182361526E+00  0.0000000000E+00  4.0182361526E+00
     rprimd(1:3,3)           =  4.0182361526E+00  4.0182361526E+00  0.0000000000E+00
     stmbias,tphysel,tsmear  =  0.0000000000E+00  0.0000000000E+00  1.0000000000E-02

     The header contain   4 additional records.
    ```

    Use `--prtvol 1` to output more records.

Now we discuss in more detail the output file produced by *mrgdv*

{% dialog tests/tutorespfn/Refs/teph4zpr_2.stdout %}

For each $\qq$-point found in the partial POT files,
the code prints a lists with the atomic perturbations that have been merged in the database.

```md
 qpoint: [ 0.0000E+00,  0.0000E+00,  0.0000E+00] is present in the DVDB file
 The list of irreducible perturbations for this q vector is:
    1)  idir= 1, ipert=   1, type=independent, found=Yes
    2)  idir= 2, ipert=   1, type=symmetric, found=No
    3)  idir= 3, ipert=   1, type=symmetric, found=No
    4)  idir= 1, ipert=   2, type=independent, found=Yes
    5)  idir= 2, ipert=   2, type=symmetric, found=No
    6)  idir= 3, ipert=   2, type=symmetric, found=No
```

The term **symmetric** means that this particular *(idir, ipert)* perturbation
can be reconstructed by symmetry from the other **independent** entries with the same $\qq$-point.
If all the independent entries are available, the code prints the following message at the end of the output file:

```md
 All the independent perturbations are available
 Done
```

!!! warning

    If you don't get this message, the DVDB **cannot be used** by the EPH code.
    In this case, check carefully your DFPT input files and the list of POT files that have been merged.
    Also, note that it is not possible to change the value of [[nsym]] at the level of the EPH calculation
    as symmetries are automatically inherited from the previous GS/DFPT calculations.

## Computing the WFK files with empty states

At this point we have all the ingredients (**DDB** and **DVDB**) required to compute/interpolate
the e-ph scattering potentials and we can finally start to generate the WFK files.

For our first NSCF calculation, we use a 4x4x4 $\Gamma$-centered $\kk$-mesh and 70 bands
so that we can perform initial convergence studies for the number of empty states in the self-energy.
Then we generate WFK files with denser meshes and less bands that will be used for the Sternheimer method.
Note the use of [[getden_filepath]] to read the DEN.nc file instead of [[getden]] or [[irdden]].

You may now run the NSCF calculation by issuing:

```sh
abinit teph4zpr_3.abi > teph4zpr_3.log 2> err &
```

with the input file given by:

{% dialog tests/tutorespfn/Input/teph4zpr_3.abi %}

At this point, it is worth commenting about the use of [[nbdbuf]].
As mentioned in the documentation, **the highest energy states require more iterations to convergence**.
To avoid wasting precious computing time, we use a buffer that is ~10% of [[nband]].
This trick significantly reduces the wall-time as the NSCF calculation completes
only when the first [[nband]] - [[nbdbuf]] states are converged within [[tolwfr]].
Obviously, one should not use the last [[nbdbuf]] states in the subsequent EPH calculation.
The same trick is highly recommended when computing WFK files for $GW$ calculations.

!!! important

    For mobility calculations, it is possible to reduce significantly the cost of the WFK computation
    by restricting the NSCF calculation to the $\kk$-points inside the electron (hole) pockets
    relevant for transport.
    Unfortunately, this optimization is not possible when computing the real part of the self-energy
    as the integration must be performed in the full $\text{IBZ}_\kk$.
    On the other hand, ZPR calculations can take advange of the Sternheimer method to reduce the number
    of empty bands required to converge.

## Our first ZPR calculation

For our first example, we use a minimalistic input file so that we can
discuss the most important input variables and the content of the main output file.
First of all, you may want to start immediately the computation by issuing:

```sh
abinit teph4zpr_4.abi > teph4zpr_4.log 2> err &
```

with the following input file:

{% dialog tests/tutorespfn/Input/teph4zpr_4.abi %}

!!! tip

    To run the examples in parallel with e.g 2 MPI processes use:

    ```sh
    mpirun -n 2 abinit teph4zpr_4.abi > teph4zpr_4.log 2> err &
    ```

    The EPH code will automatically distribute the workload using a predefined distribution scheme
    (not necessarily the most efficient in terms of memory and wall-time).
    In the last part of the tutorial, we explain how to specify a particular
    MPI distribution scheme with the [[eph_np_pqbks]] input variable.

Let's now discuss the meaning of the different variables in more detail.
We use [[optdriver]] 7 to enter the EPH code while [[eph_task]] 4 activates
the computation of the full self-energy (real + imaginary parts).
The paths to the external files (**DDB**, **WFK**, **DVDB**) are specified
with the three variables:

- [[getddb_filepath]]
- [[getwfk_filepath]]
- [[getdvdb_filepath]].

This is an excerpt of the input file:

```sh
getddb_filepath "teph4zpr_1_DDB"

ddb_ngqpt 4 4 4  # The code expects to find in the DDB
                 # all the IBZ q-points corresponding to a 4x4x4 q-mesh

getdvdb_filepath "teph4zpr_2_DVDB"
getwfk_filepath "teph4zpr_3o_WFK"  # 4x4x4 k-mesh with 70 bands
```

The mesh for electrons ([[ngkpt]], [[nshiftk]] and [[shiftk]]) must
correspond to the one used for the input WFK file.
[[ddb_ngqpt]] is set to 4x4x4 as this is the $\qq$-mesh used in the DFPT part to generate the DDB and DVDB files,
but the integration in $\qq$-space is performed with the [[eph_ngqpt_fine]] mesh.
As [[eph_ngqpt_fine]] differs from [[ddb_ngqpt]], the code will automatically activate
the interpolation of the DFPT potentials as discussed in the [introduction to the EPH code](/tutorial/eph_intro).
The $\qq$-space integration is defined by [[eph_intmeth]] and [[zcut]].
<!--
Default is standard quadrature (naive sum over $\qq$-points with weights to account for multiplicity).
The linear tetrahedron method is also implemented but it is not very efficient.
-->

We can now have a look at the main output file:

{% dialog tests/tutorespfn/Refs/teph4zpr_4.abo %}

First of all, we find a section that summarizes the most important parameters:

```md
 Number of bands in e-ph self-energy sum: 30
 From bsum_start: 1 to bsum_stop: 30
 Symsigma: 1 Timrev: 1
 Imaginary shift in the denominator (zcut): 0.010 [eV]
 Method for q-space integration:  Standard quadrature
 Both Real and Imaginary part of Sigma will be computed.
 Number of frequencies along the real axis: 0 , Step: 0.000 [eV]
 Number of frequency in generalized Eliashberg functions: 0
 Number of temperatures: 1 From: 0.000000E+00 to 0.000000E+00 [K]
 Ab-initio q-mesh from DDB file: [4, 4, 4]
 Q-mesh used for self-energy integration [ngqpt]: [8, 8, 8]
 Number of q-points in the IBZ: 29
 asr: 1 chneut: 1
 dipdip: 1 symdynmat: 1
 Number of k-points for self-energy corrections: 1
 Including all final {mk+q} states inside energy window: [4.347 9.112 ] [eV]
 List of K-points for self-energy corrections:
   1     1  [ 0.0000E+00,  0.0000E+00,  0.0000E+00]   6    9
```

!!! note

    Note how the acoustic sum-rule ([[asr]]), the charge neutrality of the Born effective charges ([[chneut]]),
    the treatment of dipole-dipole interaction in the dynamical matrix ([[dipdip]]),
    are activated by default in v9.

Then we find another section related to MPI parallelism.
In this case we are running in sequential but the output will change if we run in parallel
(see also [[eph_np_pqbks]]).
The final message informs the user that the EPH code will either read the qpts from file 
(if the DVDB contains all of them, in case
[[eph_ngqpt_fine]] is not defined in the input) or interpolate the scattering potentials
from [[ddb_ngqpt]] to [[eph_ngqpt_fine]].

```md
 === MPI parallelism ===
P Allocating and summing bands from my_bsum_start: 1 up to my_bsum_stop: 30
P Number of CPUs for parallelism over perturbations: 1
P Number of perturbations treated by this CPU: 6
P Number of CPUs for parallelism over q-points: 1
P Number of q-points in the IBZ treated by this proc: 29 of 29
P Number of CPUs for parallelism over bands: 1
P Number of CPUs for parallelism over spins: 1
P Number of CPUs for parallelism over k-points: 1
P Number of k-point in Sigma_nk treated by this proc: 1 of 1
```

```md
 DVDB file contains all q-points in the IBZ --> Reading DFPT potentials from file.
```

or

```md
 Cannot find eph_ngqpt_fine q-points in DVDB --> Activating Fourier interpolation.
```

Finally, we have the section with the QP results for each spin, $\kk$-point and temperature,
followed by the value of the **direct gap** computed with the two approaches (linearized QP equation and OTMS):

```md
================================================================================
 Final results in eV.
 Notations:
     eKS: Kohn-Sham energy. eQP: quasi-particle energy.
     eQP - eKS: Difference between the QP and the KS energy.
     SE1(eKS): Real part of the self-energy computed at the KS energy, SE2 for imaginary part.
     Z(eKS): Renormalization factor.
     FAN: Real part of the Fan term at eKS. DW: Debye-Waller term.
     DeKS: KS energy difference between this band and band-1, DeQP same meaning but for eQP.
     OTMS: On-the-mass-shell approximation with eQP ~= eKS + Sigma(omega=eKS)
     TAU(eKS): Lifetime in femtoseconds computed at the KS energy.
     mu_e: Fermi level for given (T, nelect)


K-point: [ 0.0000E+00,  0.0000E+00,  0.0000E+00], T:    0.0 [K], mu_e:    7.721
   B    eKS     eQP    eQP-eKS   SE1(eKS)  SE2(eKS)  Z(eKS)  FAN(eKS)   DW      DeKS     DeQP
   6   4.490    4.563    0.073    0.089   -0.002    0.820   -0.301    0.390    0.000    0.000
   7   4.490    4.563    0.073    0.089   -0.002    0.820   -0.301    0.390    0.000    0.000
   8   4.490    4.563    0.073    0.089   -0.002    0.820   -0.301    0.390    0.000    0.000
   9   8.969    8.886   -0.083   -0.085   -0.000    0.981    0.117   -0.201    4.479    4.323

 KS gap:    4.479 (assuming bval:8 ==> bcond:9)
 QP gap:    4.323 (OTMS:    4.305)
 QP_gap - KS_gap:   -0.156 (OTMS:   -0.174)
```

The calculation has produced the following output files:

```sh
$ ls teph4zpr_4o_*

teph4zpr_4o_EBANDS.agr  teph4zpr_4o_PHBANDS.agr
teph4zpr_4o_PHDOS.nc    teph4zpr_4o_PHBST.nc    teph4zpr_4o_SIGEPH.nc
```

where:

- EBANDS.agr --> Electron bands in |xmgrace| format. See also [[prtebands]]
- PHBST.agr  --> Phonon bands in |xmgrace| format. See also [[prtphbands]]
                 [[ph_ndivsm]], [[ph_nqpath]], and [[ph_qpath]].
- PHDOS.nc   --> Phonon DOS in netcdf format (see [[prtphdos]] is given by [[ph_ngqpt]]).
- PHBST.nc   --> Phonon band structure in netcdf format
- SIGEPH.nc  --> Netcdf file with $\Sigma^{\text{e-ph}}$ results.

All the QP results are stored in the **SIGPEPH.nc** netcdf file for all $\kk$-points, spins and temperatures.
As usual, one can use |abiopen| with the `-p` option (`--print`) to print a summary to terminal:

```text
$ abiopen.py teph4zpr_4o_SIGEPH.nc -p

============================ SigmaEPh calculation ============================
Calculation type: Real + Imaginary part of SigmaEPh
Number of k-points in Sigma_{nk}: 1
sigma_ngkpt: [0 0 0], sigma_erange: [0. 0.]
Max bstart: 5, min bstop: 9
Initial ab-initio q-mesh:
	ngqpt: [4 4 4], with nqibz: 8
q-mesh for self-energy integration (eph_ngqpt_fine): [4 4 4]
k-mesh for electrons:
	mpdivs: [4 4 4] with shifts [0. 0. 0.] and kptopt: 1
Number of bands included in e-ph self-energy sum: 30
zcut: 0.00037 (Ha), 0.010 (eV)
Number of temperatures: 4, from 0.0 to 300.0 (K)
symsigma: 1
Has Eliashberg function: False
Has Spectral function: False

Printing QP results for 2 temperatures. Use --verbose to print all results.

KS, QP (Z factor) and on-the-mass-shell (OTMS) direct gaps in eV for T = 0.0 K:
  Spin  k-point                              KS_gap    QPZ_gap    QPZ - KS    OTMS_gap    OTMS - KS
------  ---------------------------------  --------  ---------  ----------  ----------  -----------
     0  [+0.000, +0.000, +0.000] $\Gamma$     4.479      4.323      -0.156       4.305       -0.174


KS, QP (Z factor) and on-the-mass-shell (OTMS) direct gaps in eV for T = 300.0 K:
  Spin  k-point                              KS_gap    QPZ_gap    QPZ - KS    OTMS_gap    OTMS - KS
------  ---------------------------------  --------  ---------  ----------  ----------  -----------
     0  [+0.000, +0.000, +0.000] $\Gamma$     4.479      4.288      -0.191       4.262       -0.217
```

where `QPZ` stands for the results obtained with the linearized QP equation and
`OTMS` are the results with the on-the-mass-shell approach.
We can also plot the most important results by just replacing `-p` with `-e` (`--expose`):

```sh
$ abiopen.py teph4zpr_4o_SIGEPH.nc -e
```

that produces:

![](eph4zpr_assets/qpgaps_4o.png)

On the left, we have the QP direct gap at $\Gamma$ ad a function of T while the OTMS results are shown in the right panel.
In both cases, the QP direct band gap **decreases with increasing temperature**
(remember that MgO is a direct gap semiconductor).
This is the so-called Varshni's effect that is observed in many (but not all) semiconductors.

The ZPR computed with the two approaches differ by ~26 meV but this is expected as
the linearized QP equations reduces to the OTMS only if the $Z$ factor is very close to one or if there is
some sort of fortuitous cancellation between the CBM/VBM corrections.
In our calculation, the Z factor for the VBM is 0.644 while for the CBM we obtain Z = 0.963.
<!--
On physical grounds, these values are reasonable as Z corresponds to the area under the QP peak
in the spectral function and values in [~0.7, 1] indicates a well-defined QP excitations.
-->
These values are reasonable, still it's not so uncommon to obtain **unphysical Z factors** in e-ph calculations 
i.e. values > 1, especially for states far from the band edge as the e-ph self-energy has a lot of structure 
in frequency-space and the linearized QP approach is not always justified.
For this reason, in the rest of the tutorial, **we will be focusing on the analysis of the OTMS results**.

!!! important

    To compute the imaginary part of $\Sigma^{\text{e-ph}}$ at the KS energy, we **strongly recommend** to use
    [[eph_task]] -4 as this option activates several important optimizations that are not possible
    when the full self-energy is wanted.
    Note, however, that [[eph_task]] -4 is not able to provide the full frequency dependence, only the
    value of the imaginary part at the KS eigenvalue.
    The computation of spectral functions and Eliashberg functions therefore requires [[eph_task]] +4.

## Convergence w.r.t. nband

At this point it should not be so difficult to write an input file to perform ZPR
calculations for different values of [[nband]] for a fixed $\qq$-mesh.
It is just a matter of adding the following set of variables to the previous input file:

```sh
ndtset 3
nband: 20
nband+ 20
```

An example is given in

{% dialog tests/tutorespfn/Input/teph4zpr_5.abi %}

Run the calculation, as usual, using

```sh
abinit teph4zpr_5.abi > teph4zpr_5.log 2> err &
```

Now we can extract the results from the output file

{% dialog tests/tutorespfn/Refs/teph4zpr_5.abo %}

or, alternatively, use the AbiPy |abicomp| script to post-process the results stored in the **SIGPEPH** files:

```sh
abicomp.py sigeph teph4zpr_5o_DS*_SIGEPH.nc -e
```

![](eph4zpr_assets/qpgaps_5o.png)

The figure on the left shows the convergence of the OTMS ZPR as a function of [[nband]].
The figure on the right, gives the correction to the KS band gas as a function of
$T$ obtained for different numbers of bands.
A similar behaviour is observed also for the linearized equation.

The results are quite disappointing in the sense that the QP gaps are far from being converged
and the convergence rate is even worse if we look at the QP energies (the same behaviour is observed in $GW$).
A more careful (and expensive) convergence study would reveal that 300 bands are needed.
This computation, although feasible, would be too costly for a tutorial and is therefore left as an extra exercise.
In the next section, we will see that the EPH code provides a much more efficient algorithm
to accelerate the convergence.

??? note "Exercise"

    Change the input file to use [[mixprec]] 1 and [[boxcutmin]] 1.1.
    Rerun the calculation and compare with the previous results.
    Do you see significant differences? What about the wall-time and the memory allocated for $W(\rr, \RR)$
    Hint: use

    ```sh
    grep log <<< MEM
    ```

    to extract the lines where the most imporant memory allocation are reported.

## How to reduce the number of bands with the Sternheimer method

To activate the Sternheimer approach, we need to set [[eph_stern]] to 1
and use [[getpot_filepath]] to specify the external file with the GS KS potential.
This file is produced at the end of the GS calculations provided [[prtpot]] is set to 1 in the input file.

!!! important

    The Sternheimer equation is solved non-self-consistently using max [[nline]] NSCF iterations
    and the solver stops when the first-order wavefunction is converged within [[tolwfr]].
    Default values for these two variables are provided if not specified by the user in the input file.
    **The code aborts if the solver cannot converge**.
    This may happen if the first [[nband]] states are not well separated from the remaining high-energy states.
    Increasing [[nband]] should solve the problem.

<!--
From the user's point of view, the Sternheimer method requires to add
the following section to our initial input:

```sh
eph_stern 1
getpot_filepath "MgO_eph_zpr/flow_zpr_mgo/w0/t0/outdata/out_POT.nc"

# nline 100 tolwfr 1e-16 # Default values
```
-->

An example of input file is provided in:

{% dialog tests/tutorespfn/Input/teph4zpr_6.abi %}

and can be run with:

```sh
abinit teph4zpr_6.abi > teph4zpr_6.log 2> err &
```

Four calculations are performed with different values of [[nband]]:

```sh
ndtset 4
nband: 10
nband+ 10
```

in order to monitor the convergence of the QP corrections.

To analyze the convergence behavior, we can extract the results from the main output file

{% dialog tests/tutorespfn/Refs/teph4zpr_6.abo %}


or, alternatively, we can pass the list of SIGEPH files to the |abicomp| script and use the
`sigpeh` command:

```sh
abicomp.py sigeph teph4zpr_6o_DS*_SIGEPH.nc -e
```

![](eph4zpr_assets/qpgaps_6o.png)

Now the convergence is much better. **Note the different scale on the y-axis**.
<!--
and the ZPR is converged with 1 meV for nband ??
This is the value one should obtain when summing all the bands up to maximum number of plane-waves [[mpw]].
-->

!!! important

    The big advange of the Sternheimer method is that we don't need to compute WFK files with many
    empty bands in order to converge the real part of the self-energy.
    This means that one can use the computing power to densify the $\kk$-mesh while keeping the number of
    empty states at a reasonable level.
    Producing a WFK file with 1000 bands and a $100^3$ $\kk$-mesh is indeed way more expensive than
    performing the same computation with, let's say, 50 bands.

    Note, however, that the cost of the Sternheimer method quickly increases with [[nband]]
    due to the orthogonalization process. This means that a ZPR calculation with 300 bands (occ + empty)
    **without** the Sternheimer method is much faster than the same computation done with [[eph_stern]] 1.
    As a matter of fact, we use the Sternheimer method so that we don't need 300 bands to converge the results.

??? note "Exercise"

    Generate a new WFK with [[nband]] ~ [[mpw]] and run a ZPR calculation without the Sternheimer method.
    Compare the sum-over-states results with the Sternheimer ones.
    Why it is not a good idea to use nband >= mpw?

## Convergence of the ZPR w.r.t. the q-mesh

Now we fix this value of *nband* to 20 and perform a convergence study for the $\qq$-sampling.
Unfortunately, we won't be able to fully convergence the results but, at least,
you will get an idea on how to perform this kind of convergence study.

In the input *teph4zpr_3.abi*, we have computed WFK files on different $\kk$-meshes
and a relatively small number of empty states (25 - 5 = 20).
We can finally use these WFK files to perform ZPR calculations by just adding:

```sh
ndtset 3
nband 20

ngkpt1 4 4 4    eph_ngqpt_fine1 4 4 4    getwfk_filepath1 "teph4zpr_3o_DS1_WFK"
ngkpt2 8 8 8    eph_ngqpt_fine2 8 8 8    getwfk_filepath2 "teph4zpr_3o_DS2_WFK"
ngkpt3 12 12 12 eph_ngqpt_fine3 12 12 12 getwfk_filepath3 "teph4zpr_3o_DS3_WFK"

eph_stern 1
getpot_filepath "MgO_eph_zpr/flow_zpr_mgo/w0/t0/outdata/out_POT.nc"
```

{% dialog tests/tutorespfn/Input/teph4zpr_7.abi %}

Run the calculation, as usual, with:

```sh
abinit teph4zpr_7.abi > teph4zpr_7.log 2> err &
```

The output file is reported here for your convenience:

{% dialog tests/tutorespfn/Refs/teph4zpr_7.abo %}

Now use:

```sh
abicomp.py sigeph teph4zpr_7o_DS*_SIGEPH.nc -e
```

to analyze the convergence with the $\qq$-mesh:

![](eph4zpr_assets/qpgaps_7o.png)

The convergence of the ZPR is shown in the left panel while the $T$-dependent band gaps obtained
with different $\qq$-meshes are reported in the right panel.

Obviously we are still far from convergence but a 12x12x12 $\qq$-mesh is the best we can afford in a tutorial.
It is clear that one should test denser $\qq$-meshes and, last but not least, monitor the behaviour for [[zcut]] --> 0.
These additional convergence tests cannot be covered in this lesson and they are left as exercises.

## How to compute the spectral function

To compute the spectral function, we need to specify the number of frequencies via [[nfreqsp]].
The code will compute $A_\nk(\ww)$ on a linear frequency mesh centered on the KS eigenvalue
that spans the interval $[\ee_\nk - \Delta, \ee_\nk + \Delta]$ with $\Delta$ given by [[freqspmax]].
An odd number of frequency points is enforced by the code.

In a nutshell, one should add e.g.:

```sh
nfreqsp 301
freqspmax 1.0 eV
```

An example of input file is available here:

{% dialog tests/tutorespfn/Input/teph4zpr_8.abi %}

Execute it with:

```sh
abinit teph4zpr_8.abi > teph4zpr_8.log 2> err &
```

To plot the spectral function $A_\nk(\ww)$ in an easy way, use AbiPy to extract the data from the netcdf file.
We can do it in two different ways:

- using a small python script that calls the AbiPy API
- using |abiopen| to interact with the netcdf file inside |ipython|

<!--
The advantage of the second approach is that you can interact with the python object in an interactive environment.
The first approach is more powerful if you need a programmatic API to automate operations.
-->

The python script is reported here:

??? note "Script to plot $A_\nk(\ww)$"

    ```python
    #!/usr/bin/env python

    import sys
    from abipy.abilab import abiopen

    # Get file name from command line and open the file
    filepath = sys.argv[1]
    abifile = abiopen(filepath)

    # Plot Sigma(omega), A(omega) and QP solution
    # Adjust the parameters according to your system as AbiPy cannot (yet) read your mind
    abifile.plot_qpsolution_skb(spin=0, kpoint=[0, 0, 0], band=7)
    ```

Alternatively, one can use

```sh
abiopen.py teph4zpr_8o_DS1_SIGEPH.nc
```

to open the SIGEPH.nc file inside ipython.
Then, inside the ipython terminal, issue:

```ipython
%matplotlib

abifile.plot_qpsolution_skb(spin=0, kpoint=[0, 0, 0], band=7)
```

to plot $\Sigma_\nk(\ww)$ and $A_\nk(\ww)$ at the $\Gamma$ point for band index 7 (conduction band maximum).
Note that in python indices start from 0 whereas ABINIT uses Fortran conventions with indices starting from 1.
**In a nutshell, one should substract 1 from the Fortran band index when calling AbiPy functions.**

![](eph4zpr_assets/qp_solution.png)

Some comments are in order here.

(i) The **intersection** between the blue solid and dashed lines
gives the graphical solution of the **non-linear QP equation**:

$$ \ee^\QP_\nk = \ee^\KS_\nk + \Sigma_\nk^{\text{e-ph}}(\ee^\QP_\nk) $$

restricted to the real axis whereas the blue circle represents the solution obtained with
the **linearized QP equation** with the renormalization factor $Z$.
For this particular state, the real part of $\Sigma$ is almost linear around $\ee^\KS_\nk$ and
the linearized solution is **very close** to the non-linear one.
There may be cases, however, in which the real part is highly non-linear and the two approaches
will give different answers.

(ii) The QP solution is located inside the KS gap where $\Im\Sigma$ is zero
(well, strictly speaking, it is very small yet finite because
we are using a finite [[zcut]] in the calculation).
As a net result, $A_\nk(\ww)$ presents a **sharp peak in correspondence of the QP energy**.
Note also that our computed $A_\nk(\ww)$ does not integrate to 1 because the number of points [[nfreqsp]]
is too small and part of the weight is lost in the numerical integration.
This can be easily fixed by increasing the resolution of the frequency mesh.

(iii) For this particular $\nk$ state, $\ww - \ee^0$ has a single intersection with $\Re \Sigma_\nk(\ww)$.
However, it is possible to have configurations with multiple intersections that may lead to
some background and/or additional satellites in $A_\nk(\ww)$
provided the imaginary part of $\Sigma_\nk(\ww)$ is sufficiently small in that energy range.

(iiii) The vertical red line in the second figure gives the on-shell QP energy.
As already mentioned previously, the OTMS energy derives from a static formalism
that reduces to the linearized QP result only if $Z = 1$ so it is not surprising
to see that the QP peak differs from the OTMS energy.
Still, this does not mean that the OTMS is wrong.
Actually, it is more puzzling the fact that the background
in $A_\nk(\ww)$is strongly suppressed and no phonon satellite is clearly visible.
Obviously, our entire discussion is based on completely underconverged results
but the more careful investigation reported in [[cite:Nery2018]] indicates that
an accurate description of dynamical effects requires the (approximate) inclusion
of additional Feynmann diagrams through the cumulant expansion and that
**the OTMS approximation gives QP energies that are much closer to the QP peak
obtained with the cumulant method**.

This (lengthy) discussion was somehow needed to justify the reason why we have been focusing
on the OTMS results in most of this tutorial.
Now we focus on more technical aspects and, in particular, on how to **compare spectral functions
and self-energies obtained with different settings**.

Remember that in *teph4zpr_8.abi* we computed $A_\nk(\ww)$ with/without the Sternheimer method.
So the question is "how can we compare the two calculations and what can we learn from this analysis?"

To compare the results obtained with/wo the Sternheimer, use |abicomp|

```sh
abicomp.py sigeph teph4zpr_8o_DS*_SIGEPH.nc
```

and then, inside *ipython*, type:

```ipython
%matplotlib

robot.plot_selfenergy_conv(spin=0, kpoint=0, band=7)
```

to produce the figure below with the real part, the imaginary part of the self-energy and the
spectral function obtained with the different approaches:

![](eph4zpr_assets/Aw_empty_vs_stern.png)

Note the following:

(i) The imaginary part of $\Sigma_\nk(\ww)$ is not affected by the Sternheimer method as long
    as we explicitly include enough [[nband]] bands around the $\nk$ state to account
    for phonon absorption/emission with the full dynamical self-energy.

(ii) The real part of $\Sigma_\nk(\ww)$ obtained with the two methods differ but
     the results should be interpreted with a critical eye.
     The Sternheimer method, indeed, is designed to accelerate the convergence of
     $\Sigma_\nk(\ee^\KS_\nk)$ w.r.t. [[nband]] but cannot exactly reproduce the behaviour of
     $\Sigma_\nk(\ww)$ at large frequencies since it is a static approximation.
     In other words, once the two approaches are properly converged one should see
     that the real part of the two self-energies agree around the bare KS eigenvalue
     and that the two curves stars to deviate at large $\ww$.
     This test is left as an additional excercise for volunteers.

For additional examples for Diamond, see this
[jupyter notebook](https://nbviewer.jupyter.org/github/abinit/abitutorials/blob/master/abitutorials/sigeph/lesson_sigeph.ipynb)

## MPI parallelism and memory requirements

There is an important difference with respect to [[eph_task]] -4 (computation of the imaginary part)
that is worth discussing in more detail.
When computing the imaginary part at the KS energy for transport properties,
the EPH code is able to filter both $\kk$- and $\qq$-points so that only the relevant states
around the band edge(s) are stored in memory.
Unfortunately, in the case of full self-energy calculations, this filtering algorithm
is not possible and each MPI process needs to store all the $\kk$-wavevectors the IBZ
to be able to compute the e-ph matrix elements connecting $\kk$ to $\kq$.
In other words, **the wavefunctions in the IBZ are not MPI-distributed** and this leads to a significant
increase in the memory requirements, especially for dense meshes and/or large [[nband]].
Fortunately, **the code is able to distribute bands** among the MPI processes in the band communicator
hence the memory required for the wavefunctions will scale as [[nband]] / **np_band** where **np_band**
is the number of MPI processes in the band communicator (see [[eph_np_pqbks]]).

For ZPR calculations, the priorities are as follows:

1. Use enough *np_band* MPI processes to **decrease the memory for the wavefunctions**.
   Ideally, *np_band* should divide [[nband]] to distribute the work equally.
   The maximum number of MPI procs that can used for this level is [[nband]] but
   let's not exaggerate. Keep some procs for the other MPI levels that are usually more efficient in
   terms of wall-time.
<!--
   Note that the band parallelism is beneficial also when the Sternheimer method is used
   as the solver will operate on MPI-distributed bands.
   Perhaps the parallel efficiency won't be perfect but the memory for the wavefunctions will continue to scale.
-->

2. Once the memory for the wavefunctions reaches a reasonable level, activate the parallelism
   over perturbations to decrease the memory for $W(\rr, \RR, 3\times\text{natom})$.
   For better efficiency, *eph_nperts* should divide 3 * [[natom]].
   As explained in the [introduction page for the EPH code](/tutorial/eph_intro), this MPI level
   allows one to reduce the memory allocated for $W(\rr, \RR, 3\times\text{natom})$ so the number of procs
   in this communicator should divide 3 * [[natom]].

3. If the memory for the wavefunctions and $W$ is under control,
   you may want to activate the $\qq$-point parallelism to speedup the calculation.
   The memory requirements will not decrease, though.

4. Finally, use the $\kk$-point parallelism if there are enough CPUs available to boost the calculation.
   Obviously, this kind of parallelism makes sense if you are computing QP corrections for several $\kk$ points
   and the number of *np_kpt* MPI processes should be adjusted accordingly.
   Keep in mind, however, that each $\kk$-point has a different computational cost so load imbalance is expected.

!!! important

    Finally, remember that setting [[boxcutmin]] to a value smaller than 2 (e.g. 1.1)
    leads to a significant decrease in the memory required to store $W$
    while [[mixprec]] = 1 allows one to decrease the computational cost of the FFTs.

<!--
## Estimating the ZPR with a generalized Fr\"ohlich model

Last but not least, one can estimate the correction to the ZPR in polar materials
using a generalized Fr\"ohlich model based on *ab initio* effective masses computed with DFPT [[cite:Laflamme2016]]
The formalism is detailed in XXX.
An example of input file is available in [[test:v7_88]].
-->


## Eliashberg function

The Fan-Migdal self-energy can be rewritten in terms of the spectral representation:

\begin{equation} 
\Sigma^\FM_{n\kk}(\ww) = 
\int \dd\ee\dd\ww  \left [
\frac{n(\ww') + f(\ee)}{\ww - \ee  + \ww' + i \eta} +
\frac{n(\ww') + 1 - f(\ee)}{\omega - \ee  - \ww' + i \eta} 
\right ] 
\alpha^2 F_\nk(\ee,\ww')
\end{equation}

where we have introduced the real, positive and T-independent Eliashberg function

\begin{equation}
\alpha^2 F_\nk(\ee,\ww') =
\sum_{m,\nu} \int_\BZ \frac{d\qq}{\Omega_\BZ} |\gkq|^2
\delta(\ee - \ee_{m\kq})\delta(\ww - \wqnu).
\end{equation}

The computation of $\alpha^2 F_\nk$ is activated by setting [[prteliash]] to 3.
The frequency mesh for phonons is defined by [[ph_wstep]], [[ph_smear]]
The frequency mesh for electrons is defined by [[dosdeltae]], [[tsmear]]

In the adiabatic approximation the phonon frequencies in the denominator of the Fan-Migdal term are neglected and 
the FM term simplifies to:

\begin{equation}
\Sigma^{a-\FM}_{n\kk}(\ee_\nk) =
\sum_{m\nu} \int_\BZ \frac{d\qq}{\Omega_\BZ} 
\dfrac{(2 n_\qnu + 1) |\gkq|^2} {\ee_\nk - \emkq + i \eta}
\label{eq:adiabatic_fan_selfen}
\end{equation}

The adiabatic ZPR can also be expressed as:

$$
\int \dd\ww (2 n(\ww) + 1) F_2(\ww)
$$

where $F_2^\nk(\ww)$ is given by:

$$
F_2^\nk(\ww) = 
\sum_{m\nu} \int_\BZ \frac{d\qq}{\Omega_\BZ} (|\gkq|^2 - g_{mn\nu}^{2,\DW}(\kk,\qq)) 
\dfrac{\delta(\ww - \wqnu)}{\ee_\nk - \ee_{m\kq}}
$$

<!--
-->
---
authors: SS, XG, YG, NAP, FG
---

# Tutorial on optical properties

## Frequency-dependent linear and second-order nonlinear optical response.

This tutorial aims at showing how to get the following physical properties, for semiconductors:

  * Frequency-dependent linear dielectric tensor
  * Frequency-dependent second-order nonlinear susceptibility tensor
  * Frequency-dependent electro-optical susceptibility tensor

in the simple *Random-Phase Approximation* or *Sum-over-states* approach.
This tutorial will help you to understand and make use of *optic*.
Before starting, you should first have some theoretical background.
We strongly suggest that you first read the first two sections of the [[help:optic]].

This tutorial should take about 1 hour.

[TUTORIAL_README]

## Computing the momentum matrix elements

*Before beginning, you might consider working in a different subdirectory.
Why not create Work_optic?*

We also need to copy *toptic_1.abi* from
*$ABI_TESTS/tutorial/Input* to *Work_optic*.

```sh
cd $ABI_TESTS/tutorespfn/Input
mkdir Work_optic
cd Work_optic
cp ../toptic_1.abi .
```

Now, you are ready to run Abinit and prepare the files needed for Optic.
Issue:

    abinit toptic_1.abi > log 2> err

We now examine the input file.

{% dialog tests/tutorespfn/Input/toptic_1.abi %}

The computation concerns a crystal of GaAs, in the zinc-blende structure (2 atoms per primitive cell).
It has six datasets.

The first dataset is a quite standard self-consistent determination of the ground state for a fixed geometry.
Only the occupied bands are treated. The density is output and used in later datasets.

The second dataset is a non-self-consistent calculation, where the number of bands has been increased to include unoccupied states.
The k points are restricted to the Irreducible Brillouin Zone.

The third dataset uses the result of the second one to produce the
wavefunctions for all the bands, for the full Brillouin Zone
(this step could be skipped, but is included for later CPU time saving).
If only the linear optical response is computed, then time-reversal symmetry can be used, and the computation
might be restricted to the half Brillouin zone ([[kptopt]]=2).

The fourth, fifth, and sixth datasets correspond to the computation of the *ddk* matrix elements,
that is, matrix elements of the $\partial H / \partial k$ operators where $H$ is the Hamiltonian.
Note that the number of bands is the same as for datasets 2 and 3.
Note also that these are non-self-consistent calculations, moreover, restricted to [[nstep]] = 1 and [[nline]] = 0.
Indeed, only the matrix elements between explicitly computed (unperturbed) states are required.
This also is why [[prtwf]]=3 is used.
Using a larger [[nstep]] would lead to a full computation of the derivative of the wavefunction with respect to
the wavevector, while in Optic, only the matrix elements between unperturbed states are needed.
Thus a value of [[nstep]] larger than one would not only lead to erroneous matrix elements, but would be a waste of time.

In order to have a sufficiently fast tutorial, the k point sampling was chosen to be extremely dense.
Instead of a $4\times 4\times 4$
FCC lattice (256 k points), it should be something like $28\times 28\times 28$ FCC (about 100000 k points).
Also, the cut-off energy (2 Ha) is too small. As usual, convergence studies are the responsibility of the user.
Moreover, we emphasize that in general the results of a sum-over-states approach, as is used in Optic,
typically converges quite slowly with the k point mesh. Thus it is of uttermost importance to
test convergence carefully.

The run takes less than one minute on a 2.8 GHz PC. The files *toptic_1o_DS3_WFK*,
*toptic_1o_DS4_1WF7*, *toptic_1o_DS5_1WF8* and *toptic_1o_DS6_1WF9* are the four
files requested for the Optic run.
The first file contains the wavefunctions for the filled and empty states in the entire
Brillouin zone, while the latter three contain the matrix elements of the
$\partial/\partial k$ operators, one file for each Cartesian direction.

Real preparation runs (with adequate k point sampling and cut-off energy) can
last several hours (or even days) on such a PC.

## Computing the linear and nonlinear optical response

The next step is to compute the linear and nonlinear optical response: once the
momentum matrix elements are available, you are ready to determine the optical
response (up to second order in the current implementation) for the material under study.

First, read the [[help:optic#input|section 3]] of the Optic help file.

Copy the *toptic_2.abi* input file from *$ABI_TESTS/tutorial/Input* to *Work_optic*:

```sh
cp ../toptic_2.abi .
```

The *toptic_2.abi* is your input file. You should edit it and read it carefully. For
help on various input parameters in this file, please see the [[help:optic]].

{% dialog tests/tutorespfn/Input/toptic_2.abi %}

When you have read the input file, you can run the code, as usual, using the
following command (assuming optic is in $PATH - copy the
executable in the current directory if needed):

    optic toptic_2.abi > log 2> err &

It will take a few seconds to run. You have produced numerous output files.
Now, you can examine some of these output files.

The headers contains information about the calculation.
See the [[help:optic#output|section 4]] of the Optic help file.
These files can be plotted in |xmgrace| or |gnuplot|.
If you do not have xmgrace installed on your computer, please get it from the Web, and install it,
or alternatively, use your preferred plotting package.

We will first have a look at the linear optic file.

    xmgrace toptic_2_0001_0001-linopt.out

This file contains the *xx* component of the dielectric tensor, and includes,
as a function of energy, the magnitude, real, and imaginary parts of the tensor element.
On the graph, you should see three curves. One of them is positive, and always
larger than the two others. It is the modulus of the dielectric function.
Another one is also always positive, it is the imaginary part of the
dielectric function. The last one is the real part.
There are a large number of peaks. This is at variance with the experimental
spectra, which are much smoother. The origin of this discrepancy is to be found
in the very sparse k point sampling that we used in order to be able to perform
the tutorial quickly.
In the next section, we will improve this sampling, and start a convergence study.

Concerning the non-linear optics, the graphs for the *xyz* components are also
quite bad, with many isolated (but broadened) peaks. However, the *yyy* ones are
*perfect*. Indeed, they vanish due to symmetry reasons!
Visualize the imaginary part with:

    xmgrace toptic_2_0002_0002_0002-ChiTotIm.out

and the Real part with:

    xmgrace toptic_2_0002_0002_0002-ChiTotRe.out


!!! tip

    If |AbiPy| is installed on your machine, you can use the |abiopen| script
    with the `--expose` option to visualize the results stored in the OPTIC.nc file:

        abiopen.py toptic_2_OPTIC.nc --expose -sns=paper

    ![](optic_assets/abiopen_toptic_2.png)


This would be a good time to review [[help:optic#troubleshooting|section 5]] of the optic help file.

For comparison, we have included in the tutorial directory of the ABINIT package (ask your administrator to have access 
to the full ABINIT package if you do not have your own installation), three files that have been
obtained with a much better k point sampling (still with a low cut-off energy
and a number of bands that should be larger). You can visualize them as follows:

    xmgrace $ABI_HOME/doc/tutorial/optic_assets/toptic_ref_0001_0001-linopt.out

for the linear optics, obtained with a 28x28x28 grid (keeping everything else fixed), and

    xmgrace $ABI_HOME/doc/tutorial/optic_assets/toptic_ref_0001_0002_0003-ChiTotIm.out

as well as

    xmgrace $ABI_HOME/doc/tutorial/optic_assets/toptic_ref_0001_0002_0003-ChiTotRe.out

for the non-linear optics, obtained with a 18x18x18 grid (keeping everything else fixed).

Concerning the linear spectrum, we will now compare this (underconverged)
result *toptic_ref_0001_0001-linopt.out* with experimental data and converged
theoretical results.

The book by Cohen M.L. and Chelikowsky [[cite:Cohen1988]] presents a
comparison of experimental data with the empirical pseudopotential method
spectrum. If you do not have access to this book, you can see an experimental
spectrum in [[cite:Philipp1963]],
and a theoretical spectrum in [[cite:Huang1993]], as well as other sources.

We discuss first the imaginary spectrum. Prominent experimental features of
this spectrum are two peaks located close to 3 eV and 5 eV, both with the same
approximate height. The spectrum is zero below about 1.5 eV (the direct band
gap), and decreases with some wiggles beyond 5.5 eV. Converged theoretical
spectra also show two peaks at about the same location, although their heights
are markedly different: about 10 for the first one (at 3 eV), and 25 for the
second one (at 5 eV). Other features are rather similar to the experimental
ones. In the linear optic spectrum of *toptic_ref_0001_0001-linopt.out*, we note
that there is a shoulder at around 3 eV, and a peak at 4.2 eV, with respective
approximate heights of 7 and 25. Some comments are in order:

  * The main difference between experimental and converged theoretical spectra is due
    to the presence of excitons (electron-hole bound states), not treated at all in
    this rather elementary theoretical approach: excitons transfer some oscillator strength
    from the second peak (at 5 eV) to the first one (at 3 eV). Going beyond the Sum-Over-State approach,
    but still keeping the independent-electron approximation, e.g., in the framework of the TDDFT (adiabatic LDA)
    will not correct this problem. One needs to use the Bethe-Salpeter approximation, or to rely on
    *fancy* exchange-correlation kernels, to produce an optical spectrum in qualitative agreement with the experimental data.
    Still, trends should be correct (e.g. change of the peak positions with pressure,
    comparison between different semiconductors, etc.).

  * In many early theoretical spectra (including the ones in [[cite:Cohen1988]]),
    the agreement between the theoretical and experimental band gap is artificially good.
    In straight DFT, one cannot avoid the band gap problem. However, it is possible to
    add an artificial "[[scissor@optic|scissor shift]]", to make the theoretical band gap match the experimental one.

  * Our theoretical spectrum presents additional deficiencies with respect to the other ones,
    mostly due to a still too coarse sampling of the k space (there are too many wiggles in the spectrum),
    and to a rather inaccurate band structure (the cut-off energy was really very low, so that
    the first peak only appears as a shoulder to the second peak).

The real part of the spectrum is related by the Kramers-Kronig relation to the imaginary part.
We note that the deficiencies of the imaginary part of the
spectrum translate to the real part: the first peak is too low, and the
second peak too high, while the spectrum correctly changes sign around 5 eV,
and stays negative below 8 eV.

In our simulation, more empty states are needed
to obtain a better behaviour. Also, the limiting low-frequency value is only
4.3, while it should be on the order of 10. This can be corrected by
increasing the cut-off energy, the k point sampling and the number of unoccupied states.

Similar considerations apply to the non-linear spectra.

## Faster computation of the imaginary part of the linear optical response

In the case of the imaginary part of the linear optical response, there are
several points that make the calculation easier:

  * The time-reversal symmetry can be used to decrease the number of k points
    by a factor of two (this is also true for the computation of the real spectrum);

  * The number of unoccupied bands can be reduced to the strict minimum needed
    to span the target range of frequencies.

We will focus on the energy range from 0 eV to 8 eV, for which only 5 unoccupied bands are needed.

Copy the input file *toptic_3.abi* in *Work_optic*:

    cp ../toptic_3.abi .

{% dialog tests/tutorespfn/Input/toptic_3.abi %}

Issue:

    abinit toptic_3.abi > log 2> err &

Now, examine the file *toptic_3.abi*. There are two important changes with respect to the file *toptic_1.abi*:

  * the number of unoccupied bands has been reduced, so that the total number of bands is 9 instead of 20
  * when applicable, the value of [[kptopt]] 3 in our previous simulation has been changed to 2,
    in order to take advantage of the time-reversal symmetry

When the run is finished (it is only 8 secs on a 2.8 GHz PC), you can process
the WFK files and obtain the linear optic spectra.
Copy the *toptic_4.abi* input file in *Work_optic*:

    cp ../toptic_4.abi .

{% dialog tests/tutorespfn/Input/toptic_4.abi %}

Examine *toptic_4.abi* file: only the linear optic spectra will be built.

When you have read the input file, you can run the code, as usual using the following command

    optic toptic_4.abi > log 2> err &

Then, you can visualize the files *toptic_2_0001_0001-linopt.out* and
*toptic_4_0001_0001-linopt.out* using xmgrace and compare them. The spectra
looks completely identical. However, a careful look at these files, by editing
them, show that indeed, the imaginary part is very similar:

     #calculated the component:  1  1  of dielectric function
     #broadening:    0.000000E+00    2.000000E-03
     #scissors shift:    0.000000E+00
     #energy window:    3.897388E+01eV    1.432264E+00Ha

     # Energy(eV)         Im(eps(w))
        8.163415E-03    1.463528E-04
        1.632683E-02    2.927094E-04
        2.449025E-02    4.390740E-04
        3.265366E-02    5.854504E-04
	  ...

But the real parts differ slightly (this is seen at lines 1007 and beyond):

     # Energy(eV)         Re(eps(w))
        8.163415E-03    6.623599E+00
        1.632683E-02    6.623632E+00
        2.449025E-02    6.623686E+00
        3.265366E-02    6.623763E+00
      ...

for *toptic_2_0001_0001-linopt.out* and

     # Energy(eV)         Re(eps(w))
        8.163415E-03    6.518576E+00
        1.632683E-02    6.518608E+00
        2.449025E-02    6.518663E+00
        3.265366E-02    6.518740E+00


for *toptic_4_0001_0001-linopt.out*.
This small difference is due to the number
of bands ([[nband]] 20 for *toptic_2_0001_0001-linopt.out* and [[nband]] 9 for *toptic_4_0001_0001-linopt.out*).

Then, you can increase the number of k points, and watch the change in the
imaginary part of the spectrum. There will be more and more peaks, until they
merge, and start to form a smooth profile (still not completely smooth even with $28\times 28\times 28$).
For your information, we give some timings of the corresponding Abinit run for a 2.8 GHz PC:

    k-point grid     CPU time
    4 x 4 x 4            8 secs
    6 x 6 x 6            20 secs
    8 x 8 x 8            43 secs
    10 x 10 x 10         80 secs
    12 x 12 x 12         138 secs
    16 x 16 x 16         338 secs
    20 x 20 x 20         702 secs
    24 x 24 x 24         1335 secs
    28 x 28 x 28         2633 secs

For grids on the order of $16\times 16\times 16$, the treatment by optics also takes several
minutes, due to IO (30 minutes for the $28\times 28\times 28$ grid).
You might note how the first peak slowly develop with increasing number of k
points but nevertheless stays much smaller than the converged one, and
even smaller than the experimental one.

## Computing the linear electro-optical susceptibility
Calculations of the linear electro-optical susceptibility follows the same inital calculations
as those described in the first two sections of this tutorial. 
To calculate the coefficients of the linear electro-optical susceptibility
one needs to modify the optic input file with two additional keywords.

Copy the *toptic_5.abi* input file in *Work_optic*:

    cp ../toptic_5.abi .

{% dialog tests/tutorespfn/Input/toptic_5.abi %}

For *toptic_5.abi*, only the linear electro-optic susceptibility will be calculated.

When you have read the input file, you can run the code, as usual using the following command

    optic toptic_5.abi > log 2> err &

The calculation should run in a few seconds on a modern PC.

The resulting calculation produces a number of files ending in *ChiEO* and are related to different parts of the linear electro-optical tensor:

   * *ChiEOAbs.out* gives the absolute value of the linear electro-optical susceptibility
   * *ChiEOIm.out* gives the imaginary components of the calculated linear electro-optical susceptibility
   * *ChiEORe.out* gives the real components of the calculated linear electro-optical susceptibility
   * *ChiEOTotIm.out* gives the total imaginary part of the calculated linear electro-optical susceptibility
   * *ChiEOTotRe.out* gives the total real part of the calculated linear electro-optical susceptibility

Generally, the low energy (or frequency) range of the linear electro-optical susceptibility
is linear and of experimental importance. Here, low energy means energies much less than the band gap energy.
---
title: Parallelization of ground state
authors: FB, JB, MT
---

# Parallelization of ground state calculations

## Explore the *k-points/plane waves/bands* parallelization

This tutorial discusses how to perform ground-state calculations on hundreds/thousands
of computing unit (CPUs) cores using ABINIT.

You will learn how to use some keywords related to the **KGB** parallelization scheme where
**K** stands for *k-point*, **G** refers to the wavevector of a *planewave*, and **B** stands for *Band*.
It is possible to use ABINIT with other levels of parallelism but this is not the focus of this tutorial.
You will learn how to speedup your calculations and how to improve their convergence rate.

This tutorial should take about 1.5 hour and requires access to 64 CPU core parallel computer except for the last section which requires 256 CPU cores.

You are supposed to know already some basics of ABINIT.
Some useful references: [[cite:Levitt2015]], [[cite:Bottin2008]], [[cite:Knyazev2001]]

[TUTORIAL_README]

## Introduction

*Before continuing you might work in a different subdirectory as for the other
tutorials. Why not work_paral_kgb?*

!!! important 

    In what follows, the names of files are mentioned as if you were in this subdirectory.
    All the input files can be found in the `\$ABI_TESTS/tutoparal/Input` directory.
    You can compare your results with reference output files located in `\$ABI_TESTS/tutoparal/Refs`.

    In the following, when "run ABINIT over _nn_ CPU cores" appears, you have to use
    a specific command line according to the operating system and architecture of
    the computer you are using. This can be for instance: `mpirun -n nn abinit input.abi`
    or the use of a specific submission file.


When the size of the system increases up to 100 or 1000 atoms, it is usually
impossible to perform *ab initio* calculations with a single computing core.
This is because the basis sets used to solve the problem (plane waves, bands, ...) increase
&mdash; linearly, as the square, or even exponentially &mdash;.
The computational resources are limited by two factors:

* The memory, *i.e*. the amount of data stored in RAM,
* The computing efficiency, with specific bottlenecks.

Therefore, it is mandatory to adopt a parallelization strategy:

1. Distribute or share the data across a large number of computing nodes,
2. Parallelize the time consuming routines.

In this tutorial, we will discuss:

* How to improve performance by using a large number of computing units (CPU cores),
* How to decrease the computational time for a given number of CPU cores by:

    1. Reducing the time needed to perform one electronic iteration (improve efficiency)
    2. Reducing the number of electronic iterations (improve convergence)

The tests are performed on a 107 gold atom system.
In this tutorial the plane-wave
cutoff energy is strongly reduced, for practical reasons.

## A simple way to begin: automatic distributed parallelism

The easiest way to activate the KGB parallelization in ABINIT is to
add just one input variable in the input file, [[paral_kgb]], which controls
everything concerning the KGB parallelization, including the choice of the iterative
eigensolver ([[wfoptalg]] **= 1, 4, 14, 114**) and the use of a parallel 3dim-FFT.
Then you have to choose between 2 strategies:

* Activate the &mdash; not so good &mdash; flag [[autoparal]]**=1** (automatic parallelization)
and use the associated [[max_ncpus]] variable (maximum number of CPU cores you want),

or

* Manually define the number of processes associated to each level of parallelism:
  [[npkpt]] (number of processes for k points),
  [[npband]] (number of processes for bands),
  [[npfft]] (number of processes for plane-waves/FFT).

OK, let's start!

Copy the `tgspw_01.abi` file the tutorial directory into your working directory.

{% dialog tests/tutoparal/Input/tgspw_01.abi %}

Then run ABINIT on 1 CPU core (using 1 MPI process and 1 *OpenMP* thread).

ABINIT should stop without starting a calculation (don't pay attention to the error message).
At the end of the output file `tgspw_01.abo`, you will see:

```md
 Searching for all possible proc distributions for this input with #CPUs<=64:

 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 |       npkpt|       npfft|      npband|      bandpp|  #MPI(proc)|    WEIGHT|
 |    1<<    1|    1<<   22|    1<<   64|    1<<  640|    1<<   64|  <=    64|
 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 |           1|           4|          16|          10|          64|    42.328|
 |           1|           8|           8|          20|          64|    41.844|
 |           1|          16|           4|          40|          64|    41.730|
 |           1|           6|          10|          16|          60|    40.093|
 |           1|          15|           4|          40|          60|    39.054|
 |           1|           4|          16|           8|          64|    39.043|
 |           1|          12|           5|          32|          60|    39.026|
 |           1|           8|           8|          16|          64|    38.598|
 |           1|          16|           4|          32|          64|    38.493|
 |           1|           3|          20|           8|          60|    38.319|
 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 Only the best possible choices for nproc are printed...

```

A weight is assigned to each distribution of processors. As indicated, you are advised
to select a processor distribution with a high *weight*. If we just focus on [[npband]]
and [[npfft]], we see that, for 64 MPI processes, the recommended distribution [[npband]]x[[npfft]] is 16x4.

In a second step you can launch ABINIT in parallel on 64 cores by
changing your input file as follows:

```diff
- autoparal     1
- max_ncpus     64
+ npband        16
+ npfft         4
```

You can now perform your calculations using the KGB parallelization in ABINIT.
But somehow, you did it without understanding how you got the result...

## A more sophisticated method

In this part we will try to recover the previous process distribution, but with a better
understanding. As shown above, the pair ([[npband]] x [[npfft]]) of input variables
can have various values: (64x1), (32x2), (16x4), (8x8), (4x16).
In order to perform these 5 calculations you can use the `tgspw_02.abi`.

{% dialog tests/tutoparal/Input/tgspw_02.abi %}

Change the line corresponding to the processor distribution.
A first calculation with:
```diff
- npband         16
- npfft          4
+ npband         64
+ npfft          1
```

A second one with:
```diff
- npband         64
- npfft          1
+ npband         32
+ npfft          2
```

And so on, until you have run all 5 calculations ([[npband]]x[[npfft]]) : (64x1), (32x2), (16x4), (8x8), (4x16).

Store all the output files by renaming them as follows:
The timing of each calculation can be retrieved using the shell command:
`tgspw_02.64.01.abo`, `tgspw_02.32.02.abo`,
`tgspw_02.16.04.abo`, `tgspw_02.08.08.abo` and
`tgspw_02.04.16.abo`

The timing of each calculation can be retrieved using the shell command:
```bash
grep Proc *.abo
```

```bash
tgspw_02.04.16.abo:- Proc.   0 individual time (sec): cpu=         62.9  wall=         69.4
tgspw_02.08.08.abo:- Proc.   0 individual time (sec): cpu=         57.9  wall=         64.0
tgspw_02.16.04.abo:- Proc.   0 individual time (sec): cpu=         56.0  wall=         61.8
tgspw_02.32.02.abo:- Proc.   0 individual time (sec): cpu=         57.1  wall=         62.8
tgspw_02.64.01.abo:- Proc.   0 individual time (sec): cpu=         60.7  wall=         66.4
```

As far as the timing is concerned, the best distributions are the ones
proposed in previous section: (16x4) seems to be the best one.
The prediction using [[autoparal]]**=1** was pretty good.

Up to now, we have not learned more than before. We have so far only
considered the timing of 10 electronic steps.

However the *Locally Optimal Block Preconditioned Conjugate Gradient* algorithm (LOBPCG)
&mdash; used in ABINIT by default when [[paral_kgb]] == 1 &mdash; operates a diagonalization by block of eigenvectors.
Each block of eigenvectors is concurrently diagonalized by the [[npband]] processes,
one block after the other. When the [[npband]] value is modified, the size of the block
changes accordingly (it is exactly equal to `npband`), and the solutions of the eigensolver are modified.
One calculation can be the quickest if we look at the time needed by one iteration
but the slowest at the end because many more steps are performed.
In order to see this, we can have a look at the convergence rate at the end
of the calculations.
The last iterations of the SCF loops are:

```bash
grep "ETOT  5" *.abo
tgspw_02.04.16.abo: ETOT  5  -3654.9080524851    -1.683E-04 6.622E-05 2.509E-04
tgspw_02.08.08.abo: ETOT  5  -3654.9081359132    -2.710E-04 6.111E-05 2.318E-04
tgspw_02.16.04.abo: ETOT  5  -3654.9082768015    -6.727E-05 5.277E-05 1.490E-04
tgspw_02.32.02.abo: ETOT  5  -3654.9081759902    -2.737E-04 2.495E-05 1.968E-04
tgspw_02.64.01.abo: ETOT  5  -3654.9083410155    -1.580E-04 6.181E-05 1.440E-04
```

The last column indicates the convergence of the **residual of the density** (because we are using PAW, otherwise it would be the residual of the potential in norm-conserving).
You can see that this quantity is the smallest when [[npband]] is
the highest. This result is expected: the convergence is better when the size
of the block is the largest. But this **best convergence** is obtained for
the (64x1) distribution... when the **worst timing** is measured.

Already at the third iteration this behaviour is observed !
```bash
grep "ETOT  3" *.abo
tgspw_02.04.16.abo: ETOT  3  -3654.8846449612    -2.877E-01 5.690E-04 7.055E-02
tgspw_02.08.08.abo: ETOT  3  -3654.8848503583    -2.884E-01 8.889E-04 7.093E-02
tgspw_02.16.04.abo: ETOT  3  -3654.8858758622    -2.798E-01 5.805E-04 6.792E-02
tgspw_02.32.02.abo: ETOT  3  -3654.8866790037    -2.689E-01 6.794E-05 6.472E-02
tgspw_02.64.01.abo: ETOT  3  -3654.8885890816    -2.528E-01 4.112E-05 5.918E-02
```

So, you face a dilemma. The calculation with the smallest number of iterations
(the best convergence) is not the best concerning the timing of one iteration
(the best efficiency), and vice versa... The best choice is a compromise.

In the following we will choose the (16x4) pair, because it
definitively offers more guarantees concerning the convergence and the timing.

!!! note

    You could check that the convergence is not changed when the [[npfft]] value is modified.


## Even more sophisticated: BANDs Per Process (bandpp)

We have seen in the previous section that the best convergence is obtained
when the size of the block is the largest. This size was exactly equal to the
[[npband]] value. It was only possible to increase the block size by increasing
the number of MPI processes.

* *Is it possible to do better?*

    The answer is **yes**! The input variable named [[bandpp]] (BANDs Per Process)
    enables to increase the block size without changing the number of processes
    dedicated to bands.

* *How does this work?*

    As previoulsy, each block of bands is diagonalized by [[npband]] MPI processes in parallel.
    But, if [[bandpp]] is activated, each process handles [[bandpp]] bands.
    The block size is now equal to `npband x bandpp`. 
    Accordingly the block size can be
    modified (usually increased) by playing with the value of [[bandpp]], without changing
    the number of MPI processes.
    Note that FFTs are still performed by the [[npfft]] MPI processes.

In the following we use the same settings as previously, just performing more electronic step:

```diff
+ nstep         10
+ bandpp        1 # This is the default value
```

{% dialog tests/tutoparal/Input/tgspw_03.abi %}

Copy the input file `tgspw_03.abi` then run ABINIT over 64 CPUs,
setting [[bandpp]]**=1** and then [[bandpp]]**=2**. The output files will be
named `tgspw_03.bandpp1.abo` and `tgspw_03.bandpp2.abo`, respectively. A
comparison of these two files shows that the convergence is better in the
second case.
Conclusion: for a given number of processors, it is possible to improve
the convergence by increasing bandpp.

However, as you can see, the CPU time per iteration
increases when [[bandpp]] increases: 

```bash
grep Proc tgspw_03.bandpp1.abo tgspw_03.bandpp2.abo
tgspw_03.bandpp1.abo:- Proc.   0 individual time (sec): cpu=         90.0  wall=         95.6
tgspw_03.bandpp2.abo:- Proc.   0 individual time (sec): cpu=         90.6  wall=         96.4
```

Now look at the last iteration
```bash
grep "ETOT 10" tgspw_03.bandpp1.abo tgspw_03.bandpp2.abo
tgspw_03.bandpp1.abo: ETOT 10  -3654.9085401615    -2.882E-08 5.100E-05 1.247E-08
tgspw_03.bandpp2.abo: ETOT 10  -3654.9085401624    -3.744E-08 2.632E-05 8.003E-09
```

With [[bandpp]]=2, the calculation is more converged ! 

We can also compare the (16x4)+[[bandpp]]**=2** distribution (16x2=32) with
the (32x2)+[[bandpp]]**=1** (32x1=32) one.
These 2 calculation have [[npband]]x[[bandpp]]=32 (and [[nband]]x[[npfft]]=64).
Thus the convergence is the same !
Use the same input file and change it according to:

```diff
- npband        16 
- bandpp        2  
- npfft         4  
+ npband        32 
+ bandpp        1  
+ npfft         2  
```

Then run ABINIT over 64 MPI processes and name the output file `tgspw_03.32.02.bandpp1.abo` 
(This is one of the calculation you already did in the previous section but with [[nstep]]=10 instead of 5).
Perform a `diff` between the two output files `tgspw_03.bandpp2.abo` 
and `tgspw_03.32.02.bandpp1.abo`. 
As you can see, the two calculations give exactly the same 
convergence rate. This was expected since, in both cases, the block sizes are equal (to 32)
and the number of FFT processors [[npfft]] does not affect the convergence.

!!! tip

    It is possible to adjust the distribution of processes, without changing the
    convergence, by reducing [[npband]] and increasing [[bandpp]] proportionally.


* *Where does this CPU time consumption come from?*

    As previously explained, each MPI processes handles `bandpp` bands **sequentially**.
    Thus the *sequential part* of the code increases when [[bandpp]] increases.

!!! note

    Be carefull ! Depending on the number of plane waves and the the number of bands, increasing [[bandpp]]
    can increase or decrease the cpu time ! Usually starting from 1, increasing [[bandpp]] first decreases or maintains the cpu time,
    and continuing increasing [[bandpp]] will then increase the cpu time. 
    Experience it by yourself (also depends on the hardware).

We will see in the next section how the use of *hybrid parallelism* can again improve this...

!!! important

    Using only MPI parallelism, the timing of a single electronic step should theoretically increase
    when [[bandpp]] increases but the convergence rate is better.


## Hybrid parallelism: MPI+*OpenMP*

In modern supercomputers, the computing units (CPU cores) are no more equally distributed.
They are grouped by **nodes** in which they share the same memory access.
In so-called *many-core* architecture CPU cores can be numerous on the same node.
You could continue to use them as if they were not sharing the memory (using MPI only)
but this is not the most efficient way to take benefit from the computer.
The best practice is to use *hybrid* parallelism, mixing distributed memory parallelism
(MPI, between nodes) and shared memory parallelism ( *OpenMP*, inside a node). As you will
see, this will also have consequences on the performance of the iterative diagonalization
algorithm (LOBPCG).
Let's try!

!!! important
    
    To use ABINIT with both MPI and OpenMP, make sure to compile abinit with
    `--enable-openmp` to activate OpenMP support
    **and** to link against a linear algebra library that support multithreading like [mkl](https://software.intel.com/content/www/us/en/develop/tools/oneapi/components/onemkl/link-line-advisor.html)

!!! note

    When we say "set `OMP_NUM_THREADS=XX`", set your environment with `export OMP_NUM_THREADS=XX` for `bash` or `setenv OMP_NUM_THREADS XX` for `csh`

!!! important

    When using threads, we have to impose [[npfft]] **= 1**.
    The best is to suppress it from the input file.

The `tgspw_04.abi` input file has the parallelization set to [[npband]]=32 and [[npfft]]=2.
Thus it uses 32 MPI processes. We have set [[bandpp]]=2.

1. Run ABINIT using 64 MPI processes and 1 *OpenMP* threads (set `OMP_NUM_THREADS=1`). Copy the output file to `tgspw_04.bandpp2.1thread.abo`
2. Set [[npfft]]=1 and run ABINIT using 32 MPI processes and 2 *OpenMP* threads (set `OMP_NUM_THREADS=2`). Copy the output file to `tgspw_04.bandpp2.2thread.abo`

!!! note

    32 MPI x 2 threads = 64 cores.


{% dialog tests/tutoparal/Input/tgspw_04.abi %}

Let's have a look at the timings and compare them :

```bash
grep Proc tgspw_04.bandpp2.1thread.abo tgspw_04.bandpp2.2thread.abo
tgspw_04.bandpp2.1thread.abo:- Proc.   0 individual time (sec): cpu=         97.0  wall=        102.5
tgspw_04.bandpp2.2thread.abo:- Proc.   0 individual time (sec): cpu=        148.3  wall=         86.5
```

As you can wee, the new output file show a larger computing time for process 0: (cpu=....) disappointing?
Not really: you have to keep in mind that this timing is for one MPI process, adding the timings
of all the *OpenMP* tasks for this process. In the pure MPI case, we thus have `97 sec.` per task;
but in the *hybrid* case, we have `148/2=74 sec.` per task. 
As you can see, the "wall= ...." time is closer to thos value and is more or less the real user time.
Therefore with 2 threads, the run last 86s whereas with only 1 thread, the run last 102s !!

For the total 64 cores, the total cpu time used by ABINIT is `97x64=6208 sec.` in the MPI case, `74*32=2368 sec.` in the hybrid case.
This is much better!
The best way to confirm that is to look at the *Wall Time* (cumulated on all tasks) at the end
of the output file:

```bash
grep Overall tgspw_04.bandpp2.1thread.abo tgspw_04.bandpp2.2thread.abo
tgspw_04.bandpp2.1thread.abo:+Overall time at end (sec) : cpu=       6419.3  wall=       6555.8
tgspw_04.bandpp2.2thread.abo:+Overall time at end (sec) : cpu=       4853.7  wall=       2766.3
```

*How does ABINIT distribute the workload?*

Each block of bands is diagonalized by [[npband]] MPI processes.
As previously, each process handles `bandpp` bands but now using the *OpenMP* tasks.
This means that `bandpp x npband` bands are computed in parallel using
`nthreads x npband` tasks (`bandpp` is supposed to be a multiple of `nthreads` to get the best performances).
This is in principle more efficient than in the pure MPI case.
Scalar products, matrix multiplications and other algebra operations are done in parallel by the *OpenMP* tasks ([[npfft]] not used).
Doing so, the communications between MPI processes are also reduced !

!!! important

    When using threads, [[bandpp]] needs not to be a multiple of threads but it is **highly recommanded**!

* *How do we choose the number of threads?*

  Well, it strongly depends on the computer architecture and the case!
  
  A computer is made of *nodes*. On each node, there are *sockets* containing a given number
  of CPU cores. All the cores of one *node* can access the RAM of all the *sockets* but this access
  is faster on their own *socket*. This is the origin of the famous *Non Uniform Memory
  Access* effect (NUMA). The number of *threads* has thus to be a divisor of the total
  number of cores in the node, but it is better to choose a divisor of the number of
  cores in a *socket*. Indeed ABINIT performance is very sensitive to NUMA effect.

In the following, let's learn how to use ABINIT on 4 *OpenMP* threads...
First we change the input file as follows:

```diff
- npband        32
- bandpp        2
+ npband        16
+ bandpp        4
```

Then we run ABINIT using 16 MPI processes and 4 threads (still 64 cores) and rename the output file `tgspw_04.bandpp4.4thread.abo`
And we compare the timing of this "4 threads" case with the previous "2 threads" case:

```bash
grep Overall tgspw_04.bandpp2.2thread.abo tgspw_04.bandpp4.4thread.abo
tgspw_04.bandpp2.2thread.abo:+Overall time at end (sec) : cpu=       4853.7  wall=       2766.3
tgspw_04.bandpp4.4thread.abo:+Overall time at end (sec) : cpu=       4423.2  wall=       1316.8
```

We again have improved ABINIT performances!

* *Can we do better?*

  In principle, yes. As previously explained, we have to increase the block size for
  the LOBPCG diagonalization algorithm. 

Let's try it, just changing the [[bandpp]] value in input file:

```diff
- bandpp        4
+ bandpp        8
```

We don't change here the number of threads (keeping 4).
And we obtain the following timings:

```diff
grep Overall tgspw_04.bandpp4.4thread.abo tgspw_04.bandpp8.4thread.abo
tgspw_04.bandpp4.4thread.abo:+Overall time at end (sec) : cpu=       4423.2  wall=       1316.8
tgspw_04.bandpp8.4thread.abo:+Overall time at end (sec) : cpu=       4704.6  wall=       1417.3
```

The new settings do not give a better result...**but** for a few second you have a better convergence
```diff
grep "ETOT 10" tgspw_04.bandpp4.4thread.abo tgspw_04.bandpp8.4thread.abo
tgspw_04.bandpp4.4thread.abo: ETOT 10  -3654.9085401627    -5.105E-09 2.274E-05 1.673E-09
tgspw_04.bandpp8.4thread.abo: ETOT 10  -3654.9085401627    -1.376E-09 1.912E-05 6.053E-10
```
Convergence has a cost...

To help you in choosing the distribution of processes/tasks, you can launch ABINIT with
the [[autoparal]]**=1** and [[max_ncpus]] keywords. [[max_ncpus]] should be equal the
total number of targeted CPU cores, i.e. `nthreads x nMPI` and you should launch ABINIT on 1 MPI
process with `OMP_NUM_THREADS=nn`.
You can try this with `max_ncpus=64` and `OMP_NUM_THREADS=4`...

!!! tip

    The rules to distribute the workload are:

    * `npband x bandpp` (size of a block) should be maximized.
       It has to divide the number of bands (`nband`)
    * `bandpp` should be a multiple of the number of *OpenMP* tasks
    * `nband` has to be a multiple of `npband x bandpp`.
    * In any case, the ideal distribution is system dependent!

## 6 The KGB parallelization

!!! note
    For this part, a cluster was used and therefore the timing for `tgspw_03.abi` is different than the previous run in the previous section.
    **Use the same cluster to run `tgspw_03.abi` and the following run.**

Up to now, we only performed a "GB" parallelization, using 2 levels (bands and/or FFT).
If the system has more than 1 *k-point*, one can add a third level of parallelization
and perform a full "KBG" parallelization. There is no difficulty in
adding processes to this level.

To test the full parallelism, we restart
with the same input file as in `tgspw_03.abi` and add a denser *k-point* grid.
In this case, the system has 4 *k-points* in
the *irreducible Brillouin zone* (IBZ) so the calculation can be parallelized over (at most) 4 *k-points*
MPI processes. This is done using the [[npkpt]] input variable:

```diff
- nkpt          1
+ ngkpt         4 4 4
+ npkpt         4
```

We need 4 times more processes than before, so run ABINIT over
256 CPU cores (only MPI) with the `tgspw_05.abi` file.
The timing obtained in the output file `tgspw_05.abo` and `tgspw_03.abo` are:

```bash
grep Proc tgspw_03.abo tgspw_05.abo 
tgspw_03.abo:- Proc.   0 individual time (sec): cpu=         44.2  wall=         45.5
tgspw_05.abo:- Proc.   0 individual time (sec): cpu=         49.3  wall=         50.4
```
They are quasi-identical !
This means that the scalability of ABINIT is quasi-linear on the *k-point* level.

!!!important

    When you want to parallelize a calculation, begin by the k-point level, then
    follow with the band level; then activate the FFT level or OpenMP threads.

Here, the timing obtained for the output `tgspw_05.abo` leads to a hypothetical
speedup of $45.5/50.4 \times 256\approx 231$, which is good, but not 256 as expected if the scaling was
linear. Indeed, the time needed here is slightly longer (5 sec. more) than
the one obtained in `tgspw_03.abo`.
To go further, let's compare the time spent in all the routines.
All the input files you have used contain the input variable [[timopt]]=-3 which activates the timing of different subparts of the run. 
You can find the results a the end of the output files before the references.

A first clue to undestand this not perfect speedup comes from the sequential code which is not parallelized. 
This sequential part is mainly (99%) done outside the "vtowfk level".

Let's have a look at the time spend in the well parallelized subroutine `vtowfk`:

```bash
grep -e '- vtowfk *[[:digit:]]' tgspw_03.abo tgspw_05.abo
tgspw_03.abo:- vtowfk                      2574.132  89.4   2589.639  89.3            640                   0.99       0.99
tgspw_03.abo:- vtowfk                      2574.132  89.4   2589.639  89.3            640                   0.99       0.99
tgspw_05.abo:- vtowfk                     10521.057  82.3  10595.093  82.3           2560                   0.99       0.99
tgspw_05.abo:- vtowfk                     10521.057  82.3  10595.093  82.3           2560                   0.99       0.99
```

We see that the KGB parallelization performs really well, since the wall time
spent within `vtowfk` is approximatively equal: $10521.057/256\approx 2574.132/64\approx 40$.
So, the speedup is quasi-linear in `vtowfk`. The problem comes from parts outside
`vtowfk` which are not parallelized and are responsible for the negligible
(100-82.3)% of time spent in sequential. These parts are no longer negligible
when you parallelize over hundreds of processors. The time spent in `vtowfk`
corresponds to 89.4% of the overall time when you don't parallelize over k-points,
and only 82.3% when you parallelize.

This behaviour is related to the [Amdhal's law](https://en.wikipedia.org/wiki/Amdahl%27s_law):

> The speedup of a program using multiple processes in parallel computing is
> limited by the time needed for the sequential fraction of the program.

For example, if a program needs 20 hours using a single processor core,
and a particular portion of 1 hour cannot be parallelized, while the remaining
portion of 19 hours (95%) can be parallelized, then regardless of how
many processor cores we have, the minimum execution time cannot be
less than that critical 1 hour. Hence the speedup is limited to 20.

In our case, the part above the loop over *k-points* in not parallelized by the
KGB parallelization. Even if this part is very small &mdash; less than 1% &mdash;
it determines an upper bound for the speedup.

<!--
<sub><sup>To do in the future: discuss convergence, wfoptalg, nline</sup></sub>
-->
---
authors: XG
---

# ABINIT package: context of development

The Corning code for electronic structure calculations began
to be developed in the late eighties by D.C. Allan, Corning Inc.
The fundamental algorithm of this code, the band-by-band
conjugate gradient algorithm, was proposed by M.P. Teter, M. Payne
and D.C.Allan, and described thoroughly in the review paper [[cite:Payne1992]]
where other references can be found.
It was written in Fortran 77. Some technical features
were: a plane-wave representation of wavefunctions, use of
pseudopotentials, local-density approximation (LDA) within
the Density-Functional Theory (DFT).

In July 1990, X. Gonze joined Cornell University, where
M.P.Teter was professor. Building upon the Corning code, the Respfn
(Response Function code) was written. See [[cite:Gonze1997a]]
for a complete description of the algorithms. While the Corning and
Respfn code were consistent at the end of X. Gonze stay
in Cornell University (in september 1992),
they began to diverge afterwards. Corning Inc. had
agreed to allow further development based on the version 920813
in Louvain-la-Neuve. Some features of Respfn were
developed by J.-C. Charlier and C. Lee both before the separation
of codes as well as after it.

In 1993 Corning Inc. agreed with Biosym Inc. that the Corning code,
renamed *Plane_Wave*, would be commercialized by Biosym Inc.
In 1995, Biosym Inc. merged with MSI (Molecular Simulation Inc.),
and shortly afterwards, the decision was taken by MSI not to
continue the development of the *Plane_Wave* code.

In 1996, D.C. Allan and X. Gonze explored the possibility to write
a new code, that should not be commercialized, but rendered
available to the scientific community as a freeware. It would be written
in Fortran 90, would include parallel features, would be based
on a SCF (Self-Consistency Field) algorithm. Its writing would
be facilitated by the availability of the
Respfn codes and Corning 920813 codes, as well as later
developments of the Corning code (then renamed *Plane_Wave*),
excluding the features developed by Biosym computer scientists,
or tightly bound with Biosym proprietary features.

The project was first named DFT2000, but the name ABINIT was definitely
adopted in September 1998. Version 1.9 of ABINIT was made available
in March 1999 to beta testers (for some it was done a bit earlier),
outside Louvain-la-Neuve or Corning. It had only "ground-state" features.

Version 2.0 was released in July 1999. The possibility to compute
response-functions (phonons, Born effective charges, dielectric constant)
was available.

Version 3.0, the first version to be proposed
on the Web under GNU GPL licence,
has features that considerably improve upon the existing
Respfn and the 920813 version of Corning  (or even the latest available
version of *Plane_Wave*), i.e. much faster, better parallelisation ...
ABINIT v3.0 was made available in December 2000.

An advisory commitee was set up in June 2000, and served
for the whole lifetime of the version 3.

Version 4.0 was delivered after the first international ABINIT
developer workshop, in January 2003, with a renewed advisory committee,
and the aim to bridge the speed gap with respect to other plane-wave
based first-principles codes: PAW, parallel FFT, better geometry
optimization, better molecular dynamics.

The second international ABINIT workshop, in May 2005 was held in Paris.

Version 5.0 was delivered in Autumn 2005, with a completely renewed
build system.
## v9.6

Version 9.6, released on October 4, 2021.
List of changes with respect to version 9.4.
<!-- Release notes updated on November 9, 2021. -->

Many thanks to the contributors to the ABINIT project between
February 2021 and September 2021. These release notes
are relative to modifications/improvements of ABINIT v9.6 with respect to v9.4.
<!-- Merge requests up to and including MR814 except MR812, then also MR818, 819, 820 and 822 are taken into account. -->

The list of contributors includes:
L. Baguet, J.-M. Beuken, J. Bieder, A. Blanchet,
J. Clerouin, C. Espejo, M. Giantomassi, O. Gingras, X. Gonze, F. Goudreault,
B. Guster, Ch. Paillard,
Y. Pouillon, M. Rodriguez-Mayorga, M. Royo, F. Soubiran,
M. Torrent, M. Verstraete, J. Zwanziger.

It is worth to read carefully all the modifications that are mentioned in the present file,
and examine the links to help files or test cases.
This might take some time ...

Xavier

### **A.** Important remarks and warnings.

(nothing to mention for this v9.6)

* * *

### **B.** Most noticeable achievements

**B.1** Band-parallel implementation of DFPT: the memory footprint is now distributed over different processors.
Previously, the memory was distributed only for k-point parallelism.
This is automatically managed, no user action is to be taken to activate this memory saving.

See test [[test:dfpt_04]].

By M. Verstraete (MR784, 803)

**B.2** The Iterative Boltzmann Transport Equation (IBTE) to compute the electric conductivity has been implemented.
To activate the IBTE, use [[ibte_prep]] = 1 with [[eph_task]] -4.
The IBTE solver can also be invoked in standalone mode by providing a SIGEPH file with [[eph_task]] = 8.
Related input variables: [[ibte_niter]], [[ibte_abs_tol]] and [[ibte_alpha_mix]].
See test [[test:v9_65]].

By M. Giantomassi (MR794)

**B.3** The computation of dynamical quadrupoles and flexoelectricity is now available within the GGA.
Test for GGA + longwaves [[test:v9_46]].

Also, the usage of the quadrupoles has been rationalized (and made easier) in anaddb
as the default value of [[dipquad@anaddb]] and [[quadquad@anaddb]] has been changed to 1.
This means that dipole-quadrupole and quadrupole-quadrupole contributions are always included
in the Fourier interpolation of the dynamical matrix provided the DDB provides these terms.

This "default behaviour" is similar to the one used for the dipole-dipole treatment.
Indeed, the default value of [[dipdip@anaddb]] is 1 hence the dipolar term is automatically included
if the DDB contains the Born effective charges and the electronic dielectric tensor.
Still, the user can deactivate the inclusion of the different terms by setting the corresponding
variable to zero for testing purposes.
See the ANADDB input variables and test [[test:lw_6]]

By M. Royo with contribution by M. Giantomassi for the change of default (MR795)


**B.4** Stresses are available within cDFT (constrained DFT).
See tests [[test:v9_01]], [[test:v9_02]] and [[test:v9_03]].

By X. Gonze (MR802)


**B.5** The computation of effective mass renormalization due to electron-phonon coupling, treated in the generalized Frohlich model,
is now available, for cubic materials. An article has been submitted, see <https:arxiv.org/abs/2109.12594>.
Activate it using [[eph_task]]=10.

See test [[test:v9_66]].

By B. Guster (MR800)

**B.6** Important speed-up of the PAW calculations is allowed thanks to the storage of "cprj" coefficients.
See the input variable [[cprj_update_lvl]]. However, at present this is only possible for ground-state
calculations, with several restrictions, spelled in [[cprj_update_lvl]]. So, this is not activated by default.
There is also an internal variable [[cprj_in_memory]] exposed in the documentation.
Other input variables have been introduced in the development process : [[fft_count]] and [[nonlop_ylm_count]].
They allow one to monitor better the number of FFTs and non-local operator applications.

See tests [[test:v9_71]], [[test:v9_72]], [[test:v9_73]] and [[test:v9_74]].

By L. Baguet (MR793).

**B.7** The Extended First-Principles Molecular Dynamics has been implemented.
This method allows one to drastically reduce the needed number of bands for high temperature simulations,
using pure single plane waves description based on the Fermi gas model beyond explicitly computed bands.
The implementation and usage will be described in an upcoming paper which is currently under review (Authors: *A. Blanchet, J. Clérouin, M. Torrent, F. Soubiran*).

See [[topic:ExtFPMD]], as well as the input variables [[useextfpmd]] and [[extfpmd_nbcut]],
and test [[test:v9_92]].

By A. Blanchet, J. Clérouin, M. Torrent, F. Soubiran. (MR788).


* * *

### **C.** Changes for the developers (including information about compilers)

**C.1** Supported compilers

* gfort (GNU) compiler: v11 newly supported.
* ifort (Intel) compiler: v21.4 newly supported.
Two new bots introduced in the test farm : alps_intel_21.4_elpa and graphene_gnu_11.2_macports .

By JM Beuken

* * *

### **D.**  Other changes (or on-going developments, not yet finalized)

**D.1** New input variable for "optic": [[prtlincompmatrixelements@optic]].

Added this flag in order to make it possible to print the different elements that are used to build the susceptibility of the linear component of the dielectric tensor.
These elements are namely: the matrix elements, the renormalized electronic eigenvalues, the occupations and the kpt weights.
Everything is dumped into the _OPTIC.nc file by the main process. Thus optimization could be done memory-wise and speed wise if MPI-IO is implemented for this nc file.

[[test:v9_49]] was created which is the same as [[test:v9_48]] except with the aforementioned flag set to 1.
This test checks that everything works well even though we print the matrix elements.
It does not test that matrix elements are well printed because that would require testing of the OPTIC.nc file.
Although it is possible to check that it works well using a simple python script (see the figure in the merge request on Gitlab).
(Note that the tests v9_13 and v9_14 have been moved to v9_47 and v9_48 in this change).

By F. Goudreault (MR776)

**D.2** New radial sine transform for the vdW-DF kernel.

By C. Espejo (MR 797)


**D.3** New test of orbital magnetism, [[test:v9_37]].
Also, on-going work on orbital magnetism, including use with DDK wavefunctions.

By J. Zwanziger (MR767, MR775, MR779 and MR787)

**D.4** Migration to mkdocs==1.1.2 and  mkdocs-material==7.0.6.
mksite.py now requires python >= 3.6 .
Activated search capabilities, available in the new mkdocs version.

By M. Giantomassi (MR774)

**D.5** Fixed bug in make_efg_onsite for [[nspden]]=2 case.

By J. Zwanziger (MR783)

**D.6** Correction of tutorials Rf1 and Rf2 for version 9

By O. Gingras (MR785)

**D.7** Fixed errors and bugs detected by using -ftrapuv intel option

By M. Giantomassi (MR789)

**D.8** Bug fix in [[nspden]]=4 DFPT for Fe

By M. Verstraete (MR790)

**D.9** Fixed a spurious test line 711 of m_occ.F90 that caused abinit to abort in the case of [[occopt]]=9,
if the number of conduction bands was enough to accommodate nqFD but not enough to accommodate nelect.

By Ch. Paillard (MR791)

**D.10** GW methodology with Kohn-Sham density matrix.
Solving a bug producing a segmentation fault when using [[bdgw]] and [[gw1rdm]].
New test [[test:v9_37]].

By M. Rodriguez-Mayorga (MR792)

**D.11** Introduced new input variable use_oldchi.
This input variable is temporary, for testing purposes. It is documented, but not tested.

By Wei Chen (modified line 743 in src/95_drive/screening.F90 on 23 April 2021).

**D.12** The input variable [[rfstrs_ref]] has been introduced, but not yet documented and tested, as this is on-going work.

By M. Royo 


**D.13** Miscellaneous additional bug fixes, or upgrade of build system.
in the upgrade of tutorials).
By J. Bieder, M. Giantomassi, Y. Pouillon, M. Torrent, J. Zwanziger.

* * *

## v9.4

Version 9.4, released on February 25, 2021.
List of changes with respect to version 9.2.
<!-- Release notes updated on April 30, 2021. -->

Many thanks to the contributors to the ABINIT project between
November 2020 and April 2021. These release notes
are relative to modifications/improvements of ABINIT v9.4 with respect to v9.2.
<!-- Merge requests up to and including MR766 are taken into account, also MR768 (backported) up to MR772 and MR780, 781, 782. -->

The list of contributors includes:
B. Amadon, L. Baguet, J.-M. Beuken, J. Bieder, E. Bousquet, V. Brousseau, F. Bruneval,
W. Chen, M. Cote, M. Giantomassi, O. Gingras, X. Gonze, F. Goudreault,
B. Guster, T. Karatsu, A. H. Larsen, O. Nadeau, R. Outerovich, Ch. Paillard, G. Petretto,
S. Ponce, Y. Pouillon, G.-M. Rignanese, M. Rodriguez-Mayorga, M. Schmitt,
M. Torrent, M. Verstraete, He Xu, J. Zwanziger.

It is worth to read carefully all the modifications that are mentioned in the present file,
and examine the links to help files or test cases.
This might take some time ...

Xavier

### **A.** Important remarks and warnings.

**A.1** The [[charge]] variable is obsolete, and has been replaced by [[cellcharge]]. Indeed, [[charge]] was quite ambiguous,
and the string `charge` present in some other input variables. For the time being, ABINIT still recognizes [[charge]]
in the input file, but this might not last longer than in ABINITv9.

**A.2** There is a new check, governed by the input variable [[chksymtnons]], to examine
whether the [[tnons]] of all symmetry operations
is zero or a rational number with small denominator, which is required for GW calculations as implemented in ABINIT.
It is always possible to choose
the origin of coordinate with such properties, and ABINIT can gives suggestions. If you do not want to change
your coordinate origin (e.g. you have no intention to perform a GW calculation), set [[chksymtnons]] to zero,
or (but this is more dangerous) set the meta-variable [[expert_user]] to one to disable several checks of input variables at once.

By X. Gonze (MR712)

**A.3** The input variable npkpt has been changed to np_spkpt . Indeed the parallelism governed by npkpt was about spin and k points,
not only k points. For the time being npkpt is still admitted, but will become obsolete at the next major version change.

By X. Gonze

**A.4** When [[nimage]]>1, the default value of [[prtgsr]] is now 0, like for several prt* variables.

By X. Gonze

**A.5** The code does not stop anymore at the first occurence of overlap between PAW spheres being larger than [[pawovlp]]
in case of [[ionmov]]/=0 or [[imgmov]]/=0, but only at the second occurrence per dataset. Indeed, such trespassing might only be transient.
See the description of [[pawovlp]].

By X. Gonze

* * *

### **B.** Most noticeable achievements

**B.1** The RMM-DIIS algorithm has been implemented.
This SCF (ground-state) algorithm is faster, but potentially more unstable,
than the CG or LOBPCG algorithms, for medium to large size systems,
as it has less cubic scaling steps (e.g. orthogonalisation). Typically used for molecular dynamics
or structural relaxations as the restart from the previous time step gives RMM-DIIS less opportunities to fail.
See input variables [[rmm_diis]] and [[rmm_diis_savemem]].
Several tests exist ([[test:paral_32]], [[test:paral_63]], [[test:paral_64]], [[test:v9_29]], [[test:v9_30]])
covering many cases, including NC/PAW and spin-orbit.
Note that the PAW version of RMM-DIIS is more unstable than the NC one and extra operations are needed to make it convergence.
So the speedup for PAW calculations is not as good as the one observed for NC.

By M. Giantomassi (MR757, MR719, MR718)

**B.2** The treatment of quasi-Fermi energies in the valence and conduction band,
with populations of electrons (in the conduction bands) and holes (in the valence bands)
has been implemented (gapped materials only, of course).
This has been used e.g. in [[cite:Paillard2019]].
See the variables [[occopt]]=9, and [[nqfd]].
See also the related input variable : [[ivalence]].
Internal variables ne_qFD and nh_qFD are presently initialized to [[nqfd]], which is NOT INTERNAL.
See test [[test:v9_91]].

By Ch. Paillard (MR755).

**B.3** All the tutorials have been carefully reexamined and improved, when appropriate.
In particular:

- the old pseudopotentials have been replaced by new ones from e.g. [pseudodojo](http://www.pseudo-dojo.org/) or [JTH](https://www.abinit.org/psp-tables);
- the text of the tutorial uses the new convention for launching abinit (e.g. `abinit input_file` instead of `abinit < files_file`) and the pseudopotentials are mentioned in the input file;
- the input file suffix has been changed from `.in` to `.abi`and the output file suffix has been changed from `.out`to `.abo`;
- the input files have been cleaned when adequate, and many have been restructured using a template;
with populations of electrons (in the conduction bands) and holes (in the valence bands)

<!--
Also, a new tutorial, [[tutorial:eph4isotc]], is available (with tests [[test:eph4isotc_1]] to [[test:eph4isotc_4]],
-->
Also, [[tutorial:nlo]] and [[tutorial:eph4zpr]] have been enlarged to new developments
(see [[test:eph4zpr_8]] and [[test:nlo_6]]).

By B. Amadon, L. Baguet, J. Bieder, E. Bousquet, F. Bruneval,
W. Chen, M. Cote, O. Gingras, M. Giantomassi, X. Gonze, F. Goudreault,
B. Guster, O. Nadeau, R. Outerovich, S. Ponce, M. Schmitt,
M. Torrent, M. Verstraete, He Xu, J. Zwanziger (numerous MRs)

**B.4** The GW 1-body reduced density matrix (1RDM) from the linearized Dyson equation has been implemented.
Its effect on the Hartree-Fock expectation values and therefore on the GW quasiparticle energies can be evaluated.
The resulting total energy parts, kinetic energy (including correlation), electron-nucleus, Hartree, Exchange, can be calculated.
Together with the Galitskii-Migdal correlation, it gives a new approximation the self-consistent GW total energy.
See input variables [[gw1rdm]], [[x1rdm]], also [[irdchkprdm]], [[prtchkprdm]] and [[gwgmcorr]].
See tests [[test:v9_33]] to [[test:v9_36]].

Also, some missing tests have been added:
- GW calculations based on Hartree-Fock wavefunctions can use mini Brillouin Zone integration technique, see [[test:v9_31]].
- A new test [[test:v9_40]] has been provided for the computation of the susceptibility matrix
$\chi_0$ with [[inclvkb]].

By Mauricio Rodriguez-Mayorga and F. Bruneval (MR722).

**B.5** The pSIC (polaron self-interaction corrected) methodology has been implemented.
See [[cite:Sadigh2015]] and [[cite:Sadigh2015a]]. This is based on the `images` capability
of ABINIT, that has been extended to different values of the input variable [[cellcharge]]
for different images, and also parallelized. To activate pSIC, use [[imgmov]]=6 with the proper occupation numbers.
See the test examples [[test:v9_22]], [[test:psic_01]], [[test:psic_02]] and [[test:psic_03]].

By X. Gonze (initial test from C. Tantardini) (MR770).

**B.6** The computation of the electric conductivity has been implemented for metals
in the relaxation-time approximation with transport lifetimes computed from the imaginary part of the Fan-Migdal self-energy.
See tests [[test:v9_62]] to [[test:v9_65]].

By O. Nadeau (MR756, MR716)

**B.7** Implementation of the  i-pi client-server protocol as described in [[cite:Kapil2019]].
This option requires [[ionmov]] 28 and the specification of the socket via command line options.
For UNIX socket, use: --ipi {unixsocket}:UNIX .
For INET socket, use  --ipi {host}:{port} .
Usage example:

     abinit run.abi --ipi {unixsocket}:UNIX > run.log

Note that, at present, this feature is mainly used to interface ABINIT
with the ASE optimization routines. Moreover the user is responsible for creating an input
file with tuned tolerances to prevent Abinit from exiting when internal convergence is reached.
See examples available in the [ASE documentation](https://wiki.fysik.dtu.dk/ase/dev/ase/calculators/socketio/socketio.html)

By M. Giantomassi and A. H. Larsen.


* * *

### **C.** Changes for the developers (including information about compilers)

**C.2** Test farm: new and obsolete bots

* Bots introduced in the test farm: scope_gnu_10.2_paral
* Bots removed: cronos2 (replaced by scope_gnu_10.2_paral)

Bigdft tests have been activated on ALPS.

By JM Beuken

**C.3** Supported compilers

* gfort (GNU) compiler: v10 newly supported, v5 obsolete
* ifort (INTEL) compiler: v15 obsolete (but to be reintroduced for licence reasons)

Support for AOCC has been added in the build system.

By JM Beuken

* * *

<a name="v9.4.D.1"></a>
### **D.**  Other changes (or on-going developments, not yet finalized)

**D.1** Calculation of Luttinger parameters (in the Frohlich model) and echo (see [[test:v8_57]]).
By V. Brousseau (MR736)

**D.2** New test [[test:v9_90]] of the treatment of the Coulomb interaction for low dimensional materials (0D and 2D).
By B. Guster.

**D.3** DFPT (including ddk perturbation) can now be done in the presence of [[nucdipmom]].
By J. Zwanziger (MR749)

**D.4** Bug fix and new test [[test:v9_43]] for the use of ANADDB in the presence of a large [[tolsym]] value.
By G. Petretto

**D.5** Several bug fixes related to the treatment of inaccurate atomic positions (and large tolsym).
Several test have been created ([[test:v9_17]] to [[test:v9_20]]).

**D.6** AiiDA+ABINIT developments

  - A AiiDA plugin for Abinit has been developed: <https://github.com/sponce24/aiida-abinit>, also indexed at <https://aiidateam.github.io/aiida-registry/>.
  - AiiDA is now supporting psp8 type pseudopotentials, <https://github.com/aiidateam/aiida-pseudo>. This also implied modifications of the
    [pseudodojo](http://www.pseudo-dojo.org), that now includes .djrepo for each type of pseudopotential.
  - Work on a [common relaxation workflow](https://github.com/aiidateam/aiida-common-workflows) for a dozen of codes, including ABINIT.

By S. Ponce, also with G.-M. Rignanese, G. Petretto, M. Giantomassi.

**D.7** The new input variable dmft_wanorthnorm has been introduced, see [[test:v6_07]] and [[test:v6_46]]. However, it should still be documented.
By B. Amadon.

**D.8** Increase stack size limit inside xmpi_init using POSIX C-API
By M. Giantomassi. MR 770.

**D.9** Document i-pi interface with links to ASE docs.
By M. Giantomassi. MR 770.

**D.10** Correction (from a message on the forum) related to the forces in electron-positron mode.
By M. Torrent. MR 780

**D.11** Miscellaneous additional bug fixes, improvements of documentation including for the build system (many other were made
in the upgrade of tutorials)..
By B. Amadon, L. Baguet, F. Bruneval, T. Karatsu, G. Petretto, Y. Pouillon, M. Torrent, J. Zwanziger.

* * *

## v9.2

Version 9.2, released on September 30, 2020.
List of changes with respect to version 8.10.
Release notes updated on November 10, 2020.

Many thanks to the contributors to the ABINIT project between
October 2018 and November 2020. These release notes
are relative to modifications/improvements of ABINIT v9.2 with respect to v8.10.
Merge requests up to and including MR692 are taken into account, also MR 697-702, 705, 707-710, 712, 715.

The list of contributors includes:
B. Amadon, G. Antonius, L. Baguet, J.-M. Beuken, J. Bieder, J. Bouchet, E. Bousquet, F. Bruneval, G. Brunin, Wei Chen,
J.-B. Charraud, Ph. Ghosez, M. Giantomassi, O. Gingras, X. Gonze, F. Goudreault,
B. Guster, G. Hautier, Xu He, N. Helbig, F. Jollet,
H. Miranda, F. Naccarato, R. Outerovitch, G. Petretto, N. Pike, Y. Pouillon, F. Ricci, M. Royo,
M. Schmitt, M. Stengel, M. Torrent, J. Van Bever, M. Verstraete, J. Zwanziger.

It is worth to read carefully all the modifications that are mentioned in the present file,
and examine the links to help files or test cases.
This might take some time ...

Xavier

### **A.** Important remarks and warnings.

**A.1** At the occasion of the switch from ABINITv8 to ABINITv9, many improvements of the formats and content of files written
    by ABINIT have been made, so the backward compatibility of ABINITv9 may be broken.

In particular:

1. The build system relies on new `.ac9` files (see [B.6](#v9.2.B.6)), superceeding the v8 `.ac` files.
   A bash script (`upgrade-build-config-file.sh`) located in the top level directory of the package can be used
   to convert from the old `.ac`format to `.ac9`.
2. The build system of ABINITv9 does not build anymore the hard dependencies (Linalg, NetCDF4, HDF5, LibXC, ...),
   as this was not sustainable (see [B.6](#v9.2.B.6)) and nowadays most users install prerequisite libraries themselves.
   See also the specialized INSTALL notes for
   [CentOS](/INSTALL_CentOS), [EasyBuild](/INSTALL_EasyBuild), [MacOS](/INSTALL_MacOS), and [Ubuntu](/INSTALL_Ubuntu).
3. The main ABINIT output file now contains sections written in YAML (sometimes replacing text sections, sometimes adding information).
   This means that some user-developed parsing tools might not work anymore,
   and should be adapted to the new ABINITv9 output file (see [B.9](#v9.2.B.9)).
   Note that the YAML output is still under development and modifications may appear in the next versions.
   A python API to extract the results of the calculation will be provided when the implementation is finalized.
4. Several default values have been changed, see [A.3](#v9.2.A.3).


**A.2**
A new account of the ABINIT effort has been published in Computer Phys. Comm. [[cite:Gonze2020]].
It provides description of several new features.
A version of this paper that is not formatted
for Computer Phys. Comm. [is also available](https://www.abinit.org/sites/default/files/ABINIT20.pdf).
The licence allows the authors to put it on the Web.

A second new account of the ABINIT effort has been published in J. Chem. Phys. [[cite:Romero2020]].
The scope of this second paper is different from the first one. It is more a survey of ABINIT,
focusing on its specific capabilities. Still, it contains also some description of some new features.
A version of this paper that is not formatted for J. Chem. Phys.
[is also available](https://www.abinit.org/sites/default/files/ABINIT20_JPC.pdf).
The licence allows the authors to put it on the Web.

Other specific publications are mentioned in the [Suggested acknowledgment page](/theory/acknowledgments).

<a name="v9.2.A.3"></a>
**A.3**  The default values of the following ABINIT input variables have been changed:
    [[ixcrot]], [[chneut]], [[ntime]], [[prtkden]], [[symsigma]] and [[tolsym]]. In particular the new default value
    of [[tolsym]], 1e-5, is more in line with the tolerances of other codes, so that for users of such
    codes, one barrier to the use of ABINIT is removed. By the same token, some bug in the recognition of symmetries
    has been fixed, when [[tolsym]] is close to the default, see the new tests [[test:v9_15]] and [[test:v9_16]].
    The new input variable [[chksymtnons]] has been introduced, to govern the possible automatic alignment
    of the [[tnons]] with the FFT grid (actually needed for GW calculations).
    By X. Gonze (MR 689 and others)

**A.4** The initialization of the wavefunctions when [[paral_kgb]]=1 and [[nspinor]]=2 has been changed, since the previous one could prevent the code to converge.
    By M Torrent (MR 562).

**A.5** The input variable xangst has been disabled. Use [[xcart]] instead, and specify the unit, namely Angstrom.

**A.6** Work is on-going concerning the Coulomb singularity treatment, see [D.32](#v9.2.D.32). The usage of the input variable [[icutcoul]] is changing.
For the time being, use [[gw_icutcoul]] instead.

**A.7** The name of the t-DEP main executable has been changed from `tdep` to `atdep`, in line with [[cite:Romero2020]].
By J. Bieder (MR 642, 641).

* * *

### **B.** Most noticeable achievements

**B.1** Electron-phonon interaction (mobilities in the self-energy relaxation time approximation,
temperature-dependent electronic band structures including the zero-point renormalization, etc.)

The new capabilities of ABINITv9 related to electron-phonon calculations are described
fully in the Sec. 3.3.2 of [[cite:Gonze2020]], as follows.

>   In abinit v9, it is possible to compute the EPH self-energy
>   in the Kohn–Sham representation using the EPH matrix
>   elements. The code employs optimized algorithms to compute
>   either the full self-energy (needed for QP corrections and spectral
>   functions) or just the imaginary part that is then used to evaluate
>   mobilities within the self-energy relaxation time approximation
>   (SERTA). The computation of the mobility is fully
>   integrated inside abinit, and is an automatic output of the
>   computation of the imaginary part of the self-energy, bypassing
>   the need to post-process results. When computing the full self-energy,
>   it is possible to reduce the number of empty states
>   required for convergence by using the first-order wavefunctions
>   obtained by solving the relevant Sternheimer equation.
>
>   In the case of lifetime computations, the code takes advantage of the
>   tetrahedron method to filter contributing q-points, a double-grid
>   integration technique to accelerate the convergence at marginal
>   additional computational cost, and samples the relevant regions
>   in the Brillouin zone contributing to transport properties thus
>   leading to a significant reduction of the computational effort.
>   Crystalline symmetries are used throughout the code in order to
>   reduce the number of k- and q-points that must be explicitly
>   included in the integrals. To achieve good parallel efficiently, the
>   most CPU demanding parts are parallelized with MPI employing a
>   distribution schemes over k/q-points, perturbations and bands (the
>   band level is available only when computing the full self-energy).

Moreover, the interpolation of the DFPT potential, described in Sec. 3.3.1 of [[cite:Gonze2020]] is fully operational,
with many tests, and three tutorials provided.

List of tests: [[test:v8_44]], [[test:v9_50]], [[test:v9_53]], [[test:v9_56]], [[test:v9_60]], [[test:v9_61]],
[[test:eph4mob_1]], [[test:eph4mob_2]], [[test:eph4mob_3]],
[[test:eph4mob_4]],
[[test:eph4mob_5]],
[[test:eph4mob_6]],
[[test:eph4mob_7]],
[[test:eph4zpr_1]],
[[test:eph4zpr_2]],
[[test:eph4zpr_3]],
[[test:eph4zpr_4]],
[[test:eph4zpr_5]],
[[test:eph4zpr_6]],
[[test:eph4zpr_7]].

New input variables: [[brav]], [[dvdb_add_lr]], [[dvdb_qcache_mb]], [[dvdb_qdamp]],
[[dvdb_rspace_cell]], [[eph_doping]],
[[eph_phrange]], [[eph_tols_idelta]], [[eph_ecutosc]], [[eph_restart]],
[[eph_stern]], [[eph_use_ftinterp]], [[eph_phwinfact]],
[[getdvdb]], [[getdvdb_filepath]], [[getkerange_filepath]],
[[getsigeph_filepath]],
[[irddvdb]], [[prteliash]], [[rifcsph]], [[sigma_bsum_range]], [[sigma_erange]],
[[sigma_ngkpt]], [[sigma_nshiftk]], [[sigma_shiftk]], [[symv1scf]].

Note that the new EPH processing unit of ABINIT [[optdriver]]=7 has a different implementation than the one implemented in anaddb.
Three new tutorials are availables, [[tutorial:eph_intro]], [[tutorial:eph4mob]] and [[tutorial:eph4zpr]], and supercede the legacy tutorials
[[tutorial:eph]] and [[tutorial:tdepes]].
For further details about the implementation and usage, please consult [[cite:Brunin2020b]].

By G. Brunin, H. Miranda, M. Giantomassi, G.-M. Rignanese, G. Hautier.

**B.2** Flexoelectricity and dynamical quadrupoles

A new driver has been included in abinit that allows one to compute
4 spatial dispersion tensorial quantities: the clamped-ion flexoelectric tensor, the dynamical quadrupoles,
the first moment of IFC matrix and the first moment of the piezoelectric force response tensor.
Precalculation of ground state, first and second (d2_dkdk) order response functions is required.
After execution, the driver creates a 3rd order energy derivative database file
that is used by anaddb to compute the mixed and lattice-mediated flexoelectric tensors
or to include the dipole-quadrupole and quadrupole-quadrupole electrostatic interactions
in the calculation of the dynamical matrix.

See the complementary description
in the Sec. V. D of [[cite:Romero2020]], with underlying theory and test calculations
presented in [[cite:Royo2019]].
At the practical level, see [[cite:Romero2020]]:

>   In this way, both perturbations are generalized to finite q, as
>   is already the case for atomic displacements. This enables us
>   to carry out an analytical third order derivative of the energy
>   with respect to two of the standard perturbations, and to the
>   momentum q, which directly provides the sought-after spatial
>   dispersion tensors. Remarkably, by virtue of the 2n+1 theorem,
>   the third-order energies are computed in one shot using
>   precalculated first-order response functions to the standard
>   perturbations, without the necessity of self-consistently computing
>   any response function to a perturbation gradient. After
>   execution, the long-wave DFPT routines generate a derivative
>   database that is subsequently used by post-processing tools
>   implemented in ANADDB to compute and print the different
>   contributions to the FxE tensor.

>   The dynamical quadrupoles are the spatial dispersion counterparts of the Born effective charges,
>   and can be used in lattice dynamics calculations
>   to improve the prevalent dipole-dipole treatment of the
>   long-range interactions. The ANADDB routines that carry
>   out the process of interpolating the dynamical matrix following
>   Ref. 34 have been adapted to incorporate the dipole-quadrupole
>   and quadrupole-quadrupole electrostatic interactions
>   derived in Ref. 102. This new functionality results in a
>   faster convergence of the phonon bands calculation with respect
>   to the density of q points and, in some materials, represents
>   the only route to obtain the correct sound velocities.

>   Currently, the implementation is restricted to the use of
>   norm-conserving pseudopotentials without non-linear core corrections, and the LDA
>   functional.

A tutorial is in preparation, with tests [[test:lw_1]] to [[test:lw_7]].

See the [[topic:longwave]]. The relevant input variable is [[optdriver]]==10.
New input variables have been defined: [[lw_flexo]], [[lw_qdrpl]], [[prepalw]], [[flexoflag@anaddb]],
[[dipquad@anaddb]], [[quadquad@anaddb]].

This capability is still under development and not completely stable.
Interested users are strongly recommended to contact Miquel Royo (mroyo@icmab.es)
or Massimiliano Stengel (mstengel@icmab.es) before start using it.

By M. Royo, M. Stengel


**B.3** DFT+DMFT

The new capabilities of ABINITv9 related to DFT+DMFT calculations are described
fully in the Sec. 3.7 of [[cite:Gonze2020]], as follows.

>   The DFT+DMFT parallelism was improved for large
>   systems. In particular, it is now possible to parallelize the calculation
>   on both k-points and bands/g-vectors by using the input
>   variable [[paral_kgb]] = 1 and related input variables.
>
>   Two new approaches to CT-QMC have been added to
>   solve the AIM. In the first one, the density–density CT-QMC code
>   available in abinit [[cite:Gonze2016]], [[cite:Bieder2014]] was generalized in order to take into
>   account off-diagonal elements of the hybridization function. This
>   implementation is activated with the input variable [[dmft_solv]]
>   = 8. Spin–orbit coupling calculations are possible, but using a
>   real valued imaginary time hybridization function. This solver was
>   used in Refs. [[cite:Amadon2015]], [[cite:Amadon2016]].
>
>   In the second approach, we use the Toolbox for Research
>   on Interacting Quantum System (TRIQS)library [[cite:Parcollet2015]], which is an
>   open-source project that provides a framework for many-body
>   quantum physics and more specifically for strongly-correlated
>   electronic systems. TRIQS provides an open source implementation
>   of the continuous-time hybridization expansion quantum
>   impurity solver (CT-HYB) [[cite:Seth2016]], considered a state-of-the art
>   solver for multi-orbital AIM. An interface between abinit and
>   the impurity solver TRIQS/CT-HYB is now available and will make
>   use of the independent progress made by the TRIQS library. <...>

Also, the DMFT k-resolved spectral function is available (MR 529, 490).

List of tests: [[test:paral_84]], [[test:paral_86]], [[test:paral_99]], [[test:v8_01]].
New input variables: [[dmft_charge_prec]] and [[dmft_kspectral_func]] (test to be provided for the latter).
Also [[dmft_occnd_imag]], but only for keeping backward compatibility for tests.

By T. Cavignac, B. Amadon and O. Gingras.


**B.4** Spin model within Multibinit

The new capabilities of Multibinit within ABINITv9 are described
fully in the Sec. 4.1 of [[cite:Gonze2020]]. See also Sec. [D.1](#v9.2.D.1).
In particular, a spin model, described specifically in Sec. 4.1.2 of [[cite:Gonze2020]], is available, as follows.

>Multibinit implements the most commonly used model for spin systems,
>via the Heisenberg Hamiltonian including magnetic exchange and Dzyaloshinskii Moriya interactions.
>Single ion anisotropy and dipole–dipole interactions are also included,
>and all terms bear a very strong similarity to the quadratic part of the lattice model Hamiltonian.
>A number of open source spin dynamics codes already exist, such as UPPASD, VAMPIR, OOMF;
>the distinguishing features of multibinit are the integration with abinit,
>to fit parameters, and the simultaneous dynamics with other
>degrees of freedom (in particular using the inter-atomic force constants).

A tutorial for the multibinit spin model has been written, [[tutorial:spin_model]].

Many new input variables are present, tested and documented:
[[slc_coupling@multibinit|slc_coupling]],
[[spin_calc_thermo_obs@multibinit|spin_calc_thermo_obs]],
[[spin_damping@multibinit|spin_damping]],
[[spin_init_orientation@multibinit|spin_init_orientation]],
[[spin_init_qpoint@multibinit|spin_init_qpoint]],
[[spin_init_rotate_axis@multibinit|spin_init_rotate_axis]],
[[spin_init_state@multibinit|spin_init_state]],
[[spin_ntime_pre@multibinit|spin_ntime_pre]],
[[spin_projection_qpoint@multibinit|spin_projection_qpoint]],
[[spin_sia_add@multibinit|spin_sia_add]],
[[spin_sia_k1amp@multibinit|spin_sia_k1amp]],
[[spin_sia_k1dir@multibinit|spin_sia_k1dir]],
[[spin_temperature_start@multibinit|spin_temperature_start]],
[[spin_temperature_end@multibinit|spin_temperature_end]],
[[spin_temperature_nstep@multibinit|spin_temperature_nstep]],
[[spin_var_temperature@multibinit|spin_var_temperature]],
[[spin_write_traj@multibinit|spin_write_traj]].

List of tests in addition to those of the tutorial: [[test:v8_16]], [[test:v8_23]], [[test:v9_81]], [[test:v9_82]],
[[test:v9_86]], [[test:v9_87]].

By Xu He, N. Helbig, J. Bieder, E. Bousquet, Ph. Ghosez, M. Verstraete

<a name="v9.2.B.5"></a>
**B.5** Constrained DFT

Constrained Density-Functional Theory (see [[topic:ConstrainedDFT]]) is available,
with a new algorithm allowing to impose the constraints to arbitrary precision,
whether it relates to the charge, magnetization, magnetization direction, or magnetisation size,
or a combination thereof for different atoms. The constraints are smeared spherical integrals
with ajustable sphere radius, centered on atoms. The algorithms has been demonstrated for norm-conserving pseudopotentials
as well as PAW. Forces and derivatives with respect to the constraints
are available (i.e. magnetic torque for the non-collinear spin case).
Stresses are still to be coded, will be available in ABINITv9.4 or ABINITv9.6.

New tests: v8#24-29, v8#95-97 and v9#1-3.
New input variables: [[chrgat]], [[constraint_kind]], [[ratsm]].

By X. Gonze.

<a name="v9.2.B.6"></a>
**B.6** Large modifications of the build system

The build system relies on new <hostname>.ac9 files, superceeding the v8 <hostname>.ac files.
Fully documented example files can be found in doc/build/config-examples.
A bash script (`upgrade-build-config-file.sh`) located in the top level directory of the package can be used
   to convert from the old `.ac`format to `.ac9`.

The build system of ABINITv9 does not build anymore the (hard and soft) dependencies (Linalg, NetCDF4, HDF, LibXC, Wannier90, ...), as this was not sustainable.
Three libraries are now mandatory: linalg, NetCDF4/HDF5 and LibXC. Failing to link to them will prevent building ABINIT.
The other libraries are optional, there will only be a warning if they are not available.
If the user does not provide the path to these libraries,
the build system will try to find them in the "usual" directories, and inform the user that it has done so.
The build system also can make suggestions to the user, to complete its *.ac9 file.

Specialized INSTALL notes are available to help the user for
[CentOS](/INSTALL_CentOS), [EasyBuild](/INSTALL_EasyBuild), [MacOS](/INSTALL_MacOS), and [Ubuntu](/INSTALL_Ubuntu).

By Y. Pouillon and JM Beuken

**B.7** New command line interface

There is a new (**recommended**) command line interface to run ABINIT, without the "files" file.
The new syntax is:

    abinit run.abi

or

    abinit run.abi > run.log 2> run.err &

where `run.abi` is the Abinit input file that now provides all the information related to pseudos
and the prefixes that were previously passed via the "files" file. For comparison, the old syntax is

    abinit < run.files > run.log 2> run.err &      ! This is the old syntax

A file extension for the input file is highly recommended (in this example we use `.abi`)
as by default the parser will use the string before the file extension as root to build the prefixes
for the input/output/temporary files.

The user can specify the name of the main output file thanks to the [[output_file]] input variable,
the list of pseudopotentials thanks to the [[pseudos]] input variable and the directory where
all pseudos are located with [[pp_dirpath]].
The prefix for other input, output or temporary files can be specified with [[indata_prefix]], [[outdata_prefix]] and
[[tmpdata_prefix]], respectively.
A default set of prefixes computed from the basename of the input file is used if
these variables are not specified in the input.

For some examples, see tests [[test:v8_90]], [[test:v7_45]], and [[test:v5_54]]. See also [[topic:Control]].

A similar command line interface can also be used for the anaddb code.
In this case, the relevant variables are:
[[output_file@anaddb]], [[ddb_filepath@anaddb]], [[ddk_filepath@anaddb]], [[gkk_filepath@anaddb]], [[eph_prefix@anaddb]].

The new syntax is:

    anaddb run.in > run.log 2> run.err &

See tests [[test:v8_52]] for a standard analysis of the DDB file and
[[test:v7_94]] for the (old implementation) of electron-phonon calculations in anaddb.

!!! important

    The old "files file" interface is still operational although deprecated and will be **REMOVED** in Abinit v10.


By M. Giantomassi (MR 586).


**B.8** Reading strings from the input file

A new mechanism to read strings enclosed between **double quotation marks** from the input file has been activated.
So, many new input keywords are reading strings as data, and, often, can be used alternatively to similar input keywords
that were expecting numerical values such as the `get*` and `ird*` variables.
The goal is to encourage a new approach for performing ABINIT calculations in which multiple datasets
and `get*` variables are replaced by independent input files that are connected together via file paths.

List of new input variables that rely on this feature:

- [[getddb_filepath]], an alternative to [[getddb]] or [[irdddb]], see test [[test:v9_60]]
- [[getden_filepath]], an alternative to [[getden]] or [[irdden]], see test [[test:v8_36]] and [[test:v8_41]]
- [[getscr_filepath]], an alternative to [[getscr]] or [[irdscr]], see test [[test:v67mbpt_51]]
- [[getwfkfine_filepath]], an alternative to [[getwfkfine]] or [[irdwfkfine]], see tests [[test:v9_55]], [[test:v9_56]]
- [[getwfk_filepath]], an alternative to [[getwfk]] or [[irdwfk]], see test [[test:v9_60]]
- [[getwfq_filepath]], an alternative to [[getwfq]] or [[irdwfq]], see test [[test:v7_98]]
- [[getkerange_filepath]], see test [[test:v9_60]]
- [[getpot_filepath]], see test [[test:v8_44]]
- [[pseudos]], [[indata_prefix]], [[outdata_prefix]], [[tmpdata_prefix]], [[output_file]], see test [[test:v9_04]]
- [[pp_dirpath]]: Used in all the input files of the Test Suite
- [[output_file@anaddb]], [[ddb_filepath@anaddb]], [[gkk_filepath@anaddb]], [[eph_prefix@anaddb]].
  See tests [[test:v8_52]] for a standard analysis of the DDB file and
  [[test:v7_94]] for the (old implementation) of electron-phonon calculations in anaddb.

By M. Giantomassi

<a name="v9.2.B.9"></a>
**B.9** YAML sections in the output file

YAML sections are now generated in the output file, sometimes replacing text sections, sometime providing new information.
At present there is a YAML section for the components of the total energy, the GS results including forces and stresses as well as a YAML section for GW calculations, and some YAML sections giving information about the iteration status.
<!--
Example of tests: paral#86, v67mbpt#2. See the input variable use_yaml (TO BE DOCUMENTED).
-->
At the occasion of the development of this capability, and its adaptation to the test farm, the
perl script fldiff.pl has been replaced by a Python version.
See related information in Sec. 5.5 of [[cite:Gonze2020]].

By T. Cavignac, M. Giantomassi, GM Rignanese, X Gonze.


**B.10** New approach to define crystalline structures in the Abinit input

The new variable [[structure]] can be used to initialize the lattice vectors
and the atomic positions **from an external file**.
Variables such as [[natom]], [[ntypat]], [[typat]] and [[znucl]] are automatically initialized
and need not to be specified in the ABINIT input.
At present, the code can read netcdf files produced by ABINIT (`GSR.nc`, `WFK.nc`, `DEN.nc`, `HIST.nc`)
and POSCAR files in VASP-5 format.
See the documentation for the syntax and limitations.

By M. Giantomassi.


**B.11** New capabilities of abipy and abiflows

The abipy and abiflows projects have been significantly extended.
See Sec. 6 of [[cite:Gonze2020]], as well as the [gallery of plotting scripts](http://abinit.github.io/abipy/gallery/index.html) &nbsp;
and the [gallery of abipy workflows](http://abinit.github.io/abipy/flow_gallery/index.html) &nbsp;.

By M. Giantomassi, G. Petretto, F. Naccarato.


* * *

### **C.** Changes for the developers (including information about compilers)

**C.1** A python script to help ABINIT developers and development.

The new python script abisrc.py located in the top directory of the ABINIT package has been developed.
It has superceded abilint.py in the makemake procedure.

Try

    ./abisrc.py --help

then follow the suggestions, to get info about files, directories, interfaces, to visualize
dependencies of the ABINIT subroutines, etc.

Note that there are dependencies of abisrc.py, to be installed prior being able to use some of its capabilities.
Use:

    pip install -r requirements.txt --user

to install the dependencies in user mode.

By M. Giantomassi.

**C.2** New characteristics of input variables

In the description of input variables (e.g. files abimkdocs/variables_abinit.py), a new field `added_in_version` has been introduced,
for example,

     added_in_version="9.2.0"

or, for variables introduced prior to v9,

     added_in_version="before_v9"

**C.2** Test farm: new and obsolete bots

* Bots introduced in the test farm: alps_nag_7.0_openmpi, atlas_gnu_9.1_openmpi, buda2_gnu_8.2_cuda, cronos2_gnu_7.4_paral,
    scope_gnu_10.2_mpich, scope_gnu_7.5_dep.
* Bots upgraded: abiref_gnu_5.3_* to abiref_gnu_9.2_*; bob_gnu_5.3_openmp to bob_gnu_7.5_openmp; buda2_gnu_8.1_mpich3
    to buda2_gnu_8.2_mpich3; graphene_gnu_6.4_macports to graphene_gnu_9.2_macports; max2_gnu_5.3_* to max2_gnu_6.5_*; ubu_gnu_5.3_openmpi to ubu_gnu_9.2_openmpi.
* Bots removed: abiref_nag_6.2_openmpi (superceded by alps_nag_7.0_openmpi), atlas_gnu_7.2_fb.ac (no nightly test of the fallbacks anymore), cronos_gnu_5.3_paral(replaced by cronos2), inca_gnu_6.3_py3k (inca too old), tikal_gnu_* (tikal too old).
Working also on a cronos-cronos2 cluster.

By JM Beuken

**C.3** Supported compilers

* gfort (GNU) compiler: v9 newly supported, v4 obsolete
* ifort (INTEL) compiler: v19 newly supported.
* NAG 7.0 instead of 6.2

By JM Beuken

**C.4** Unitary ttransposer#1 . Test of the transposer for linear algebra to KGB parallelisation.

By J. Bieder.

**C.5** Linkchecker has been reenabled, only for internal link checking.

By JM Beuken (MR 513).

**C.6** Enable the generation of a HTML side-by-side diff on the test farm when fldiff fails with a line count error and it was not caused by a crash of Abinit. The diff algorithms uses a specialized heuristic to improve line synchronization and prevent weird matching.

By Th. Cavignac (MR 526)

**C.7** Split of the source tree (ongoing).

In order to improve modularity, the source tree must be split in two parts, one for low-level routines, largely independent of ABINIT,
and one for more specific routines to ABINIT. The low-level routines should become a separate library, with its own build system and make.
At present the low-level library have been moved out of src, inside the shared/common/src directory.
See related information in Sec. 5.4 of [[cite:Gonze2020]].

**C.8** New FFT specifications for the build system
See https://gitlab.abinit.org/pouillon/abinit/-/issues/33 .

By Y. Pouillon (MR 619)

**C.9** Reorganisation of the tests/Psps_for_tests directory, in preparation of the beautification.
Clarification on which pseudopotentials are of recent format, and which are legacy pseudopotentials.
Preparation of beautification.

By X. Gonze (MR 683)

* * *

<a name="v9.2.D.1"></a>
### **D.**  Other changes (or on-going developments, not yet finalized)

**D.1**  Miscellaneous improvements of Multibinit (lattice part)

Miscellaneous improvements have been made to the lattice part of Multibinit.
See the new input variables below, also see the Sec. 4.1.1 of [[cite:Gonze2020]].

New tests: [[test:v8_38]], [[test:v8_94]], [[test:v8_98]], [[test:v8_99]],
[[test:v8_101]],
[[test:v8_102]],
[[test:v8_103]],
[[test:v9_83]],  [[test:v9_84]], [[test:v9_85]].
New input variables are listed below.
Not all these new input variables are present in automatic tests, though.

- [[analyze_anh_pot@multibinit|analyze_anh_pot]]
- [[dyn_chksym@multibinit|dyn_chksym]]
- [[dyn_tolsym@multibinit|dyn_tolsym]]
- [[fit_efs@multibinit|fit_EFS]]
- [[fit_iatom@multibinit|fit_iatom]]
- [[fit_spc_maxs@multibinit|fit_SPC_maxS]]
- [[latt_friction@multibinit|latt_friction]]
- [[latt_taup@multibinit|latt_taup]] NOT TESTED
- [[latt_taut@multibinit|latt_taut]]
- [[opt_coeff@multibinit|opt_coeff]]
- [[opt_effpot@multibinit|opt_effpot]]
- [[opt_ncoeff@multibinit|opt_ncoeff]]
- [[sel_efs@multibinit|sel_EFS]]
- [[strfact@multibinit|strfact]]
- [[test_effpot@multibinit|test_effpot]]
- [[test_prt_ph@multibinit|test_prt_ph]]
- [[ts_option@multibinit|ts_option]] NOT TESTED

Finally, several input variables of the main ABINIT application are also reused for Multibinit, without modification
of meaning, like [[iatfix]], [[natfix]] (and related similar input variables), and also [[tolmxf]].

By M. Schmitt, Xu He, F. Ricci, M. Verstraete, Ph. Ghosez


**D.2** Miscellaneous improvements in the Chern number and orbital magnetization calculations,
including parallelization over k points of the Chern number calculation.

By J. Zwanziger (MR 469, 500, 545, 588)

**D.3** Calculation of Debye-Waller tensor. New test [[test:v8_58]].

By M. Giantomassi

**D.4** Speed-up of susceptibility matrix calculations and GW analytic continuation calculations.
See the new input variable [[gwaclowrank]] and new test [[test:v9_32]].

By F. Bruneval (MR 687).

**D.5** NCPP Wavefunction mixing with Variational Energy
and minor improvements to prepare PAW+Hybrid variational energy.
New test [[test:v7_73]], simple system for testing Hartree-Fock and the SCF algorithms.

By X. Gonze (MR 434, 444, 445).

**D.6** New weight distribution of the Fourier components of the force constants.
Test tolerance in the new integration weights, tests [[test:v8_52]], [[test:v8_53]], [[test:v8_54]].

By H. Miranda and M. Giantomassi

**D.7** Test calculation of velocity matrix elements (DDK) with
 optdriver 8 and [[wfk_task]] "wfk_ddk”, see [[test:v8_59]].

By M. Giantomassi

**D.8** Upgraded [[tutorial:paral_gspw]], new version of auto paral (with threads)

By M. Torrent (MR502).

**D.9** Test wannier90 interface with [[nsppol]]=2 and [[nspden]]=2, [[test:wannier90_04]].

By Xu He

**D.10** Mixed precision for FFT transforms. New input variable [[mixprec]]
see [[test:v8_44]], [[test:v9_57]], [[test:v9_60]], and [[test:v9_61]].

From M. Giantomassi (MR491).

**D.11** Multibinit has been interfaced with [scale-up](https://www.secondprinciples.unican.es).

By Marcus Schmitt, Jordan Bieder, Matthieu Verstraete and Philippe Ghosez

**D.12** The following units are now also allowed in input files:

- S Sec Second  for the ABINIT input file;
- nm (for nanometer)  for the ABINIT and ANADDB input files.

**D.13** a-TDEP utility:
added [[guide:a-TDEP|a-TDEP user guide]],
[[topic:a-TDEP|a-TDEP topic]], and corresponding input variable documentation.
References: [[pdf:a-TDEP_Paper|a-TDEP paper]].
Also, see Sec. 4.2 of [[cite:Gonze2020]].

By F. Bottin, J. Bouchet, J. Bieder (MR491,422).

**D.14** Improvements of NLO calculations.
Optimize memory allocation. In particular for [[usepead]] = 1.
Write Raman susceptibilities to netcdf in anaddb.

By G. Petretto  (MR 599).

**D.15** MetaGGA + PAW in progress.
All internal and unit tests are OK. The implementation seems close to the end. Still need to perform physical tests.

By M. Torrent and J.-B. Charraud (MR 587, 558, 625).

**D.16** Implementation of an alternate version of MPI reductions valid when the number of data exceeds 2^32.

By M. Torrent (MR 587)

**D.17** Build system and src support for llvm (clang/flang) and ARM (armflang/armclang/armpl).

By M Torrent (MR 571)

**D.18** Improvement of core WF reading (optics + electron-positron)

By M Torrent (MR 557)

**D.19** Fix bootstrap kernel convergence issues

By Wei Chen (MR 546)

**D.20** Upgrade versions of libraries used with ABINIT.
Upgrade atompaw to 4.1.0.6. Upgrade Libxc to 4+. Prepare the interface to LibXC 5 and a bit of LibXC 6.

By M. Torrent, JM Beuken (MR 649, 532, 470, 465, 441)

**D.21** Write yaml file for fatbands (phonopy format) with a-TDEP

By J. Bieder (MR510)

**D.22** Write polarization vectors in GSR.nc.

By H. Miranda (MR 462).

**D.23** Updated user interface of Raman_spec.py . Added one section to tutorial NLO to describe use of Raman_spec.py.

By N. Pike (MR 460, MR 581)

**D.24** XML format for core wavefunctions

By F. Jollet (MR 423)

**D.25** Wavefunction prediction for molecular dynamics.

By F. Jollet (MR 412)

**D.26** Added a preview for the toptic_4.abi file in the optic tutorial.

By F. Goudreault (MR 408)

**D.27** Improve ELPA detection; make abinit compatible with ELPA2019

By M. Torrent (MR 626)

**D.28** Upgrade of [[tutorial:base3]] and [[tutorial:basepar]].

By X. Gonze (MR628)

**D.29** New input variable [[prtprocar]], see test [[test:v5_40]].

By M. Verstraete (MR630)

**D.30** The ABINIT input variable [[supercell_latt]] is documented.
See also [[test:v8_94]].

By X. Gonze

**D.31** New implementation of the cRPA (to compute U and J in DFT+U or DFT+DMFT)
that can consider any subset of orbitals in the system.
Compilation was a bit long on certain compilers, so new keywords have been added to the build system:
enable_crpa_optim and enable_crpa_no_optim

By R. Outerov and B. Amadon (MR622).

<a name="v9.2.D.32"></a>
**D.32** On-going work on refactoring the Coulomb interaction part of ABINIT.

New input variables [[fock_icutcoul]], and [[gw_icutcoul]], that should superceed [[icutcoul]].
New test added for the mini-Brillouin Zone integration, [[gw_icutcoul]]=14, 15, 16, see [[test:v9_21]].

By B. Guster, M. Giantomassi, F. Bruneval and X. Gonze (MR 627, 633, 673, 679, 686).

**D.33** New TB2J python script to compute the superexchange (J) and the Dzyaloshinskii-Moriya (DMI) interactions. The script can be found in [http://gitlab.abinit.org/xuhe/TB2J](http://gitlab.abinit.org/xuhe/TB2J) with doc and tutorials. The script is interfaced with wannier90 and use the w90 output files. The J calculation works in production, the DMI is much more sensitive to disentanglement noise and have to be use with care. An article is under construction to describe the method and its implementation. The script can deliver input data file for the spin model of Multibinit.

By He Xu, M. Verstraete and E. Bousquet (MR 639).

**D.34** Test linear electro-optical coefficient, tutorespfn [[test:optic_5]].

By N. Pike (MR 575).


**D.35** Optic: print spin-decomposition when applicable.
Fixed reflectivity screwed results. Test more thoroughly optics (incl. interfacing using NetCDF),
see new tests [[test:v9_05]] to [[test:v9_12]], also [[test:v9_47]] and [[test:v9_48]]

By X. Gonze (MR 654, 674).

**D.36** Fixed DFPT+PAW+GGA+usexcnhat=1+q<> bug, and also add new related tests
[[test:v9_41]] and [[test:v9_42]].

By M. Torrent and X. Gonze (MR 657).

**D.37** Update PAW tutorial

By F. Jollet (MR 643, 645, 652, 653).

**D.38** Nonlinear xc preliminary

Preliminary work for some changes in the exchange correlation terms in nonlinear (3rd order DFPT).
One term was implemented in both pead and dfptnl routines. Now it is merged in one routine (dfptnl_exc3).
Due to a subtle reordering of nonlinear core correction terms, some pead refs are changed, but the final result ("First order change in electronic dielectric susceptibility tensor") remain the same.

By L. Baguet (MR 650).

**D.39** Anaddb output data prefix

The former writing of anaddb.nc (fixed path and fixed name) has been made more flexible, by
introducing a prefix

By J. Bieder and He Xu (MR702)

**D.40** New "macro" input variable [[expert_user]]

When non-zero [[expert_user]] automatically switch off all checks done by [[chkprim]], [[chkdilatmx]], [[chksymbreak]] and [[chksymtnons]].

By X. Gonze (MR715)


**D.41** Miscellaneous additional bug fixes, improvements of documentation including for the build system.
G. Antonius, L. Baguet, JM Beuken, J. Bieder, E. Bousquet, F. Bruneval, T. Cavignac, M. Giantomassi, X. Gonze,
F. Jollet, R. Outerovitch, N. Pike, Y Pouillon, M. Royo, M. Torrent, J. Van Bever, M. Verstraete, Xu He.

* * *

## v9.0

Version 9.0, released on March 29, 2020.
List of changes with respect to version 8.10.

Many thanks to the contributors to the ABINIT project between
October 2018 and March 2020. These release notes
are relative to modifications/improvements of ABINIT v9.0 with respect to v8.10
(merge requests up to, and including, MR636 are taken into account)

The list of contributors includes:
B. Amadon, L. Baguet, J.-M. Beuken, J. Bieder, J. Bouchet, E. Bousquet, F. Bruneval, G. Brunin, Wei Chen,
J.-B. Charraud, Ph. Ghosez, M. Giantomassi, O. Gingras, X. Gonze, F. Goudreault,
B. Guster, G. Hautier, Xu He, N. Helbig, F. Jollet,
H. Miranda, F. Naccarato, R. Outerov, G. Petretto, N. Pike, Y. Pouillon, F. Ricci, M. Royo,
M. Schmitt, M. Stengel, M. Torrent, J. Van Bever, M. Verstraete, J. Zwanziger.

It is worth to read carefully all the modifications that are mentioned in the present file,
and examine the links to help files or test cases.
This might take some time ...

Xavier

### **A.** Important remarks and warnings.

**A.1** At the occasion of the switch from ABINITv8 to ABINITv9, many improvements of the formats and content of files written
    by ABINIT have been made, so the backward compatibility of ABINITv9 may be broken.
    The present ABINITv9.0 is NOT to be considered a production version. It is a beta release, allowing developers to get feedback
    from the users. Many features will work correctly, of course. Still, beginners are advised
    to stick to ABINITv8.10.3 except if ABINITv8.10.3 is not appropriate (or not working) for them.

In particular:

1. The build system relies on new `.ac9` files (see [B.6](#v9.0.B.6)), superceeding the v8 `.ac` files.
   A bash script (`upgrade-build-config-file.sh`) located in the top level directory of the package can be used
   to convert from the old `.ac`format to `.ac9`.
2. The build system of ABINITv9 does not build anymore the hard dependencies (Linalg, NetCDF4, HDF5, LibXC, ...),
as this was not sustainable (see [B.6](#v9.0.B.6)) and nowadays most users install prerequisite libraries themselves.
3. The main ABINIT output file now contains sections written in YAML (sometimes replacing text sections, sometimes adding information).
    This means that some user-developed parsing tools might not work anymore, and should be adapted to the new ABINITv9 output file (see [B.9](#v9.0.B.9)). Note that the YAML output is still under development and modifications may appear in the next versions. A python API to extract the results of the calculation will be provided when the implementation is finalized.
4. Several default values have been changed, see [A.3](#v9.0.A.3).


**A.2**
A new account of the ABINIT effort has been published in Computer Phys. Comm. [[cite:Gonze2020]]
It provides description of several new features.
A version of this paper that is not formatted for Computer Phys. Comm.
[is also available](https://www.abinit.org/sites/default/files/ABINIT20.pdf).
The licence allows the authors to put it on the Web.

A second new account of the ABINIT effort has been published in J. Chem. Phys. [[cite:Romero2020]].
The scope of this second paper is different from the first one. It is more a survey of ABINIT,
focusing on its specific capabilities. Still, it contains also some description of some new features.
A version of this paper that is not formatted for J. Chem. Phys.
[is also available](https://www.abinit.org/sites/default/files/ABINIT20_JPC.pdf).
The licence allows the authors to put it on the Web.

Other specific publications are mentioned in the [Suggested acknowledgment page](/theory/acknowledgments).

<a name="v9.0.A.3"></a>
**A.3**  The default values of the following ABINIT input variables have been changed:
    [[ixcrot]], [[chneut]], [[ntime]], [[symsigma]], [[prtkden]].

**A.4** The initialization of the wavefunctions when [[paral_kgb]]=1 and [[nspinor]]=2 has been changed, since the previous one could prevent the code to converge.
    By M Torrent (MR 562).

**A.5** The input variable xangst has been disabled. Use xcart instead, and specify the unit, namely Angstrom.

**A.6** The name of the t-DEP main executable has been changed from `tdep" to `atdep`, in line with [[cite:Romero2020]].

* * *

### **B.** Most noticeable achievements

**B.1** Electron-phonon interaction (mobilities in the self-energy relaxation time approximation,
temperature-dependent electronic band structures including the zero-point renormalization, etc.)

The new capabilities of ABINITv9 related to electron-phonon calculations are described
fully in the Sec. 3.3.2 of [[cite:Gonze2020]], as follows.

>   In abinit v9, it is possible to compute the EPH self-energy
>   in the Kohn–Sham representation using the EPH matrix
>   elements. The code employs optimized algorithms to compute
>   either the full self-energy (needed for QP corrections and spectral
>   functions) or just the imaginary part that is then used to evaluate
>   mobilities within the self-energy relaxation time approximation
>   (SERTA). The computation of the mobility is fully
>   integrated inside abinit, and is an automatic output of the
>   computation of the imaginary part of the self-energy, bypassing
>   the need to post-process results. When computing the full self-energy,
>   it is possible to reduce the number of empty states
>   required for convergence by using the first-order wavefunctions
>   obtained by solving the relevant Sternheimer equation.
>
>   In the case of lifetime computations, the code takes advantage of the
>   tetrahedron method to filter contributing q-points, a double-grid
>   integration technique to accelerate the convergence at marginal
>   additional computational cost, and samples the relevant regions
>   in the Brillouin zone contributing to transport properties thus
>   leading to a significant reduction of the computational effort.
>   Crystalline symmetries are used throughout the code in order to
>   reduce the number of k- and q-points that must be explicitly
>   included in the integrals. To achieve good parallel efficiently, the
>   most CPU demanding parts are parallelized with MPI employing a
>   distribution schemes over k/q-points, perturbations and bands (the
>   band level is available only when computing the full self-energy).

Moreover, the interpolation of the DFPT potential, described in Sec. 3.3.1 of [[cite:Gonze2020]] is fully operational,
with many tests provided.

List of tests: [[test:v9_50]], [[test:v9_61]] and [[test:v8_44]].

New input variables: [[dvdb_qcache_mb]],
[[eph_phrange]], [[eph_tols_idelta]], [[eph_ecutosc]], [[eph_restart]],
[[eph_stern]], [[eph_use_ftinterp]],
[[getdvdb]], [[getdvdb_filepath]], [[getkerange_filepath]],
[[irddvdb]], [[prteliash]], [[sigma_bsum_range]], [[sigma_erange]],
[[sigma_ngkpt]], [[sigma_nshiftk]], [[sigma_shiftk]], [[symv1scf]].

Note that the new EPH processing unit of ABINIT [[optdriver]]=7 has a different implementation than the one implemented in anaddb.
A new set of tutorials are in preparation and they will be made available in the forthcoming versions.
For further details about the implementation, please consult this [preprint](https://arxiv.org/abs/2002.00630).

By G. Brunin, H. Miranda, M. Giantomassi, G.-M. Rignanese, G. Hautier.

**B.2** Flexoelectricity and dynamical quadrupoles

A new driver has been included in abinit that allows one to compute
4 spatial dispersion tensorial quantities: the clamped-ion flexoelectric tensor, the dynamical quadrupoles,
the first moment of IFC matrix and the first moment of the piezoelectric force response tensor.
Precalculation of ground state, first and second (d2_dkdk) order response functions is required.
After execution, the driver creates a 3rd order energy derivative database file
that is used by anaddb to compute the mixed and lattice-mediated flexoelectric tensors
or to include the dipole-quadrupole and quadrupole-quadrupole electrostatic interactions
in the calculation of the dynamical matrix.

See the complementary description
in the Sec. V. D of [[cite:Romero2020]], with underlying theory and test calculations
presented in [[cite:Royo2019]].

At the practical level, see [[cite:Romero2020]]:

>   In this way, both perturbations are generalized to finite q, as
>   is already the case for atomic displacements. This enables us
>   to carry out an analytical third order derivative of the energy
>   with respect to two of the standard perturbations, and to the
>   momentum q, which directly provides the sought-after spatial
>   dispersion tensors. Remarkably, by virtue of the 2n+1 theorem,
>   the third-order energies are computed in one shot using
>   precalculated first-order response functions to the standard
>   perturbations, without the necessity of self-consistently computing
>   any response function to a perturbation gradient. After
>   execution, the long-wave DFPT routines generate a derivative
>   database that is subsequently used by post-processing tools
>   implemented in ANADDB to compute and print the different
>   contributions to the FxE tensor.

>   The dynamical quadrupoles are the spatial dispersion counterparts of the Born effective charges,
>   and can be used in lattice dynamics calculations
>   to improve the prevalent dipole-dipole treatment of the
>   long-range interactions. The ANADDB routines that carry
>   out the process of interpolating the dynamical matrix following
>   Ref. 34 have been adapted to incorporate the dipole-quadrupole
>   and quadrupole-quadrupole electrostatic interactions
>   derived in Ref. 102. This new functionality results in a
>   faster convergence of the phonon bands calculation with respect
>   to the density of q points and, in some materials, represents
>   the only route to obtain the correct sound velocities.

>   Currently, the implementation is restricted to the use of
>   norm-conserving pseudopotentials without non-linear core corrections, and the LDA
>   functional.

A tutorial is in preparation, with tests [[test:lw_1]] to [[test:lw_7]],
as well as a specific topic.

New input variables have been defined: [[lw_flexo]], [[lw_qdrpl]], [[prepalw]], [[flexoflag@anaddb]],
[[dipquad@anaddb]], [[quadquad@anaddb]].

This capability is still under development and not completely stable.
Interested users are strongly recommended to contact Miquel Royo (mroyo@icmab.es)
or Massimiliano Stengel (mstengel@icmab.es) before start using it.

By M. Royo, M. Stengel, M. Giantomassi.


**B.3** DFT+DMFT

The new capabilities of ABINITv9 related to DFT+DMFT calculations are described
fully in the Sec. 3.7 of [[cite:Gonze2020]], as follows.

>   The DFT+DMFT parallelism was improved for large
>   systems. In particular, it is now possible to parallelize the calculation
>   on both k-points and bands/g-vectors by using the input
>   variable [[paral_kgb]] = 1 and related input variables.
>
>   Two new approaches to CT-QMC have been added to
>   solve the AIM. In the first one, the density–density CT-QMC code
>   available in abinit [[cite:Gonze2016]], [[cite:Bieder2014]] was generalized in order to take into
>   account off-diagonal elements of the hybridization function. This
>   implementation is activated with the input variable [[dmft_solv]]
>   = 8. Spin–orbit coupling calculations are possible, but using a
>   real valued imaginary time hybridization function. This solver was
>   used in Refs. [[cite:Amadon2015]], [[cite:Amadon2016]].
>
>   In the second approach, we use the Toolbox for Research
>   on Interacting Quantum System (TRIQS)library [[cite:Parcollet2015]], which is an
>   open-source project that provides a framework for many-body
>   quantum physics and more specifically for strongly-correlated
>   electronic systems. TRIQS provides an open source implementation
>   of the continuous-time hybridization expansion quantum
>   impurity solver (CT-HYB) [[cite:Seth2016]], considered a state-of-the art
>   solver for multi-orbital AIM. An interface between abinit and
>   the impurity solver TRIQS/CT-HYB is now available and will make
>   use of the independent progress made by the TRIQS library. <...>

Also, the DMFT k-resolved spectral function is available (MR 529, 490).

List of tests: [[test:paral_84]], [[test:paral_86]], [[test:paral_99]], [[test:v8_01]].
New input variables: [[dmft_charge_prec]] and [[dmft_kspectral_func]] (test to be provided for the latter).
Also [[dmft_occnd_imag]], but only for keeping backward compatibility for tests.

By T. Cavignac, B. Amadon and O. Gingras.


**B.4** Spin model within Multibinit

The new capabilities of Multibinit within ABINITv9 are described
fully in the Sec. 4.1 of [[cite:Gonze2020]]. See also Sec. [D.1](#v9.0.D.1).
In particular, a spin model, described specifically in Sec. 4.1.2 of [[cite:Gonze2020]], is available, as follows.

>Multibinit implements the most commonly used model for spin systems,
>via the Heisenberg Hamiltonian including magnetic exchange and Dzyaloshinskii Moriya interactions.
>Single ion anisotropy and dipole–dipole interactions are also included,
>and all terms bear a very strong similarity to the quadratic part of the lattice model Hamiltonian.
>A number of open source spin dynamics codes already exist, such as UPPASD, VAMPIR, OOMF;
>the distinguishing features of multibinit are the integration with abinit,
>to fit parameters, and the simultaneous dynamics with other
>degrees of freedom (in particular using the inter-atomic force constants).

A tutorial for the multibinit spin model has been written, [[tutorial:spin_model]].

Many new input variables are present.
Not all these new input variables are present in automatic tests, though, in this beta-release.
These "non-tested" input variables are indicated below with 'NT'.
This will be completed for the production version v9.2 .
List of tests in addition to those of the tutorial: [[test:v8_16]], [[test:v8_23]], [[test:v9_81]], and [[test:v9_82]].

New input variables tested and documented:
[[spin_calc_thermo_obs@multibinit|spin_calc_thermo_obs]],
[[spin_damping@multibinit|spin_damping]],
[[spin_init_orientation@multibinit|spin_init_orientation]],
[[spin_init_qpoint@multibinit|spin_init_qpoint]],
[[spin_init_rotate_axis@multibinit|spin_init_rotate_axis]],
[[spin_init_state@multibinit|spin_init_state]],
[[spin_ntime_pre@multibinit|spin_ntime_pre]],
[[spin_projection_qpoint@multibinit|spin_projection_qpoint]],
[[spin_sia_add@multibinit|spin_sia_add]],
[[spin_sia_k1amp@multibinit|spin_sia_k1amp]],
[[spin_sia_k1dir@multibinit|spin_sia_k1dir]],
[[spin_temperature_start@multibinit|spin_temperature_start]],
[[spin_temperature_end@multibinit|spin_temperature_end]],
[[spin_temperature_nstep@multibinit|spin_temperature_nstep]],
[[spin_var_temperature@multibinit|spin_var_temperature]],
[[spin_write_traj@multibinit|spin_write_traj]].
Additionnal new input variables:
[[slc_coupling@multibinit|slc_coupling]] (NT),
spin_calc_correlation_obs (NT and not documented),
spin_calc_traj_obs (NT and not documented),
[[spin_projection_qpoint@multibinit|spin_projection_qpoint]] (NT).

By Xu He, N. Helbig, J. Bieder, E. Bousquet, Ph. Ghosez, M. Verstraete


**B.5** Constrained DFT

Constrained Density-Functional Theory (see [[topic:ConstrainedDFT]]) is available,
with a new algorithm allowing to impose the constraints to arbitrary precision,
whether it relates to the charge, magnetization, magnetization direction, or magnetisation size,
or a combination thereof for different atoms. The constraints are smeared spherical integrals
with ajustable sphere radius, centered on atoms. The algorithms has been demonstrated for norm-conserving pseudopotentials
as well as PAW. Forces and derivatives with respect to the constraints
are available (i.e. magnetic torque for the non-collinear spin case).
Stresses are still to be coded, will be available in ABINITv9.2.

New tests: v8#24-29, v8#95-97 and v9#1-3.
New input variables: [[chrgat]], [[constraint_kind]], [[ratsm]].

By X. Gonze.

<a name="v9.0.B.6"></a>
**B.6** Large modifications of the build system

The build system relies on new <hostname>.ac9 files, superceeding the v8 <hostname>.ac files.
Fully documented example files can be found in doc/build/config-examples.
A bash script (`upgrade-build-config-file.sh`) located in the top level directory of the package can be used
   to convert from the old `.ac`format to `.ac9`.

The build system of ABINITv9 does not build anymore the (hard and soft) dependencies (Linalg, NetCDF4, HDF, LibXC, Wannier90, ...), as this was not sustainable.
Three libraries are now mandatory: linalg, NetCDF4/HDF5 and LibXC. Failing to link to them will prevent building ABINIT.
The other libraries are optional, there will only be a warning if they are not available.
If the user does not provide the path to these libraries,
the build system will try to find them in the "usual" directories, and inform the user that it has done so.
The build system also can make suggestions to the user, to complete its *.ac9 file.

By Y. Pouillon and JM Beuken

**B.7** New command line interface

There is a new (**recommended**) command line interface to run ABINIT, without the "files" file.
The new syntax is:

    abinit run.abi

or

    abinit run.abi > run.log 2> run.err &

where `run.abi` is the Abinit input file that now provides all the information related to pseudos
and the prefixes that were previously passed via the "files" file. For comparison, the old syntax is

    abinit < run.files > run.log 2> run.err &      ! This is the old syntax

A file extension for the input file is highly recommended (in this example we use `.abi`)
as by default the parser will use the string before the file extension as root to build the prefixes
for the input/output/temporary files.

The user can specify the name of the main output file thanks to the [[output_file]] input variable,
the list of pseudopotentials thanks to the [[pseudos]] input variable and the directory where
all pseudos are located with [[pp_dirpath]].
The prefix for other input, output or temporary files can be specified with [[indata_prefix]], [[outdata_prefix]] and
[[tmpdata_prefix]], respectively.
A default set of prefixes computed from the basename of the input file is used if
these variables are not specified in the input.

For some examples, see tests [[test:v8_90]], [[test:v7_45]], and [[test:v5_54]]. See also [[topic:Control]].

A similar command line interface can also be used for the anaddb code.
In this case, the relevant variables are:
[[output_file@anaddb]], [[ddb_filepath@anaddb]], [[ddk_filepath@anaddb]], [[gkk_filepath@anaddb]], [[eph_prefix@anaddb]].

The new syntax is:

    anaddb run.in > run.log 2> run.err &

See tests [[test:v8_52]] for a standard analysis of the DDB file and
[[test:v7_94]] for the (old implementation) of electron-phonon calculations in anaddb.

!!! important

    The old "files file" interface is still operational although deprecated and will be **REMOVED** in Abinit v10.


By M. Giantomassi (MR 586).


**B.8** Reading strings from the input file

A new mechanism to read strings enclosed between **double quotation marks** from the input file has been activated.
So, many new input keywords are reading strings as data, and, often, can be used alternatively to similar input keywords
that were expecting numerical values such as the `get*` and `ird*` variables.
The goal is to encourange a new approach for performing ABINIT calculations in which multiple datasets
and `get*` variables are replaced by indipendent input files that are connected together via file paths.

List of new input variables that rely on this feature:

- [[getddb_filepath]], an alternative to [[getddb]] or [[irdddb]], see test [[test:v9_60]]
- [[getden_filepath]], an alternative to [[getden]] or [[irdden]], see test [[test:v8_36]] and [[test:v8_41]]
- [[getscr_filepath]], an alternative to [[getscr]] or [[irdscr]], see test [[test:v67mbpt_51]]
- [[getwfkfine_filepath]], an alternative to [[getwfkfine]] or [[irdwfkfine]], see tests [[test:v9_55]], [[test:v9_56]]
- [[getwfk_filepath]], an alternative to [[getwfk]] or [[irdwfk]], see test [[test:v9_60]]
- [[getwfq_filepath]], an alternative to [[getwfq]] or [[irdwfq]], see test [[test:v7_98]]
- [[getkerange_filepath]], see test [[test:v9_60]]
- [[getpot_filepath]], see test [[test:v8_44]]
- [[pseudos]], [[indata_prefix]], [[outdata_prefix]], [[tmpdata_prefix]], [[output_file]], see test [[test:v9_04]]
- [[pp_dirpath]]: cannot be tested  EXPLICITLY because system dependent but used by runtests.py when generating the input file.
- [[output_file@anaddb]], [[ddb_filepath@anaddb]], [[gkk_filepath@anaddb]], [[eph_prefix@anaddb]].
  See tests [[test:v8_52]] for a standard analysis of the DDB file and
  [[test:v7_94]] for the (old implementation) of electron-phonon calculations in anaddb.

By M. Giantomassi

<a name="v9.0.B.9"></a>
**B.9** YAML sections in the output file

YAML sections are now generated in the output file, sometimes replacing text sections, sometime providing new information.
At present there is a YAML section for the components of the total energy, the GS results including forces and stresses as well as a YAML section for GW calculations, and some YAML sections giving information about the iteration status.
<!--
Example of tests: paral#86, v67mbpt#2. See the input variable use_yaml (TO BE DOCUMENTED).
-->
At the occasion of the development of this capability, and its adaptation to the test farm, the
perl script fldiff.pl has been replaced by a Python version.
See related information in Sec. 5.5 of [[cite:Gonze2020]].

By T. Cavignac, M. Giantomassi, GM Rignanese, X Gonze.


**B.10** New approach to define crystalline structures in the Abinit input

The new variable [[structure]] can be used to initialize the lattice vectors
and the atomic positions **from an external file**.
Variables such as [[natom]], [[ntypat]], [[typat]] and [[znucl]] are automatically initialized
and need not to be specified in the ABINIT input.
At present, the code can read netcdf files produced by ABINIT (`GSR.nc`, `WFK.nc`, `DEN.nc`, `HIST.nc`)
and POSCAR files in VASP-5 format.
See the documentation for the syntax and limitations.

By M. Giantomassi.


**B.11** New capabilities of abipy and abiflows

The abipy and abiflows projects have been significantly extended.
See Sec. 6 of [[cite:Gonze2020]], as well as the [gallery of plotting scripts](http://abinit.github.io/abipy/gallery/index.html) &nbsp;
and the [gallery of abipy workflows](http://abinit.github.io/abipy/flow_gallery/index.html) &nbsp;.

By M. Giantomassi, G. Petretto, F. Naccarato.


* * *

### **C.** Changes for the developers (including information about compilers)

**C.1** A python script to help ABINIT developers and development.

The new python script abisrc.py located in the top directory of the ABINIT package has been developed.
It has superceded abilint.py in the makemake procedure.

Try

    ./abisrc.py --help

then follow the suggestions, to get info about files, directories, interfaces, to visualize
dependencies of the ABINIT subroutines, etc.

Note that there are dependencies of abisrc.py, to be installed prior being able to use some of its capabilities.
Use:

    pip install -r requirements.txt --user

to install the dependencies in user mode.

By M. Giantomassi.

**C.2** New characteristics of input variables

In the description of input variables (e.g. files abimkdocs/variables_abinit.py), a new field `added_in_version` has been introduced,
for example,

     added_in_version="9.0.0"

or, for variables introduced prior to v9,

     added_in_version="before_v9"

**C.2** Test farm: new and obsolete bots

* Bots introduced in the test farm: atlas_gnu_9.1_openmpi, buda2_gnu_8.2_cuda, cronos2_gnu_7.4_paral.
* Bots upgraded: abiref_gnu_5.3_* to abiref_gnu_9.2_* ; abiref_nag_6.2_openmpi to
    abiref_nag_7.0_openmpi; bob_gnu_5.3_openmp to bob_gnu_7.5_openmp; buda2_gnu_8.1_mpich3
    to buda2_gnu_8.2_mpich3; graphene_gnu_6.4_macports to graphene_gnu_9.2_macports; max2_gnu_5.3_* to max2_gnu_6.5_*; ubu_gnu_5.3_openmpi to ubu_gnu_9.2_openmpi.
* Bots removed: atlas_gnu_7.2_fb.ac (no nightly test of the fallbacks anymore), cronos_gnu_5.3_paral(remplaced by cronos2), inca_gnu_6.3_py3k (inca too old), tikal_gnu_* (tikal too old).
Working also on a cronos-cronos2 cluster.

By JM Beuken

**C.3** Supported compilers

* gfort (GNU) compiler: v9 newly supported, v4 obsolete
* ifort (INTEL) compiler: v19 newly supported.
* NAG 7.0 instead of 6.2

By JM Beuken

**C.4** Unitary ttransposer#1 . Test of the transposer for linear algebra to KGB parallelisation.

By J. Bieder.

**C.5** Linkchecker has been reenabled, only for internal link checking.

By JM Beuken (MR 513).

**C.6** Enable the generation of a HTML side-by-side diff on the test farm when fldiff fails with a line count error and it was not caused by a crash of Abinit. The diff algorithms uses a specialized heuristic to improve line synchronization and prevent weird matching.

By Th. Cavignac (MR 526)

**C.7** Split of the source tree (ongoing).

In order to improve modularity, the source tree must be split in two parts, one for low-level routines, largely independent of ABINIT,
and one for more specific routines to ABINIT. The low-level routines should become a separate library, with its own build system and make.
At present the low-level library have been moved out of src, inside the shared/common/src directory.
See related information in Sec. 5.4 of [[cite:Gonze2020]].

**C.8** New FFT specifications for the build system
See https://gitlab.abinit.org/pouillon/abinit/-/issues/33 .

By Y. Pouillon (MR 619)

* * *

<a name="v9.0.D.1"></a>
### **D.**  Other changes (or on-going developments, not yet finalized)

**D.1**  Miscellaneous improvements of Multibinit (lattice part)

Miscellaneous improvements have been made to the lattice part of Multibinit.
See the new input variables below, also see the Sec. 4.1.1 of [[cite:Gonze2020]].

New tests: [[test:v8_38]], [[test:v8_94]], [[test:v8_98]], [[test:v8_99]], [[test:v9_83]],  [[test:v9_84]], and [[test:v9_85]].
New input variables are listed below.
Not all these new input variables are present in automatic tests, though, in this beta-release.
These "non-tested" input variables are indicated below with 'NT'.
Also, not all of these are documented, or only partly documented (e.g. variable type, acronym, default, but no more).
This will be completed for the production version v9.2 .

- [[analyze_anh_pot@multibinit|analyze_anh_pot]]
- fit_SPC_maxS@multibinit (NT)
- fit_iatom@multibinit [[test:paral_81]], [[test:paral_82]], [[test:v8_13]], [[test:v8_14]], but not documented.
- latt_compressibility@multibinit, NOT TESTED, NOT DOCUMENTED
- [[latt_friction@multibinit|latt_friction]]
- latt_mask@multibinit, NOT TESTED, NOT DOCUMENTED
- [[latt_taup@multibinit|latt_taup]] Despite the link, NOT TESTED, only partly DOCUMENTED
- [[latt_taut@multibinit|latt_taut]] Despite the link only partly DOCUMENTED
- [[opt_coeff@multibinit|opt_coeff]]
- [[opt_effpot@multibinit|opt_effpot]]
- [[opt_ncoeff@multibinit|opt_ncoeff]]
- prt_names, NOT TESTED, NOT DOCUMENTED
- [[test_effpot@multibinit|test_effpot]] v8#98

By M. Schmitt, Xu He, F. Ricci, M. Verstraete, Ph. Ghosez


**D.2** Miscellaneous improvements in the Chern number and orbital magnetization calculations,
including parallelization over k points of the Chern number calculation.

By J. Zwanziger (MR 469, 500, 545, 588)

**D.3** Calculation of Debye-Waller tensor. New test [[test:v8_58]].

By M. Giantomassi


**D.4** Test linear electro-optical coefficient, tutorespfn [[test:optic_5]].

By N. Pike (MR 575).

**D.5** NCPP Wavefunction mixing with Variational Energy
and minor improvements to prepare PAW+Hybrid variational energy.
New test [[test:v7_73]], simple system for testing Hartree-Fock and the SCF algorithms.

By X. Gonze (MR 434, 444, 445).

**D.6** New weight distribution of the Fourier components of the force constants.
Test tolerance in the new integration weights, tests [[test:v8_52]], [[test:v8_53]], [[test:v8_54]].

By H. Miranda and M. Giantomassi

**D.7** Test calculation of velocity matrix elements (DDK) with
 optdriver 8 and [[wfk_task]] "wfk_ddk”, see [[test:v8_59]].

By M. Giantomassi

**D.8** Upgraded [[tutorial:paral_gspw]], new version of auto paral (with threads)

By M. Torrent (MR502).

**D.9** Test wannier90 interface with [[nsppol]]=2 and [[nspden]]=2, [[test:wannier90_04]].

By Xu He

**D.10** Mixed precision for FFT transforms. New input variable [[mixprec]]
see [[test:v8_44]], [[test:v9_57]], [[test:v9_60]], and [[test:v9_61]].

From M. Giantomassi (MR491).

**D.11** Multibinit has been interfaced with scale-up,
[[https://www.secondprinciples.unican.es]]

By Marcus Schmitt, Jordan Bieder, Matthieu Verstraete and Philippe Ghosez

**D.12** The following units are now also allowed in input files:

- S Sec Second  for the ABINIT input file;
- nm (for nanometer)  for the ABINIT and ANADDB input files.

**D.13** a-TDEP utility:
added [[guide:a-TDEP|A-TDEP user guide]],
[[topic:a-TDEP|a-TDEP topic]], and corresponding input variable documentation.
References: [[pdf:a-TDEP_Paper|a-TDEP paper]].
Also, see Sec. 4.2 of [[cite:Gonze2020]].

By F. Bottin, J. Bouchet, J. Bieder (MR491,422).

**D.14** Improvements of NLO calculations.
Optimize memory allocation. In particular for [[usepead]] = 1.
Write Raman susceptibilities to netcdf in anaddb.

By G. Petretto  (MR 599).

**D.15** MetaGGA + PAW in progress.
All internal and unit tests are OK. The implementation seems close to the end. Still need to perform physical tests.

By M. Torrent and J.-B. Charraud (MR 587, 558, 625).

**D.16** Implementation of an alternate version of MPI reductions valid when the number of data exceeds 2^32.

By M. Torrent (MR 587)

**D.17** Build system and src support for llvm (clang/flang) and ARM (armflang/armclang/armpl).

By M Torrent (MR 571)

**D.18** Improvement of core WF reading (optics + electron-positron)

By M Torrent (MR 557)

**D.19** Fix bootstrap kernel convergence issues

By Wei Chen (MR 546)

**D.20** Upgrade versions of libraries used with ABINIT.
Upgrade atompaw to 4.1.0.6. Upgrade Libxc to 4+.

By M. Torrent, JM Beuken (MR 532, 470, 465, 441)

**D.21** Write yaml file for fatbands (phonopy format) with a-TDEP

By J. Bieder (MR510)

**D.22** Write polarization vectors in GSR.nc.

By H. Miranda (MR 462).

**D.23** Updated user interface of Raman_spec.py . Added one section to tutorial NLO to describe use of Raman_spec.py.

By N. Pike (MR 460, MR 581)

**D.24** XML format for core wavefunctions

By F. Jollet (MR 423)

**D.25** Wavefunction prediction for molecular dynamics.

By F. Jollet (MR 412)

**D.26** Added a preview for the toptic_4.abi file in the optic tutorial.

By F. Goudreault (MR 408)

**D.27** Improve ELPA detection; make abinit compatible with ELPA2019

By M. Torrent (MR 626)

**D.28** Upgrade of [[tutorial:base3]] and [[tutorial:basepar]].

By X. Gonze (MR628)

**D.29** New input variable [[prtprocar]], see test [[test:v5_40]].

By M. Verstraete (MR630)

**D.30** The ABINIT input variable [[supercell_latt]] is documented.
See also [[test:v8_94]].

By X. Gonze

**D.31** New implementation of the cRPA (to compute U and J in DFT+U or DFT+DMFT)
that can consider any subset of orbitals in the system.
Compilation was a bit long on certain compilers, so new keywords have been added to the build system:
enable_crpa_optim and enable_crpa_no_optim

By R. Outerov and B. Amadon (MR622).

**D.32** Work on refactoring the Coulomb interaction part of ABINIT.

By B. Guster, M. Giantomassi and X. Gonze (MR 627&633).

**D.33** New TB2J python script to compute the superexchange (J) and the Dzyaloshinskii-Moriya (DMI) interactions. The script can be found in [http://gitlab.abinit.org/xuhe/TB2J](http://gitlab.abinit.org/xuhe/TB2J) with doc and tutorials. The script is interfaced with wannier90 and use the w90 output files. The J calculation works in production, the DMI is much more sensitive to disentanglement noise and have to be use with care. An article is under construction to describe the method and its implementation. The script can deliver input data file for the spin model of Multibinit.

By He Xu, M. Verstraete and E. Bousquet

**D.34** Miscellaneous additional bug fixes and improvements of documentation.
L. Baguet, JM Beuken, J. Bieder, E. Bousquet, F. Bruneval, T. Cavignac, M. Giantomassi, X. Gonze, F. Jollet, N. Pike, Y Pouillon, M. Torrent, J. Van Bever, M. Verstraete, Xu He.


* * *

## v8.10

Version 8.10, released on October 15, 2018.
List of changes with respect to version 8.8.

Many thanks to the contributors to the ABINIT project between
April 2018 and October 2018. These release notes
are relative to modifications/improvements of ABINIT v8.10 with respect to v8.8.
The merge request #408 is the first MR not reported in these release notes. Then, #410-411, #413-416 have also been included.

The list of contributors includes:
B. Amadon, G. Antonius, L. Baguet, J.-M. Beuken, J. Bieder, E. Bousquet, F. Bruneval, Wei Chen, M. Cote,
J. Denier, G. Geneste, Ph. Ghosez, M. Giantomassi, O. Gingras, X. Gonze, F. Goudreault, B. Guster, Xu He, Y. Jia, F. Jollet,
A. Lherbier, A. Martin, H. Miranda, F. Naccarato, G. Petretto, N. Pike,
S. Ponce, Y. Pouillon, S. Prokhorenko, F. Ricci, M. Torrent, M. van Setten, B. Van Troeye, M. Verstraete, J. Zwanziger.

It is worth to read carefully all the modifications that are mentioned in the present file,
and examine the links to help files or test cases.
This might take some time ...

Xavier

### A. Warnings and important remarks

A.1 The correct definition of the temperature has been implemented in the isokinetic algorithm [[ionmov]]=12.

A.2 The multibinit input variable "supercell" has been renamed "supercell_latt".

A.3 Tests v8#100-107 have been moved to v8#81-88.
    Tests paral#100-103 have been moved to paral#80-83

* * *

### B. Most noticeable achievements

B.1 The computation of the Raman intensity in DFPT with PAW is now possible (it was only available with norm conserving psps previously).
    This is based on the second-order Sternheimer equation for the derivative
    with respect to an electric field.
    See tests [[test:v8_81]] to [[test:v8_89]].
    By L. Baguet and M. Torrent.

B.2 There has been a large effort to clean the documentation, that was needed and made possible thanks to the recent move to the mkdocs system.
    In particular, many new hyperlinks have been created in dozens of markdown files,
    references have been centralized in the [[theory:bibliography]].
    By B. Amadon, G. Antonius, L. Baguet, J. Bieder, E. Bousquet, F. Bruneval, Wei Chen, M. Cote, G. Geneste,
    M. Giantomassi, X. Gonze, Xu He, F. Jollet, A. Lherbier, H. Miranda, F. Naccarato, G. Petretto, N. Pike,
    S. Ponce, Y. Pouillon, M. Torrent, M. van Setten, B. Van Troeye, M. Verstraete, J. Zwanziger.

B.3 The multibinit application (for second-principles calculations) has considerably progressed.
    Documentation has thus been set up: "topics" have been written, as well as a tutorial,
    in addition to the already existing input variable documentation and test cases.
    See [[topic:LatticeModel]], [[topic:BoundingProcess]], [[topic:FitProcess]] and [[topic:DynamicsMultibinit]],
    that are hub to the relevant tutorial, input variables and test cases
    (e.g. [[lesson:lattice_model]],[[test:v8_15]], [[test:v8_16]]...).
    By A. Martin, in collaboration with Fabio Ricci and Ph. Ghosez

B.4 Several new options are available for the [[ionmov]] input variable governing ionic dynamic or geometry optimization:

* [[ionmov]]=15 for the FIRE algorithm, [[test:v8_17]];
* for [[ionmov]]=12, isokinetic ensemble, the fixing of atomic positions is now allowed, [[test:v8_21]] and [[test:v8_22]].

Also, the documentation for the hybrid Monte Carlo algorithm has been improved, see [[test:v8_34]], and the new input variables [[hmcsst]] and [[hmctt]].

By Xu He, S. Prokhorenko and X. Gonze.

B.5 The linear combination of images is now allowed, with the new value for input variable [[imgmov]]=6,
    and mixing factor given by [[mixesimgf]].
    In this case, the total energy and forces are also assembled as a linear combination, and the geometry is optimized using
    algorithms selected with the usual [[ionmov]] input variable.
    See test [[test:v8_20]].
    The wavefunctions from the previous itimimage value (see [[ntimimage]] input variables) can be stored,
    using the new input variable [[imgwfstor]]. This allows saving CPU time at the expense of memory, in all
    the image based algorithms.
By X. Gonze, testing by Y. Jia.

B.6 Tutorial [[tutorial:nuc]]
    has now a section for the computation of the isomer shift (Mossbauer spectroscopy) based on Fermi contact interaction.
By J. Zwanziger.

B.7 The Frohlich model is now implemented in the electron-phonon part of ABINIT, [[optdriver]]=7.
    The Frohlich average of effective masses is computed with the DFPT computation of effective masses, see [[test:v8_56]].
    Also, the zero-point renormalization of the band extrema is computed using a general formula valid for isotropic and anisotropic
    solids, as well as for non-degenerate or degenerate extrema, see [[eph_frohlichm]] and [[test:v8_57]].
By X. Gonze.

* * *

### C. Changes for the developers (also compilers)

C.1 All F90 ABINIT sources are now inside modules.
    Makemake now aborts if F90 procedures outside modules.
    By M. Giantomassi, with some help by J. Zwanziger, M. Torrent and B. Amadon.

C.2 Prepared the removal of the bindings subsystem.
    By Y. Pouillon and M. Torrent.

C.2 New [Howto for developers](/developers/developers_howto) (variables, mkparents, robodoc, test_suite).
    By M. Giantomassi.

C.3 New [Howto for the test suite](/developers/testsuite_howto).
    By M. Giantomassi.

* * *

### D.  Other changes (or on-going developments, not yet finalized)

D.1 New input variable [[prtkbff]].
    This input variable activates the output of the Kleynman-Bylander form factors in the netcdf WFK file produced at the end of the ground-state calculation.
    The form factors are needed to compute the matrix elements of the commutator [Vnl, r] of the non-local part of the (NC) pseudopotentials.
    This WFK file can therefore be used to perform optical and/or many-body calculations with external codes such as DP/EXC and Yambo. The option is ignored if PAW.
By H. Miranda and M. Giantomassi.

D.2. A new routine to calculate and write the DDK matrix elements in a EVK.nc files for norm-conserving pseudo-potentials (and nspinor == 1),
    from a WFK file using the commutator [H, r] used to calculate the matrix elements for chi.
    These files are in the same format and contain the same information as the EVK.nc files (former DDK.nc) produced by the DFPT part of the code.
    This routine is parallelized over k-points and bands and requires a much smaller memory footprint than the DFPT code.
    By H. Miranda and M. Giantomassi.

D.3 New input variable [[slk_rankpp]].
    This variable controls how the number of processes to be used in Scalapack diagonalization algorithm: [[np_slk]] will be calculated according to this value.
    By J. Bieder.

D.4 New input variables [[prtefmas]], [[irdefmas]] and [[getefmas]], to deal with the effective masses, e.g. to allow feeding computed effective masses
    from one dataset to another one.
    By X. Gonze.

D.5 The spin dynamics has been implemented in multibinit.
    By He Xu.

D.6 New value for input variable [[usepawu]]=4.
    The FLL double counting is used. However, and in comparison to usepaw=1, the calculation is done without polarization in the exchange correlation functional.
By B. Amadon.

D.7 New extensive testing of the DFPT+PAW+GGA, see [[test:v8_51]].
    However, [[pawxcdev]]=0 is still needed.
    By M. Torrent.

D.8 New input variable [[prtfull1wf]] to print the full perturbed wavefunctions.
    By G. Antonius.

D.9 Allows one to suppress completion of the 2DTE for vanishing elements using symmetries only,
    negative values of [[rfmeth]].
    By X. Gonze.

D.10 Interface with TRIQS_2.0.
    By O. Gingras.

D.11 Allows to define different occupations for different spins, when [[occopt]]=0 and [[nsppol]]=2.
     By X. Gonze.

D.12 NEB is now tested (at last), [[test:v6_29]].
     By X. Gonze.

D.13 Many |abitutorials| have been improved.
     By X. Gonze.

D.14 Work on orbital magnetization.
     By J. Zwanziger.

D.15 Elastic and piezoelectric tensors in netcdf format.
     By M. Giantomassi

D.16 Allow [[boxcutmin]] for DFPT case, after check by H. Miranda.

D.17 ABIWAN.nc files with Wannier output that can be analyzed wit AbiPy.
     See the |AbiwanFileNb| for futher information.
     By M. Giantomassi.

D.18 The [[topic:Macroave]] has been created.
     By X. Gonze.

D.19. Include CTQMC Code using non diagonal hybridization function. See [[test:paral_83]].
     By B. Amadon, J. Denier and J. Bieder.

D.20. Finalize scalapack/elpa integration in lobpcg.
      Use ONLY if no openmp. Warning: scalapack is not thread-safe in general.
      By J. Bieder.

D.21 Automatic test [[test:v8_37]] for TDep application.
    By J. Bieder.

D.22 Python scripts to calculate physical properties that can be derived from the elastic tensor
     (which is the result of an anaddb calculation).
     By N. Pike.

D.23 Miscellaneous additional bug fixes and improvements of documentation.
     By B. Amadon, G. Antonius, L Baguet, J.-M. Beuken, J. Bieder, F. Goudreault, F. Jollet, H. Miranda,
     F. Nacaratto, N. Pike, M. Torrent, M. Verstraete.

* * *

## v8.8

Version 8.8, released on April 28, 2018.
List of changes with respect to version 8.6.

Many thanks to the contributors to the ABINIT project between
November 2017 and April 2018. These release notes
are relative to modifications/improvements of ABINIT v8.8 with respect to v8.6.
The merge request #285 is the first MR not reported in these release notes.

The list of contributors includes:
B. Amadon, G. Antonius, L. Baguet, J.-M. Beuken, J. Bieder, F. Bottin, Y. Bouchet, E. Bousquet, W. Chen,
C. Espejo, Ph. Ghosez, M. Giantomassi, X. Gonze, F. Jollet, A. Martin,
H. Miranda, G. Petretto, N. Pike, Y. Pouillon, S. Prokhorenko, F. Ricci,
G.-M. Rignanese, M. Torrent, M. Verstraete, J. Zwanziger

It is worth to read carefully all the modifications that are mentioned in the present file,
and examine the links to help files or test cases ...
This might take some time ...

Xavier

### A. Warnings and important remarks

A.1 Due to the availability of new input variables, some obsolete input variables have been suppressed:

* input variable cgtyphf, see now [[fockoptmix]];
* input variable gwfockmix, see now [[hyb_mixing]] and [[hyb_mixing_sr]].

A.2 The algorithm used for computing the weights for the phonon band structure interpolation in ANADDB
    in the case [[brav@anaddb]] = 1 has changed. See B.5.

A.3 Tests v7#71-72 have been moved to v7#76-77.

A.4 Replaced *tribes* by *relevance* in doc/topics.

A.5 Replace `EELF` file extension by `ELF`. See [[prtelf]].
    By Guido Petretto.

A.6 Definition of a maximal value for dilatmx, at 1.15, than can be bypassed by setting chkdilatmx=0.
    This is to prevent users slowing down ABINIT too much inadvertantly.

* * *

### B. Most noticeable achievements

B.1 The whole ABINIT documentation has been placed under the control of [mkdocs](http://www.mkdocs.org/),
    and most files have been translated to markdown
    (well, there are still a few remaining files not placed in this system, but these are quite few).
    The capabilities developed for [v8.6](#v86)
    (Topics -B.1-, central bibliography -B.2-, restructured and searcheable list of input variables -B.3-,
    frequency statistics -B.4-) have been maintained and consolidated.
    More documentation is available for developers than before. The whole system is better integrated and easier to maintain.
    The appearance is also new.
    The work on documentation is nearly complete, still not all bibliographical references of the doc have been entered in this
    central bibliographic database.
    Entry point: see the new header of any ABINIT documentation file e.g. the [new user's guide](/guide/new_user).
    By M. Giantomassi, with some help from X. Gonze.

B.2 The DFPT has been extended to non-collinear systems ([[nspden]] = 4), with or without spin-orbit coupling,
    for the ddk, electric field and atomic displacement perturbations,
    as well as for the Zeeman magnetic field perturbation (see B.3).
    See tests from [[test:v8_66]] to [[test:v8_80]]. For experts, see the new input variable [[ixcrot]].
    By F. Ricci, S. Prokhorenko, M. Verstraete, M. Torrent and E. Bousquet.

B.3 DFPT can now treat the magnetic field perturbation (Zeeman interaction - magnetic field couple to the spin).
    See the input variable [[rfmagn]], as well as tests [[test:v8_66]] to [[test:v8_70]].
    The new input variable [[tim1rev]] has been introduced, to allow treating perturbations with non-zero q wavevectors.
    By S. Prokhorenko and E. Bousquet.

B.4 The python library [AbiPy](https://github.com/abinit/abipy), for launching ABINIT (also in high-throughput mode)
    and the jupyter notebook based tutorials, nicknamed [abitutorials](https://github.com/abinit/abitutorials) are now sufficiently
    mature to be advertised. They have been used at the ICTP electron-phonon doctoral school in March 2018.
    Feedback on AbiPy and abitutorials is welcome.
    By M. Giantomassi.

B.5 A new algorithm (Wigner-Seitz cell based) for computing the weights for the phonon band structure interpolation in ANADDB
    has been implemented. It has replaced the old algorithm in case [[brav@anaddb]] = 1.
    The old algorithm is still available for back-compatibility purposes, now corresponding to [[brav@anaddb]] = -1,
    see [[test:v7_93]], although there is no real reason for using it.
    The new algorithm is very general, respect better the symmetries, and should even supercede
    the use of other values of [[brav@anaddb]].
    By G. Petretto following discussions with GM Rignanese and XGonze, and tests by Henrique Pereira Miranda.

B.6 The Chern number can be computed, in the norm-conserving case as well as in the PAW case.
    See the theory in [[cite:Ceresoli2006]].
    Associated input variable: [[orbmag]].
    Nuclear magnetic dipole moment code has been improved for efficiency. In particular,
    this improvement is due to converted nucdipmom_k to complex type and explicit BLAS call.
    Tutorial [[tutorial:nuc|nuc]] is nightly tested.
    By J. Zwanziger ([[tutorial:nuc|nuc]] testing by X. Gonze).

* * *

###C. Changes for the developers (also compilers)

C.1 Add support for NAG 6.2, use netcdf4 and hdf5 with NAG 6.2.
    New builders: abiref_nag_6.2_openmpi and atlas_gnu_7.2_fb

C.2 New version of ELPA module (2011 -> 2017 compatible)
    By M. Torrent

C.3 Replaced http://www.abinit.org by https://www.abinit.org everywhere in the doc.
    By JM Beuken

C.4 Added fake -n/--no-split option to makemake.
    This little change will let the test farm keep on working with all branches while the source tree is split into common + core.
    By Y. Pouillon

C.5 Upgrade Abinit to PSML API 1.1.
    By Y. Pouillon

C.6 Allowed for free-form link flags statements in configure options
    By Y. Pouillon

C.7 Intel 18.0 is now officially supported (one bot using it is present in the test farm).
    The situation is not perfect though, as  in mrgscr and mrgdv, the ADVANCE='NO' specification for the write instruction does not work, but
   it works in simple programs. Thus all tests (and their chain of tests) that rely on mrgscr and mrgdv have been disabled for this compiler.
   Namely, v3#87-91, vv67mbpt#37-39, v7#86-88, v8#41-44, v8#63
   Also, the reading of WFK files using MPIIO is not correct, for tests mpiio#26 and 62.

* * *

###D.  Other changes (or on-going developments, not yet finalized)

D.1 Implementation of the LDA-1/2 methodology (see the announcement B.10 of v8.6): [[test:v8_32]] has been provided.
    By F. Jollet.

D.2 Numerous progresses have been made related to the hybrid functionals (although hybrid functionals
    are not yet in production).

* Stresses are now computed correctly.

* The downsampling of the Brillouin Zone to build the Fock operator has been implemented and tested.
See the input variable [[fockdownsampling]] as well as tests [[test:libxc_72]] and [[test:paral_09]].

* The B3LYP functional has been implemented, [[ixc]] = -402.

* The new input variables [[hyb_mixing]], [[hyb_mixing_sr]], [[hyb_range_dft]], and [[hyb_range_fock]]
give sufficient flexibility in the PBE0 and HSE family of functionals.

* GW calculations can now start on top of hybrid functional calculations.

* At variance, there is also now more flexibility to run hybrid calculations using the GW infrastructure
([[gwcalctyp]] = 5, 15, 25) by the definition of the [[ixc_sigma]] input variable.

* There has been also important work concerning the self-consistency, although this work is not finalized yet
(one reason why hybrid functionals are not yet in production).
The self-consistency at fixed ACE operator can take advantage of an auxiliary XC functional to decrease
the number of inner iterations, see [[fockoptmix]], [[auxc_ixc]] and [[auxc_scal]], while for the outer loop,
in which the ACE operator is upgraded, the wavefunction mixing has been implemented ([[fockoptmix]] and [[wfmix]]).

See the new tests v7#67-72 libxc#44, 45, 72, 73, 74,
and also the updated tests v4#86, 87, v67mbpt#09, v7#65, libxc#41, 42, 43, paral#09.
By X. Gonze and F. Jollet, with help by M. Torrent.

D.3 The [[tutorial:tdepes|tutorial on temperature-dependence of the electronic structure]] has been upgraded, and carefully tested.
    See all tests in `tutorespfn/tdepes*`.
    By X. Gonze and M. Giantomassi

D.4 Output of interpolated density in the MPI-IO case is now tested, [[test:mpiio_26]] and [[test:mpiio_27]].

D.5 Ongoing work on the multibinit project.
    New input variables fit_nfixcoeff, fit_fixcoeff, fix_generateTerm,
    see [[test:v8_13]] and [[test:v8_14]].
    New input variable dipdip_prt, see [[test:v8_06]], as well as tests [[test:paral_96]] to paral[102].
    New generator for the polynomial coefficients, debug strain for the fit process, add tolerance in the fit process,
    add the plot of the comparison between model and DFT.
    By A. Martin, M. Verstraete and Ph. Ghosez.

D.6 Adjustment of tutorial tutoparal ucrpa, see test tutoparal#tucrpa_4.
    By B. Amadon

D.7 The ddk file is now available in netCDF format (lightweight version without first-order wavefunctions),
    and test with the optic post-processor has been set up.
    See the new [[test:v7_49]].
    By M. Giantomassi

D.8 Continued development of the electron-phonon [[optdriver]] = 7 module of ABINIT.
    New input variable [[tmesh]], defining a linear mesh of temperatures, see tests [[test:v8_44]] and [[test:v8_45]].
    Also, debugging and improvement of doc.
    By M. Giantomassi

D.9 Added netcdf output of phonons for full grid, not just band structure. Only in tetrahedron prtdos 2 case.
    By M. Verstraete

D.10 On-going development: main executable `atdep`, for the TDEP algorithm, by Hellman and coworkers.
     See [[src:98_main/tdep.F90]], as well as directory 80_tdep.
     No automatic tests provided yet, no documentation as well ...
     By F. Bottin, J. Bouchet, J. Bieder.

D.11 Capability to print perturbed vxc potential in response function calculations.
     By G. Antonius

D.12 On-going modularization of all source F90 files, to get rid off abilint.
     By M. Giantomassi

D.13 On-going improvements in the doc, to benefit from the new processing capabilities,
     like central bibliography, matjax, etc ...
     By M. Giantomassi, X. Gonze

D.14 Post-processing script for Raman calculations (script/post-processing/Raman_spec.py).
     Reads the anaddb output file and extracts the Raman tensor and then calculates
     the Raman spectra as a function of frequency at a user-defined temperature.
     Additionally, the script will automatically extract the dielectric tensor as a function of frequency if it is available.
     By N. Pike

D.15 Refactoring for DFPT+NON_COLL: first version for PAW
     By M. Torrent

D.16 Fix of several bugs in constrained magnetization calculations [[magconon]].
     By E. Bousquet

D.17 Wrong sign of the derivative of spherical harmonics for f orbitals.
     Can lead to problems in BSE and GW calculations if pseudos with explicit f-projectors are used.
     Detected and corrected by Henrique Pereira Miranda.
     See [[gitsha:87617d261b5905e368081af4b899b3ddd7ec83fe]] for the corrected expressions.

D.18 Add possibility to do DFT+U calculations without spin polarization
     in the exchange and correlation functional: the spin polarization thus only comes from the U and J terms.
     Can be used with [[usepawu]] = 4, but still under tests.
     By B. Amadon

D.19 GW is now available with [[nspinor]] = 2 with or without spin-orbit coupling,
    and with [[nspden]] = 1 or 4 (collinear or non-collinear spin-magnetisation).
    Implemented only in the norm-conserving case. Still under testing.
    See tests from [[test:v8_90]] to [[test:v8_93]].
    By M. Giantomassi.

D.20 Miscellaneous additional bug fixes and improvements of documentation by:
     L. Baguet, W. Chen, C. Espejo, M. Giantomassi, Y. Pouillon, M. Torrent, J. Zwanziger.


* * *

## v8.6

Many thanks to the contributors to the ABINIT project between
May 2017 and October 2017. These release notes
are relative to modifications/improvements of ABINITv8.6 with respect to v8.4.

The list of contributors includes:
B. Amadon, G. Antonius, L. Baguet, J.-M. Beuken, J. Bieder, F. Bottin, Y. Bouchet, E. Bousquet,
M. Giantomassi, O. Gingras, Ph. Ghosez, M. Giantomassi, X. Gonze, F. Jollet, J. Junquera, A. Martin,
F. Naccarato, G. Petretto, N. Pike, Y. Pouillon, S. Prokhorenko, M. Torrent, M. Verstraete, J. Wiktor, J. Zwanziger

It is worth to read carefully all the modifications that are mentioned in the present file,
and examine the links to help files or test cases.
This might take some time ...

Xavier

* * *

Version 8.6, released on November 3, 2017.

List of changes with respect to version 8.4 .

* * *

###A. Warnings and important remarks

A.1 The interface between ABINIT and TRIQS has been changed, such that TRIQS 1.4 is used now instead of TRIQS 1.3.
    No backward compatibility with TRIQS 1.3 has been coded, sorry. See D.4.

A.2 Some changes of names:

* input variable gwls_sternheimer_kmax has become [[gwls_stern_kmax]];
* input variable gwls_dielectric_model has become [[gwls_diel_model]];
* input variable prt_effpot has become prt_model (multibinit input variable);
* input variable effmass has become [[effmass_free]];
* tutorial tlda has become tdftu;

    Also, the input variable gwls_second_model_parameter, not used, has been suppressed.

A.3 The definition of Hund's coupling J as computed within cRPA has changed: it now uses the same convention as
    the convention used (for the variable [[jpawu]]) in DFT+U and DFT+DMFT in ABINIT (automatic tests, and tutorial
    are accordingly changed).
    By B. Amadon

* * *

###B.  Most noticeable achievements

B.1 The whole ABINIT documentation has been significantly improved by the introduction of Topics, replacing the
    previous "lists of ABINIT features". All the capabilities of ABINIT are now presented in about 70 short topic Web pages.
    Those topic web pages usually have:
    - a brief introduction;
    - the list of related tutorials -if any-;
    - the list of related input variables (ordered according to their importance for the topics -compulsory, basic, useful or expert-);
    - possibly example input files;
    - list of references.
    Entry point: see the new header of any ABINIT documentation file e.g. the [new user's guide](/guide/new_user)
    By F. Jollet and X. Gonze (also tests/fixes by B. Amadon, M. Torrent).

B.2 A central [[theory:bibliography]] database abiref.bib has been created, and linked to the
    above-mentioned topics (B.1) but also to other parts of the ABINIT documentation (e.g. input variable list,
    the tutorials, the theory documents, the acknowledgments).
    More than 200 bibliographical references are present.  Not all bibliographical references of the doc have been entered in this
    central bibliographic database, though.
    By X. Gonze and F. Jollet.

B.3 The list of input variables has been restructured, and is now searchable.
    The input variables for anaddb, aim and optic have been included in the database.
    By J. Bieder, X. Gonze and F. Jollet.

B.4 The frequency of usage of each input variable (in the set of automatic tests) is now automatically
    computed, and mentioned in the documentation for this input variable. Examples input files are also now mentioned in the documentation.
    The input files for the automatic tests can now be directly accessed on the Web, as well as the reference files for the tutorials.
    By. X. Gonze and F. Jollet.

B.5 Several important developments related to electron-phonon matrix element computation have been made.
    The Frohlich interpolation procedure for electron-phonon matrix elements, as explained in PRL 115, 176401 (2015), has been implemented.
    The long-range part of the phonon coupling potential is modeled with the Born effective charges and the dielectric tensor.
    This long-range part is substracted from the potential before the Fourier interpolation then added after the interpolation.
    The resulting potential is in much better agreement with the full calculation, as can be verified from the el-ph matrix elements.
    Also, a functionality has been added in the eph driver ([[eph_task]]=5) to only interpolate the phonon potential onto a fine q-point grid.
    The interpolation is performed one perturbation at a time, and is thus more memory efficient than the previous procedures.
    There is additional testing of the new "driver" optdrive=7 specifically dealing with electron-phonon
    related computations (including zero-point renormalisation), especially the interpolation.
    The symmetries have been fixed.
    See new tests [[test:v8_61]]-[[test:v8_65]].
    By  G. Antonius and M. Giantomassi.

B.6 ABINIT can now read pseudopotentials in the PSML 1.1 format, as described in https://arxiv.org/abs/1707.08938. This XML-based format is
    produced by ONCVPSP 3.2 and 3.3, as well as SIESTA's ATOM 4.2, and allows to perform calculations with the exact same pseudopotential files
    in both ABINIT and SIESTA. See the new directory ~abinit/tests/psml, tests [[test:psml_01]] to [[test:psml_14]].
    Note: patches are provided at https://launchpad.net/pspgenpatch to enable PSML output in ONCVPSP.
    By Y. Pouillon, M. Verstraete, J. Junquera and A. Garcia.

B.7 ABINIT is now interfaced with Libxc 3.0. The interface with Libxc 4.0 is in preparation.
    Tests [[test:libxc_06]], [[test:libxc_07]], [[test:libxc_17]], [[test:libxc_18]], [[test:libxc_20]], [[test:libxc_21]]
    have been modified, because some functionals of libxc v2.0 have changed category in v3.0.
    By M. Torrent.

B.8 A new tutorial, called [[tutorial:positron|Electron-positron annihilation]] has been created.
    By J. Wiktor and M. Torrent.

B.9 The new input variable [[chkdilatmx]] has been introduced, to allow expert users to make
    ABINIT bypass the stopping criterion related to dilatmx. In practice, if the condition related
    to dilatmx is not met, ABINIT continues, and delivers an (approximate) optimized geometry and energy,
    that might be used by external drivers like e.g. USPEX to continue the search for global optimized structures.
    See input variable [[chkdilatmx]], and test [[test:v3_42]].
    By X. Gonze.

B.10 Implementation of the LDA-1/2 methodology.
     Tests to be provided.
     By F. Jollet.

* * *

###C. Changes for the developers (also compilers)

C.1 There are large changes of the procedure to document ABINIT, for most of the documentation files.
    The HTML files are now produced from YAML files, under the control of the script ~abinit/doc/generate_doc.py .
    The documentation that describes this procedure is available on the ABINIT wiki, at https://wiki.abinit.org/doku.php?id=developers:generate_doc .
    This is directly linked to the modifications in the doc presented in B1-B4.
    In particular, the Dokuwiki syntax is used for the hyperlinks.
    By F. Jollet and X. Gonze.

* * *

###D.  Other changes (or on-going developments, not yet finalized)

D.1 The "Adaptively Compressed Operator" approach for the fast application of the Fock operator has been implemented,
    and replaces the traditional way to apply the Fock operator in hybrid functionals (e.g. HSE06, PBE0, ...).
    Hybrid functionals are not yet to be considered in production, though (see D.6), but likely for ABINITv8.8.
    See tests libxc 51, 52, 53, 67, 68, 69, 70, 71, and also v7#65, 66, 70.
    By F. Jollet and X. Gonze.

D.2 A set of 6 input files and accompanying references (from 32 to 2048 procs), for benchmarking high-performance computing
    is available in the new directory ~abinit/tests/hpc .
    Not yet tested automatically, but this future capability is prepared.
    By M. Torrent.

D.3 The tutorial on the temperature-dependent electronic structure has been imported from the ABINIT wiki to the
    usual location ~abinit/doc/tutorial and suppressed from the Wiki. However, it is not yet operational.
    Work is also going on on tutorial fold2bloch.
    By X. Gonze.

D.4 Interfacing with TRIQS 1.4 (instead of 1.3).
    By O. Gingras, B. Amadon and J.-M. Beuken.

D.5 Anaddb can now interpolate and print out the DDB onto an arbitrary set of q-point.
    The same procedure was used to produce the phonon band structure.
    Now, with the input variable prtddb, anaddb will produce both the _DDB file and the _DDB.nc files, the latter being separated for each q-point.
    By G. Antonius.

D.6 On-going work on hybrid functionals: speed-up of the SCF loop, computation of stresses,
    joint computation of forces and stresses, downsampling the wavevectors.
    By X. Gonze and F. Jollet.

D.7 On-going work on the implementation of the TDEP algorithm (temperature dependent sampling).
    By J. Bieder, F. Bottin and Y. Bouchet.

D.8 Replacements of http:// by https:// in many documentation files.
    By J.M. Beuken.

D.9 Test of non-magnetic DFT+U and DFT+U+SO.
    See the new test v5#16
    By M. Torrent.

D.10 Make DFT+U and local EX-exchange compatible with nspden=1/nspinor=2
     By M. Torrent.

D.11 Test of the Velocity Verlet algorithm ionmov=24
     See the new test v8#13
     By S. Prokhorenko.

D.12 Make thermally occupied supercell of a given size, with input variable thermal_supercell.
     See test v8#46
     By M. Giantomassi

D.13 Write dielectric tensor to anaddb.nc when only perturbations w.r.t. electric field are present; Test for nlflag=2,3
     Test the computation of the nonlinear coefficients and first change of dielectric tensor.
     See test v8#47-50
     By F. Naccarato.

D.14 Ongoing work: Raman intensities, in the PAW case, using DFPT.
     By L. Baguet and M. Torrent.

D.15 Ongoing work on the multibinit project.
     New hist storage.
     New tests paral#101-102, to test the anharmonic part.
     Rationalization of supercell treatment with multibinit
     By A. Martin, M. Verstraete and Ph. Ghosez.

D.16 On-going work on the extension of DFPT within non-collinear magnetism.
     By F. Ricci, S. Prokhorenko, M. Verstraete, M. Torrent and E. Bousquet.

D.17 On-going work on DFPT with magnetic field perturbation (Zeeman field).
     By S. Prokhorenko and E. Bousquet.

D.18 Begin transport epc calculations within the eph part of the code.
     By M. Verstraete.

D.19 Improvements for the reading and initialization of density (esp. nspden=4).
     By M. Torrent.

D.20 New interface to the build system and new ac8 config file format.
     By Y. Pouillon.

D.21 Store fold2bloch results in NetCDF format.
     Add new option to cut3d to convert DEN/POT from Fortran to netcdf
     By M. Giantomassi.

D.22 Add mdtemp in the _HIST file. This can be useful (mandatory) for post processing MD/PIMD
     Add imgmov in the _HIST file. Convenient to know if it is PIMD or NEB/string for postprocessing
     By J. Bieder.

D.23 Use inversion symmetry if nspden == 4 and NC
     By M. Giantomassi.

D.24 Update elastic tutorial
     By J. Zwanziger.

D.25 Add LO-TO terms to netcdf files
     By M. Giantomassi.

D.26 Numerous miscellaneous additional bug fixes and improvements of documentation by:
     G. Antonius, J. Bieder, M. Giantomassi, F. Jollet,
     G. Petretto, N. Pike, Y. Pouillon, M. Verstraete, M. Torrent.

* * *

## v8.4

Many thanks to the contributors to the ABINIT project between
January 2017 and May 2017.
These release notes are relative to modifications/improvements of ABINITv8.4 with respect to v8.2.

The list of contributors includes:
F. Altvater, G. Antonius, L. Baguet, J.-M. Beuken, J. Bieder, E. Bousquet,
W. Chen, G. Geneste, M. Giantomassi, Y. Gillet, X. Gonze, F. Jollet, A. Martin,
F. Naccarato, G. Petretto, S. Prokhorenko, F. Ricci, M. Torrent, M. Verstraete, J. Zwanziger

It is worth to read carefully all the modifications that are mentioned in the present file,
and examine the links to help files or test cases ...
This might take some time ...

Xavier

* * *

Version 8.4, released on June 2, 2017.

List of changes with respect to version 8.2 .

* * *

A.  Warnings and important remarks

A.1 The Github ABINIT Web page, at https://github.com/abinit, allows one to
    access the mirror of the repository of ABINIT, as well as some other projects
    related to ABINIT, like AbiPy or the pseudo-dojo.

A.2 The content of the ABINIT Web portal, specifically for the pages related to the presentation of
    ABINIT (http://www.abinit.org/about/what-is-abinit), has been upgraded.

* * *

B.  Most noticeable achievements

B.1 Implementation of algorithms to interpolate the electronic band structure,
    based either on "star functions" of on "B-splines" (as alternatives to Wannier function interpolation).
    See the input variables [[einterp]], [[nkpath]], and [[prtebands]], and tests
    old syntax: `Tlibxc#42, Tv8#04` replaced by
    [[tests/libxc/Input/t41.abi]], [[test:v8_04]].
    Work by M. Giantomassi

B.2 The Fock mixing factor for the HSE hybrid functional can be tuned thanks to the input variable gwfockmix.
    (Warning: this is for the GW-type approach to electronic structure only, not for total energies)
    See test Tlibxc#43 .
    Work by W. Chen.

B.3 Implementation of spatially-varying chemical potential, for each atomic species.
    See the input variables [[chempot]] and [[nzchempot]], and tests Tv8#30 and 31.
    Work by X. Gonze

B.4 Linear geometrical constraints can now be imposed on PIMD runs.
    This allows ABINIT to use the blue moon sampling technique.
    See the input variable pimd_contraint, and test Tv8#05
    The use of history (HIST) files is now possible with images.
    Work by G. Geneste and M. Torrent.

B.5 Computation of linear reponse (optics executable as well as BSE part of ABINIT)
    in the case of temperature-dependent electronic structure.
    See tests Tv67mbpt#50-53.
    WARNING: This capability of ABINIT has not been fully tested. However, the
    basic tests for non-spin-polarized simple semiconductors are OK.
    As usual, use at your own risk.
    Work by Y. Gillet and M. Giantomassi.

B.6 Computation of Gruneisen parameters by finite differences, within ANADDB.
    See test v8#45
    Work by M. Giantomassi

B.7 New LOBPCG implementation [[wfoptalg]] = 114.
    This is the default when [[paral_kgb]] == 1.
    Performances are equivalent in standard use (MPI alone) and much better with openmp+multithreaded linalg.
    It also allows to have only one block for large system and to reduce memory copies.
    This version has been developed keeping in mind the next generation of HPC.
    Work by J. Bieder

B.8 New algorithms for the displacement of nuclei ([[ionmov]]):
    - Hybrid Monte Carlo (HMC) predictor (ionmov=25)
    - Velocity Verlet (VV) NVE molecular dynamics predictor (ionmov=24)
    See Tv8#12 for ionmov=24.
    Work by S. Prokhorenko

B.9 Refactoring of ANADDB for the production of DOS and other thermodynamic quantities,
    also the mean square displacement and mean square velocity.
    The DOS is obtained using usual DOS methods, instead of the histogram method,
    and converges much faster with [[anaddb:ng2qpt]]. Activated with [[anaddb:prtdos]] 1 or 2 in anaddb.
    Work by M. Verstraete.

* * *

C. Changes for the developers (also compilers)

C.1 Management of the test farm: the new bot ubu_intel_17_openmpi
    has been activated, so that Intel 17 is now supported.
    Also, replacement of shiva_gnu_6.3_py3k by inca_gnu_6.3_py3k,
    update of graphene (MacPorts) to gcc6.3 + scalapack.
    By J.M. Beuken.

* * *

D.  Other changes (or on-going developments, not yet finalized).

D.1 The printing of potentials (e.g. [[prtvxc]] ...) works now for DFPT.
    By M. Verstraete.

D.2 New input variable [[prtphbands]].
    See tests v7#88 and v8#46.
    By M. Giantomassi

D.3 In case the ANADDB interpolation of phonon frequencies produces
    erroneously a negative slope around gamma, the new [[anaddb:nsphere]] = -1 possibility
    allows ANADDB to select a set of IFCs that does not lead to such non-physical behaviour.
    See test v8#46.
    Work by M. Giantomassi.

D.4 Added new value for [[anaddb:nlflag]] = 3 that computes only the non-linear susceptibility.
    By F. Naccarato.

D.5 Computation of forces is now possible in the Wavelets + PAW case.
    By M. Torrent.

D.6 Ongoing work concerning the new "driver" [[optdriver]] = 7 specifically dealing with electron-phonon
    related computations (including zero-point renormalisation).
    See new tests v8#41 to 44.
    Work by M. Giantomassi.

D.7 Ongoing work: Raman intensities, in the PAW case, using DFPT.
    By L. Baguet and M. Torrent

D.8 Ongoing work related to non-collinear DFPT.
    Definition of the new input variable [[rfmagn]], as well as Tv8#20 .
    DFPT with static B-field (Zeeman term only) works for q=(0 0 0).
    Adjustmanet of dfpt corrections for metals to non-collinear case.
    Bug fix for GS calculations with finite magnetic field (case of collinear spins).
    By S. Prokhorenko, F. Ricci, and E. Bousquet

D.9 Ongoing work on the multibinit project.
    NPT simulations are possible.
    New tests v8#06 and paral#100, to check the correct IO of XML files.
    Note that tests Tv7#120-124 have been renumbered Tv8#07-11.
    Work by A. Martin

D.10 New test paral#62, to make sure [[paral_kgb]] = 0 works when there are some idle procs.
     By M. Giantomassi

D.11 Ongoing work on forces and stresses for hybrid functionals.
     By F. Jollet

D.12 Ongoing work on the electron-phonon postprocessor ElectronPhononCoupling.
     Fix bug affecting MPI runs.
     By G. Antonius.

D.13 Improvement of zheevd using MAGMA (from a msg on the forum)
     By M. Torrent

D.14 Implemented KSS.nc output with netcdf primitives
     By M. Giantomassi

D.15 Concerning  the Fourier interpolation of the phonon band structure,
     inside ANADDB, work by G Petretto:
     - Updates in the calculation of sound velocity,
     - Small modifications for [[anaddb:nlflag]] == 3 and added some quantities to the anaddb netcdf file,
     - Fix a bug in the implementation of the new weights

D.16 Concerning Path Integral Molecular Dynamics with Quantum Thermal Bath:
     allow restart from history file.
     By M. Torrent

D.17 Refactored the computation of the electric dipole moment.
     By M. Torrent

D.18 The energy width for the bands in the Boltztrap intrans file is now automatically set.
     Previously a constant 0.4 Ryd, and the user should have checked by hand if
     the value was sufficient. Should be considered a bug fix.
     By M Verstraete

D.19 Numerous miscellaneous additional bug fixes and improvements of documentation by:
     F. Altvater, G. Antonius, J. Bieder, M. Giantomassi, F. Jollet,
     G. Petretto, M. Verstraete, M. Torrent, J. Zwanziger.

* * *

## v8.2

Many thanks to the contributors to the ABINIT project between
June 2016 and January 2017. These release notes
are relative to modifications/improvements of ABINITv8.2 with respect to v8.0.

Moreover, most of them are also described in the Computer Physics Communications 2016 ABINIT paper,
doi:10.1016/j.cpc.2016.04.003

The list of contributors includes:
B. Amadon, G. Antonius, L. Baguet, J.-M. Beuken, J. Bieder, E. Bousquet, F. Bruneval,
W. Chen, M. Giantomassi, Y. Gillet, X. Gonze, G. Petretto, F. Jollet, A. Martin,
V. Planes, Y. Pouillon, T. Rangel, F. Ricci, M. Torrent, M. Verstraete

It is worth to read carefully all the modifications that are mentioned in the present file,
and examine the links to help files or test cases ...
This might take some time ...

Xavier

* * *

Version 8.2, released on February 16, 2017.

List of changes with respect to version 8.0 .

* * *

A.  WARNINGS AND IMPORTANT REMARKS

A.0 The 2016 article by the ABINIT group is now mentioned in the acknowledgments:
    "Recent developments in the ABINIT software package.
    Computer. Phys. Communications 205, 106 (2016)".
    See http://www.abinit.org/doc/helpfiles/for-v8.2/users/acknowledgments.html, as well as
    the notice at the end of ABINIT runs.

A.1 [[inclvkb]] 1 has been removed. Now the possible values are either 0 or 2

A.2 The default strategy of [[so_psp]] has been changed
    (see the description of the input variable so_psp).

* * *

B.  Most noticeable achievements

B.1 Implementation of the Limited-memory Broyden-Fletcher-Goldfarb-Shanno (LBFGS)
    minimization algorithm.  Activate this algorithm using [[ionmov]] = 22.
    From the tests that have been run, this algorithm can be much better
    than the native implementation of BFGS in ABINIT when one approaches convergence,
    perhaps because of better treatment of numerical details.
    This algorithm might become the default in ABINIT, if the better behaviour is confirmed.
    Test case: v8#02 .
    The working routines were based on the original implementation of J. Nocedal
    available on netlib.org.  They have been reshaped and translated into modern fortran,
    then interfaced to ABINIT by F. Bruneval (sources in 45_geomoptim/m_lbfgs.F90).

B.2 A new tutorial is available, on the calculation of the effective interactions U and J
    using constrained Random Phase Approximation (cRPA) for DFT+DMFT (or DFT+U) calculations.
    See doc/tutorial/_ucalc_crpa.md as well as the automatic tests tutorial/tucrpa#1-5 .
    This tutorial was prepared by B. Amadon.

B.3 Implementation of temperature-dependent spectral functions
    (electronic spectral function, with electron-phonon interactions),
    as well as real part of phonon self-energy (Pi) for gapped systems.
    Also, automatic test for spectral function, v7#89, and improved documentation.
    Work by G. Antonius.

B.4 The RPA one-shot bootstrap fxc kernel has been implemented for GW calculations (gwgamma=-8).
    See Rigamonti et al PRL 114, 146402 (2014) and Berger PRL 115, 137402 (2015).
    The test v67mbpt#36 has been updated.
    Work by W. Chen.

* * *

C. Changes for the developers (also compilers)

C.1 The version control system that is used for the development of ABINIT has been changed:
    the whole ABINIT project
    has been ported from bzr to git.
    Work by J.-M. Beuken, Y. Pouillon, M. Giantomassi, X. Gonze,
    with discussions with many developers.

C.2 New versions of Fortran compilers have been integrated in the test farm:
    - intel 16.0
    - gnu 6.1 and 6.2
    - IBM xlf compiler 14.1
    - NAG 5.3
    Corresponding examples are available in doc/build/config-examples.
    On the contrary, g95 is not tested anymore.
    Work by J.-M. Beuken

C.3 The v8 directory for tests has been initialized.
    By Matteo Giantomassi.

C.4 Python 3 >= 3.4 is now supported in build system scripts
    (compatibility with py2_ version >= is maintained).
    By Matteo Giantomassi.

* * *

D.  Other changes
(or on-going developments, not yet finalized).

D.1 The main executable "multibinit" has been created.
    Its goal is to perform "second-principles calculations", building model Hamiltonians
    using the data provided by the DDB (or other info from ABINIT).
    Tests v7#120-124 (should be renamed) as well as paral#95-98.
    Work by A. Martin.

D.2 A new "driver" within ABINIT has been defined, specifically dealing with electron-phonon
    related computations (including zero-point renormalisation).
    Set optdriver=7 .
    New input variables: ddb_shiftq, eph_task, eph_transport, prtphdos, prtphsurf.
    See tests v7#88 and 89.
    Work by M. Giantomassi and G. Antonius.

D.3 The generation of k-point meshes with kptrlatt and shiftk is now tested.
    See test v8#03 .
    Work by M. Giantomassi

D.4 As a follow-up of the Achievement B3 in the release notes of ABINITv8.0 (DMFT + TRIQS),
    new input variables have been defined for DMFT: dmft_tolfreq and dmftctqmc_triqs_nleg.
    Automatic tests have been set-up, tests v8#01 and paral#99.
    Work by B. Amadon and V. Planes

D.5 More systematic tests of the IO in parallel (different files) have been set up,
    in the norm-conserving, PAW and PAW + spin-orbit cases.
    See tests mpiio#26, 27 and 28.
    Also, the case [[paral_kgb]] = 0 is now tested with idle processors. See test paral#62.
    Work by M. Giantomassi

D.6 The load balancing for the repartition of the plane waves among procs is now monitored,
    and a dynamical equilibration is made possible thanks to the new input variable pw_unbal_thresh.
    See test mpiio#26.
    Work by M. Torrent

D.7 The capability to output spin-resolved DOS in case of spin-orbit NC pseudopotential
    calculations has been checked, and a test has been set-up (test v7#17).
    Work by M. Giantomassi

D.8 Files generated by ABINIT, and used by BOLTZTRAP are now tested.
    See v6#11.
    Work by M. Giantomassi

D.9 Unit tests (fftprof) have been set up for the use of the MKL-DFTI routines:
    unitary#tfftmkl_03 and 04.
    Work by M. Giantomassi

D.10 Ongoing work: Raman intensities, in the PAW case, using DFPT.
     By L. Baguet and M. Torrent

D.11 prtvcbm working with all parallelizations.
     Tests mpiio 26:28 have been reactivated on 4 processors.
     By M. Torrent

D.12 On going work related to non-collinear DFPT.
     By F. Ricci, S. Prokhorenko, and E. Bousquet

D.13 MBPT: support for the commutator [Vnl r] in the case of NC pseudos
     with more than one projector per l-channel has been added.
     Tests in v67mbpt[40] (GW run with psp8 files).
     SOC is not yet available, though.
     The KSS file continues to use the old implementation to maintain backward compatibility
     hence outkss won't produce the file if multiple-projectors are detected
     inclvkb 1 has been removed. Now the possible values are either 0 or 2
     By M Giantomassi

D.14 For Hirshfeld and Bader: doc and warning.
     For Hirshfeld charges the output was unclear: the density integral (electrons only)
     was called the hirshfeld charge, as opposed to the net one.
     For Bader there was no check that the core charge file
     corresponded to the pseudopotential used.
     Now checks at least that it integrates to znucl-zion, which is a first step.
     Important as the default fc files on the web do not have semicore electrons,
     whereas many of the psp8 will.
     Contribution by M. Verstraete.

D.15 Updated acknowledgments, including the 2016 paper.
     By X. Gonze

D.16 Ongoing work on forces and stresses for hybrid functionals.
     By F. Jollet

D.17 Ongoing work concerning weights for the Fourier interpolation inside ANADDB.
     By G. Petretto

D.18 Numerous miscellaneous additional bug fixes
     (to the sources, as well as to the build system, including patches for the fallbacks),
     and improvements of documentation by:
     G. Antonius, L. Baguet, J. Bieder, F. Bruneval,
     M. Giantomassi, Y. Gillet, G. Petretto, Y. Pouillon,
     M. Verstraete, M. Torrent (in particular, for DFPT+PAW).

* * *
---
authors: XG
---


# How to contribute to ABINIT

!!! Warning
    This file is obsolete, and should be reexamined/rewritten.
    By contrast, the documentation for developers on the [ABINIT Wiki](https://wiki.abinit.org/doku.php?id=developers:overview) 
    is up-to-date.

This page provides a description of the procedures followed for development of
the ABINIT package through collaboration of different groups of persons, based
in different places in the world. 

Any comment or suggestion to improve these procedures will be welcome!

[TOC]

* * *

## Introduction

The ABINIT package is aimed at being used by different groups of people,
without mandatory control by the main contributors of the ABINIT group. In the
same way, the ABINIT development project is fundamentally open to the
contributions of different persons, not located in Louvain-la-neuve or
Corning. These contributing persons are members _de facto_ of the ABINIT
group.

People using the code might consider adding their personal subroutines,
without trying to make them part of the official ABINIT package. However, this
has two drawbacks for them: in subsequent versions, their modifications will
not be incorporated, so that they might have to check and modify the interface
for each new version ; moreover, their contribution is not tested by other
users of the code, so that bugs might remain unnoticed. It is also nicer to
share the result of their coding efforts with other users of the code.

Of course, a collaborative effort has also some drawbacks. In particular, the
collaboration between distant developers should be carefully planned, since
orthogonal modifications of the same piece of code by two different people at
the same time is very likely to happen, generating "negative progress", i.e. a
large waste of time when synchronization is to be done. Also, it is required
to use a well-defined coding style, to provide test case files, and to comment
the modifications and additions as much as possible, in order to facilitate
the maintenance and the future modifications.

This document aims at defining the protocol to be followed to avoid "negative
progress" due to lack of synchronization. The analogy with the procedures to
be used for the parallelization of a code is obvious. The aim is that each
external 'node' does not waste its time, that communications are kept at the
lowest level possible, and that the final result is correct ! We will need
barriers for synchronization, and so on ...

The ability to incorporate the contributions of different groups in a
harmonious way might become a noticeable strength of the ABINIT project.

## Code repositories

Managing different versions of ABINIT is done thanks to a utility called
[**Bazaar**](http://www.bazaar-vcs.org/). For an introduction to this powerful
and versatile tool, you can have a look at our dedicated pages in the
Developers' corner of the ABINIT web site.

Thanks to Bazaar, the development of a project becomes completely transparent,
since all the changes in the files are registered: the latest version is of
course available, but one can come backwards in time to track bugs easily or
to know what anybody did. The development of different branches is also
managed, as well as the subsequent merging procedure. This feature is
important to allow development by many different people. The place where all
the files are stored, including their history, is called a **repository**.

For developers, using the repository is a privileged way of managing their
contributions, since the coordinator is automatically and instantly informed
of their progresses. They also benefit from regular backups of all the
contributions.

The ABINIT repository is divided into several **main categories**:

  * **abinit-release**, where go all contributions for the official release packages, these contributions consisting mainly in fixes and documentation;
  * **abinit-devel**, where all new and ongoing developments are managed;
  * **abinit-doc**, containing new and incomplete documentation, until it is consistent enough to be moved to the source code.

Each category is divided into **versions**, 3 digits for _abinit-release_ and
2 digits for _abinit-devel_. Then, in each category and for each version,
there is one **reference branch**, codename _merge_, which is the backbone of
the development effort. All concerned developers are supposed to use this
branch as a starting point for their tasks and to keep permanently in sync
with it. They may have at least one branch of their own.

## Basic philosophy

In the following, we will distinguish between **"debugging" contributions**
and **"development" contributions**.

_Debugging contributions_ are typically modifications of a few lines in one or
relatively few routines, needed for the code to work properly or to be
properly documented. Sometimes they are related to comments within routines or
corrections to documents. Usually, such modifications do not need any
synchronization and can be sent directly to the coordinator via email, in the
form of a patch. It is however possible to have them synchronized by recording
them inside the **abinit-release** category of the repository.

For the time being, Xavier
[&lt;xavier.gonze@uclouvain.be&gt;](mailto:xavier.gonze@uclouvain.be) should be
contacted.

_Development contributions_ usually involve the addition of new capabilities
to the code. Despite the use of Bazaar, synchronization with the coordinator
is **ALWAYS** needed: one has to make sure that nobody else is already in
charge of a similar project! The development contributions might be quite
local (basically adding one routine, called by a few lines from an existing
routine), or, on the contrary, involve modifications of many existing
routines. Even for the local type of modifications, discussion with the
coordinator is mandatory. Though Bazaar now deals nicely with conflicts at the
file level, it is of great importance to avoid semantic conflicts from the
very beginning.

The developer will be allocated a _development task_. Related to this task,
they will be free to code, experiment, debug, and check the result of their
work without any communication with the rest of the ABINIT group. The
allocation of the task has obviously to be done in coordination with the rest
of the group prior to the work, while the result of the development has to be
incorporated into a new official version. The task is thus also limited in
time.

The prior allocation and subsequent incorporation, taking into account the
possibility that many different developers work independently, must be done
centrally by one or more coordinators (at present Xavier but this might change
in the future), in order to guarantee the harmony, relevancy and consistency
of all contributions.

It will be the responsibility of the developer to make enough checks of the
correctness of their modifications or additions. The developer should provide
adequate documentation: basically the description of the input variables and
output data in the _abinit_help_ file, as well as a possible update of the
bibliography. They should also provide one or more tests to be added to the
standard suite of tests. This is needed to ensure that the transfer to the
official version of the code has been done properly, and also that the new
capabilities will be preserved by the subsequent modifications of the code.
Finally, they have to show that their modifications have not suppressed
existing capabilities of the codes, by running the set of already existing
automatic tests.

It will be the responsibility of the coordinator to transfer the result of the
development effort of each developer to an official version, in such a way
that the test is reproduced in a satisfactory way. The subsequent maintenance
will be automatically done by checking that the corresponding test is still
working despite modifications.

## Detailed protocol for the developer

**a.** The developer proposes a modification or addition to the code to the coordinator. In most cases, such a proposal will be warmly welcome! There might be some discussions possibly involving other scientists to improve the original proposal. The developers mailing list has been set-up for that purpose (see the "Community section" on the ABINIT web site). 

**b.** With respect to the latest official version, the developers define their task and make a list of routines they wish to heavily modify (and those they would like to add, but this is not really needed at that time). Following the current coding practice, adding new input variables would need rather small modifications of a very large number of routines (invars, abinit, gstate, brdmin, mover, scfcv_core, vtorho ...). In order to avoid the allocation of too large an area of development within the source code, which would make other development efforts impossible, it is assumed that the developer will make appropriate use of the different 'user' input variables. This appropriate use means that none of the aforementioned routines will be modified for that purpose. The easiest case to handle is when a routine or a set of routines are added to the package, and the interface to the rest of the code involves only a set of called subroutines the interface (list of arguments) should not change, and just one (modified) calling routine. Many development projects of this type can coexist. By contrast, if one of the development efforts involves major changes to major routines, it will prohibit the execution of another development project of the same type, taking into account the current structure of the code. 

**c.** Having gathered the different suggestions made on the basis of the current official version, the coordinator releases the next version, adequately prepared to allow the proposed developments by different developer groups, with a list of routines allocated to each group, the number of the test cases allocated, and, for information, the tasks that will be done. 

**d.** After installation on the development machine, and prior to any modification, the developer runs the internal tests, as well as the tests series _v1_ to _v5_ and the _paral_ (sequential cases) suite of tests. The "`make tests_dev`" command is perfectly suited for that purpose. Checking against previous reference files that the results are OK is always nice enough. The developer can replace the reference files provided with the official version by those produced on their machines, to ease further checking of the results. Then the development effort can begin. It is expected _a priori_ that the following files will be modified in any case: 

  * _abinit.src_ files in some _src/*_ directories, because the new routines must be listed there;
  * the _allvariables.html_ and some of the _var*.html_ files, because the new input variables must be listed and documented there (the 'user' variables are now used, but a final, aesthetic, name should be proposed);
  * the _README_ and _tests.cnf_ files in the subdirectory _v5_, as well as an additional input file (at least).

**IMPORTANT**  
  
During the development, only the allocated routines should be modified. This
is very important. Many others can be created.

In both cases, the developers **MUST** follow the current ABINIT coding style,
presented in the latest version of the document _rules_coding_, in the
_~abinit/doc/developers_ subdirectory. In particular, they should mention
their initials in the header of the new or modified routines. This will be
useful if somebody needs information about this routine some time later.

At the end of the development effort, it is mandatory that the developer runs
again the _tests_dev_ series, to be sure that the developments have not
spoiled some other feature of the code. This is easily done by issuing **`make
tests_dev`**. This command will run automatically the required tests, and
produce a file _summary_tests.tar.gz_ that should be send with the updated
routines and separate new tests. This is important, since the coordinator will
have to run the suite of tests as well, but at that time, having trouble with
a modification done by some developer would mean an important delay in the
delivery of the new official version, and thus a large waste of time. It is
desirable that the tests of the new feature do not last more than 1 minute on
Pentium III 1GHz, or about 20 seconds on other (faster) processors.

**e.** After the development has occurred, the developer prepares a gzipped (compressed) tar file with all the needed files (additional routines, modified routines, makemake, abinit_help, README, tests.cnf, ...), the _summary_tests.tar.gz_ file, and makes it available to the coordinator (by email, FTP or SSH). 

## Example

a. David proposes a nice addition to the ABINIT package. Doug and Xavier are
enthusiastic about it.

b. On the basis of version 5.3.4, David proposes a list of routines that
should be allocated to him.

c. Xavier delivers a 5.4.x version that has been adequately prepared for the
independent development that David wants to do.

d. David implements his addition and modifications on the basis of version
5.4.x, checks whether the suite of tests is still OK, and makes the files
available to Xavier.

e. Xavier thanks David very much, transfers the work to the official version
5.5.y and performs the final modification of the names of the (new) input
variables, including the propagation through the routines that were not
allocated to the developper.

How to build ABINIT
===================

Introduction
------------

This document describes how to build ABINIT using the autotools-based
build framework.

If you find something out-of-date or incorrect, please send a message
to Yann Pouillon.



Build requirements
------------------

Before you try to build ABINIT, please make sure that you have installed
the correct tools, and they are correctly setup too.

### Unix/Linux ###

#### Hardware ####

 * 256 MB RAM (128 may be sufficient if you're very patient). For static
   builds, 512MB RAM minimum.
 * For debug builds: at least 2.5 GB free disk space (4 GB recommended).
 * For optimized builds: at least 400 MB free disk space (600 MB recommended).

#### Build Tools ####

 * A recent POSIX Shell
   The default /bin/sh that comes with some older unices (notably OSF/1)
   is known to be deficient. In that case, you should use 'make SHELL=ksh'
   for instance.
 * A C compiler
   GCC 3.2 or higher is recommended; egcs 1.0.3 (or higher), gcc 2.95.2
   (or higher) will also work; or your platform's native C compiler.
   Redhat 7.0 users, the compiler distributed with RH 7.0 is buggy, and it
   is recommended that you upgrade to the latest gcc 2.9x compiler
   (2.96-77 or later). You need the packages named *gcc* and *cpp*.
 * Perl 5.6 or higher
   Older Perl versions may work, though they have not been tested.
 * Python 2.2 or higher
   Older Python versions may work, though they have not been tested. Please
   note that this will become a true requirement soon.
 * GNU make 3.79.1 or higher
   Other flavours of make may work, yet we do not support them.

#### Optional Software ####

The following software are used only by developers. If you have no idea
what their purposes are, then don't worry. You don't need them to
build ABINIT.

 * Autoconf 2.59 or higher
   Autoconf is necessary if you want to hack on configure.in. Earlier versions
   might work, though they have not been tested.
 * Automake 1.9 or higher
   Automake is necessary to generate a *Makefile.in* from a *Makefile.am*.
   Autmoake 1.6 and earlier will likely not work.
 * Bazaar 1.4 or TLA 1.3.2 (or higher)
   TLA will allow you to access the latest version of the source.



### Windows ###

### Mac OS X ###

ABINIT builds for Mac OS X are Mach-O builds, built with gcc and Makefiles.
Doing builds on Mac OS X is therefore very similar to doing Unix builds.
You should be comfortable with the Terminal app (in /Applications/Utilities).
To install the software requirements, you will need to have admin privileges
on the machine.



#### Hardware ####

 * Mac OS X 10.1 or later
 * 2 GB or more free disk space



#### Build tools ####

 * The latest Developer Tools from Apple, if you don't have them already.
   You'll need a (free) ADC membership to download the tools. The December
   2002 tools or later are required to build ABINIT.

 * The current version of Fink (0.7.0 at time of writing). Note that there
   is conflict between Fink and Virex (both use /sw), so if you have Virex
   installed, you need to update to the latest version of Virex (7.2.1) and
   resolve the conflict as described in the Fink news (2003-04-16).

Earlier versions of Mac OS are not supported.



Get the source
--------------

The source package of ABINIT is usually available through the ABINIT website,
https://www.abinit.org/. Developers are strongly advised to use Bazaar
instead, as it will accelerate a lot the integration of their contributions.
For more details on Bazaar, please consult the ABINIT website
(https://www.abinit.org/bzr/) and Bazaar's website (http://bazaar-vcs.org/).



Configure build options
-----------------------

Running configure and make with the default options will not give you
a fully optimized build. In order to take full benefit from your machine,
you should use a config file. Please read these directions carefully before
building ABINIT.

### Using a per-user config file ###

Though it is possible to manually call "configure" with command-line
options, there is a better and more permanent way to tune the build
parameters. This is typically done by placing a config file in your
home directory: <tt>~/.abinit/build/*<hostname>*.ac</tt> .

This file is actually a Bourne shell-script which will be executed by
*configure* to set-up or override its default values. Since it is named
after the host it is stored on, you will be able to build ABINIT on several
machines sharing the same home directory.

You will find a self-documented tunable configuration for ABINIT in
~abinit/doc/build/config-template.ac9.



### Building with an OBJDIR ###

It is **highly recommended** that you use a separate object directory (OBJDIR)
when building ABINIT. This means that the source code and object files are not
intermingled, and that you can build ABINIT for multiple architectures from
the same source tree.

To build ABINIT using an OBJDIR, just create it, go inside and run *configure*
from there, e.g.:

<pre>
mkdir tmp-builddir ; cd tmp-builddir ; ../configure ; make
</pre>



### Other options ###

There are many other options recognized by the configure script. Some of
them are special-purpose options intended for embedders or other users,
and should not be used to build the full suite. The full list
can be obtained by running *./configure --help*.

Advice: if you can't figure out what a configure option does, don't use it!



Build & install
---------------

Building ABINIT is done by following the now traditional
*configure - make - make install* trilogy. To be more precise:

 * preliminary step: <tt>mkdir tmp-builddir ; cd tmp-builddir</tt>
 * 1. <tt>../configure</tt>
 * 2. <tt>make</tt>
 * 3. <tt>make install</tt>

A <tt>make check</tt> may be performed after <tt>make</tt>, at your option.



This file contains information about the use of spin-orbit
for format pspcod=5 of pseudopotentials.

The first line containing Haman grid parameters must be completed by
an information about the spin-orbit format of the pseudopotential.
So, one replaces:

    2.508991628593723E-4  0.0125           r1,al

by

    2.508991628593723E-4  0.0125  2        r1,al,pspso

pspso is 1 for non spin-orbit (optional),
         2 for spin-orbit


The next lines describing the number of projectors and some of their
characteristics must be duplicated, as soon as l/=0.
The number of projectors must be 2.

So, one replaces

    0    0    0    1   2.76        l,e99.0,e99.9,nproj,rcpsp
    .00000000    .0000000000    .0000000000    .00000000   rms,ekb1,ekb2,epsatm
    1    0    0    1   3.91        l,e99.0,e99.9,nproj,rcpsp
    .00000000    .0000000000    .0000000000    .00000000   rms,ekb1,ekb2,epsatm
    2    0    0    1   1.57        l,e99.0,e99.9,nproj,rcpsp
    .00000000    .0000000000    .0000000000    .00000000   rms,ekb1,ekb2,epsatm

by

    0    0    0    1   2.76        l,e99.0,e99.9,nproj,rcpsp
    .00000000    .0000000000    .0000000000    .00000000   rms,ekb1,ekb2,epsatm
    1    0    0    2   3.91        l,e99.0,e99.9,nproj,rcpsp
    .00000000    .0000000000    .0000000000    .00000000   rms,ekb1,ekb2,epsatm
    1    0    0    2   3.91        l,e99.0,e99.9,nproj,rcpsp
    .00000000    .0000000000    .0000000000    .00000000   rms,ekb1,ekb2,epsatm
    2    0    0    2   1.57        l,e99.0,e99.9,nproj,rcpsp
    .00000000    .0000000000    .0000000000    .00000000   rms,ekb1,ekb2,epsatm
    2    0    0    2   1.57        l,e99.0,e99.9,nproj,rcpsp
    .00000000    .0000000000    .0000000000    .00000000   rms,ekb1,ekb2,epsatm


The pseudopotentials and pseudowavefunctions are then entered,
in the order  s, p1/2, p3/2, d3/2, d5/2, f5/2, f7/2.


The local potential is specified by lloc with the following values:

    lloc=0  refers to the s potential
    lloc=1  refers to the p3/2 potential
    lloc=2  refers to the d5/2 potential
    lloc=3  refers to the f7/2 potential
    lloc=-1 refers to the p1/2 potential
    lloc=-2 refers to the d3/2 potential
    lloc=-3 refers to the f5/2 potential
---
authors: JB
---

Documentation of `Xg_t` and `XgBlock_t` for abinit
==================================================

Written on: 2018/09/07  
Updated on:

Introduction and motivation
---------------------------

For the last decade, computers and more precisely processors have evolved a lot. 
To answer the issue of renewal, a new set of functions has been written to ease and adapt abinit to new architectures. 
This *abstract layer* is still a prototype and proof of concept. 
The true library should be written in an Oriented Object way to allow to switch during runtime between different libraries, or accelerators.

This current state of the code is stable. 
It is designed for **2D arrays** of **complex** or **real**
It is `Fortran2003` compliant and without any OOP.
Here is a list of what you can do

* Memory management

    - Allocate memory
    - Free memory
    - Block and sub-blocks
    - Map an object to an already allocated memory space
    - Get a `Fortran90` array from an object
    - Get/Set values from/to `Fortran90` array
    - Copy an object to an other one.
    - Pack a matrix to Upper/Lower Triangular Matrix
    - Reshape

* BLAS operations

    - POTRF
    - TRSM
    - GEMM
    - ADD
    - Scale

* LAPACK operations
    - HEEV
    - HEEVD
    - HPEV
    - HPEVD
    - HEGV
    - HEGVX
    - HPGVX
    - HPGVD

* ScaLAPACK
    - PHEEV
    - PHEGV

* Custom extensions
    - Array Shift
    - Colwise Norm 2
    - Colwise $Y-aX$
    - Set to 0
    - Set to Identity
    - Set the diagonal
    - Set the diagonal and zero off-diagonal terms
    - Get average
    - Get deviation
    - Print

* Tools
    - Get size
    - Get space

Each function is explained in detail below.

Memory management
-----------------
In this section, two types are explained. The first one, `xg_t`, is for allocating memory whereas the second one, `xgBlock_t` works on an already allocated object or pointer. I did not find an other way of doing thing in Fortran. Of course, in `C`, both types would be the same.

## Allocate memory
To create a new `xg_t`, one has to declare a `xg_t` and allocate its memory through the `xg_init` function:

```fortran
type(xg_t) :: xg1
call xg_init(xg1,space,rows,cols,comm)
```

`space` can either be `SPACE_C`, `SPACE_R` or `SPACE_CR`.
`SPACE_C` is for true complex numbers, `SPACE_R` is for real numbers taken from only the real part of complex numbers whereas `SPACE_CR` is consider both the real and imaginary parts of complex numbers as independent real numbers.
The difference between `SPACE_R` and `SPACE_CR` is only visible for getting and setting the values.
See the *Get/Set values* section for more details.  

`rows` would be the number of rows to consider and `cols` the number of columns.
`comm` is optional and is use if reductions are needed for algebra operations, and even ScaLAPACK if available.

## Free memory
Once you do not need an array anymore, or you do not need the memory anymore, you can free it with `xg_free`:

```fortran
call xg_free(xg1)
```

## Block and sub block

It may happen that you do not want to work on the full array but only a part of the array.
This is the main motivation to introduce the *block* notion.
A block is a part of an array on which you can work.
The main thing to remember is that a block work on an already allocated memory.
Therefore there is no check allocation nor freeing of block.
The `Fortran90` type for this object is `xgBlock_t` and you can create it with

```fortran
type(xgBlock_t) :: xgblockA
```

Basically, this is the main object you will use.
Even if you created a `xg_t` object to allocate memory, you cannot work on `xg_t` but rather on its own block
that you can access with `xg%self`.
`xg%self` is of type `xgBlock_t` and represents the full array you allocated.

To work on a subarray, you will need a `xgBlock_t` object, and build it with the `xg_setBlock` function

```fortran
type(xg_t)::xg1
type(xgBlock_t)::xgblockA
call xg_init(xg1,SPACE_C,maxRows,maxCols)
call xg_setBlock(xg1,xgblockA,firstColumn,rows,cols)
```

`xgblockA` represents a subarray of `xg1` starting from the first column `firstColumn`
(the very first one is 1) of `xg1` and has `rows` rows and `cols` columns.

## Map an object to an already allocated memory space

Usually, you will have a `Fortran90` array that you may want to plug in an `xgBlock_t` or *vice versa* you have a `xgBlock_t` object and want to get a `Fortran90` array variable.
You can achieve this operation with what is called *mapping* and *reverse mapping*.
From `Fortran90` array to `xgBlock_t` it is  *mapping*
On a regular cpu, this should be free, no allocations, no memory transfer, nothing.
But keep in mind that if you `xgBlock_t` represent an array on a GPU or other coprocessor unit, 
this will allocate memory and transfer it!

```fortran
double precision :: array(2,10)
type(xgBlock_t) :: xgblockA
! if array represents complex numbers, it has 10 complex numbers
call xgBlock_map(xgblockA,array,SPACE_C,rows,cols,comm)
! if array represents real numbers, it has 20 real numbers (SPACE_CR)
call xgBlock_map(xgblockA,array,SPACE_CR,rows,cols,comm)
```
`rows` and `cols` are the number of rows and columns for the representation inside the `xgBlock_t`.
It takes into account the space: regardless the space you chose, row*cols is the size of elements.
Example:
`array` if `Fortran90`

| 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10|
|---|---|---|---|---|---|---|---|---|---|
| a | c | e | g | i | k | m | o | q | s |
| b | d | f | h | j | l | n | p | r | t |

```fortran
call xgBlock_map(xgblockA,array,SPACE_C,5,2,comm)
```

gives

|   1   |   2   |
|-------|-------|
| (a,b) | (k,l) | 
| (c,d) | (m,n) | 
| (e,f) | (o,p) | 
| (g,h) | (q,r) | 
| (i,j) | (s,t) | 

`(a,b)` is the complex number at position (1,1)

```fortran
call xgBlock_map(xgblockA,array,SPACE_CR,5,2,comm)
```

gives

|   1   |   2   |
|-------|-------|
|   a   |   f   | 
|   b   |   g   | 
|   c   |   h   | 
|   d   |   i   | 
|   e   |   j   | 

`a` is the real number at position (1,1)

## Get a `Fortran90` array from an object

From `xgBlock_t` to `Fortran90` array is a reverse mapping.
This is exactly the opposite operation as the mapping.

```fortran
double precision :: array(2,10)
type(xgBlock_t) :: xgblockA
! if array represents complex numbers, we take only the 10 first complex numbers
call xgBlock_reverseMap(xgblockA,array,rows,cols)
! if array represents real numbers, it has 20 real numbers (SPACE_CR) to put in a (2,10) array.
call xgBlock_reverseMap(xgblockA,array,rows,cols)
```

Be careful! If you work with complex, the array(2,10) has 10 complex numbers, only **one** row of complex numbers, not 2!!!  
Example:
`xgBlock_t` is (complex)

|   1   |   2   |
|-------|-------|
| (a,b) | (k,l) | 
| (c,d) | (m,n) | 
| (e,f) | (o,p) | 
| (g,h) | (q,r) | 
| (i,j) | (s,t) | 

```fortran
call xgBlock_reverseMap(xgblockA,array,1,10)
```

gives

| 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10|
|---|---|---|---|---|---|---|---|---|---|
| a | c | e | g | i | k | m | o | q | s |
| b | d | f | h | j | l | n | p | r | t |

and
`xgBlock_t` is (real)

|   1   |   2   |
|-------|-------|
|   a   |   f   | 
|   b   |   g   | 
|   c   |   h   | 
|   d   |   i   | 
|   e   |   j   | 

```fortran
call xgBlock_reverseMap(xgblockA,array,2,5)
```

gives

| 1 | 2 | 3 | 4 | 5 |
|---|---|---|---|---|
| a | c | e | g | i |
| b | d | f | h | j |

```fortran
call xgBlock_reverseMap(xgblockA,array,1,10)
```

gives

| 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10|
|---|---|---|---|---|---|---|---|---|---|
| a | b | c | d | e | f | g | h | i | j |

You can access the space of an `xgBlock_t` and the dimensions with 

```fortran
! depending on the version it can be space(xgblockA)
space = getSpace(xgblockA)
! depending on the version it can be cols(xgblockA)
cols = getCols(xgBlock1)
! In a next version there will be a getRows(xgblockA) function too
call xgBlock_getSize(xgBlock1,rows,cols)
```

## Get/Set values from/to `Fortran90` array

Those are old functions that should not be used that much.
Before the mapping and reverse mapping processes, on had to copy values from `Fortran90` array to `xgBlock_t` and *vice versa*
Here it is very import to take care of the declared space for the `xgBlock_t`.
Furthermore, for abinit compliance, a `Fortran90` array is here always a 2D array with the first dimension equals to 2 `double precision :: array(2,:)`.
Each setter/getter exist for `xg_t` and `xgBlock_t`. 
Only the `xgBlock_t` version follows.

```fortran
double precision :: array(2,10)
type(xgBlock_t) :: xgblockA
... Manage memory
call xgBlock_set(xgblockA,array,shiftCol,rows)
```

Let's say array is

| 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10|
|---|---|---|---|---|---|---|---|---|---|
| a | c | e | g | i | k | m | o | q | s |
| b | d | f | h | j | l | n | p | r | t |

`shiftCol = 1` and `rows = 5`.  
The result array in `xgblockA` will be
* if `SPACE_C`

|   1   |   2   |   3   |
|-------|-------|-------|
| xxxxx | (a,b) | (k,l) |
| xxxxx | (c,d) | (m,n) |
| xxxxx | (e,f) | (o,p) |
| xxxxx | (g,h) | (q,r) |
| xxxxx | (i,j) | (s,t) |

* if `SPACE_CR`
Here we set 5 rows of complex numbers so there are 5*2=10 rows of reals

|   1   |   2   |   3   |
|-------|-------|-------|
| xxxxx |   a   |   b   |
| xxxxx |   c   |   d   |
| xxxxx |   e   |   f   |
| xxxxx |   g   |   h   |
| xxxxx |   i   |   j   |
| xxxxx |   k   |   l   |
| xxxxx |   m   |   n   |
| xxxxx |   o   |   p   |
| xxxxx |   q   |   r   |
| xxxxx |   s   |   t   |

Just to be clear, the first half columns are the real parts and the second half parts are the imaginary parts.
This is absolutely not important since both are decoupled.

* if `SPACE_R`

Here we only take the reals parts of the number so there is actually 5 rows.

|   1   |   2   |   3   |
|-------|-------|-------|
| xxxxx |   a   |   k   |
| xxxxx |   c   |   m   |
| xxxxx |   e   |   o   |
| xxxxx |   g   |   q   |
| xxxxx |   i   |   s   |

The getter works to reverse the `set` operation.

## Copy an object to an other one.

If you want to copy a `xgBlock_t` into an other one, then you can use this functions.
As in the BLAS `copy` function, you can specify the incrementation for each object

```fortran
call xgBlock_copy(xgblockA,xgblockB,inc1,inc2)
```

This will copy `xgblockA` by incrementation of `inc1` into `xgblockB` by incrementation of `inc2`

## Pack a matrix to Upper/Lower Triangular Matrix
Packing is only usefull if you intent to use LAPACK functions that need packing.
Both upper and lower triangular packing are implemented.

```fortran
call xgBlock_pack(xgblockA,xgblockB,uplo)
```

`xgblockA` will be packed into `xgblockB` according to the value of `uplo`: *l* or *L* for lower packing and *u* or *U* for upper packing (See BLAS documentation for more explanations).

## Reshape

This tool is only here if the shape of the block should be changed.

```fortran
call xgBlock_reshape(xgblockA,newShape)
```

`newShape` is a 2 values array, the first one for the number of rows and the second one for the number of columns.

Here were presented the tools to allocate, free and manage memory blocks.
The next part will focus on BLAS functions and ease of use compare to bare BLAS functions.

BLAS operations
---------------

All Blas functions have input variable names identical to real BLAS calls. 
Missing arguments are not needed with this library.
Please refer to blas documentation at
[Netlib documentation](http://www.netlib.org/lapack/explore-html/dir_8ff1c41005a6034efef54f08b5a85d05.html)

## POTRF
## TRSM
## GEMM
## ADD
## Scale

LAPACK operations
-----------------
All Lapack functions have input variable names identical to real Lapack calls. 
Missing arguments are not needed with this library.
Please refer to blas documentation at
[Netlib documentation](http://www.netlib.org/lapack/explore-html/df/d9a/group__complex16_h_eeigen.html)

## HEEV
## HEEVD
## HPEV
## HPEVD
## HEGV
## HEGVX
## HPGVX
## HPGVD

ScaLAPACK
---------

!!! warning

    At the present time (2018/10), none of scalapack implementation is thread safe.
    ELPA should work but requires `MPI_THREAD_MULTIPLE` to work which is not yet available on our architectures! 
    So it is not tested and for security, scalapack/elpa is disabled at runtime with threads.

_______________________________

Before using a ScaLAPACK function, one has to check if it is possible or not (or event efficient).
The `xgScalapack_init` function is design for this purpose and is very similar to what was already done in the old implementation but in a more flexible way.

```fortran
use m_xgScalapack
type(xgScalapack_t) :: xgScalapack
integer :: comm
integer :: maxDim
integer :: verbosity
logical :: usable

call xgScalapack_init(xgScalapack,comm,maxDim,verbosity,usable)
```

`comm` is the MPI communicator to use. 
Note that not all of the processes available will be used.
`maxDim` is the maximum dimension of a matrix that will be used with this `xgScalapack` object.
`verbosity` is 0 or positive value.
If 0 then nothing is printed. 
If positive, then the number of processes inside the communicator `comm` is displayed.

To configure the number of processus that will be selected, you must invoke the function `xgScalapack_config` before calling `xgScalapack_init`.

```fortran
call xgScalapack_config(config,rankpp)
```

where `config` is one of `{SLK_AUTO,SLK_DISABLED}` or any positive value. 
A positive value will force scalapack to use exactly `config` processes.
In `SLK_AUTO` mode, `maxDim` will drive the number of processes to use on the fly.
Basically it is the maximum size of a matrix that a processes can hold.
For instance, if the full matrix is `2000x2000` and `maxDim=500`, then scalapack will use at most 4 processes.
When `config=SLK_DISABLED`, scalapack will not be used and `usable` will be set to `false`.
In addition, if the matrix is too small with respect to the configuration of the moddule, `usable` is set to `false`.
`usable` is set to true only if scalapack can be used.
In this case, subcommincators are created in the `xgScalapack` object.
Do not forget to call `xgScalapack_free(xgScalapack)` to free memory and destroy communicators.

With threads, `usable` is always set to `false`

Calling the following functions when `usable=false` is not tested and will crash (I think) or give trash results !

## PHEEV

This is equivalent to `HEEV` to solve `matrixA*X=eigenvalues*X`

```fortran
call xgScalapack_heev(xgScalapack,matrixA,eigenvalues)
```

`xgScalapack` is a previously initialized `xgScalapac_t` object, `matrixA` is the matrix to diagonalize and `eigenvalues` the eigenvalues.
Eigenvectors are placed in `matrixA` at the end.

## PHEGV

This is equivalent to `HEGV` to solve `matrixA*X=eigenvalues*matrixB*X`

```fortran
call xgScalapack_hegv(xgScalapack,matrixA,matrixB,eigenvalues)
```

Eigenvectors are placed in `matrixA` at the end.

Custom extensions
-----------------

## Array Shift

This function is the same as the standard fortran `cshift` function.
It is just an interface.

```fortran
  integer :: nshift
  integer :: shiftdim
  call xgBlock_cshift(xgBlock,nshift,shiftdim)
```

`nshift` is the shift to perform along the `shiftdim` dimension.
See fortran documentation for more information.

## Colwise Norm 2

This function is usefull to compute the square L2 norm -- meaning dot product -- of each column.
The function also provide information on min and max if there are present.

```fortran
  call xgBlock_colwiseNorm2(xgBlock,dot,max_val,max_elt,min_val,min_elt)
```

`dot` is the resulting dot product (BLAS call) for each column of `xgBlock`.
Therefore `dot` is a `SPACE_R` column vector with `xgBlock%cols` rows.
`max_val`, `min_val` are the maximal and minimal values of the `dot` vector and `max_elt`, `min_elt` are the indices of maximal and minimal values.
All those 4 variables are optional.

## Colwise $$Y-aX$$

The colwise operation done here is done column by columns.
It is a useful function to compute residuals for eigen problems.
Let suppose you solve HX=eX or HX=eBX, then calling this function will compute HX-eX or HX-eBX for each eigenvector of the matrix X.

```fortran
  call xgBlock_colwiseCymax(xgBlockA, da, xgBlockB,xgBlockW)
```

Here the operation is `xgBlockA(:,i) = xgBlockW(:,i)-da(i)*xgBlockB(:,i)`.
`da` is a `xgBlock_t` colum vector of `xgBlockA%cols` rows. 
In our example, `xgBlockA` is HX and `xgBlockW` is X or BX and `da` is the eigenvalues vector.
*Note:* this function is very similar to the BLAS `saxpy` function by the last one is not used
because the minus operation would require an array copy that I want to avoid.

## Set to 0

Nullify --set to 0-- all coefficient of the block.

```fortran
  call xgBlock_zero(xgBlock)
```

## Set to Identity

This function just set the diagonal to ones. 
The off-diagonal coefficients are not modified.
If you want to build the identity matrix, first call the `xgBlock_zero` function.

```fortran
  call xgBlock_one(xgBlock)
```

## Set the diagonal

Impose the diagonal of a matrix.

```fortran
  call xgBlock_diagonal(xgBlock,diag)
```

`diag` is of type `xgBlock_t` is is a column vector.
It is the diagonal to impose to `xgBlock` object.

## Set the diagonal and zero off-diagonal terms

Only keep the diagonal termes and set the off-diagonal terms to 0.

```fortran
  call xgBlock_diagonalOnly(xgBlock)
```

## Get average

Compute the average of the full block

```fortran
  call xgBlock_average(xgBlock,average)
```

`average` is the resulting real number.

## Get deviation

Compute the deviation of the full block.

```fortran
  call xgBlock_deviation(xgBlock,deviation)
```

`deviation` is the resulting real number.

## Print

As obvious as it seems, this function just print in a convenient way the data inside the block.

```fortran
  call xgBlock_print(xgBlock,outunit)
```

`xgBlock` is the block to print and `outunit` is the unit number in which the data will be written.
---
description: Tools available for YAML based tests
authors: TC
---

# YAML-based test suite: Fortran and Python API

This page gives an overview of the internal implementation in order 
to facilitate future developments and extensions.
In the first part we discuss how to write new Yaml documents using the low-level Fortran API.
In the second part, we describe the python implementation and the steps required to 
register a new Yaml document in the framework.
Finally, we provide recommendations for the name of the variables used in Yaml documents in 
order to improve readability and facilitate the integration between Yaml documents and the python code.

## Fortran API

The low-level Fortran API consists of the following modules:

**m_pair_list.F90**:

: a Fortran module for constructing dictionaries mapping strings to values.
  Internally, the dictionary is implemented in C in terms of a list of key-value pairs.
  The pair list can dynamically hold either real, integer or string values.
  It implements the basic getter and setter methods and an iterator system to allow looping
  over the key-value pairs easily in Fortran.
  <!--
  Lists have been chosen over hash tables because, in this particular context,
  performance is not critical and the O(n) cost required to locate a key is negligible
  given that there will never be a lot of elements in the table.
  -->

**m_stream_string.F90**:

:  a Fortran module providing tools to manipulate variable length
  strings with a file like interface (**stream** object)

**m_yaml_out.F90:**

:  a Fortran module to easily output YAML documents from Fortran.
  This module represents the main entry point for client code and may be used
  to implement methods to output Fortran objects.
  It provides basic routines to produce YAML documents based on a mapping at the root
  and containing well formatted (as human readable as possible and valid YAML) 1D and 2D arrays of numbers
  (integer or real), key-value mapping (using pair lists), list of key-value
  mapping and scalar fields.
  Routines support tags for specifying special data structures.

**m_neat.F90** (NEw Abinit Test system):

: a higher level module providing Fortran procedures to create specific
  documents associated to important physical properties. Currently routines
  for total energy components and ground state results are implemented.

<!--
A more detailed example is provided [below](#creating-a-new-document-in-fortran).
The new infrastructure has been designed with extensibility and ease-of-use in mind.
-->
From the perspective of the developer, adding a new YAML document requires three steps:

1. Implement the output of the YAML document in Fortran using the pre-existent API.
   Associate a **unique tag** to the new document. The tag identify the document and
   its structure.

2. register the tag and the associated class in the python code. 
   Further details about this procedure can be found below.

3. Create a YAML configuration file defining the quantities that should be compared
   with the corresponding tolerances.

An example will help clarify. Let's assume we want to implement the output of
the `!Etot` document with the different components of the total free energy.
<!--
The workflow is split in two parts. The idea is to separate the computation
from the composition of the document, and separate the composition of the
document from the actual rendering.
-->
The Fortran code that builds the list of (key, values) entries will look like:
**MG: The Fortran examples should be refactored and merged with the text in Creating a new document in Fortran**

```fortran
! Import modules
use m_neat, only: neat_etot
use m_pair_list, only: pair_list

real(dp) :: etot
type(pair_list) :: e_components  ! List of (key, value) entries

call e_components%set("comment", s="Total energie and its components")
call e_components%set("Total Energy", r=etot)

! footer of the routine, once all data have been stored
call neat_etot(e_components, unit)
```

In *47_neat/m\_neat.F90* we will implement *neat\_etot*:

```fortran
subroutine neat_etot(components, unit)

type(pair_list), intent(in) :: components
integer, intent(in) :: unit

call yaml_single_dict("Etot", "", components, 35, 500, width=20, &
                      file=unit, real_fmt='(ES20.13)')
 !
 ! 35 -> max size of the labels, needed to extract from the pairlist,
 ! 500 -> max size of the strings, needed to extract the comment
 !
 ! width -> width of the field name side, permit a nice alignment of the values

end subroutine neat_etot
```

### Creating a new document in Fortran

Developers are invited to browse the sources of `m_neat` and `m_yaml_out` to
have a comprehensive overview of the available tools. Indeed the routines are
documented and commented and creating a hand-written reference is likely to go
out of sync quicker than on-site documentation. Here we show a little example to
give the feeling of the process.

The simplest way to create a YAML document have been introduced above with the
use of `yaml_single_dict`. However this method is limited to scalars only. It is
possible to put 1D and 2D arrays, dictionaries and even tabular data.
The best way to create a new document is to have a routine `neat_my_new_document` 
that will take all required data in argument and will do all the formatting work at once.
This routine will declare a `stream_string` object to build the YAML document
inside, open the document with `yaml_open_doc`, fill it, close it with
`yaml_close_doc` and finally print it with `wrtout_stream`.
Here come a basic skeleton of `neat` routine:

```fortran
subroutine neat_my_new_document(data_1, data_2,... , iout)
  ! declare your pieces of data... (arrays, numbers, pair_list...)
  ...
  integer,intent(in) :: iout  ! this is the output file descriptor
!Local variables-------------------------------
  type(stream_string) :: stream

  ! open the document
  call yaml_open_doc('my label', 'some comments on the document', stream=stream)

  ! fill the document
  ...

  ! close and output the document
  call yaml_close_doc(stream=stream)
  call wrtout_stream(stream, iout)

end subroutine neat_my_new_document
```

Suppose we want a 2D matrix of real number with the tag `!NiceMatrix` in our document
for the field name 'that matrix' we will add the following to the middle section:

```fortran
call yaml_add_real2d('that matrix', dimension_1, dimension_2, mat_data, &
                     tag='NiceMatrix', stream=stream)
```

Other `m_yaml_out` routines provide a similar interface: first the label, then the
data and its structural metadata, then a bunch of optional arguments for
formatting, adding tags, tweaking spacing etc.

## Python API

<!--
Roughly speaking, one can group the python modules into three different parts:

- the *fldiff* algorithm
- the interface with the PyYAML library and the parsing of the output data
- the parsing of the YAML configuration file and the testing logic
-->

- The fldiff algorithm has been slightly modified to extract YAML
  documents from the output file and store them for later treatment.

- Newt tools have been written to facilitate the creation of new python classes
  corresponding to YAML tags for implementing new logic operating on the extracted data.
  These tools are very easy to use via class decorators.

- These tools have been used to create basic classes for futures tags, among
  other classes that directly convert YAML list of numbers into NumPy arrays.
  These classes may be used as examples for the creation of further tags.

- A parser for test configuration have been added and all facilities to do
  tests are in place.

- A command line tool `testtools.py` to allow doing different manual actions
  (see Test CLI)


### fldiff algorithm

The machinery used to extract data from the output file is defined in *~abinit/tests/pymods/data_extractor.py*. 
This module takes cares of the identification of the YAML documents inside the output file including 
the treatment of the meta-characters found in the first column following the original fldiff.pl implementation.

*~abinit/tests/pymods/fldiff.py* is the main driver called by *testsuite.py*.
This part implements the legacy fldiff algorithm and uses the objects defined in *yaml_tools*
to perform the YAML-based tests and produce the final report.

### Interface with the PyYAML library

The interface with the PyYAML library is implemented in *~abinit/tests/pymods/yaml_tools/*.
The `__init__.py` module exposes the public API that consists in:

- the `yaml_parse` function that parses documents from the output files
- the `Document` class that gives an interface to an extracted document with its metadata.

<!--
- flags `is_available` (`True` if both PyYAML and Numpy are available) and
  `has_pandas` (`True` if Pandas is available)
-->

- *register_tag.py* defines tools to register tags.
   See below for further details.

### YAML-based testing logic

The logic used to parse configuration files is defined in *meta_conf_parser.py*. 
It contains the creation of config trees, the logic to register constraints and
parameters and the logic to apply constraints.
The `Constraint` class hosts the code to build a constraint, to identify candidates
to its application and to apply it. The identification is based on the type of the
candidate which is compared to a reference type or as set of types.

## How to extend the test suite

### Main entry points

There are three main entry points to the system that are discussed in order of increasing complexity.
The first one is the YAML configuration file.
It does not require any Python knowledge, only a basic comprehension of the conventions
used for writing tests. Being fully *declarative* (no logic) it should be quite easy
to learn its usage from the available examples.
The second one is the *~abinit/tests/pymods/yaml_tests/conf_parser.py* file. It
contains the declarations of the available constraints and parameters. A basic
python understanding is required in order to modify this file. Comments and doc strings
should help users to grasp the meaning of this file. More details available
[in this section](#constraints-and-parameters-registration).

The third one is the file *~abinit/tests/pymods/yaml_tests/structures/*. It
defines the structures used by the YAML parser when encountering a tag (starting
with !), or in some cases when reaching a given pattern (__undef__ for example).
The *structures* directory is a package organised by features (ex: there is a
file *structures/ground_state.py*). Each file define structures for a given
feature. All files are imported by the main script `structures/__init__.py`.
Even if the abstraction layer on top of the _yaml_ module should help, it is
better to have a good understanding of more "advanced" python concepts like
*inheritance*, *decorators*, *classmethod* etc.

## Tag registration and classes implicit methods and attributes

### Basic tag registration tools

The *register_tag.py* module defines python decorators that are used to build python classes 
associated to Yaml documents.

`yaml_map`
: Registers a structure based on a YAML mapping. The class must provide a
  method `from_map` that receives a `dict` object as argument and returns an instance of
  the class. `from_map` should be a class method, but might be a normal method if
  the constructor accepts zero arguments.

`yaml_seq`
: Register a structure based on a YAML sequence/list. The class must provide
  a `from_seq` method that takes a `list` as input and returns an instance of the class.

`yaml_scalar`
: Register a structure based on a YAML scalar/anything that does not fit in the
  two other types but can be a string. The class must provide a `from_scalar`
  method that takes a `string` in argument and returns an instance of the class. It
  is useful to have complex parsing. A practical case is the CSV table parsed by pandas.

`yaml_implicit_scalar`
: Provide the possibility to have special parsing without explicit tags. The
  class it is applied to should meet the same requirements than for `yaml_scalar`
  but should also provide a class attribute named `yaml_pattern`. It can be either
  a string or a compile regex and it should match the expected structure of the
  scalar.

`auto_map`
: Provide a basic general purpose interface for map objects:
  - dict-like interface inherited from `BaseDictWrapper` (`get`, `__contains__`,
    `__getitem__`, `__setitem__`, `__delitem__`, `__iter__`, `keys` and `items`)
  - a `from_map` method that accept any key from the dictionary and add them as
  attributes.
  - a `__repr__` method that show all attributes

`yaml_auto_map`
: equivalent to `auto_map` followed by `yaml_map`

`yaml_not_available_tag`
: This is a normal function (not a decorator) that take a tag name and a message as
  arguments and will register the tag but will raise a warning each time the tag
  is used. An optional third argument `fatal` replace the warning by an error if
  set to `True`.


### Implicit methods and attributes

Some class attributes and methods are used here and there in the tester logic
when available. They are never required but can change some behaviour when provided.

`_is_base_array` Boolean class attribute
: Assert that the object derived from `BaseArray` when set to `True` (`isinstance`
  is not reliable because of the manipulation of the `sys.path`)

`_not_available` Boolean class attribute
: set to `True` in object returned when a tag is registered with
  `yaml_not_available_tag` to make all constraints fail with a more
  useful message.

`is_dict_like` Boolean class attribute
: Assert that the object provide a full `dict` like interface when set to
  `True`. Used in `BaseDictWrapper`.

`__iter__` method
: Standard python implicit method for iterables. If an object has it the test
  driver will crawl its elements.

`get_children` method
: Easy way to allow the test driver to crawl the children of the object.
  It does not take any argument and return a dictionary `{child_name: child_object ...}`.

`has_no_child`  Boolean class attribute
: Prevent the test driver to crawl the children even if it has an `__iter__`
  method (not needed for strings).

`short_str` method
: Used (if available) when the string representation of the object is too long
  for error messages. It should return a string representing the object and not
  too long. There is no constraints on the output length but one should keep
  things readable.

## Constraints and parameters registration

### Add a new parameter

To have the parser recognise a new token as a parameter, one should edit the
*~abinit/tests/pymods/conf_parser.py*.

The `conf_parser` variable in this file have a method `parameter` to register a
new parameter. Arguments are the following:

- `token`: mandatory, the name used in configuration for this parameter
- `default`: optional (`None`), the default value used when the parameter is
  not available in the configuration.
- `value_type`: optional (`float`), the expected type of the value found in
  the configuration.
- `inherited`: optional (`True`), whether or not an explicit value in the
  configuration should be propagated to deeper levels.

Example:

```python
conf_parser.parameter('tol_eq', default=1e-8, inherited=True)
```

### Adding a constraint

To have the parser recognise a new token as a constraint, one should also edit
*~abinit/tests/pymods/conf_parser.py*.

`conf_parser` has a method `constraint` to register a new constraint. It is
supposed to be used as a decorator (on a function) that takes keywords arguments. 
The arguments are all optional and are the following:

- `name`: (`str`) the name to be used in config files. If not
  specified, the name of the function is used.
- `value_type`: (`float`) the expected type of the value found in the
  configuration.
- `inherited`: (`True`), whether or not the constraint should be propagated to
  deeper levels.
- `apply_to`: (`'number'`), the type of data this constraint can be applied to.
  This can be a type or one of the special strings (`'number'`, `'real'`,
  `'integer'`, `'complex'`, `'Array'` (refer to numpy arrays), `'this'`) `'this'`
  is used when the constraint should be applied to the structure where it is
  defined and not be inherited.
- `use_params`: (`[]`) a list of names of parameters to be passed as argument to
  the test function.
- `exclude`: (`set()`) a set of names of constraints that should not be applied
  when this one is.
- `handle_undef`: (`True`) whether or not the special value `undef`
  should be handled before calling the test function.
  If `True` and a `undef` value is present in the data the test will fail or
succeed depending on the value of the special parameter `allow_undef`.
  If `False`, `undef` values won't be checked. They are equivalent to *NaN*.

The decorated function contains the actual test code. It should return `True` if
the test succeed and either `False` or an instance of `FailDetail` (from
*common.py*) if the test failed.

If the test is simple enough one should use `False`.  However if the
test is compound of several non-trivial checks `FailDetail` come in handy to
tell the user which part failed. When you want to signal a failed test and
explaining what happened return `FailDetail('some explanations')`. The message
passed to `FailDetail` will be transmitted to the final report for the user.

Example with `FailDetail`:

```python
@conf_parser.constraint(exclude={'ceil', 'tol_abs', 'tol_rel', 'ignore'})
def tol(tolv, ref, tested):
    '''
        Valid if both relative and absolute differences between the values
        are below the given tolerance.
    '''
    if abs(ref) + abs(tested) == 0.0:
        return True
    elif abs(ref - tested) / (abs(ref) + abs(tested)) >= tolv:
        return FailDetail('Relative error above tolerance.')
    elif abs(ref - tested) >= tolv:
        return FailDetail('Absolute error above tolerance.')
    else:
        return True
```

### How to add a new tag

Pyyaml offer the possibility to directly convert YAML documents to a
Python class using tags. To register a new tag, edit the file `~abinit/tests/pymods/yaml_tools/structures.py`. 
In this file there are several
classes that are *decorated* with one of `@yaml_map`, `@yaml_scalar`, `@yaml_seq` or `@yaml_auto_map`.
These [decorators](https://realpython.com/primer-on-python-decorators/) are the functions that actually
register the class as a known tag.
Whether you should use one or another depend on the organization of the data in the YAML document.
Is it a dictionary, a scalar or a list/sequence ?

In the majority of the cases, one wants the `tester` to browse the children of the structure and apply
the relevant constraints on it and to access attributes through their original name
from the data. In this case, the simpler way to register a new tag is to use
`yaml_auto_map`. For example to register a tag Etot that simply register all
fields from the data tree and let the tester check them with `tol_abs` or
`tol_rel` we would put the following in *~abinit/tests/pymods/yaml_tools/structures/ground_state.py*:

```python
@yaml_auto_map
class Etot(object):
    pass
```

Now the name of the class "Etot" is associated with a tag recognized by the YAML
parser.

`yaml_auto_map` does several things for us:

- it gives the class a `dict` like interface by defining relevant methods, which
  allows the `tester` to browse the children
- it registers the tag in the YAML parser
- it automatically registers all attributes found in the data tree as attributes
  of the class instance. These attributes are accessible through the attribute
  syntax (ex: `my_object.my_attribute_unit`) with a normalized name (basically remove
  characters that cannot be in a python identifier like spaces and punctuation characters)
  or through the dictionary syntax with their original name (ex: `my_object['my attribute (unit)']`)

Sometimes one wants more control over the building of the class instance.
This is what `yaml_map` is for.
Let suppose we still want to register tag but we want to select only a subset of
the components for example. We will use `yaml_map` to gives us control over the
building of the instance. This is done by implementing the `from_map` class
method (a class method is a method that is called from the class instead of
being called from an instance). This method take in argument a dictionary built
from the data tree by the YAML parser and should return an instance of our class.

```python
@yaml_map
class EnergyTerms(object):
    def __init__(self, kin, hart, xc, ew):
        self.kinetic = kin
        self.hartree = hart
        self.xc = xc
        self.ewald = ew

    @classmethod
    def from_map(cls, d):
        # cls is the class (Etot here but it can be
        # something else if we subclass Etot)
        # d is a dictionary built from the data tree

        kin = d['kinetic']
        hart = d['hartree']
        xc = d['xc']
        ew = d['Ewald energy']
        return cls(kin, hart, xc, ew)
```

Now we fully control the building of the structure, however we lost the ability
for the tester to browse the components to check them. If we want to only make
our custom check it is fine. For example we can define a method
`check_components` and use the `callback` constraint like this:

In `~abinit/tests/pymods/yaml_tools/structure/ground_state.py`

```python
@yaml_map
class EnergyTerms(object):
    # same code as the previous example

    def check_components(self, tested):
        # self will always be the reference object
        return (
            abs(self.kinetic - other.kinetic) < 1.0e-10
            and abs(self.hartree - other.hartree) < 1.0e-10
            and abs(self.xc - other.xc) < 1.0e-10
            and abs(self.ewald - other.ewald) < 1.0e-10
        )
```

In the configuration file

```yaml
EnergyTerms:
    callback:
        method: check_components
```

However it can be better to give back the control to `tester`. For that purpose
we can implement the method `get_children` that should return a dictionary of the
data to be checked automatically. `tester` will detect it and use it to check
what you gave him.

```python
@yaml_map
class EnergyTerms(object):
    # same code as the previous example

    def get_children(self):
        return {
            'kin': self.kinetic,
            'hart': self.hartree,
            'xc': self.xc,
            'ew': self.ewald
        }
```

Now `tester` will be able to apply `tol_abs` and friends to the components we gave him.

If the class has a __complete__ dict-like read interface (`__iter__` yielding
keys, `__contains__`, `__getitem__`, `keys` and `items`) then it can have a
class attribute `is_dict_like` set to `True` and it will be treated as any other
node (it not longer need `get_children`). `yaml_auto_map` registered classes
automatically address these requirements.

`yaml_seq` is analogous to `yaml_map` however `to_map` became `to_seq` and the
YAML source data have to match the YAML sequence structure that is either `[a, b, c]` or

```yaml
- a
- b
- c
```

The argument passed to `to_seq` is a list. If one wants `tester` to browse the
elements of the resulting object one can either implement a `get_children`
method or implement the `__iter__` python special method.

If for some reason a class have the `__iter__` method implemented but one does
__not__ want tester to browse its children (`BaseArray` and its subclasses are
such a case) one can defined the `has_no_child` attribute and set it to `True`.
Then tester won't try to browse it. Strings are a particular case. They have the
`__iter__` method but will never been browsed.

To associate a tag to anything else than a YAML mapping or sequence one can use
`yaml_scalar`. This decorator expect the class to have a method `from_scalar`
that takes the raw source as a string in argument and return an instance of the
class. It can be used to create custom parsers of new number representation.
For example to create a 3D vector with unit tag:

```python
@yaml_scalar
class Vec3Unit(object):

    def __init__(self, x, y, z, unit):
        self.x, self.y, self.z = x, y, z
        self.unit = unit

    @classmethod
    def from_scalar(cls, raw):
        sx, sy, sz, unit, *_ = raw.split()  # split on blanks
        return cls(float(sx), float(sy), float(sz), unit)
```

With that new tag registered the YAML parser will happily parse something like

```yaml
kpt: !Vec3Unit 0.5 0.5 0.5 Bohr^-1
```

Finally when the scalar have a easily detectable form one can create an implicit
scalar. An implicit scalar have the advantage to be detected by the YAML parser
without the need of writing the tag before as soon as it match a given regular
expression. For example to parse directly complex numbers we could (naively) do
the following:

```python
@yaml_implicit_scalar
class YAMLComplex(complex):

    # A (quite rigid) regular expression matching complex numbers
    yaml_pattern = r'[+-]?\d+\.\d+) [+-] (\d+\.\d+)i'

    # this have nothing to do with yam_implicit_scalar, it is just the way to
    # create a subclass of a python *native* object
    @staticmethod
    def __new__(*args, **kwargs):
        return complex.__new__(*args, **kwargs)

    @classmethod
    def from_scalar(cls, scal):
        # few adjustment to match the python restrictions on complex number
        # writing
        return cls(scal.replace('i', 'j').replace(' ', ''))
```

## Recommend conventions for Yaml documents

This section discusses the basic conventions that should be followed when writing YAML documents in Fortran.
Note that these conventions are motivated by technical aspects that will facilitate the integration with the
python language as well as the implementation of post-processing tools.

* The tag name should use CamelCase so that one can directly map tag names to python classes
  that are usually given following this convention (see also [PEP8](https://www.python.org/dev/peps/pep-0008/)).

* The tag name should be self-explanatory.

* Whenever possible, the keywords should be a **valid python identifier**. This means one should avoid white spaces
  as much possible and avoid names starting with non-alphabetic characters. White spaces should be replaced by
  underscores. Prefer lower-case names whenever possible.

* By default, quantities are supposed to be given in **atomic units**. If the document contains quantities given
  in other units, we recommended to encode this information in the name using the syntax: `foo_eV`.

*  Avoid names starting with an underscore because these names are reserved for future additions 
   in the python infrastructure.

* `tol_abs`, `tol_rel`, `tol_vec`, `tol_eq`, `ceil`, `ignore`, `equation`, `equations`, `callback` and `callbacks` are reserved 
   keywords that shall not be used in Yaml documents.

* The *comment* field is optional but it is recommended especially when the purpose of the document is not obvious.

An example of well-formed document

```yaml
--- !EnergyTerms
comment             : Components of total free energy (in Hartree)
kinetic_energy      :  5.279019930263079807E+00
hartree_energy      :  8.846303409910728499E-01
xc_energy           : -4.035286123400158687E+00
total_energy_eV     : -2.756386620520307815E+02
...
```

An example of Yaml document that does not follow our guidelines:

```yaml
--- !ETOT
Kinetic energy      :  5.279019930263079807E+00
Hartree energy      :  8.846303409910728499E-01
_XC energy          : -4.035286123400158687E+00
Total energy(eV)    : -2.756386620520307815E+02
equation            : 1 + 1 = 3
...
```

<!--
In a nutshell:

* no savage usage of write statements!
* In the majority of the cases, YAML documents should be opened and closed in the same routine!
* To be discussed: `crystal%yaml_write(stream, indent=4)`
* One should also promote the use of python-friendly keys: white space replaced by underscore, no number as first character,
  tags should be CamelCase
* Is Yaml mode supported only for main output files? What happens to the other files in the files\_to\_test section?
Tags are now mandatory for documents
* A document with a tag is considered a standardized document i.e. a document for which there's
  an official commitment from the ABINIT community to maintain backward compatibility.
  Official YAML documents may be used by third-party software to implement post-processing tools.


: The *label* field appears in all data document. It should be a unique identifier
  at the scale of an _iteration state_. The tag is not necessarily unique, it
  describes the structure of the document and there is no need to use it unless
  special logic have to be implemented for the document. 
-->

<!--
## Further developments

This new YAML-based infrastructure can be used as building block to implement the
high-level logic required by more advanced integration tests such as:

Parametrized tests

: Tests in which multiple parameters are changed either at the level of the input variables
  or at the MPI/OpenMP level.
  Typical example: running calculations with [[useylm]] in [0, 1] or [[paral_kgb]] = 1 runs
  with multiple configurations of [[npfft]], [[npband]], [[npkpt]].

Benchmarks

: Tests to monitor the scalability of the code and make sure that serious bottlenecks are not
  introduced in trunk/develop when important dimension are increased (e.g.
  [[chksymbreak]] > 0 with [[ngkpt]] > 30\*\*3).
  Other possible applications: monitor the memory allocated to detect possible regressions.

Interface with AbiPy and Abiflows

: AbiPy has its own set of integration tests but here we mainly focus on the python layer
  without testing for numerical values.
  Still it would be nice to check for numerical reproducibility, especially when it comes to
  workflows that are already used in production for high-throughput applications (e.g. DFPT).
-->
## Information on the format 6 for pseudopotentials

The format 6 for ABINIT pseudopotentials allows to use pseudopotentials
generated from the FHI98PP code (Fritz-Haber-Institute, Berlin).
This code is available at this [URL](http://www.FHI-Berlin.MPG.DE/th/fhi98md/fhi98PP).

A few lines must be added to the file generated by the FHI98PP code,
and are described in the present file. ABINITv1.9 is able to
read format 6 pseudopotential files without core correction.
ABINITv2.2 is able to read them with core correction (thanks to AF).

# The original cpi file

We will suppose that the user has been able to generate a `cpi` file from the FHI98PP code. 
There is one such file in the `~abinit/tests/Psps_for_tests` directory, 
with the name `al_h.cpi` (the FHI98PP code generated a file named `al:h.cpi`, 
but the name was changed, to allow portability under DOS/Windows) .
It begins by:

    0.30000000000000E+01   3
  0.0000    0.0000    0.0000   0.0000
  0.0000    .00e+00   .00e+00
  0.0000    .00e+00   .00e+00
  0.0000    .00e+00   .00e+00
  0.0000    .00e+00   .00e+00
  0.0000    .00e+00   .00e+00
  0.0000    .00e+00   .00e+00
  0.0000    .00e+00   .00e+00
  0.0000    .00e+00   .00e+00
  0.0000    .00e+00   .00e+00
    493  0.10247000000000E+01
   1 0.48076923076923E-03 0.91926957204792E-04 0.72762392428392E+00
   2 0.49264423076923E-03 0.94197553047751E-04 0.72762434262851E+00
   3 0.50481254326923E-03 0.96524232608030E-04 0.72762477651319E+00
    ...
    ...

From these numbers, we will make direct use of:

- those on the first line, i.e. the number of valence electrons ( 0.30000000000000E+01 )
   and the number of pseudopotential components ( 3 )
- the first on the 12th line, i.e. the number of mesh points ( 493 )


# The new file

For the purpose of generating the corresponding file readable by ABINIT (also
found in the `~abinit/tests/Psps_for_tests` directory, with the name `13al.981214.fhi`),
the following seven lines have been added at the beginning of the `al_h.cpi file`:

    Aluminum, fhi98PP : Hamann-type, LDA CA PerdewWang, l=2 local
 13.000  3.000    981214              zatom,zion,pspdat
 6       7        2   2    493     0  pspcod,pspxc,lmax,lloc,mmax,r2well
 0.0     0.0     0.0                  rchrg, fchrg, qchrg
    5--- These two lines are available for giving more information, later
    6
    7-Here follows the cpi file from the fhi98pp code-


Similar lines must be added at the beginning of other cpi files, in order
to make them readable by ABINIT.

Line 1 is simply a header, that might include any information, and that will
be printed without modification in the output of ABINIT.

Line 2 describes:

- the atomic number (zatom);
- the ionic charge (zion, number of valence electrons);
- the date of pseudopotential generation.

The two first information are crucial, the third one is not
really important. The atomic number is 1 for Hydrogen, 8 for Oxygen, and so on.
The ionic charge should be the same number as the first number mentioned
in the cpi file (called 'number of valence electrons').

Line 3 describes:

- the format of the pseudopotential (pspcod; must be 6 for this format);
- the XC functional used to generate the pseudopotential (pspxc ; the
   same numbers as for the input variable ixc should be used,
   see `~abinit/doc/users/abinit_help.html`)
- the maximal angular momentum of the wavefunctions described
   in the pseudopotential file (lmax=0 if only s-wavefunctions are present,
   lmax=1 for s and p wfs, lmax=2 for s, p and d wfs ... ; lmax+1 must be
   equal to the 'number of pseudopotential components' mentioned in the cpi
   file, as the second number of the first line)
- the angular momentum of the potential to be used as local pseudopotential
   (lloc=0 if s-potential is local; lloc=1 if p-potential is local ; ... )
- the number of radial mesh points (mmax, same meaning as in the cpi file;
   first number on the 12th line)
- the last number can be set to 0 .

Line 4 describes 3 parameters needed to read the model core-charge
   if core correction is needed.
- rchrg: radius at which the core charge vanish (i.e. cut-off in a.u.)
- fchrg: amplitude of core charge (if =0 non core-correction performed,
         if >0 core-correction is applied). The specific value of fchrg
         has no physical meaning in fhi98pp scheme of core correction.
- qchrg: integrated model core charge (it's not actually used by
         abinit, and can be set to any value)

  rchrg can be inferred by the pp file. Indeed, if the pp has been generated
  with the core-correction option, at the end of 'cpi' file there will be:

     do i=1,mmax
        rad(i), f(i), f1(i), f2(i)
     end do

  that is, the radial grid, the core charge (f), and their first(f1)
  and second (f2) derivative on the radial mesh; rchrg can be equal
  to any value of rad(i) to which f,f1 and f2 are reasonably vanished.

Line 5-7 have no interest up to now.

A pseudopotential with format 6 will be treated by
the routine `psp6in.f`, that calls `psp5lo.f` (local part) and `psp5nl.f` (non-local part). 
There is no `psp6lo.f` or `psp6nl.f`.

The integral of (V(r)+Zion/r) r^2 in psp5lo.f is performed
from 0 to the highest allowed radius (usually about 100 a.u.),
except that beyond 20 a.u. no value of abs(V(r)) larger than 2.0d-8 is tolerated. 
This will allow to cut off spurious behaviour of pseudopotential whose data file 
is written in single precision.
---
authors: MG, XG
plotly: true
---

# Writing Documentation

This page is intended as a quick reference to the Markdown syntax and the extensions
available in the Abinit documentation.
Markdown can be used *almost everywhere*: user guides, tutorials, release notes, theory notes, the
description of the input variables stored in python files inside `abimkdocs`, as well as
in the `TEST_INFO` section of the automatic tests.

As the [original/official Markdown syntax rules](https://daringfireball.net/projects/markdown/syntax#html) &nbsp;
state:

> Markdown’s syntax is intended for one purpose: to be used as a format for writing for the web.
>
> Markdown is not a replacement for HTML, or even close to it. Its syntax is very small, corresponding only
> to a very small subset of HTML tags. The idea is not to create a syntax that makes it easier to insert HTML tags.
> In my opinion, HTML tags are already easy to insert. The idea for Markdown is to make it easy to read, write, and edit prose.
> HTML is a publishing format; Markdown is a writing format. Thus, Markdown’s formatting syntax only addresses issues
> that can be conveyed in plain text.
>
> For any markup that is not covered by Markdown’s syntax, you simply use HTML itself.

Basic Markdown syntax already covers most of our needs and the *Abinit extensions*
([wikilinks](#wikilinks) and [Abinit extensions](#abinit-extensions))
facilitate the integration between the documentation on the website and the new developments done in the gitlab branch.
This page, for example, is entirely written in Markdown with the exception of the last
two sections in which we discuss advanced features requiring some HTML code.

## Markdown quick reference

### Inline styles

| Markdown | Result | Extension required |
| :-- | :-- | :-- |
| `*italics*` | *italics* | --
| `**bold**` | **bold** |  --
| `***bold and italic***` | ***bold and italic*** |  --
| `` `monospace` `` | `monospace` |  --
| `~~strikethrough~~` | ~~strikethrough~~ | [Tilde](http://facelessuser.github.io/pymdown-extensions/extensions/tilde/)
| `CH~3~CH~2~OH` | CH~3~CH~2~OH |  [Tilde](https://facelessuser.github.io/pymdown-extensions/extensions/tilde/)
| `==highlight==` | ==highlight== | [Mark](http://facelessuser.github.io/pymdown-extensions/extensions/mark/)
| `^^underline me^^` | ^^underline me^^ | [Caret](https://facelessuser.github.io/pymdown-extensions/extensions/caret/)

As Markdown is not a "publishing format", providing a way to color text is out-of-scope for Markdown,
but it is possible to use raw HTML code.
For example, the following Markdown text:

```md
Some Markdown text with <span style="color:red">some *red* text</span>.
```

produces: Some Markdown text with <span style="color:red">some *red* text</span>.

For a more complete introduction to Markdown, please consult the
[Markdown Cheatsheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet).

### Lists

Unnumbered lists are created by a **blank line** followed by a star (or a dash) for each line in the list.
For example, after a blank line, the following Markdown text:
```md
* List item A
* List item B
```

This produces

* List item A
* List item B

For numbered list, start with the numbers instead of stars:

```md
1. List item 1
2. List item 2
```

This produces

1. List item 1
2. List item 2

### Code and syntax highlighting

Blocks of code are either fenced by lines with three back-ticks ``` or are indented with **four spaces**.
For example, the Markdown text
~~~text
```
Fenced code has three back-ticks around it.
```
~~~

produces
```
Fenced code has three back-ticks around it.
```

while indenting the code with four space, such as in

```md
    abinit run.abi 2> log &
```

produces

    abinit run.abi 2> log &

Fenced blocks is an alternative form that allows the specification of the language
used for syntax highlighting.
Fortran code, for example, can be included with:

~~~text
```fortran
do ii=1, 10
  write(*,*)"Hello world"
end do
```
~~~

that is displayed as:

```fortran
do ii=1, 10
  write(*,*)"Hello world"
end do
```

To obtain inline highlighting, simply use back-ticks. As an example:
~~~text
Inline `code` has `back-ticks` around it.
~~~

produces:

Inline `code` has `back-ticks` around it.


### Tables

To create a table in Markdown use the syntax:

```md
First Header  | Second Header
------------- | -------------
Content Cell  | Content Cell
Content Cell  | Content Cell
```

that produces:

First Header  | Second Header
------------- | -------------
Content Cell  | Content Cell
Content Cell  | Content Cell


!!! warning
    If the text inside the colum contains pipes (|), enclose it with back-ticks,
    or use a `\` before the pipe.


### Figures

To include figures, use the standard Markdown syntax:

```md
![](../tutorial/bse_assets/tbs2_1.png)
```

![](../tutorial/bse_assets/tbs2_1.png)

For figures with a caption use the [markdown-figures extension](https://github.com/helderco/markdown-figures):

```md
![](../tutorial/bse_assets/tbs5.png)
:   Convergence of BSE optical spectrum wrt $\kk$-point sampling.
    See also [[ngkpt]] and [[shiftk]].
```

![](../tutorial/bse_assets/tbs5.png)
:   Convergence of BSE optical spectrum wrt $\kk$-point sampling.
    See also [[ngkpt]] and [[shiftk]].

The caption can contain Latex equations as well as [Abinit wikilinks](#wikilinks).
`#!html <img>` and `#!html <figure>` elements are automatically centered via CSS directives declared in `extra.css`.

<!--
!!! note
    In the two examples, the location of the png files is given by relative URLs (relative to this page).
    In the first example we have used a root-relative URL (`/tutorial/bse_assets/tbs2_1.png`) where
    the "root" `/` corresponds to the `~abinit/doc` directory if you are running the webserver locally
    or to the Abinit website domain if the documentation is served on the internet.
    In the second example, we have used a relative URL (relative to this page).
    Mkdocs will convert all Markdown URLs into relative URLs so that the website can be easily deployed.
    A concise explanation of absolute, relative and root-relative links is given
    [here](https://mor10.com/html-basics-hyperlink-syntax-absolute-relative-and-root-relative/).
-->

If you need to customize the height and width of the image, use

```md
![](../tutorial/eph4mob_assets/workflow.png ){: style="height:500px;width:400px"}
```

![](../tutorial/eph4mob_assets/workflow.png ){: style="height:500px;width:400px"}

Note that this is not standard markdown but an extension provided by
[Attribute Lists extension](https://python-markdown.github.io/extensions/attr_list/)
that adds a syntax to define attributes on the various HTML elements in markdown’s output.

### Pdf Files

Links to internal pdf files shipped with the Abinit documentation are inserted using the
base name of the pdf file and the [wikilink syntax](#wikilinks):

```
    Please consult the [[pdf:howto_chebfi]] document.
```

that gives: Please consult the [[pdf:howto_chebfi]] document.

!!! note

    The `.pdf` extension at the end of the file is optional.

This is the recommended approach to link pdf documents in the description of the input variables.
In the tutorials and in the theory notes, on the other hand, you may want to
display the pdf file directly in the HTML page.
In this case, use the HTML embed element:

```html
<embed src="../howto_chebfi.pdf" type="application/pdf" width="100%" height="480px">`
```

See the automatically generated [pdf gallery](/theory/documents) for examples.


### Videos

Links to videos can be included with the standard Markdown syntax:

```md
The video below gives an overwiew of the command line options of `runtests.py`

[![asciicast](https://asciinema.org/a/40324.png)](https://asciinema.org/a/40324)
```

that produces:

The video below gives an overwiew of the command line options of `runtests.py`

[![asciicast](https://asciinema.org/a/40324.png)](https://asciinema.org/a/40324)

More advanced features such as video galleries require a bit of HTML/CSS/JS code in the Markdown page.
See for example the [Abinit video gallery](/topics/external_resources#abinit-videos)
built with [lightGallery](http://sachinchoolur.github.io/lightGallery/).


## Links

### Markdown links

To create a link, enclose the link text in brackets (e.g., [Duck Duck Go]) and then follow
it immediately with the URL in parentheses (e.g., (https://duckduckgo.com)).
Alternatively, one can enclose the URLs directly between `<>`.
The table below shows typical scenarios:

| Markdown | Result
| :-- | :--
| `[Duck Duck Go](https://duckduckgo.com)` | [Duck Duck Go](https://duckduckgo.com)
| `<https://www.abinit.org>` | <https://www.abinit.org>
| `[Links for videos section in the same page](#videos)` | [Links to videos section in the same page](#videos)

!!! note

    Links to external websites are signaled with the [fontawesome](http://fontawesome.io/) &nbsp; icon:
    <i class="fa fa-external-link" aria-hidden="true"></i>. See CSS rules in **extra.css**.

Markdown can also be used for creating internal links to **other pages** of the ABINIT website.
The main problem is that you need to know the **relative URL** (or the root-relative URL) of the HTML page you want to refer to.
Unfortunately, this is not so easy since the HTML pages are automatically generated by *mksite.py* and *mkdocs* 
hence you need to understand how these tools work internally.
Just to make things even more complicated, there are different ways to achieve the same result 
(**relative urls**, **root-relative urls**, **mkdocs md syntax**).
Let's try to clarify this point using a very pragmatic approach.

First of all, it is very important to understand that in the *mkdocs.yml* configuration file,
the [use_directory_urls](https://www.mkdocs.org/user-guide/configuration/#use_directory_urls) option is set to True.
This means that mkdocs creates a directory with an *index.html* file for each md page
included in the ABINIT documentation.
For instance, the **abimkdocs.md** and **markdown.md** files located in ~abinit/doc/developers
are converted by mkdocs into the following hierarchy of HTML files:

- /developers/abimkdocs/index.html
- /developers/markdown/index.html

in which the content of e.g. *abimkdocs.md* is used to generate *abimkdocs/index.html*.

!!! important

    In these examples the `/` symbol **does not correspond to the root of your filesystem**
    but to the root directory used by the web-server for serving HTML files.
    Roughly speaking, your ~abinit/doc/ directory is consired as the root node.

We use this convention because it leads to more user-friendly URLs as the abimkdocs page
is now served by the *local web server* at **http://127.0.0.1:8000/developers/abimkdocs**
instead of the lengthier **http://127.0.0.1:8000/developers/abimkdocs.html**.
Once the website is deployed in production, the same page will be served at <https://docs.abinit.org/developers/abimkdocs>
instead of *https://docs.abinit.org/developers/abimkdocs.html*.

At this point it should be clear that the relative path you see on your file system **is not the same**
as the relative URL seen by the web-server.
In other words, you should **add an extra** "../" to the relative path you have on your file system
to account for the creation of the directory.
This syntax is clearly unintuitive and error-prone but it is worth knowing especially if, for some reason,
you need to inject in the md page HTML code with relative hrefs.

Fortunately, you are not obliged to use relative URLs in markdown links since there are two other approaches
that are much more user-friendly and much easier to maintain.
The first one is based on **root-relative URLs** while the second one relies on
[preprocessing operations performed by mkdocs](https://www.mkdocs.org/user-guide/writing-your-docs/#internal-links).

In brief, root-relative URLs can be seen as paths relative to your ~abinit/doc/ directory provided you add
a `/` at the beginning of the string while the mkdocs syntax is activated by using relative paths as seen on your files system
with the crucial difference that you **must include the .md extension** at the end of the URL.
As usual, it is much easier to explain the different cases by examples:

| Markdown | Result | Link Type |
| :-- | :-- | :--
| `[MBPT document](/theory/mbt)` | [MBPT document](/theory/mbt) | root-relative URL (**recommended**)
| `[MBPT document](../theory/mbt.md)` | [MBPT document](../theory/mbt.md) | mkdocs preprocessed relative URL
| `[About topics](/developers/abimkdocs#topics)` | [About topics](/developers/abimkdocs#topics)  | root-relative + HTML fragment  (**recommended**)
| `[About topics](abimkdocs.md#topics)` | [About topics](abimkdocs.md#topics)  | mkdocs preprocessed relative URL + HTML fragment
| `[About topics](../abimkdocs#topics)` | [About topics](../abimkdocs#topics)  | relative URL (no mkdocs preprocessing) + HTML fragment (**don't use it**)

!!! important

    Relative URLs obviously depend on the location of the page containg the link whereas root-relative URLs are invariant.
    It is evident that absolute (file-system dependent) paths such as `~gmatteo/abinit/doc/theory/mbt` won't work when
    the website is deployed so don't use them.


At this point you may ask why do we have all these different approaches and which one should I use when writing the documentation?
To make a long story short, **we strongly suggest to use root-relative URLs**.
The reason is that root-relative URLs look like "absolute paths" so one can easily use regular expressions
to update the links everywhere if md pages are moved around.
The mkdocs syntax is shorter if you are referring to another md page in the same directory but since the ABINIT
website does not have so many nested levels, root-relative URLs are not necessarily much longer than the mkdocs syntax.

The mkdocs syntax has the advantage that mkdocs can immediately check whether the internal URL is correct while building
the web site but, as a matter of fact, we perform a similar check by running linkchecker with our buildbot infrastructure.
This means that if you use root-relative URLs in the docs, mkdocs won't be able to detect broken links while
you are working interactively but the error will be automatically detected by our test farm.

There are however cases in which we would like to have an even **simpler syntax** to automatically generate 
links within our documentation, in particular links to:

* The input variables declared in the `abimkdocs` directory.
* The bibliographic citations declared in `abiref.bib`.
* Input files or pseudopotentials used in the Abinit test suite.
* Website pages commonly mentioned such as e.g. the [[topic:index|topics page]].

For this reason, we use the [extensions API](https://python-markdown.github.io/extensions/api) &nbsp;
provided by python Markdown to extend the syntax of the parser, using the Wikilink syntax so that 
you don't need to know the root-relative or the relative URL of the HTML page you want to link.
Typical cases are discussed in the next sections.

### Wikilinks

The [wikilink syntax](https://python-markdown.github.io/extensions/wikilinks) &nbsp; is used with two pairs of square brackets and possible separators (:, # and |).
In the simple case, this gives <span style="background-color: #E0E0E0;font-size:90%;"> &#91; [name] &#93;</span> although the more general form is

<span style="background-color: #E0E0E0;font-size:90%;"> &#91; [namespace:name#section|text] &#93;</span>

where `namespace`, `section` and `text` are optional (in such case, the adequate separator should not be mentioned).
The namespace is not echoed in the Web page, while if a `text` is given, it will supercede the echo of the
`name` in the Web page (see examples below).

!!! warning

    Do not use parentheses within the pair of double brackets, the whole expression will not be recognized.

When an internal link is recognized, the wikilink string is replaced by the adequate HTML link
There are a couple of names immediately recognized:

* the name of an Abinit input variable e.g. "ecut"  (provided it is mentioned in `variables_abinit.py`)
* the name of a bibliographical reference (provided it is mentioned in `abiref.bib`)
* the path to a file in one of the `~abinit/tests/*/Input` directory
* the path to a reference output file in one of the ~abinit/tests/tuto*/Refs directories
* the label of a section inside the own file

Examples:

| Markdown | Result
| :-- | :--
| `[[ecut]]` | [[ecut]]
| `[[abinit:ecut]]` | [[abinit:ecut]]
| `[[anaddb:dipdip]]` | [[anaddb:dipdip]]
| `[[dipdip@anaddb]]` | [[dipdip@anaddb]]
| `[[cite:Amadon2008]]` | [[cite:Amadon2008]]
| `[[~abinit/tests/tutorial/Input/tbase1_1.abi]]` | [[~abinit/tests/tutorial/Input/tbase1_1.abi]]
| `[[tests/tutorial/Input/tbase1_1.abi]]` | [[tests/tutorial/Input/tbase1_1.abi]]
| `[[test:libxc_41]]` | [[test:libxc_41]]
| `[[tests/tutorial/Refs/tbase1_1.abo]]` |  [[tests/tutorial/Refs/tbase1_1.abo]]
| `[[~abinit/tests/tutorial/Refs/tbase1_1.abo]]` |  [[~abinit/tests/tutorial/Refs/tbase1_1.abo]]
| `[[~abinit/tests/Psps_for_tests/6c_lda.paw]]` | [[~abinit/tests/Psps_for_tests/6c_lda.paw]]
| `[[tests/Psps_for_tests/6c_lda.paw]]` | [[tests/Psps_for_tests/6c_lda.paw]]
| `[[:digit:]]' ` | [[:digit:]]

The input variables for anaddb, optic and aim will be recognized if they are used with
the namespaces `anaddb`, `optic` and `aim`.
One has thus also the choice between the syntax `[[anaddb:dipdip]]` and `[[dipdip@anaddb]]`.
In the first case, only `dipdip` is echoed,  while in the second case, `dipdip@anaddb` is echoed.
This syntax is needed because there's also a `dipdip` variable in Abinit.

A wikilink that starts with `#` is interpreted as an internal link within the page hence

```md
See [[#markdown-quick-reference|this section]] for more info
```

becomes: See [[#markdown-quick-reference|this section]] for more info

although the same result can be obtained with the more readable Markdown syntax:

```md
See [this section](#markdown-quick-reference) for more info
```

To specify the name of the anchor in a bibliographic citation use the syntax with the `|` separator:

    Please consult [[cite:Gonze2016 | the last generic ABINIT article]].

that is rendered in HTML as: Please consult [[cite:Gonze2016 | the last generic ABINIT article]].

The script does a bit of formatting in these examples: it keeps one pair of square brackets
in the case of a bibliographic reference, and addd *~abinit/* in the case of a path.
The syntax `[[test:libxc_41]]` is preferable when documenting new tests in the release notes.
The python code issues a warning in the terminal if the link cannot be established.

!!! note
    Links to input files have a popover with the description of the test.
    Hovering on a citation opens a popover with the title reported in the Bibtex entry.
    Links to variables and internal files use a different font declared in *extra.css*.


### Internal links with namespace

Other internal links can be recognized thanks to the namespace.

Examples:

Namespace      | Markdown                         | Result
-------------  | -------------------------------- |
 `tutorial`    | `[[tutorial:gw1]]`               | [[tutorial:gw1]]
 `tutorial`    | `[[tutorial:index]]`             | [[tutorial:index]]
 `topic`       | `[[topic:BSE]]`                  | [[topic:BSE]]
 `topic`       | `[[topic:index]]`                | [[topic:index]]
 `help`        | `[[help:abinit]]`                | [[help:abinit]]
 `help`        | `[[help:abinit#files-file]]`     | [[help:abinit#files-file]]
 `theory`      | `[[theory:mbt]]`                 | [[theory:mbt]]
 `varset`      | `[[varset:bse]]`                 | [[varset:bse]]
 `cite`        | `[[cite:Amadon2008]]`            | [[cite:Amadon2008]]
 `ac`          | `[[ac:abiref_nag_7.0_openmpi.ac]]` | [[ac:abiref_nag_7.0_openmpi.ac]]
 `pdf`         | `[[pdf:howto_chebfi.pdf]]`       | [[pdf:howto_chebfi.pdf]]
 `pdf`         | `[[pdf:howto_chebfi]]`           | [[pdf:howto_chebfi]]
 `src`         | `[[src:94_scfcv/m_scfcv.F90]]`   | [[src:94_scfcv/m_scfcv.F90]]


`#files-file` is an HTML id defined in *~abinit/doc/guide/abinit.md with*:

```html
<a id="files-file"></a>
## 4 More detailed presentation of the files file
```

Also in this case, it's possible to specify the name of the link
with the `|` separator so `[[topic:PIMD#1|Introduction]]` becomes [[topic:PIMD#1|Introduction]].


!!! important
    Internal links are automatically generated by the Markdown parser
    as discussed in the [Permalinks section](#Permalinks).


Be careful when including a wikilink inside other square brackets e.g. <span style="background-color: #E0E0E0;font-size:90%;">[2+ &#91; [ecut] &#93; ]**2</span>
as the occurrence of `]]]` confuses the parser.
The problem is easily solved by inserting whitespaces in the expression:

    [ 2 + [[ecut]] ] ** 2

This version if much more readable and it also avoids possible problems with the `**` that
has a special meaning in Markdown.

To refer to a particular git commit inside a Markdown document use:

    Solved in [[gitsha:f74dba1ed8346ca586dc95fd10fe4b8ced108d5e]]

that produces: [[gitsha:f74dba1ed8346ca586dc95fd10fe4b8ced108d5e]].
This extension is useful to generate nice changelogs and [release notes](/about/release-notes).

<!--
It's also possible to mention a particular github issue with the syntax:

    Fix https://github.com/abinit/abinit/issues/1

that produces: Fix https://github.com/abinit/abinit/issues/1
-->

### External links

As for the internal wikilinks, some external links are also recognized. The following case are treated:

* a link that starts with `www.`
* the namespaces `http`, `https`, `ftp`, `file`

| Markdown | Result |
| :-- | :-- |
| `[[https://www.abinit.org]]` | [[https://www.abinit.org]]
| `https://www.abinit.org` | https://www.abinit.org

It is also possible to specify the name of the link with the `|` separator:
For example, `[[https://wiki.abinit.org|The ABINIT Wiki]]` produces [[https://wiki.abinit.org|The ABINIT Wiki]]

The markdown parser supports aliases for commonly used links.
The aliases are defined in the `mkdocs.yml` configuration file (`abimkdocs_aliases`):

| Markdown | Result |
| :-- | :-- |
| `|AbiPy|` | |AbiPy| |
| `|AbipyStructureNb|` | |AbipyStructureNb| |
| `|xmgrace|` | |xmgrace| |
| `|gnuplot|` | |gnuplot| |
| `|today|` | |today| |


### Permalinks

Permalinks are a feature of the [Table of Contents extension](https://python-markdown.github.io/extensions/toc) &nbsp;,
which is part of the standard Markdown library.
The extension inserts an anchor at the end of each headline, which makes it possible to directly link to a subpart of the document.

By default, all headers will automatically have unique id attributes generated based upon the text of the header.
The name of the anchor is constructed from the header by converting the string to lower-case ASCII,
removing dots and other symbols such as `&` and replacing white spaces with a dash `-`.
For instance, `#pdf-files` is the anchor associated to the "Pdf Files" section
in this page and we can thus refer to it with the Markdown syntax:

```md
As we have seen in the [previous section](#pdf-files)
```

that produces: As we have seen in the [previous section](#pdf-files)

!!! tip
    Hover with the mouse on the header in the HTML page to show the permalink in the browser.
    You can also copy the link and use the last part to generate the reference.

<!--
Links to internal pages are easy to include when the website is built with Mkdocs
There are two possible approaches: *relative links* and *root-relative links*.
Let's assume, for example, that we can to insert a link to a section of `abimkdocs.md`
that is located in the `~abinit/doc/developers/` directory (the same directory as this page)
We can thus use the relative URL syntax:

```md
An example of [relative link](abimkdocs.md#getting-started)
```

that produces: An example of [relative link](abimkdocs.md#getting-started)

or, alternatively, the root-relative syntax:

```md
An example of [root-relative link](/developers/abimkdocs.md#getting-started)
```

where the path is now relative to `~abinit/doc` and must start with `/`.
Also in this case, we get the correct result:

An example of [root-relative link](/developers/abimkdocs.md#getting-started)

Note that Mkdocs converts all URLs to relative URLs so the two approaches are completely equivalent
still the use of relative URLs is strongly suggested because developers will be able to open the link
in their editor (provided the editor is Markdown-aware).
-->

!!! note
    It is also possible to generate automatically the Table of Contents by just
    placing the `[TOC]` marker in the document where you would like the Table of Contents to appear.
    Then, a nested list of all the headers in the document will replace the marker.
    Note, however, that the use of `[TOC]` in our pages is not recomended as
    the Table of Contents is automatically generated by the Mkdocs theme and displayed
    in the navigation bar on the right.


## Markdown extensions

### SmartSymbols

[SmartSymbols](https://facelessuser.github.io/pymdown-extensions/extensions/smartsymbols/) &nbsp;
adds syntax for creating special characters such as trademarks, arrows, fractions, etc.
The list of symbols supported by the extension is:

Markdown       | Result
-------------- |--------
`(tm)`         | (tm)
`(c)`          | (c)
`(r)`          | (r)
`c/o`          | c/o
`+/-`          | +/-
`-->`          | -->
`<--`          | <--
`<-->`         | <-->
`=/=`          | =/=
`1/4, etc.`    | 1/4, etc.
`1st 2nd etc.` | 1st 2nd etc.


### Definition Lists

The [Definition Lists](https://python-markdown.github.io/extensions/definition_lists) &nbsp; extension
adds the ability to create definition lists in Markdown documents.
This extension is included in the standard Markdown library.
The following text:

```md
Apple
:   Pomaceous fruit of plants of the genus Malus in
    the family Rosaceae.

Orange
:   The fruit of an evergreen tree of the genus Citrus.
```

will be rendered as:

Apple
:   Pomaceous fruit of plants of the genus Malus in
    the family Rosaceae.

Orange
:   The fruit of an evergreen tree of the genus Citrus.


### Admonitions

[Admonitions](
https://python-markdown.github.io/extensions/admonition) &nbsp; are useful
to stress important sections (useful e.g. in the Abinit tutorials).
Admonition are created using the Markdown syntax:

```md
!!! note
    Here is a note for you.
```

and

```md
!!! danger "Don't try this at home!"
    Stand back. I'm about to try science!
```

for an admonition with a custom title (make sure to quote the title).

The types of admonitions available for use in MkDocs depend on the theme being used.
The Material theme [supports](http://squidfunk.github.io/mkdocs-material/extensions/admonition/#types) &nbsp; the following types:

!!! note
    I am a "note" admonition and look the same as "seealso".

!!! tip
    I am a "tip" admonition and look the same as "hint" and "important".

!!! warning
    I am a "warning" admonition and look the same as "attention" and "caution".

!!! danger
    I am a "danger" admonition and look the same as "error".

!!! summary
    I am a "summary" admonition and look the same as "tldr".

!!! success
    I am a "success" admonition and look the same as "check" and "done".

!!! failure
    I am a "failure" admonition and look the same as "fail" and "missing".

!!! bug
    I am a "bug" admonition.


For the complete list, please consult the mkdocs-material
[documentation](http://squidfunk.github.io/mkdocs-material/extensions/admonition/).

### Details

[Detail](https://facelessuser.github.io/pymdown-extensions/extensions/details/) &nbsp;
is an extension that creates collapsible elements that hide their content.
It uses the HTML5 `#!html <details><summary>` tags to accomplish this.
It supports nesting and you can also force the default state to be open.
This extension is used in the documentation of the input variable to generate
a container with the list of tests associated to the variable but it can also be used for
long FAQs of Q&A sections in the tutorials.

Examples:

```md
???+ note "List of variables"
     [[ecut]] [[asr@anaddb]]
```

produces the *open* element:

???+ note "List of variables"
     [[ecut]] [[asr@anaddb]]


while

```md
??? note "Click to open!"
     [[ecut]] [[asr@anaddb]]
```

creates a *closed* element:

??? note "Click to open!"
     [[ecut]] [[asr@anaddb]]


## Abinit extensions

To create a button that opens a ==dialog== containing an **input file**, use:

```
    {% dialog tests/v1/Input/t01.abi %}
```

that produces:

{% dialog tests/v1/Input/t01.abi %}

If multiple files are used such as in:

    {% dialog tests/v1/Input/t01.abi tests/v1/Input/t02.abi tests/v1/Input/t03.abi %}

multiple buttons are produced:

{% dialog tests/v1/Input/t01.abi tests/v1/Input/t02.abi tests/v1/Input/t03.abi %}


## MathJax

Formulas written in LaTeX are interpreted automatically (at visualization time) thanks to the
[MathJax](http://docs.mathjax.org/en/latest/mathjax.html) &nbsp; on-the-flight processor
while the math extension for Python-Markdown is provided by
[python-markdown-math](https://github.com/mitya57/python-markdown-math).

Latex equations can be used **everywhere** including the description of the variables
reported in `abinit_vars.yml` and the description of the tests gives in the `TEST_INFO` section.
For the Abinit documentation, the conventions are:

* `$...$`  yields an *onlinecite* translation of the LaTeX formula.
* `$$...$$` yields *display* mode, the LaTeX formula being rendered on one dedicated line (moreover, centered).
* To have the equations numbered, use the display mode above, and (inside the markers) declare your equation
  within the standard `\begin{equation}...\end{equation}` markers.
* When a `$` sign is inside a `#!html <pre>...</pre>` HTML section, MathJax does not interpret it.
* Use `\$` to prevent a real \$ to be interpreted.

For instance `#!latex $|\Phi\rangle$` produces $|\Phi\rangle$ while `#!latex $$\Phi_\kq(\rr)$$` produces

$$\Phi_\kq(\rr)$$

Equations enclosed by `$$...$$` or `\begin{equation}...\end{equation}` markers are automatically numbered
and can be referenced inside the Markdown text using the standard Latex syntax:

```latex
\begin{equation}
G(12) = -i \langle \Theta^N_0|T\bigl[\Psi(1)\Psi^\dagger(2)\bigr]|\Theta^N_0 \rangle \label{eq:GreenDef}
\end{equation}
```

that produces:

\begin{equation}
G(12) = -i \langle \Theta^N_0|T\bigl[\Psi(1)\Psi^\dagger(2)\bigr]|\Theta^N_0 \rangle \label{eq:GreenDef}
\end{equation}

Equations can be referenced with:

    The propagator in Eq.(\ref{eq:GreenDef})

that produces: The propagator in Eq.(\ref{eq:GreenDef})

Note that MathJax is configured with Latex macros to facilitate the insertion of symbols
commonly used in our domain:

| Markdown | Result
| :--      | :--
| `$\rr$`  | $\rr$
| `$\GG$`  | $\GG$
| `$\kk$`  | $\kk$
| `$\qq$`  | $\qq$
| `$\kq$`  | $\kq$

Please consult the preamble in `abinit_theme/main.html` for the complete list of macros.

!!! bug
    It seems that the plotly javascript library does not play well with MathJax
    as equations sometimes are not displayed when plotly is activated.
    This problem can be fixed by reloading the page.
    It should not represent a serious issue since plotly is used only in selected pages (like this one).


## Unicode

Unicode characters in particular Greek symbols and accented characters can be used in the documentation.
The websites uses the [Google's Roboto font](https://fonts.google.com/specimen/Roboto) &nbsp; so Greek symbols
can be included without using MathJax either by specifying the HTML entity or by copying the unicode character
given in the two tables below.
This could be useful if the page does not contain Latex equations and there are just a few Greek symbols to be inserted.
Please do not use unicode characters in Latex equations.

| Character Name                  | Character   | Entity       | Hex Entity     | HTML  Entity
|-------------------------------- |------------ | -------------|----------------|-------------
| GREEK CAPITAL  LETTER ALPHA	  | Α		| &‌#913; | &‌#x0391	| &‌Alpha;
| GREEK CAPITAL  LETTER BETA	  | Β		| &‌#914; | &‌#x0392	| &‌Beta;
| GREEK CAPITAL  LETTER GAMMA	  | Γ		| &‌#915; | &‌#x0393	| &‌Gamma;
| GREEK CAPITAL  LETTER DELTA	  | Δ		| &‌#916; | &‌#x0394	| &‌Delta;
| GREEK CAPITAL  LETTER EPSILON	  | Ε		| &‌#917; | &‌#x0395	| &‌Epsilon;
| GREEK CAPITAL  LETTER ZETA	  | Ζ		| &‌#918; | &‌#x0396	| &‌Zeta;
| GREEK CAPITAL  LETTER ETA	  | Η		| &‌#919; | &‌#x0397	| &‌Eta;
| GREEK CAPITAL  LETTER THETA	  | Θ		| &‌#920; | &‌#x0398	| &‌Theta;
| GREEK CAPITAL  LETTER IOTA	  | Ι		| &‌#921; | &‌#x0399	| &‌Iota;
| GREEK CAPITAL  LETTER KAPPA	  | Κ		| &‌#922; | &‌#x039A	| &‌Kappa;
| GREEK CAPITAL  LETTER LAM(B)DA  | Λ		| &‌#923; | &‌#x039B	| &‌Lambda;
| GREEK CAPITAL  LETTER MU	  | Μ		| &‌#924; | &‌#x039C	| &‌Mu;
| GREEK CAPITAL  LETTER NU	  | Ν		| &‌#925; | &‌#x039D	| &‌Nu;
| GREEK CAPITAL  LETTER XI	  | Ξ		| &‌#926; | &‌#x039E	| &‌Xi;
| GREEK CAPITAL  LETTER OMICRON	  | Ο		| &‌#927; | &‌#x039F	| &‌Omicron;
| GREEK CAPITAL  LETTER PI	  | Π		| &‌#928; | &‌#x03A0	| &‌Pi;
| GREEK CAPITAL  LETTER RHO	  | Ρ		| &‌#929; | &‌#x03A1	| &‌Rho;
| GREEK CAPITAL  LETTER SIGMA	  | Σ		| &‌#931; | &‌#x03A3	| &‌Sigma;
| GREEK CAPITAL  LETTER TAU	  | Τ		| &‌#932; | &‌#x03A4	| &‌Tau;
| GREEK CAPITAL  LETTER UPSILON	  | Υ		| &‌#933; | &‌#x03A5	| &‌Upsilon;
| GREEK CAPITAL  LETTER PHI	  | Φ		| &‌#934; | &‌#x03A6	| &‌Phi;
| GREEK CAPITAL  LETTER CHI	  | Χ		| &‌#935; | &‌#x03A7	| &‌Chi;
| GREEK CAPITAL  LETTER PSI	  | Ψ		| &‌#936; | &‌#x03A8	| &‌Psi;
| GREEK CAPITAL  LETTER OMEGA	  | Ω		| &‌#937; | &‌#x03A9	| &‌Omega;

| Character Name                  | Character   | Entity       | Hex Entity     | HTML  Entity
|-------------------------------- |------------ | -------------|----------------|-------------
| GREEK SMALL LETTER ALPHA	  | α		| &‌#945; | &‌#x03B1	| &‌alpha;
| GREEK SMALL LETTER BETA	  | β		| &‌#946; | &‌#x03B2	| &‌beta;
| GREEK SMALL LETTER GAMMA	  | γ		| &‌#947; | &‌#x03B3	| &‌gamma;
| GREEK SMALL LETTER DELTA	  | δ		| &‌#948; | &‌#x03B4	| &‌delta;
| GREEK SMALL LETTER EPSILON	  | ε		| &‌#949; | &‌#x03B5	| &‌epsilon;
| GREEK SMALL LETTER ZETA	  | ζ		| &‌#950; | &‌#x03B6	| &‌zeta;
| GREEK SMALL LETTER ETA	  | η		| &‌#951; | &‌#x03B7	| &‌eta;
| GREEK SMALL LETTER THETA	  | θ		| &‌#952; | &‌#x03B8	| &‌theta;
| GREEK SMALL LETTER IOTA	  | ι		| &‌#953; | &‌#x03B9	| &‌iota;
| GREEK SMALL LETTER KAPPA	  | κ		| &‌#954; | &‌#x03BA	| &‌kappa;
| GREEK SMALL LETTER LAM(B)DA	  | λ		| &‌#955; | &‌#x03BB	| &‌lambda;
| GREEK SMALL LETTER MU		  | μ		| &‌#956; | &‌#x03BC	| &‌mu;
| GREEK SMALL LETTER NU		  | ν		| &‌#957; | &‌#x03BD	| &‌nu;
| GREEK SMALL LETTER XI		  | ξ		| &‌#958; | &‌#x03BE	| &‌xi;
| GREEK SMALL LETTER OMICRON	  | ο		| &‌#959; | &‌#x03BF	| &‌omicron;
| GREEK SMALL LETTER PI		  | π		| &‌#960; | &‌#x03C0	| &‌pi;
| GREEK SMALL LETTER RHO	  | ρ		| &‌#961; | &‌#x03C1	| &‌rho;
| GREEK SMALL LETTER FINAL SIGMA  | ς		| &‌#962; | &‌#x03C2	|
| GREEK SMALL LETTER SIGMA	  | σ		| &‌#963; | &‌#x03C3	| &‌sigma;
| GREEK SMALL LETTER TAU	  | τ		| &‌#964; | &‌#x03C4	| &‌tau;
| GREEK SMALL LETTER UPSILON	  | υ		| &‌#965; | &‌#x03C5	| &‌upsilon;
| GREEK SMALL LETTER PHI	  | φ		| &‌#966; | &‌#x03C6	| &‌phi;
| GREEK SMALL LETTER CHI	  | χ		| &‌#967; | &‌#x03C7	| &‌chi;
| GREEK SMALL LETTER PSI	  | ψ		| &‌#968; | &‌#x03C8	| &‌psi;
| GREEK SMALL LETTER OMEGA	  | ω		| &‌#969; | &‌#x03C9	| &‌omega;

Taken from <https://sites.psu.edu/symbolcodes/languages/ancient/greek/greekchart/>


## Plotly

[plotly](https://plot.ly/api/) &nbsp; is a high-level, declarative charting library built on top of d3.js and stack.gl.
plotly.js ships with over 20 chart types, including scientific charts, 3D graphs, statistical charts, SVG maps,
financial charts, and more.
Note that plotly is deactivated by default so you have to activate it inside the Markdown page by adding

```yaml
---
plotly: true
---
```

to the front matter.
This option will tell the browser to load the javascript library in the HTML page
so that it's possible to include HTML+javascript code to generate nice interactive plots inside the Markdown documentation.
For example the HTML + javascript code:

```html
<!-- Plots go in blank <div> elements.
     You can size them in the plot layout, or give the div a size as shown here.-->
<p>Here's a simple Plotly plot - <a href="https://bit.ly/1Or9igj">plotly.js documentation</a></p>

<div id="plotly_plot" style="width:90%;height:250px;"></div>
<script>
$(function() {
    Plotly.plot(document.getElementById('plotly_plot'), [{
        x: [1, 2, 3, 4, 5],
        y: [1, 2, 4, 8, 16] }],
        {margin: {t: 0}}
    );
});
</script>
```

produces the following plot:

<p>Here's a simple Plotly plot - <a href="https://bit.ly/1Or9igj">plotly.js documentation</a></p>

<div id="plotly_plot" style="width:90%;height:250px;"></div>
<script>
$(function() {
    Plotly.plot(document.getElementById('plotly_plot'), [{
        x: [1, 2, 3, 4, 5],
        y: [1, 2, 4, 8, 16] }],
        {margin: {t: 0}}
    );
});
</script>

plotly is used to plot the [code statistics](codestats.md) but it's not required for the proper functioning of the website.


## Using HTML directly

HTML code can be used in Markdown but keep in mind that
standard Markdown parsers ignore text inside block-level HTML tags so

```html
<div>
*Emphasized* text.
</div>
```

won't work (but this situation only occurs if you are trying to write some advanced HTML code).

Another thing worth noticing is that Mkdocs (by default)
generates a directory with an `index.html` file for every markdown page declared in `mkdocs.yml`
(see also [the official documentation](http://www.mkdocs.org/user-guide/configuration/#preview-controls)).
This means that a local webserver will serve this page at `http://127.0.0.1:8000/developers/markdown/index.html`
that can be equivalently reached from the more user friendly URL `http://127.0.0.1:8000/developers/markdown/`.

This implementation detail does not affect links specified either with wikilink or markdown syntax because
the python code will perform the automatic translation of the URLs.
It does affect, however, the way you should specify `src` or `href` in HTML code because
one should take into account the *extra directory* created by Mkdocs.
In a nutshell, **prepend** a `../` to the relative path you would use inside the shell to specify the location
of that resource with respect to the present page.

<!--
!!! warning
    Do not use root-relative URLs (e.g. `/tutorial/bse_assets/tbs5.png`) in HTML code
    because this will create problems when the site is deployed.
    Besides relative URLs allow us to serve multiple versions of the Abinit documentation
    associated to the different versions of the code.
-->
---
authors: MG, XG
---

# HowTo use git and the gitlab server

This page is intended as a quick reference to *git* and its integration
with the Abinit project.
If you are not familiar with *git*, we would strongly advise to watch this tutorial:

<iframe width="1384" height="629" src="https://www.youtube.com/embed/HVsySz-h9r4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

For further information about *git*, please consult 
the [official documentation](https://git-scm.com/).

!!! tip

    To access the online help for COMMAND, use:

        git COMMAND --help

    See also this page with the [most commonly used git tips and tricks](https://github.com/git-tips/tips)

In the next sections, we explain how to configure *git* to interoperate with
our [ABINIT gitlab server](https://gitlab.abinit.org/).
It is assumed you already have an account on our **internal gitlab server**.
Note that having an account on gitlab.com or github.com is not enough since we run our own server 
(you need to contact Jean-Michel Beuken to have an account created for you).
    

## Initial configuration

If this is the very first time you use *git*, please set the following global parameters before doing anything else:

```sh
mkdir -p $HOME/.config/git
git config --global user.name "Firstname Lastname"
git config --global user.email "someone@someserver.somedomain"
git config --global core.editor "my_preferred_editor"
git config --global core.excludesFile "$HOME/.config/git/ignore"
git config --global color.ui "auto"
git config --global merge.conflictstyle "diff3"
```

Replace *Firstname*, *Lastname*, *someone@someserver.somedomain*, and *my_preferred_editor*,
by your respective first name, last name, email address, and preferred editor.

To be able to **push your contributions** to the Abinit Forge, you need to add the
following section to your *~/.ssh/config* file

```
Host abinit-forge
HostName gitlab.abinit.org
    User git
    ServerAliveInterval 52
    Compression yes
```

so that one can use the **abinit-forge** hostname to **clone**, **pull**, and **push** to your repository.

For further info, please consult the [official documention](https://gitlab.abinit.org/help/ssh/README.md)

To clone your repository, execute:

    git clone abinit-forge:DEVELOPER/abinit

where DEVELOPER must be replaced by your Abinit Forge login.

In order to avoid typing your password every time you issue a command that accesses gitlab,
you have to introduce your public keys in your profile.
See <https://gitlab.abinit.org/profile/keys>.
On your local machine, generate a RSA ssh key **WITHOUT** passphrase:

    ssh-keygen -t rsa

and call it *id_rsa_gitlab*. Then add a section in the *~/.ssh/config file*:

```
host gitlab
  Hostname gitlab.abinit.org
  User git
  KeepAlive yes
  IdentityFile ~/.ssh/id_rsa_gitlab
```

Finally, copy the public key *id_rsa_gitlab.pub* on gitlab.
Now, you can use (on your local machine) the following syntax:

    git clone gitlab:user_id/abinit.git

instead of:

    git clone git@gitlab.abinit.org:user_id/abinit.git

To be sure the key is proposed each time git calls ssh, you can use ssh-agent:

    ssh-agent # this starts the agent, and provides the process id
    # execute the 3 lines of commands that ssh-agent proposes, e.g.
    SSH_AUTH_SOCK=/tmp/ssh-ngsERHER3K1HS/agent.15589; export SSH_AUTH_SOCK;
    SSH_AGENT_PID=15590; export SSH_AGENT_PID;
    echo Agent pid 15590;
    ssh add ~/.ssh/id_rsa_gitlab # add the corresponding ssh key for gitlab


Every developer has his/her specific gitlab **user_id** e.g. *gonze*.
An additional *virtual* developer, called **trunk**, is also defined.
<!--
===== Standard names (projects, ID, branches) =====
On the ABINIT gitlab server, an ABINIT project can be defined, specifically for you,
with address <color blue>git@gitlab.abinit.org</color>:<color red>user_id</color><color blue>/abinit.git</color>.
In order to do this, log on https://gitlab.abinit.org, go to "Explore projects", find "Trunk / Abinit" (+perhaps select "All"),
then click on the "Fork" button. You need to contact Jean-Michel Beuken to have access to the ABINIT gitlab.
In order to start to work locally (not on the ABINIT gitlab server, but on your own machine),
you should setup the SSH environment for gitlab, as described
In particular on some machines you need to have an ssh agent running with your rsa key already available,
so that git finds it when it runs ssh.
-->
You have by default a **master** branch and a **develop** branch in your repo,
but it is also possible to create and work in other branches.

To clone from the gitlab repository to your local repository, the recommended command is:

    git clone gitlab:user_id/abinit.git -b develop

where **user_id** is to be replaced by the adequate name.
Additional explanation: the normal *clone* command from the gitlab repository
to his/her gitlab repository on his/her local machine is:

    git clone git@gitlab.abinit.org:user_id/abinit.git

However, with the proper SSH environment, the following *clone* command can be used:

    git clone gitlab:user_id/abinit.git

This will clone the *master*, while the usual working branch is the
*develop* branch, that can be obtained directly with the above-mentioned command.
After some modifications, using the git commands (e.g. git add, git commit ...),
you can push on the gitlab server thanks to:

    git push  # Add --tags to push tags.

In order for the modifications to be merged in the trunk, a merge request (MR) has to be issued, as described later.
The same might be done for other branches, created from the local *develop* one, using:

    git branch another_branch
    git checkout another_branch

For pushing, the first time, use:

    git push -u origin another_branch

You are able to create your own additional branches, either locally or on gitlab.
The name is your own choice.


!!! important

    Note that, according to the [gitflow branching model](http://nvie.com/posts/a-successful-git-branching-model),
    the **master** branch will be of little use for the *physical* developers (including you)
    while the **develop** and **release-*** branches will be quite important for normal developments.
    In a nutshell, new developments are always merged in **trunk/develop** while **trunk/master** contains
    the stable version. Finally, **release-** branches are mainly used for bug fixes and minor changes
    that should be included in the next release.

    ![](https://miro.medium.com/max/1400/1*9yJY7fyscWFUVRqnx0BM6A.png)

### Git branches

In the ABINIT + git workflow:

  * In order to be merged in the trunk, the branch has to be **on-track**, as explained later
  * You have to issue a "merge request" on gitlab (usually to **trunk/develop**).
  * By default, all branches pushed on gitlab might be tested on-demand by the user.
  * Only branches that are **on-track** and that have succeeded (or passed) all the automatic tests
    are considered for merge (as was the case previously).

The *develop* branch corresponds to the development stage.
The *release-* branches corresponds to release candidates or hotfix branches.
It is slightly simpler than the gitflow naming scheme.

!!! tip

    In the git philosophy, branches are often created, then merged, then destroyed.
    Developers are encouraged to take advantage of this flexibility.

### How to trigger the action of buildbot? (On-demand)

The developer has to use the on-demand form on the ABINIT [buildbot portal](https://bbportal.abinit.org)
The authentification is made with the gitlab login/password.
One important feature has to be taken into account: with git, each commit is characterized
by a SHA-1 code (40 hexadecimal characters).
The four first characters actually define the commit with 1 combination out of 65536 4-character combinations,
while the five first characters define the commit with 1 combination out of 1048576 5-character combinations.
Gitlab knows the repository of the different users, the existing branches, as well as the SHA-1 code of each commit.
The three following fields must be defined by the developer for buildbot to know which is the revision of ABINIT to be run:

  - “User” : choose one of the existing User_ID from the menu. Default value: *user_id*.
  - “Branch” : choose one of the existing User_ID from the menu. Default value: *develop*.
  - “Commit” : the developer can enter a free text with the first hexadecimal characters
     of the SHA-1 code of the commit (in case different commits of the user on the branch
     have the same first characters, the last commit will be taken by buildbot).
     Default value: the last commit of User/Branch.

### What is an on-track branch?

For a branch to be **on-track**, a specific commit must be contained in the history of the branch.
This specific commit will be created in *trunk/develop* and corresponds to the release
of a new version (not necessarly a public version).
For a *release-* branch to be **on-track**, not only a specific commit must
be contained in the past history of the branch, but also the commit that starts
the next development version **cannot** be present.
Only one among the *release-* branches is **on-track** at any time.

!!! note

As an example, suppose we were on ABINIT v8.9.x, and want to start preparing a release 8.10.0 (for production)
and a new v8.11.0 (for development):

  * a branch entitled *release-8.10.0* will be forked from the *develop* branch
  * after this branching, the first commit in the *release-8.10.0* branch will be tagged start-8.10.0,
    while the first commit in the *develop* branch will be tagged start-8.11.0,
  * for a *develop* branch to be considered **on-track** (and eligible for a merge request), the commit
    tagged start-8.11.0 will have to be present;
  * for a *release-8.10.0* branch to be considered **on-track** (and eligible for a merge request),
    the commit tagged start-8.10.0 will have to be present, while it will be forbidden to contain
    the commit tagged start-8.11.0 .

In complement to the start of a X.Y.Z version being tagged as "start-X.Y.Z",
the commit that ends some X.Y.Z version of ABINIT will be tagged "X.Y.Z".

### What is shown in the Buildbot Status table?

The [Buildbot Status table](https://bbportal.abinit.org/#/status)
shows a summary of the recent results obtained by the test farm for alll active branches.
If you want to see all the results, select the **filters**: all + all, then click on the **update button**.
The default selection of bots, that is the *night* set, should be adequate for all merge requests.
The selection capabilities of the Buildbot Status table are rather extended, you have to play a bit wit them.

By the way, the behaviour of this Buildbot Status table when changing the selections has still some problems -as of May 2018-.
Do not hesitate to click on the update button, and the "Grouping" button -the latter back and forth-, sorry for the inconvenience.

### How and when will the merge in the master branch be done?

In order for your development to be incorporated, there should be **merge request**
from your specific branch to the *trunk*.
For most branches, the merge request will be to the *trunk/develop* branch.
The list of merge requests is available [here](https://bbportal.abinit.org/#/mr).

However, when a *release-* branch is ready to be merged,
the merge request should target the corresponding *trunk/release-*</color> branch.
The master branch is only used by the trunk.
So, never issue a merge request to trunk/master.
You are supposed to merge, inside your branches, specific tagged branches from trunk,
in order to avoid divergences. These specific tagged branches will be advertised.
As mentioned earlier,
the presence of such tagged commits allows one to identify whether the branch is **on-track**,
before an automatic testing to be done.

### How to synchronize with the trunk?

In order to keep your branches up to date with those
of the trunk, you should first register *git@gitlab.abinit.org:trunk/abinit.git* as a remote with:

    git remote add trunk gitlab:trunk/abinit.git

At this point, one can fetch the branches in trunk with:

    git fetch trunk

then, if the develop branch is to be updated, supposing it is checked out,
To merge *trunk/develop* in your develop branch:

    git checkout develop
    git merge trunk develop

You can combine the last two commands in one as:

    git pull trunk develop

If, on the contrary, a new branch (e.g. a release branch, let's says 8.8 to fix the ideas) has to be created:

    git branch release-8.8 start-8.8.1    # this creates the branch release-8.8 from the start-8.8.1 tag
    git checkout release-8.8
    git merge remotes/trunk/release-8.8
    git push -u origin release-8.8

That's it! You can now make modifications in your release-8.8, then issue a merge request to the trunk/release-8.8.

<!--
### Additional info: Setup of the SSH environment
## How to clone your repository with git and track `trunk`

To clone your repository on your `localhost`:

    git clone

To track `trunk`:

    git remote add trunk

To show the list of remote branches:

    git remote -v

To merge the `develop` branch of `trunk` in your branch:

    git checkout develop
    git pull trunk develop

To push to the gilab server:

    git push origin develop


!!! tip

    To access the online help use: `git COMMAND --help`


!!! important

    gitflow: You should always send pull requests to trunk/develop
-->
Information on the format 3 for pseudopotentials.

(Note: the implementation of format 3 was done by Fr. Detraux).

The format 3 for ABINIT pseudopotentials allows to use pseudopotentials
from the table I in the Phys. Rev. B 58, 3641 (1998) paper
by C. Hartwigsen, S. Goedecker and J. Hutter (HGH). This paper
presents LDA pseudopotentials for all elements from H to Rn. Some of them
are even presented twice, because of the possibility to include semi-core
states. Their accuracy has been demonstrated in the HGH paper, but note
that the energy cut-off needed to get this high accuracy might be
larger than the one usually needed for Troullier-Martins or other
pseudopotentials. So, convergence studies are very important !

A few lines must be added to the data mentioned in that table,
and are described in the present file. ABINITv1.5 is able to
read format 3 pseudopotential files, as well as later versions of ABINIT.

We will suppose that the user has the HGH table.
For the description that follows, we will focus on the Sn pseudopotential.
In the table (p. 3651), one finds:

    Sn  4  0.605000  4.610912
           0.663544  1.648791 -.141974  -.576546
           0.745865  0.769355 -.445070
                     0.103931 0.005057
           0.944459  0.225115
                     0.007066

These data must be put in the following format to be read by ABINIT
(this file is also found in the `~abinit/tests/Psps_for_tests directory`,
with the name `50sn.psphgh`):

    Hartwigsen-Goedecker-Hutter psp for Tin,  from PRB58, 3641 (1998) paper
    50  4   980509                     zatom,zion,pspdat
    3   1   2    0    2001    0        pspcod,pspxc,lmax,lloc,mmax,r2well
    0.605000  4.610912 0         0         0  rloc, c1, c2, c3, c4
    0.663544  1.648791 -.141974  -.576546     rs, h11s, h22s, h33s
    0.745865  0.769355 -.445070  0            rp, h11p, h22p, h33p
              0.103931 0.005057  0                k11p, k22p, k33p
    0.944459  0.225115 0         0            rd, h11d, h23d, h33d
              0.007066 0         0                k11d, k22d, k33d


First, the following three lines have been added at the beginning
of the data from the table:

    Hartwigsen-Goedecker-Hutter psp for Tin,  from PRB58, 3641 (1998) paper
    50  4   980509                     zatom,zion,pspdat
    3   1   2    0    2001    0        pspcod,pspxc,lmax,lloc,mmax,r2well

Similar lines must be added to the data for other elements, in order
to make them readable by ABINIT.

Line 1 is simply a header, that might include any information, and that will
be printed without modification in the output of ABINIT.

Line 2 describes:

- the atomic number (zatom);
- the ionic charge (zion, number of valence electrons);
- the date of pseudopotential generation.

The two first information are crucial, the third one is not
really important. The atomic number is 1 for Hydrogen, 8 for Oxygen, and so on.
The ionic charge should be the same number as the second number mentioned
in the data table (described as 'Zion' in section VI of the paper)

Line 3 describes:

- the format of the pseudopotential (pspcod ; must be 3 for this format);
- the XC functional used to generate the pseudopotential (pspxc; here, must be 1)
- the maximal angular momentum of the wavefunctions described
   in the pseudopotential file
   (lmax=0 if purely local pseudopotential: H, He, Li^sc, Be^sc ;
    lmax=1 for s and p projectors : Li, B ... Ar, K^sc, Au ;
    lmax=2 for s, p and d projectors : K, Ca, ... Cs, Ba, Hf^sc ... Pt^sc,
     Au^sc ... Rn ;
    lmax=3 for s, p, d and f projectors : Cs^sc, Ba^sc, La^sc ... Lu^sc )
- lloc has no meaning, and is set to 0
- mmax has no meaning, and is set to 2001
- the last number can be set to 0 .

The lines that follow these three lines
are generated from the data in the table, though the name of the
element and Zion are not reproduced. Note also that
each line has to be completed with zeroes, to give the format presented
at the beginning of the section VI of the HGH paper :
five columns for the first of these lines,
then 4 columns for each projector, and 3 columns for the spin-orbit splitting.
For readability, the meaning of each number has also been added
in our example pseudopotential file
(for example, 'rloc, c1, c2, c3, c4' ) but these are NOT read by ABINIT.
Thus, unlike the zeroes, it is not important to add them to the
data from the table.

Inside ABINIT, the pseudopotential with format 1 will be treated by
the routine psp2in.f, that calls psp2lo.f (local part) and
psp2nl.f (non-local part).

As a matter of numerical accuracy, note that the integral
of (V(r)+Zion/r) r^2 in psp2lo.f is performed analytically without cut-off.
## This file contains information about formats of pspcod=4 and pspcod=5 pseudopotentials.

Pspcod=4 corresponds to the case of Teter pseudopotentials
generated in Louvain-La-Neuve using the code ATOM. Pspcod=5 corresponds
to "Phoney" pseudopotentials built on a Hamman grid. At this stage (22 July
1998) it is possible to treat these pseudopotentials (in the version 1.5 and
later).  However, one has to be careful about the formats of these
pseudopotentials. Indeed, to be read by the ABINIT code, they have to be slightly modified. 
Let's take examples to explain these modifications:

# pspcod = 4 case (Teter pseudopotentials). 
 
The example corresponds to Lead.

OLD HEADER

    (Xe+4f14)+6s1.8 5d10 6p0.2 5f0.05;rcs=rcd=2.0(exnc11),rcp=2.0(26),rcf=1.3(11) no chem-hard; ecut 19/25
     82.00000  14.00000  0.000000000000000000E+00      z,zion,etot(wrong)
            4         3         2      2001            iexc,ipsp,lmax-1,ngrid
     .000   .000000000000000000E+00   .000000000000000000E+00 rchrg,fchrg,qchrg
    2.00426660272461010 2.00426660272461010 2.00426660272461010 1.29915156996312131
    2 2 2 0

NEW HEADER (READABLE BY ABINIT)

    (Xe+4f14)+6s1.8 5d10 6p0.2 5f0.05;rcs=rcd=2.0(exnc11),rcp=2.0(26),rcf=1.3(11) no chem-hard; ecut 19/25
      82.00000  14.00000  960808                 zatom,zion,pspdat
      4         3         3   3      2001    0  pspcod,pspxc,lmax,lloc,mmax,r2well
      0  0  0   2    2.00426660272461010          l,e99.0,e99.9,nproj,rcpsp
      .000 .000 .000 .000                         rms,ekb1,ekb2,epsatm
      1  0  0   2    2.00426660272461010          l,e99.0,e99.9,nproj,rcpsp
      .000 .000 .000 .000                         rms,ekb1,ekb2,epsatm
      2  0  0   2    2.00426660272461010          l,e99.0,e99.9,nproj,rcpsp
      .000 .000 .000 .000                         rms,ekb1,ekb2,epsatm
      3  0  0   0    1.29915156996312131          l,e99.0,e99.9,nproj,rcpsp
      .000 .000 .000 .000                         rms,ekb1,ekb2,epsatm
      .000   .000   .000                          rchrg,fchrg,qchrg


This header corresponds exactly to these of the TM pseudoptentials (pspcod=1).
In addition, in the new form of the Teter pseudopotentials, one indicates the
number of angular momentum before each set of data corresponding to the
considered angular momentum. This allows a clearer pseudopotential file.

# pspcod = 5 case ("Phoney" pseudopotentials).

The example corresponds to Oxygen.

OLD HEADER

    Compromise psp for oxygen with rc=1.5 ec=25 double reference
       8.00000000000000000       6.00000000000000000      -15.6648128583845843
               4           2           0         600
      0.999999999999999955E-06  0.307523885541775704E-01
       1.49157651759552068       1.49157651759552068
               2           0

NEW HEADER  (READABLE BY ABINIT)

    Compromise psp for oxygen with rc=1.5 ec=25 double reference
       8.000       6.000      980710     zatom,zion,pspdat
       5   3   1   1   600     0         pspcod,pspxc,lmax,lloc,mmax,r2well
       0.999999999999999955E-06  0.307523885541775704E-01  r1,al
       0  0  0   2     1.49157651759552068               l,e99.0,e99.9,nproj,rcpsp
       0  0  0   0                                       rms,ekb1,ekb2,epsatm
       1  0  0   0      1.49157651759552068              l,e99.0,e99.9,nproj,rcpsp
       0  0  0   0                                       rms,ekb1,ekb2,epsatm
       0  0  0                                           rchrg,fchrg,qchrg

Note that this header is almost the same as for the pspcod=1 and pspcod=4 case.
The only difference lies in the presence of "r1" and "al" parameters (defining
the Hamman grid). Here also, we indicate in addition the number of each angular
momentum before the set of data corresponding to the considered angular
momentum.

Inside ABINIT, a pseudopotential with format 4 will be treated by
the routine psp1in.f, that calls psp1lo.f (local part),
psp1nl.f (non-local part), and psp1cc (core correction).

As a matter of numerical accuracy, note that the integral
of (V(r)+Zion/r) r^2 in psp1lo.f is performed from 0 to the highest
allowed radius (usually about 100 a.u.), without cut-off.
V(r)+Zion/r should tend rapidly to zero
for large radii (beyond 5 a.u.), but this correct behaviour will
not be enforced by psp1lo . If the tail of V(r) is inaccurate
(i.e. if the pseudopotential is in single precision), there
will be large inaccuracies in the integral, because of the r^2
factor.

By contrast, a pseudopotential with format 5 will be treated by
the routine psp5in.f, that calls psp5lo.f (local part),
psp5nl.f (non-local part), and psp5cc (core correction).

The integral of (V(r)+Zion/r) r^2 in psp5lo.f is performed
from 0 to the highest allowed radius (usually about 100 a.u.),
except that beyond 20 a.u. no value of abs(V(r)) larger
than 2.0d-8 is tolerated. This will allow to cut off spurious
behaviour of pseudopotential whose data file is written in
single precision.
## Information on the format 8 for pseudopotentials.

(Note: the implementation of format 8 was done by D. R. Hamann).

The format 8 for ABINIT norm-conserving separable pseudopotentials 
([[cite:Hamann1979]], [[cite:Hamann1989]] and [[cite:Kleinman1982]])
is designed to allow users
who wish to experiment with pseudopotentials, possibly with non-standard
features, to have great flexibility in doing so. It does not correspond 
to any publicly available tabulation. The open-source ONCVPSP package available at 
[www.mat-simresearch.com](http://www.mat-simresearch.com) produces pseudopotentials in this format.  

An annotated example is presented in detail below, followed by a second 
example incorporating spin-orbit coupling.  An extended discussion follows 
below the examples, with a separate section discussing spin-orbit as produced by ONCVPSP.

When first producing a pseudopotential in this format, it would be a very
good idea to do a well-converged calculation of an isolated atom in a big
box to confirm that you are correctly recreating the atomic valence levels you intend.

The format is best explained by an example, which is presented in
detail below.  (The example is excerpted from `Psps_for_tests/20ca_sic.drh`.
Another example of pspcod=8 is `Psps_for_tests/8o_sic.drh`.)
All but the last data block of this file are read in subroutine psp8in.f.
The "comment" (#) lines below are not permitted in the actual file. Nor are blank lines.

    Header

    line 1: Title
    line 2: atomic number, pseudoion charge, date
    line 3: pspcod=8 identifies this format
            pspxc=2 identifies the exchange-correlation functional (see
            ixc list in the input variables documentation)
            lmax=2 gives the largest angular momentum potential present
            lloc=4 (which is >lmax) indicates that the local potential
            is independent of that for any angular momentum.  In other
            cases, it can be <=lmax, and the local potential can be
            that of a particular angular momentum channel.  (The choice
            of 4 is arbitrary, but the index of the corresponding data block
            below must be consistent with the value assigned in the header.)
            mmax=313, giving the number of radial grid points (arbitrary).
            r2well=0 is an historical header holdover, and is not used
     line 4: rchrg=6.18.. is the radius beyond which the model core
             charge (if used) is zero or negligible.  The maximum
             radial mesh point must equal or exceed this value.
             fchrg=1.0 signals that a model core charge is present.  Any
             value >0.0 will do, and the value is not used otherwise.
             qchrg=0.0 is a historical holdover, not used.
     line 5: number of Bloechl-Kleinman-Bylander projectors nproj for each
             angular momentum l=0 to l=lmax.  The value of nproj for lloc 
             must be 0 (if lloc <= lmax).
     line 6: extension_switch=0, not initially used, but values >0 may
             be utilized to signal the presence of additonal lines of header
             information for special-purpose use in the future.
             The value 2 is now used to indicate the presence of spin-orbit
             projectors (see SPIN-ORBIT section below).

**EXPERIMENTAL** 
self-interaction-corrected psp for calcium (D. R. Hamann):

    20.0000      2.0000    040701      zatom,zion,pspd
    8     2     2     4   313     0    pspcod,pspxc,lmax,lloc,mmax,r2well
    6.18295050  1.00000000  0.00000000 rchrg fchrg qchrg
    2     1     1     0     0          nproj
    0                                  extension_switch

First data block

first line: angular momentum l=0, ekb(ii) for ii=1,2(=nproj(1))
        following lines labeled 1,2,...,mmax -

*   1st column: radial grid index
*   2nd column: radial grid mesh point (LINEAR mesh starting at 0.0)
*   3rd column: first  BKB projector for l=0
*   4th column: second BKB projector for l=0

The projectors should go smoothly to zero, or decay to negligible
values within the range of the radial grid.
In general there are nproj ekb and projector columns for each l, as
many as you want.
If nproj is zero, AND lloc/= the current l, this block is omitted.
While some may be absent, l blocks must be in ascending order.

    0                       1.8442549429363D+01 -1.5467656752626D+01
    1  0.0000000000000D+00  0.0000000000000D+00  0.0000000000000D+00
    2  2.0000000000000D-02  3.2695476840515D-03 -7.7803072439537D-05
    3  4.0000000000000D-02  6.5413292698932D-03 -1.5573364986856D-04
    .....
    .....

Second data block

first line - angular momentum l=1, ekb(ii) for ii=1,1(=nproj(2))
    following lines labeled 1,2,...,mmax -

* 1st column: radial grid index
* 2nd column: radial grid mesh point
* 3rd column: first (only)  BKB projector for l=1

``` 
    1                       1.0250215833500D+01
    1  0.0000000000000D+00  0.0000000000000D+00
    2  2.0000000000000D-02  4.1259561971920D-05
    3  4.0000000000000D-02  1.6501618805883D-04
    .....
    .....
```

Third data block as above, for l=2

    2                      -4.9318560092991D-01
    1  0.0000000000000D+00  0.0000000000000D+00
    2  2.0000000000000D-02 -2.8426374194219D-04
    3  4.0000000000000D-02 -2.2672423014226D-03
    .....
    .....

Fourth data block

first line - index corresponding to lloc in header.  In this case,
      the local potential does not correspond to that for any angular
      momentum.  lloc can be arbitrary in this case, but MUST be >lmax,
      and this block must follow the blocks for projectors with l <=lmax.
      following lines labeled 1,2,...,mmax -

*       1st column: radial grid index
*       2nd column: radial grid mesh point
*       3rd column: local potential

If lloc<lmax, the corresponding block must occur in its proper l order
among the blocks of nonlocal projectors.
The last line should, ideally, be equal to -zion/rad(mmax), and the
numerical data should approach -zion/r as continuously as possible,
since the Fourier transform is extended to infinity analytically
assumming this functional form.  This is not precisely satisfied for
the experimental potential in the example, and the result will be a
small-amplitude Gibbs oscillation as a function of the Q of the
Fourier-transformed local potential with a period of 2*Pi/rad(mmax).

    4
    1  0.0000000000000D+00 -9.3285752771727D-01
    2  2.0000000000000D-02 -9.3284556214441D-01
    3  4.0000000000000D-02 -9.3280966542583D-01
    .....
    .....
    313  6.2400000000000D+00 -3.3275722650983D-01

Fifth (last) data block (read in subroutine psp8cc)

lines labeled 1,2,...,mmax -

*       1st column: radial grid index
*       2nd column: radial grid mesh point (LINEAR mesh starting at 0.0)
*       3rd column: rhoc = 4*Pi*model core charge density
*       4th column: d rhoc / dr
*       5th column: d^2 rhoc / dr^2
*       6th column: d^3 rhoc / dr^3
*       7th column: d^4 rhoc / dr^4

This block is only read if the header variable fchrg >0.0.

     1  0.0000000000000D+00  1.8263604328455D+00 -1.3822038662342D-06 -3.4248477651502D-05  2.2949926081983D-05 -1.3721984594931D+01
     2  2.0000000000000D-02  1.8263581757935D+00 -1.8290946429655D-05 -2.7430630291457D-03 -2.7416158741933D-01 -1.3679142685104D+01
     3  4.0000000000000D-02  1.8263568047539D+00 -1.4618869292620D-04 -1.0954897796014D-02 -5.4658870398836D-01 -1.3549201500559D+01
    .....
    .....

## SPIN-ORBIT

Spin-orbit coupling is present in the file `Psps_for_tests/78_Pt_r.oncvpsp.psp8`.
This pseudopotential treats the 5s and 5p core states as valence.  
More details are given in the spin-orbit portion of the discussion section 

    Header: lines 1-4 as above
    line 5: number of Vanderbilt-Kleinman-Bylander projectors nproj for 
            the scalar-relativistic non-local potential for each
            angular momentum l=0 to l=lmax. The value of nproj for lloc 
            must be 0 (if lloc <= lmax).
    line 6: extension_switch=2 indicating spin-orbit projectors are present
    line 7: number of Vanderbilt-Kleinman-Bylander projectors nprojso for 
            the spin-orbit non-local potential for each angular momentum 
            l=1 to l=lmax.

    Pt    NOPTPSP  r_core=  2.21  2.52  2.40  3.02
         78.0000     18.0000      140202    zatom,zion,pspd
         8     2     3     4   500     0    pspcod,pspxc,lmax,lloc,mmax,r2well
      4.99000000  0.00000000  0.00000000    rchrg fchrg qchrg
         2     4     3     3    nproj
         2                 extension_switch
         4     4     3    nprojso

    Data blocks 1-4 are as-above for the scalar-relativistic non-local projector
    coefficients ("ekb"), grid indices, grid points, and projectors.

    Data block 5 is the local potential lloc=4 as above.

    Data blocks 6-8 are as-above for the spin-orbit non-local projector
    coefficients ("ekb"), grid indices, grid points, and projectors.

    Non-linear core corrections are not used in this example, but if they
    were, the model core charge and its derivatives would be the last data block

## DISCUSSION

This introduces a norm-conserving pseudopotential input file format
designed by D. R. Hamann which offers additional flexibility over
previously available Abinit psp formats, as well as possible improvements
in performance in certain respects. Its new features are:

- Numerical psp's, which offer the ability to experiment beyond existing
  tabulated collections.  Available psp's given as basis function sums 
  are easily convertible to this format.

- Linear radial grids, which have the following advantages:

    (a) Computational efficiency compared to log grids because the
        core region of those grids is highly redundant for psp's.

    (b) Increased accuracy at large ecut values.  The spacing of
        mesh points for log grids at larger radii, where there are
	usually still significant contributions to the psp's, becomes
	too large and introduces aliasing noise into the Fourier-
	transform psp's.  In fact, this code is written to double,
	triple, etc. the linear grid and interpolate prior to performing
	the transform to ensure accuracy, based on ecut.

    (c) Storage economy (probably not important in modern computers, but
        the files are easier to read).

    (d) While redundant, repeating the grid in the data block with each
        function facilitates inspection and plotting of the various inputs.

- Allowance for any number of nonlocal projectors for each anugular
momentum channel, previously available for several formats but without
the current flexibility.

- Direct input of the Bloechl-Kleinman-Bylander projectors rather
  than wave functions and semi-local potenials.  This allows greater
  flexibility important for some exploratory applications such
  as self-interaction-corrected psp's.  The projectors are applied
  to the wave functions as the following operators
  ```
                   Sum    |fkb(n,l),m> ekb(n,l) <fkb(n,l),m|
                   (nlm)
  ```
  The conventional Kleinman-Bylander [2] choice for a single projector
  (single n) would be
  ```
                   fkb(r,l) = (V(r,l) - V_loc(r))u(r,l),
  ```
  where V(r,l) is the semi-local norm-conserving pseudopotential for
  angular momentum l, V_loc(r) is the local potential, and u(r,l) is
  the radial Schroedinger equation solution (bound or scattering).[1,2]
  Note that u(r) has the conventional `r` factor, u(r)=r psi(r).
  In this case, the "energies" ekb (which have dimensions 1/energy
  for the above definition of fkb) are
  ```
                   ekb(l) = 1 / [Integral fkb(r,l)*u(r,l) dr],
                                  (0,inf)
  ```
  (no 4*Pi factor).  For the Bloechl generalization, see [[cite:Bloechl1990]].
  Other possible generalizations include the treatment of self-
  interaction corrections as additional projectors, see [[cite:Vogel1995]].</br>
  Finally, creating numerical projectors as part of the psp generation
  process (rather than within Abinit) allows one to solve the radial
  Schrodinger equation in its fully nonlocal form and compare logarithmic
  derivatives of the pseudo and all-electron wave functions over a wide
  energy range. This is a superior test of transferability, and will
  locate shallow "ghost resonances" above the reference valence
  states, which would be missed in the standard ghost tests 
  [[cite:Gonze1990a]], [[cite:Gonze1991]], but
  can lead to poor results.

- Ability to use an arbitrary local potential rather than that for
  one particular angular momentum channel.  This can be helpful in
  eliminating ghost states [[cite:Gonze1990a]], [[cite:Gonze1991]].</br>
  In the example above, a calcium pseudopotential, the "conventional"
  choice of the d-channel (lloc=2) for the local potential is particuarly
  bad because the incipient d shell represented by a shallow d
  resonance leads to a very deep d potential.  With this potential as
  local, it is nearly impossible to avoid ghost states.  Using p as
  the local potential would avoid this, but would place a spurious
  core-orthogonalization repulsive term in channels for higher angular
  momenta (f, g, h, ...) which could lead to errors in, for example,
  lattice constants.  The local potential here was formed by a
  smooth (polynomial) extrapolation of the tail-region all-electron
  potential to the origin.  While some other Abinit psp formats
  include the ability to form the local potential as a simple average
  over semi-local potentials, the d potential would likely still cause
  problems in this case.

- Spherical bessel functions for Fourier transforms in the `psp8*.f`
  routines are computed by a highly accurate recursion method in sbf8.f,
  and all angular momentum channels are treated simultaneously,
  increasing accuracy and efficiency compared to sin/cos formulations,
  especially for large and small arguments.

- The ability to experiment with various model core charges for the
  non-linear core correction [[cite:Louie1982]] is advantageous.  Functions that are
  not sufficiently smooth can lead to potentially severe convergence
  errors, especially in response function calculations.  For other
  formats, Abinit internally generates higher derivatives of the model
  core charge up to the 4th derivative, and piecewise-constructed
  model functions that LOOK smooth can cause havoc.  The requirement
  that all the derivatives to be present in the input file permits
  early detection of such problems, and aids their remediation.

- Release 7.x introduces the ablity to include spin-orbit coupling.  The
  Pt pseudopotential used as an example above was initially generated
  with two non-local projectors each from Dirac wave functions with j=l+1/2 
  and j=l-1/2 using Vanderbilt generalized norm conservation, which
  gives much greater accuracy than the Bloechl construction discussed
  above.[[cite:Hamann2013]]  Weighted sums and differences of these non-local operators
  were separately re-diagonalized to produce efficient orthonormal 
  projectors for the scalar-relativistic and spin-orbit terms which are
  required internally by abinit.  For a detailed description of the
  projectors, see Ref. 7.  The operation of SR and SO terms on the
  spinor wave functions is a straightforward generalization of
  ```
                   Sum    |fkb(n,l),m> ekb(n,l) <fkb(n,l),m|
                   (nlm)
  ```
  where "fkb" and "ekb" are appropriately reinterpreted.  Since the
  "fkb" for the Pt example are orthonormal, the "ekb" have the
  dimensions energy.  Projectors with negligibly small energies
  (<2E-5 Ha at present) are neglected.  There is full flexibility 
  to use other methods to generate the SR and SO terms.
---
authors: JMB, XG
---

This page contains some link errors that must be detected by LinkChecker

## Errors in abiref.bib file 

entry [[cite:Testlink2019]] : link error ->  dx.doi.orgg instead of dx.doi.org

entry [[cite:Testlink2019a]] : diff between doi (10.1103) and url (10.1102) 

entry [[cite:Testlink2019b]] : diff between doi (physrevb.87.085322) and url (physrevb.87.085323)

## Citation error

Error in citation ('c' at the end of 'Testlink2019c' ) : [[cite:Testlink2019c]]

## Broken external links

Error in link 'en.wwikipedia.org' ( 2 'ww' ) :  [[https://en.wwikipedia.org/wiki/Markdown]]

Error in link 'https://github.com/abinit/abiconfigg' ( 2 'gg' ) : [[https://github.com/abinit/abiconfigg]]

## Variable name error

Error in the name of [[rfphon]] -> 'rfphongg'  : [[rfphongg]]
---
authors: MG
---

## Design

The ABINIT test suite is a collection of input and output files organized in a hierarchical structure of directories. 
Each directory contains tests sharing some kind of property such as 
the ABINIT major version in which the test was added or the external libraries whose presence 
is required for the execution of the test.

The purpose of the test suite is two-fold: developers use the test farm
to validate their code before pushing their developments to the trunk whereas 
users can use the test suite to validate the executable before using it for production calculations.

The code that drives the execution of the tests and the validation of the final results 
in written in python (version >= 2.7 is required) and relies on the following principles:

*  Each test is **self-describing**, in the sense that the input file 
   provides all the information needed to run the test and analyze/validate the final results.

*  Each test has **keywords** so that it is possible to 
   select and run only the tests containing these tags. 

*  It should be possible to run the entire suite in **parallel** with MPI/OpenMP and an arbitrary 
   number of processors (not only the tests in the *paral* directory)

*  The execution of the tests can be distributed among **python processes** in order 
   to reduce the execution time.

## Configuration files

The configuration parameters of the test suite are defined in the `__init__.py`
files located in the test suite directories.
These files are standard python files and the parameters are specified with the standard python syntax
for list of strings.

`tests/__init__.py` is the top level configuration file.
This file defines the list of directories containing the ABINIT tests:

```python
cat tests/__init__.py

testsuite_dirs = [
    "atompaw",
    "bigdft",
    "built-in",
    "etsf_io",
    "fast",
    #"cpu",  <<< disabled
]
```

To add a new directory to the test suite, it is sufficient to add its name to the python list.

Each directory listed in `testsuite_dirs` contains an additional `__init__.py` file 
with the list of input files associated to the tests and (optional) global attributes::

```python
cat tests/libxc/__init__.py

# CPP variables 
need_cpp_vars = [
    "HAVE_LIBXC",
]

# List of input files
inp_files = [
    "t00.in",
    "t01.in",
    "t02.in",
    "t03.in",
    "-t11.in", # Disabled
]
```

**inp_files** is the list with the names of the input files contained in the Input directory.
One can prepend a dash to the name of file (e.g. “-t11.in”) to signal to the python code 
that this test has been disabled.

The variable **need_cpp_vars** is a list of CPP variables that must be defined 
in the include file *config.h* in order to activate the tests in **this directory**.
In this case, for example, the *libxc* tests are executed only if **HAVE_LIBXC** is defined in *config.h*.
Note that one can prepend the character `!` to the name of the CPP variable to specify that the tests 
should not be executed if the variable is not defined in the build.

Each input file contains all the information needed to run the test and to analyze the 
final results. These meta-variables are gathered in the **TEST_INFO** section.

## Conventions

This paragraph summarizes the conventions used for the names of the tests.
The ABINIT tests are grouped in suites whose name is given by the name of the 
directory as specified in *tests/\_\_init\__.py*.
Each suite can be optionally divided into sub-suites whose names must be registered in *dirname/\_\_init\_\_.py*
To each test is therefore assigned a *suite*, a *sub-suite* and an integer number (>= 0) 

!!! important

    The name of the suite/subsuite must be unique. 

[![asciicast](https://asciinema.org/a/40324.png)](https://asciinema.org/a/40324)

The organization in terms of suites/sub-suites allows one to select easily the tests
with the command line interface `runtests.py`.
The name of the input file must match one of the two possible regular expressions:

1. **t[integer].in**  *e.g.* `t11.in`
2. **t[subsuitename]_[integer].in**  *e.g.* `tgw1_1.in`

The latter syntax is used, for example, in the tutorial directory to 
group the tests associated to a particular lesson.
The name of sub-suite can be passed as argument to *runtests.py* to specify
that only the tests in that particular sub-suite should be executed.

For example, the command:
  
    runtests.py gw1[1:3]

will run the tests *tgw1_1.in*, *tgw1_2.in*.

!!! warning

    Each test has a standard input, a standard output and a standard error 
    whose names are *tgw1_1.stdin*, *tgw1.stdout*, and *tgw1.stderr*, respectively.
    These names are constructed automatically by the python code hence 
    developers should avoid creating similar file names in the Fortran executables at runtime.

## The TEST_INFO section

Each input file contains, in addition to the parameters passed to 
the Fortran code, an additional section (`<TEST_INFO>`) that provides all the 
information needed to run the test and to analyze the final results. 

The `<TEST_INFO>` section is placed at the end of the input file
in order to avoid problems with executables reading from standard input.
The XML-like markers `<BEGIN_TEST_INFO>` and `<END_TEST_INFO>` enclose the section. 
Each line starts with the sentinel `#%%` that will be removed during the parsing. 
What is left is a standard INI configuration file providing all the information 
needed to run the test and to analyze the final results.
The figure below shows how options are grouped in different sections:

![](testsuite_howto_assets/testinfo.png)

A simple example of TEST_INFO section is reported in the figure below:

![](testsuite_howto_assets/simple_testinfo.png)

Note that tokens are separated by a comma (e.g. *tolabs=1.0, tolrel=0.0*), while fields
are separated by a colon

!!! tip
    Use *testlint.py* to validate a new TEST_INFO section

## Important options

This paragraph discusses in more detail the most important options present in the TEST_INFO section.
For the complete documentation of the options, one can execute the script tests/pymods/testsuite.py.

executable
:    The name of the binary file e.g. abinit (mandatory)

max_nprocs 
:    the maximum number of MPI processes that can be used (mandatory)

pre_commands and post_commands 
:    List of shell commands ([shell] section) that are executed before and after running 
     `executable`. At present only the commands `cp`, `mv`, and `touch` are supported.
     Since the python code uses absolute pathnames to deal with files, one has 
     to specify where the files are located
     (we cannot use relative paths as the python function os.curdir is not thread-safe).

     - i --> input directory
     - p --> Psps_for_tests  
     - w --> Working directory (the directory where the test is executed)
        
     Example: *ww_mv foo bar* corresponds *mv workdir/foo wordir/bar* while
     *iw_cp twan.in wannier.in* corresponds to *cp Input/twan.in workdir/wannier.in*

keywords
:    List of keywords that can be used to select the tests.
     Note that the name of executable is always added to the list 
     of keywords, hence `runtests.py -k cut3d` run all the tests
     associated to the executable `cut3d`

## Chained Tests

In some cases, one has to connect different steps.
A typical case is depicted in the figure below:

![](testsuite_howto_assets/dep.png)

In this case, test *t95.in* produces a density file DEN that
is needed to start the second calculation in test *t96.in*.
The python code must be aware of this dependency so that 
*t95.in* and *t96.in* can be executed in the correct order. 

The *TEST_INFO* section defines the option *test_chain*
with the name of the tests forming the chain.
The sequence is ordered in the sense that the python script will
execute the tests starting from the first item of the list.

!!! note

    All the input files belonging to a test chain must contain
    the *test_chain* option (obviously with the same list of tests)

During the execution of a test chain, one usually has to perform basic operations 
such as file renaming or file copying in order to connect the different steps 
(for example, we may want to rename the DEN file produced in *t95.in* so that 
the output density is automatically read in test *t96.in*).
The shell section contains two options that are used to specify the list of 
shell commands that are executed before and after running the test.
A typical example is shown below:

![](testsuite_howto_assets/test_chain.png)

Note the peculiar syntax for the shell commands:

![](testsuite_howto_assets/rshell_syntax.png)

## Multi-parallel Tests

A multi-parallel test is a test that can be executed in parallel
with different numbers of MPI processes (e.g. the tests in tests/paral).
The ABINIT test suite is designed so that it is possible to define a single input file
from which multiple parallel tests are automatically generated.
In what follows, we describe how to specify a multi-parallel test in a *TEST_INFO* section. 

The most important option is *nprocs_to_tests* (defined in the `[mpi]` section) 
that lists the number of MPI processes to use for the different tests. 
*max_nprocs* is the maximum number of MPI processes 
that are supported by the calculation (just set it to the maximum value present in *nprocs_to_test*)

Then for each possible number of MPI processes we have a section whose name 
is constructed with the rule: `NCPU_#nproc`.
A simplified example of *TEST_INFO* section for a multi-parallel test is reported below

![](testsuite_howto_assets/multipara.png)


<!--
## A more complicate example: a chain of multi parallel tests

!!! warning

    You cannot change the number of MPI processes inside a test chain
-->


## How to run the tests

The *abinit/tests/runtests.py* script provides a user-friendly command line interface
that allows the user to select tests by **keywords**, **authors**, **suite name**, **input variables** etc. 
It also provides options for controlling the number of MPI processes, 
the number of OpenMP threads as well as the number of python threads (task parallelism).

The syntax is:

    runtests.py [suite_args] [options]. 

where `suite_args` is a list of suite names that can be optionally selected (sliced) using python syntax.
For example:

    runtests v3[:4] v4[45:] v5[3] 

executes all the tests in *v3* from 0 up to 4 (excluded), all the tests in *v4* starting from 45 and *v5/t3*

!!! warning

    If *suite_args* is not given, the script will execute the entire Test suite.

The most useful options are:

-j
: specifies the number of python processes (task parallelism)

-n
: specifies the number of MPI nodes for e.g. *abinit*

-o
: specifies the number of OpenMP threads for e.g. *abinit*

-k
: selects tests by keywords


!!! tip

    For the complete list of options supported, use `runtests -h`

## How to run tests with MPI

In the simplest case, one can run MPI tests by just issuing:

    runtests.py -n 2 paral

where *n* specifies the number of MPI processes.
*runtests.py* employs very simple rules to guess the MPI implementation to be invoked. 
In particular, it assumes that:

1. The environment is properly setup (*PATH*, *LD_LIBRARY_PATH*)

2. MPI demons (e.g. *mpd*) are already running in the background 
   (you have to initialize the demon manually before launching the tests)

3. It uses the *mpirnuner* in *PATH* (default: *mpirun*) and assumes 
   the standard syntax for specifying the number of processors 
   (i.e. mpirun -n 4 executable < stdin > stdout)

If you need to override the default settings, you can pass options to the script
via a configuration file in the INI format (-c options):

    runtests.py -n 2 -c mpi.cfg

    cat mpi.cfg

        [mpi]
          mpi_prefix = /usr/local/openmpi-gcc47/
          mpirun_np = %(mpi_prefix)s/bin/mpirun -np

*mpirun_np* is the string with the path to the mpirunner, followed by any additional option you may want
to use. The option used to specifying the number of MPI nodes that must be added at the end of the string.

!!! tip

    For more examples, consult the configuration files in abinit/tests/mpi_cfg.

## How to run testbot.py on a buildbot slave 

This section is intended for developers who may want to run interactively 
the entire test suite on a slave builder (example: one or more tests 
are failing on a slave. You have already modified the source code to fix the problem
and you want avoid to launch a full buildbot build just to check that your changes are OK)

In this case, one can re-rerun the entire test suite (or part of it) by just executing the following two steps

    cd abinit/tests
    testbot.py

The script `testbot.py` reads the configuration file `testbot.cfg` (already present in the working directory), 
runs the entire set of tests and produces the final report.

Note that one modify the configuration options defined in `testbot.cfg` in order to speed-up the execution of the tests.
In particular one can use the options:

    # with_tdirs = list of directories to execute
    # without_tdirs = list of directories that will be excluded

## How to add a new test

Let's assume you want to add a new test `tfoo_1.in` to the directory `v5`.
This test will be hence located in the `foo` sub-suite within suite `v5` 
so that we can easily run it with `runtests`:
    
    runtests foo[1] 

In order to add the new test, one has to follow the following steps:

1. Add the name of the input file to the `inp_files` list in `v5/__init__.py`

2. Register the name of the sub-suite to the list sub-suites 
   defined in `v5/__init__.py` 
   (this step can be skipped if the test does not belong to a sub-suite)

3. Add the input files to *v5/Input* and the reference files to *v5/Refs*

!!! warning

       The code uses a pickle file (*test_suite.cpkl*) to store the set of objects
       representing the tests of the test suite.
       Remember to regenerate the database after any change to the TEST_INFO section
       or any modification of the configuration parameters of the test suite (*\_\_init\_\_.py* files).
       *runtests.py* provides the handy option -r (regenerate) to automatically regenerate the database
       before running the tests.


<!--
## How to add a chain of tests

TODO

## How to add a parallel test
-->

## How to add support for a new executable

Let's assume that you have a new executable named *foo.x*
and you want to change the python code so that the results of *foo.x* are automatically tested.
What are the modifications needed to integrate foo.x in the ABINIT test suite?

First you have to provide an input file for *foo.x* with a
`<TEST_INFO>` section that provides **all the information** needed to run the executable. 
If **foo.x** requires some kind of input data or extra rules that are not supported, 
you will have to modify the parser so that the new options are stored in *BaseTest*.

Then you have to tell the code how to construct the standard input that will be passed to *foo.x*. 
This is the most complicated part as it requires some understanding of the internal implementation.

The code uses three different objects:
 
1. *BaseTest*
2. *ChainOfTests*
3. *TestSuite*

to represent the tests of the test suite.

*BaseTest* is the base class that provides methods to run and analyze the results
of the test (you should try to reuse this piece of code as much as possible).

*ChainOfTests* is a list of *BaseTest* instances that cannot be executed separately. 
Typical example: t1.in produces an output result that is used as the input of tests *t2.in*.

*TestSuite* is a list of (*BaseTest*, *ChainOfTests*) instances and provides
user-friendly methods to extract tests according to some rule (e.g. keywords) 
and to run these tests with Python threads.

The *BaseTest* class provides the *make_stdin* method that returns 
a string with the standard input that should be passed to the Fortran executable.
Since each executable has its own format for the input file, the *BaseTest* is not able to handle all the different cases.
Hopefully, you only need to replace the *make_stdin* method of *BaseTest*
so that the appropriate standard input is constructed from the values specified in the `<TEST_INFO>` section.  
---
authors: MG, XG
---

# HowTo guide for developers

This page is intended as a quick reference to solve some of the problems
that are commonly encountered when developing within the Abinit project.

## How to generate the configure script via *makemake*

Abinit uses the standard **configure && make** [approach](https://thoughtbot.com/blog/the-magic-behind-configure-make-make-install)
to build from source.
Note, however, that the developmental version does not contain the *configure* script.
To generate the *configure* script and the other files required by the build system,
you need to execute **makemake**:

    cd ~abinit
    ./config/scripts/makemake

*makemake* requires a recent version of the python interpreter as well as
[m4](https://www.gnu.org/software/m4/), [autoconf](https://www.gnu.org/software/autoconf/),
and [automake](https://www.gnu.org/software/automake/).
If these tools are not installed on your machine, you need to compile them from source or use
your preferred package manager to install them.
I usually use the [conda](https://docs.conda.io/en/latest/) package manager and:

    conda install m4 autoconf automake -c conda-forge

!!! important

    Remember to run *makemake* every time you add/remove a Fortran file or a new directory or you
    change parts the buildsystem *i.e.* the files in *~abinit/config*.

## How to build Abinit

Developers are invited to build the executables inside a **build** directory *i.e.* a directory that is **separated**
from the source tree in order to keep the source directory as clean as possible and allow for multiple builds.
I usually use the naming scheme: `_build_[compiler_name]` for the build directory and an external file
(e.g. *gcc.ac*) storing the configuration options that can be passed to *configure* via the **--with-config-file** option:

```sh
mkdir _build_gcc
cd _build_gcc

../configure --with-config-file=gcc.ac

make -j8  # use 8 processes to compile
```

Note that the name of the options in the *config-file* is in normalized form that is:

* Remove the initial `--` from the name of the option
* Replace `-` with underscore `_` everywhere

For instance, `--with-mpi-prefix` in normalized form becomes `with_mpi_prefix`.
Examples of configuration files for clusters can be found in the [abiconfig package](https://github.com/abinit/abiconfig).
A detailed description of the configuration options supported by the build system is given in this guide by Marc:

<embed src="https://school2019.abinit.org/images/lectures/abischool2019_installing_abinit_lecture.pdf"
type="application/pdf" width="100%" height="480px">

Once the build is completed, it is a good idea to check whether the executable works as expected
by running the tests in the *v1* directory with:

```sh
cd tests
../../tests/runtests.py v1 -j4
```

As usual, use:

    ../../tests/runtests.py --help

to list the available options.
A more detailed discussion is given in [this page](/developers/testsuite_howto).

[![asciicast](https://asciinema.org/a/40324.png)](https://asciinema.org/a/40324)

!!! tip

    Remember to run the tests as **frequently** as possible while developing new features
    in order to spot possible regressions or incompatibilities.
    Trust me, you can save a lot of time if you run *runtests.py* systematically!

## How to browse the source files

The HTML documentation generated by Robodoc is available at
[this page](https://www.abinit.org/sites/default/files/robodoc-html/masterindex.html).

If you need a tool to navigate the Abinit code inside the editor,
I would sugest [exuberant-ctags](http://ctags.sourceforge.net/).

To generate a **tags** file containing the list of procedures, modules, datatypes for all files inside *~abinit/src*, use:

    cd ~abinit/src
    ctags -R

Now it is possible to open the file containing the declaration of the *dataset_type* Fortran datatype
directly from the terminal with:

    vi -t dataset_type

Inside the editor, you can go directly to a tag definition by entering the following in vim command mode:

    :tag dataset_type

More tips for vim users are available [here](https://andrew.stwrt.ca/posts/vim-ctags/).
For `emacs` see [this page](https://www.emacswiki.org/emacs/EmacsTags).

Finally, one can use the *abisrc.py* script in the `~abinit` directory.

TODO


## How to debug with gdb

Load the executable in the GNU debugger using the syntax:

    gdb path_to_abinit_executable

Run the code with the gdb *run* command and redirect the standard input with:

    (gdb) run < run.files

Wait for the error e.g. SIGSEGV, then print the **backtrace** with:

    (gdb) bt


!!! tip

    Remember to compile the code with the `-g` option. Avoid debugging code compiled with -O3.
    In some tricky cases, you may need to resort to -O0 or use Fortran `print` statements to avoid miscompilation.


For a more complete introduction to *gdb*, we suggest this youtube tutorial:

<iframe width="1384" height="629" src="https://www.youtube.com/embed/bWH-nL7v5F4" frameborder="0"
allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<!--
How to use LLDB

$ lldb ../../../src/98_main/abinit
(lldb) target create "../../../src/98_main/abinit"
Current executable set to '../../../src/98_main/abinit' (x86_64).
(lldb) settings set target.input-path t85.in
(lldb) run
-->

<!--
## Basic conventions

* All Fortran procedures should be declared in modules and imported by client code with the `use` statements
* Module names usually start with `m_`
* CPP macros and standard imports
* Low-level code is located in low-level directories
* Avoid cyclic dependencies inside a directory

We use lot of CPP macros defined in `abi_common.h` to wrap basic Fortran statements.
Please, familiarize yourself with the definitions in `abi_common.h`.
In particular:

* **Never ever** use Fortran `stop` to abort but handle the error with `MSG_ERROR(msg)` or `MSG_BUG(msg)`
* Return an exit status if low-level procedure and let the caller handler the error.
* Use `call wrtout(unit, ...)` instead of `print` or `write(*,*)`
* Use `ABI_MALLOC(array, (3, 3))` instead of `allocated(array(3, 3))`
* Use `ABI_FREE(array)` to deallocate memory or the `ABI_SFREE` variant if you need to check the allocation status.

!!! important

    Abinit uses specialized logic to track Fortran **allocations** and **deallocations** in order to spot possible memory leaks
-->

## How to add a new Fortran file

* Create the F90 module and `git add` it
* Register the F90 file in the `abinit.src` file (avoid duplicated names in the public API, **abisrc.py** will complain about that)
* Rerun `makemake` in the source directory
* Rerun `configure` and `make` in the build directory (possibly `make clean && make`)

## How to add a new Abinit input variable

This section documents the procedure required to add a new Abinit input variable.
To make things as simple as possible, we ignore the (more complicated) case of dimensions
such as [[nkpt]] or [[nsym]] whose value may depend on the dataset.
To add a new variables follow the below steps:

- Add the new variable to **dataset_type**.
  Remember that the name cannot end with a digit as this enters into conflict with the multidataset syntax.

- The default value of the new input variable can be specified in two different ways:

    * in the **declaration** of the Fortran type if the size is known at compile time
      and the initial value does not depend on other variables.
    * in the **indefo** routine if the value must be computed at runtime.

- Add the name of the new variable to **chkvars**.

- Add a new section to **dtset_copy** to copy the new variable (use **alloc_copy** if allocatable).

- If you need an **allocatable entity**, remember to **deallocate** memory in **dtset_free**.

- Read the variable in the **invars2** (if it is not a basic dimension).

- Change one of the outvars routines (**outvar_a_h**, **outvar_i_n**, **outvar_o_z**) to print the variable
  according to the first letter of the new variable

- The logic for checking the consistency of input variables goes to **chkinp**.
  Use the routines *chkint_eq*, *chkint_ne*, *chkint_ge*, *chkint_le*, *chkdpr*.

- Add the documentation of the new variable to `~abinit/abimkdocs/variables_CODE.py`
  following the instructions given in [this section](/developers/abimkdocs#how-to-addmodify-an-input-variable).

Finally,

    make clean && make -j8

since you *broke* the [ABI](https://en.wikipedia.org/wiki/Application_binary_interface)
of a public datastructure and all the object files depending on this datastructure must be recompiled
(if you are developing a library, you should release a new major version!)

No, it's not a typo, **ABIs** and **APIs** are two different concepts!
From this answer on [stackoverflow](https://stackoverflow.com/questions/2171177/what-is-an-application-binary-interface-abi)

>   If you expand, say, a 16-bit data structure field into a 32-bit field, then already-compiled code
    that uses that data structure will not be accessing that field (or any following it) correctly.
    Accessing data structure members gets converted into memory addresses and offsets during compilation
    and if the data structure changes, then these offsets will not point to what the code is expecting
    them to point to and the results are unpredictable at best.

For the treatment of dimensions see **invars0**, **invars1m**

## How to add a new test in the test suite?

The following information complements the [testsuite documentation](/developers/testsuite_howto).

In order to introduce a test, one needs to:

-  Provide a new input file in e.g. **tests/v8/Input**
   (or modify an existing one, possibly in another directory, but please do not suppress the existing capability testing!)

-  Provide a new reference file in the corresponding **tests/v8/Refs**

-  Insert the test in the list of tests to be done, by adding its name in **tests/v8/\_\_init\_\_.py**.

-  Document the test by adding a commented section inside the input file
   (edit an existing input file to follow its style)

- Declare the pseudopotentials, the flow of actions, the files to be analyzed, the tolerances for the test,
  inside this documentation. For the tolerances, start with

        tolnlines = 0, tolabs = 0.000e+00, tolrel = 0.000e+00

  The different fields control how strictly the test results will be analysed:

  *  the maximum floating point difference found must be less than tolabs,
  *  the maximum relative difference less than tolrel, for each line individually,
  *  **tolnlines** is the maximal number of lines found to be different between the output and reference file
     (within another tolerance that can be tuned by the option **opt=-medium**, **opt=-easy** or **opt=-ridiculous**,
     see the added section in other input files).

If this procedure fails, contact the code maintainers in order adjust the test case.
This might mean modifying the tolerances files.
Unless you are really expert, let the maintainer do the final adjustment.

!!! important

    Please, try to keep preferably the total time per test case to less than 10 seconds.
    30 seconds is a maximum for most test cases. Going being this needs exceptional reasons.

Supposing now that you have introduced a new test case.
It can be used, with the other tests of the ABINIT test suite, through different channels:

*  these tests can be triggered on the test farm Web Page.

*  locally (on your machine), the whole set of sequential tests, or particular tests or series of tests,
   can be triggered by issuing, in the ABINIT/tests directory, the command *./runtests.py*.
   (see the many capabilities of this scripts by issuing *./runtests.py --help*).
   Other sets of calculations can be triggered, as described by issuing *make help*.
   The result will appear in a new subdirectory *Test_suite*

Last but not least: are you sure that your modifications do not deteriorate the performance of the code
in the regime where your modifications are not used?
You should inspect your modifications for both memory use and CPU time.

## Code Coverage

In computer science, [code coverage](http://en.wikipedia.org/wiki/Code_coverage)
is a measure used to describe the degree to which the source code
of a program is tested by a particular test suite.
A program with high code coverage has been more thoroughly tested and has a lower chance of containing
software bugs than a program with low code coverage.
Many different metrics can be used to calculate code coverage; some of the most basic
are the percent of program subroutines and the percent of program statements called
during execution of the test suite.
We aim that the test suite covers all the functionalities of ABINIT.

How to trigger a coverage report?

There is one slave dedicated to *on-demand* execution of branches by the developers
that produces a code coverage report, at present, **higgs_gnu_7.3_cov**.
It can be launched by the general [on-demand interface](https://bbportal.abinit.org)
(contact Jean-Michel or Xavier if you do not yet have access to it).
Code coverage reports from recent runs of the tests are available [here](http://coverage.abinit.org).
If you see parts of the code which are not well tested, please contribute to improving coverage by writing new tests!


!!! info

    How does it work?

    ABINIT is built with special options such that every function that is executed in the program
    is mapped back to the function points in the source code.
    A `.gcno` file is generated when the source file is compiled with the GCC *-ftest-coverage* option.
    It contains information to reconstruct the basic block graphs and assign source line numbers to blocks.
    More info are available in the [Gvoc page](https://gcc.gnu.org/onlinedocs/gcc-9.1.0/gcc/index.html#toc-gcov---a-Test-Coverage-Program).
    A *.gcda* file is generated when a program containing object files built with the GCC *-fprofile-arcs* option is executed.
    A separate *.gcda* file is created for each object file compiled with this option.
    It contains arc transition counts, and some summary information.
    Finally, we use [lcov](http://ltp.sourceforge.net/coverage/lcov.php) to analyze the *.gcda* files for generating a html report


{% include doc/developers/robodoc.doc.txt %}

{% include doc/developers/debug_make_parents %}

{% include doc/developers/debug_make_abiauty %}
---
description: Design of the new YAML-based test suite.
authors: TC, MG
---

# YAML-based tests: design principles and implementation details

This document discusses the new YAML-based format used to represent physical results
in the main output file.
The connection with the ABINIT test suite and the yaml syntax used to define
tolerances, parameters and constraints is also discussed.

The new infrastructure consists of a set of Fortran modules to output 
structured data in YAML format and Python code to parse the output files and analyze data.

## Motivations

In ABINITv8 and previous versions, the ABINIT test suite is based on input files with the
associated reference output files.
Roughly speaking, an automatic test consists in comparing the reference file with
the output file in a **line-oriented fashion**
by computing differences between floating-point numbers **without any knowledge** about
the meaning and the importance of the numerical values.
This approach is very rigid and rather limited because it obliges developers to use a
single (usually large) tolerance to account for possibly large fluctuations in
the intermediate results whereas the tolerance criteria should be ideally
applied only to the final results that are (hopefully) independent of details
such as hardware, optimization level and parallelism.

This limitation is clearly seen when comparing the results of iterative algorithms.
The number of iterations required to converge, indeed, may depend
on several factors especially when the input parameters are far from convergence
or when different parallelization schemes or stochastic methods are used.
From the point of view of code validation, what really matters is the **final converged value**
plus the **time to solution** if performance ends up being of concern.
Unfortunately, any line-by-line comparison algorithm will miserably fail in such
conditions because it will continue to insist on having the same number of
iterations (lines) in the two calculations to consider the test succeeded.
It is therefore clear that the approach used so far to validate new developments in
Abinit is not able to cope with the challenges posed by high-performance
computing and that smarter and more flexible approaches are needed to address these limitations.
Ideally, we would like to be able to

* use **different tolerances** for particular quantities that are selected by keyword
* be able to replace the check on the absolute and relative difference with a **threshold check**
  (is this quantity smaller that the give threshold?)
* have some sort of syntax to apply different rules depending on the **iteration state** e.g. the dataset index
* execute python code (**callbacks**) that operates on the data to perform more advanced tests requiring 
  some sort of post-processing
* provide an easy-to-use **declarative interface** that allows developers to define the logic
  to compare selected quantities.

In what follows, we present this new infrastructure, the design principles and the steps required
to define YAML-based tests.

<!--
The goal of this project is to provide ABINIT developers with reusable tools to implement physics-aware tests.
With "physics-aware" we intend tests in which the developer can customize the tolerances and the logic used to
compare numerical results thanks to the fact that the new parser
is aware of the context and of the meaning of the numerical values extracted from the output file.
The new infrastructure consists of a set of Fortran modules to output
structured data in YAML format as well as Python code to parse the output files and analyze data.
One of the goals of this project is to implement a 

The new python-based infrastructure allows developers to implement more rigorous tests on the portability of
important physical quantities while ignoring intermediate results.
This goal is achieved by providing a *declarative* interface that allows developers to define
the logic used to compare selected physical quantities.
We also provide a declarative API to check that the results computed by the ab-initio code
satisfies fundamental physical and mathematical rules such as energy
conservation, Newton's third law, symmetry properties, etc.

That is not quite as simple as it sounds, especially because one of our goals is
to minimize the amount of (coding) work required to express such logic. 
There are therefore basic rules and design principles Abinit developers should be
aware of in order to take fully advantage of the new infrastructure. 
In what follows, we briefly describe the philosophy employed to implement the YAML-based
test suite, the syntax used to define tests and the modifications required to extend the framework.
-->

!!! important

    The new YAML-based testsuite relies on libraries that are not provided
    by the python standard library.
    To install these dependencies in *user* mode, use:

        pip install numpy pyyaml pandas --user

    If these dependencies are not available, the new test system will be
    disabled and a warning message is printed to the terminal.


## Implementation details

For reasons that will be clear later, implementing smart algorithms requires metadata and context.
In other words, the python code needs to have some basic understanding of the meaning of the numerical values 
extracted from the output file and must be able to locate a particular property by name or by its "position"
inside the output file.
For this reason, the most important physical results are now written in the main output file (*ab_out*)
using machine-readable [YAML](https://en.wikipedia.org/wiki/YAML) documents.

A YAML document starts with three hyphens (---) followed by an
*optional* tag beginning with an exclamation mark (e.g. `!ETOT`). 
Three periods (...) signals the end of the document. 
Following these rules, one can easily write a dictionary containing the different contributions
to the total free energy using:


```yaml
--- !EnergyTerms
comment             : Components of total free energy (in Hartree)
kinetic             :  4.323825067238190201E+01
hartree             :  3.133994553363548619E+01
xc                  : -2.246182631222462689E+01
Ewald energy        : -1.142203169790575572E+02
psp_core            :  6.536925226004570710E+00
local_psp           : -9.852521179991468614E+01
spherical_terms     :  2.587718945528139081E+00
internal            : -1.515045147136467563E+02
'-kT*entropy'       : -3.174881766478041684E-03
total_energy        : -1.515076895954132397E+02
total_energy_eV     : -4.122733899322517573E+03
...
```

Further details about the meaning of **tags**, **labels** and their connection with the
testing infrastructure will be given in the below sections. For the time being,
it is sufficient to say that we opted for YAML because it is a *human-readable*
data serialization language already used in the log file to record important
events such as WARNINGs, ERRORs and COMMENTs (this is indeed the *protocol*
employed by AbiPy to monitor the status of Abinit calculations).
Many programming languages, including python, provide support for YAML hence
it is relatively easy to implement post processing tools
based on well-established python libraries for scientific computing such as
NumPy, SciPy, Matplotlib and Pandas. Last but not least, writing YAML in
Fortran does not represent an insurmountable problem provided one keeps the
complexity of the YAML document at a *reasonable level*.
Our Fortran implementation, indeed, supports only a subset of the YAML specifications:

* scalars
* arrays with one or two dimensions
* dictionaries mapping strings to scalars
* tables in CSV format (this is an extension of the standard)

!!! important

    YAML is not designed to handle large amount of data therefore it should
    not be used to represent large arrays for which performance is critical and human-readability
    is lost by definition (do you really consider a YAML list with one thousand numbers human-readable?).
    Following this philosophy, YAML is supposed to be used to print the most important results
    in the main output file and should not be considered as a replacement for binary netcdf files
    when it comes to storing large data structures with lots of metadata.

    Note also that we do not plan to rewrite entirely the main output file in YAML syntax
    but we prefer to focus on those physical properties that will be used by the new test procedure
    to validate new developments.
    This approach, indeed, will facilitate the migration to the new YAML-based approach as only selected portions
    of the output file will be ported to the new format thus maintaining the look and the feel relatively
    close to the previous *unstructured* format.

<!--
In this tutorial we mainly focus on the user interface i.e. on the syntax used to define Yaml-based tests.
For the technical details related to the internal implementation see
-->

## YAML configuration file

### How to activate the YAML mode

The parameters governing the execution of the test
are specified in the `TEST_INFO` section located at the end of the input file.
The options are given in the [INI file format](https://en.wikipedia.org/wiki/INI_file).
The integration of the new YAML-based tests with the pre-existent infrastructure
is obtained via two modifications of the current specifications.
More specifically:

- the **files_to_test** section now accepts the optional argument **use_yaml**.
  The allowed values are:

    * "yes" --> activate YAML mode
    * "no" -->  do not use YAML mode (default)
    * "only" --> use YAML mode, deactivate legacy fldiff algorithm

- a new *optional* section **[yaml_test]** has been added. 
  This section contains two mutually exclusive fields:

    * *file* --> path of the YAML configuration file.
      The path is relative to the input file. A natural choice
      would be to use the same prefix as the input file e.g. "./t21.yaml" 
      is the configuration file associated to the input file "t21.in".

    * *test* --> multi-line string with the YAML specifications. This option may
      be used for really short configurations heavily relying on the default values.


An example of `TEST_INFO` section that activates the YAML mode can be found in [[test:paral_86]]:

```sh
#%%<BEGIN TEST_INFO>
#%% [setup]
#%% executable = abinit
#%% [files]
#%% psp_files = 23v.paw, 38sr.paw, 8o.paw
#%% [paral_info]
#%% nprocs_to_test = 4
#%% max_nprocs = 4
#%% [NCPU_4]
#%% files_to_test =
#%%   t86_MPI4.out, use_yaml = yes, tolnlines = 4, tolabs = 2.0e-2, tolrel = 1.0, fld_options = -easy;
#%% [extra_info]
#%% authors = B. Amadon, T. Cavignac
#%% keywords = DMFT, FAILS_IFMPI
#%% description = DFT+DMFT for SrVO3 using Hubard I code with KGB parallelism
#%% topics = DMFT, parallelism
#%% [yaml_test]
#%% file = ./t86.yaml
#%%<END TEST_INFO>
```

with the associated YAML configuration file given by:

{% dialog tests/paral/Input/t86.yaml %}


### Our first example of YAML configuration file 

Let us start with a minimalistic example in which we compare the components of
the total free energy in the `Etot` document with an absolute tolerance of 1.0e-7 Ha.
The YAML configuration file will look like:

```yaml
Etot:
    tol_abs: 1.0e-7
```

The `tol_abs` keyword defines the **constraint** that will applied to **all the children** of the `Etot` document.
In other words, all the entries in the `Etot` dictionary will be compared with 
an absolute tolerance of 1.0e-7 and the **default value** for the relative difference `tol_rel` as this 
tolerance is not explicitly specified.

<!--
In order to pass the test, all the absolute differences must be smaller that `tol_abs`.
If this condition is not fulfilled, the test will fail and python will list the entries that did not pass the test.
-->

There are however cases in which we would like to specify different tolerances for particular entries
instead of relying on the *global* tolerances.
The `Etot` document, for example, contains the total energy in eV in the `Total energy (eV)` entry.
To use a different absolute tolerance for this property, we **specialize** the rule with the syntax:

<!--
Since the unit is different, the absolute tolerance does not have the same impact on the precision.
We want to achieve the same relative precision on this term but we cannot
achieve the same absolute precision. Though we will **specialize** the rules
and use the `tol_rel` constraint. 
We could do something like this:

```yaml
Etot:
    tol_abs: 1.0e-7
    Total energy (eV):
        tol_rel: 1.0e-10
```

However the `tol_abs` constraint defined in `Etot` is _inherited_ by `Total
energy (eV)` which means that the comparison will be performed with a relative tolerance `tol_rel` of
1.0e-10 **and** an absolute tolerance `tol_abs` of 1.0e-7. Most of the time it is
what we need even though here it's not. So we will just use a different
tolerance for `tol_abs` in `Total energy (eV)`.
-->

```yaml
Etot:
    tol_abs: 1.0e-7
    Total energy (eV):
        tol_abs: 1.0e-5
```

<!--
Now we achieve the same relative precision and the test does not fail because
of the looser absolute precision of the total energy in eV.
-->

To change the default value for the relative difference, it is sufficient to specify the 
constraint outside of the document:

```yaml
tol_rel: 1.0e-2

Etot:
    tol_abs: 1.0e-7
    Total energy (eV):
        tol_abs: 1.0e-5
```


### Basic concepts

In the previous section, we presented a minimal example of configuration file.
In the next paragraphs we will discuss in more detail how to implement more advanced test
but before proceeding with the examples, we need to introduce some basic terminology
to facilitate the discussion.
<!--
To explain how to build a test specification we will go step-by-step through
the definition of [[test:paral_86]].
First we have to introduce the most important concepts.
-->

Document tree
: The YAML document is a dictionary that can be treated as a tree
  whose nodes have a label and leaf are scalars or special data structures
  identified by a tag (note however that not all tags mark a leaf).
  The top-level nodes are the YAML documents and their labels are the names of their tag.

Config tree
: The YAML configuration also takes the form of a tree where nodes are
  **specializations** and its leaf represent **parameters** or **constraints**.
  Its structure matches the structure of the **document tree** thus one can define rules
  (constraint and parameters) that will be applied to a specific part of the **document tree**.

Specialization
: The rules defined under a specialization will apply only on the matching node
  of the *document tree* and its children.

Constraint
: A constraint is a condition one imposes for the test to succeed. Constraints
  can apply to leafs of the document tree or to nodes depending of the nature of the constraint.

Parameter
: A parameter is a value that can be used by the constraints to modify their behavior.

Iteration state
: An iteration state describes how many iterations of each possible level are present
  in the run (e.g. idtset = 2, itimimage = not used, image = 5, time = not used).
  It gives information on the current state of the run. Documents are implicitly
  associated to their iteration state. This information is made available to
  the test engine through specialized YAML documents with `IterStart` tag.

!!! tip

    To get the list of constraints and parameters, run:

        ~abinit/tests/testtools.py explore

    and type `show *`. You can then type for example `show tol_eq` to learn more
    about a specific constraint or parameter.

<!--
A few conventions on documents writing

: All data document have a **tag**. It should be a unique identifier
  at the scale of an _iteration state_. It also describes the structure of the document
  like other uses of tags but in this case it identify the document. The *comment* field is
  optional but it is recommended especially when the purpose of the document is not obvious.
-->

### A more complicated example

The `Etot` document is the simplest possible document. It only contains fields
with real values. Now we will have a look at the `ResultsGS` document
that represents the results stored in the corresponding Fortran datatype used in Abinit.
The YAML document is now given by:

```yaml
--- !ResultsGS
comment   : Summary of ground states results.
natom     :        5
nsppol    :        1
cut       : {"ecut":   1.20000000000000000E+01, "pawecutdg":   2.00000000000000000E+01, }
convergence: {
    "deltae":   2.37409381043107715E-09, "res2":   1.41518780109792898E-08, 
    "residm":   2.60254842131463755E-07, "diffor": 0.00000000000000000E+00, 
}
etotal    :  -1.51507711707660150E+02
entropy   :   0.00000000000000000E+00
fermie    :   3.09658145725792422E-01
stress tensor: !Tensor
- [  3.56483996349480498E-03,   0.00000000000000000E+00,   0.00000000000000000E+00, ]
- [  0.00000000000000000E+00,   3.56483996349480151E-03,   0.00000000000000000E+00, ]
- [  0.00000000000000000E+00,   0.00000000000000000E+00,   3.56483996349478416E-03, ]

cartesian forces: !CartForces
- [ -0.00000000000000000E+00,  -0.00000000000000000E+00,  -0.00000000000000000E+00, ]
- [ -0.00000000000000000E+00,  -0.00000000000000000E+00,  -0.00000000000000000E+00, ]
- [ -0.00000000000000000E+00,  -0.00000000000000000E+00,  -0.00000000000000000E+00, ]
- [ -0.00000000000000000E+00,  -0.00000000000000000E+00,  -0.00000000000000000E+00, ]
- [ -0.00000000000000000E+00,  -0.00000000000000000E+00,  -0.00000000000000000E+00, ]
...
```

This YAML document is more complicated as it contains scalar fields, dictionaries and even 2D arrays.
**MG: Are integer values always compared without tolerance?**
Still, the parsers will be able to locate the entire document via its tag/label and address all the entries by name.
To specify the tolerance for the relative difference for all the scalar quantities in `ResultsGS`,
we just add a new entry to the YAML configuration file similarly to what we did for `EnergyTerms`: 

```yaml
EnergyTerms:
    tol_abs: 1.0e-7
    total_energy_eV:
        tol_abs: 1.0e-5
        tol_rel: 1.0e-10

ResultsGS:
    tol_rel: 1.0e-8
```

At this point we have to precise that there are implicit top-level settings that
are applied on all quantities that are not subject to a more specific rule.
For example in the above example ResultsGS is subject to the default absolute
tolerance, whereas the default relative tolerance have been overridden.

<!--
For simplicity sake I will only write the `ResultsGS` part in next examples.
-->
Unfortunately, such a strict value for `tol_rel` will become very problematic
when we have to compare the residues stored in the `convergence` dictionary!
In this case, it makes more sense to check that all the residues are below a certain threshold.
This is what the **ceil** constraint is for:

```yaml
ResultsGS:
    tol_rel: 1.0e-8
    convergence:
        ceil: 3.0e-7
```

Now the test will fail if one of the components of the `convergence` dictionary is above 3.0e-7.
Note that the `ceil` constraint automatically disables the check for `tol_rel` and `tol_abs` inside `convergence`.
In other words, all the scalar entries in `ResultsGS` will be compared with our `tol_rel` and the default `tol_abs`
whereas the entries in the `convergence` dictionary will be tested against `ceil`.

!!! tip

    Within the explore shell `show ceil` will list
    the constraints that are disabled by the use of `ceil` in the _exclude_ field.

Up to now we have been focusing on scalar quantities for which the concept of 
relative and absolute difference is unambiguously defined but how do we compare vectors and matrices?
Fields with the `!TensorCart` tags are leafs of the tree. The tester routine
won't try to compare each individual coefficient with `tol_rel`. However we
still want to check that it does not change too much. For that purpose we use the
`tol_vec` constraint which apply to all arrays derived from `BaseArray` (most
arrays with a tag). `BaseArray` let us use the capabilities of Numpy arrays with
YAML defined arrays. `tol_vec` check the euclidean distance between the reference
and the output arrays. Since we also want to apply this constraint to
`cartesian_force`, we will define the constraint at the top level of `ResultsGS`.

```yaml
ResultsGS:
    tol_rel: 1.0e-8
    tol_vec: 1.0e-5
    convergence:
        ceil: 3.0e-7
```

### How to use filters to select documents by iteration state

Thanks to the syntax presented in the previous sections, one can customize tolerances for different 
documents and different entries.
Note however that these rules will be applied to all the documents found in the output file.
This means that we are implicitly assuming that all the different steps of the calculation
have similar numerical stability.
There are however cases in which the results of particular datasets are less numerically stable than the others.
An example will help clarify. 

The test [[test:paral_86]] uses two datasets to perform two different computations.
The first dataset computes the DFT density with LDA while
the second dataset uses the LDA density to perform a DMFT computation.
The entire calculation is supposed to take less than ~3-5 minutes hence the input parameters 
are severely under converged and the numerical noise propagates quickly through the different steps.
As a consequence, one cannot expect the DFMT results to have the same numerical stability as the LDA part. 
Fortunately, one can use **filters** to specify different convergence criteria for the two datasets.

A filter is a mechanism that allows one to associate a specific configuration to a set of **iteration states**.
A filter is defined in a separated section of the configuration file under the node `filters`.
Let's declare two filters with the syntax:

```yaml
filters:
    ks:
        dtset: 1
    dmft:
        dtset: 2
```

Here we are simply saying that we want to associate the label `ks` to all
documents created in the first dataset and the label `dmft` to all document
created in the second dataset. The chose of the names `ks` and `dmft` are
absolutely arbitrary. Pick anything that make sense to your test. This is the
simplest filter declaration possible.  See [here](#filters-api) for more info on
filter declarations.  Now we can use our filters. First of all we will associate
the configuration we already wrote to the `ks` filter so we can have a different
configuration for the second dataset. The YAML file now reads

```yaml
filters:
    ks:
        dtset: 1
    dmft:
        dtset: 2

ks:
    EnergyTerms:
        tol_abs: 1.0e-7
        total_energy_eV:
            tol_abs: 1.0e-5
            tol_rel: 1.0e-10

    ResultsGS:
        tol_rel: 1.0e-8
        tol_vec: 1.0e-5
        convergence:
            ceil: 3.0e-7

```

By inserting the configuration options under the `ks` node, we specify that these rules
apply only to the first dataset. We will then create a new `dmft` node and create a
configuration following the same procedure as before.
We end up with something like this:

```yaml
filters:
    ks:
        dtset: 1
    dmft:
        dtset: 2

ks:
    EnergyTerms:
        tol_abs: 1.0e-7
        total_energy_eV:
            tol_abs: 1.0e-5
            tol_rel: 1.0e-10

    ResultsGS:
        tol_rel: 1.0e-8
        tol_vec: 1.0e-5
        convergence:
            ceil: 3.0e-7

dmft:
    tol_abs: 2.0e-8
    tol_rel: 5.0e-9

    ResultsGS:
        convergence:
            ceil: 1.0e-6
            diffor:
                ignore: true
        fermie:
            tol_abs: 1.0e-7
            tol_rel: 1.0e-8

        stress tensor:
            ignore: true
    EnergyTerms:
        total_energy_eV:
            tol_abs: 1.0e-5
            tol_rel: 1.0e-8

    EnergyTermsDC:
        tol_abs: 1.0e-7
        total_energy_dc_eV:
            tol_abs: 1.0e-5
            tol_rel: 1.0e-8
```

## Filters API

Filters provide a practical way to specify different configuration for different
states of iterations without having to rewrite everything from scratch.

### Filter declaration

A filter can specify all currently known iterators: **dtset**, **timimage**, **image**, and **time**.
For each iterator, a set of integers can be defined with three different methods:

- a single integer value e.g. `dtset: 1`
- a YAML list of values e.g. `dtset: [1, 2, 5]`
- a mapping with the optional members "from" and "to" specifying the boundaries (both
  included) of the integer interval e.g. `dtset: {from: 1, to: 5}`. 
  If "from" is omitted, the default is 1. If "to" is omitted the default is no upper boundary. 

!!! tip

    The order is never relevant in parsing YAML (unless you are writing a list
    of course). As a consequence you can define filter wherever you want in the
    file.

### Filter overlapping

Several filters can apply to the same document even when they overlap. Note, however, 
that overlapping filters must have a trivial order of *specificity*.
In other words, one filter must be a subset of the other one.
The example below is OK because _f2_ is included in _f1_  i.e. is more specific:

```yaml
# this is fine
filters:
    f1:
        dtset:
            from: 2
            to: 7
        image:
            from: 4

    f2:
        dtset: 7
        image:
        - 4
        - 5
        - 6
```

whereas this second example will raise an error because _f4_ is not included in _f3_.

```yaml
# this will raise an error
filters:
    f3:
        dtset:
            from: 2
            to: 7
        image:
            from: 4

    f4:
        dtset: 7
        image:
            from: 1
            to: 5
```

When a test is defined, the default tree is overridden by the user-defined tree.
When a filtered tree is used, it overrides the less specific tree. Trees are
sequentially applied to the tree from the most general to the most specific one.
The overriding process is often used, though it is important to know how it
works. By default, only what is explicitly specified in the file is overridden which means
that if a constraint is defined at a deeper level on the default tree than what
is done on the new tree, the original constraints will be kept. For example let
`f1`  and `f2` be two filters such that `f2` is included in `f1`.

```yaml
filters:
    f1:
        dtset: 1
    f2:
        dtset: 1
        image: 5

f1:
    ResultsGS:
        tol_abs: 1.0e-6
        convergence:
            ceil: 1.0e-6
            diffor:
                1.0e-4

f2:
    ResultsGS:
        tol_rel: 1.0e-7
        convergence:
            ceil: 1.0e-7
```

When the tester will reach the fifth image of the first dataset, the config tree
used will be the following:

```yaml
ResultsGS:
    tol_abs: 1.0e-6  # this come from application of f1
    tol_rel: 1.0e-7  # this has been appended without modifying anything else when appling f2
    convergence:
        ceil: 1.0e-7  # this one have been overridden
        diffor:
            1.0e-4  # this one have been kept
```

If this is not the behavior you need, you can use the "hard reset marker".
Append `!` to the name of the specialization you want to override to completely
replace it. Let the `f2` tree be:

```yaml
f2:
    ResultsGS:
        convergence!:
            ceil: 1.0e-7
```

and now the resulting tree for the fifth image of the first dataset is:

```yaml
ResultsGS:
    tol_abs: 1.0e-6
    convergence:  # the whole convergence node have been overriden
        ceil: 1.0e-7
```

!!! tip

    Here again the `explore` shell could be of great help to know what is inherited
    from the other trees and what is overridden.

### How to use equation and callback

**equation** and **callback** are special constraints because their
actual effects are defined directly in the configuration file. They have been introduced to
increase the flexibility of the configuration file without having to change the python code.

**equation** takes a string in input. This string will
be interpreted as a python expression that must return in a number. The
absolute value of this number will be compared to the value of the `tol_eq`
parameter and if `tol_eq` is greater the test will succeed. The expression can
also result in a numpy array. In this case, the returned value if the euclidean norm of the
array that will be compared to `tol_eq` value.
A minimal example:

```yaml
EnergyTerms:
    tol_eq: 1.0e-6
    equation: 'this["Etotal"] - this["Total energy(eV)"]/27.2114'
```

**equations** works exactly the same but has a list of string as value. 
Each string is a different expression
that will be tested independently from the others. In both case the tested
object can be referred as `this` and the reference object can be referred as `ref`.


**callback** requires a bit of python coding since it will invoke a method of the
structure. Suppose we have a tag `!AtomSpeeds` associated to a document and
a class `AtomSpeeds`. The `AtomSpeeds` class have a method `not_going_anywhere` that checks
that the atoms are not going to try to leave the box. We would like to
pass some kind of tolerance `d_min` the minimal distance atoms can approach
the border of the box. The signature of the method have to be
`not_going_anywhere(self, tested, d_min=DEFAULT_VALUE)` and should return
`True`, `False` or an instance of `FailDetail` (see __Add a new constraint__
for explanations about those). Note that `self` will be the reference
instance. We can then use it by with the following configuration:

```yaml
AtomSpeeds:
    callback:
        method: not_going_anywhere
        d_min: 1.0e-2
```

## Command line interface

The `~abinit/tests/testtools.py` script provides a command line interface to facilitate 
the creation of new tests and the exploration of the YAML configuration file.
The syntax is:

    ./testtools.py COMMAND [options]

Run the script without arguments to get the list of possible commands and use:

    ./testtools.py COMMAND --help

to display the options supported by `COMMAND`.
The list of available commands is:

fldiff

:   Interface to the *fldiff.py* module.
    This command can be used to compare output and reference files without executing ABINIT.
    It is also possible to specify the YAML configuration file with the `--yaml-conf` option so 
    that one can employ the same parameters as those used by *runtests.py*

explore

:   This command allows the user to *explore* and *validate* a YAML configuration file. 
    It provides a shell like interface in which the user can explore
    the tree defined by the configuration file and print the constraints. 
    It also provides documentation about constraints and parameters via the *show* command.
Information on the format 1 for pseudopotentials.

The format 1 for ABINIT pseudopotentials allows to use pseudopotentials
from the set of LDA pseudotentials for the whole periodic table,
build by DC Allan and A.Khein. 
They have been generated according to the Troullier-Martins technique.
See ~abinit/doc/users/bibliography.html for the corresponding references.

The pspcod=1 psp files are formatted data files
which give potentials and projector functions on a real space
radial grid.

Firstly, the radial grid runs from index 0 to 2000 (2001 points),
with grid points given by the following equation (given as fortran):

      nmax=2000
      do j=0,nmax
       x=dble(j)/dble(nmax)
       r(j)=100.d0*(x+.01d0)**5-1.d-8
      enddo

The psp file consists of a number of header lines followed by the
data on the radial grid.  The header section is as follows:

     title  (single 80 character line)
     zatom, zion, pspdat
     pspcod, pspxc, lmax, lloc, mmax, r2well
     ...then, for l=0 to lmax, the following 2 lines:
      l,e99.0,e99.9,nproj,rcpsp
      rms,ekb1,ekb2,epsatm
     ...finally one more line:
      rchrg,fchrg,qchrg

The data may be located anywhere on the line as long as it is provided
in the order indicated (it is read with free format).
In the case of Si with lmax=2, the header may look like the following 10 lines:

    Si  Fri Oct 08 11:18:59 1993
    14.00000   4.00000    930920                zatom, zion, pspdat
      1    1    2    2      2001    .00050      pspcod,pspxc,lmax,lloc,mmax,r2well
      0  19.464  25.000    2   1.8971118        l,e99.0,e99.9,nproj,rcpsp
      .00112760   6.1457108933   4.4765165955  29.74712295   rms,ekb1,ekb2,epsatm
      1  21.459  28.812    2   1.8971118        l,e99.0,e99.9,nproj,rcpsp
      .00119946   3.2090654032   2.0935248528  19.11150542   rms,ekb1,ekb2,epsatm
      2   8.223  21.459    0   1.8971118        l,e99.0,e99.9,nproj,rcpsp
      .00098688    .0000000000    .0000000000  -3.97301006   rms,ekb1,ekb2,epsatm
      1.70000000000000     .22513330685109     .96523597101781   rchrg,fchrg,qchrg

zatom is the atomic number of the atom (14 for Si)
zion is the number of valence electrons (4 for Si)
pspdat is a code revision date (930920 for this case)
pspcod is another index describing the code (1 for this case)
pspxc is an index showing the choice of exchange-correlation (1)
lmax is the highest angular momentum for which a pseudopotential
 is defined, which is also used for the local potential (2)
lloc is the angular momentum used for the local potential (2)
mmax is the number of grid points (2001)
r2well is the prefactor of a harmonic well sometimes used to bind
 electrons which would otherwise be unbound in lda (.00050)
l is the angular momentum (0, 1, or 2 for Si for this case)
e99.0 is the planewave cutoff needed to converge to within 99.0% of the
 kinetic energy of the atom (various numbers for various l)
e99.9 is the planewave cutoff needed to converge to within 99.9% of the
 kinetic energy of the atom (various numbers for various l)
nproj is the number of projection functions used for each l (2)
rcpsp is the pseudopotential core radius
rms is a measure of pseudopotential quality reflecting the value of the
 penalty function in designing the potential
ekb1, ekb2 are the Kleinman-Bylander energies for each projection
 function for each l
epsatm is the integral Int[0 to Inf] (4*Pi*r*(r*V(r)+Zion))
rchrg is the core charge radius for additional core charge used to
 match the xc contribution to the hardness matrix
fchrg is the prefactor of the core charge expression
qchrg is the total (integrated) core charge

Following the header are, for l=0 to lmax, the pseudopotentials
in the form of a title line followed by 667 lines of data, each line
containing three numbers so that the radial grid values from 0 to 2000
are given.  The title line gives the value of l first followed by some
text.  For the case of Si, e.g., for l=0, this line is

      0 =l for Teter pseudopotential

followed by the 667 lines, 3 numbers each, giving the l=0
potential on the radial grid described above.

Following the pseudopotentials are the first projection functions,
again given for each l with a title line followed by 667 data lines.

Following the first projection functions for each l are the second
projection functions, if any (determined by nproj).

Following the second projection functions, if any, are several lines
of additional data which is not read by plane_wave but is provided
to describe more about the details of the construction of the
pseudopotential.  Omission of these lines does not affect the running
of plane_wave (at this time).

If you do not wish to use core charges, simply set fchrg to 0 and
use rchrg=1, qchrg=0.

If you wish to make a local potential, use lmax=lloc=0, nproj=0, and
you need not provide any projection function(s) (at this time).
The values of rms, ekb1, ekb2, epsatm, e99.0, e99.9 are used only
for information (at this time) so may be set to 0 when creating
a pseudopotential file.  rcpsp is still used as the definition of the
pseudopotential core radius so it must be provided.


To best understand this data structure, it is recommended to study
several pseudopotential files and compare their contents with the
description given here.


Inside ABINIT, a pseudopotential with format 1 will be treated by
the routine psp1in.f, that calls psp1lo.f (local part),
psp1nl.f (non-local part), and psp1cc.f (XC core correction).

As a matter of numerical accuracy, note that the integral
of (V(r)+Zion/r) r^2 in psp1lo.f is performed from 0 to the highest
allowed radius (usually about 100 a.u.), without cut-off.
V(r)+Zion/r should tend rapidly to zero
for large radii (beyond 5 a.u.), but this correct behaviour will
not be enforced by the routine. If the tail of V(r) is inaccurate
(i.e. if the pseudopotential is in single precision), there
will be large inaccuracies in the integral, because of the r^2 factor.
---
plotly: true
---

<div class="plotly-graph-div" id="abinit_stats_plot" style="width:90%;height:750px;"></div>

<script>
$(function() {
    Plotly.d3.json("../statistics.json", function(stats) {
        var mode = "lines+markers";
        var x = stats.dates;
        var trace1 = {x: x, y: stats.num_f90lines, mode: mode, name: "Number of F90 lines"};
        var trace2 = {x: x, y: stats.num_f90files, mode: mode, name: "Number of F90 files", yaxis: 'y2'};
        var trace3 = {x: x, y: stats.num_tests, mode: mode, name: "Number of tests", yaxis: 'y3'};
        var trace4 = {x: x, y: stats.targz_sizes, mode: mode, name: "Tarball size [Mb]",  yaxis: 'y4'};
        var data = [trace1, trace2, trace3, trace4];

        var layout = {
          //title: "Date released: date mentioned in the release notes",
          //xaxis: {tickvals: stats.date, ticktex: stats.version},
          legend: {traceorder: 'reversed'},
          yaxis: {domain: [0, 0.25]},
          yaxis2: {domain: [0.25, 0.5]},
          yaxis3: {domain: [0.5, 0.75]},
          yaxis4: {domain: [0.75, 1.0]}
        };

        Plotly.newPlot(document.getElementById('abinit_stats_plot'), data, layout, {showLink: false});
    });
});
</script>
---
authors: MG, XG
---

This page describes the details of the documentation system of Abinit and how to contribute to it.

Most of the documentation is written in [Markdown](https://en.wikipedia.org/wiki/Markdown)
a lightweight markup language with plain text
[formatting syntax](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet).
The documentation includes the User Guide, the Abinit tutorial, the topics, the release notes
as well as the pages with the [input variables](/variables/) and the [bibliographic references](/theory/bibliography)
that are generated *automatically* in python from the information reported in
`~abinit/mkdocs/variables_abinit.py` (and similar files in the same directory for other main executables) and the bibtex
entries given in the `~abinit/doc/abiref.bib` file.

The website is automatically generated with [MkDocs](http://www.mkdocs.org/)
a static site generator geared towards project documentation.
MkDocs employs [Python-Markdown](https://pypi.python.org/pypi/Markdown) to parse the Markdown documentation
and use a single [YAML](http://en.wikipedia.org/wiki/YAML) configuration file (`mkdocs.yml`)
defining the organization of the pages on the website.
Navigation bars, header and footer are generated *automatically* by the framework
using the [jinja template engine](http://jinja.pocoo.org/).

MkDocs includes a couple built-in themes as well as various third party themes,
all of which can easily be customized with extra CSS or JavaScript or overridden from the theme directory.
The Abinit website uses [Mkdocs-Material](http://squidfunk.github.io/mkdocs-material/), a theme
built using Google's [Material Design](https://www.google.com/design/spec/material-design) guidelines.
<!--
We also use [fontawesome icons](https://fontawesome.com/) and
-->

Note that the majority of the Abinit developers do not need to know how to use these technologies
since they will mainly interact with markdown files (plain text files that can be easily modified in the editor)
while Mkdocs and mkdocs-material will handle the HTML/CSS/Javascript part.

In addition to the basic markdown syntax, the Abinit documentation supports extensions and shortcuts
to ease the writing of hyperlinks and the inclusion of bibliographic citations.
A detailed description of *our markdown dialect* is given in [our markdown page](/developers/markdown).
Also [MathJax](https://www.mathjax.org/) for equations in LaTeX is activated,
and the (few) specificities of its usage in the Abinit docs are explained [in this section](/developers/markdown#mathjax).

As a net result, Abinit developers can write nice-looking documentation and release notes without having to use
HTML explicitly while working in an environment that is well-integrated with the Abinit ecosystem
(the yaml database of input variables, the test suite, bibtex citations).
Adding new content is straightforward: write a new page in Markdown, add the new entry to `mkdocs.yml`
and finally regenerate the website with MkDocs.


## Getting started

Make sure you are in the top-level ABINIT directory.
To install the python packages required to build the website use:

```sh
pip install -r requirements.txt --user
```

if the packages are not already installed.

If you already have a pre-existent installation and you need to upgrade to more recent versions
add the *-upgrade* option (-U for short):

```sh
pip install -r requirements.txt --user -U
```

This step may be needed if the version of mkdocs and MkDocs are updated upstream in trunk/develop
(check the version listed in requirements.txt).

!!! note

    Python >= 3.6 is required.
    The entire documentation supports Unicode so feel free to use unicode symbols in the docs.

MkDocs comes with a built-in dev-server that lets you preview your documentation as you work on it. 
First issue \*/\*/makemake (actually config/scripts/makemake) in the top ABINIT directory. Then start *our customized* server 
by running the `./mksite.py serve` command:

```sh
./mksite.py serve

Regenerating database...
Saving database to /Users/gmatteo/git_repos/abidocs/doc/tests/test_suite.cpkl
Initial website generation completed in 9.17 [s]
Generating markdown files with input variables of code: `abinit`...
...
...
INFO    -  Building documentation...
INFO    -  Cleaning site directory
[I 170826 03:37:05 server:283] Serving on http://127.0.0.1:8000
[I 170826 03:37:05 handlers:60] Start watching changes
[I 170826 03:37:05 handlers:62] Start detecting changes
```

Open up `http://127.0.0.1:8000/` in your browser, and you'll see the default home page being displayed.
Note that the generation of the website takes 1-2 minutes but this is a price that must be paid only once.
The web server, indeed, reloads automatically the source files that are modified by the user
so that one can easily change the documentation and inspect the changes in the corresponding HTML files.

!!! tip
    Use `./mksite.py serve --dirtyreload` to enable the live reloading in the development server,
    but only re-build files that have changed.
    This option is designed for site development purposes and is **much faster** than the default live reloading.

!!! warning
    The server re-builds automatically the pages generated from changed `.md` files,
    but not the ones from changed `~abinit/doc/abiref.bib`
    neither from changed `~abinit/abimkdocs/\*.py` . This means that the upgrade of the description of an input variable
    or a bibtex reference is done by closing the server and reissuing the `./mksite.py` command.
    Also, the case of the `.md` files in the `~abinit/doc/topics` directory is similar, as the `.md` source files,
    prepended with an underscore, must be **preprocessed** by `./mksite.py` to deliver the `.md` files, without underscore,
    that are live reloaded.

`./mksite serve` builds the website in a temporary directory. 
If you need to inspect the HTML files produced by the script, use:

    ./mksite.py build

The HTML pages will be available in the `site` directory.

<!--
It is also possible to validate all the HTML documents in `site` by using:

    ./mksite.py validate

This command will connect to the [W3C validator service](https://validator.w3.org/) and possible
errors are printed to the terminal.
To validate a given list of HTML files use:

    ./mksite.py validate site/index.html site/variables/index.html

At present (v8.7.7), many html files are not compliant with the strict html syntax, so this procedure is ==not yet operational==.
-->

Note that the HTML files are produced in a temporary directory, thus they **are not under revision control**.
The real source is represented by the `.md` files and the other `.yml` files. 
These are the files that can be changed by the developers and are therefore known to git.
The Markdown pages generated by `./mksite.py` are automatically listed in `~abinit/doc/.gitignore`
and are thus ignored by git.

The `~abinit/doc/mksite.py` script generates the website by converting markdown files into HTML.
The script:

* Starts by creating python objects using the information reported in
    - the python files in abimkdocs with the input variables,
    - the `~abinit/doc/abiref.bib` for the list of Bibliographic references,
    - the input files contained in `~abinit/tests/*/Input`.
* Performs initial consistency checks.
* Generate the markdown files for variables, citations, etc.
* Invoke `mkdocs` to parse the markdown files declared in `mkdocs.yml`
* Expands special strings, of the form <span style="background-color: #E0E0E0;font-size:90%;"> &#91; [namespace:name#section|text] &#93; </span> to create HTML links.
* Creates the needed HMTL files

The expansion of special strings is documented in the [links section](markdown.md#links).
It can be used in all the YAML files mentioned below.
For equations/formulas, [Mathjax](http://docs.mathjax.org/en/latest/mathjax.html) is activated, and allows
to process and visualize LaTeX formulas, see also [this section](markdown.md#MathJax) for further details.


## Writing docs

The markdown files are stored inside the `doc` directory according to the following structure:

```console
├── doc
│   ├── about
│   ├── css
│   ├── developers
│   ├── extra_javascript
│   ├── images
│   ├── variables
│   ├── tests
│   ├── theory
│   ├── topics
│   ├── tutorial
│   └── guide
```

* about: Files with release notes, license
* *css*: Extra CSS files used by the website
* developers: Documentation for developers (documentation howtos, git, coding rules...)
* *extra_javascript*: Extra javascript code used by the website
* *images*: logos and favicon
* variables: files with input variables (automatically generated).
* *tests*: symbolic links to the `~abinit/tests` directory.
* theory: files with theoretical notes
* topics: files with Abinit topics
* tutorial: official Abinit tutorials
* guide: help files for main executables

The directory in *italic* are mainly used to build the website and are not visible outside.
The other directories contain markdown files, each directory is associated to an
entry in the website menu (see `pages` in `mkdocs.yml`).
The [pages configuration](http://www.mkdocs.org/user-guide/writing-your-docs/) in `mkdocs.yml`
defines which pages are built by MkDocs and how they appear in the documentation navigation.

Each directory contains an `index.md` file that is supposed to be a "general" page with an overview
of the topics treated in that directory.
Some of these `index.md` files are automatically generated by python (e.g. `variables/index.md`)
but others such as `tutorial/index.md` are not.
So make sure that new documentation pages are properly mentioned and linked in the corresponding
`index.md` file when a new page is added.

Images and additional material (e.g. scripts) associated to a markdown page are stored
in the corresponding **"assets" directory** whose name is constructed from the base name of the markdown page.
For instance the figures used in `tutorial/bse.md` are stored in `tutorial/bse_assets`

### Front matter

Front matter is the first section of the markdown file and must take the form of valid YAML
document enclosed between triple-dashed lines. Here is a basic example:

```yaml
---
title: Documenting Code Like a Hacker
authors: MG
---
```

Between these triple-dashed lines, you can set predefined variables (see below for a reference)
or even create custom ones of your own.
These variables will then be made available to the framework.
For instance, the list of authors is reported in the HTML page footer while `title` is added to
to the HTML meta section.


## Documentation Guide lines

* Each paragraph name should be short enough to fit nicely in the menu, but also long enough to stand
  on its own to a reasonable extent.
  The titles set here are used in the navigation menu and the page title that displays in the browser tab.

* Each page should start with a paragraph that explains what will be covered.
  The first heading in a page should be Heading1 (`#` in Markdown).
  All others should be in H2  (`##`) and H3 (`###`), only where necessary.
  If you find yourself wanting to use H4, consider if it's truly necessary.

* Don't use terms like "previous page", etc. because we may add or re-arrange pages in the future.
  Instead, use a hyperlink to the chapter.
  Also avoid sentences like *If you follow the tutorial, you should go back to the tutorial window now.*

* Number the paragraphs only if really needed: the links in the navigation bar are not readable and besides
  the number will appear in the [permalink](/developers/markdown#permalinks).
  This means that you may need to change several links if you decide to add a new section to the page later on.
  Users may want to share or bookmarks links to the Abinit documentation so broken links should be avoided
  as much as possible.

* The fact the [wikilink syntax](/developers/markdown#wikilinks) facilitates the inclusion of hyperlinks does not
  mean that we have to add links *everywhere*.
  This is especially true in the documentation of the input variables in which it does not make sense to
  put links to the same variable we are describing or the same link over and over again in the same paragraph.

* Text in all uppercase is significantly more difficult to read than lower and mixed case text.
  Writing in all caps is like shouting so use all caps sparingly.


## How to add/modify an input variable

The variables for a given executable are declared in `~abinit/abimkdocs/variables_CODENAME.py`.
This file consists of a list of python dictionaries, each dictionary
contains the declaration of a single variable and the associated documentation in markdown format.
Wikilinks, latex and markdown extensions can be used inside `text`.
Adding a new variable is easy. Edit the python module and add a new item at the end of the list.
A template is provided.

Remember that `\` is an escaping character in python so the interpreter may
raise an Exception if you start to add Latex equations in the documentation e.g.

```python
Error: '\alpha' is an unrecognized escape in character string starting ""^\alpha"
```

The solution is simple. Declare the string as `raw string` by prepending `r` e.g.:

```python
    text=r"""
The [[spmeth]] input variable defines the method used to calculate the
irreducible polarizability $\chi^{(0)}_{KS}$.
"""
```

Note that input variables for the executables other than the main abinit (e.g. anaddb, aim, optic) are
denoted `input_variable_name@executable`, e.g. `dipdip@anaddb`
(this allows to waive the ambiguity with the e.g. dipdip input variable used in the main abinit).

After having edited the python modules you **must rerun** `./mksite serve` to see the changes.

!!! important

    Use ```pytest abimkdocs_tests/test_variables.py``` to validate your changes
    before rebuilding the documentation.

    Well, at present (v8.7.7) this script detect too many problems for this procedure to be useful. 
    So this is (==not yet operational==).


## How to add a bibliographic reference

Bibliographic references must be in bibtex format and should provide enough information so that the python code
can generate appropriate links in the website.
The central bibliography database is presently located in `~abinit/doc/abiref.bib`.

For published work with a DOI, we strongly recommend *avoiding* a *cut&paste* from your own bibtex file
to the central bibliography database.
Indeed, there are units tests to enforce the presence of particular entries in the bibtex document and
your bibtex may not fulfill these requirements.

Providing bibtex data from the publisher site is a better method.
If you know the DOI of the article, it is also possible to use [BetterBib](https://github.com/nschloe/betterbib)
to fetch data from [Crossref](http://www.crossref.org/) and produce the bibtex entry.
BetterBib is available from the Python Package Index, so simply type:

    pip install betterbib

and then use `doi2bibtex` from the command line:

```text
betterbib-doi2bibtex 10.1103/PhysRevLett.96.066402

@article{bibtex,
  author = {Amadon, B. and Biermann, S. and Georges, A. and Aryasetiawan, F.},
  doi = {10.1103/physrevlett.96.066402},
  issn = {0031-9007, 1079-7114},
  journal = {Physical Review Letters},
  month = feb,
  number = {6},
  publisher = {American Physical Society (APS)},
  source = {Crossref},
  title = {{The α−γ Transition} of Cerium Is Entropy Driven},
  url = {http://dx.doi.org/10.1103/physrevlett.96.066402},
  volume = {96},
  year = {2006}
}
```

Add the entry to the bibtex file and use the `FirstAuthorYear` convention for the key
(make sure it's not a duplicated entry).
Note that the bibtex ID must be of the form "FirstauthornameYEAR", e.g. "Amadon2008"
(start with an uppercase letter, then lower case, then four-digit year).
Possibly, a letter might be added in case of ambiguity: e.g. there exists also `Amadon2008a`
Then, build the HTML pages using `./mksite.py serve`.

Run the tests with:

    pytest abimkdocs_tests/test_bibtex.py

with pytest to validate your changes.

In order to refer to a bibliography entry, use the [Wikilink syntax](/developers/markdown#wikilinks) with the "cite" namespace.

## Topics

The topic files are written in Markdown and can be found in ~abinit/doc/topics.
The source files start with an underscore e.g. `_AbiPy.md`.
These are **template files** containing the text and two variables:

```
## Related Input Variables

{{ related_variables }}

## Selected Input Files

{{ selected_input_files }}
```

that will be filled by `./mksite.py` by inspecting the database of variables and the tests of the test suite..
A new Markdown file **without underscore** will be generated and included in `mkdocs.yml`.

!!! important

    Developers are supposed to edit the version with the underscore and provide enough
    information in the declaration of the variable and in the `TEST_INFO` section
    so that `./mksite.py` can fill the template.
    Remember to restart `./mksite.py` to see the changes.

## How to a add a new document

In order to add a new tutorial, create a new Markdown file in doc/tutorial and
register it in `mkdocs.yml`
Then, build the HTML using `./mksite.py serve` and start to enjoy the Markdown syntax.
The organization of help files and theory documents is very similar to the one for the other tutorials.

### Topics and relevances

Since the beginning of the ABINIT HTML documentation, every input variable
has been required to belong to a **varset** (set of variables, e.g. `varbas`, `varfil`).
However, starting in Summer 2017, we require every input variable to be also mentioned in at least one of the
documentation **topics** and, for such topic, to be characterized by a **relevance**.

The allowed list of relevancies (a generic list, irrespective of the topic) is declared in
`~abinit/abimkdocs/variables.py`.
Standard names are:

- *compulsory* (when such input variable **must** be present in the input file when the "feature" of the topic is activated)
- *basic* (when such input variable is usually explicitly specified in the standard usage, although the default might be adequate)
- *useful* (when the default value is used most of the time)
- *expert* (when only expert users should use other values than the default)

Other relevance names have been allowed for specific topics, in which such a classification
(compulsory/basic/useful/expert) is not a relevant one.

In order to specify the (possibly several) combinations of topic+relevance to which an input variable is attached,
the field "topics" is used inside the `~abinit/abimkdocs/variables_abinit.py` file
(and similar files in the same directory for the other executables).

Some examples:

* for dmatpawu: *DFT+U_useful*
* for mdwall: *MolecularDynamics_expert*
* for gwpara: *parallelism_useful, GW_basic*

The latter is a case where one input variable is associated to two topics, with a different relevance
for topic "parallelism" and topic "GW".


## Release Notes

Release notes are written in Markdown so it is possible to use the [wikilink syntax](/developers/markdown#wikilinks)
to insert links to new tests, new autoconf files and even links to pull-requests and issues that will redirect
the reader to the Abinit repository on github.
For example, the following markdown text

```md
B.1
Implementation of algorithms to interpolate the electronic band structure.
See the new input variables [[einterp]], [[nkpath]], and [[prtebands]],
and the new tests [[test:v8_04]], [[test:libxc_41]].
Added in [[gitsha:f74dba1ed8346ca586dc95fd10fe4b8ced108d5e]]

B.2
Added subsuite syntax [[test:gspw_01]]

C.2
New versions of Fortran compilers have been integrated in the test farm:

- intel 16.0
- gnu 6.1 and 6.2
- IBM xlf compiler 14.1
- NAG 7.0

Corresponding examples are available in [[ac:abiref_nag_7.0_openmpi.ac]]
```

produces a nice report with links to the features available in the new version:


B.1
Implementation of algorithms to interpolate the electronic band structure.
See the new input variables [[einterp]], [[nkpath]], and [[prtebands]],
and the new tests [[test:v8_04]], [[test:libxc_41]].
Added in [[gitsha:f74dba1ed8346ca586dc95fd10fe4b8ced108d5e]].

B.2
Added subsuite syntax [[test:gspw_01]]

C.2
New versions of Fortran compilers have been integrated in the test farm:

- intel 16.0
- gnu 6.1 and 6.2
- IBM xlf compiler 14.1
- NAG 7.0

Corresponding examples are available in [[ac:abiref_nag_7.0_openmpi.ac]].

!!! important
    We are already using Markdown on gitlab to document our merge requests.
    This means that we can easily integrate all this gitlab documentation
    with the release notes published on the website.


## Variable object

It is the type that contains the other fields.

abivarname
: The name of the variable. Note that the name for input variables
  of the executables anaddb, aim and optic is always finished with @anaddb, @aim or @optic.

characteristics
: Possibly, a specific characteristics of the input variable.
  To be chosen among the names in `~abinit/doc/input_variables/origin_files/characteristics.yml`.

commentdefault
: Possibly, some comment about a default value.

commentdims
: Possibly, some comment about the dimension of an array.

defaultval
: Must be an integer or real value, possibly specified using the types presented below (e.g. !multiplevalue)

dimensions
: Either scalar or a list of dimensions, using YML syntax.

excludes
: Possible excluded values

mnemonics
: A longer description of the variable role, in a few words

requires
: The input variable is relevant only if this condition is fulfilled

text
: Free text describing the input variable

topics
: A string, specified in [topics_and_relevances](#topics-and-relevances)

varset
: a unique "set of variables" to which the variable belong.
  To be chosen among the names in `~abinit/doc/input_variables/origin_files/varsets.yml`.

vartype
: to be chosen among integer, real or string
  If there is no information of a type for a specific variable, its value must be "null".


### MultipleValue object

This is the equivalent to the X * Y syntax in the Abinit parser.

<code>
  X * Y
</code>

will become

```yaml
  !multiplevalue
    number : X
    value : Y
```

If X is null, it means that you want to do *Y (all Y)


### Range object

```yaml
  !range
     start: 1
     stop: N
```

As a default value, it means that the default value is 1, 2, ... N


### ValueWithConditions object

This type allows to specify conditions on values:

```yaml
!valuewithconditions
    defaultval: -[[diemix]]
    '70 < [[iprcel]] and [[iprcel]] < 80': '[[diemix]]'
    '[[iscf]]<10': '[[diemix]]'
    '[[iprcel]]==0': '[[diemix]]'
```

defaultval is the default value if no condition is fulfilled.
As condition, please use strings with the most basic expressions,
containing <, < =, >, >=, ==, !=, +, -, *, /, etc to allow for further simple parsing !

As a convention, we use "pythonic" way for expressions, so you can use "or", "and" and "in"
also as <span style="background-color: #E0E0E0;font-size:90%;"> &#91; [varname] &#93; in [1,2,5]</span> for example ...


### ValueWithUnit object

This type allows to specify values with units:

```yaml
!valuewithunit
    units: eV
    value: 100.0
```

means "100 eV".


### Constraints between variables

In the YML file (and via the GUI), there are some constraints between variables that have been introduced.
You can specify "requires: CONDITION" and "excludes: CONDITION" in the YML file
(or fill the fields requires and excludes in the GUI).

If a varname has "requires: CONDITION", it means that the variable is only relevant when CONDITION is fulfilled.
If a varname has as "excludes: CONDITION", it means that the specification of the variable in the input file forbids
the CONDITION to be fulfilled.

Pay attention to strings. If it is recognized as string directly, you don't need ticks (' ').
Otherwise, you need to put ticks.
For example, if you want to use a link as a value, use a link shortcut like <span style="background-color: #E0E0E0;font-size:90%;"> &#91; [abivarname] &#93; </span>.
See the doc about link shortcuts at [links shortcuts](markdown.md#links).
ABINIT developers and maintainers scripts
=========================================

Contains several scripts to ease development.


- bzr_helper
    Various script to facilitate the use of bzr.
    See the documentation within.

- mkroutine.sh
    Make a new F90 routine, with the correct robodoc header,
    and  structure that follows ABINIT rules.

- mkmodule.sh
    Make a new F90 module.


- FT77to90 and fixed_to_free
    `FT77to90` is a perl script that is able to translate a file written
    in Fortran77 fixed format to Fortran90 free format. What it does
    is relatively well explained. The csh script `fixed_to_free`
    is the driver of `FT77to90`, and slightly change its output.

- abirules.pl
    The source of the abirules script. The latter is able to enforce 
    automatically some of the ABINIT rules in F90 files.
 
- change
    A bash script, to change automatically some expression into another
    is a whole set of files, while making back-up copies of the old version.

- parents
    Locate all parents and children of a F90 routines, and write
    them in the routine.

- var-file-index.py
    Build the file `Infos/varfileindex.html` which refers all the
    input variables inside the input files of tests.
Notes for the split of the source tree
======================================


Relevance
---------

The source tree of ABINIT has gone from a monolithic structure to a set of 3
autonomous blocks: ABINIT Common, LibPAW, and ABINIT Core. The first 2 blocks
represent the shared part of ABINIT, while the latter contains much more
specific code and is tightly bound to the identity of the package.

Knowing exactly how the use of CPP options is distributed within the source
tree of ABINIT is essential to determine the elements on which each build
system will have to focus. For instance, if something is only used in
low-level routines of ABINIT Common, including it in the build system of
ABINIT Core will be useless, and vice-versa. The same holds for
LibPAW-specific features.

The knowledge extracted from the CPP options will actually define the options
of each configure script and the relationships between the build systems,
block by block. It will also help identify a set of small refactoring
operations with the highest impact on the simplicity and velocity of each
build system. Last but not least, it will pinpoint obsolete designs and make
easier the elimination of dead code.

To give a concrete example, since most of the CPP options related to the
Fortran standard version supported by the compiler are used in ABINIT Common,
it is worth taking a few design decisions in order not to repeat the detection
tests within ABINIT Core, thus saving a significant amount of time when
running the configure scripts, one after the other.

Another motivation to simplify the design of build systems adds up in the case
of ABINIT Common and LibPAW: they will be used by more than one DFT code.
Since they will export Fortran modules -- the least standard software
component that has ever existed -- it is of utmost importance to limit the
number of variants produced during the build of each of these blocks.


Accessing information at different levels
-----------------------------------------

One simple strategy to avoid repeating time-consuming detection tests is to
provide a channel that passes information from one block to the others. ABINIT
would benefit from one of the 3 following implementations:

- define a common space where each block would drop the required information
  to be used by the others, e.g. */INSTALL_PREFIX/share/org.abinit/*;
- have each block produce a config file in its source tree / build tree that
  would be included by the others;
- have ABINIT Common install an executable script, e.g. *abinit-config*, that
  the other blocks would call to retrieve essential information and ensure
  consistency at each build step.

Each of these implementations has pros and cons that should be discussed among
the core developers of ABINIT before selecting the most adequate one.


Classifying CPP options
-----------------------

The Python scripts found in the *split/* subdirectory of the *pouillon*
repository of ABINIT provide both the locations of the CPP options in the
source code and some statistics about them. They extract the following
information relevant to the design of build systems:

- how much each CPP option is used in the source code;
- which CPP options are used in each block;
- which CPP options are exclusively used in one of the blocks;
- which CPP options are used in one block plus only once in another block;
- which CPP options are used in one block plus only twice in another block;
- which CPP are used everywhere and should be treated globally.

CPP options only found in one block are fully aligned with the split of the
source tree and should appear in one of the build systems only. Those which
are mostly used in one block only, plus once or twice in another block, are
the best candidates to explore the impact of refactoring operations on the
design of the individual build systems and their mutual relationships. Those
appearing everywhere are those which will define how the build systems will
communicate.


CPP options aligned with the split
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Only used in ABINIT Common:

- ABINIT_SINGLE
- CUDA_COMMON_H
- DEV_LINALG_TIMING
- FC_GNU
- FC_PGI
- GPU_FOUR_HEADER_H
- HAVE_CCLOCK
- HAVE_ELPA_FORTRAN2008
- HAVE_ERRNO_H
- HAVE_FC_ASYNC
- HAVE_FC_BACKTRACE
- HAVE_FC_CONTIGUOUS
- HAVE_FC_CPUTIME
- HAVE_FC_IEEE_ARITHMETIC
- HAVE_FC_IEEE_EXCEPTIONS
- HAVE_FC_IOMSG
- HAVE_FC_ISO_FORTRAN_2008
- HAVE_FC_MACRO_NEWLINE
- HAVE_FC_PRIVATE
- HAVE_FC_PROTECTED
- HAVE_IBM6
- HAVE_INTTYPES_H
- HAVE_LIBTETRA_ABINIT
- HAVE_LINALG_ASL
- HAVE_LINALG_ELPA_2013
- HAVE_LINALG_ELPA_2014
- HAVE_LINALG_ELPA_2015
- HAVE_LINALG_ELPA_2016
- HAVE_LINALG_ELPA_2017
- HAVE_LINALG_ESSL
- HAVE_LINALG_MAGMA_15
- HAVE_LINALG_MKL_IMATCOPY
- HAVE_LINALG_MKL_OMATCOPY
- HAVE_LINALG_PLASMA
- HAVE_MALLOC_H
- HAVE_MATH_H
- HAVE_MPI_TYPE_CREATE_STRUCT
- HAVE_OMP_COLLAPSE
- HAVE_OS_MACOSX
- HAVE_PAPI
- HAVE_STDARG_H
- HAVE_STDDEF_H
- HAVE_STDINT_H
- HAVE_STDIO_H
- HAVE_STDLIB_H
- HAVE_STRING_H
- HAVE_SYS_STAT_H
- HAVE_SYS_TYPES_H
- HAVE_UNISTD_H
- HAVE_VARMACROS
- _ABINIT_CLIB_H
- _ABINIT_COMMON_H
- _CRAY
- _MD5_H
- _XMALLOC_H_
- __GNUC_MINOR__
- __GNUC__
- __STDC_VERSION__
- __STDC__
- __cplusplus

Only used in LibPAW:

- HAVE_AVX_SAFE_MODE
- HAVE_FOX
- HAVE_LIBPAW
- HAVE_LIBPAW_ABINIT
- HAVE_MPI2_INPLACE
- HAVE_YAML
- LIBPAW_HAVE_FOX
- LIBPAW_HAVE_LIBXC
- LIBPAW_HAVE_NETCDF
- LIBPAW_ISO_C_BINDING

Only used in core:

- CUDA_HEADER_H
- CUDA_REC_HEAD_H
- DEBUG_VERBOSE
- DEV_DEBUG_THIS
- DEV_MG_DEBUG_MODE
- DEV_MG_DEBUG_THIS
- DEV_NEW_CODE
- DEV_NEW_HDR
- DEV_RC_BUG
- DEV_USESPLINE
- DEV_YP_DEBUG_PSP
- DEV_YP_VDWXC
- FFT_PRECISION
- HAVE_BIGDFT
- HAVE_BSE_UNPACKED
- HAVE_DFTI
- HAVE_DFTI_MIXED_PRECISION
- HAVE_FC_COMMAND_ARGUMENT
- HAVE_FFTW3
- HAVE_FFTW3_MPI
- HAVE_FFTW3_THREADS
- HAVE_GPU_CUDA_SP
- HAVE_GPU_CUDA_TM
- HAVE_LEVMAR
- HAVE_LIBPSML
- HAVE_LINALG_MKL_THREADS
- HAVE_LOTF
- HAVE_MPI_IALLTOALL
- HAVE_MPI_IBCAST
- HAVE_MPI_IO_DEFAULT
- HAVE_TEST_TIME_PARTITIONING
- HAVE_TRIQS
- HAVE_TRIQS_v1_4
- HAVE_TRIQS_v2_0
- HAVE_WANNIER90
- HAVE_WANNIER90_V1
- HAVE_XML
- M_AB7_KPOINTS_EXPORT_H
- M_AB7_SYMMETRY_EXPORT_H
- READ_FROM_FILE
- _ABINIT_XC_VDW_H

As can be seen, most of the CPP options related to the capabilities of the
Fortran compiler, as well as the interface between Fortran and other languages
like C and C++, are found in the ABINIT Common block. Most of the intricacies
of linear algebra are also found there. These topics will thus be major focus
areas for the build system of ABINIT Common.

The LibPAW block has few CPP options of its own. A finer analysis that
includes information about the history of ABINIT -- that we will not detail
here -- also shows that most of them are now outdated, such as *HAVE_FOX* or
*HAVE_YAML*. Regarding Fortran standards, *LIBPAW_ISO_C_BINDING* could easily
be removed. LibPAW is also the only place in ABINIT where *HAVE_MPI2_INPLACE*
is used. The implementation of the build system of LibPAW will take place in
parallel with a refactoring of its source code.

ABINIT Core is where most of the optional features are managed, as well as
where the different levels of parallelism are interacting. CPP options related
to FFT are of particular importance at this level. All those associated to
experimental developments are found there too. What this tells us *a
posteriori* is that the split of ABINIT into ABINIT Common, LibPAW and ABINIT
Core is actually consistent with the contents of the source code. It is
definitely the core build system of ABINIT that will manage most of the
external dependencies and related aspects like the fallbacks.


Best candidates for a refactoring
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

*14_hidewrite/m_cppopts_dumper.F90* could be moved to the upper part of ABINIT
if it were not used by m_errors. Exclusively because of this module, a lot of
detection code that is only useful for ABINIT Core would have to be duplicated
into ABINIT Common.

CPP options which are used almost uniquely in one block and only once in
another block point to a few easy refactoring operations:

- by stopping to use CPP options related to legacy Fortran standards (Fortran
  90/95) and assuming that Fortran compilers support Fortran 2003, the build
  systems of ABINIT Common and LibPAW could be simplified substantially;
- getting rid of the *HAVE_ETSF_IO* CPP option, which has been replaced by a
  direct implementation of the ETSF File Format;
- moving linear algebra-related Fortran modules from ABINIT Common to ABINIT
  Core, after confirming that LibPAW does not require them.

In a second round, the following CPP options could be used exclusively in
ABINIT Core:

- HAVE_GPU_xxx, if the GPU-related modules and C headers, as well as
  m_abi_linalg are moved to ABINIT Core;
- HAVE_LEVMAR, if a low-level C subdirectory is created upwards (level 40);
- HAVE_MEM_PROFILING, if moved away from m_errors (abinit_doctor) and
  abi_common.h;
- HAVE_NETCDF_DEFAULT, if moved away from m_nctk or the latter moved to ABINIT
  Core;
- HAVE_NETCDF_MPI, if m_nctk is moved to ABINIT Core.

The *HAVE_GW_DPC* option could be used exclusively in ABINIT Core if:

- it is defined somewhere else than *10_defs/defs_basis.F90*;
- *28_numeric_noabirule/m_array.F90* is moved upwards.

All these possible refactoring operations only involve small efforts, which is
why they should be discussed among the core developers of ABINIT before
starting ABINIT 9.


Proposed schedule
-----------------

#. Polish the test farm configuration. (YP+JMB, in progress)
#. Make the new build-system interface more user-friendly. (YP+JMB, in
   progress)
#. Move transient/ to  fallbacks/. (YP)
#. Use a fallbacks tarball. (YP)
#. Install fallbacks within ABINIT build dir by default, with FC vendor and
   version. (YP)
#. Make configure use fallbacks automatically. (YP)
#. Write a proper warning for LibPSML (no dynlibs, design flaw). (YP)

Internal ref: YP/2019/Q4/57+77

ElectronPhononCoupling
======================

ElectronPhononCoupling (EPC) is a python module
to analyze electron-phonon related quantities computed with Abinit.


Istallation
-----------

Issue

    >$ python setup.py install

Requires

    * numpy >= 1.8.1
    * mpi4py >= 2.0.0
    * netCDF4 >= 1.2.1

Building the netCDF4 dependency is sometimes delicate. On certain systems,
it might be necessary to set the CC environment variable to the same compiler
that was used to build python. E.g. CC=gcc

Usage
-----

Example:

    import ElectronPhononCoupling as epc

    epc.compute(
        renormalization=True,
        broadening=True,
        self_energy=True,
        spectral_function=True,
        temperature=True,
        ...


You can run such python script in parallel with, e.g.:

    mpirun -n 4 python myscript.py

Documentation
-------------
 
* For how to use this module, see the Examples directory.

* For the theory pertaining the electronic self-energy
    due to electron-phonon coupling, and temperature dependence
    of electronic structure, see [PRB 92, 085137 (2015)].

* For the advanced user and developer, see the Doc directory.



This directory contains example usages of the module ElectronPhononCoupling.

To run the examples, you must first run the Abinit calculations
in Calculations/01-LiF-dynamical/.

This directory contains documentation for the users and the developpers
(that is, anyone).

