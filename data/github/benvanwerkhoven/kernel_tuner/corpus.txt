chang log notabl chang project document file project adher semant versioninghttpsemverorg unreleas ad new optim strategi dual anneal greedli il order greedi ml greedi ml support constant memori cupi backend remov altern bayesian optim strategi could use directli c wrapper modul specif hardli use ad support pytorch tensor input data type kernel support smem_arg run_kernel support lambda function string dynam share memori size new bayesian optim strategi chang option store kernel_str store_result improv report skip configur ad support lambda function instead list string restrict support lambda function instead list specifi grid divisor support lambda function instead tupl specifi problem_s function store top tune result function creat header file devic target store result support use tune result pythonkernel option control measur use observ support nvml tunabl paramet option simul autotun search exist cach file cupi backend support c templat cuda kernel support templat cuda kernel use pycuda backend document tunabl paramet vocabulari ad support loop unrol use param start loop_unroll_factor alway insert defin kernel_tun allow preprocessor ifdef kernel_tun support userdefin metric support choos optim start point x strategi chang compact output print termin sequenti runner run first kernel paramet space warm devic updat tutori demonstr use userdefin metric ad kernelbuild function includ kernel python applic smem_arg option dynam alloc share memori cuda kernel chang bugfix nvidia devic without intern current sensor chang fix output check custom verifi function call benchmark return multipl result time sophist implement genet algorithm strategi method option pass use strategy_opt ad bayesian optimizaton strategi use strategybayes_opt support kernel use textur memori cuda support measur energi consumpt cuda kernel option set strategy_opt pass strategi specif option option cach restart tune kernel configur cachefil remov python support may still work longer test python noodl parallel runner chang longer replac kernel name instanc string tune bugfix tempfil creation lead mani open file error ad minim fortran exampl basic fortran support particl swarm optim strategi use strategypso simul anneal strategi use strategysimulated_ann firefli algorithm strategi use strategyfirefly_algorithm genet algorithm strategi use strategygenetic_algorithm chang bugfix c backend byte array argument argument type mismatch throw warn instead except ad wrapper function wrap c function citat file zenodo doi gener releas chang bugfix use iter smaller instal procedur use extra eg cudaopencl option quiet make tune_kernel complet quiet extens updat document ad type check kernel argument answer list check reserv keyword tunabl paramt check whether thread block dimens specifi print unit measur time cuda opencl option print measur execut time chang bugfix instal scipi present bugfix gpu cleanup use noodl runner rework way string handl intern ad option set compil name use c backend chang activ free gpu memori tune bugfix grid use opencl ad support dynam parallel use pycuda option use differenti evolut optim global optim strategi basinhop minim chang option pass fraction sampl runner fix bug memset opencl backend ad parallel tune singl node use noodl runner option pass new default block dimens option pass python function code gener option pass custom function output verif chang devic kernel name print runner tune_kernel also return dict environ info use differ timer c vector add exampl chang chang scalar argument handl intern ad separ instal contribut guid chang allow nontupl problem_s grid chang default grid_div_i none block_size_i convert tutori jupyt notebook cuda backend print devic use similar opencl backend migrat nosetest pytest rewrot mani exampl save result json file ad full support grid includ option grid_div_z separ convolut exampl chang chang output format list dictionari ad option set compil option chang verbos also print debug output correct check fail restructur util function util core restructur code prepar differ strategi shorten output print tune_kernel allow numpi integ specifi problem size ad public roadmap requirementstxt exampl show gpu code unit test kernel tuner support pass list filenam instead kernel string runner take random sampl percent support opencl platform select support use tune paramet name problem size ad function type check argument kernel exampl convolut tune number stream devic interfac c function tune host code correct check kernel tune function run singl kernel instanc changelog file comput cartesian product process restrict main loop python compat code thank berend support constant memori argument cuda kernel use mock unittest report coverag codaci opencl support document page convolut matrix multipli exampl inspect devic properti runtim basic kernel tune function roadmap kernel tuner roadmap present overview featur current plan implement pleas note live document evolv prioriti grow shift version allow strategi tune metric time version multiobject optim version function implement version may alreadi implement earlier version tune kernel parallel set node gpu cluster function includ autotun kernel applic wish list thing would like implement current immedi demand interest let us know provid api analysi tune result tune compil option combin paramet exampl tune kernel use thread block reindex extend fortran support warn data type miss block size paramet etc turn c backend gener compil backend get_parameterized_kernel_sourc function return parameter kernel sourc inspect function gener wrapper kernel directli callingtest devic function notebook part kernel tuner document pageshttpsbenvanwerkhovengithubiokernel_tun materi belong instructorl tutori kernel tuner pleas see separ kernel tuner tutori repositoryhttpsgithubcombenvanwerkhovenkernel_tuner_tutori contribut guid thank consid contribut kernel tuner report issu contribut code creat issu also help us improv creat issu problem pleas ensur follow describ expect happen possibl includ minim exampl help us reproduc issu describ actual happen includ output error print list version python cuda opencl c compil applic contribut code contribut code kernel tuner pleas select issu work creat new issu propos chang addit signific chang requir first creat issu discuss propos chang fork repositori creat branch one per chang addit creat pull request kernel tuner follow googl python style guid sphinxdoc docstr modul public function pleas use pylint check python chang creat pull request pleas ensur follow written unit test test addit unit test pass exampl still work produc better result code compat python newer run pylint check code entri chang addit creat changelogmd match entri roadmapmd updatedremov doubt put addit kernel tuner pleas look design document httpbenvanwerkhovengithubiokernel_tunerdesignhtml__ discuss issu regard addit develop setup instal packag requir run test use codeblock bash pip instal e dev command abl run test build document see e flag instal packag develop mode mean file copi link instal track chang sourc file run test run test use pytest v test toplevel directori note test requir pycuda andor cuda capabl gpu skip installedpres hold test requir pyopencl contribut make kernel tuner break test even run local exampl seen integr test kernel tuner note also use instal packag build document document locat doc directori type make html gener html page docbuildhtml directori sourc file use build document locat docsourc tutori includ tutori directori symlink use add sourc file directori build document updat document page host github gener content docbuildhtml copi toplevel directori ghpage branch instal guid kernel tuner requir sever packag instal first need work python version sever python packag option cuda andor opencl instal explain detail guid python need python instal recommend use python instal miniconda httpscondaiominicondahtml__ linux user could type follow download instal python use miniconda codeblock bash wget httpsrepocontinuumiominicondaminicondalatestlinuxx_sh bash minicondalatestlinuxx_sh cours also free use python instal kernel tuner develop fulli compat python newer instal python packag note use nativ python instal pip command use kernel tuner depend requir sudo right system wide instal sudo right typic requir use miniconda virtual environ could also use eg user prefix option pip instal home directori requir home directori pythonpath environ variabl see detail pip document follow command instal kernel tuner togeth requir depend codeblock bash pip instal kernel_tun also option depend explain cuda pycuda instal cuda pycuda option may want use kernel tuner tune opencl c kernel want use kernel tuner tune cuda kernel first need instal cuda toolkit httpsdevelopernvidiacomcudatoolkit recent version cuda toolkit pycuda python bind cuda recommend older version may work may support featur kernel tuner import instal cuda toolkit tri instal pycuda instal pycuda manual use codeblock bash pip instal pycuda could instal kernel tuner pycuda togeth havent done alreadi codeblock bash pip instal kernel_tunercuda run troubl instal pycuda make sure cuda instal first also make sure python packag numpi alreadi instal eg use pip instal numpi retri pip instal pycuda command may need use nocachedir option ensur pycuda instal realli start continu instal fail fail recommend see pycuda instal guid httpswikitikernetpycudainstal opencl pyopencl instal pyopencl youll need opencl compil sever opencl compil avail depend opencl platform want code run amd app sdk httpdeveloperamdcomtoolsandsdksopenclzoneamdacceleratedparallelprocessingappsdk__ intel opencl httpssoftwareintelcomenusiocl_rt_ref__ cuda toolkit httpsdevelopernvidiacomcudatoolkit__ appl opencl httpsdeveloperapplecomopencl__ beignet httpswwwfreedesktoporgwikisoftwarebeignet__ also look opencl instal guid httpswikitikernetopenclhowto__ pyopencl cuda toolkit recent version one opencl sdk pyopencl recommend support featur kernel tuner youv instal opencl compil choic instal pyopencl use codeblock bash pip instal pyopencl could instal kernel tuner pyopencl togeth havent done alreadi codeblock bash pip instal kernel_tuneropencl fail pleas see pyopencl instal guid httpswikitikernetpyopenclinstal instal kernel tuner also instal git repositori way also get exampl tutori codeblock bash git clone httpsgithubcombenvanwerkhovenkernel_tunergit cd kernel_tun pip instal instal kernel tuner sever option depend full list cuda instal pycuda along kernel_tun opencl instal pycuda along kernel_tun doc instal packag requir build document tutori instal packag requir run tutori dev instal everyth need start develop kernel tuner exampl use pip instal devcudaopencl instal kernel tuner along packag requir develop depend tutori addit python packag requir run tutori packag actual commonli use chanc alreadi instal howev instal kernel tuner along depend run tutori could use codeblock bash pip instal kernel_tunertutorialcuda alreadi instal kernel tuner pycuda use codeblock bash pip instal jupyt matplotlib panda kernel tuner build statu codecov badg pypi badg zenodo badg sonarcloud badg fairsoftwar badg kernel tuner simplifi softwar develop optim autotun gpu program enabl pythonbas unit test gpu code make easi develop script autotun gpu kernel also mean extens chang new depend requir kernel code kernel still compil use normal host program languag kernel tuner provid comprehens solut autotun gpu program support autotun userdefin paramet host devic code support output verif benchmark kernel tune well mani optim strategi speed tune process document full document avail httpbenvanwerkhovengithubiokernel_tunerindexhtml__ instal easiest way instal kernel tuner use pip tune cuda kernel first make sure cuda toolkit httpsdevelopernvidiacomcudatoolkit_ instal type pip instal kernel_tunercuda tune opencl kernel first make sure opencl compil intend opencl platform type pip instal kernel_tuneropencl pip instal kernel_tunercudaopencl inform instal kernel tuner depend found instal guid httpbenvanwerkhovengithubiokernel_tunerinstallhtml__ exampl usag follow show simpl exampl tune cuda kernel code python kernel_str __global__ void vector_addfloat c float float b int n int blockidxx block_size_x threadidxx ci ai bi size numpyrandomrandnsizeastypenumpyfloat b numpyrandomrandnsizeastypenumpyfloat c numpyzeros_likeb n numpyints arg c b n tune_param dict tune_paramsblock_size_x tune_kernelvector_add kernel_str size arg tune_param exact python code use tune opencl kernel code python kernel_str __kernel void vector_add__glob float c __global float __global float b int n int get_global_id ci ai bi kernel tuner detect kernel languag select right compil runtim everi kernel paramet space kernel tuner insert c preprocessor defin tunabl paramet compil benchmark kernel time result print consol also return tune_kernel allow analysi note default behavior tune_kernel exactli control mani option argument httpbenvanwerkhovengithubiokernel_tuneruserapihtmlkernel_tunertune_kernel__ find mani extens exampl code exampl directori httpsgithubcombenvanwerkhovenkernel_tunerblobmasterexamples__ kernel tuner document page httpbenvanwerkhovengithubiokernel_tunerindexhtml__ search strategi tune kernel tuner support mani optim algorithm acceler autotun process current implement search algorithm brute forc default neldermead powel cg bfg lbfgsb tnc cobyla slsqp random search basinhop differenti evolut genet algorithm particl swarm optim firefli algorithm simul anneal imag docgemmamdsummarypng width align center use search strategi easi need specifi tune_kernel strategi method would like use exampl strategygenetic_algorithm strategybasinhop full overview support search strategi method pleas see user api document httpbenvanwerkhovengithubiokernel_tuneruserapihtml__ tune host kernel code possibl tune combin tunabl paramet host kernel code allow number powerful thing tune number stream kernel use cuda stream opencl command queue overlap transfer host devic kernel execut done combin tune paramet insid kernel code see convolution_stream exampl code httpsgithubcombenvanwerkhovenkernel_tunerblobmasterexamples__ document httpbenvanwerkhovengithubiokernel_tunerhostcodehtml__ detail explan kernel tuner python script correct verif option let kernel tuner verifi output everi kernel compil benchmark pass answer list list match list argument kernel contain expect output kernel input argument replac none code python answer ab none none order match argument arg kernel tune_kernelvector_add kernel_str size arg tune_param answeransw contribut pleas see contribut guid httpbenvanwerkhovengithubiokernel_tunercontributinghtml__ citat use kernel tuner research research softwar pleas cite relev among follow public code latex articlekerneltun author ben van werkhoven titl kernel tuner searchoptim gpu code autotun journal futur gener comput system year volum page url httpswwwsciencedirectcomsciencearticlepiisx doi httpsdoiorgjfutur articlewillemsenbayesian author willemsen florisjan van nieuwpoort rob van werkhoven ben titl bayesian optim autotun gpu kernel journal intern workshop perform model benchmark simul high perform comput system pmb supercomput sc year url httpsarxivorgab relat work may also like cltune httpsgithubcomcnugterencltune__ cedric nugteren cltune c librari kernel tune build statu imag httpsgithubcombenvanwerkhovenkernel_tuneractionsworkflowspythonappymlbadgesvg target httpsgithubcombenvanwerkhovenkernel_tuneractionsworkflowspythonappyml codecov badg imag httpscodecovioghbenvanwerkhovenkernel_tunerbranchmastergraphbadgesvg target httpscodecovioghbenvanwerkhovenkernel_tun pypi badg imag httpsimgshieldsiopypivkernel_tunersvgcolorbblu target httpspypipythonorgpypikernel_tun zenodo badg imag httpszenodoorgbadgedoizenodosvg target httpsdoiorgzenodo sonarcloud badg imag httpssonarcloudioapiproject_badgesmeasureprojectbenvanwerkhoven_kernel_tunermetricalert_statu target httpssonarcloudiodashboardidbenvanwerkhoven_kernel_tun fairsoftwar badg imag httpsimgshieldsiobadgefairsoftwareeuefefefefefgreen target httpsfairsoftwareeu kernel tuner exampl exampl show use kernel tuner tune cuda opencl c kernel demonstr particular usecas kernel tuner except test_vector_addpi httpsgithubcombenvanwerkhovenkernel_tunerblobmasterexamplescudatest_vector_addpy__ test_vector_add_parameterizedpi httpsgithubcombenvanwerkhovenkernel_tunerblobmasterexamplescudatest_vector_add_parameterizedpy__ show write test gpu kernel kernel tuner list exampl applic featur illustr vector add cuda httpsgithubcombenvanwerkhovenkernel_tunerblobmasterexamplescudavector_addpy__ cudac httpsgithubcombenvanwerkhovenkernel_tunerblobmasterexamplescudacvector_addpy__ opencl httpsgithubcombenvanwerkhovenkernel_tunerblobmasterexamplesopenclvector_addpy__ c httpsgithubcombenvanwerkhovenkernel_tunerblobmasterexamplescvector_addpy__ fortran httpsgithubcombenvanwerkhovenkernel_tunerblobmasterexamplesfortranvector_addpy__ use kernel tuner tune simpl kernel stencil cuda httpsgithubcombenvanwerkhovenkernel_tunerblobmasterexamplescudastencilpy__ opencl httpsgithubcombenvanwerkhovenkernel_tunerblobmasterexamplesopenclstencilpy__ use dimension problem domain dimension thread block simpl clean exampl matrix multipl cuda httpsgithubcombenvanwerkhovenkernel_tunerblobmasterexamplescudamatmulpy__ opencl httpsgithubcombenvanwerkhovenkernel_tunerblobmasterexamplesopenclmatmulpy__ pass filenam instead string code use dimension thread block tile dimens tell kernel tuner comput grid dimens thread block tile use restrict option limit search valid configur use userdefin perform metric like gflop convolut sever differ exampl center around convolut kernel cuda httpsgithubcombenvanwerkhovenkernel_tunerblobmasterexamplescudaconvolutioncu__ opencl httpsgithubcombenvanwerkhovenkernel_tunerblobmasterexamplesopenclconvolutioncl__ convolutionpi cuda httpsgithubcombenvanwerkhovenkernel_tunerblobmasterexamplescudaconvolutionpy__ opencl httpsgithubcombenvanwerkhovenkernel_tunerblobmasterexamplesopenclconvolutionpy__ use tunabl paramet tune multipl input size pass constant memori argument kernel write output json file sepconvpi cuda httpsgithubcombenvanwerkhovenkernel_tunerblobmasterexamplescudasepconvpy__ opencl httpsgithubcombenvanwerkhovenkernel_tunerblobmasterexamplesopenclsepconvpy__ use convolut kernel separ filter write output csv file use panda convolution_correctpi cuda httpsgithubcombenvanwerkhovenkernel_tunerblobmasterexamplescudaconvolution_correctpy__ opencl httpsgithubcombenvanwerkhovenkernel_tunerblobmasterexamplesopenclconvolution_correctpy__ use run_kernel comput refer answer verifi output everi benchmark kernel convolution_streamspi cuda httpsgithubcombenvanwerkhovenkernel_tunerblobmasterexamplescudaconvolution_streamspy__ alloc pagelock host memori python overlap transfer gpu comput tune paramet host code combin kernel use langc option set compil option pass list filenam instead string kernel code reduct cuda httpsgithubcombenvanwerkhovenkernel_tunerblobmasterexamplescudareductionpy__ opencl httpsgithubcombenvanwerkhovenkernel_tunerblobmasterexamplesopenclreductionpy__ use vector type shuffl instruct shuffl avail cuda tune number thread block kernel execut tune partial loop unrol factor forloop tune pipelin consist two kernel tune custom output verif function spars matrix vector multipl cuda httpsgithubcombenvanwerkhovenkernel_tunerblobmasterexamplescudaspmvpy__ use scipi comput refer answer verifi benchmark kernel express number thread block depend valu tunabl paramet pointinpolygon cuda httpsgithubcombenvanwerkhovenkernel_tunerblobmasterexamplescudapnpolypy__ overlap transfer devic map host memori tune differ implement algorithm expdist cuda httpsgithubcombenvanwerkhovenkernel_tunerblobmasterexamplescudaexpdistpy__ inthread block reduct use cub librari c cuda kernel code tune multipl kernel pipelin code gener cuda httpsgithubcombenvanwerkhovenkernel_tunerblobmasterexamplescudavector_add_codegenpy__ opencl httpsgithubcombenvanwerkhovenkernel_tunerblobmasterexamplesopenclvector_add_codegenpy__ use python function code gener toctre maxdepth _contribut includ contributingrst toctre maxdepth _detail api document file provid detail need call kernel tuner function includ option argument autofunct kernel_tunertune_kernel autofunct kernel_tunerrun_kernel autofunct kernel_tunerstore_result autofunct kernel_tunercreate_device_target toctre maxdepth _exampl includ examplesreadmerst highlight python linenothreshold paramet vocabulari certain tunabl paramet special mean kernel tuner document specifi paramet special use autotun gpu kernel gener best avoid use paramet name purpos one indic document codeblock python kernel_tun insert kernel tuner signal code compil use tuner block_size_ reserv thread block dimens grid_size_ reserv grid dimens want tune use problem_s compiler_opt_ reserv futur support tune compil option loop_unroll_factor_ reserv tunabl paramet specifi loop unrol factor nvml_ reserv tunabl paramet output relat nvml nvml_pwr_limit use nvml set power limit nvml_gr_clock use nvml set graphic clock nvml_mem_clock use nvml set memori clock also number name kernel tuner use report benchmark result report along tunabl paramet gener good idea use name tunabl paramet codeblock python time reserv time measur inform observ use kernel_tunernvmlnvmlobserv nvml_energi nvml_power power_read core_freq mem_freq temperatur ps_energi energi measur powersensor ps_power power measur powersensor highlight python linenothreshold tune host code kernel tuner also possibl tune host code gpu program even c function matter tune host code use contain paramet impact perform kernel gpu number stream use execut kernel across multipl stream anoth exampl want includ data transfer host devic tune setup tune differ method move data host devic differ tune singl cuda opencl kernel list specifi langc option c function return float time error handl c specifi languag c kernel tuner call host function mean kernel tuner interfac c fact use differ backend also mean use way tune without pycuda instal c function interfac call cuda compil directli c function return float convent use kernel tuner return float also number tune mean necessarili need time could also optim program differ qualiti long express qualiti singl floatingpoint valu benchmark instanc paramet space return float averag multipl run way direct cuda opencl kernel tune c languag provid precis time function tune host code cuda program use cuda event time howev use plain c suppli time function c vector add exampl httpsgithubcombenvanwerkhovenkernel_tunerblobmasterexamplescvector_addpy__ use omp_get_wtim function openmp measur time cpu tune number stream follow describ exampl examplescudaconvolution_streamspi exampl convolut kernel use correct check convolut applic exampl differ also suppli host code find examplescudaconvolution_streamscu bit long complex includ explain idea behind host code kernel comput spread across number cuda stream way possibl overlap data transfer host devic kernel execut transfer devic back host way split comput across stream divid problem ydimens chunk data transfer first stream slightli larger account overlap border data need differ stream kernel stream n start execut data transfer stream n n finish ensur latter use cuda event particular cudastreamwaitev halt stream n transfer stream n finish way use kernel tuner tune cuda program similar tune cuda kernel directli see codeblock python openconvolution_streamscu r f kernel_str fread problem_s size numpyprodproblem_s input_s problem_s problem_s output numpyzerossizeastypenumpyfloat orderc input numpyrandomrandninput_sizeastypenumpyfloat orderc filter numpyrandomrandnastypenumpyfloat arg output input filter tune_param dict tune_paramsblock_size_x rang tune_paramsblock_size_i rang tune_paramstile_size_x rang tune_paramstile_size_i rang tune_paramsnum_stream rang grid_div_x block_size_x tile_size_x grid_div_i block_size_i tile_size_i num_stream kernel_tunertune_kernelconvolution_stream kernel_str problem_s arg tune_param grid_div_ygrid_div_i grid_div_xgrid_div_x verbosetru langc fact differ simpl convolut exampl sourc file also contain host code num_stream ad tune paramet num_stream ad grid_div_i list kernel_nam convolution_stream c function langc use tell c function filter pass constant memori argument differ explain clarifi thing function tune c function launch cuda kernel yet suppli grid_div_x grid_div_i list howev requir c function could comput grid dimens whatev way see fit use grid_div_i grid_div_x point matter choic support conveni valu grid_size_x grid_size_i insert kernel tuner compil c code way dont comput grid size c use grid dimens comput kernel tuner filter pass separ constant memori argument cudamemcpytosymbol oper perform c host function also code compil differ direct refer compil modul upload devic therefor perform oper directli python tune host code perform memori alloc free memcpi oper insid c host code that purpos host code also time c may want includ time spent memori alloc setup time measur highlight python linenothreshold kernel correct verif whenev optim program perform import ensur program still produc correct output good program fast correct therefor import featur kernel tuner verifi output everi kernel instanc paramet space use kernel tuner correct check need pass answer option tune_kernel answer list match order type kernel argument howev argument kernel inputonli may insert none locat list kernel compil benchmark kernel kernel tuner run kernel verifi output produc argument answer list none check result produc current kernel expect result specifi answer comparison current implement use numpyallclos maximum allow absolut error e want use differ toler valu use option argument atol exampl examplescudaconvolution_correctpi demonstr use answer option tune_kernel codeblock python import numpi import kernel_tun openconvolutioncu r f kernel_str fread problem_s size numpyprodproblem_s input_s problem_s problem_s output numpyzerossizeastypenumpyfloat input numpyrandomrandninput_sizeastypenumpyfloat filter numpyrandomrandnastypenumpyfloat cmem_arg d_filter filter arg output input filter tune_param dict tune_paramsblock_size_x rang tune_paramsblock_size_i rang tune_paramstile_size_x rang tune_paramstile_size_i rang grid_div_x block_size_x tile_size_x grid_div_i block_size_i tile_size_i comput answer use naiv kernel param block_size_x block_size_i result kernel_tunerrun_kernelconvolution_na kernel_str problem_s arg param grid_div_yblock_size_i grid_div_xblock_size_x set nonoutput field none answer result none none start kernel tune correct verif kernel_tunertune_kernelconvolution_kernel kernel_str problem_s arg tune_param grid_div_ygrid_div_i grid_div_xgrid_div_x verbosetru cmem_argscmem_arg answeransw exampl use run_kernel function kernel tuner run singl kernel return result almost interfac tune_kernel exampl run naiv cuda kernel whose result trust correct answer list construct result naiv kernel includ kernel argument actual output argument input replac none valu answer list list pass tune_kernel case howev simpli compar result comput devic precomput valu enough flexibl necessari case possibl use verifi option tune_kernel specifi callabl object implement userdefin correct check function accept three paramet cpu_result gpu_result atol although name paramet differ semant posit depend reflect name use document exampl examplescudareductionpi demonstr use verifi option tune_kernel follow snippet exampl codeblock python gpu_result arg sum_x x n cpu_result refer numpysumx none none custom verifi function def verify_partial_reducecpu_result gpu_result atolnon return numpyisclosecpu_result numpysumgpu_result atolatol call tune_kernel first_kernel _ tune_kernelsum_float kernel_str problem_s arg tune_param grid_div_x verbosetru answerrefer verifyverify_partial_reduc first argument cpu_result map numpi array provid answer option exampl map refer second argument gpu_result map numpi array provid argument option tune_kernel exampl map arg third argument atol set none default maximum allow absolut error e use exampl userdefin verifi function use compar partial result comput gpu final result comput cpu could achiev use answer option number element arg necessarili match number element refer exampl toctre maxdepth design document section provid detail inform design intern kernel tuner inform mostli relev develop kernel tuner design extens support differ search execut strategi current architectur kernel tuner seen imag designpng width pt top kernel code python script tune use main function expos user interfac strategi respons iter search search space default strategi brute_forc iter valid kernel configur search space random_sampl simpli take random sampl search space advanc strategi current implement kernel tuner minim basinhop differenti evolut diff_evo use explain docuserapi see option strategi strategy_opt runner respons compil benchmark kernel configur select strategi sequenti runner current support runner exactli name say compil benchmark configur use singl sequenti python process runner foreseen futur releas runner implement top highlevel devic interfac wrap function compil benchmark kernel configur base lowlevel devic function interfac current four differ implement devic function interfac basic abstract differ backend set simpl function ready_argument_list alloc gpu memori move data gpu function like compil benchmark run_kernel function core basic main build block implement runner bottom three backend shown pycuda pyopencl tune either cuda opencl kernel rel new addit cupi backend base cupi tune cuda kernel use nvrtc compil c function implement actual call compil typic nvcc gcc use backend creat abl tune c function mostli tune c function turn launch gpu kernel rest section contain api document modul discuss document user api see docuserapi strategi kernel_tunerstrategiesbrute_forc automodul kernel_tunerstrategiesbrute_forc member kernel_tunerstrategiesrandom_sampl automodul kernel_tunerstrategiesrandom_sampl member kernel_tunerstrategiesminim automodul kernel_tunerstrategiesminim member kernel_tunerstrategiesbasinhop automodul kernel_tunerstrategiesbasinhop member kernel_tunerstrategiesdiff_evo automodul kernel_tunerstrategiesdiff_evo member kernel_tunerstrategiesgenetic_algorithm automodul kernel_tunerstrategiesgenetic_algorithm member kernel_tunerstrategiespso automodul kernel_tunerstrategiespso member kernel_tunerstrategiesfirefly_algorithm automodul kernel_tunerstrategiesfirefly_algorithm member kernel_tunerstrategiessimulated_ann automodul kernel_tunerstrategiessimulated_ann member runner kernel_tunerrunnerssequentialsequentialrunn autoclass kernel_tunerrunnerssequentialsequentialrunn specialmemb __init__ member kernel_tunerrunnerssequentialsimulationrunn autoclass kernel_tunerrunnerssimulationsimulationrunn specialmemb __init__ member devic interfac kernel_tunercoredeviceinterfac autoclass kernel_tunercoredeviceinterfac specialmemb __init__ member kernel_tunercudacudafunct autoclass kernel_tunercudacudafunct specialmemb __init__ member kernel_tunercupycupyfunct autoclass kernel_tunercupycupyfunct specialmemb __init__ member kernel_tuneropenclopenclfunct autoclass kernel_tuneropenclopenclfunct specialmemb __init__ member kernel_tunerccfunct autoclass kernel_tunerccfunct specialmemb __init__ member util function kernel_tunerutil automodul kernel_tunerutil member highlight python linenothreshold templat kernel quit common cuda program write kernel use c templat use write code work sever type exampl float doubl howev use c templat make slightli difficult directli integr cuda kernel applic written c exampl matlab fortran python sinc kernel tuner written python need take extra step provid support templat cuda kernel let first look exampl like tune templat kernel kernel tuner exampl say templat cuda kernel file call vector_addcu codeblock cuda templatetypenam __global__ void vector_addt c b int n auto blockidxx block_size_x threadidxx ci ai bi python script tune kernel would follow codeblock python import numpi kernel_tun import tune_kernel size numpyrandomrandnsizeastypenumpyfloat b numpyrandomrandnsizeastypenumpyfloat c numpyzeros_likeb n numpyints arg c b n tune_param dict tune_paramsblock_size_x rang tune_kernelvector_addfloat vector_addcu size arg tune_param see python code specifi templat instanti use kernel tuner detect use templat kernel kernel_nam posit argument tune_kernel contain templat argument featur also allow use autotun templat paramet kernel could exampl defin tunabl paramet codeblock python tune_paramsmy_typ float doubl call tune_kernel use tunabl paramet insid templat argument codeblock python tune_kernelvector_addmy_typ vector_addcu size arg tune_param select backend kernel tuner support multipl backend cuda base pycuda cupi follow explain enabl tune templat kernel either backend pycuda backend default backend kernel tuner select user suppli lang option cuda code detect kernel sourc lang set cuda user pycuda requir cuda kernel extern c linkag mean c templat kernel support support templat kernel regardless limit kernel tuner attempt wrap templat cuda kernel insert compiletim templat instanti statement wrapper kernel call templat cuda kernel actual demot __device__ function process automat code rewrit real risk break code minim chanc error due kernel tuner automat code rewrit best isol templat kernel singl sourc file includ need larger applic cupi backend provid much advanc support c templat kernel intern use nvrtc nvidia runtim compil nvrtc come restrict howev exampl nvrtc allow host code insid code pass like pycuda backend help separ sourc code devic host function seper file forc kernel tuner use cupi backend pass langcupi option tune_kernel kernel_tun document master file creat sphinxquickstart tue mar adapt file complet like least contain root toctre direct kernel tuner document content toctre maxdepth introduct self instal convolut diffus exampl matrix_multipl correct hostcod templat userapi vocabulari design contribut introduct includ readmerst indic tabl refgenindex refmodindex refsearch toctre maxdepth _instal includ installrst