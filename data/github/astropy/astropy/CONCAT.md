# Astropy Project Governance

Please visit our website to learn more about the [Astropy Team](http://www.astropy.org/team.html).
All Astropy community members are expected to abide by the [Astropy Project Code of Conduct](http://www.astropy.org/code_of_conduct.html).
Contributing to Astropy
=======================

Reporting Issues
----------------

When opening an issue to report a problem, please try to provide a minimal code
example that reproduces the issue along with details of the operating
system and the Python, NumPy, and `astropy` versions you are using.

Contributing Code and Documentation
-----------------------------------

So you are interested in contributing to the Astropy Project?  Excellent!
We love contributions! Astropy is open source, built on open source, and
we'd love to have you hang out in our community.

Anti Imposter Syndrome Reassurance
----------------------------------

We want your help. No, really.

There may be a little voice inside your head that is telling you that you're not
ready to be an open source contributor; that your skills aren't nearly good
enough to contribute. What could you possibly offer a project like this one?

We assure you - the little voice in your head is wrong. If you can write code or
documentation, you can contribute code to open source.
Contributing to open source projects is a fantastic way to advance one's coding
and open source workflow skills. Writing perfect code isn't the measure of a good
developer (that would disqualify all of us!); it's trying to create something,
making mistakes, and learning from those mistakes. That's how we all improve,
and we are happy to help others learn.

Being an open source contributor doesn't just mean writing code, either. You can
help out by writing documentation, tests, or even giving feedback about the
project (and yes - that includes giving feedback about the contribution
process). Some of these contributions may be the most valuable to the project as
a whole, because you're coming to the project with fresh eyes, so you can see
the errors and assumptions that seasoned contributors have glossed over.

Note: This text was originally written by
[Adrienne Lowe](https://github.com/adriennefriend) for a
[PyCon talk](https://www.youtube.com/watch?v=6Uj746j9Heo), and was adapted by
Astropy based on its use in the README file for the
[MetPy project](https://github.com/Unidata/MetPy).

How to Contribute, Best Practices
---------------------------------

Most contributions to Astropy are done via [pull
requests](https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/about-pull-requests)
from GitHub users' forks of the [astropy
repository](https://github.com/astropy/astropy). If you are new to this
style of development, you will want to read over our [development
workflow](https://docs.astropy.org/en/latest/development/workflow/development_workflow.html).

You may also/instead be interested in contributing to an
[astropy affiliated package](https://www.astropy.org/affiliated/).
Affiliated packages are astronomy-related software packages that are not a part
of the `astropy` core package, but build on it for more specialized applications
and follow the Astropy guidelines for reuse, interoperability, and interfacing.
Each affiliated package has its own developers/maintainers and its own specific
guidelines for contributions, so be sure to read their docs.

Once you open a pull request (which should be opened against the ``main``
branch, not against any of the other branches), please make sure to
include the following:

- **Code**: the code you are adding, which should follow
  our [coding guidelines](https://docs.astropy.org/en/latest/development/codeguide.html) as much as possible.

- **Tests**: these are usually tests to ensure code that previously
  failed now works (regression tests), or tests that cover as much as possible
  of the new functionality to make sure it does not break in the future and
  also returns consistent results on all platforms (since we run these tests on
  many platforms/configurations). For more information about how to write
  tests, see our [testing guidelines](https://docs.astropy.org/en/latest/development/testguide.html).

- **Documentation**: if you are adding new functionality, be sure to include a
  description in the main documentation (in ``docs/``). Again, we have some
  detailed [documentation guidelines](https://docs.astropy.org/en/latest/development/docguide.html) to help you out.

- **Performance improvements**: if you are making changes that impact `astropy`
  performance, consider adding a performance benchmark in the
  [astropy-benchmarks](https://github.com/astropy/astropy-benchmarks)
  repository. You can find out more about how to do this
  [in the README for that repository](https://github.com/astropy/astropy-benchmarks#contributing-benchmarks).

- **Changelog entry**: whether you are fixing a bug or adding new
  functionality, you should add a changelog fragment in the ``docs/changes/``
  directory. See ``docs/changes/README.rst`` for some guidance on the creation
  of this file.

  If you are opening a pull request you may not know
  the PR number yet, but you can add it once the pull request is open. If you
  are not sure where to put the changelog entry, wait until a maintainer
  has reviewed your PR and assigned it to a milestone.

  You do not need to include a changelog entry for fixes to bugs introduced in
  the developer version and therefore are not present in the stable releases. In
  general you do not need to include a changelog entry for minor documentation
  or test updates. Only user-visible changes (new features/API changes, fixed
  issues) need to be mentioned. If in doubt, ask the core maintainer reviewing
  your changes.

Checklist for Contributed Code
------------------------------

Before being merged, a pull request for a new feature will be reviewed to see if
it meets the following requirements. If you are unsure about how to meet all of these
requirements, please submit the PR and ask for help and/or guidance. An Astropy
maintainer will collaborate with you to make sure that the pull request meets the
requirements for inclusion in the package:

**Scientific Quality** (when applicable)
  * Is the submission relevant to astronomy?
  * Are references included to the origin source for the algorithm?
  * Does the code perform as expected?
  * Has the code been tested against previously existing implementations?

**Code Quality**
  * Are the [coding guidelines](https://docs.astropy.org/en/latest/development/codeguide.html) followed?
  * Is the code compatible with Python >=3.8?
  * Are there dependencies other than the `astropy` core, the Python Standard
    Library, and NumPy 1.18.0 or later?
    * Is the package importable even if the C-extensions are not built?
    * Are additional dependencies handled appropriately?
    * Do functions that require additional dependencies raise an `ImportError`
      if they are not present?

**Testing**
  * Are the [testing guidelines](https://docs.astropy.org/en/latest/development/testguide.html) followed?
  * Are the inputs to the functions sufficiently tested?
  * Are there tests for any exceptions raised?
  * Are there tests for the expected performance?
  * Are the sources for the tests documented?
  * Have tests that require an [optional dependency](https://docs.astropy.org/en/latest/development/testguide.html#tests-requiring-optional-dependencies)
    been marked as such?
  * Does ``tox -e test`` run without failures?

**Documentation**
  * Are the [documentation guidelines](https://docs.astropy.org/en/latest/development/docguide.html) followed?
  * Is there a docstring in [numpydoc format](https://numpydoc.readthedocs.io/en/latest/format.html) in the function describing:
    * What the code does?
    * The format of the inputs of the function?
    * The format of the outputs of the function?
    * References to the original algorithms?
    * Any exceptions which are raised?
    * An example of running the code?
  * Is there any information needed to be added to the docs to describe the
    function?
  * Does the documentation build without errors or warnings?

**License**
  * Is the `astropy` license included at the top of the file?
  * Are there any conflicts with this code and existing codes?

**Astropy requirements**
  * Do all the GitHub Actions and CircleCI tests pass? If not, are they allowed to fail?
  * If applicable, has an entry been added into the changelog?
  * Can you check out the pull request and repeat the examples and tests?

Other Tips
----------

- Behind the scenes, we conduct a number of tests or checks with new pull requests.
  This is a technique that is called continuous integration, and we use GitHub Actions
  and CircleCI. To prevent the automated tests from running, you can add ``[ci skip]``
  to your commit message. This is useful if your PR is a work in progress (WIP) and
  you are not yet ready for the tests to run. For example:

      $ git commit -m "WIP widget [ci skip]"

  - If you already made the commit without including this string, you can edit
    your existing commit message by running:

        $ git commit --amend

- If your commit makes substantial changes to the documentation but none of
  those changes include code snippets, then you can use ``[ci skip]``,
  which will skip all CI except RTD, where the documentation is built.

- When contributing trivial documentation fixes (i.e., fixes to typos, spelling,
  grammar) that don't contain any special markup and are not associated with
  code changes, please include the string ``[ci skip]`` in your commit
  message.

      $ git commit -m "Fixed typo [ci skip]"

- ``[ci skip]`` and ``[skip ci]`` are the same and can be used interchangeably.
<!-- This comments are hidden when you submit the pull request,
so you do not need to remove them! -->

<!-- Please be sure to check out our contributing guidelines,
https://github.com/astropy/astropy/blob/main/CONTRIBUTING.md .
Please be sure to check out our code of conduct,
https://github.com/astropy/astropy/blob/main/CODE_OF_CONDUCT.md . -->

<!-- If you are new or need to be re-acquainted with Astropy
contributing workflow, please see
http://docs.astropy.org/en/latest/development/workflow/development_workflow.html .
There is even a practical example at
https://docs.astropy.org/en/latest/development/workflow/git_edit_workflow_examples.html#astropy-fix-example . -->

<!-- Astropy coding style guidelines can be found here:
https://docs.astropy.org/en/latest/development/codeguide.html#coding-style-conventions
Our testing infrastructure enforces to follow a subset of the PEP8 to be
followed. You can check locally whether your changes have followed these by
running the following command:

tox -e codestyle

-->

<!-- Please just have a quick search on GitHub to see if a similar
pull request has already been posted.
We have old closed pull requests that might provide useful code or ideas
that directly tie in with your pull request. -->

<!-- We have several automatic features that run when a pull request is open.
They can appear daunting but do not worry because maintainers will help
you navigate them, if necessary. -->

### Description
<!-- Provide a general description of what your pull request does.
Complete the following sentence and add relevant details as you see fit. -->

<!-- In addition please ensure that the pull request title is descriptive
and allows maintainers to infer the applicable subpackage(s). -->

<!-- READ THIS FOR MANUAL BACKPORT FROM A MAINTAINER:
Apply "skip-basebranch-check" label **before** you open the PR! -->

This pull request is to address ...

<!-- If the pull request closes any open issues you can add this.
If you replace <Issue Number> with a number, GitHub will automatically link it.
If this pull request is unrelated to any issues, please remove
the following line. -->

Fixes #<Issue Number>

### Checklist for package maintainer(s)
<!-- This section is to be filled by package maintainer(s) who will
review this pull request. -->

This checklist is meant to remind the package maintainer(s) who will review this pull request of some common things to look for. This list is not exhaustive.

- [ ] Do the proposed changes actually accomplish desired goals?
- [ ] Do the proposed changes follow the [Astropy coding guidelines](https://docs.astropy.org/en/latest/development/codeguide.html)?
- [ ] Are tests added/updated as required? If so, do they follow the [Astropy testing guidelines](https://docs.astropy.org/en/latest/development/testguide.html)?
- [ ] Are docs added/updated as required? If so, do they follow the [Astropy documentation guidelines](https://docs.astropy.org/en/latest/development/docguide.html#astropy-documentation-rules-and-guidelines)?
- [ ] Is rebase and/or squash necessary? If so, please provide the author with appropriate instructions. Also see ["When to rebase and squash commits"](https://docs.astropy.org/en/latest/development/when_to_rebase.html).
- [ ] Did the CI pass? If no, are the failures related? If you need to run daily and weekly cron jobs as part of the PR, please apply the `Extra CI` label.
- [ ] Is a change log needed? If yes, did the change log check pass? If no, add the `no-changelog-entry-needed` label. If this is a manual backport, use the `skip-changelog-checks` label unless special changelog handling is necessary.
- [ ] Is a milestone set? Milestone must be set but `astropy-bot` check might be missing; do not let the green checkmark fool you.
- [ ] At the time of adding the milestone, if the milestone set requires a backport to release branch(es), apply the appropriate `backport-X.Y.x` label(s) *before* merge.
---
name: Bug report
about: Create a report describing unexpected or incorrect behavior in astropy.
labels: Bug
---

<!-- This comments are hidden when you submit the issue,
so you do not need to remove them! -->

<!-- Please be sure to check out our contributing guidelines,
https://github.com/astropy/astropy/blob/main/CONTRIBUTING.md .
Please be sure to check out our code of conduct,
https://github.com/astropy/astropy/blob/main/CODE_OF_CONDUCT.md . -->

<!-- Please have a search on our GitHub repository to see if a similar
issue has already been posted.
If a similar issue is closed, have a quick look to see if you are satisfied
by the resolution.
If not please go ahead and open an issue! -->

<!-- Please check that the development version still produces the same bug.
You can install development version with
pip install git+https://github.com/astropy/astropy
command. -->

### Description
<!-- Provide a general description of the bug. -->

### Expected behavior
<!-- What did you expect to happen. -->

### Actual behavior
<!-- What actually happened. -->
<!-- Was the output confusing or poorly described? -->

### Steps to Reproduce
<!-- Ideally a code example could be provided so we can run it ourselves. -->
<!-- If you are pasting code, use triple backticks (```) around
your code snippet. -->
<!-- If necessary, sanitize your screen output to be pasted so you do not
reveal secrets like tokens and passwords. -->

1. [First Step]
2. [Second Step]
3. [and so on...]

```python
# Put your Python code snippet here.
```

### System Details
<!-- Even if you do not think this is necessary, it is useful information for the maintainers.
Please run the following snippet and paste the output below:
import platform; print(platform.platform())
import sys; print("Python", sys.version)
import numpy; print("Numpy", numpy.__version__)
import erfa; print("pyerfa", erfa.__version__)
import astropy; print("astropy", astropy.__version__)
import scipy; print("Scipy", scipy.__version__)
import matplotlib; print("Matplotlib", matplotlib.__version__)
-->
---
name: Feature request
about: Suggest an idea to improve astropy
labels: "Feature Request"
---

<!-- This comments are hidden when you submit the issue,
so you do not need to remove them! -->

<!-- Please be sure to check out our contributing guidelines,
https://github.com/astropy/astropy/blob/main/CONTRIBUTING.md .
Please be sure to check out our code of conduct,
https://github.com/astropy/astropy/blob/main/CODE_OF_CONDUCT.md . -->

<!-- Please have a search on our GitHub repository to see if a similar
issue has already been posted.
If a similar issue is closed, have a quick look to see if you are satisfied
by the resolution.
If not please go ahead and open an issue! -->

### Description
<!-- Provide a general description of the feature you would like. -->
<!-- If you want to, you can suggest a draft design or API. -->
<!-- This way we have a deeper discussion on the feature. -->


### Additional context
<!-- Add any other context or screenshots about the feature request here. -->
<!-- This part is optional. -->
[![Travis CI Build Status](https://travis-ci.org/libexpat/libexpat.svg?branch=master)](https://travis-ci.org/libexpat/libexpat)
[![AppVeyor Build Status](https://ci.appveyor.com/api/projects/status/github/libexpat/libexpat?svg=true)](https://ci.appveyor.com/project/libexpat/libexpat)
[![Packaging status](https://repology.org/badge/tiny-repos/expat.svg)](https://repology.org/metapackage/expat/versions)


# Expat, Release 2.2.9

This is Expat, a C library for parsing XML, started by
[James Clark](https://en.wikipedia.org/wiki/James_Clark_(programmer)) in 1997.
Expat is a stream-oriented XML parser.  This means that you register
handlers with the parser before starting the parse.  These handlers
are called when the parser discovers the associated structures in the
document being parsed.  A start tag is an example of the kind of
structures for which you may register handlers.

Expat supports the following compilers:
- GNU GCC >=4.5
- LLVM Clang >=3.5
- Microsoft Visual Studio >=8.0/2005

Windows users should use the
[`expat_win32` package](https://sourceforge.io/projects/expat/files/expat_win32/),
which includes both precompiled libraries and executables, and source code for
developers.

Expat is [free software](https://www.gnu.org/philosophy/free-sw.en.html).
You may copy, distribute, and modify it under the terms of the License
contained in the file
[`COPYING`](https://github.com/libexpat/libexpat/blob/master/expat/COPYING)
distributed with this package.
This license is the same as the MIT/X Consortium license.

If you are building Expat from a check-out from the
[Git repository](https://github.com/libexpat/libexpat/),
you need to run a script that generates the configure script using the
GNU autoconf and libtool tools.  To do this, you need to have
autoconf 2.58 or newer. Run the script like this:

```console
./buildconf.sh
```

Once this has been done, follow the same instructions as for building
from a source distribution.

To build Expat from a source distribution, you first run the
configuration shell script in the top level distribution directory:

```console
./configure
```

There are many options which you may provide to configure (which you
can discover by running configure with the `--help` option).  But the
one of most interest is the one that sets the installation directory.
By default, the configure script will set things up to install
libexpat into `/usr/local/lib`, `expat.h` into `/usr/local/include`, and
`xmlwf` into `/usr/local/bin`.  If, for example, you'd prefer to install
into `/home/me/mystuff/lib`, `/home/me/mystuff/include`, and
`/home/me/mystuff/bin`, you can tell `configure` about that with:

```console
./configure --prefix=/home/me/mystuff
```

Another interesting option is to enable 64-bit integer support for
line and column numbers and the over-all byte index:

```console
./configure CPPFLAGS=-DXML_LARGE_SIZE
```

However, such a modification would be a breaking change to the ABI
and is therefore not recommended for general use &mdash; e.g. as part of
a Linux distribution &mdash; but rather for builds with special requirements.

After running the configure script, the `make` command will build
things and `make install` will install things into their proper
location.  Have a look at the `Makefile` to learn about additional
`make` options.  Note that you need to have write permission into
the directories into which things will be installed.

If you are interested in building Expat to provide document
information in UTF-16 encoding rather than the default UTF-8, follow
these instructions (after having run `make distclean`).
Please note that we configure with `--without-xmlwf` as xmlwf does not
support this mode of compilation (yet):

1. Mass-patch `Makefile.am` files to use `libexpatw.la` for a library name:
   <br/>
   `find -name Makefile.am -exec sed
       -e 's,libexpat\.la,libexpatw.la,'
       -e 's,libexpat_la,libexpatw_la,'
       -i {} +`

1. Run `automake` to re-write `Makefile.in` files:<br/>
   `automake`

1. For UTF-16 output as unsigned short (and version/error strings as char),
   run:<br/>
   `./configure CPPFLAGS=-DXML_UNICODE --without-xmlwf`<br/>
   For UTF-16 output as `wchar_t` (incl. version/error strings), run:<br/>
   `./configure CFLAGS="-g -O2 -fshort-wchar" CPPFLAGS=-DXML_UNICODE_WCHAR_T
       --without-xmlwf`
   <br/>Note: The latter requires libc compiled with `-fshort-wchar`, as well.

1. Run `make` (which excludes xmlwf).

1. Run `make install` (again, excludes xmlwf).

Using `DESTDIR` is supported.  It works as follows:

```console
make install DESTDIR=/path/to/image
```

overrides the in-makefile set `DESTDIR`, because variable-setting priority is

1. commandline
1. in-makefile
1. environment

Note: This only applies to the Expat library itself, building UTF-16 versions
of xmlwf and the tests is currently not supported.

When using Expat with a project using autoconf for configuration, you
can use the probing macro in `conftools/expat.m4` to determine how to
include Expat.  See the comments at the top of that file for more
information.

A reference manual is available in the file `doc/reference.html` in this
distribution.


The CMake build system is still *experimental* and will replace the primary
build system based on GNU Autotools at some point when it is ready.
For an idea of the available (non-advanced) options for building with CMake:

```console
# rm -f CMakeCache.txt ; cmake -D_EXPAT_HELP=ON -LH . | grep -B1 ':.*=' | sed 's,^--$,,'
// Choose the type of build, options are: None Debug Release RelWithDebInfo MinSizeRel ...
CMAKE_BUILD_TYPE:STRING=

// Install path prefix, prepended onto install directories.
CMAKE_INSTALL_PREFIX:PATH=/usr/local

// Path to a program.
DOCBOOK_TO_MAN:FILEPATH=/usr/bin/docbook2x-man

// build man page for xmlwf
EXPAT_BUILD_DOCS:BOOL=ON

// build the examples for expat library
EXPAT_BUILD_EXAMPLES:BOOL=ON

// build fuzzers for the expat library
EXPAT_BUILD_FUZZERS:BOOL=OFF

// build the tests for expat library
EXPAT_BUILD_TESTS:BOOL=ON

// build the xmlwf tool for expat library
EXPAT_BUILD_TOOLS:BOOL=ON

// Character type to use (char|ushort|wchar_t) [default=char]
EXPAT_CHAR_TYPE:STRING=char

// install expat files in cmake install target
EXPAT_ENABLE_INSTALL:BOOL=ON

// Use /MT flag (static CRT) when compiling in MSVC
EXPAT_MSVC_STATIC_CRT:BOOL=OFF

// build a shared expat library
EXPAT_SHARED_LIBS:BOOL=ON

// Treat all compiler warnings as errors
EXPAT_WARNINGS_AS_ERRORS:BOOL=OFF

// Make use of getrandom function (ON|OFF|AUTO) [default=AUTO]
EXPAT_WITH_GETRANDOM:STRING=AUTO

// utilize libbsd (for arc4random_buf)
EXPAT_WITH_LIBBSD:BOOL=OFF

// Make use of syscall SYS_getrandom (ON|OFF|AUTO) [default=AUTO]
EXPAT_WITH_SYS_GETRANDOM:STRING=AUTO
```
5.0.1 (2022-01-26)
==================

Bug Fixes
---------

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- Trying to create an instance of ``astropy.coordinates.Distance`` by providing
  both ``z`` and ``parallax`` now raises the expected ``ValueError``. [#12531]

- Fixed a bug where changing the wrap angle of the longitude component of a
  representation could raise a warning or error in certain situations. [#12556]

- ``astropy.coordinates.Distance`` constructor no longer ignores the ``unit``
  keyword when ``parallax`` is provided. [#12569]

astropy.cosmology
^^^^^^^^^^^^^^^^^

- ``astropy.cosmology.utils.aszarr`` can now convert ``Column`` objects. [#12525]

- Reading a cosmology from an ECSV will load redshift and Hubble parameter units
  from the cosmology units module. [#12636]

astropy.io.fits
^^^^^^^^^^^^^^^

- Fix formatting issue in ``_dump_coldefs`` and add tests for ``tabledump`` and
  ``tableload`` convenience functions. [#12526]

astropy.io.misc
^^^^^^^^^^^^^^^

- YAML can now also represent quantities and arrays with structured dtype,
  as well as structured scalars based on ``np.void``. [#12509]

astropy.modeling
^^^^^^^^^^^^^^^^

- Fixes error when fitting multiplication or division based compound models
  where the sub-models have different output units. [#12475]

- Bugfix for incorrectly initialized and filled ``parameters`` data for ``Spline1D`` model. [#12523]

- Bugfix for ``keyerror`` thrown by ``Model.input_units_equivalencies`` when
  used on ``fix_inputs`` models which have no set unit equivalencies. [#12597]

astropy.table
^^^^^^^^^^^^^

- ``astropy.table.Table.keep_columns()`` and
  ``astropy.table.Table.remove_columns()`` now work with generators of column
  names. [#12529]

- Avoid duplicate storage of info in serialized columns if the column
  used to serialize already can hold that information. [#12607]

astropy.timeseries
^^^^^^^^^^^^^^^^^^

- Fixed edge case bugs which emerged when using ``aggregate_downsample`` with custom bins. [#12527]

astropy.units
^^^^^^^^^^^^^

- Structured units can be serialized to/from yaml. [#12492]

- Fix bad typing problems by removing interaction with ``NDArray.__class_getitem__``. [#12511]

- Ensure that ``Quantity.to_string(format='latex')`` properly typesets exponents
  also when ``u.quantity.conf.latex_array_threshold = -1`` (i.e., when the threshold
  is taken from numpy). [#12573]

- Structured units can now be copied with ``copy.copy`` and ``copy.deepcopy``
  and also pickled and unpicked also for ``protocol`` >= 2.
  This does not work for big-endian architecture with older ``numpy<1.21.1``. [#12583]

astropy.utils
^^^^^^^^^^^^^

- Ensure that a ``Masked`` instance can be used to initialize (or viewed
  as) a ``numpy.ma.Maskedarray``. [#12482]

- Ensure ``Masked`` also works with numpy >=1.22, which has a keyword argument
  name change for ``np.quantile``. [#12511]

- ``astropy.utils.iers.LeapSeconds.auto_open()`` no longer emits unnecessary
  warnings when ``astropy.utils.iers.conf.auto_max_age`` is set to ``None``. [#12713]


5.0 (2021-11-15)
================


New Features
------------

astropy.convolution
^^^^^^^^^^^^^^^^^^^

- Added dealiasing support to ``convolve_fft``. [#11495]

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- Added missing coordinate transformations where the starting and ending frames
  are the same (i.e., loopback transformations). [#10909]

- Allow negation, multiplication and division also of representations that
  include a differential (e.g., ``SphericalRepresentation`` with a
  ``SphericalCosLatDifferential``).  For all operations, the outcome is
  equivalent to transforming the representation and differential to cartesian,
  then operating on those, and transforming back to the original representation
  (except for ``UnitSphericalRepresentation``, which will return a
  ``SphericalRepresentation`` if there is a scale change). [#11470]

- ``RadialRepresentation.transform`` can work with a multiplication matrix only.
  All other matrices still raise an exception. [#11576]

- ``transform`` methods are added to ``BaseDifferential`` and ``CartesianDifferential``.
  All transform methods on Representations now delegate transforming differentials
  to the differential objects. [#11654]

- Adds new ``HADec`` built-in frame with transformations to/from ``ICRS`` and ``CIRS``.
  This frame complements ``AltAz`` to give observed coordinates (hour angle and declination)
  in the ``ITRS`` for an equatorially mounted telescope. [#11676]

- ``SkyCoord`` objects now have a ``to_table()`` method, which allows them to be
  converted to a ``QTable``. [#11743]

astropy.cosmology
^^^^^^^^^^^^^^^^^

- Cosmologies now store metadata in a mutable parameter ``meta``.
  The initialization arguments ``name`` and ``meta`` are keyword-only. [#11542]

- A new unit, ``redshift``, is defined. It is a dimensionless unit to distinguish
  redshift quantities from other non-redshift values. For compatibility with
  dimensionless quantities the equivalency ``dimensionless_redshift`` is added.
  This equivalency is enabled by default. [#11786]

- Add equality operator for comparing Cosmology instances. Comparison is done on
  all immutable fields (this excludes 'meta').

  Now the following will work:

  .. code-block:: python

      >>> from astropy.cosmology import Planck13, Planck18
      >>> Planck13 == Planck18
      False

      >>> Planck18 == Planck18
      True [#11813]

- Added ``read/write`` methods to Cosmology using the Unified I/O registry.
  Now custom file format readers, writers, and format-identifier functions
  can be registered to read, write, and identify, respectively, Cosmology
  objects. Details are discussed in an addition to the docs. [#11948]

- Added ``to_format/from_format`` methods to Cosmology using the Unified I/O
  registry. Now custom format converters and format-identifier functions
  can be registered to transform Cosmology objects.
  The transformation between Cosmology and dictionaries is pre-registered.
  Details are discussed in an addition to the docs. [#11998]

- Added units module for defining and collecting cosmological units and
  equivalencies. [#12092]

- Flat cosmologies are now set by a mixin class, ``FlatCosmologyMixin`` and its
  FLRW-specific subclass ``FlatFLRWMixin``. All ``FlatCosmologyMixin`` are flat,
  but not all flat cosmologies are instances of ``FlatCosmologyMixin``. As
  example, ``LambdaCDM`` **may** be flat (for the a specific set of parameter
  values),  but ``FlatLambdaCDM`` **will** be flat.

  Cosmology parameters are now descriptors. When accessed from a class they
  transparently stores information, like the units and accepted equivalencies.
  On a cosmology instance, the descriptor will return the parameter value.
  Parameters can have custom ``getter`` methods.

  Cosmological equality is refactored to check Parameters (and the name)
  A new method, ``is_equivalent``, is added to check Cosmology equivalence, so
  a ``FlatLambdaCDM`` and flat ``LambdaCDM`` are equivalent. [#12136]

- Replaced ``z = np.asarray(z)`` with ``z = u.Quantity(z, u.dimensionless_unscaled).value``
  in Cosmology methods. Input of values with incorrect units raises a UnitConversionError
  or TypeError. [#12145]

- Cosmology Parameters allow for custom value setters.
  Values can be set once, but will error if set a second time.
  If not specified, the default setter is used, which will assign units
  using the Parameters ``units`` and ``equivalencies`` (if present).
  Alternate setters may be registered with Parameter to be specified by a str,
  not a decorator on the Cosmology. [#12190]

- Cosmology instance conversion to dict now accepts keyword argument ``cls`` to
  determine dict type, e.g. ``OrderedDict``. [#12209]

- A new equivalency is added between redshift and the Hubble parameter and values
  with units of little-h.
  This equivalency is also available in the catch-all equivalency ``with_redshift``. [#12211]

- A new equivalency is added between redshift and distance -- comoving, lookback,
  and luminosity. This equivalency is also available in the catch-all equivalency
  ``with_redshift``. [#12212]

- Register Astropy Table into Cosmology's ``to/from_format`` I/O, allowing
  a Cosmology instance to be parsed from or converted to a Table instance.
  Also adds the ``__astropy_table__`` method allowing ``Table(cosmology)``. [#12213]

- The WMAP1 and WMAP3 are accessible as builtin cosmologies. [#12248]

- Register Astropy Model into Cosmology's ``to/from_format`` I/O, allowing
  a Cosmology instance to be parsed from or converted to a Model instance. [#12269]

- Register an ECSV reader and writer into Cosmology's I/O, allowing a Cosmology
  instance to be read from from or written to an ECSV file. [#12321]

astropy.io.ascii
^^^^^^^^^^^^^^^^

- Added new way to specify the dtype for tables that are read: ``converters``
  can specify column names with wildcards. [#11892]

- Added a new ``astropy.io.ascii.Mrt`` class to write tables in the American
  Astronomical Society Machine-Readable Table format,
  including documentation and tests for the same. [#11897, #12301, #12302]

- When writing, the input data are no longer copied, improving performance.
  Metadata that might be changed, such as format and serialization
  information, is copied, hence users can continue to count on no
  changes being made to the input data. [#11919]

astropy.io.misc
^^^^^^^^^^^^^^^

- Add Parquet serialization of Tables with pyarrow, including metadata support and
  columnar access. [#12215]

astropy.modeling
^^^^^^^^^^^^^^^^

- Added fittable spline models to ``modeling``. [#11634]

- Extensive refactor of ``BoundingBox`` for better usability and maintainability. [#11930]

- Added ``CompoundBoundingBox`` feature to ``~astropy.modeling``, which allows more flexibility in
  defining bounding boxes for models that are applied to images with many slices. [#11942]

- Improved parameter support for ``astropy.modeling.core.custom_model`` created models. [#11984]

- Added the following trigonometric models and linked them to their appropriate inverse models:
    * ``Cosine1D`` [#12158]
    * ``Tangent1D``
    * ``ArcSine1D``
    * ``ArcCosine1D``
    * ``ArcTangent1D`` [#12185]

astropy.table
^^^^^^^^^^^^^

- Added a new method ``Table.update()`` which does a dictionary-style update of a
  ``Table`` by adding or replacing columns. [#11904]

- Masked quantities are now fully supported in tables.  This includes ``QTable``
  automatically converting ``MaskedColumn`` instances to ``MaskedQuantity``,
  and ``Table`` doing the reverse. [#11914]

- Added new keyword arguments ``keys_left`` and ``keys_right`` to the table ``join``
  function to support joining tables on key columns with different names. In
  addition the new keywords can accept a list of column-like objects which are
  used as the match keys. This allows joining on arbitrary data which are not part
  of the tables being joined. [#11954]

- Formatting of any numerical values in the output of ``Table.info()`` and
  ``Column.info()`` has been improved. [#12022]

- It is now possible to add dask arrays as columns in tables
  and have them remain as dask arrays rather than be converted
  to Numpy arrays. [#12219]

- Added a new registry for mixin handlers, which can be used
  to automatically convert array-like Python objects into
  mixin columns when assigned to a table column. [#12219]

astropy.time
^^^^^^^^^^^^

- Adds a new method ``earth_rotation_angle`` to calculate the Local Earth Rotation Angle.
  Also adjusts Local Sidereal Time for the Terrestrial Intermediate Origin (``TIO``)
  and adds a rigorous correction for polar motion. The ``TIO`` adjustment is approximately
  3 microseconds per century from ``J2000`` and the polar motion correction is at most
  about +/-50 nanoseconds. For models ``IAU1982`` and ``IAU1994``, no such adjustments are
  made as they pre-date the TIO concept. [#11680]

astropy.timeseries
^^^^^^^^^^^^^^^^^^

- A custom binning scheme is now available in ``aggregate_downsample``.
  It allows ``time_bin_start`` and ``time_bin_size`` to be arrays, and adds
  an optional ``time_bin_end``.
  This scheme mirrors the API for ``BinnedTimeSeries``. [#11266]

astropy.units
^^^^^^^^^^^^^

- ``Quantity`` gains a ``__class_getitem__`` to create unit-aware annotations
   with the syntax ``Quantity[unit or physical_type, shape, numpy.dtype]``.
   If the python version is 3.9+ or ``typing_extensions`` is installed,
   these are valid static type annotations. [#10662]

- Each physical type is added to ``astropy.units.physical``
  (e.g., ``physical.length`` or ``physical.electrical_charge_ESU``).
  The attribute-accessible names (underscored, without parenthesis) also
  work with ``astropy.units.physical.get_physical_type``. [#11691]

- It is now possible to have quantities based on structured arrays in
  which the unit has matching structure, giving each field its own unit,
  using units constructed like ``Unit('AU,AU/day')``. [#11775]

- The milli- prefix has been added to ``astropy.units.Angstrom``. [#11788]

- Added attributes ``base``, ``coords``, and ``index`` and method ``copy()`` to
  ``QuantityIterator`` to match ``numpy.ndarray.flatiter``. [#11796]

- Added "angular frequency" and "angular velocity" as aliases for the "angular
  speed" physical type. [#11865]

- Add light-second to units of length [#12128]

astropy.utils
^^^^^^^^^^^^^

- The ``astropy.utils.deprecated_renamed_argument()`` decorator now supports
  custom warning messages. [#12305]

- The NaN-aware numpy functions such as ``np.nansum`` now work on Masked
  arrays, with masked values being treated as NaN, but without raising
  warnings or exceptions. [#12454]

astropy.visualization
^^^^^^^^^^^^^^^^^^^^^

- Added a feature so that SphericalCircle will accept center parameter as a SkyCoord object. [#11790]

astropy.wcs
^^^^^^^^^^^

- ``astropy.wcs.utils.obsgeo_to_frame`` has been added to convert the obsgeo coordinate
  array on ``astropy.wcs.WCS`` objects to an ``ITRS`` coordinate frame instance. [#11716]

- Updated bundled ``WCSLIB`` to version 7.7 with several bugfixes. [#12034]


API Changes
-----------

astropy.config
^^^^^^^^^^^^^^

- ``update_default_config`` and ``ConfigurationMissingWarning`` are deprecated. [#11502]

astropy.constants
^^^^^^^^^^^^^^^^^

- Removed deprecated ``astropy.constants.set_enabled_constants`` context manager. [#12105]

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- Positions for the Moon using the 'builtin' ephemeris now use the new
  ``erfa.moon98`` function instead of our own implementation of the Meeus
  algorithm. As this also corrects a misunderstanding of the frame returned by
  the Meeus, this improves the agreement with the JPL ephemeris from about 30 to
  about 6 km rms. [#11753]

- Removed deprecated ``representation`` attribute from
  ``astropy.coordinates.BaseCoordinateFrame`` class. [#12257]

- ``SpectralQuantity`` and ``SpectralCoord`` ``.to_value`` method can now be called without
  ``unit`` argument in order to maintain a consistent interface with ``Quantity.to_value`` [#12440]

astropy.cosmology
^^^^^^^^^^^^^^^^^

- ``z_at_value`` now works with arrays for all arguments (except ``func``,
  ``verbose``, and ``method``). Consequently, ``coordinates.Distance.z`` can
  be used when Distance is an array. [#11778]

- Remove deprecation warning and error remapping in ``Cosmology.clone``.
  Now unknown arguments will raise a ``TypeError``, not an ``AttributeError``. [#11785]

- The ``read/write`` and ``to/from_format`` Unified I/O registries are separated
  and apply only to ``Cosmology``. [#12015]

- Cosmology parameters in ``cosmology.parameters.py`` now have units,
  where applicable. [#12116]

- The function ``astropy.cosmology.utils.inf_like()`` is deprecated. [#12175]

- The function ``astropy.cosmology.utils.vectorize_if_needed()`` is deprecated.
  A new function ``astropy.cosmology.utils.vectorize_redshift_method()`` is added
  as replacement. [#12176]

- Cosmology base class constructor now only accepts arguments ``name`` and ``meta``.
  Subclasses should add relevant arguments and not pass them to the base class. [#12191]

astropy.io
^^^^^^^^^^

- When ``astropy`` raises an ``OSError`` because a file it was told to write
  already exists, the error message now always suggests the use of the
  ``overwrite=True`` argument. The wording is now consistent for all I/O formats. [#12179]

astropy.io.ascii
^^^^^^^^^^^^^^^^

- Removed deprecated ``overwrite=None`` option for
  ``astropy.io.ascii.ui.write()``. Overwriting existing files now only happens if
  ``overwrite=True``. [#12171]

astropy.io.fits
^^^^^^^^^^^^^^^

- The internal class _CardAccessor is no longer registered as a subclass of
  the Sequence or Mapping ABCs. [#11923]

- The deprecated ``clobber`` argument will be removed from the
  ``astropy.io.fits`` functions in version 5.1, and the deprecation warnings now
  announce that too. [#12311]

astropy.io.registry
^^^^^^^^^^^^^^^^^^^

- The ``write`` function now is allowed to return possible content results, which
  means that custom writers could, for example, create and return an instance of
  some container class rather than a file on disk. [#11916]

- The registry functions are refactored into a class-based system.
  New Read-only, write-only, and read/write registries can be created.
  All functions accept a new argument ``registry``, which if not specified,
  defaults to the global default registry. [#12015]

astropy.io.votable
^^^^^^^^^^^^^^^^^^

- Deprecated the ``pedantic`` keyword argument in the
  ``astropy.io.votable.table.parse`` function and the corresponding configuration
  setting. It has been replaced by the ``verify`` option. [#12129]

astropy.modeling
^^^^^^^^^^^^^^^^

- Refactored how ``astropy.modeling.Model`` handles model evaluation in order to better
  organize the code. [#11931]

- Removed the following deprecated modeling features:
      ``astropy.modeling.utils.ExpressionTree`` class,
      ``astropy.modeling.functional_models.MexicanHat1D`` model,
      ``astropy.modeling.functional_models.MexicanHat2D`` model,
      ``astropy.modeling.core.Model.inputs`` setting in model initialize,
      ``astropy.modeling.core.CompoundModel.inverse`` setting in model initialize, and
      ``astropy.modeling.core.CompoundModel.both_inverses_exist()`` method. [#11978]

- Deprecated the ``AliasDict`` class in ``modeling.utils``. [#12411]

astropy.nddata
^^^^^^^^^^^^^^

- Removed ``block_reduce`` and ``block_replicate`` functions from
  ``nddata.utils``. These deprecated functions in ``nddata.utils`` were
  moved to ``nddata.blocks``. [#12288]

astropy.stats
^^^^^^^^^^^^^

- Removed the following deprecated features from ``astropy.stats``:

  * ``conf`` argument for ``funcs.binom_conf_interval()`` and
    ``funcs.binned_binom_proportion()``,
  * ``conflevel`` argument for ``funcs.poisson_conf_interval()``, and
  * ``conf_lvl`` argument for ``jackknife.jackknife_stats()``. [#12200]

astropy.table
^^^^^^^^^^^^^

- Printing a ``Table`` now shows the qualified class name of mixin columns in the
  dtype header row  instead of "object". This applies to all the ``Table`` formatted output
  methods whenever ``show_dtype=True`` is selected. [#11660]

- The 'overwrite' argument has been added to the jsviewer table writer.
  Overwriting an existing file requires 'overwrite' to be True. [#11853]

- The 'overwrite' argument has been added to the pandas table writers.
  Overwriting an existing file requires 'overwrite' to be True. [#11854]

- The table ``join`` function now accepts only the first four arguments ``left``,
  ``right``, ``keys``, and ``join_type`` as positional arguments. All other
  arguments must be supplied as keyword arguments. [#11954]

- Adding a dask array to a Table will no longer convert
  that dask to a Numpy array, so accessing t['dask_column']
  will now return a dask array instead of a Numpy array. [#12219]

astropy.time
^^^^^^^^^^^^

- Along with the new method ``earth_rotation_angle``, ``sidereal_time`` now accepts
  an ``EarthLocation`` as the ``longitude`` argument. [#11680]

astropy.units
^^^^^^^^^^^^^

- Unit ``littleh`` and equivalency ``with_H0`` have been moved to the
  ``cosmology`` module and are deprecated from ``astropy.units``. [#12092]

astropy.utils
^^^^^^^^^^^^^

- ``astropy.utils.introspection.minversion()`` now uses
  ``importlib.metadata.version()``. Therefore, its ``version_path`` keyword is no
  longer used and deprecated. This keyword will be removed in a future release. [#11714]

- Updated ``utils.console.Spinner`` to better resemble the API of
  ``utils.console.ProgressBar``, including an ``update()`` method and
  iterator support. [#11772]

- Removed deprecated ``check_hashes`` in ``check_download_cache()``. The function also
  no longer returns anything. [#12293]

- Removed unused ``download_cache_lock_attempts`` configuration item in
  ``astropy.utils.data``. Deprecation was not possible. [#12293]

- Removed deprecated ``hexdigest`` keyword from ``import_file_to_cache()``. [#12293]

- Setting ``remote_timeout`` configuration item in ``astropy.utils.data`` to 0 will
  no longer disable download from the Internet; Set ``allow_internet`` configuration
  item to ``False`` instead. [#12293]

astropy.visualization
^^^^^^^^^^^^^^^^^^^^^

- Removed deprecated ``imshow_only_kwargs`` keyword from ``imshow_norm``. [#12290]

astropy.wcs
^^^^^^^^^^^

- Move complex logic from ``HighLevelWCSMixin.pixel_to_world`` and
  ``HighLevelWCSMixin.world_to_pixel`` into the helper functions
  ``astropy.wcs.wcsapi.high_level_api.high_level_objects_to_values`` and
  ``astropy.wcs.wcsapi.high_level_api.values_to_high_level_objects`` to allow
  reuse in other places. [#11950]


Bug Fixes
---------

astropy.config
^^^^^^^^^^^^^^

- ``generate_config`` no longer outputs wrong syntax for list type. [#12037]

astropy.constants
^^^^^^^^^^^^^^^^^

- Fixed a bug where an older constants version cannot be set directly after
  astropy import. [#12084]

astropy.convolution
^^^^^^^^^^^^^^^^^^^

- Passing an ``array`` argument for any Kernel1D or Kernel2D subclasses (with the
  exception of CustomKernel) will now raise a ``TypeError``. [#11969]

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- If a ``Table`` containing a ``SkyCoord`` object as a column is written to a
  FITS, ECSV or HDF5 file then any velocity information that might be present
  will be retained. [#11750]

- The output of ``SkyCoord.apply_space_motion()`` now always has the same
  differential type as the ``SkyCoord`` itself. [#11932]

- Fixed bug where Angle, Latitude and Longitude with NaN values could not be printed. [#11943]

- Fixed a bug with the transformation from ``PrecessedGeocentric`` to ``GCRS``
  where changes in ``obstime``, ``obsgeoloc``, or ``obsgeovel`` were ignored.
  This bug would also affect loopback transformations from one ``PrecessedGeocentric``
  frame to another ``PrecessedGeocentric`` frame. [#12152]

- Fixed a bug with the transformations between ``TEME`` and ``ITRS`` or between ``TEME``
  and itself where a change in ``obstime`` was ignored. [#12152]

- Avoid unnecessary transforms through CIRS for AltAz and HADec and
  use ICRS as intermediate frame for these transformations instead. [#12203]

- Fixed a bug where instantiating a representation with a longitude component
  could mutate input provided for that component even when copying is specified. [#12307]

- Wrapping an ``Angle`` array will now ignore NaN values instead of attempting to wrap
  them, which would produce unexpected warnings/errors when working with coordinates
  and representations due to internal broadcasting. [#12317]

astropy.cosmology
^^^^^^^^^^^^^^^^^

- Dictionaries for in-built cosmology realizations are not altered by creating
  the realization and are also made immutable. [#12278]

astropy.io.fits
^^^^^^^^^^^^^^^

- Prevent zero-byte writes for FITS binary tables to
  speed up writes on the Lustre filesystem. [#11955]

- Enable ``json.dump`` for FITS_rec with variable length (VLF) arrays. [#11957]

- Add support for reading and writing int8 images [#11996]

- Ensure header passed to ``astropy.io.fits.CompImageHDU`` does not need to contain
  standard cards that can be automatically generated, such as ``BITPIX`` and ``NAXIS``. [#12061]

- Fixed a bug where ``astropy.io.fits.HDUDiff`` would ignore the ``ignore_blank_cards``
  keyword argument. [#12122]

- Open uncompressed file even if extension says it's compressed [#12135]

- Fix the computation of the DATASUM in a ``CompImageHDU`` when the data is >1D. [#12138]

- Reading files where the SIMPLE card is present but with an invalid format now
  issues a warning instead of raising an exception [#12234]

- Convert UNDEFINED to None when iterating over card values. [#12310]

astropy.io.misc
^^^^^^^^^^^^^^^

- Update ASDF tag versions in ExtensionType subclasses to match ASDF Standard 1.5.0. [#11986]

- Fix ASDF serialization of model inputs and outputs and add relevant assertion to
  test helper. [#12381]

- Fix bug preventing ASDF serialization of bounding box for models with only one input. [#12385]

astropy.io.votable
^^^^^^^^^^^^^^^^^^

- Now accepting UCDs containing phot.color. [#11982]

astropy.modeling
^^^^^^^^^^^^^^^^

- Added ``Parameter`` descriptions to the implemented models which were
  missing. [#11232]

- The ``separable`` property is now correctly set on models constructed with
  ``astropy.modeling.custom_model``. [#11744]

- Minor bugfixes and improvements to modeling including the following:
      * Fixed typos and clarified several errors and their messages throughout
        modeling.
      * Removed incorrect try/except blocks around scipy code in
        ``convolution.py`` and ``functional_models.py``.
      * Fixed ``Ring2D`` model's init to properly accept all combinations
        of ``r_in``, ``r_out``, and ``width``.
      * Fixed bug in ``tau`` validator for the ``Logarithmic1D`` and
        ``Exponential1D`` models when using them as model sets.
      * Fixed ``copy`` method for ``Parameter`` in order to prevent an
        automatic ``KeyError``, and fixed ``bool`` for ``Parameter`` so
        that it functions with vector values.
      * Removed unreachable code from ``Parameter``, the ``_Tabular`` model,
        and the ``Drude1D`` model.
      * Fixed validators in ``Drude1D`` model so that it functions in a
        model set.
      * Removed duplicated code from ``polynomial.py`` for handing of
        ``domain`` and ``window``.
      * Fixed the ``Pix2Sky_HEALPixPolar`` and ``Sky2Pix_HEALPixPolar`` modes
        so that their ``evaluate`` and ``inverse`` methods actually work
        without raising an error. [#12232]

astropy.nddata
^^^^^^^^^^^^^^

- Ensure that the ``wcs=`` argument to ``NDData`` is always parsed into a high
  level WCS object. [#11985]

astropy.stats
^^^^^^^^^^^^^

- Fixed a bug in sigma clipping where the bounds would not be returned for
  completely empty or masked data. [#11994]

- Fixed a bug in ``biweight_midvariance`` and ``biweight_scale`` where
  output data units would be dropped for constant data and where the
  result was a scalar NaN. [#12146]

astropy.table
^^^^^^^^^^^^^

- Ensured that ``MaskedColumn.info`` is propagated in all cases, so that when
  tables are sliced, writing will still be as requested on
  ``info.serialize_method``. [#11917]

- ``table.conf.replace_warnings`` and ``table.jsviewer.conf.css_urls`` configuration
  items now have correct ``'string_list'`` type. [#12037]

- Fixed an issue where initializing from a list of dict-like rows (Mappings) did
  not work unless the row values were instances of ``dict``. Now any object that
  is an instance of the more general ``collections.abc.Mapping`` will work. [#12417]

astropy.uncertainty
^^^^^^^^^^^^^^^^^^^

- Ensure that scalar ``QuantityDistribution`` unit conversion in ufuncs
  works properly again. [#12471]

astropy.units
^^^^^^^^^^^^^

- Add quantity support for ``scipy.special`` dimensionless functions
  erfinv, erfcinv, gammaln and loggamma. [#10934]

- ``VOUnit.to_string`` output is now compliant with IVOA VOUnits 1.0 standards. [#11565]

- Units initialization with unicode has been expanded to include strings such as
  'M' and 'e'. [#11827]

- Give a more informative ``NotImplementedError`` when trying to parse a unit
  using an output-only format such as 'unicode' or 'latex'. [#11829]

astropy.utils
^^^^^^^^^^^^^

- Fixed a bug in ``get_readable_fileobj`` that prevented the unified file read
  interface from closing ASCII files. [#11809]

- The function ``astropy.utils.decorators.deprecated_attribute()`` no longer
  ignores its ``message``, ``alternative``, and ``pending`` arguments. [#12184]

- Ensure that when taking the minimum or maximum of a ``Masked`` array,
  any masked NaN values are ignored. [#12454]

astropy.visualization
^^^^^^^^^^^^^^^^^^^^^

- The tick labelling for radians has been fixed to remove a redundant ``.0`` in
  the label for integer multiples of pi at 2pi and above. [#12221]

- Fix a bug where non-``astropy.wcs.WCS`` WCS instances were not accepted in
  ``WCSAxes.get_transform``. [#12286]

- Fix compatibility with Matplotlib 3.5 when using the ``grid_type='contours'``
  mode for drawing grid lines. [#12447]

astropy.wcs
^^^^^^^^^^^

- Enabled ``SlicedLowLevelWCS.pixel_to_world_values`` to handle slices including
  non-``int`` integers, e.g. ``numpy.int64``. [#11980]


Other Changes and Additions
---------------------------

- In docstrings, Sphinx cross-reference targets now use intersphinx, even if the
  target is an internal link (``link`` is now ``'astropy:link``).
  When built in Astropy these links are interpreted as internal links. When built
  in affiliate packages, the link target is set by the key 'astropy' in the
  intersphinx mapping. [#11690]

- Made PyYaml >= 3.13 a strict runtime dependency. [#11903]

- Minimum version of required Python is now 3.8. [#11934]

- Minimum version of required Scipy is now 1.3. [#11934]

- Minimum version of required Matplotlib is now 3.1. [#11934]

- Minimum version of required Numpy is now 1.18. [#11935]

- Fix deprecation warnings with Python 3.10 [#11962]

- Speed up ``minversion()`` in cases where a module with a ``__version__``
  attribute is passed. [#12174]

- ``astropy`` now requires ``packaging``. [#12199]

- Updated the bundled CFITSIO library to 4.0.0. When compiling with an external
  library, version 3.35 or later is required. [#12272]


4.3.1 (2021-08-11)
==================

Bug Fixes
---------

astropy.io.fits
^^^^^^^^^^^^^^^

- In ``fits.io.getdata`` do not fall back to first non-primary extension when
  user explicitly specifies an extension. [#11860]

- Ensure multidimensional masked columns round-trip properly to FITS. [#11911]

- Ensure masked times round-trip to FITS, even if multi-dimensional. [#11913]

- Raise ``ValueError`` if an ``np.float32`` NaN/Inf value is assigned to a
  header keyword. [#11922]

astropy.modeling
^^^^^^^^^^^^^^^^

- Fixed bug in ``fix_inputs`` handling of bounding boxes. [#11908]

astropy.table
^^^^^^^^^^^^^

- Fix an error when converting to pandas any ``Table`` subclass that
  automatically adds a table index when the table is created. An example is a
  binned ``TimeSeries`` table. [#12018]

astropy.units
^^^^^^^^^^^^^

- Ensure that unpickling quantities and units in new sessions does not change
  hashes and thus cause problems with (de)composition such as getting different
  answers from the ``.si`` attribute. [#11879]

- Fixed cannot import name imperial from astropy.units namespace. [#11977]

astropy.utils
^^^^^^^^^^^^^

- Ensure any ``.info`` on ``Masked`` instances is propagated correctly when
  viewing or slicing. As a consequence, ``MaskedQuantity`` can now be correctly
  written to, e.g., ECSV format with ``serialize_method='data_mask'``. [#11910]


4.3 (2021-07-26)
================

New Features
------------

astropy.convolution
^^^^^^^^^^^^^^^^^^^

- Change padding sizes for ``fft_pad`` in ``convolve_fft`` from powers of
  2 only to scipy-optimized numbers, applied separately to each dimension;
  yielding some performance gains and avoiding potential large memory
  impact for certain multi-dimensional inputs. [#11533]

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- Adds the ability to create topocentric ``CIRS`` frames. Using these,
  ``AltAz`` calculations are now accurate down to the milli-arcsecond
  level. [#10994]

- Adds a direct transformation from ``ICRS`` to ``AltAz`` frames. This
  provides a modest speedup of approximately 10 percent. [#11079]

- Adds new ``WGS84GeodeticRepresentation``, ``WGS72GeodeticRepresentation``,
  and ``GRS80GeodeticRepresentation``. These are mostly for use inside
  ``EarthLocation`` but can also be used to convert between geocentric
  (cartesian) and different geodetic representations directly. [#11086]

- ``SkyCoord.guess_from_table`` now also searches for differentials in the table.
  In addition, multiple regex matches can be resolved when they are exact
  component names, e.g. having both columns dec and pm_dec no longer errors
  and will be included in the SkyCoord. [#11417]

- All representations now have a ``transform`` method, which allows them to be
  transformed by a 3x3 matrix in a Cartesian basis. By default, transformations
  are routed through ``CartesianRepresentation``. ``SphericalRepresentation`` and
  ``PhysicssphericalRepresentation`` override this for speed and to prevent NaN
  leakage from the distance to the angular components.
  Also, the functions ``is_O3`` and ``is_rotation`` have been added to
  ``matrix_utities`` for checking whether a matrix is in the O(3) group or is a
  rotation (proper or improper), respectively. [#11444]

- Moved angle formatting and parsing utilities to
  ``astropy.coordinates.angle_formats``.
  Added new functionality to ``astropy.coordinates.angle_utilities`` for
  generating points on or in spherical surfaces, either randomly or on a grid. [#11628]

- Added a new method to ``SkyCoord``, ``spherical_offsets_by()``, which is the
  conceptual inverse of ``spherical_offsets_to()``: Given angular offsets in
  longitude and latitude, this method returns a new coordinate with the offsets
  applied. [#11635]

- Refactor conversions between ``GCRS`` and ``CIRS,TETE`` for better accuracy
  and substantially improved speed. [#11069]

- Also refactor ``EarthLocation.get_gcrs`` for an increase in performance of
  an order of magnitude, which enters as well in getting observed positions of
  planets using ``get_body``. [#11073]

- Refactored the usage of metaclasses in ``astropy.coordinates`` to instead use
  ``__init_subclass__`` where possible. [#11090]

- Removed duplicate calls to ```transform_to``` from ```match_to_catalog_sky```
  and ```match_to_catalog_3d```, improving their performance. [#11449]

- The new DE440 and DE440s ephemerides are now available via shortcuts 'de440'
  and 'de440s'.  The DE 440s ephemeris will probably become the default
  ephemeris when chosing 'jpl' in 5.0. [#11601]

astropy.cosmology
^^^^^^^^^^^^^^^^^

- Cosmology parameter dictionaries now also specify the Cosmology class to which
  the parameters correspond. For example, the dictionary for
  ``astropy.cosmology.parameters.Planck18`` has the added key-value pair
  ("cosmology", "FlatLambdaCDM"). [#11530]

astropy.io.ascii
^^^^^^^^^^^^^^^^

- Added support for reading and writing ASCII tables in QDP (Quick and Dandy
  Plotter) format. [#11256]

- Added support for reading and writing multidimensional column data (masked and
  unmasked) to ECSV. Also added formal support for reading and writing object-type
  column data which can contain items consisting of lists, dicts, and basic scalar
  types. This can be used to store columns of variable-length arrays. Both of
  these features use JSON to convert the object to a string that is stored in the
  ECSV output. [#11569, #11662, #11720]

astropy.io.fits
^^^^^^^^^^^^^^^

- Added ``append`` keyword to append table objects to an existing FITS file [#2632, #11149]

- Check that the SIMPLE card is present when opening a file, to ensure that the
  file is a valid FITS file and raise a better error when opening a non FITS
  one. ``ignore_missing_simple`` can be used to skip this verification. [#10895]

- Expose ``Header.strip`` as a public method, to remove the most common
  structural keywords. [#11174]

- Enable the use of ``os.PathLike`` objects when dealing with (mainly FITS) files. [#11580]

astropy.io.registry
^^^^^^^^^^^^^^^^^^^

- Readers and writers can now set a priority, to assist with resolving which
  format to use. [#11214]

astropy.io.votable
^^^^^^^^^^^^^^^^^^

- Version 1.4 VOTables now use the VOUnit format specification. [#11032]

- When reading VOTables using the Unified File Read/Write Interface (i.e. using
  the ``Table.read()`` or ``QTable.read()`` functions) it is now possible to
  specify all keyword arguments that are valid for
  ``astropy.io.votable.table.parse()``. [#11643]

astropy.modeling
^^^^^^^^^^^^^^^^

- Added a state attribute to models to allow preventing the synching of
  constraint values from the constituent models. This synching can
  greatly slow down fitting if there are large numbers of fit parameters.
  model.sync_constraints = True means check constituent model constraints
  for compound models every time the constraint is accessed, False, do not.
  Fitters that support constraints will set this to False on the model copy
  and then set back to True when the fit is complete before returning. [#11365]

- The ``convolve_models_fft`` function implements model convolution so that one
  insures that the convolution remains consistent across multiple different
  inputs. [#11456]

astropy.nddata
^^^^^^^^^^^^^^

- Prevent unnecessary copies of the data during ``NDData`` arithmetic when units
  need to be added. [#11107]

- NDData str representations now show units, if present. [#11553]

astropy.stats
^^^^^^^^^^^^^

- Added the ability to specify stdfunc='mad_std' when doing sigma clipping,
  which will use a built-in function and lead to significant performance
  improvements if cenfunc is 'mean' or 'median'. [#11664]


- Significantly improved the performance of sigma clipping when cenfunc and
  stdfunc are passed as strings and the ``grow`` option is not used. [#11219]

- Improved performance of ``bayesian_blocks()`` by removing one ``np.log()``
  call [#11356]

astropy.table
^^^^^^^^^^^^^

- Add table attributes to include or exclude columns from the output when
  printing a table. This functionality includes a context manager to
  include/exclude columns temporarily. [#11190]

- Improved the string representation of objects related to ``Table.indices`` so
  they now indicate the object type and relevant attributes. [#11333]

astropy.timeseries
^^^^^^^^^^^^^^^^^^

- An exception is raised when ``n_bins`` is passed as an argument while
  any of the parameters ``time_bin_start`` or ``time_bin_size`` is not
  scalar. [#11463]

astropy.units
^^^^^^^^^^^^^

- The ``physical_type`` attributes of each unit are now objects of the (new)
  ``astropy.units.physical.PhysicalType`` class instead of strings and the
  function ``astropy.units.physical.get_physical_type`` can now translate
  strings to these objects. [#11204]

-  The function ``astropy.units.physical.def_physical_type`` was created to
   either define entirely new physical types, or to add more physical type
   names to an existing physical types. [#11204]

- ``PhysicalType``'s can be operated on using operations multiplication,
  division, and exponentiation are to facilitate dimensional analysis. [#11204]

- It is now possible to define aliases for units using
  ``astropy.units.set_enabled_aliases``. This can be used when reading files
  that have misspelled units. [#11258]

- Add a new "DN" unit, ``units.dn`` or ``units.DN``, representing data number
  for a detector. [#11591]

astropy.utils
^^^^^^^^^^^^^

- Added ``ssl_context`` and ``allow_insecure`` options to ``download_file``,
  as well as the ability to optionally use the ``certifi`` package to provide
  root CA certificates when downloading from sites secured with
  TLS/SSL. [#10434]

- ``astropy.utils.data.get_pkg_data_path`` is publicly scoped (previously the
  private function ``_find_pkg_data_path``) for obtaining file paths without
  checking if the file/directory exists, as long as the package and module
  do. [#11006]

- Deprecated ``astropy.utils.OrderedDescriptor`` and
  ``astropy.utils.OrderedDescriptorContainer``, as new features in Python 3
  make their use less compelling. [#11094, #11099]

- ``astropy.utils.masked`` provides a new ``Masked`` class/factory that can be
  used to represent masked ``ndarray`` and all its subclasses, including
  ``Quantity`` and its subclasses.  These classes can be used inside
  coordinates, but the mask is not yet exposed.  Generally, the interface should
  be considered experimental. [#11127, #11792]

- Add new ``utils.parsing`` module to with helper wrappers around
  ``ply``. [#11227]

- Change the Time and IERS leap second handling so that the leap second table is
  updated only when a Time transform involving UTC is performed. Previously this
  update check was done the first time a ``Time`` object was created, which in
  practice occured when importing common astropy subpackages like
  ``astropy.coordinates``. Now you can prevent querying internet resources (for
  instance on a cluster) by setting ``iers.conf.auto_download = False``. This
  can  be done after importing astropy but prior to performing any ``Time``
  scale transformations related to UTC. [#11638]


- Added a new module at ``astropy.utils.compat.optional_deps`` to consolidate
  the definition of ``HAS_x`` optional dependency flag variables,
  like ``HAS_SCIPY``. [#11490]

astropy.wcs
^^^^^^^^^^^

- Add IVOA UCD mappings for some FITS WCS keywords commonly used in solar
  physics. [#10965]

- Add ``STOKES`` FITS WCS keyword to the IVOA UCD mapping. [#11236]

- Updated bundled version of WCSLIB to version 7.6. See
  https://www.atnf.csiro.au/people/mcalabre/WCS/CHANGES for a list of
  included changes. [#11549]


API Changes
-----------

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- For input to representations, subclasses of the class required for a
  given attribute will now be allowed in. [#11113]

- Except for ``UnitSphericalRepresentation``, shortcuts in representations now
  allow for attached differentials. [#11467]

- Allow coordinate name strings as input to
  ``SkyCoord.is_transformable_to``. [#11552]

astropy.cosmology
^^^^^^^^^^^^^^^^^

- Change ``z_at_value`` to use ``scipy.optimize.minimize_scalar`` with default
  method ``Brent`` (other options ``Bounded`` and ``Golden``) and accept
  ``bracket`` option to set initial search region. [#11080]

- Clarified definition of inputs to ``angular_diameter_distance_z1z2``.
  The function now emits ``AstropyUserWarning`` when ``z2`` is less than
  ``z1``. [#11197]

- Split cosmology realizations from core classes, moving the former to new file
  ``realizations``. [#11345]

- Since cosmologies are immutable, the initialization signature and values can
  be stored, greatly simplifying cloning logic and extending it to user-defined
  cosmology classes that do not have attributes with the same name as each
  initialization argument.  [#11515]

- Cloning a cosmology with changed parameter(s) now appends "(modified)" to the
  new instance's name, unless a name is explicitly passed to ``clone``. [#11536]

- Allow ``m_nu`` to be input as any quantity-like or array-like -- Quantity,
  array, float, str, etc. Input is passed to the Quantity constructor and
  converted to eV, still with the prior mass-energy equivalence
  enabled. [#11640]

astropy.io.fits
^^^^^^^^^^^^^^^

- For conversion between FITS tables and astropy ``Table``, the standard mask
  values of ``NaN`` for float and null string for string are now properly
  recognized, leading to a ``MaskedColumn`` with appropriately set mask
  instead of a ``Column`` with those values exposed. Conversely, when writing
  an astropy ``Table`` to a FITS tables, masked values are now consistently
  converted to the standard FITS mask values of ``NaN`` for float and null
  string for string (i.e., not just for tables with ``masked=True``, which no
  longer is guaranteed to signal the presence of ``MaskedColumn``). [#11222]

astropy.io.votable
^^^^^^^^^^^^^^^^^^

- The use of ``version='1.0'`` is now fully deprecated in constructing
  a ``astropy.io.votable.tree.VOTableFile``. [#11659]

astropy.modeling
^^^^^^^^^^^^^^^^

- Removed deprecated ``astropy.modeling.blackbody`` module. [#10972]

astropy.table
^^^^^^^^^^^^^

- Added ``Column.value`` as an alias for the existing ``Column.data`` attribute.
  This makes accessing a column's underlying data array consistent with the
  ``.value`` attribute available for ``Time`` and ``Quantity`` objects. [#10962]

- In reading from a FITS tables, the standard mask values of ``NaN`` for float
  and null string for string are properly recognized, leading to a
  ``MaskedColumn`` with appropriately set mask. [#11222]

- Changed the implementation of the ``table.index.Index`` class so instantiating
  from this class now returns an ``Index`` object as expected instead of a
  ``SlicedIndex`` object. [#11333]

astropy.units
^^^^^^^^^^^^^

- The ``physical_type`` attribute of units now returns an instance of
  ``astropy.units.physical.PhysicalType`` instead of a string.  Because
  ``PhysicalType`` instances can be compared to strings, no code changes
  should be necessary when making comparisons.  The string representations
  of different physical types will differ from previous releases. [#11204]

- Calling ``Unit()`` with no argument now returns a dimensionless unit,
  as was documented but not implemented. [#11295]

astropy.utils
^^^^^^^^^^^^^

- Removed deprecated ``utils.misc.InheritDocstrings`` and ``utils.timer``. [#10281]

- Removed usage of deprecated ``ipython`` stream in ``utils.console``. [#10942]

astropy.wcs
^^^^^^^^^^^

- Deprecate ``accuracy`` argument in ``all_world2pix`` which was mistakenly
  *documented*, in the case ``accuracy`` was ever used. [#11055]


Bug Fixes
---------

astropy.convolution
^^^^^^^^^^^^^^^^^^^

- Fixes for ``convolve_fft`` documentation examples. [#11510]

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- Allow ``Distance`` instances with negative distance values as input for
  ``SphericalRepresentation``.  This was always noted as allowed in an
  exception message when a negative ``Quantity`` with length units was
  passed in, but was not actually possible to do. [#11113]

- Makes the ``Angle.to_string`` method to follow the format described in the
  docstring with up to 8 significant decimals instead of 4. [#11153]

- Ensure that proper motions can be calculated when converting a ``SkyCoord``
  with cartesian representation to unit-spherical, by fixing the conversion of
  ``CartesianDifferential`` to ``UnitSphericalDifferential``. [#11469]

- When re-representing coordinates from spherical to unit-spherical and vice
  versa, the type of differential will now be preserved. For instance, if only a
  radial velocity was present, that will remain the case (previously, a zero
  proper motion component was added). [#11482]

- Ensure that wrapping of ``Angle`` does not raise a warning even if ``nan`` are
  present.  Also try to make sure that the result is within the wrapped range
  even in the presence of rounding errors. [#11568]

- Comparing a non-SkyCoord object to a ``SkyCoord`` using ``==`` no longer
  raises an error. [#11666]

- Different ``SkyOffsetFrame`` classes no longer interfere with each other,
  causing difficult to debug problems with the ``origin`` attribute. The
  ``origin`` attribute now no longer is propagated, so while it remains
  available on a ``SkyCoord`` that is an offset, it no longer is available once
  that coordinate is transformed to another frame. [#11730] [#11730]

astropy.cosmology
^^^^^^^^^^^^^^^^^

- Cosmology instance names are now immutable. [#11535]

astropy.io.ascii
^^^^^^^^^^^^^^^^

- Fixed bug where writing a table that has comments defined (via
  ``tbl.meta['comments']``) with the 'csv' format was failing. Since the
  formally defined CSV format does not support comments, the comments are now
  just ignored unless ``comment=<comment prefix>`` is supplied to the
  ``write()`` call. [#11475]

- Fixed the issue where the CDS reader failed to treat columns
  as nullable if the ReadMe file contains a limits specifier. [#11531]

- Made sure that the CDS reader does not ignore an order specifier that
  may be present after the null specifier '?'. Also made sure that it
  checks null values only when an '=' symbol is present and reads
  description text even if there is no whitespace after '?'. [#11593]

astropy.io.fits
^^^^^^^^^^^^^^^

- Fix ``ColDefs.add_col/del_col`` to allow in-place addition or removal of
  a column. [#11338]

- Fix indexing of ``fits.Header`` with Numpy integers. [#11387]

- Do not delete ``EXTNAME`` for compressed image header if a default and
  non-default ``EXTNAME`` are present. [#11396]

- Prevent warnings about ``HIERARCH`` with ``CompImageHeader`` class. [#11404]

- Fixed regression introduced in Astropy 4.0.5 and 4.2.1 with verification of
  FITS headers with HISTORY or COMMENT cards with long (> 72 characters)
  values. [#11487]

- Fix reading variable-length arrays when there is a gap between the data and the
  heap. [#11688]

astropy.io.votable
^^^^^^^^^^^^^^^^^^

- ``NumericArray`` converter now properly broadcasts scalar mask to array. [#11157]

- VOTables are now written with the correct namespace and schema location
  attributes. [#11659]

astropy.modeling
^^^^^^^^^^^^^^^^

- Fixes the improper propagation of ``bounding_box`` from
  ``astropy.modeling.models`` to their inverses. For cases in which the inverses
  ``bounding_box`` can be determined, the proper calculation has been
  implemented. [#11414]

- Bugfix to allow rotation models to accept arbitrarily-shaped inputs. [#11435]

- Bugfixes for ``astropy.modeling`` to allow ``fix_inputs`` to accept empty
  dictionaries and dictionaries with ``numpy`` integer keys. [#11443]

- Bugfix for how ``SPECIAL_OPERATORS`` are handled. [#11512]

- Fixes ``Model`` crashes when some inputs are scalars and during some types of
  output reshaping. [#11548]

- Fixed bug in ``LevMarLSQFitter`` when using weights and vector inputs. [#11603]

astropy.stats
^^^^^^^^^^^^^

- Fixed a bug with the ``copy=False`` option when carrying out sigma
  clipping - previously if ``masked=False`` this still copied the data,
  but this will now change the array in-place. [#11219]

astropy.table
^^^^^^^^^^^^^

- Ensure that adding a ``Quantity`` or other mixin column to a ``Table``
  does not have side effects, such as creating an associated ``info``
  instance (which would lead to slow-down of, e.g., slicing afterwards). [#11077]

- When writing to a FITS tables, masked values are again always converted to
  the standard FITS mask values of ``NaN`` for float and null string
  for string, not just for table with ``masked=True``. [#11222]

- Using ``Table.to_pandas()`` on an indexed ``Table`` with masked integer values
  now correctly construct the ``pandas.DataFrame``. [#11432]

- Fixed ``Table`` HTML representation in Jupyter notebooks so that it is
  horizontally scrollable within Visual Studio Code. This was done by wrapping
  the ``<table>`` in a ``<div>`` element. [#11476]

- Fix a bug where a string-valued ``Column`` that happened to have a ``unit``
  attribute could not be added to a ``QTable``.  Such columns are now simply
  kept as ``Column`` instances (with a warning). [#11585]

- Fix an issue in ``Table.to_pandas(index=<colname>)`` where the index column name
  was not being set properly for the ``DataFrame`` index. This was introduced by
  an API change in pandas version 1.3.0. Previously when creating a ``DataFrame``
  with the index set to an astropy ``Column``, the ``DataFrame`` index name was
  automatically set to the column name. [#11921]

astropy.time
^^^^^^^^^^^^

- Fix a thread-safety issue with initialization of the leap-second table
  (which is only an issue when ERFA's built-in table is out of date). [#11234]

- Fixed converting a zero-length time object from UTC to
  UT1 when an empty array is passed. [#11516]

astropy.uncertainty
^^^^^^^^^^^^^^^^^^^

- ``Distribution`` instances can now be used as input to ``Quantity`` to
  initialize ``QuantityDistribution``.  Hence, ``distribution * unit``
  and ``distribution << unit`` will work too. [#11210]

astropy.units
^^^^^^^^^^^^^

- Move non-astronomy units from astrophys.py to a new misc.py file. [#11142]

- The physical type of ``astropy.units.mol / astropy.units.m ** 3`` is now
  defined as molar concentration.  It was previously incorrectly defined
  as molar volume. [#11204]

- Make ufunc helper lookup thread-safe. [#11226]

- Make ``Unit`` string parsing (as well as ``Angle`` parsing) thread-safe. [#11227]

- Decorator ``astropy.units.decorators.quantity_input`` now only evaluates
  return type annotations based on ``UnitBase`` or ``FunctionUnitBase`` types.
  Other annotations are skipped over and are not attempted to convert to the
  correct type. [#11506]

astropy.utils
^^^^^^^^^^^^^

- Make ``lazyproperty`` and ``classdecorator`` thread-safe. This should fix a
  number of thread safety issues. [#11224]

astropy.visualization
^^^^^^^^^^^^^^^^^^^^^

- Fixed a bug that resulted in some parts of grid lines being visible when they
  should have been hidden. [#11380]

- Fixed a bug that resulted in ``time_support()`` failing for intervals of
  a few months if one of the ticks was the month December. [#11615]

astropy.wcs
^^^^^^^^^^^

- ``fit_wcs_from_points`` now produces a WCS with integer ``NAXIXn``
  values. [#10865]

- Updated bundled version of ``WCSLIB`` to v7.4, fixing a bug that caused
  the coefficients of the TPD distortion function to not be written to the
  header. [#11260]

- Fixed a bug in assigning type when converting ``colsel`` to
  ``numpy.ndarray``. [#11431]

- Added ``WCSCOMPARE_*`` constants to the list of WCSLIB constants
  available/exposed through the ``astropy.wcs`` module. [#11647]

- Fix a bug that caused APE 14 WCS transformations for FITS WCS with ZOPT, BETA,
  VELO, VOPT, or VRAD CTYPE to not work correctly. [#11781]


Other Changes and Additions
---------------------------

- The configuration file is no longer created by default when importing astropy
  and its existence is no longer required. Affiliated packages should update their
  ``__init__.py`` module to remove the block using ``update_default_config`` and
  ``ConfigurationDefaultMissingWarning``. [#10877]

- Replace ``pkg_resources`` (from setuptools) with ``importlib.metadata`` which
  comes from the stdlib, except for Python 3.7 where the backport package is added
  as a new dependency. [#11091]

- Turn on numpydoc's ``numpydoc_xref_param_type``  to create cross-references
  for the parameter types in the Parameters, Other Parameters, Returns and Yields
  sections of the docstrings. [#11118]

- Docstrings across the package are standardized to enable references.
  Also added is an Astropy glossary-of-terms to define standard inputs,
  e.g. ``quantity-like`` indicates an input that can be interpreted by
  ``astropy.units.Quantity``. [#11118]

- Binary wheels are now built to the manylinux2010 specification. These wheels
  should be supported on all versions of pip shipped with Python 3.7+. [#11377]

- The name of the default branch for the astropy git repository has been renamed
  to ``main``, and the documentation and tooling has been updated accordingly.
  If you have made a local clone you may wish to update it following the
  instructions in the repository's README. [#11379]

- Sphinx cross-reference link targets are added for every ``PhysicalType``.
  Now in the parameter types in the Parameters, Other Parameters, Returns and
  Yields sections of the docstring, the physical type of a quantity can be
  annotated in square brackets.
  E.g. `` distance : `~astropy.units.Quantity` ['length'] `` [#11595]

- The minimum supported version of ``ipython`` is now 4.2. [#10942]

- The minimum supported version of ``pyerfa`` is now 1.7.3. [#11637]


4.2.1 (2021-04-01)
==================

Bug Fixes
---------

astropy.cosmology
^^^^^^^^^^^^^^^^^

- Fixed an issue where specializations of the comoving distance calculation
  for certain cosmologies could not handle redshift arrays. [#10980]

astropy.io.fits
^^^^^^^^^^^^^^^

- Fix bug where manual fixes to invalid header cards were not preserved when
  saving a FITS file. [#11108]

astropy.io.votable
^^^^^^^^^^^^^^^^^^

- ``NumericArray`` converter now properly broadcasts scalar mask to array.
  [#11157]

astropy.table
^^^^^^^^^^^^^

- Fix bug when initializing a ``Table`` subclass that uses ``TableAttribute``'s.
  If the data were an instance of the table then attributes provided in the
  table initialization call could be ignored. [#11217]

astropy.time
^^^^^^^^^^^^

- Change epoch of ``TimeUnixTAI`` (``"unix_tai"``) from ``1970-01-01T00:00:00 UTC``
  to ``1970-01-01T00:00:00 TAI`` to match the intended and documented behaviour.
  This essentially changes the resulting times by 8.000082 seconds, the initial
  offset between TAI and UTC. [#11249]

astropy.units
^^^^^^^^^^^^^

- Fixed a bug with the ``quantity_input`` decorator where allowing
  dimensionless inputs for an argument inadvertently disabled any checking of
  compatible units for that argument. [#11283]

astropy.utils
^^^^^^^^^^^^^

- Fix a bug so that ``np.shape``, ``np.ndim`` and ``np.size`` again work on
  classes that use ``ShapedLikeNDArray``, like representations, frames,
  sky coordinates, and times. [#11133]

astropy.wcs
^^^^^^^^^^^

- Fix error when a user defined ``proj_point`` parameter is passed to ``fit_wcs_from_points``. [#11139]


Other Changes and Additions
---------------------------


- Change epoch of ``TimeUnixTAI`` (``"unix_tai"``) from ``1970-01-01T00:00:00 UTC``
  to ``1970-01-01T00:00:00 TAI`` to match the intended and documented behaviour.
  This essentially changes the resulting times by 8.000082 seconds, the initial
  offset between TAI and UTC. [#11249]


4.2 (2020-11-24)
================

New Features
------------

astropy.convolution
^^^^^^^^^^^^^^^^^^^

- Methods ``convolve`` and ``convolve_fft`` both now return Quantity arrays
  if user input is given in one. [#10822]

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- Numpy functions that broadcast, change shape, or index (like
  ``np.broadcast_to``, ``np.rot90``, or ``np.roll``) now work on
  coordinates, frames, and representations. [#10337]

- Add a new science state ``astropy.coordinates.erfa_astrom.erfa_astrom`` and
  two classes ``ErfaAstrom``, ``ErfaAstromInterpolator`` as wrappers to
  the ``pyerfa`` astrometric functions used in the coordinate transforms.
  Using ``ErfaAstromInterpolator``, which interpolates astrometric properties for
  ``SkyCoord`` instances with arrays of obstime, can dramatically speed up
  coordinate transformations while keeping microarcsecond resolution.
  Depending on needed precision and the obstime array in question, speed ups
  reach factors of 10x to >100x. [#10647]

- ``galactocentric_frame_defaults`` can now also be used as a registry, with
  user-defined parameter values and metadata. [#10624]

- Method ``.realize_frame`` from coordinate frames now accepts ``**kwargs``,
  including ``representation_type``. [#10727]

- Avoid an unnecessary call to ``erfa.epv00`` in transformations between
  ``CIRS`` and ``ICRS``, improving performance by 50 %. [#10814]

- A new equatorial coordinate frame, with RA and Dec measured w.r.t to the True
  Equator and Equinox (TETE). This frame is commonly known as "apparent place"
  and is the correct frame for coordinates returned from JPL Horizons. [#10867]

- Added a context manager ``impose_finite_difference_dt`` to the
  ``TransformGraph`` class to override the finite-difference time step
  attribute (``finite_difference_dt``) for all transformations in the graph
  with that attribute. [#10341]

- Improve performance of ``SpectralCoord`` by refactoring internal
  implementation. [#10398]

astropy.cosmology
^^^^^^^^^^^^^^^^^

- The final version of the Planck 2018 cosmological parameters are included
  as the ``Planck18`` object, which is now the default cosmology.  The
  parameters are identical to those of the ``Planck18_arXiv_v2`` object,
  which is now deprecated and will be removed in a future release. [#10915]

astropy.modeling
^^^^^^^^^^^^^^^^

- Added NFW profile and tests to modeling package [#10505]

- Added missing logic for evaluate to compound models [#10002]

- Stop iteration in ``FittingWithOutlierRemoval`` before reaching ``niter`` if
  the masked points are no longer changing. [#10642]

- Keep a (shallow) copy of ``fit_info`` from the last iteration of the wrapped
  fitter in ``FittingWithOutlierRemoval`` and also record the actual number of
  iterations performed in it. [#10642]

- Added attributes for fitting uncertainties (covariance matrix, standard
  deviations) to models. Parameter covariance matrix can be accessed via
  ``model.cov_matrix``, standard deviations by ``model.stds`` or individually
  for each parameter by ``parameter.std``. Currently implemented for
  ``LinearLSQFitter`` and ``LevMarLSQFitter``. [#10552]

- N-dimensional least-squares statistic and specific 1,2,3-D methods [#10670]

astropy.stats
^^^^^^^^^^^^^

- Added ``circstd`` function to obtain a circular standard deviation. [#10690]

astropy.table
^^^^^^^^^^^^^

- Allow initializing a ``Table`` using a list of ``names`` in conjunction with
  a ``dtype`` from a numpy structured array. The list of ``names`` overrides the
  names specified in the ``dtype``. [#10419]

astropy.time
^^^^^^^^^^^^

- Add new ``isclose()`` method to ``Time`` and ``TimeDelta`` classes to allow
  comparison of time objects to within a specified tolerance. [#10646]

- Improve initialization time by a factor of four when creating a scalar ``Time``
  object in a format like ``unix`` or ``cxcsec`` (time delta from a reference
  epoch time). [#10406]

- Improve initialization time by a factor of ~25 or more for large arrays of
  string times in ISO, ISOT or year day-of-year formats. This is done with a new
  C-based time parser that can be adapted for other fixed-format custom time
  formats. [#10360]

- Numpy functions that broadcast, change shape, or index (like
  ``np.broadcast_to``, ``np.rot90``, or ``np.roll``) now work on times.
  [#10337, #10502]

astropy.timeseries
^^^^^^^^^^^^^^^^^^

- Improve memory and speed performance when iterating over the entire time
  column of a ``TimeSeries`` object. Previously this involved O(N^2) operations
  and memory. [#10889]

astropy.units
^^^^^^^^^^^^^

- ``Quantity.to`` has gained a ``copy`` option to allow copies to be avoided
  when the units do not change. [#10517]

- Added the ``spat`` unit of solid angle that represents the full sphere.
  [#10726]

astropy.utils
^^^^^^^^^^^^^

- ``ShapedLikeNDArray`` has gained the capability to use numpy functions
  that broadcast, change shape, or index. [#10337]

- ``get_free_space_in_dir`` now takes a new ``unit`` keyword and
  ``check_free_space_in_dir`` takes ``size`` defined as ``Quantity``. [#10627]

- New ``astropy.utils.data.conf.allow_internet`` configuration item to
  control downloading data from the Internet. Setting ``allow_internet=False``
  is the same as ``remote_timeout=0``. Using ``remote_timeout=0`` to control
  internet access will stop working in a future release. [#10632]

- New ``is_url`` function so downstream packages do not have to secretly use
  the hidden ``_is_url`` anymore. [#10684]

astropy.visualization
^^^^^^^^^^^^^^^^^^^^^

- Added the ``Quadrangle`` patch for ``WCSAxes`` for a latitude-longitude
  quadrangle.  Unlike ``matplotlib.patches.Rectangle``, the edges of this
  patch will be rendered as curved lines if appropriate for the WCS
  transformation. [#10862]

- The position of tick labels are now only calculated when needed. If any text
  parameters are changed (color, font weight, size etc.) that don't effect the
  tick label position, the positions are not recomputed, improving performance.
  [#10806]

astropy.wcs
^^^^^^^^^^^

- ``WCS.to_header()`` now appends comments to SIP coefficients. [#10480]

- A new property ``dropped_world_dimensions`` has been added to
  ``SlicedLowLevelWCS`` to record information about any world axes removed by
  slicing a WCS. [#10195]

- New ``WCS.proj_plane_pixel_scales()`` and ``WCS.proj_plane_pixel_area()``
  methods to return pixel scales and area, respectively, as Quantity. [#10872]


API Changes
-----------

astropy.config
^^^^^^^^^^^^^^

- ``set_temp_config`` now preserves the existing cache rather than deleting
  it and relying on reloading it from the previous config file. This ensures
  that any programmatically made changes are preserved as well. [#10474]

- Configuration path detection logic has changed: Now, it looks for ``~`` first
  before falling back to older logic. In addition, ``HOMESHARE`` is no longer
  used in Windows. [#10705]

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- The passing of frame classes (as opposed to frame instances) to the
  ``transform_to()`` methods of low-level coordinate-frame classes has been
  deprecated.  Frame classes can still be passed to the ``transform_to()``
  method of the high-level ``SkyCoord`` class, and using ``SkyCoord`` is
  recommended for all typical use cases of transforming coordinates. [#10475]

astropy.stats
^^^^^^^^^^^^^

- Added a ``grow`` parameter to ``SigmaClip``, ``sigma_clip`` and
  ``sigma_clipped_stats``, to allow expanding the masking of each deviant
  value to its neighbours within a specified radius. [#10613]

- Passing float ``n`` to ``poisson_conf_interval`` when using
  ``interval='kraft-burrows-nousek'`` now raises ``TypeError`` as its value
  must be an integer. [#10838]

astropy.table
^^^^^^^^^^^^^

- Change ``Table.columns.keys()`` and ``Table.columns.values()`` to both return
  generators instead of a list. This matches the behavior for Python ``dict``
  objects. [#10543]

- Removed the ``FastBST`` and ``FastRBT`` indexing engines because they depend
  on the ``bintrees`` package, which is no longer maintained and is deprecated.
  Instead, use the ``SCEngine`` indexing engine, which is similar in
  performance and relies on the ``sortedcontainers`` package. [#10622]

- When slicing a mixin column in a table that had indices, the indices are no
  longer copied since they generally are not useful, having the wrong shape.
  With this, the behaviour becomes the same as that for a regular ``Column``.
  (Note that this does not affect slicing of a table; sliced columns in those
  will continue to carry a sliced version of any indices). [#10890]

- Change behavior so that when getting a single item out of a mixin column such
  as ``Time``, ``TimeDelta``, ``SkyCoord`` or ``Quantity``, the ``info``
  attribute is no longer copied. This improves performance, especially when the
  object is an indexed column in a ``Table``. [#10889]

- Raise a TypeError when a scalar column is added to an unsized table. [#10476]

- The order of columns when creating a table from a ``list`` of ``dict`` may be
  changed. Previously, the order was alphabetical because the ``dict`` keys
  were assumed to be in random order. Since Python 3.7, the keys are always in
  order of insertion, so ``Table`` now uses the order of keys in the first row
  to set the column order. To alphabetize the columns to match the previous
  behavior, use ``t = t[sorted(t.colnames)]``. [#10900]

astropy.time
^^^^^^^^^^^^

- Refactor ``Time`` and ``TimeDelta`` classes to inherit from a common
  ``TimeBase`` class. The ``TimeDelta`` class no longer inherits from ``Time``.
  A number of methods that only apply to ``Time`` (e.g. ``light_travel_time``)
  are no longer available in the ``TimeDelta`` class. [#10656]

astropy.units
^^^^^^^^^^^^^

- The ``bar`` unit is no longer wrongly considered an SI unit, meaning that
  SI decompositions like ``(u.kg*u.s**-2* u.sr**-1 * u.nm**-1).si`` will
  no longer include it. [#10586]

astropy.utils
^^^^^^^^^^^^^

- Shape-related items from ``astropy.utils.misc`` -- ``ShapedLikeNDArray``,
  ``check_broadcast``, ``unbroadcast``, and ``IncompatibleShapeError`` --
  have been moved to their own module, ``astropy.utils.shapes``. They remain
  importable from ``astropy.utils``. [#10337]

- ``check_hashes`` keyword in ``check_download_cache`` is deprecated and will
  be removed in a future release. [#10628]

- ``hexdigest`` keyword in ``import_file_to_cache`` is deprecated and will
  be removed in a future release. [#10628]


Bug Fixes
---------

astropy.config
^^^^^^^^^^^^^^

- Fix a few issues with ``generate_config`` when used with other packages.
  [#10893]

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- Fixed a bug in the coordinate-frame attribute ``CoordinateAttribute`` where
  the internal transformation could behave differently depending on whether
  the input was a low-level coordinate frame or a high-level ``SkyCoord``.
  ``CoordinateAttribute`` now always performs a ``SkyCoord``-style internal
  transformation, including the by-default merging of frame attributes. [#10475]

astropy.modeling
^^^^^^^^^^^^^^^^

- Fixed an issue of ``Model.render`` when the input ``out`` datatype is not
  float64. [#10542]

astropy.visualization
^^^^^^^^^^^^^^^^^^^^^

- Fix support for referencing WCSAxes coordinates by their world axes names.
  [#10484]

astropy.wcs
^^^^^^^^^^^

- Objective functions called by ``astropy.wcs.fit_wcs_from_points`` were
  treating longitude and latitude distances equally. Now longitude scaled
  properly. [#10759]


Other Changes and Additions
---------------------------

- Minimum version of required Python is now 3.7. [#10900]

- Minimum version of required Numpy is now 1.17. [#10664]

- Minimum version of required Scipy is now 1.1. [#10900]

- Minimum version of required PyYAML is now 3.13. [#10900]

- Minimum version of required Matplotlib is now 3.0. [#10900]

- The private ``_erfa`` module has been converted to its own package,
  ``pyerfa``, which is a required dependency for astropy, and can be imported
  with ``import erfa``.  Importing ``_erfa`` from ``astropy`` will give a
  deprecation warning.  [#10329]

- Added ``optimize=True`` flag to calls of ``yacc.yacc`` (as already done for
  ``lex.lex``) to allow running in ``python -OO`` session without raising an
  exception in ``astropy.units.format``. [#10379]

- Shortened FITS comment strings for some D2IM and CPDIS FITS keywords to
  reduce the number of FITS ``VerifyWarning`` warnings when working with WCSes
  containing lookup table distortions. [#10513]

- When importing astropy without first building the extension modules first,
  raise an error directly instead of trying to auto-build. [#10883]



4.1 (2020-10-21)
================

New Features
------------

astropy.config
^^^^^^^^^^^^^^

- Add new function ``generate_config`` to generate the configuration file and
  include it in the documentation. [#10148]

- ``ConfigNamespace.__iter__`` and ``ConfigNamespace.keys`` now yield ``ConfigItem``
  names defined within it. Similarly, ``items`` and ``values`` would yield like a
  Python dictionary would. [#10139]

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- Added a new ``SpectralCoord`` class that can be used to define spectral
  coordinates and transform them between different velocity frames. [#10185]

- Angle parsing now supports ``cardinal direction`` in the cases
  where angles are initialized as ``string`` instances. eg ``"1753'27"W"``.[#9859]

- Allow in-place modification of array-valued ``Frame`` and ``SkyCoord`` objects.
  This provides limited support for updating coordinate data values from another
  coordinate object of the same class and equivalent frame attributes. [#9857]

- Added a robust equality operator for comparing ``SkyCoord``, frame, and
  representation objects. A comparison like ``sc1 == sc2`` will now return a
  boolean or boolean array where the objects are strictly equal in all relevant
  frame attributes and coordinate representation values. [#10154]

- Added the True Equator Mean Equinox (TEME) frame. [#10149]

- The ``Galactocentric`` frame will now use the "latest" parameter definitions
  by default. This currently corresponds to the values defined in v4.0, but will
  change with future releases. [#10238]

- The ``SkyCoord.from_name()`` and Sesame name resolving functionality now is
  able to cache results locally and will do so by default. [#9162]

- Allow in-place modification of array-valued ``Representation`` and ``Differential``
  objects, including of representations with attached differentials. [#10210]

astropy.io.ascii
^^^^^^^^^^^^^^^^

- Functional Units can now be processed in CDS-tables. [#9971]

- Allow reading in ASCII tables which have duplicate column names. [#9939]

- Fixed failure of ASCII ``fast_reader`` to handle ``names``, ``include_names``,
  ``exclude_names`` arguments for ``RDB`` formatted tables. Homogenised checks
  and exceptions for invalid ``names`` arguments. Improved performance when
  parsing "wide" tables with many columns. [#10306]

- Added type validation of key arguments in calls to ``io.ascii.read()`` and
  ``io.ascii.write()`` functions. [#10005]

astropy.io.misc
^^^^^^^^^^^^^^^
- Added serialization of parameter constraints fixed and bounds.  [#10082]

- Added 'functional_models.py' and 'physical_models.py' to asdf/tags/transform,
  with to allow serialization of all functional and physical models. [#10028, #10293]

- Fix ASDF serialization of circular model inverses, and remove explicit calls
  to ``asdf.yamlutil`` functions that became unnecessary in asdf 2.6.0. [#10189, #10384]

astropy.io.fits
^^^^^^^^^^^^^^^

- Added support for writing Dask arrays to disk efficiently for ``ImageHDU`` and
  ``PrimaryHDU``. [#9742]

- Add HDU name and ver to FITSDiff report where appropriate [#10197]

astropy.io.votable
^^^^^^^^^^^^^^^^^^

- New ``exceptions.conf.max_warnings`` configuration item to control the number of times a
  type of warning appears before being suppressed. [#10152]

- No longer ignore attributes whose values were specified as empty
  strings. [#10583]

astropy.modeling
^^^^^^^^^^^^^^^^
- Added Plummer1D model to ``functional_models``. [#9896]

- Added ``UnitsMapping`` model and ``Model.coerce_units`` to support units on otherwise
  unitless models. [#9936]

- Added ``domain`` and ``window`` attributes to ``repr`` and ``str``. Fixed bug with
  ``_format_repr`` in core.py. [#9941]

- Polynomial attributes ``domain`` and ``window`` are now tuples of size 2 and are
  validated. `repr` and `print` show only their non-default values. [#10145]

- Added ``replace_submodel()`` method to ``CompoundModel`` to modify an
  existing instance. [#10176]

- Delay construction of ``CompoundModel`` inverse until property is accessed,
  to support ASDF deserialization of circular inverses in component models. [#10384]

astropy.nddata
^^^^^^^^^^^^^^

- Added support in the ``bitmask`` module for using mnemonic bit flag names
  when specifying the bit flags to be used or ignored when converting a bit
  field to a boolean. [#10095, #10208]

- Added ``reshape_as_blocks`` function to reshape a data array into
  blocks, which is useful to efficiently apply functions on block
  subsets of the data instead of using loops.  The reshaped array is a
  view of the input data array. [#10214]

- Added a ``cache`` keyword option to allow caching for ``CCDData.read`` if
  filename is a URL. [#10265]

astropy.table
^^^^^^^^^^^^^

- Added ability to specify a custom matching function for table joins.  In
  particular this makes it possible to do cross-match table joins on ``SkyCoord``,
  ``Quantity``, or standard columns, where column entries within a specified
  distance are considered to be matched. [#10169]

- Added ``units`` and ``descriptions`` keyword arguments to the Table object
  initialization and ``Table.read()`` methods.  This allows directly setting
  the ``unit`` and ``description`` for the table columns at the time of
  creating or reading the table. [#9671]

- Make table ``Row`` work as mappings, by adding ``.keys()`` and ``.values()``
  methods. With this ``**row`` becomes possible, as does, more simply, turning
  a ``Row`` into a dictionary with ``dict(row)``. [#9712]

- Added two new ``Table`` methods ``.items()`` and ``.values()``, which return
  respectively ``tbl.columns.items()`` (iterator over name, column tuples)  and
  ``tbl.columns.values()`` (list of columns) for a ``Table`` object ``tbl``. [#9780]

- Added new ``Table`` method ``.round()``, which rounds numeric columns to the
  specified number of decimals. [#9862]

- Updated ``to_pandas()`` and ``from_pandas()`` to use and support Pandas
  nullable integer data type for masked integer data. [#9541]

- The HDF5 writer, ``write_table_hdf5()``, now allows passing through
  additional keyword arguments to the ``h5py.Group.create_dataset()``. [#9602]

- Added capability to add custom table attributes to a ``Table`` subclass.
  These attributes are persistent and can be set during table creation. [#10097]

- Added support for ``SkyCoord`` mixin columns in ``dstack``, ``vstack`` and
  ``insert_row`` functions. [#9857]

- Added support for coordinate ``Representation`` and ``Differential`` mixin
  columns. [#10210]

astropy.time
^^^^^^^^^^^^

- Added a new time format ``unix_tai`` which is essentially Unix time but with
  leap seconds included.  More precisely, this is the number of seconds since
  ``1970-01-01 00:00:08 TAI`` and corresponds to the ``CLOCK_TAI`` clock
  available on some linux platforms. [#10081]

astropy.units
^^^^^^^^^^^^^

- Added ``torr`` pressure unit. [#9787]

- Added the ``equal_nan`` keyword argument to ``isclose`` and ``allclose``, and
  updated the docstrings. [#9849]

- Added ``Rankine`` temperature unit. [#9916]

- Added integrated flux unit conversion to ``spectral_density`` equivalency.
  [#10015]

- Changed ``pixel_scale`` equivalency to allow scales defined in any unit.
  [#10123]

- The ``quantity_input`` decorator now optionally allows passing through
  numeric values or numpy arrays with numeric dtypes to arguments where
  ``dimensionless_unscaled`` is an allowed unit. [#10232]

astropy.utils
^^^^^^^^^^^^^

- Added a new ``MetaAttribute`` class to support easily adding custom attributes
  to a subclass of classes like ``Table`` or ``NDData`` that have a ``meta``
  attribute. [#10097]

astropy.visualization
^^^^^^^^^^^^^^^^^^^^^

- Added ``invalid`` keyword to ``SqrtStretch``, ``LogStretch``,
  ``PowerStretch``, and ``ImageNormalize`` classes and the
  ``simple_norm`` function.  This keyword is used to replace generated
  NaN values. [#10182]

- Fixed an issue where ticks were sometimes not drawn at the edges of a spherical
  projection on a WCSAxes. [#10442]

astropy.wcs
^^^^^^^^^^^

- WCS objects with a spectral axis will now return ``SpectralCoord``
  objects when calling ``pixel_to_world`` instead of ``Quantity``,
  and can now take either ``Quantity`` or ``SpectralCoord`` as input
  to ``pixel_to_world``. [#10185]

- Implemented support for the ``-TAB`` algorithm (WCS Paper III). [#9641]

- Added an ``_as_mpl_axes`` method to the ``HightLevelWCSWrapper`` class. [#10138]

- Add .upper() to ctype or ctype names to wcsapi/fitwcs.py to mitigate bugs from
  unintended lower/upper case issues [#10557]

API Changes
-----------

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- The equality operator for comparing ``SkyCoord``, frame, and representation
  objects was changed. A comparison like ``sc1 == sc2`` was previously
  equivalent to ``sc1 is sc2``. It will now return a boolean or boolean array
  where the objects are strictly equal in all relevant frame attributes and
  coordinate representation values. If the objects have different frame
  attributes or representation types then an exception will be raised. [#10154]

- ```SkyCoord.radial_velocity_correction``` now allows you to pass an ```obstime``` directly
  when the ```SkyCoord``` also has an ```obstime``` set. In this situation, the position of the
  ```SkyCoord``` has space motion applied to correct to the passed ```obstime```. This allows
  mm/s radial velocity precision for objects with large space motion. [#10094]

- For consistency with other astropy classes, coordinate ``Representations``
  and ``Differentials`` can now be initialized with an instance of their own class
  if that instance is passed in as the first argument. [#10210]

astropy.io.ascii
^^^^^^^^^^^^^^^^

- Changed the behavior when reading a table where both the ``names`` argument
  is provided (to specify the output column names) and the ``converters``
  argument is provided (to specify column conversion functions). Previously the
  ``converters`` dict names referred to the *input* table column names, but now
  they refer to the *output* table column names. [#9739]

astropy.io.votable
^^^^^^^^^^^^^^^^^^

- For FIELDs with datatype="char", store the values as strings instead
  of bytes. [#9505]

astropy.table
^^^^^^^^^^^^^

- ``Table.from_pandas`` now supports a ``units`` dictionary as argument to pass units
  for columns in the ``DataFrame``. [#9472]

astropy.time
^^^^^^^^^^^^

- Require that ``in_subfmt`` and ``out_subfmt`` properties of a ``Time`` object
  have allowed values at the time of being set, either when creating the object
  or when setting those properties on an existing ``Time`` instance.  Previously
  the validation of those properties was not strictly enforced. [#9868]

astropy.utils
^^^^^^^^^^^^^

- Changed the exception raised by ``get_readable_fileobj`` on missing
  compression modules (for ``bz2`` or ``lzma``/``xz`` support) to
  ``ModuleNotFoundError``, consistent with ``io.fits`` file handlers. [#9761]

astropy.visualization
^^^^^^^^^^^^^^^^^^^^^

- Deprecated the ``imshow_only_kwargs`` keyword in ``imshow_norm``.
  [#9915]

- Non-finite input values are now automatically excluded in
  ``HistEqStretch`` and ``InvertedHistEqStretch``. [#10177]

- The ``PowerDistStretch`` and ``InvertedPowerDistStretch`` ``a``
  value is restricted to be ``a >= 0`` in addition to ``a != 1``.
  [#10177]

- The ``PowerStretch``, ``LogStretch``, and ``InvertedLogStretch``
  ``a`` value is restricted to be ``a > 0``. [#10177]

- The ``AsinhStretch`` and ``SinhStretch`` ``a`` value is restricted
  to be ``0 < a <= 1``. [#10177]

Bug Fixes
---------

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- Fix a bug where for light deflection by the Sun it was always assumed that the
  source was at infinite distance, which in the (rare and) absolute worst-case
  scenario could lead to errors up to 3 arcsec. [#10666]

astropy.io.votable
^^^^^^^^^^^^^^^^^^

- For FIELDs with datatype="char", store the values as strings instead
  of bytes. [#9505]

astropy.table
^^^^^^^^^^^^^

- Fix a bug that prevented ``Time`` columns from being used to sort a table.
  [#10824]

astropy.wcs
^^^^^^^^^^^

- WCS objects with a spectral axis will now return ``SpectralCoord``
  objects when calling ``pixel_to_world`` instead of ``Quantity``
  (note that ``SpectralCoord`` is a sub-class of ``Quantity``). [#10185]

- Add .upper() to ctype or ctype names to wcsapi/fitwcs.py to mitigate bugs from
  unintended lower/upper case issues [#10557]

- Added bounds to ``fit_wcs_from_points`` to ensure CRPIX is on
  input image. [#10346]


Other Changes and Additions
---------------------------

- The way in which users can specify whether to build astropy against
  existing installations of C libraries rather than the bundled one
  has changed, and should now be done via environment variables rather
  than setup.py flags (e.g. --use-system-erfa). The available variables
  are ``ASTROPY_USE_SYSTEM_CFITSIO``, ``ASTROPY_USE_SYSTEM_ERFA``,
  ``ASTROPY_USE_SYSTEM_EXPAT``, ``ASTROPY_USE_SYSTEM_WCSLIB``, and
  ``ASTROPY_USE_SYSTEM_ALL``. These should be set to ``1`` to build
  against the system libraries. [#9730]

- The infrastructure of the package has been updated in line with the
  APE 17 roadmap (https://github.com/astropy/astropy-APEs/blob/master/APE17.rst).
  The main changes are that the ``python setup.py test`` and
  ``python setup.py build_docs`` commands will no longer work. The easiest
  way to replicate these commands is to install the tox
  (https://tox.readthedocs.io) package and run ``tox -e test`` and
  ``tox -e build_docs``. It is also possible to run pytest and sphinx
  directly. Other significant changes include switching to setuptools_scm to
  manage the version number, and adding a ``pyproject.toml`` to opt in to
  isolated builds as described in PEP 517/518. [#9726]

- Bundled ``expat`` is updated to version 2.2.9. [#10038]

- Increase minimum asdf version to 2.6.0. [#10189]

- The bundled version of PLY was updated to 3.11. [#10258]

- Removed dependency on scikit-image. [#10214]

4.0.5 (2021-03-26)
==================

Bug Fixes
---------

astropy.io.fits
^^^^^^^^^^^^^^^

- Fix bug where manual fixes to invalid header cards were not preserved when
  saving a FITS file. [#11108]

- Fix parsing of RVKC header card patterns that were not recognised
  where multiple spaces were separating field-specifier and value like
  "DP1.AXIS.1:   1". [#11301]

- Fix misleading missing END card error when extra data are found at the end
  of the file. [#11285]

- Fix incorrect wrapping of long card values as CONTINUE cards when some
  words in the value are longer than a single card. [#11304]

astropy.io.misc
^^^^^^^^^^^^^^^

- Fixed problem when writing serialized metadata to HDF5 using h5py >= 3.0.
  With the newer h5py this was writing the metadata table as a variable-length
  string array instead of the previous fixed-length bytes array. Fixed astropy
  to force using a fixed-length bytes array. [#11359]

astropy.modeling
^^^^^^^^^^^^^^^^

- Change ``Voigt1D`` function to use Humlicek's approximation to avoid serious
  inaccuracies + option to use (compiled) ``scipy.special.wofz`` error function
  for yet more accurate results. [#11177]

astropy.table
^^^^^^^^^^^^^

- Fixed bug when initializing a ``Table`` with a column as list of ``Quantity``,
  for example ``Table({'x': [1*u.m, 2*u.m]})``. Previously this resulted in an
  ``object`` dtype with no column ``unit`` set, but now gives a float array with
  the correct unit. [#11329]

- Fixed byteorder conversion in ``to_pandas()``, which had incorrectly
  triggered swapping when native endianness was stored with explicit
  ``dtype`` code ``'<'`` (or ``'>'``) instead of ``'='``. [#11288, #11294]

- Fixed a compatibility issue with numpy 1.21. Initializing a Table with a
  column like ``['str', np.ma.masked]`` was failing in tests due to a change in
  numpy. [#11364]

- Fixed bug when validating the inputs to ``table.hstack``, ``table.vstack``,
  and ``table.dstack``. Previously, mistakenly calling ``table.hstack(t1, t2)``
  (instead of ``table.hstack([t1, t2]))`` would return ``t1`` instead of raising
  an exception. [#11336]

- Fixed byteorder conversion in ``to_pandas()``, which had incorrectly
  triggered swapping when native endianness was stored with explicit
  ``dtype`` code ``'<'`` (or ``'>'``) instead of ``'='``. [#11288]

astropy.time
^^^^^^^^^^^^

- Fix leap second update when using a non english locale. [#11062]

- Fix default assumed location to be the geocenter when transforming times
  to and from solar-system barycenter scales. [#11134]

- Fix inability to write masked times with ``formatted_value``. [#11195]

astropy.units
^^^^^^^^^^^^^

- Ensure ``keepdims`` works for taking ``mean``, ``std``, and ``var`` of
  ``Quantity``. [#11198]

- For ``Quantity.to_string()``, ensure that the precision argument is also
  used when the format is not latex. [#11145]

astropy.wcs
^^^^^^^^^^^

- Allow "un-setting" of auxiliary WCS parameters in the ``aux`` attribute of
  ``Wcsprm``. [#11166]





4.0.4 (2020-11-24)
==================

Bug Fixes
---------

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- The ``norm()`` method for ``RadialDifferential`` no longer requires ``base``
  to be specified.  The ``norm()`` method for other non-Cartesian differential
  classes now gives a clearer error message if ``base`` is not specified. [#10969]

- The transformations between ``ICRS`` and any of the heliocentric ecliptic
  frames (``HeliocentricMeanEcliptic``, ``HeliocentricTrueEcliptic``, and
  ``HeliocentricEclipticIAU76``) now correctly account for the small motion of
  the Sun when transforming a coordinate with velocity information. [#10970]

astropy.io.ascii
^^^^^^^^^^^^^^^^

- Partially fixed a performance issue when reading in parallel mode. Parallel
  reading currently has substantially worse performance than the default serial
  reading, so we now ignore the parallel option and fall back to serial reading.
  [#10880]

- Fixed a bug where "" (blank string) as input data for a boolean type column
  was causing an exception instead of indicating a masked value. As a
  consequence of the fix, the values "0" and "1" are now also allowed as valid
  inputs for boolean type columns. These new allowed values apply for both ECSV
  and for basic character-delimited data files ('basic' format with appropriate
  ``converters`` specified). [#10995]

astropy.modeling
^^^^^^^^^^^^^^^^

- Fixed use of weights with ``LinearLSQFitter``. [#10687]

astropy.stats
^^^^^^^^^^^^^

- Fixed an issue in biweight stats when MAD=0 to give the same output
  with and without an input ``axis``. [#10912]

astropy.time
^^^^^^^^^^^^

- Fix a problem with the ``plot_date`` format for matplotlib >= 3.3 caused by
  a change in the matplotlib plot date default reference epoch in that release.
  [#10876]

- Improve initialization time by a factor of four when creating a scalar ``Time``
  object in a format like ``unix`` or ``cxcsec`` (time delta from a reference
  epoch time). [#10406]

astropy.visualization
^^^^^^^^^^^^^^^^^^^^^

- Fixed the calculation of the tight bounding box of a ``WCSAxes``. This should
  also significantly improve the application of ``tight_layout()`` to figures
  containing ``WCSAxes``. [#10797]


4.0.3 (2020-10-14)
==================

Bug Fixes
---------

astropy.table
^^^^^^^^^^^^^

- Fixed a small bug where initializing an empty ``Column`` with a structured dtype
  with a length and a shape failed to give the requested dtype. [#10819]

Other Changes and Additions
---------------------------

- Fixed installation of the source distribution with pip<19. [#10837, #10852]


4.0.2 (2020-10-10)
==================

New Features
------------

astropy.utils
^^^^^^^^^^^^^

- ``astropy.utils.data.download_file`` now supports FTPS/FTP over TLS. [#9964]

- ``astropy.utils.data`` now uses a lock-free mechanism for caching. This new
  mechanism uses a new cache layout and so ignores caches created using earlier
  mechanisms (which were causing lockups on clusters). The two cache formats can
  coexist but do not share any files. [#10437, #10683]

- ``astropy.utils.data`` now ignores the config item
  ``astropy.utils.data.conf.download_cache_lock_attempts`` since no locking is
  done. [#10437, #10683]

- ``astropy.utils.data.download_file`` and related functions now interpret the
  parameter or config file setting ``timeout=0`` to mean they should make no
  attempt to download files. [#10437, #10683]

- ``astropy.utils.import_file_to_cache`` now accepts a keyword-only argument
  ``replace``, defaulting to True, to determine whether it should replace existing
  files in the cache, in a way as close to atomic as possible. [#10437, #10683]

- ``astropy.utils.data.download_file`` and related functions now treat
  ``http://example.com`` and ``http://example.com/`` as equivalent. [#10631]

astropy.wcs
^^^^^^^^^^^

- The new auxiliary WCS parameters added in WCSLIB 7.1 are now exposed as
  the ``aux`` attribute of ``Wcsprm``. [#10333]

- Updated bundled version of ``WCSLIB`` to v7.3. [#10433]


Bug fixes
---------

astropy.config
^^^^^^^^^^^^^^

- Added an extra fallback to ``os.expanduser('~')`` when trying to find the
  user home directory. [#10570]

astropy.constants
^^^^^^^^^^^^^^^^^

- Corrected definition of parsec to 648 000 / pi AU following IAU 2015 B2 [#10569]

astropy.convolution
^^^^^^^^^^^^^^^^^^^

- Fixed a bug where a float-typed integers in the argument ``x_range`` of
  ``astropy.convolution.utils.discretize_oversample_1D`` (and the 2D version as
  well) fails because it uses ``numpy.linspace``, which requires an ``int``.
  [#10696]

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- Ensure that for size-1 array ``SkyCoord`` and coordinate frames
  the attributes also properly become scalars when indexed with 0.
  [#10113]

- Fixed a bug where ``SkyCoord.separation()`` and ``SkyCoord.separation_3d``
  were not accepting a frame object. [#10332]

- Ensure that the ``lon`` values in ``SkyOffsetFrame`` are wrapped correctly at
  180 degree regardless of how the underlying data is represented. [#10163]

- Fixed an error in the obliquity of the ecliptic when transforming to/from the
  ``*TrueEcliptic`` coordinate frames. The error would primarily result in an
  inaccuracy in the ecliptic latitude on the order of arcseconds. [#10129]

- Fixed an error in the computation of the location of solar system bodies where the
  Earth location of the observer was ignored during the correction for light travel
  time. [#10292]

- Ensure that coordinates with proper motion that are transformed to other
  coordinate frames still can be represented properly. [#10276]

- Improve the error message given when trying to get a cartesian representation
  for coordinates that have both proper motion and radial velocity, but no
  distance. [#10276]

- Fixed an error where ``SkyCoord.apply_space_motion`` would return incorrect
  results when no distance is set and proper motion is high. [#10296]

- Make the parsing of angles thread-safe so that ``Angle`` can be used in
  Python multithreading. [#10556]

- Fixed reporting of ``EarthLocation.info`` which previously raised an exception.
  [#10592]

astropy.io.ascii
^^^^^^^^^^^^^^^^

- Fixed a bug with the C ``fast_reader`` not correctly parsing newlines when
  ``delimiter`` was also set to ``\n`` or ``\r``; ensured consistent handling
  of input strings without newline characters. [#9929]

astropy.io.fits
^^^^^^^^^^^^^^^

- Fix integer formats of ``TFORMn=Iw`` columns in ASCII tables to correctly read
  values exceeding int32 - setting int16, int32 or int64 according to ``w``. [#9901]

- Fix unclosed memory-mapped FITS files in ``FITSDiff`` when difference found.
  [#10159]

- Fix crash when reading an invalid table file. [#10171]

- Fix duplication issue when setting a keyword ending with space. [#10482]

- Fix ResourceWarning with ``fits.writeto`` and ``pathlib.Path`` object.
  [#10599]

- Fix repr for commentary cards and strip spaces for commentary keywords.
  [#10640]

- Fix compilation of cfitsio with Xcode 12. [#10772]

- Fix handling of 1-dimensional arrays with a single element in ``BinTableHDU`` [#10768]

astropy.io.misc
^^^^^^^^^^^^^^^

- Fix id URL in ``baseframe-1.0.0`` ASDF schema. [#10223]

- Write keys to ASDF only if the value is present, to account
  for a change in behavior in asdf 2.8. [#10674]

astropy.io.registry
^^^^^^^^^^^^^^^^^^^

- Fix ``Table.(read|write).help`` when reader or writer has no docstring. [#10460]

astropy.io.votable
^^^^^^^^^^^^^^^^^^

- Fixed parsing failure of VOTable with no fields. When detecting a non-empty
  table with no fields, the following warning/exception is issued:
  E25 "No FIELDs are defined; DATA section will be ignored." [#10192]

astropy.modeling
^^^^^^^^^^^^^^^^

- Fixed a problem with mapping ``input_units`` and ``return_units``
  of a ``CompoundModel`` to the units of the constituent models. [#10158]

- Removed hard-coded names of inputs and outputs. [#10174]

- Fixed a problem where slicing a ``CompoundModel`` by name will crash if
  there ``fix_inputs`` operators are present. [#10224]

- Removed a limitation of fitting of data with units with compound models
  without units when the expression involves operators other than addition
  and subtraction. [#10415]

- Fixed a problem with fitting ``Linear1D`` and ``Planar2D`` in model sets. [#10623]

- Fixed reported module name of ``math_functions`` model classes. [#10694]

- Fixed reported module name of ``tabular`` model classes. [#10709]

- Do not create new ``math_functions`` models for ufuncs that are
  only aliases (divide and mod). [#10697]

- Fix calculation of the ``Moffat2D`` derivative with respect to gamma. [#10784]

astropy.stats
^^^^^^^^^^^^^

- Fixed an API regression where ``SigmaClip.__call__`` would convert masked
  elements to ``nan`` and upcast the dtype to ``float64`` in its output
  ``MaskedArray`` when using the ``axis`` parameter along with the defaults
  ``masked=True`` and ``copy=True``. [#10610]

- Fixed an issue where fully masked ``MaskedArray`` input to
  ``sigma_clipped_stats`` gave incorrect results. [#10099]

- Fixed an issue where ``sigma_clip`` and ``SigmaClip.__call__``
  would return a masked array instead of a ``ndarray`` when
  ``masked=False`` and the input was a full-masked ``MaskedArray``.
  [#10099]

- Fixed bug with ``funcs.poisson_conf_interval`` where an integer for N
  with ``interval='kraft-burrows-nousek'`` would throw an error with
  mpmath backend. [#10427]

- Fixed bug in ``funcs.poisson_conf_interval`` with
  ``interval='kraft-burrows-nousek'`` where certain combinations of source
  and background count numbers led to ``ValueError`` due to the choice of
  starting value for numerical optimization. [#10618]

astropy.table
^^^^^^^^^^^^^

- Fixed a bug when writing a table with mixin columns to FITS, ECSV or HDF5.
  If one of the data attributes of the mixin (e.g. ``skycoord.ra``) had the
  same name as one of the table column names (``ra``), the column (``ra``)
  would be dropped when reading the table back. [#10222]

- Fixed a bug when sorting an indexed table on the indexed column after first
  sorting on another column. [#10103]

- Fixed a bug in table argsort when called with ``reverse=True`` for an
  indexed table. [#10103]

- Fixed a performance regression introduced in #9048 when initializing a table
  from Python lists. Also fixed incorrect behavior (for data types other than
  float) when those lists contain ``np.ma.masked`` elements to indicate masked
  data. [#10636]

- Avoid modifying ``.meta`` when serializing columns to FITS. [#10485]

- Avoid crash when reading a FITS table that contains mixin info and PyYAML
  is missing. [#10485]

astropy.time
^^^^^^^^^^^^

- Ensure that for size-1 array ``Time``, the location also properly becomes
  a scalar when indexed with 0. [#10113]

astropy.units
^^^^^^^^^^^^^

- Refined test_parallax to resolve difference between 2012 and 2015 definitions. [#10569]

astropy.utils
^^^^^^^^^^^^^

- The default IERS server has been updated to use the FTPS server hosted by
  CDDIS. [#9964]

- Fixed memory allocation on 64-bit systems within ``xml.iterparse`` [#10076]

- Fix case where ``None`` could be used in a numerical computation. [#10126]

astropy.visualization
^^^^^^^^^^^^^^^^^^^^^

- Fixed a bug where the ``ImageNormalize`` ``clip`` keyword was
  ignored when used with calling the object on data. [#10098]

- Fixed a bug where ``axes.xlabel``/``axes.ylabel`` where not correctly set
  nor returned on an ``EllipticalFrame`` class ``WCSAxes`` plot. [#10446]

astropy.wcs
^^^^^^^^^^^

- Handled WCS 360 -> 0 deg crossover in ``fit_wcs_from_points`` [#10155]

- Do not issue ``DATREF`` warning when ``MJDREF`` has default value. [#10440]

- Fixed a bug due to which ``naxis`` argument was ignored if ``header``
  was supplied during the initialization of a WCS object. [#10532]

Other Changes and Additions
---------------------------

- Improved the speed of sorting a large ``Table`` on a single column by a factor
  of around 5. [#10103]

- Ensure that astropy can be used inside Application bundles built with
  pyinstaller. [#8795]

- Updated the bundled CFITSIO library to 3.49. See
  ``cextern/cfitsio/docs/changes.txt`` for additional information.
  [#10256, #10665]

- ``extract_array`` raises a ``ValueError`` if the data type of the
  input array is inconsistent with the ``fill_value``. [#10602]


4.0.1 (2020-03-27)
==================

Bug fixes
---------

astropy.config
^^^^^^^^^^^^^^

- Fixed a bug where importing a development version of a package that uses
  ``astropy`` configuration system can result in a
  ``~/.astropy/config/package..cfg`` file. [#9975]

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- Fixed a bug where a vestigal trace of a frame class could persist in the
  transformation graph even after the removal of all transformations involving
  that frame class. [#9815]

- Fixed a bug with ``TransformGraph.remove_transform()`` when the "from" and
  "to" frame classes are not explicitly specified. [#9815]

- Read-only longitudes can now be passed in to ``EarthLocation`` even if
  they include angles outside of the range of -180 to 180 degrees. [#9900]

- ```SkyCoord.radial_velocity_correction``` no longer raises an Exception
  when space motion information is present on the SkyCoord. [#9980]

astropy.io
^^^^^^^^^^

- Fixed a bug that prevented the unified I/O infrastructure from working with
  datasets that are represented by directories rather than files. [#9866]

astropy.io.ascii
^^^^^^^^^^^^^^^^

- Fixed a bug in the ``fast_reader`` C parsers incorrectly returning entries
  of isolated positive/negative signs as ``float`` instead of ``str``. [#9918]

- Fixed a segmentation fault in the ``fast_reader`` C parsers when parsing an
  invalid file with ``guess=True`` and the file contains inconsistent column
  numbers in combination with a quoted field; e.g., ``"1  2\n 3  4 '5'"``.
  [#9923]

- Magnitude, decibel, and dex can now be stored in ``ecsv`` files. [#9933]

astropy.io.misc
^^^^^^^^^^^^^^^

- Magnitude, decibel, and dex can now be stored in ``hdf5`` files. [#9933]

- Fixed serialization of polynomial models to include non default values of
  domain and window values. [#9956, #9961]

- Fixed a bug which affected overwriting tables within ``hdf5`` files.
  Overwriting an existing path with associated column meta data now also
  overwrites the meta data associated with the table. [#9950]

- Fixed serialization of Time objects with location under time-1.0.0
  ASDF schema. [#9983]

astropy.io.fits
^^^^^^^^^^^^^^^

- Fix regression with ``GroupsHDU`` which needs to modify the header to handle
  invalid headers, and fix accessing ``.data`` for empty HDU. [#9711, #9934]

- Fix ``fitsdiff`` when its arguments are directories that contain other
  directories. [#9711]

- Fix writing noncontiguous data to a compressed HDU. [#9958]

- Added verification of ``disp`` (``TDISP``) keyword to ``fits.Column`` and
  extended tests for ``TFORM`` and ``TDISP`` validation. [#9978]

- Fix checksum verification to process all HDUs instead of only the first one
  because of the lazy loading feature. [#10012]

- Allow passing ``output_verify`` to ``.close`` when using the context manager.
  [#10030]

- Prevent instantiation of ``PrimaryHDU`` and ``ImageHDU`` with a scalar.
  [#10041]

- Fix column access by attribute with FITS_rec: columns with scaling or columns
  from ASCII tables where not properly converted when accessed by attribute
  name. [#10069]

astropy.io.misc
^^^^^^^^^^^^^^^

- Magnitude, decibel, and dex can now be stored in ``hdf5`` files. [#9933]

- Fixed serialization of polynomial models to include non default values of
  domain and window values. [#9956, #9961]

- Fixed a bug which affected overwriting tables within ``hdf5`` files.
  Overwriting an existing path with associated column meta data now also
  overwrites the meta data associated with the table. [#9950]

- Fixed serialization of Time objects with location under time-1.0.0
  ASDF schema. [#9983]

astropy.modeling
^^^^^^^^^^^^^^^^

- Fixed a bug in setting default values of parameters of orthonormal
  polynomials when constructing a model set. [#9987]

astropy.table
^^^^^^^^^^^^^

- Fixed bug in ``Table.reverse`` for tables that contain non-mutable mixin columns
  (like ``SkyCoord``) for which in-place item update is not allowed. [#9839]

- Tables containing Magnitude, decibel, and dex columns can now be saved to
  ``ecsv`` files. [#9933]

- Fixed bug where adding or inserting a row fails on a table with an index
  defined on a column that is not the first one. [#10027]

- Ensured that ``table.show_in_browser`` also worked for mixin columns like
  ``Time`` and ``SkyCoord``. [#10068]

astropy.time
^^^^^^^^^^^^

- Fix inaccuracy when converting between TimeDelta and datetime.timedelta. [#9679]

- Fixed exception when changing ``format`` in the case when ``out_subfmt`` is
  defined and is incompatible with the new format. [#9812]

- Fixed exceptions in ``Time.to_value()``: when supplying any ``subfmt`` argument
  for string-based formats like 'iso', and for ``subfmt='long'`` for the formats
  'byear', 'jyear', and 'decimalyear'. [#9812]

- Fixed bug where the location attribute was lost when creating a new ``Time``
  object from an existing ``Time`` or list of ``Time`` objects. [#9969]

- Fixed a bug where an exception occurred when creating a ``Time`` object
  if the ``val1`` argument was a regular double and the ``val2`` argument
  was a ``longdouble``. [#10034]

astropy.timeseries
^^^^^^^^^^^^^^^^^^

- Fixed issue with reference time for the ``transit_time`` parameter returned by
  the ``BoxLeastSquares`` periodogram. Now, the ``transit_time`` will be within
  the range of the input data and arbitrary time offsets/zero points no longer
  affect results. [#10013]

astropy.units
^^^^^^^^^^^^^

- Fix for ``quantity_input`` annotation raising an exception on iterable
  types that don't define a general ``__contains__`` for checking if ``None``
  is contained (e.g. Enum as of python3.8), by instead checking for instance of
  Sequence. [#9948]

- Fix for ``u.Quantity`` not taking into account ``ndmin`` if constructed from
  another ``u.Quantity`` instance with different but convertible unit [#10066]

astropy.utils
^^^^^^^^^^^^^

- Fixed ``deprecated_renamed_argument`` not passing in user value to
  deprecated keyword when the keyword has no new name. [#9981]

- Fixed ``deprecated_renamed_argument`` not issuing a deprecation warning when
  deprecated keyword without new name is passed in as positional argument.
  [#9985]

- Fixed detection of read-only filesystems in the caching code. [#10007]

astropy.visualization
^^^^^^^^^^^^^^^^^^^^^

- Fixed bug from matplotlib >=3.1 where an empty Quantity array is
  sent for unit conversion as an empty list. [#9848]

- Fix bug in ``ZScaleInterval`` to return the array minimum and
  maximum when there are less then ``min_npixels`` in the input array. [#9913]

- Fix a bug in simplifying axis labels that affected non-rectangular frames.
  [#8004, #9991]


Other Changes and Additions
---------------------------

- Increase minimum asdf version to 2.5.2. [#9996, #9819]

- Updated bundled version of ``WCSLIB`` to v7.2. [#10021]



4.0 (2019-12-16)
================

New Features
------------

astropy.config
^^^^^^^^^^^^^^

- The config and cache directories and the name of the config file are now
  customizable. This allows affiliated packages to put their configuration
  files in locations other than ``CONFIG_DIR/.astropy/``. [#8237]

astropy.constants
^^^^^^^^^^^^^^^^^

- The version of constants can be specified via ScienceState in a way
  that ``constants`` and ``units`` will be consistent. [#8517]

- Default constants now use CODATA 2018 and IAU 2015 definitions. [#8761]

- Constants can be pickled and unpickled. [#9377]

astropy.convolution
^^^^^^^^^^^^^^^^^^^

- Fixed a bug [#9168] where having a kernel defined using unitless astropy
  quantity objects would result in a crash [#9300]

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- Changed ``coordinates.solar_system_ephemeris`` to also accept local files
  as input. The ephemeris can now be selected by either keyword (e.g. 'jpl',
  'de430'), URL or file path. [#8767]

- Added a ``cylindrical`` property to ``SkyCoord`` for shorthand access to a
  ``CylindricalRepresentation`` of the coordinate, as is already available
  for other common representations. [#8857]

- The default parameters for the ``Galactocentric`` frame are now controlled by
  a ``ScienceState`` subclass, ``galactocentric_frame_defaults``. New
  parameter sets will be added to this object periodically to keep up with
  ever-improved measurements of the solar position and motion. [#9346]

- Coordinate frame classes can now have multiple aliases by assigning a list
  of aliases to the class variable ``name``.  Any of the aliases can be used
  for attribute-style access or as the target of ``tranform_to()`` calls.
  [#8834]

- Passing a NaN to ``Distance`` no longer raises a warning. [#9598]

astropy.cosmology
^^^^^^^^^^^^^^^^^

- The pre-publication Planck 2018 cosmological parameters are included as the
  ``Planck2018_arXiv_v2`` object.  Please note that the values are preliminary,
  and when the paper is accepted a final version will be included as
  ``Planck18``. [#8111]

astropy.io.ascii
^^^^^^^^^^^^^^^^

- Removed incorrect warnings on ``Overflow`` when reading in
  ``FloatType`` 0.0 with ``use_fast_converter``; synchronised
  ``IntType`` ``Overflow`` warning messages. [#9082]

astropy.io.misc
^^^^^^^^^^^^^^^

- Eliminate deprecated compatibility mode when writing ``Table`` metadata to
  HDF5 format. [#8899]

- Add support for orthogonal polynomial models to ASDF. [#9107]

astropy.io.fits
^^^^^^^^^^^^^^^

- Changed the ``fitscheck`` and ``fitsdiff`` script to use the ``argparse``
  module instead of ``optparse``. [#9148]

- Allow writing of ``Table`` objects with ``Time`` columns that are also table
  indices to FITS files. [#8077]

astropy.io.votable
^^^^^^^^^^^^^^^^^^

- Support VOTable version 1.4.  The main addition is the new element, TIMESYS,
  which allows defining of metadata for temporal coordinates much like COOSYS
  defines metadata for celestial coordinates. [#9475]

astropy.logger
^^^^^^^^^^^^^^

- Added a configuration option to specify the text encoding of the log file,
  with the default behavior being the platform-preferred encoding. [#9203]

astropy.modeling
^^^^^^^^^^^^^^^^

- Major rework of modeling internals. `See modeling documentation for details.
  <https://docs.astropy.org/en/v4.0.x/modeling/changes_for_4.html>`_ . [#8769]

- Add ``Tabular1D.inverse``. [#9083]

- ``Model.rename`` was changed to add the ability to rename ``Model.inputs``
  and ``Model.outputs``. [#9220]

- New function ``fix_inputs`` to generate new models from others by fixing
  specific inputs variable values to constants. [#9135]

- ``inputs`` and ``outputs`` are now model instance attributes, and ``n_inputs``
  and ``n_outputs`` are class attributes. Backwards compatible default
  values of ``inputs`` and ``outputs`` are generated. ``Model.inputs`` and
  ``Model.outputs`` are now settable which allows renaming them on per user
  case. [#9298]

- Add a new model representing a sequence of rotations in 3D around an
  arbitrary number of axes. [#9369]

- Add many of the numpy ufunc functions as models. [#9401]

- Add ``BlackBody`` model. [#9282]

- Add ``Drude1D`` model. [#9452]

- Added analytical King model (KingProjectedAnalytic1D). [#9084]

- Added Exponential1D and Logarithmic1D models. [#9351]

astropy.nddata
^^^^^^^^^^^^^^

- Add a way for technically invalid but unambiguous units in a fits header
  to be parsed by ``CCDData``. [#9397]

- ``NDData`` now only accepts WCS objects which implement either the high, or
  low level APE 14 WCS API. All WCS objects are converted to a high level WCS
  object, so ``NDData.wcs`` now always returns a high level APE 14 object. Not
  all array slices are valid for wcs objects, so some slicing operations which
  used to work may now fail. [#9067]

astropy.stats
^^^^^^^^^^^^^

- The ``biweight_location``, ``biweight_scale``, and
  ``biweight_midvariance`` functions now allow for the ``axis``
  keyword to be a tuple of integers. [#9309]

- Added an ``ignore_nan`` option to the ``biweight_location``,
  ``biweight_scale``, and ``biweight_midvariance`` functions. [#9457]

- A numpy ``MaskedArray`` can now be input to the ``biweight_location``,
  ``biweight_scale``, and ``biweight_midvariance`` functions. [#9466]

- Removed the warning related to p0 in the Bayesian blocks algorithm. The
  caveat related to p0 is described in the docstring for ``Events``. [#9567]

astropy.table
^^^^^^^^^^^^^

- Improved the implementation of ``Table.replace_column()`` to provide
  a speed-up of 5 to 10 times for wide tables.  The method can now accept
  any input which convertible to a column of the correct length, not just
  ``Column`` subclasses. [#8902]

- Improved the implementation of ``Table.add_column()`` to provide a speed-up
  of 2 to 10 (or more) when adding a column to tables, with increasing benefit
  as the number of columns increases.  The method can now accept any input
  which is convertible to a column of the correct length, not just ``Column``
  subclasses. [#8933]

- Changed the implementation of ``Table.add_columns()`` to use the new
  ``Table.add_column()`` method.  In most cases the performance is similar
  or slightly faster to the previous implementation. [#8933]

- ``MaskedColumn.data`` will now return a plain ``MaskedArray`` rather than
  the previous (unintended) ``masked_BaseColumn``. [#8855]

- Added depth-wise stacking ``dstack()`` in higher level table operation.
  It help will in stacking table column depth-wise. [#8939]

- Added a new table equality method ``values_equal()`` which allows comparison
  table values to another table, list, or value, and returns an
  element-by-element equality table. [#9068]

- Added new ``join_type='cartesian'`` option to the ``join`` operation. [#9288]

- Allow adding a table column as a list of mixin-type objects, for instance
  ``t['q'] = [1 * u.m, 2 * u.m]``. [#9165]

- Allow table ``join()`` using any sortable key column (e.g. Time), not
  just ndarray subclasses. A column is considered sortable if there is a
  ``<column>.info.get_sortable_arrays()`` method that is implemented. [#9340]

- Added ``Table.iterrows()`` for making row-wise iteration faster. [#8969]

- Allow table to be initialized with a list of dict where the dict keys
  are not the same in every row. The table column names are the set of all keys
  found in the input data, and any missing key/value pairs are turned into
  missing data in the table. [#9425]

- Prevent unnecessary ERFA warnings when indexing by ``Time`` columns. [#9545]

- Added support for sorting tables which contain non-mutable mixin columns
  (like ``SkyCoord``) for which in-place item update is not allowed. [#9549]

- Ensured that inserting ``np.ma.masked`` (or any other value with a mask) into
  a ``MaskedColumn`` causes a masked entry to be inserted. [#9623]

- Fixed a bug that caused an exception when initializing a ``MaskedColumn`` from
  another ``MaskedColumn`` that has a structured dtype. [#9651]

astropy.tests
^^^^^^^^^^^^^

- The plugin that handles the custom header in the test output has been
  moved to the ``pytest-astropy-header plugin`` package. `See the README at
  <https://github.com/astropy/pytest-astropy-header>`__ for information about
  using this new plugin. [#9214]

astropy.time
^^^^^^^^^^^^

- Added a new time format ``ymdhms`` for representing times via year, month,
  day, hour, minute, and second attributes. [#7644]

- ``TimeDelta`` gained a ``to_value`` method, so that it becomes easier to
  use it wherever a ``Quantity`` with units of time could be used. [#8762]

- Made scalar ``Time`` and ``TimeDelta`` objects hashable based on JD, time
  scale, and location attributes. [#8912]

- Improved error message when bad input is used to initialize a ``Time`` or
  ``TimeDelta`` object and the format is specified. [#9296]

- Allow numeric time formats to be initialized with numpy ``longdouble``,
  ``Decimal`` instances, and strings.  One can select just one of these
  using ``in_subfmt``.  The output can be similarly set using ``out_subfmt``.
  [#9361]

- Introduce a new ``.to_value()`` method for ``Time`` (and adjusted the
  existing method for ``TimeDelta``) so that one can get values in a given
  ``format`` and possible ``subfmt`` (e.g., ``to_value('mjd', 'str')``. [#9361]

- Prevent unnecessary ERFA warnings when sorting ``Time`` objects. [#9545]

astropy.timeseries
^^^^^^^^^^^^^^^^^^

- Adding ``epoch_phase``, ``wrap_phase`` and ``normalize_phase`` keywords to
  ``TimeSeries.fold()`` to control the phase of the epoch and to return
  normalized phase rather than time for the folded TimeSeries. [#9455]

astropy.uncertainty
^^^^^^^^^^^^^^^^^^^

- ``Distribution`` was rewritten such that it deals better with subclasses.
  As a result, Quantity distributions now behave correctly with ``to`` methods
  yielding new distributions of the kind expected for the starting
  distribution, and ``to_value`` yielding ``NdarrayDistribution`` instances.
  [#9429, #9442]

- The ``pdf_*`` properties that were used to calculate statistical properties
  of ``Distrubution`` instances were changed into methods. This allows one
  to pass parameters such as ``ddof`` to ``pdf_std`` and ``pdf_var`` (which
  generally should equal 1 instead of the default 0), and reflects that these
  are fairly involved calculations, not just "properties". [#9613]

astropy.units
^^^^^^^^^^^^^

- Support for unicode parsing. Currently supported are superscripts, Ohm,
  ngstrm, and the micro-sign. [#9348]

- Accept non-unit type annotations in @quantity_input. [#8984]

- For numpy 1.17 and later, the new ``__array_function__`` protocol is used to
  ensure that all top-level numpy functions interact properly with
  ``Quantity``, preserving units also in operations like ``np.concatenate``.
  [#8808]

- Add equivalencies for surface brightness units to spectral_density. [#9282]

astropy.utils
^^^^^^^^^^^^^

- ``astropy.utils.data.download_file`` and
  ``astropy.utils.data.get_readable_fileobj`` now provides an ``http_headers``
  keyword to pass in specific request headers for the download. It also now
  defaults to providing ``User-Agent: Astropy`` and ``Accept: */*``
  headers. The default ``User-Agent`` value can be set with a new
  ``astropy.data.conf.default_http_user_agent`` configuration item.
  [#9508, #9564]

- Added a new ``astropy.utils.misc.unbroadcast`` function which can be used
  to return the smallest array that can be broadcasted back to the initial
  array. [#9209]

- The specific IERS Earth rotation parameter table used for time and
  coordinate transformations can now be set, either in a context or per
  session, using ``astropy.utils.iers.earth_rotation_table``. [#9244]

- Added ``export_cache`` and ``import_cache`` to permit transporting
  downloaded data to machines with no Internet connection. Several new
  functions are available to investigate the cache contents; e.g.,
  ``check_download_cache`` can be used to confirm that the persistent
  cache has not become damaged. [#9182]

- A new ``astropy.utils.iers.LeapSeconds`` class has been added to track
  leap seconds. [#9365]

astropy.visualization
^^^^^^^^^^^^^^^^^^^^^

- Added a new ``time_support`` context manager/function for making it easy to
  plot and format ``Time`` objects in Matplotlib. [#8782]

- Added support for plotting any WCS compliant with the generalized (APE 14)
  WCS API with WCSAxes. [#8885, #9098]

- Improved display of information when inspecting ``WCSAxes.coords``. [#9098]

- Improved error checking for the ``slices=`` argument to ``WCSAxes``. [#9098]

- Added support for more solar frames in WCSAxes. [#9275]

- Add support for one dimensional plots to ``WCSAxes``. [#9266]

- Add a ``get_format_unit`` to ``wcsaxes.CoordinateHelper``. [#9392]

- ``WCSAxes`` now, by default, sets a default label for plot axes which is the
  WCS physical type (and unit) for that axis. This can be disabled using the
  ``coords[i].set_auto_axislabel(False)`` or by explicitly setting an axis
  label. [#9392]

- Fixed the display of tick labels when plotting all sky images that have a
  coord_wrap less than 360. [#9542]

astropy.wcs
^^^^^^^^^^^

- Added a ``astropy.wcs.wcsapi.pixel_to_pixel`` function that can be used to
  transform pixel coordinates in one dataset with a WCS to pixel coordinates
  in another dataset with a different WCS. This function is designed to be
  efficient when the input arrays are broadcasted views of smaller
  arrays. [#9209]

- Added a ``local_partial_pixel_derivatives`` function that can be used to
  determine a matrix of partial derivatives of each world coordinate with
  respect to each pixel coordinate. [#9392]

- Updated wcslib to v6.4. [#9125]

- Improved the  ``SlicedLowLevelWCS`` class in ``astropy.wcs.wcsapi`` to avoid
  storing chains of nested ``SlicedLowLevelWCS`` objects when applying multiple
  slicing operations in turn. [#9210]

- Added a ``wcs_info_str`` function to ``astropy.wcs.wcsapi`` to show a summary
  of an APE-14-compliant WCS as a string. [#8546, #9207]

- Added two new optional attributes to the APE 14 low-level WCS:
  ``pixel_axis_names`` and ``world_axis_names``. [#9156]

- Updated the WCS class to now correctly take and return ``Time`` objects in
  the high-level APE 14 API (e.g. ``pixel_to_world``. [#9376]

- ``SlicedLowLevelWCS`` now raises ``IndexError`` rather than ``ValueError`` on
  an invalid slice. [#9067]

- Added ``fit_wcs_from_points`` function to ``astropy.wcs.utils``. Fits a WCS
  object to set of matched detector/sky coordinates. [#9469]

- Fix various bugs in ``SlicedLowLevelWCS`` when the WCS being sliced was one
  dimensional. [#9693]


API Changes
-----------

astropy.constants
^^^^^^^^^^^^^^^^^

- Deprecated ``set_enabled_constants`` context manager. Use
  ``astropy.physical_constants`` and ``astropy.astronomical_constants``.
  [#9025]

astropy.convolution
^^^^^^^^^^^^^^^^^^^

- Removed the deprecated keyword argument ``interpolate_nan`` from
  ``convolve_fft``. [#9356]

- Removed the deprecated keyword argument ``stddev`` from
  ``Gaussian2DKernel``. [#9356]

- Deprecated and renamed ``MexicanHat1DKernel`` and ``MexicanHat2DKernel``
  to ``RickerWavelet1DKernel`` and ``RickerWavelet2DKernel``. [#9445]

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- Removed the ``recommended_units`` attribute from Representations; it was
  deprecated since 3.0. [#8892]

- Removed the deprecated frame attribute classes, ``FrameAttribute``,
  ``TimeFrameAttribute``, ``QuantityFrameAttribute``,
  ``CartesianRepresentationFrameAttribute``; deprecated since 3.0. [#9326]

- Removed ``longitude`` and ``latitude`` attributes from ``EarthLocation``;
  deprecated since 2.0. [#9326]

- The ``DifferentialAttribute`` for frame classes now passes through any input
  to the ``allowed_classes`` if only one allowed class is specified, i.e. this
  now allows passing a quantity in for frame attributes that use
  ``DifferentialAttribute``. [#9325]

- Removed the deprecated ``galcen_ra`` and ``galcen_dec`` attributes from the
  ``Galactocentric`` frame. [#9346]

astropy.extern
^^^^^^^^^^^^^^

- Remove the bundled ``six`` module. [#8315]

astropy.io.ascii
^^^^^^^^^^^^^^^^

- Masked column handling has changed, see ``astropy.table`` entry below.
  [#8789]

astropy.io.misc
^^^^^^^^^^^^^^^

- Masked column handling has changed, see ``astropy.table`` entry below.
  [#8789]

- Removed deprecated ``usecPickle`` kwarg from ``fnunpickle`` and
  ``fnpickle``. [#8890]

astropy.io.fits
^^^^^^^^^^^^^^^

- Masked column handling has changed, see ``astropy.table`` entry below.
  [#8789]

- ``io.fits.Header`` has been made safe for subclasses for copying and slicing.
  As a result of this change, the private subclass ``CompImageHeader``
  now always should be passed an explicit ``image_header``. [#9229]

- Removed the deprecated ``tolerance`` option in ``fitsdiff`` and
  ``io.fits.diff`` classes. [#9520]

- Removed deprecated keyword arguments for ``CompImageHDU``:
  ``compressionType``, ``tileSize``, ``hcompScale``, ``hcompSmooth``,
  ``quantizeLevel``. [#9520]

astropy.io.votable
^^^^^^^^^^^^^^^^^^

- Changed ``pedantic`` argument to ``verify`` and change it to have three
  string-based options (``ignore``, ``warn``, and ``exception``) instead of
  just being a boolean. In addition, changed default to ``ignore``, which means
  that warnings will not be shown by default when loading VO tables. [#8715]

astropy.modeling
^^^^^^^^^^^^^^^^

- Eliminates support for compound classes (but not compound instances!) [#8769]

- Slicing compound models more restrictive. [#8769]

- Shape of parameters now includes n_models as dimension. [#8769]

- Parameter instances now hold values instead of models. [#8769]

- Compound model parameters now share instance and value with
  constituent models. [#8769]

- No longer possible to assign slices of parameter values to model parameters
  attribute (it is possible to replace it with a complete array). [#8769]

- Many private attributes and methods have changed (see documentation). [#8769]

- Deprecated ``BlackBody1D`` model and ``blackbody_nu`` and
  ``blackbody_lambda`` functions. [#9282]

- The deprecated ``rotations.rotation_matrix_from_angle`` was removed. [#9363]

- Deprecated and renamed ``MexicanHat1D`` and ``MexicanHat2D``
  to ``RickerWavelet1D`` and ``RickerWavelet2D``. [#9445]

- Deprecated ``modeling.utils.ExpressionTree``. [#9576]

astropy.stats
^^^^^^^^^^^^^

- Removed the ``iters`` keyword from sigma clipping stats functions. [#8948]

- Renamed the ``a`` parameter to ``data`` in biweight stat functions. [#8948]

- Renamed the ``a`` parameter to ``data`` in ``median_absolute_deviation``.
  [#9011]

- Renamed the ``conflevel`` keyword to ``confidence_level`` in
  ``poisson_conf_interval``. Usage of ``conflevel`` now issues
  ``AstropyDeprecationWarning``. [#9408]

- Renamed the ``conf`` keyword to ``confidence_level`` in
  ``binom_conf_interval`` and ``binned_binom_proportion``. Usage of ``conf``
  now issues ``AstropyDeprecationWarning``. [#9408]

- Renamed the ``conf_lvl`` keyword to ``confidence_level`` in
  ``jackknife_stats``. Usage of ``conf_lvl`` now issues
  ``AstropyDeprecationWarning``. [#9408]

astropy.table
^^^^^^^^^^^^^

- The handling of masked columns in the ``Table`` class has changed in a way
  that may impact program behavior. Now a ``Table`` with ``masked=False``
  may contain both ``Column`` and ``MaskedColumn`` objects, and adding a
  masked column or row to a table no longer "upgrades" the table and columns
  to masked.  This means that tables with masked data which are read via
  ``Table.read()`` will now always have ``masked=False``, though specific
  columns will be masked as needed. Two new table properties
  ``has_masked_columns`` and ``has_masked_values`` were added. See the
  `Masking change in astropy 4.0 section within
  <https://docs.astropy.org/en/v4.0.x/table/masking.html>`_ for
  details. [#8789]

- Table operation functions such as ``join``, ``vstack``, ``hstack``, etc now
  always return a table with ``masked=False``, though the individual columns
  may be masked as necessary. [#8957]

- Changed implementation of ``Table.add_column()`` and ``Table.add_columns()``
  methods.  Now it is possible add any object(s) which can be converted or
  broadcasted to a valid column for the table.  ``Table.__setitem__`` now
  just calls ``add_column``. [#8933]

- Changed default table configuration setting ``replace_warnings`` from
  ``['slice']`` to ``[]``.  This removes the default warning when replacing
  a table column that is a slice of another column. [#9144]

- Removed the non-public method
  ``astropy.table.np_utils.recarray_fromrecords``. [#9165]

astropy.tests
^^^^^^^^^^^^^

- In addition to ``DeprecationWarning``, now ``FutureWarning`` and
  ``ImportWarning`` would also be turned into exceptions. [#8506]

- ``warnings_to_ignore_by_pyver`` option in
  ``enable_deprecations_as_exceptions()`` has changed. Please refer to API
  documentation. [#8506]

- Default settings for ``warnings_to_ignore_by_pyver`` are updated to remove
  very old warnings that are no longer relevant and to add a new warning
  caused by ``pytest-doctestplus``. [#8506]

astropy.time
^^^^^^^^^^^^

- ``Time.get_ut1_utc`` now uses the auto-updated ``IERS_Auto`` by default,
  instead of the bundled ``IERS_B`` file. [#9226]

- Time formats that do not use ``val2`` now raise ValueError instead of
  silently ignoring a provided value. [#9373]

- Custom time formats can now accept floating-point types with extended
  precision. Existing time formats raise exceptions rather than discarding
  extended precision through conversion to ordinary floating-point. [#9368]

- Time formats (implemented in subclasses of ``TimeFormat``) now have
  their input and output routines more thoroughly validated, making it more
  difficult to create damaged ``Time`` objects. [#9375]

- The ``TimeDelta.to_value()`` method now can also take the ``format`` name
  as its argument, in which case the value will be calculated using the
  ``TimeFormat`` machinery. For this case, one can also pass a ``subfmt``
  argument to retrieve the value in another form than ``float``. [#9361]

astropy.timeseries
^^^^^^^^^^^^^^^^^^

- Keyword ``midpoint_epoch`` is renamed to ``epoch_time``. [#9455]

astropy.uncertainty
^^^^^^^^^^^^^^^^^^^

- ``Distribution`` was rewritten such that it deals better with subclasses.
  As a result, Quantity distributions now behave correctly with ``to`` methods
  yielding new distributions of the kind expected for the starting distribution,
  and ``to_value`` yielding ``NdarrayDistribution`` instances. [#9442]

astropy.units
^^^^^^^^^^^^^

- For consistency with ``ndarray``, scalar ``Quantity.value`` will now return
  a numpy scalar rather than a python one.  This should help keep track of
  precision better, but may lead to unexpected results for the rare cases
  where numpy scalars behave differently than python ones (e.g., taking the
  square root of a negative number). [#8876]

- Removed the ``magnitude_zero_points`` module, which was deprecated in
  favour of ``astropy.units.photometric`` since 3.1. [#9353]

- ``EquivalentUnitsList`` now has a ``_repr_html_`` method to output a HTML
  table on a call to ``find_equivalent_units`` in Jupyter notebooks. [#9495]

astropy.utils
^^^^^^^^^^^^^

- ``download_file`` and related functions now accept a list of fallback
  sources, and they are able to update the cache at the user's request. [#9182]

- Allow ``astropy.utils.console.ProgressBarOrSpinner.map`` and
  ``.map_unordered`` to take an argument ``multiprocessing_start_method`` to
  control how subprocesses are started; the different methods (``fork``,
  ``spawn``, and ``forkserver``) have different implications in terms of
  security, efficiency, and behavioural anomalies. The option is useful in
  particular for cross-platform testing because Windows supports only ``spawn``
  while Linux defaults to ``fork``. [#9182]

- All operations that act on the astropy download cache now take an argument
  ``pkgname`` that allows one to specify which package's cache to use.
  [#8237, #9182]

- Removed deprecated ``funcsigs`` and ``futures`` from
  ``astropy.utils.compat``. [#8909]

- Removed the deprecated ``astropy.utils.compat.numpy`` module. [#8910]

- Deprecated ``InheritDocstrings`` as it is natively supported by
  Sphinx 1.7 or higher. [#8881]

- Deprecated ``astropy.utils.timer`` module, which has been moved to
  ``astroquery.utils.timer`` and will be part of ``astroquery`` 0.4.0. [#9038]

- Deprecated ``astropy.utils.misc.set_locale`` function, as it is meant for
  internal use only. [#9471]

- The implementation of ``data_info.DataInfo`` has changed (for a considerable
  performance boost). Generally, this should not affect simple subclasses, but
  because the class now uses ``__slots__`` any attributes on the class have to
  be explicitly given a slot. [#8998]

- ``IERS`` tables now use ``nan`` to mark missing values
  (rather than ``1e20``). [#9226]

astropy.visualization
^^^^^^^^^^^^^^^^^^^^^

- The default ``clip`` value is now ``False`` in ``ImageNormalize``. [#9478]

- The default ``clip`` value is now ``False`` in ``simple_norm``.
  [#9698]

- Infinite values are now excluded when calculating limits in
  ``ManualInterval`` and ``MinMaxInterval``.  They were already excluded in
  all other interval classes. [#9480]


Bug Fixes
---------

astropy.convolution
^^^^^^^^^^^^^^^^^^^

- Fixed ``nan_treatment='interpolate'`` option to ``convolve_fft`` to properly
  take into account ``fill_value``. [#8122]

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- The ``QuantityAttribute`` class now supports a None default value if a unit
  is specified. [#9345]

- When ``Representation`` classes with the same name are defined, this no
  longer leads to a ``ValueError``, but instead to a warning and the removal
  of both from the name registry (i.e., one either has to use the class itself
  to set, e.g., ``representation_type``, or refer to the class by its fully
  qualified name). [#8561]

astropy.io.fits
^^^^^^^^^^^^^^^

- Implemented skip (after warning) of header cards with reserved
  keywords in ``table_to_hdu``. [#9390]

- Add ``AstropyDeprecationWarning`` to ``read_table_fits`` when ``hdu=`` is
  selected, but does not match single present table HDU. [#9512]

astropy.io.votable
^^^^^^^^^^^^^^^^^^

- Address issue #8995 by ignoring BINARY2 null mask bits for string values
  on parsing a VOTable.  In this way, the reader should never create masked
  values for string types. [#9057]

- Corrected a spurious warning issued for the ``value`` attribute of the
  ``<OPTION>`` element in VOTable, as well as a test that erroneously
  treated the warning as acceptable.  [#9470]

astropy.nddata
^^^^^^^^^^^^^^

- ``Cutout2D`` will now get the WCS from its first argument if that argument
  has with WCS property. [#9492]

- ``overlap_slices`` will now raise a ``ValueError`` if the input
  position contains any non-finite values (e.g. NaN or inf). [#9648]

astropy.stats
^^^^^^^^^^^^^

- Fixed a bug where ``bayesian_blocks`` returned a single edge. [#8560]

- Fixed input data type validation for ``bayesian_blocks`` to work int
  arrays. [#9513]

astropy.table
^^^^^^^^^^^^^

- Fix bug where adding a column consisting of a list of masked arrays was
  dropping the masks. [#9048]

- ``Quantity`` columns with custom units can now round-trip via FITS tables,
  as long as the custom unit is enabled during reading (otherwise, the unit
  will become an ``UnrecognizedUnit``). [#9015]

- Fix bug where string values could be truncated when inserting into a
  ``Column`` or ``MaskedColumn``, or when adding or inserting a row containing
  string values. [#9559]

astropy.time
^^^^^^^^^^^^

- Fix bug when ``Time`` object is created with only masked elements. [#9624]

- Fix inaccuracy when converting between TimeDelta and datetime.timedelta.
  [#9679]

astropy.units
^^^^^^^^^^^^^

- Ensure that output from test functions of and comparisons between quantities
  can be stored into pre-allocated output arrays (using ``out=array``) [#9273]

astropy.utils
^^^^^^^^^^^^^

- For the default ``IERS_Auto`` table, which combines IERS A and B values, the
  IERS nutation parameters "dX_2000A" and "dY_2000A" are now also taken from
  the actual IERS B file rather than from the B values stored in the IERS A
  file.  Any differences should be negligible for any practical application,
  but this may help exactly reproducing results. [#9237]

astropy.visualization
^^^^^^^^^^^^^^^^^^^^^

- Calling ``WCSAxes.set_axis_off()`` now correctly turns off drawing the Axes.
  [#9411]

- Fix incorrect transformation behavior in ``WCSAxes.plot_coord`` and correctly
  handle when input coordinates are not already in spherical representations.
  [#8927]

- Fixed ``ImageNormalize`` so that when it is initialized without
  ``data`` it will still use the input ``interval`` class. [#9698]

- Fixed ``ImageNormalize`` to handle input data with non-finite
  values. [#9698]

astropy.wcs
^^^^^^^^^^^

- Fix incorrect value returned by
  ``wcsapi.HighLevelWCSWrapper.axis_correlation_matrix``. [#9554]

- Fix NaN-masking of world coordinates when some but not all of the coordinates
  were flagged as invalid by WCSLIB. This occurred for example with WCS with >2
  dimensions where two of the dimensions were celestial coordinates and pixel
  coordinates outside of the 'sky' were converted to world coordinates -
  previously all world coordinates were masked even if uncorrelated with the
  celestial axes, but this is no longer the case. [#9688]

- The default WCS to celestial frame mapping for world coordinate systems that
  specify ``TLON`` and ``TLAT`` coordinates will now return an ITRS frame with
  the representation class set to ``SphericalRepresentation``. This fixes a bug
  that caused ``WCS.pixel_to_world`` to raise an error for such world
  coordinate systems. [#9609]

- ``FITSWCSAPIMixin`` now returns tuples not lists from ``pixel_to_world`` and
  ``world_to_pixel``. [#9678]


Other Changes and Additions
---------------------------

- Versions of Python <3.6 are no longer supported. [#8955]

- Matplotlib 2.1 and later is now required. [#8787]

- Versions of Numpy <1.16 are no longer supported. [#9292]

- Updated the bundled CFITSIO library to 3.470. See
  ``cextern/cfitsio/docs/changes.txt`` for additional information. [#9233]

- The bundled ERFA was updated to version 1.7.0. This is based on
  SOFA 20190722. This includes a fix to avoid precision loss for negative
  JDs, and also includes additional routines to allow updates to the
  leap-second table. [#9323, #9734]

- The default server for the IERS data files has been updated to reflect
  long-term downtime of the canonical USNO server. [#9487, #9508]



3.2.3 (2019-10-27)
==================

Other Changes and Additions
---------------------------

- Updated IERS A URLs due to USNO prolonged maintenance. [#9443]



3.2.2 (2019-10-07)
==================

Bug fixes
---------

astropy.convolution
^^^^^^^^^^^^^^^^^^^

- Fixed a bug in ``discretize_oversample_1D/2D()`` from
  ``astropy.convolution.utils``, which might occasionally introduce unexpected
  oversampling grid dimensions due to a numerical precision issue. [#9293]

- Fixed a bug [#9168] where having a kernel defined using unitless astropy
  quantity objects would result in a crash [#9300]

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- Fix concatenation of representations for cases where the units were different.
  [#8877]

- Check for NaN values in catalog and match coordinates before building and
  querying the ``KDTree`` for coordinate matching. [#9007]

- Fix sky coordinate matching when a dimensionless distance is provided. [#9008]

- Raise a faster and more meaningful error message when differential data units
  are not compatible with a containing representation's units. [#9064]

- Changed the timescale in ICRS to CIRS from 'tdb' to 'tt' conversion and
  vice-versa, as the erfa function that gets called in the process, pnm06a
  accepts time in TT. [#9079]

astropy.io.ascii
^^^^^^^^^^^^^^^^

- Fixed the fast reader when used in parallel and with the multiprocessing
  'spawn' method (which is the default on MacOS X with Python 3.8 and later),
  and enable parallel fast reader on Windows. [#8853]

astropy.io.fits
^^^^^^^^^^^^^^^

- Fixes bug where an invalid TRPOS<n> keyword was being generated for FITS
  time column when no location was available. [#8784]

- Fixed a wrong exception when converting a Table with a unit that is not FITS
  compliant and not convertible to a string using ``format='fits'``. [#8906]

- Fixed an issue with A3DTABLE extension that could not be read. [#9012]

- Fixed the update of the header when creating GroupsHDU from data. [#9216]

astropy.nddata
^^^^^^^^^^^^^^

- Fix to ``add_array``, which now accepts ``array_small`` having dimensions
  equal to ``array_large``, instead of only allowing smaller sizes of
  arrays. [#9118]

astropy.stats
^^^^^^^^^^^^^

- Fixed ``median_absolute_deviation`` for the case where ``ignore_nan=True``
  and an input masked array contained both NaNs and infs. [#9307]

astropy.table
^^^^^^^^^^^^^

- Comparisons between ``Column`` instances and ``Quantity`` will now
  correctly take into account the unit (as was already the case for
  regular operations such as addition). [#8904]

astropy.time
^^^^^^^^^^^^

- Allow ``Time`` to be initialized with an empty value for all formats. [#8854]

- Fixed a troubling bug in which ``Time`` could loose precision, with deviations
  of 300 ns. [#9328]

astropy.timeseries
^^^^^^^^^^^^^^^^^^

- Fixed handling of ``Quantity`` input data for all methods of
  ``LombScarge.false_alarm_probabilty``. [#9246]

astropy.units
^^^^^^^^^^^^^

- Allow conversion of ``Column`` with logarithmic units to a suitable
  ``Quantity`` subclass if ``subok=True``. [#9188]

- Ensured that we simplify powers to smaller denominators if that is
  consistent within rounding precision. [#9267]

- Ensured that the powers shown in a unit's repr are always correct,
  not oversimplified. [#9267]

astropy.utils
^^^^^^^^^^^^^

- Fixed ``find_api_page`` access by using custom request headers and HTTPS
  when version is specified. [#9032]

- Make ``download_file`` (and by extension ``get_readable_fileobj`` and others)
  check the size of downloaded files against the size claimed by the server.
  [#9302]

- Fix ``find_current_module`` so that it works properly if astropy is being used
  inside a bundle such as that produced by PyInstaller. [#8845]

- Fix path to renamed classes, which previously included duplicate path/module
  information under certain circumstances. [#8845]

astropy.visualization
^^^^^^^^^^^^^^^^^^^^^

- Silence numpy runtime warnings in ``WCSAxes`` when drawing grids. [#8882]

astropy.wcs
^^^^^^^^^^^

- Fixed equality test between ``cunit`` where the first element was equal but
  the following elements differed. [#9154]

- Fixed a crash while loading a WCS from headers containing duplicate SIP
  keywords. [#8893]

- Fixed a possible buffer overflow when using too large negative indices for
  ``cunit`` or ``ctype`` [#9151]

- Fixed reference counting in ``WCSBase.__init__`` [#9166]

- Fix ``SlicedLowLevelWCS`` ``world_to_pixel_values`` and
  ``pixel_to_world_values`` when inputs need broadcasting to the same shape.
  (i.e. when one input is sliced out) [#9250]

- Fixed a bug that caused ``WCS.array_shape``, ``WCS.pixel_shape`` and
  ``WCS.pixel_bounds`` to be incorrect after using ``WCS.sub``. [#9095]


Other Changes and Additions
---------------------------

- Fixed a bug that caused files outside of the astropy module directory to be
  included as package data, resulting in some cases in errors when doing
  repeated builds. [#9039]



3.2.1 (2019-06-14)
==================

Bug fixes
---------

astropy.io.fits
^^^^^^^^^^^^^^^

- Avoid reporting a warning with ``BinTableHDU.from_columns`` with keywords that
  are not provided by the user.  [#8838]

- Fix ``Header.fromfile`` to work on FITS files. [#8713]

- Fix reading of empty ``BinTableHDU`` when stored in a gzip-compressed file.
  [#8848]

astropy.table
^^^^^^^^^^^^^

- Fix a problem where mask was dropped when creating a ``MaskedColumn``
  from a list of ``MaskedArray`` objects. [#8826]

astropy.wcs
^^^^^^^^^^^

- Added ``None`` to be displayed as a ``world_axis_physical_types`` in
  the ``WCS`` repr, as ``None`` values are now supported in ``APE14``. [#8811]



3.2 (2019-06-10)
================

New Features
------------

astropy.constants
^^^^^^^^^^^^^^^^^

- Add CODATA 2018 constants but not make them default because the
  redefinition of SI units that will follow has not been implemented
  yet. [#8595]

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- New ``BarycentricMeanEcliptic``, ``HeliocentricTrueEcliptic`` and
  ``GeocentricTrueEcliptic`` frames.
  The ecliptic frames are no longer considered experimental. [#8394]

- The default time scale for epochs like 'J2000' or 'B1975' is now "tt",
  which is the correct one for 'J2000' and avoids leap-second warnings
  for epochs in the far future or past. [#8600]

astropy.extern
^^^^^^^^^^^^^^

- Bundled ``six`` now emits ``AstropyDeprecationWarning``. It will be removed
  in 4.0. [#8323]

astropy.io.ascii
^^^^^^^^^^^^^^^^

- IPAC tables now output data types of ``float`` instead of ``double``, or
  ``int`` instead of ``long``, based on the column ``dtype.itemsize``. [#8216]

- Update handling of MaskedColumn columns when using the 'data_mask' serialization
  method.  This can make writing ECSV significantly faster if the data do not
  actually have any masked values. [#8447]

- Fixed a bug that caused newlines to be incorrect when writing out ASCII tables
  on Windows (they were ``\r\r\n`` instead of ``\r\n``). [#8659]

astropy.io.misc
^^^^^^^^^^^^^^^

- Implement serialization of ``TimeDelta`` in ASDF. [#8285]

- Implement serialization of ``EarthLocation`` in ASDF. [#8286]

- Implement serialization of ``SkyCoord`` in ASDF. [#8284]

- Support serialization of Astropy tables with mixin columns in ASDF. [#8337]

- No warnings when reading HDF5 files with only one table and no ``path=``
  argument [#8483]

- The HDF5 writer will now create a default table instead of raising an
  exception when ``path=`` is not specified and when writing to empty/new HDF5
  files. [#8553]

astropy.io.fits
^^^^^^^^^^^^^^^

- Optimize parsing of cards within the ``Header`` class. [#8428]

- Optimize the parsing of headers to get the structural keywords that are
  needed to find extensions. Thanks to this, getting a random HDU from a file
  with many extensions is much faster than before, in particular when the
  extension headers contain many keywords. [#8502]

-  Change behavior of FITS undefined value in ``Header`` such that ``None``
   is used in Python to represent FITS undefined when using dict interface.
   ``Undefined`` can also be assigned and is translated to ``None``.
   Previously setting a header card value to ``None`` resulted in an
   empty string field rather than a FITS undefined value. [#8572]

- Allow ``Header.fromstring`` and ``Card.fromstring`` to accept ``bytes``.
  [#8707]

astropy.io.registry
^^^^^^^^^^^^^^^^^^^

- Implement ``Table`` reader and writer for ``ASDF``. [#8261]

- Implement ``Table`` reader and writer methods to wrap ``pandas`` I/O methods
  for CSV, Fixed width format, HTML, and JSON. [#8381]

- Add ``help()`` and ``list_formats()`` methods to unified I/O ``read`` and
  ``write`` methods. For example ``Table.read.help()`` gives help on available
  ``Table`` read formats and ``Table.read.help('fits')`` gives detailed
  help on the arguments for reading FITS table file. [#8255]

astropy.table
^^^^^^^^^^^^^

- Initializing a table with ``Table(rows=...)``, if the first item is an ``OrderedDict``,
  now uses the column order of the first row. [#8587]

- Added new pprint_all() and pformat_all() methods to Table. These two new
  methods print the entire table by default. [#8577]

- Removed restriction of initializing a Table from a dict with copy=False. [#8541]

- Improved speed of table row access by a factor of about 2-3.  Improved speed
  of Table len() by a factor of around 3-10 (depending on the number of columns).
  [#8494]

- Improved the Table - pandas ``DataFrame`` interface (``to_pandas()`` and
  ``from_pandas()``).  Mixin columns like ``Time`` and ``Quantity`` can now be
  converted to pandas by flattening the columns as necessary to plain
  columns.  ``Time`` and ``TimeDelta`` columns get converted to
  corresponding pandas date or time delta types.  The ``DataFrame``
  index is now handled in the conversion methods. [#8247]

- Added ``rename_columns`` method to rename multiple columns in one call.
  [#5159, #8070]

- Improved Table performance by reducing unnecessary calls to copy and deepcopy,
  especially as related to the table and column ``meta`` attributes.  Changed the
  behavior when slicing a table (either in rows or with a list of column names)
  so now the sliced output gets a light (key-only) copy of ``meta`` instead of a
  deepcopy.  Changed the ``Table.meta`` class-level descriptor so that assigning
  directly to ``meta``, e.g. ``tbl.meta = new_meta`` no longer does a deepcopy
  and instead just directly assigns the ``new_meta`` object reference.  Changed
  Table initialization so that input ``meta`` is copied only if ``copy=True``.
  [#8404]

- Improved Table slicing performance with internal implementation changes
  related to column attribute access and certain input validation. [#8493]

- Added ``reverse`` argument to the ``sort`` and ``argsort`` methods to allow
  sorting in reverse order. [#8528]

- Improved ``Table.sort()`` performance by removing ``self[keys]`` from code
  which is creating deep copies of ``meta`` attribute and adding a new keyword
  ``names`` in ``get_index()`` to get index by using a list or tuple containing
  names of columns. [#8570]

- Expose ``represent_mixins_as_columns`` as a public function in the
  ``astropy.table`` subpackage.  This previously-private function in the
  ``table.serialize`` module is used to represent mixin columns in a Table as
  one or more plain Column objects. [#7729]

astropy.timeseries
^^^^^^^^^^^^^^^^^^

- Added a new astropy.timeseries sub-package to represent and manipulate
  sampled and binned time series. [#8540]

- The ``BoxLeastSquares`` and ``LombScargle`` classes have been moved to
  ``astropy.timeseries.periodograms`` from ``astropy.stats``. [#8591]

- Added the ability to provide absolute ``Time`` objects to the
  ``BoxLeastSquares`` and ``LombScargle`` periodogram classes. [#8599]

- Added model inspection methods (``model_parameters()``, ``design_matrix()``,
  and ``offset()``) to ``astropy.timeseries.LombScargle`` class [#8397].

astropy.units
^^^^^^^^^^^^^

- ``Quantity`` overrides of ``ndarray`` methods such as ``sum``, ``min``,
  ``max``, which are implemented via reductions, have been removed since they
  are dealt with in ``Quantity.__array_ufunc__``. This should not affect
  subclasses, but they may consider doing similarly. [#8316]  Note that this
  does not include methods that use more complicated python code such as
  ``mean``, ``std`` and ``var``. [#8370]

astropy.visualization
^^^^^^^^^^^^^^^^^^^^^
- Added ``CompositeStretch``, which inherits from ``CompositeTransform`` and
  also ``BaseStretch`` so that it can be used with ``ImageNormalize``. [#8564]

- Added a ``log_a`` argument to the ``simple_norm`` method. Similar to the
  exposing of the ``asinh_a`` argument for ``AsinhStretch``, the new
  ``log_a`` argument is now exposed for ``LogStretch``. [#8436]

astropy.wcs
^^^^^^^^^^^

- WCSLIB was updated to v 6.2.
  This adds support for time-related WCS keywords (WCS Paper VII).
  FITS headers containing ``Time`` axis are parsed and the axis is included in
  the WCS object. [#8592]

- The ``OBSGEO`` attribute as expanded to 6 members - ``XYZLBH``. [#8592]

- Added a new class ``SlicedLowLevelWCS`` in ``astropy.wcs.wcsapi`` that can be
  used to slice any WCS that conforms to the ``BaseLowLevelWCS`` API. [#8546]

- Updated implementation of ``WCS.__getitem__`` and ``WCS.slice`` to now return
  a ``SlicedLowLevelWCS`` rather than raising an error when reducing the
  dimensionality of the WCS. [#8546]


API Changes
-----------

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- ``QuantityAttribute`` no longer has a default value for ``default``.  The
  previous value of None was misleading as it always was an error. [#8450]

- The default J2000 has been changed to use be January 1, 2000 12:00 TT instead
  of UTC.  This is more in line with convention. [#8594]

astropy.io.ascii
^^^^^^^^^^^^^^^^

- IPAC tables now output data types of ``float`` instead of ``double``, or
  ``int`` instead of ``long``, based on the column ``dtype.itemsize``. [#8216]

astropy.io.misc
^^^^^^^^^^^^^^^

- Unit equivalencies can now be serialized to ASDF. [#8252]

astropy.modeling
^^^^^^^^^^^^^^^^

- Composition of model classes is deprecated and will be removed in 4.0.
  Composition of model instances remain unaffected. [#8234, #8408]

astropy.stats
^^^^^^^^^^^^^

- The ``BoxLeastSquares`` and ``LombScargle`` classes have been moved to the
  ``astropy.timeseries.periodograms`` module and will now emit a deprecation
  warning when imported from ``astropy.stats``. [#8591]

astropy.table
^^^^^^^^^^^^^

- Converting an empty table to an array using ``as_array`` method now returns
  an empty array instead of ``None``. [#8647]

- Changed the behavior when slicing a table (either in rows or with a list of column
  names) so now the sliced output gets a light (key-only) copy of ``meta`` instead of
  a deepcopy.  Changed the ``Table.meta`` class-level descriptor so that assigning
  directly to ``meta``, e.g. ``tbl.meta = new_meta`` no longer does a deepcopy
  and instead just directly assigns the ``new_meta`` object reference. Changed
  Table initialization so that input ``meta`` is copied only if ``copy=True``.
  [#8404]

- Added a keyword ``names`` in ``Table.as_array()``.  If provided this specifies
  a list of column names to include for the returned structured array. [#8532]

astropy.tests
^^^^^^^^^^^^^

- Removed ``pytest_plugins`` as they are completely broken for ``pytest>=4``.
  [#7786]

- Removed the ``astropy.tests.plugins.config`` plugin and removed the
  ``--astropy-config-dir`` and ``--astropy-cache-dir`` options from
  testing. Please use caching functionality that is natively in ``pytest``.
  [#7787, #8489]

astropy.time
^^^^^^^^^^^^

- The default time scale for epochs like 'J2000' or 'B1975' is now "tt",
  which is the correct one for 'J2000' and avoids leap-second warnings
  for epochs in the far future or past. [#8600]

astropy.units
^^^^^^^^^^^^^

- Unit equivalencies can now be introspected. [#8252]

astropy.wcs
^^^^^^^^^^^

- The ``world_to_pixel``, ``world_to_array_index*``, ``pixel_to_world*`` and
  ``array_index_to_world*`` methods now all consistently return scalars, arrays,
  or objects not wrapped in a one-element tuple/list when only one scalar,
  array, or object (as was previously already the case for ``WCS.pixel_to_world``
  and ``WCS.array_index_to_world``). [#8663]

astropy.utils
^^^^^^^^^^^^^

- It is now possible to control the number of cores used by ``ProgressBar.map``
  by passing a positive integer as the ``multiprocess`` keyword argument. Use
  ``True`` to use all cores. [#8083]


Bug Fixes
---------

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- ``BarycentricTrueEcliptic``, ``HeliocentricTrueEcliptic`` and
  ``GeocentricTrueEcliptic`` now use the correct transformation
  (including nutation), whereas the new ``*MeanEcliptic`` classes
  use the nutation-free transformation. [#8394]

- Representations with ``float32`` coordinates can now be transformed,
  although the output will always be ``float64``. [#8759]

- Fixed bug that prevented using differentials with HCRS<->ICRS
  transformations. [#8794]

astropy.io.ascii
^^^^^^^^^^^^^^^^

- Fixed a bug where an exception was raised when writing a table which includes
  mixin columns (e.g. a Quantity column) and the output format was specified
  using the ``formats`` keyword. [#8681]

astropy.io.misc
^^^^^^^^^^^^^^^

- Fixed bug in ASDF tag that inadvertently introduced dependency on ``pytest``.
  [#8456]

astropy.modeling
^^^^^^^^^^^^^^^^

- Fixed slowness for certain compound models consisting of large numbers
  of multi-input models [#8338, #8349]

- Fixed bugs in fitting of compound models with units. [#8369]

astropy.nddata
^^^^^^^^^^^^^^

- Fixed bug in reading multi-extension FITS files written by earlier versions
  of ``CCDData``. [#8534]

- Fixed two errors in the way ``CCDData`` handles FITS files with WCS in the
  header. Some of the WCS keywords that should have been removed from the
  header were not, potentially leading to FITS files with inconsistent
  WCS. [#8602]

astropy.table
^^^^^^^^^^^^^

- Fixed a bug when initializing from an empty list: ``Table([])`` no longer
  results in a crash. [#8647]

- Fixed a bug when initializing from an existing ``Table``.  In this case the
  input ``meta`` argument was being ignored.  Now the input ``meta``, if
  supplied, will be used as the ``meta`` for the new ``Table``. [#8404]

- Fix the conversion of bytes values to Python ``str`` with ``Table.tolist``.
  [#8739]

astropy.time
^^^^^^^^^^^^

- Fixed a number of issues to ensure a consistent output type resulting from
  multiplication or division involving a ``TimeDelta`` instance. The output is
  now always a ``TimeDelta`` if the result is a time unit (like u.s or u.d),
  otherwise it will be a ``Quantity``. [#8356]

- Multiplication between two ``TimeDelta`` instances is now possible, resulting
  in a ``Quantity`` with units of time squared (division already correctly
  resulted in a dimensionless ``Quantity``). [#8356]

- Like for comparisons, addition, and subtraction of ``Time`` instances with
  with non-time instances, multiplication and division of ``TimeDelta``
  instances with incompatible other instances no longer immediately raise an
  ``UnitsError`` or ``TypeError`` (depending on the other instance), but
  rather go through the regular Python mechanism of ``TimeDelta`` returning
  ``NotImplemented`` (which will lead to a regular ``TypeError`` unless the
  other instance can handle ``TimeDelta``). [#8356]

- Corrected small rounding errors that could cause the ``jd2`` values in
  ``Time`` to fall outside the range of -0.5 to 0.5. [#8763]

astropy.units
^^^^^^^^^^^^^

- Added a ``Quantity.to_string`` method to add flexibility to the string formatting
  of quantities. It produces unadorned or LaTeX strings, and accepts two different
  sets of delimiters in the latter case: ``inline`` and ``display``. [#8313]

- Ensure classes that mimic quantities by having a ``unit`` attribute and/or
  ``to`` and ``to_value`` methods can be properly used to initialize ``Quantity``
  or set ``Quantity`` instance items. [#8535]

- Add support for ``<<`` to create logarithmic units. [#8290]

- Add support for the ``clip`` ufunc, which in numpy 1.17 is used to implement
  ``np.clip``.  As part of that, remove the ``Quantity.clip`` method under
  numpy 1.17. [#8747]

- Fix parsing of numerical powers in FITS-compatible units. [#8251]

astropy.wcs
^^^^^^^^^^^

- Added a ``PyUnitListProxy_richcmp`` method in ``UnitListProxy`` class to enable
  ``WCS.wcs.cunit`` equality testing. It helps to check whether the two instances of
  ``WCS.wcs.cunit`` are equal or not by comparing the data members of
  ``UnitListProxy`` class [#8480]

- Fixed ``SlicedLowLevelWCS`` when ``array_shape`` is ``None``. [#8649]

- Do not attempt to delete repeated distortion keywords multiple times when
  loading distortions with ``_read_distortion_kw`` and
  ``_read_det2im_kw``. [#8777]


Other Changes and Additions
---------------------------

- Update bundled expat to 2.2.6. [#8343]

- Added instructions for uploading releases to Zenodo. [#8395]

- The bug fixes to the behaviour of ``TimeDelta`` for multiplcation and
  division, which ensure that the output is now always a ``TimeDelta`` if the
  result is a time unit (like u.s or u.d) and otherwise a ``Quantity``, imply
  that sometimes the output type will be different than it was before. [#8356]

- For types unrecognized by ``TimeDelta``, multiplication and division now
  will consistently return a ``TypeError`` if the other instance cannot handle
  ``TimeDelta`` (rather than ``UnitsError`` or ``TypeError`` depending on
  presumed abilities of the other instance). [#8356]

- Multiplication between two ``TimeDelta`` instances will no longer result in
  an ``OperandTypeError``, but rather result in a ``Quantity`` with units of
  time squared (division already correctly resulted in a dimensionless
  ``Quantity``). [#8356]

- Made running the tests insensitive to local user configuration when running
  the tests in parallel mode or directly with pytest. [#8727]

- Added a narrative style guide to the documentation for contributor reference.
  [#8588]

- Ensure we call numpy equality functions in a way that reduces the number
  of ``DeprecationWarning``. [#8755]

Installation
^^^^^^^^^^^^

- We now require setuptools 30.3.0 or later to install the core astropy
  package. [#8240]

- We now define groups of dependencies that can be installed with pip, e.g.
  ``pip install astropy[all]`` (to install all optional dependencies). [#8198]



3.1.2 (2019-02-23)
==================

Bug fixes
---------

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- Convert the default of ``QuantityAttribute``, thereby catching the error case
  case of it being set to None at attribute creation, and giving a more useful
  error message in the process. [#8300]

astropy.cosmology
^^^^^^^^^^^^^^^^^

- Fix elliptic analytical solution for comoving distance. Only
  relevant for non-flat cosmologies without radiation and ``Om0`` > ``Ode0``.
  [#8391]

astropy.modeling
^^^^^^^^^^^^^^^^

- Fixed slowness for certain compound models consisting of large numbers
  of multi-input models [#8338, #8349]

astropy.visualization.wcsaxes
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

- Fix a bug that caused an error when passing an array with all values the same
  to contour or contourf. [#8321]

- Fix a bug that caused contour and contourf to return None instead of the
  contour set. [#8321]


3.1.1 (2018-12-31)
==================

Bug fixes
---------

astropy.io.ascii
^^^^^^^^^^^^^^^^

- Fix error when writing out empty table. [#8279]

astropy.io.fits
^^^^^^^^^^^^^^^

- ``fitsdiff --ignore-hdus`` now prints input filenames in the diff report
  instead of ``<HDUList object at 0x1150f9778>``. [#8295]

astropy.units
^^^^^^^^^^^^^

- Ensure correctness of units when raising to a negative power. [#8263]

- Fix ``with_H0`` equivalency to use the correct direction of
  conversion. [#8292]



3.1 (2018-12-06)
================

New Features
------------

astropy.convolution
^^^^^^^^^^^^^^^^^^^

- ``convolve`` now accepts any array-like input, not just ``numpy.ndarray`` or
  lists. [#7303]

- ``convolve`` Now raises AstropyUserWarning if nan_treatment='interpolate' and
  preserve_nan=False and NaN values are present post convolution. [#8088]

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- The ``SkyCoord.from_name`` constructor now has the ability to create
  coordinate objects by parsing object catalogue names that have embedded
  J-coordinates. [#7830]

- The new function ``make_transform_graph_docs`` can be used to create a
  docstring graph from a custom ``TransformGraph`` object. [#7135]

- ``KDTree`` for catalog matching is now built with sliding midpoint rule
  rather than standard.  In code, this means setting ``compact_nodes=False``
  and ``balanced_tree=False`` in ``cKDTree``. The sliding midpoint rule is much
  more suitable for catalog matching, and results in 1000x speedup in some
  cases. [#7324]

- Additional information about a site loaded from the Astropy sites registry is
  now available in ``EarthLocation.info.meta``. [#7857]

- Added a ``concatenate_representations`` function to combine coordinate
  representation data and any associated differentials. [#7922]

- ``BaseCoordinateFrame`` will now check for a method named
  ``_astropy_repr_in_frame`` when constructing the string forms of attributes.
  Allowing any class to control how ``BaseCoordinateFrame`` represents it when
  it is an attribute of a frame. [#7745]

- Some rarely-changed attributes of frame classes are now cached, resulting in
  speedups (up to 50% in some cases) when creating new scalar frame or
  ``SkyCoord`` objects. [#7949, #5952]

- Added a ``directional_offset_by`` method to ``SkyCoord`` that computes a new
  coordinate given a coordinate, position angle, and angular separation [#5727]

astropy.cosmology
^^^^^^^^^^^^^^^^^

- The default cosmology has been changed from ``WMAP9`` to ``Planck15``. [#8123]

- Distance calculations with ``LambaCDM`` with no radiation (T_CMB0=0)
  are now 20x faster by using elliptic integrals for non-flat cases. [#7155]

- Distance calculations with ``FlatLambaCDM`` with no radiation (T_CMB0=0)
  are now 20x faster by using the hypergeometric function solution
  for this special case. [#7087]

- Age calculations with ``FlatLambdaCDM`` with no radiation (Tcmb0=0)
  are now 1000x faster by using analytic solutions instead of integrating.
  [#7117]

astropy.io.ascii
^^^^^^^^^^^^^^^^

- Latex reader now ignores ``\toprule``, ``\midrule``, and ``\bottomrule``
  commands. [#7349]

- Added the RST (Restructured-text) table format and the fast version of the
  RDB reader to the set of formats that are guessed by default. [#5578]

- The read trace (used primarily for debugging) now includes guess argument
  sets that were skipped entirely e.g. for not supporting user-supplied kwargs.
  All guesses thus removed from ``filtered_guess_kwargs`` are now listed as
  "Disabled" at the beginning of the trace. [#5578]

- Emit a warning when reading an ECSV file without specifying the ``format``
  and without PyYAML installed.  Previously this silently fell through to
  parsing as a basic format file and the file metadata was lost. [#7580]

- Optionally allow writing masked columns to ECSV with the mask explicitly
  specified as a separate column instead of marking masked elements with ""
  (empty string).  This allows handling the case of a masked string column
  with "" data rows.  [#7481]

astropy.io.misc
^^^^^^^^^^^^^^^

- Added support for saving all representation classes and many coordinate
  frames to the asdf format. [#7079]

- Added support for saving models with units to the asdf format. [#7237]

- Added a new ``character_as_bytes`` keyword to the HDF5 Table reading
  function to control whether byte string columns in the HDF5 file
  are left as bytes or converted to unicode.  The default is to read
  as bytes (``character_as_bytes=True``). [#7024, #8017]

astropy.io.fits
^^^^^^^^^^^^^^^

- ``HDUList.pop()`` now accepts string and tuple extension name
  specifications. [#7236]

- Add an ``ignore_hdus`` keyword to ``FITSDiff`` to allow ignoring HDUs by
  NAME when diffing two FITS files [#7538]

- Optionally allow writing masked columns to FITS with the mask explicitly
  specified as a separate column instead of using the FITS standard of
  certain embedded null values (``NaN`` for float, ``TNULL`` for integers).
  This can be used to work around limitations in the FITS standard. [#7481]

- All time coordinates can now be written to and read from FITS binary tables,
  including those with vectorized locations. [#7430]

- The ``fitsheader`` command line tool now supports a ``dfits+fitsort`` mode,
  and the dotted notation for keywords (e.g. ``ESO.INS.ID``). [#7240]

- Fall back to reading arrays using mode='denywrite' if mode='readonly' fails
  when using memory-mapping. This solves cases on some platforms when the
  available address space was less than the file size (even when using memory
  mapping). [#7926]

astropy.modeling
^^^^^^^^^^^^^^^^

- Add a ``Multiply`` model which preserves unit through evaluate, unlike
  ``Scale`` which is dimensionless. [#7210]

- Add a ``uses_quantity`` property to ``Model`` which allows introspection of if
  the ``Model`` can accept ``Quantity`` objects. [#7417]

- Add a ``separability_matrix`` function which returns the correlation matrix
  of inputs and outputs. [#7803]

- Fixed compatibility of ``JointFitter`` with the latest version of Numpy. [#7984]

- Add ``prior`` and ``posterior`` constraints to modeling parameters. These are
  not used by any current fitters, but are provided to allow user code to
  experiment with Bayesian fitters.  [#7558]

astropy.nddata
^^^^^^^^^^^^^^

- ``NDUncertainty`` objects now have a ``quantity`` attribute for simple
  conversion to quantities. [#7704]

- Add a ``bitmask`` module that provides functions for manipulating bitmasks
  and data quality (DQ) arrays. [#7944]

astropy.stats
^^^^^^^^^^^^^

- Add an ``astropy.stats.bls`` module with an implementation of the "box least
  squares" periodogram that is commonly used for discovering transiting
  exoplanets and eclipsing binaries. [#7391]

astropy.table
^^^^^^^^^^^^^

- Added support for full use of ``Time`` mixin column for join, hstack, and
  vstack table operations. [#6888]

- Added a new table index engine, ``SCEngine``, based on the Sorted Containers
  package. [#7574]

- Add a new keyword argument ``serialize_method`` to ``Table.write`` to
  control how ``Time`` and ``MaskedColumn`` columns are written. [#7481]

- Allow mixin columns to be used in table ``group`` and ``unique``
  functions. This applies to both the key columns and the other data
  columns. [#7712]

- Added support for stacking ``Column``, mixin column (e.g. ``Quantity``,
  ``Time``) or column-like objects. [#7674]

- Added support for inserting a row into a Table that has ``Time`` or
  ``TimeDelta`` column(s). [#7897]

astropy.tests
^^^^^^^^^^^^^

- Added an option ``--readonly`` to the test command to change the
  permissions on the temporary installation location to read-only. [#7598]

astropy.time
^^^^^^^^^^^^

- Allow array-valued ``Time`` object to be modified in place. [#6028]

- Added support for missing values (masking) to the ``Time`` class. [#6028]

- Added supper for a 'local' time scale (for free-running clocks, etc.),
  and round-tripping to the corresponding FITS time scale. [#7122]

- Added `datetime.timedelta` format class for ``TimeDelta``. [#7441]

- Added ``strftime`` and ``strptime`` methods to ``Time`` class.
  These methods are similar to those in the Python standard library
  `time` package and provide flexible input and output formatting. [#7323]

- Added ``datetime64`` format to the ``Time`` class to support working with
  ``numpy.datetime64`` dtype arrays. [#7361]

- Add fractional second support for ``strftime`` and ``strptime`` methods
  of ``Time`` class. [#7705]

- Added an ``insert`` method to allow inserting one or more values into a
  ``Time`` or ``TimeDelta`` object. [#7897]

- Remove timescale from string version of FITS format time string.
  The timescale is not part of the FITS standard and should not be included.
  This change may cause some compatibility issues for code that relies on
  round-tripping a FITS format string with a timescale. Strings generated
  from previous versions of this package are still understood but a
  DeprecationWarning will be issued. [#7870]

astropy.uncertainty
^^^^^^^^^^^^^^^^^^^

- This sub-package was added as a "preview" (i.e. API unstable), containing
  the ``Distribution`` class and associated convenience functions. [#6945]

astropy.units
^^^^^^^^^^^^^

- Add complex numbers support for ``Quantity._repr_latex_``. [#7676]

- Add ``thermodynamic_temperature`` equivalency to convert between
  Jy/sr and "thermodynamic temperature" for cosmology. [#7054]

- Add millibar unit. [#7863]

- Add maggy and nanomaggy unit, as well as associated ``zero_point_flux``
  equivalency. [#7891]

- ``AB`` and ``ST`` are now enabled by default, and have alternate names
  ``ABflux`` and ``STflux``. [#7891]

- Added ``littleh`` unit and associated ``with_H0`` equivalency. [#7970]

astropy.visualization
^^^^^^^^^^^^^^^^^^^^^

- Added ``imshow_norm`` function, which combines imshow and creation of a
  ``ImageNormalize`` object. [#7785]

astropy.visualization.wcsaxes
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

- Add support for setting ``set_separator(None)`` in WCSAxes to use default
  separators. [#7570]

- Added two keyword argument options to ``CoordinateHelper.set_format_unit``:
  ``decimal`` can be used to specify whether to use decimal formatting for the
  labels (by default this is False for degrees and hours and True otherwise),
  and ``show_decimal_unit`` can be used to determine whether the units should be
  shown for decimal labels. [#7318]

- Added documentation for ``transform=`` and ``coord_meta=``. [#7698]

- Allow ``coord_meta=`` to optionally include ``format_unit=``. [#7848]

- Add support for more rcParams related to the grid, ticks, and labels, and
  should work with most built-in Matplotlib styles. [#7961]

- Improved rendering of outward-facing ticks. [#7961]

- Add support for ``tick_params`` (which is a standard Matplotlib
  function/method) on both the ``WCSAxes`` class and the individual
  ``CoordinateHelper`` classes. Note that this is provided for compatibility
  with Matplotlib syntax users may be familiar with, but it is not the
  preferred way to change settings. Instead, methods such as ``set_ticks``
  should be preferred. [#7969]

- Moved the argument ``exclude_overlapping`` from ``set_ticks`` to
  ``set_ticklabel``. [#7969]

- Added a ``pad=`` argument to ``set_ticklabel`` to provide a way to control
  the padding between ticks and tick labels. [#7969]

- Added support for setting the tick direction in ``set_ticks`` using the
  ``direction=`` keyword argument. [#7969]

astropy.wcs
^^^^^^^^^^^

- Map ITRS frames to terrestrial WCS coordinates. This will make it possible to
  use WCSAxes to make figures that combine both celestial and terrestrial
  features. An example is plotting the coordinates of an astronomical transient
  over an all- sky satellite image to illustrate the position relative to the
  Earth at the time of the event. The ITRS frame is identified with WCSs that
  use the ``TLON-`` and ``TLAT-`` coordinate types. There are several examples
  of WCSs where this syntax is used to describe terrestrial coordinate systems:
  Section 7.4.1 of `WCS in FITS "Paper II" <https://ui.adsabs.harvard.edu/abs/2002A%26A...395.1077C>`_
  and the `WCSTools documentation <http://tdc-www.harvard.edu/software/wcstools/wcstools.multiwcs.html>`_.
  [#6990]

- Added the abstract base class for the low-level WCS API described in APE 14
  (https://doi.org/10.5281/zenodo.1188875). [#7325]

- Add ``WCS.footprint_contains()`` function to check if the WCS footprint contains a given sky coordinate. [#7273]

- Added the abstract base class for the high-level WCS API described in APE 14
  (https://doi.org/10.5281/zenodo.1188875). [#7325]

- Added the high-level wrapper class for low-level WCS objects as described in
  APE 14 (https://doi.org/10.5281/zenodo.1188875). [#7326]

- Added a new property ``WCS.has_distortion``. [#7326]

- Deprecated ``_naxis1`` and ``_naxis2`` in favor of ``pixel_shape``. [#7973]

- Added compatibility to wcslib version 6. [#8093]


API Changes
-----------

astropy.convolution
^^^^^^^^^^^^^^^^^^^

- ``kernel`` can now be a tuple. [#7561]

- Not technically an API changes, however, the doc string indicated that ``boundary=None``
  was the default when actually it is ``boundary='fill'``. The doc string has been corrected,
  however, someone may interpret this as an API change not realising that nothing has actually
  changed. [#7293]

- ``interpolate_replace_nans()`` can no longer accept the keyword argument
  ``preserve_nan``. It is explicitly set to ``False``. [#8088]


astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- Fixed ``astropy.coordinates.concatenate`` to include velocity data in the
  concatenation. [#7922]

- Changed the name of the single argument to ``Frame.realize_frame()`` from the
  (incorrect) ``representation_type`` to ``data``. [#7923]

- Negative parallaxes passed to ``Distance()`` now raise an error by default
  (``allow_negative=False``), or are converted to NaN values with a warning
  (``allow_negative=True``). [#7988]

- Negating a ``SphericalRepresentation`` object now changes the angular
  coordinates (by rotating 180) instead of negating the distance. [#7988]

- Creation of new frames now generally creates copies of frame attributes,
  rather than inconsistently either copying or making references. [#8204]

- The frame class method ``is_equivalent_frame`` now checks for equality of
  components to determine if a frame is the same when it has frame attributes
  that are representations, rather than checking if they are the same
  object. [#8218]

astropy.io.ascii
^^^^^^^^^^^^^^^^

- If a fast reader is explicitly selected (e.g. ``fast_reader='force'``) and
  options which are incompatible with the fast reader are provided
  (e.g. ``quotechar='##'``) then now a ``ParameterError`` exception will be
  raised. [#5578]

- The fast readers will now raise ``InconsistentTableError`` instead of
  ``CParserError`` if the number of data and header columns do not match.
  [#5578]

- Changed a number of ``ValueError`` exceptions to ``InconsistentTableError``
  in places where the exception is related to parsing a table which is
  inconsistent with the specified table format.  Note that
  ``InconsistentTableError`` inherits from ``ValueError`` so no user code
  changes are required. [#7425]

astropy.io.fits
^^^^^^^^^^^^^^^

- The ``fits.table_to_hdu()`` function will translate any column ``format``
  attributes to a TDISPn format string, if possible, and store it as a TDISPn
  keyword in the ``HDU`` header. [#7226]

astropy.modeling
^^^^^^^^^^^^^^^^

- Change the order of the return values from ``FittingWithOutlierRemoval``,
  such that ``fitted_model`` comes first, for consistency with other fitters.
  For the second value, return only a boolean outlier ``mask``, instead of the
  previous ``MaskedArray`` (which included a copy of the input data that was
  both redundant and inadvertently corrupted at masked points). Return a
  consistent type for the second value when ``niter=0``. [#7407]

- Set the minimum value for the ``bolometric_flux`` parameter of the
  ``BlackBody1D`` model to zero. [#7045]

astropy.nddata
^^^^^^^^^^^^^^

- Add two new uncertainty classes, ``astropy.nddata.VarianceUncertainty`` and
  ``astropy.nddata.InverseVariance``. [#6971]

astropy.stats
^^^^^^^^^^^^^

- String values can now be used for the ``cenfunc`` and ``stdfunc``
  keywords in the ``SigmaClip`` class and ``sigma_clip`` and
  ``sigma_clipped_stats`` functions. [#7478]

- The ``SigmaClip`` class and ``sigma_clip`` and
  ``sigma_clipped_stats`` functions now have a ``masked`` keyword,
  which can be used to return either a masked array (default) or an
  ndarray with the min/max values. [#7478]

- The ``iters`` keyword has been renamed (and deprecated) to
  ``maxiters`` in the ``SigmaClip`` class and ``sigma_clip`` and
  ``sigma_clipped_stats`` functions. [#7478]

astropy.table
^^^^^^^^^^^^^

- ``Table.read()`` on a FITS binary table file will convert any TDISPn header
  keywords to a Python formatting string when possible, and store it in the
  column ``format`` attribute. [#7226]

- No values provided to stack will now raise ``ValueError`` rather than
  ``TypeError``. [#7674]

astropy.tests
^^^^^^^^^^^^^

- ``from astropy.tests.helper import *`` no longer includes
  ``quantity_allclose``. However,
  ``from astropy.tests.helper import quantity_allclose`` would still work.
  [#7381]

- ``warnings_to_ignore_by_pyver`` option in
  ``enable_deprecations_as_exceptions()`` now takes ``None`` as key.
  Any deprecation message that is mapped to ``None`` will be ignored
  regardless of the Python version. [#7790]

astropy.time
^^^^^^^^^^^^

- Added the ability to use ``local`` as time scale in ``Time`` and
  ``TimeDelta``. [#6487]

- Comparisons, addition, and subtraction of ``Time`` instances with non-time
  instances will now return ``NotImplemented`` rather than raise the
  ``Time``-specific ``OperandTypeError``.  This will generally lead to a
  regular ``TypeError``.  As a result, ``OperandTypeError`` now only occurs if
  the operation is between ``Time`` instances of incompatible type or scale.
  [#7584]

astropy.units
^^^^^^^^^^^^^

- In ``UnitBase.compose()``, if a sequence (list|tuple) is passed in to
  ``units``, the default for ``include_prefix_units`` is set to
  `True`, so that no units get ignored. [#6957]

- Negative parallaxes are now converted to NaN values when using the
  ``parallax`` equivalency. [#7988]

astropy.utils
^^^^^^^^^^^^^

- ``InheritDocstrings`` now also works on class properties. [#7166]

- ``diff_values()``, ``report_diff_values()``, and ``where_not_allclose()``
  utility functions are moved from ``astropy.io.fits.diff``. [#7444]

- ``invalidate_caches()`` has been removed from the
  ``astropy.utils.compat`` namespace, use it directly from ``importlib``. [#7872]

astropy.visualization
^^^^^^^^^^^^^^^^^^^^^

- In ``ImageNormalize``, the default for ``clip`` is set to ``True``. [#7800]

- Changed ``AsymmetricPercentileInterval`` and ``MinMaxInterval`` to
  ignore NaN values in arrays. [#7360]

- Automatically default to using ``grid_type='contours'`` in WCSAxes when using
  a custom ``Transform`` object if the transform has no inverse. [#7847]


Performance Improvements
------------------------

- Reduced import time by more cautious use of the standard library. [#7647]

astropy.convolution
^^^^^^^^^^^^^^^^^^^

- Major performance overhaul to ``convolve()``. [#7293]

- ``convolve()``: Boundaries ``fill``, ``extend``, and ``wrap`` now use a single
  implementation that pads the image with the correct boundary values before convolving.
  The runtimes of these three were significantly skewed. They now have
  equivalent runtimes that are also faster than before due to performant contiguous
  memory access. However, this does increase the memory footprint as an entire
  new image array is required plus that needed for the padded region.[#7293]

- ``convolve()``: Core computation ported from Cython to C. Several optimization
  techniques have been implemented to achieve performance gains, e.g. compiler
  hoisting, and vectorization, etc. Compiler optimization level ``-O2`` required for
  hoisting and ``-O3`` for vectorization. [#7293]

- ``convolve()``: ``nan_treatment=interpolate`` was slow to compute irrespective of
  whether any NaN values exist within the array. The input array is now
  checked for NaN values and interpolation is disabled if non are found. This is a
  significant performance boost for arrays without NaN values. [#7293]

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- Sped up creating SkyCoord objects by a factor of ~2 in some cases. [#7615]

- Sped up getting xyz vectors from ``CartesianRepresentation`` (which
  is used a lot internally). [#7638]

- Sped up transformations and some representation methods by replacing
  python code with (compiled) ``erfa`` ufuncs. [#7639]

- Sped up adding differential (velocity) data to representations by a factor of
  ~20, which improves the speed of frame and SkyCoord initialization. [#7924]

- Refactored ``SkyCoord`` initializer to improve performance and code clarity.
  [#7958]

- Sped up initialization of ``Longitude`` by ~40%. [#7616]

astropy.stats
^^^^^^^^^^^^^

- The ``SigmaClip`` class and ``sigma_clip`` and
  ``sigma_clipped_stats`` functions are now significantly faster. [#7478]

- A Cython implementation for `astropy.stats.kuiper_two` and a vectorized
  implementation for `astropy.stats.kuiper_false_positive_probability` have
  been added, speeding up both functions.  [#8104]

astropy.units
^^^^^^^^^^^^^

- Sped up creating new composite units, and raising units to some power
  [#7549, #7649]

- Sped up Unit.to when target unit is the same as the original unit. [#7643]

- Lazy-load ``scipy.special`` to shorten ``astropy.units`` import time. [#7636]

astropy.visualization
^^^^^^^^^^^^^^^^^^^^^

- Significantly sped up drawing of contours in WCSAxes. [#7568]


Bug Fixes
---------

astropy.convolution
^^^^^^^^^^^^^^^^^^^

- Fixed bug in ``convolve_fft`` where masked input was copied with
  ``numpy.asarray`` instead of ``numpy.asanyarray``.
  ``numpy.asarray`` removes the mask subclass causing
  ``numpy.ma.ismasked(input)`` to fail, causing ``convolve_fft``
  to ignore all masked input. [#8137]

- Remove function side-effects of input data from ``convolve_fft``.
  It was possible for input data to remain modified if particular exceptions
  were raised. [#8152]

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- ``EarthLocation.of_address`` now uses the OpenStreetMap geocoding API by
  default to retrieve coordinates, with the Google API (which now requires an
  API key) as an option. [#7918]

- Fixed a bug that caused frame objects with NaN distances to have NaN sky
  positions, even if valid sky coordinates were specified. [#7988]

- Fixed ``represent_as()`` to not round-trip through cartesian if the same
  representation class as the instance is passed in. [#7988]

astropy.io.ascii
^^^^^^^^^^^^^^^^

- Fixed a problem when ``guess=True`` that ``fast_reader`` options
  could be dropped after the first fast reader class was tried. [#5578]

- Units in CDS-formatted tables are now parsed correctly by the units
  module. [#7348]

astropy.io.misc
^^^^^^^^^^^^^^^

- Fixed bug when writing a table with masked columns to HDF5. Previously
  the mask was being silently dropped.  If the ``serialize_meta`` option is
  enabled the data mask will now be written as an additional column and the
  masked columns will round-trip correctly. [#7481]

- Fixed a bug where writing to HDF5 failed for for tables with columns of
  unicode strings.  Now those columns are first encoded to UTF-8 and
  written as byte strings. [#7024, #8017]

- Fixed a bug with serializing the bounding_box of models initialized
  with ``Quantities`` . [#8052]

astropy.io.fits
^^^^^^^^^^^^^^^

- Added support for ``copy.copy`` and ``copy.deepcopy`` for ``HDUList``. [#7218]

- Override ``HDUList.copy()`` to return a shallow HDUList instance. [#7218]

astropy.modeling
^^^^^^^^^^^^^^^^

- Fix behaviour of certain models with units, by making certain unit-related
  attributes readonly. [#7210]

- Fixed an issue with validating a ``bounding_box`` whose items are
  ``Quantities``. [#8052]

- Fix ``Moffat1D`` and ``Moffat2D`` derivatives. [#8108]

astropy.nddata
^^^^^^^^^^^^^^

- Fixed rounding behavior in ``overlap_slices`` for even-sized small
  arrays. [#7859]

- Added support for pickling ``NDData`` instances that have an uncertainty.
  [#7383]

astropy.stats
^^^^^^^^^^^^^

- Fix errors in ``kuiper_false_positive_probability``. [#7975]

astropy.tests
^^^^^^^^^^^^^

- Fixing bug that prevented to run the doctests on only a single rst documentation
  file rather than all of them. [#8055]

astropy.time
^^^^^^^^^^^^

- Fix a bug when setting a ``TimeDelta`` array item with plain float value(s).
  This was always interpreted as a JD (day) value regardless of the
  ``TimeDelta`` format. [#7990]

astropy.units
^^^^^^^^^^^^^

- To simplify fast creation of ``Quantity`` instances from arrays, one can now
  write ``array << unit`` (equivalent to ``Quantity(array, unit, copy=False)``).
  If ``array`` is already a ``Quantity``, this will convert the quantity to the
  requested units; in-place conversion can be done with ``quantity <<= unit``.
  [#7734]

astropy.utils
^^^^^^^^^^^^^

- Fixed a bug due to which ``report_diff_values()`` was reporting incorrect
  number of differences when comparing two ``numpy.ndarray``. [#7470]

- The download progress bar is now only displayed in terminals, to avoid
  polluting piped output. [#7577]

- Ignore URL mirror caching when there is no internet. [#8163]

astropy.visualization
^^^^^^^^^^^^^^^^^^^^^

- Right ascension coordinates are now shown in hours by default, and the
  ``set_format_unit`` method on ``CoordinateHelper`` now works correctly
  with angle coordinates. [#7215]


Other Changes and Additions
---------------------------

- The documentation build now uses the Sphinx configuration from sphinx-astropy
  rather than from astropy-helpers. [#7139]

- Versions of Numpy <1.13 are no longer supported. [#7058]

- Running tests now suppresses the output of the installation stage by default,
  to allow easier viewing of the test results. To re-enable the output as
  before, use ``python setup.py test --verbose-install``. [#7512]

- The ERFA functions are now wrapped in ufuncs instead of custom C code,
  leading to some speed improvements, and setting the stage for allowing
  overrides with ``__array_ufunc__``. [#7502]

- Updated the bundled CFITSIO library to 3.450. See
  ``cextern/cfitsio/docs/changes.txt`` for additional information. [#8014]

- The ``representation`` keywords in coordinate frames are now deprecated in
  favor of the ``representation_type`` keywords (which are less
  ambiguously named). [#8119]



3.0.5 (2018-10-14)
==================

Bug Fixes
---------

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- Fixed bug in which consecutive ``StaticMatrixTransform``'s in a frame
  transform path would be combined in the incorrect order. [#7707]

astropy.tests
^^^^^^^^^^^^^

- Fixing bug that doctests were not picked up from the narrative
  documentation when tests were run for all modules. [#7767]



3.0.4 (2018-08-02)
==================

API Changes
-----------

astropy.table
^^^^^^^^^^^^^

- The private ``_parent`` attribute in the ``info`` attribute of table
  columns was changed from a direct reference to the parent column to a weak
  reference.  This was in response to a memory leak caused by having a
  circular reference cycle.  This change means that expressions like
  ``col[3:5].info`` will now fail because at the point of the ``info``
  property being evaluated the ``col[3:5]`` weak reference is dead.  Instead
  force a reference with ``c = col[3:5]`` followed by
  ``c.info.indices``. [#6277, #7448]


Bug Fixes
---------

astropy.nddata
^^^^^^^^^^^^^^

- Fixed an bug when creating the ``WCS`` of a cutout (see ``nddata.Cutout2D``)
  when input image's ``WCS`` contains ``SIP`` distortion corrections by
  adjusting the ``crpix`` of the ``astropy.wcs.Sip`` (in addition to
  adjusting the ``crpix`` of the ``astropy.wcs.WCS`` object). This bug
  had the potential to produce large errors in ``WCS`` coordinate
  transformations depending on the position of the cutout relative
  to the input image's ``crpix``. [#7556, #7550]

astropy.table
^^^^^^^^^^^^^

- Fix memory leak where updating a table column or deleting a table
  object was not releasing the memory due to a reference cycle
  in the column ``info`` attributes. [#6277, #7448]

astropy.wcs
^^^^^^^^^^^

- Fixed an bug when creating the ``WCS`` slice (see ``WCS.slice()``)
  when ``WCS`` contains ``SIP`` distortion corrections by
  adjusting the ``WCS.sip.crpix`` in addition to adjusting
  ``WCS.wcs.crpix``. This bug had the potential to produce large errors in
  ``WCS`` coordinate transformations depending on the position of the slice
  relative to ``WCS.wcs.crpix``. [#7556, #7550]


Other Changes and Additions
---------------------------

- Updated bundled wcslib to v 5.19.1 [#7688]


3.0.3 (2018-06-01)
==================

Bug Fixes
---------

astropy.io.ascii
^^^^^^^^^^^^^^^^

- Fix stripping correct (header) comment line from ``meta['comments']``
  in the ``CommentedHeader`` reader for all ``header_start`` settings. [#7508]

astropy.io.fits
^^^^^^^^^^^^^^^

- Raise error when attempting to open gzipped FITS file in 'append' mode.
  [#7473]

- Fix a bug when writing to FITS a table that has a column description
  with embedded blank lines. [#7482]

astropy.tests
^^^^^^^^^^^^^

- Enabling running tests for multiple packages when specified comma
  separated. [#7463]


3.0.2 (2018-04-23)
==================

Bug Fixes
---------

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- Computing a 3D separation between two ``SkyCoord`` objects (with the
  ``separation_3d`` method) now works with or without velocity data attached to
  the objects. [#7387]

astropy.io.votable
^^^^^^^^^^^^^^^^^^

- Fix validate with xmllint=True. [#7255, #7283]

astropy.modeling
^^^^^^^^^^^^^^^^

- ``FittingWithOutlierRemoval`` now handles model sets, as long as the
  underlying fitter supports masked values. [#7199]

- Remove assumption that ``model_set_axis == 0`` for 2D models in
  ``LinearLSQFitter``. [#7317, #7199]

- Fix the shape of the outputs when a model set is evaluated with
  ``model_set_axis=False`` . [#7317]

astropy.stats
^^^^^^^^^^^^^

- Accept a tuple for the ``axis`` parameter in ``sigma_clip``, like the
  underlying ``numpy`` functions and some other functions in ``stats``. [#7199]

astropy.tests
^^^^^^^^^^^^^

- The function ``quantity_allclose`` was moved to the ``units`` package with
  the new, shorter name ``allclose``. This eliminates a runtime dependency on
  ``pytest`` which was causing issues for some affiliated packages. The old
  import will continue to work but may be deprecated in the future. [#7252]

astropy.units
^^^^^^^^^^^^^

- Added a units-aware ``allclose`` function (this was previously available in
  the ``tests`` module as ``quantity_allclose``). To complement ``allclose``,
  a new ``isclose`` function is also added and backported. [#7252]


3.0.1 (2018-03-12)
==================

Bug Fixes
---------

astropy.io.ascii
^^^^^^^^^^^^^^^^

- Fix a unicode decode error when reading a table with non-ASCII characters.
  The fast C reader cannot handle unicode so the code now uses the pure-Python
  reader in this case. [#7103]

astropy.io.fits
^^^^^^^^^^^^^^^

- Updated the bundled CFITSIO library to 3.430. This is to remedy a critical
  security vulnerability that was identified by NASA. See
  ``cextern/cfitsio/docs/changes.txt`` for additional information. [#7274]

astropy.io.misc
^^^^^^^^^^^^^^^

- Make sure that a sufficiently recent version of ASDF is installed when
  running test suite against ASDF tags and schemas. [#7205]

astropy.io.registry
^^^^^^^^^^^^^^^^^^^

- Fix reading files with serialized metadata when using a Table subclass. [#7213]

astropy.io.votable
^^^^^^^^^^^^^^^^^^

- Fix lookup fields by ID. [#7208]

astropy.modeling
^^^^^^^^^^^^^^^^

- Fix model set evaluation over common input when model_set_axis > 0. [#7222]

- Fixed the evaluation of compound models with units. This required adding the
  ability to have ``input_units_strict`` and ``input_units_allow_dimensionless``
  be dictionaries with input names as keys. [#6952]

astropy.units
^^^^^^^^^^^^^

- ``quantity_helper`` no longer requires ``scipy>=0.18``. [#7219]


3.0 (2018-02-12)
================

New Features
------------

astropy.constants
^^^^^^^^^^^^^^^^^

- New context manager ``set_enabled_constants`` to temporarily use an older
  version. [#7008]

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- The ``Distance`` object now accepts ``parallax`` as a keyword in the
  initializer, and supports retrieving a parallax (as an ``Angle``) via
  the ``.parallax`` attributes. [#6855]

- The coordinate frame classes (subclasses of ``BaseCoordinateFrame``) now
  always have ``.velocity``, ``.proper_motion``, and ``.radial_velocity``
  properties that provide shorthands to the full-space Cartesian velocity as
  a ``CartesianDifferential``, the 2D proper motion as a ``Quantity``, and the
  radial or line-of-sight velocity as a ``Quantity``. [#6869]

- ``SkyCoord`` objects now support storing and transforming differentials - i.e.,
  both radial velocities and proper motions. [#6944]

- All frame classes now automatically get sensible representation mappings for
  velocity components. For example, ``d_x``, ``d_y``, ``d_z`` are all
  automatically mapped to frame component namse ``v_x``, ``v_y``, ``v_z``.
  [#6856]

- ``SkyCoord`` objects now support updating the position of a source given its
  space motion and a new time or time difference. [#6872]

- The frame classes now accept a representation class or differential class, or
  string names for either, through the keyword arguments ``representation_type``
  and ``differential_type`` instead of ``representation`` and
  ``differential_cls``. [#6873]

- The frame classes (and ``SkyCoord``) now give more useful error messages when
  incorrect attribute names are given.  Instead of using the representation
  attribute names, they use the frame attribute names. [#7106]

- ``EarthLocation`` now has a method to compute the  gravitational redshift due
  due to solar system bodies.  [#6861, #6935]

- ``EarthLocation`` now has a ``get_gcrs`` convenience method to get the
  location in GCRS coordinates.  [#6861, #6935]

astropy.io.fits
^^^^^^^^^^^^^^^

- Expanded the FITS ``Column`` interface to accept attributes pertaining to the FITS
  World Coordinate System, which includes spatial(celestial) and time coordinates. [#6359]

- Added ``ver`` attribute to set the ``EXTVER`` header keyword to ``ImageHDU``
  and ``TableHDU``. [#6454]

- The performance for reading FITS tables has been significantly improved,
  in particular for cases where the tables contain one or more string columns
  and when done through ``Table.read``. [#6821]

- The performance for writing tables from ``Table.write`` has now been
  significantly improved for tables containing one or more string columns. [#6920]

- The ``Table.read`` now supports a ``memmap=`` keyword argument to control
  whether or not to use  memory mapping when reading the table. [#6821]

- When reading FITS tables with ``fits.open``, a new keyword argument
  ``character_as_bytes`` can be passed - when set to `True`, character columns
  are returned as Numpy byte arrays (Numpy type S) while when set to `False`,
  the same columns are decoded to Unicode strings (Numpy type U) which uses more
  memory. [#6821]

- The ``table_to_hdu`` function and the ``BinTableHDU.from_columns`` and
  ``FITS_rec.from_columns`` methods now include a ``character_as_bytes``
  keyword argument - if set to `True`, then when string columns are accessed,
  byte columns will be returned, which can provide significantly improved
  performance. [#6920]

- Added support for writing and reading back a table which has "mixin columns"
  such as ``SkyCoord`` or ``EarthLocation`` with no loss of information. [#6912]

- Enable tab-completion for ``FITS_rec`` column names and ``Header`` keywords
  with IPython 5 and later. [#7071]

astropy.io.misc
^^^^^^^^^^^^^^^

- When writing to HDF5 files, the serialized metadata are now saved in a new
  dataset, instead of the HDF5 dataset attributes. This allows for metadata of
  any dimensions. [#6304]

- Added support in HDF5 for writing and reading back a table which has "mixin
  columns" such as ``SkyCoord`` or ``EarthLocation`` with no loss of
  information. [#7007]

- Add implementations of astropy-specific ASDF tag types. [#6790]

- Add ASDF tag and schema for ICRSCoord. [#6904]

astropy.modeling
^^^^^^^^^^^^^^^^

- Add unit support for tabular models. [#6529]

- A ``deepcopy()`` method was added to models. [#6515]

- Added units support to ``AffineTransformation``. [#6853]

- Added ``is_separable`` function to modeling to test the
  separability of a model. [#6746]

- Added ``Model.separable`` property. It returns a boolean value or
  ``None`` if not set. [#6746]

- Support masked array values in ``LinearLSQFitter`` (instead of silently
  ignoring the mask). [#6927]

astropy.stats
^^^^^^^^^^^^^

- Added false alarm probability computation to ``astropy.stats.LombScargle``
  [#6488]

- Implemented Kuiper functions in ``astropy.stats`` [#3724, #6565]

astropy.table
^^^^^^^^^^^^^

- Added support for reading and writing ``astropy.time.Time`` Table columns
  to and from FITS tables, to the extent supported by the FITS standard. [#6176]

- Improved exception handling and error messages when column ``format``
  attribute is incorrect for the column type. [#6385]

- Allow to pass ``htmldict`` option to the jsviewer writer. [#6551]

- Added new table operation ``astropy.table.setdiff`` that returns the set
  difference of table rows for two tables. [#6443]

- Added support for reading time columns in FITS compliant binary tables
  as ``astropy.time.Time`` Table columns. [#6442]

- Allowed to remove table rows through the ``__delitem__`` method. [#5839]

- Added a new ``showtable`` command-line script to view binary or ASCII table
  files. [#6859]

- Added new table property ``astropy.table.Table.loc_indices`` that returns the
  location of rows by indexes. [#6831]

- Allow updating of table by indices through the property ``astropy.table.Table.loc``. [#6831]

- Enable tab-completion for column names with IPython 5 and later. [#7071]

- Allow getting and setting a table Row using multiple column names. [#7107]

astropy.tests
^^^^^^^^^^^^^

- Split pytest plugins into separate modules. Move remotedata, openfiles,
  doctestplus plugins to standalone repositories. [#6384, #6606]

- When testing, astropy (or the package being tested) is now installed to
  a temporary directory instead of copying the build. This allows
  entry points to work correctly. [#6890]

- The tests_require setting in setup.py now works properly when running
  'python setup.py test'. [#6892]

astropy.units
^^^^^^^^^^^^^

- Deprecated conversion of quantities to truth values. Currently, the expression
  ``bool(0 * u.dimensionless_unscaled)`` evaluates to ``True``. In the future,
  attempting to convert a ``Quantity`` to a ``bool`` will raise ``ValueError``.
  [#6580, #6590]

- Modify the ``brightness_temperature`` equivalency to provide a surface
  brightness equivalency instead of the awkward assumed-per-beam equivalency
  that previously existed [#5173, #6663]

- Support was added for a number of ``scipy.special`` functions. [#6852]

astropy.utils
^^^^^^^^^^^^^

- The ``astropy.utils.console.ProgressBar.map`` class method now supports the
  ``ipython_widget`` option. You can now pass it both ``multiprocess=True`` and
  ``ipython_widget=True`` to get both multiprocess speedup and a progress bar
  widget in an IPython Notebook. [#6368]

- The ``astropy.utils.compat.funcsigs`` module has now been deprecated. Use the
  Python 'inspect' module directly instead. [#6598]

- The ``astropy.utils.compat.futures`` module has now been deprecated. Use the
  Python 'concurrent.futures' module directly instead. [#6598]

- ``JsonCustomEncoder`` is expanded to handle ``Quantity`` and ``UnitBase``.
  [#5471]

- Added a ``dcip_xy`` method to IERS that interpolates along the dX_2000A and
  dY_2000A columns of the IERS table.  Hence, the data for the CIP offsets is
  now available for use in coordinate frame conversion. [#5837]

- The functions ``matmul``, ``broadcast_arrays``, ``broadcast_to`` of the
  ``astropy.utils.compat.numpy`` module have been deprecated. Use the
  NumPy functions directly. [#6691]

- The ``astropy.utils.console.ProgressBar.map`` class method now returns
  results in sequential order. Previously, if you set ``multiprocess=True``,
  then the results could arrive in any arbitrary order, which could be a nasty
  shock. Although the function will still be evaluated on the items in
  arbitrary order, the return values will arrive in the same order in which the
  input items were provided. The method is now a thin wrapper around
  ``astropy.utils.console.ProgressBar.map_unordered``, which preserves the old
  behavior. [#6439]

astropy.visualization
^^^^^^^^^^^^^^^^^^^^^

- Enable Matplotlib's subtraction shorthand syntax for composing and
  inverting transformations for the ``WCSWorld2PixelTransform`` and
  ``WCSPixel2WorldTransform`` classes by setting ``has_inverse`` to ``True``.
  In order to implement a unit test, also implement the equality comparison
  operator for both classes. [#6531]

- Added automatic hiding of axes labels when no tick labels are drawn on that
  axis. This parameter can be configured with
  ``WCSAxes.coords[*].set_axislabel_visibility_rule`` so that labels are automatically
  hidden when no ticks are drawn or always shown. [#6774]

astropy.wcs
^^^^^^^^^^^

- Added a new function ``celestial_frame_to_wcs`` to convert from
  coordinate frames to WCS (the opposite of what ``wcs_to_celestial_frame``
  currently does. [#6481]

- ``wcslib`` was updated to v 5.18. [#7066]


API Changes
-----------

astropy.convolution
^^^^^^^^^^^^^^^^^^^

- ``Gaussian2DKernel`` now accepts ``x_stddev`` in place of ``stddev`` with
  an option for ``y_stddev``, if different. It also accepts ``theta`` like
  ``Gaussian2D`` model. [#3605, #6748]

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- Deprecated ``recommended_units`` for representations. These were used to
  ensure that any angle was presented in degrees in sky coordinates and
  frames. This is more logically done in the frame itself. [#6858]

- As noted above, the frame class attributes ``representation`` and
  ``differential_cls`` are being replaced by ``representation_type`` and
  ``differential_type``. In the next version, using ``representation`` will raise
  a deprecation warning. [#6873]

- Coordinate frame classes now can't be added to the frame transform graph if
  they have frame attribute names that conflict with any component names. This
  is so ``SkyCoord`` can uniquely identify and distinguish frame attributes from
  frame components. [#6871]

- Slicing and reshaping of ``SkyCoord`` and coordinate frames no longer passes
  the new object through ``__init__``, but directly sets attributes on a new
  instance. This speeds up those methods by an order of magnitude, but means
  that any customization done in ``__init__`` is by-passed. [#6941]

astropy.io.ascii
^^^^^^^^^^^^^^^^

- Allow ECSV files to be auto-identified by ``Table.read`` or
  ``Table.write`` based on the ``.ecsv`` file name suffix. In this case it
  is not required to provide the ``format`` keyword. [#6552]

astropy.io.fits
^^^^^^^^^^^^^^^

- Automatically detect and handle compression in FITS files that are opened by
  passing a file handle to ``fits.open`` [#6373]

- Remove the ``nonstandard`` checksum option. [#6571]

astropy.io.misc
^^^^^^^^^^^^^^^

- When writing to HDF5 files, the serialized metadata are now saved in a new
  dataset instead of the HDF5 dataset attributes. This allows for metadata of
  any dimensions. [#6304]

- Deprecated the ``usecPickle`` kwarg of ``fnunpickle`` and ``fnpickle`` as
  it was needed only for Python2 usage. [#6655]

astropy.io.votable
^^^^^^^^^^^^^^^^^^

- Add handling of ``tree.Group`` elements to ``tree.Resource``.  Unified I/O
  or conversion to astropy tables is not affected. [#6262]

astropy.modeling
^^^^^^^^^^^^^^^^

- Removed deprecated ``GaussianAbsorption1D`` model.
  Use ``Const1D - Gaussian1D`` instead. [#6542]

- Removed the registry from modeling. [#6706]

astropy.table
^^^^^^^^^^^^^

- When setting the column ``format`` attribute the value is now immediately
  validated. Previously one could set to any value and it was only checked
  when actually formatting the column. [#6385]

- Deprecated the ``python3_only`` kwarg of the
  ``convert_bytestring_to_unicode`` and ``convert_unicode_to_bytestring``
  methods it was needed only for Python2 usage. [#6655]

- When reading in FITS tables with ``Table.read``, string columns are now
  represented using Numpy byte (dtype ``S``) arrays rather than Numpy
  unicode arrays (dtype ``U``). The ``Column`` class then ensures the
  bytes are automatically converted to string as needed. [#6821]

- When getting a table row using multiple column names, if one of the
  names is not a valid column name then a ``KeyError`` exception is
  now raised (previously ``ValueError``).  When setting a table row,
  if the right hand side is not a sequence with the correct length
  then a ``ValueError`` is now raised (previously in certain cases
  a ``TypeError`` was raised). [#7107]

astropy.utils
^^^^^^^^^^^^^

- ``download_files_in_parallel`` now always uses ``cache=True`` to make the
  function work on Windows. [#6671]

astropy.visualization
^^^^^^^^^^^^^^^^^^^^^

- The Astropy matplotlib plot style has been deprecated. It will continue to
  work in future but is no longer documented. [#6991]


Bug Fixes
---------

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- Frame objects now use the default differential even if the representation is
  explicitly provided as long as the representation provided is the same type as
  the default representation. [#6944]

- Coordinate frame classes now raise an error when they are added to the frame
  transform graph if they have frame attribute names that conflict with any
  component names. [#6871]

astropy.io.ascii
^^^^^^^^^^^^^^^^

- Added support for reading very large tables in chunks to reduce memory
  usage. [#6458]

- Strip leading/trailing white-space from latex lines to avoid issues when
  matching ``\begin{tabular}`` statements.  This is done by introducing a new
  ``LatexInputter`` class to override the ``BaseInputter``. [#6311]

astropy.io.fits
^^^^^^^^^^^^^^^

- Properly handle opening of FITS files from ``http.client.HTTPResponse`` (i.e.
  it now works correctly when passing the results of ``urllib.request.urlopen``
  to ``fits.open``). [#6378]

- Fix the ``fitscheck`` script for updating invalid checksums, or removing
  checksums. [#6571]

- Fixed potential problems with the compression module [#6732]

- Always use the 'D' format for floating point values in ascii tables. [#6938]

astropy.table
^^^^^^^^^^^^^

- Fix getting a table row when using multiple column names (for example
  ``t[3]['a', 'b', 'c']``).  Also fix a problem when setting an entire row:
  if setting one of the right-hand side values failed this could result in
  a partial update of the referenced parent table before the exception is
  raised. [#7107]

astropy.time
^^^^^^^^^^^^

- Initialization of ``Time`` instances with bytes or arrays with dtype ``S``
  will now automatically attempt to decode as ASCII. This ensures ``Column``
  instances with ASCII strings stored with dtype ``S`` can be used.
  [#6823, #6903]

astropy.units
^^^^^^^^^^^^^

- Fixed a bug that caused PLY files to not be generated correctly in Python 3.
  [#7174]

astropy.utils
^^^^^^^^^^^^^

- The ``deprecated`` decorator applied to a class will now modify the class
  itself, rather than to create a class that just looks and behave like the
  original. This is needed so that the Python 3 ``super`` without arguments
  works for decorated classes. [#6615]

- Fixed ``HomogeneousList`` when setting one item or a slice. [#6773]

- Also check the type when creating a new instance of
  ``HomogeneousList``. [#6773]

- Make ``HomogeneousList`` work with iterators and generators when creating the
  instance, extending it, or using when setting a slice. [#6773]


Other Changes and Additions
---------------------------

- Versions of Python <3.5 are no longer supported. [#6556]

- Versions of Pytest <3.1 are no longer supported. [#6419]

- Versions of Numpy <1.10 are no longer supported. [#6593]

- The bundled CFITSIO was updated to version 3.41 [#6477]

- ``analytic_functions`` sub-package is removed.
  Use ``astropy.modeling.blackbody``. [#6541]

- ``astropy.vo`` sub-package is removed. Use ``astropy.samp`` for SAMP and
  ``astroquery`` for VO cone search. [#6540]

- The guide to setting up Emacs for code development was simplified, and
  updated to recommend ``flycheck`` and ``flake8`` for syntax checks. [#6692]

- The bundled version of PLY was updated to 3.10. [#7174]



2.0.16 (2019-10-27)
===================

Bug Fixes
---------

astropy.time
^^^^^^^^^^^^

- Fixed a troubling bug in which ``Time`` could loose precision, with deviations
  of 300 ns. [#9328]


Other Changes and Additions
---------------------------

- Updated IERS A URLs due to USNO prolonged maintenance. [#9443]



2.0.15 (2019-10-06)
===================

Bug Fixes
---------

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- Fixed a bug where the string representation of a ``BaseCoordinateFrame``
  object could become garbled under specific circumstances when the frame
  defines custom component names via ``RepresentationMapping``. [#8869]

astropy.io.fits
^^^^^^^^^^^^^^^

- Fix uint conversion in ``FITS_rec`` when slicing a table. [#8982]

- Fix reading of unsigned 8-bit integer with compressed fits. [#9219]

astropy.nddata
^^^^^^^^^^^^^^

- Fixed a bug in ``overlap_slices`` where the ``"strict"`` mode was
  too strict for a small array along the upper edge of the large array.
  [#8901]

- Fixed a bug in ``overlap_slices`` where a ``NoOverlapError`` would
  be incorrectly raised for a 0-shaped small array at the origin.
  [#8901]

astropy.samp
^^^^^^^^^^^^

- Fixed a bug that caused an incorrectly constructed warning message
  to raise an error. [#8966]

astropy.table
^^^^^^^^^^^^^

- Fix ``FixedWidthNoHeader`` to pay attention to ``data_start`` keyword when
  finding first data line to split columns [#8485, #8511]

- Fix bug when initializing ``Table`` with ``rows`` as a generator. [#9315]

- Fix ``join`` when there are multiple mixin (Quantity) columns as keys. [#9313]

astropy.units
^^^^^^^^^^^^^

- ``Quantity`` now preserves the ``dtype`` for anything that is floating
  point, including ``float16``. [#8872]

- ``Unit()`` now accepts units with fractional exponents such as ``m(3/2)``
  in the default/``fits`` and ``vounit`` formats that would previously
  have been rejected for containing multiple solidi (``/``). [#9000]

- Fixed the LaTeX representation of units containing a superscript. [#9218]

astropy.visualization
^^^^^^^^^^^^^^^^^^^^^

- Fixed compatibility issues with latest versions of Matplotlib. [#8961]


Other Changes and Additions
---------------------------

- Updated required version of Cython to v0.29.13 to make sure that
  generated C files are compatible with the upcoming Python 3.8 release
  as well as earlier supported versions of Python. [#9198]



2.0.14 (2019-06-14)
===================

Bug Fixes
---------

astropy.io.fits
^^^^^^^^^^^^^^^

- Fix ``Header.update`` which was dropping the comments when passed
  a ``Header`` object. [#8840]

astropy.modeling
^^^^^^^^^^^^^^^^

- ``Moffat1D.fwhm`` and ``Moffat2D.fwhm`` will return a positive value when
  ``gamma`` is negative. [#8801, #8815]

astropy.units
^^^^^^^^^^^^^

- Fixed a bug that prevented ``EarthLocation`` from being initialized with
  numpy >=1.17. [#8849]

astropy.visualization
^^^^^^^^^^^^^^^^^^^^^

- Fixed ``quantity_support`` to work around the fact that matplotlib
  does not detect subclasses in its ``units`` framework. With this,
  ``Angle`` and other subclasses work correctly. [#8818]

- Fixed ``quantity_support`` to work properly if multiple context managers
  are nested. [#8844]


2.0.13 (2019-06-08)
===================

Bug Fixes
---------

astropy.io.fits
^^^^^^^^^^^^^^^
- Fixed bug in ``ColDefs._init_from_array()`` that caused unsigned datatypes
  with the opposite endianness as the host architecture to fail the
  TestColumnFunctions.test_coldefs_init_from_array unit test. [#8460]

astropy.io.misc
^^^^^^^^^^^^^^^

- Explicitly set PyYAML default flow style to None to ensure consistent
  astropy YAML output for PyYAML version 5.1 and later. [#8500]

astropy.io.votable
^^^^^^^^^^^^^^^^^^

- Block floating-point columns from using repr format when converted to Table
  [#8358]

astropy.stats
^^^^^^^^^^^^^

- Fixed issue in ``bayesian_blocks`` when called with the ``ncp_prior``
  keyword. [#8339]

astropy.units
^^^^^^^^^^^^^

- Fix ``take`` when one gets only a single element from a ``Quantity``,
  ensuring it returns a ``Quantity`` rather than a scalar. [#8617]



2.0.12 (2019-02-23)
===================

New Features
------------

astropy.utils
^^^^^^^^^^^^^

- The ``deprecated_renamed_argument`` decorator now capable deprecating an
  argument without renaming it. It also got a new ``alternative`` keyword
  argument to suggest alternative functionality instead of the removed
  one. [#8324]


Bug Fixes
---------

astropy.io.fits
^^^^^^^^^^^^^^^

- Fixed bug in ``ColDefs._init_from_array()`` that caused non-scalar unsigned
  entries to not have the correct bzero value set. [#8353]

astropy.modeling
^^^^^^^^^^^^^^^^

- Fixed compatibility of ``JointFitter`` with the latest version of Numpy.
  [#7984]

astropy.table
^^^^^^^^^^^^^

- Fix ``.quantity`` property of ``Column`` class for function-units (e.g.,
  ``dex``). Previously setting this was possible, but getting raised
  an error. [#8425]

- Fixes a bug where initializing a new ``Table`` from the final row of an
  existing ``Table`` failed.  This happened when that row was generated using
  the item index ``[-1]``. [#8422]

astropy.wcs
^^^^^^^^^^^

- Fix bug that caused ``WCS.has_celestial``, ``wcs_to_celestial_frame``, and
  other functionality depending on it to fail in the presence of correlated
  celestial and other axes. [#8420]


Other Changes and Additions
---------------------------

- Fixed ``make clean`` for the documentation on Windows to ensure it
  properly removes the ``api`` and ``generated`` directories. [#8346]

- Updating bundled ``pytest-openfiles`` to v0.3.2. [#8434]

- Making ``ErfaWarning`` and ``ErfaError`` available via
  ``astropy.utils.exceptions``. [#8441]



2.0.11 (2018-12-31)
===================

Bug Fixes
---------

astropy.io.ascii
^^^^^^^^^^^^^^^^

- Fix fast reader C tokenizer to handle double quotes in quoted field.
  [#8283]

astropy.io.fits
^^^^^^^^^^^^^^^

- Fix a bug in ``io.fits`` with writing Fortran-ordered arrays to file
  objects. [#8282]

astropy.units
^^^^^^^^^^^^^

- Add support for ``np.matmul`` as a ``ufunc`` (new in numpy 1.16).
  [#8264, #8305]

astropy.utils
^^^^^^^^^^^^^

- Fix failures caused by IERS_A_URL being unavailable by introducing
  IERS_A_URL_MIRROR. [#8308]



2.0.10 (2018-12-04)
===================

Bug Fixes
---------

astropy.convolution
^^^^^^^^^^^^^^^^^^^

- Fix Moffat2DKernel's FWHM computation, which has an influence on the default
  size of the kernel when no size is given. [#8105]

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- Disable ``of_address`` usage due to Google API now requiring API key. [#7993]

astropy.io.fits
^^^^^^^^^^^^^^^

- ``fits.append`` now correctly handles file objects with valid modes other
  than ``ostream``. [#7856]

astropy.table
^^^^^^^^^^^^^

- Fix ``Table.show_in_notebook`` failure when mixin columns are present. [#8069]

astropy.tests
^^^^^^^^^^^^^

- Explicitly disallow incompatible versions of ``pytest`` when using the test
  runner. [#8188]

astropy.units
^^^^^^^^^^^^^

- Fixed the spelling of the 'luminous emittance/illuminance' physical
  property. [#7942]

astropy.visualization
^^^^^^^^^^^^^^^^^^^^^

- Fixed a bug that caused origin to be incorrect if not specified. [#7927]

- Fixed a bug that caused an error when plotting grids multiple times
  with grid_type='contours'. [#7927]

- Put an upper limit on the number of bins in ``hist`` and ``histogram`` and
  factor out calculation of bin edges into public function
  ``calculate_bin_edges``. [#7991]


Other Changes and Additions
---------------------------

- Fixing ``astropy.__citation__`` to provide the full bibtex entry of the 2018
  paper. [#8110]

- Pytest 4.0 is not supported by the 2.0.x LTS releases. [#8173]

- Updating bundled ``pytest-remotedata`` to v0.3.1. [#8174]

- Updating bundled ``pytest-doctestplus`` to v0.2.0. [#8175]

- Updating bundled ``pytest-openfiles`` to v0.3.0. [#8176]

- Adding ``warning_type`` keyword argument to the "deprecated" decorators to
  allow issuing custom warning types instead of the default
  ``AstropyDeprecationWarning``. [#8178]


2.0.9 (2018-10-14)
==================

Bug Fixes
---------

astropy.io.ascii
^^^^^^^^^^^^^^^^

- Fix reading of big files with the fast reader. [#7885]

astropy.io.fits
^^^^^^^^^^^^^^^

- ``HDUList.__contains__()`` now works with ``HDU`` arguments. That is,
  ``hdulist[0] in hdulist`` now works as expected. [#7282]

- ``HDUList`` s can now be written to streams in Python 3 [#7850]

astropy.nddata
^^^^^^^^^^^^^^

- Fixed the bug in CCData.read when the HDU is not specified and the first one
  is empty so the function searches for the first HDU with data which may not
  have an image extension. [#7739]

astropy.stats
^^^^^^^^^^^^^

- Fixed bugs in biweight statistics functions where a constant data
  array (or if using the axis keyword, constant along an axis) would
  return NaN. [#7737]

astropy.table
^^^^^^^^^^^^^

- Fixed a bug in ``to_pandas()`` where integer type masked columns were always
  getting converted to float. This could cause loss of precision. Now this only
  occurs if there are actually masked data values, in which case ``pandas``
  does require the values to be float so that ``NaN`` can be used to mark the
  masked values. [#7741, #7747]

astropy.tests
^^^^^^^^^^^^^

- Change the name of the configuration variable controlling the location of the
  Astropy cache in the Pytest plugin from ``cache_dir`` to
  ``astropy_cache_dir``. The command line flag also changed to
  ``--astropy-cache-dir``.  This prevents a conflict with the ``cache_dir``
  variable provided by pytest itself. Also made similar change to
  ``config_dir`` option as a precaution. [#7721]

astropy.units
^^^^^^^^^^^^^

- ``UnrecognizedUnit`` instances can now be compared to any other object
  without raising `TypeError`. [#7606]

astropy.visualization
^^^^^^^^^^^^^^^^^^^^^

- Fix compatibility with Matplotlib 3.0. [#7839]

- Fix an issue that caused a crash when using WCSAxes with a custom Transform
  object and when using ``grid_type='contours'`` to plot a grid. [#7846]

astropy.wcs
^^^^^^^^^^^

- Instead of raising an error ``astropy.wcs`` now returns the input when
  the input has zero size.                                       [#7746]

- Fix ``malloc(0)`` bug in ``pipeline_all_pixel2world()`` and
  ``pipeline_pix2foc()``. They now raise an exception for input with
  zero coordinates, i.e. shape = (0, n). [#7806]

- Fixed an issue with scalar input when WCS.naxis is one. [#7858]

Other Changes and Additions
---------------------------

- Added a new ``astropy.__citation__`` attribute which gives a citation
  for Astropy in bibtex format. Made sure that both this and
  ``astropy.__bibtex__`` works outside the source environment, too. [#7718]



2.0.8 (2018-08-02)
==================

Bug Fixes
---------

astropy.convolution
^^^^^^^^^^^^^^^^^^^

- Correct data type conversion for non-float masked kernels. [#7542]

- Fix non-float or masked, zero sum kernels when ``normalize_kernel=False``.
  Non-floats would yield a type error and masked kernels were not being filled.
  [#7541]

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- Ensure that relative humidities can be given as Quantities, rather than take
  any quantity and just strip its unit. [#7668]

astropy.nddata
^^^^^^^^^^^^^^

- Fixed ``Cutout2D`` output WCS NAXIS values to reflect the cutout
  image size. [#7552]

astropy.table
^^^^^^^^^^^^^

- Fixed a bug in ``add_columns`` method where ``rename_duplicate=True`` would
  cause an error if there were no duplicates. [#7540]

astropy.tests
^^^^^^^^^^^^^

- Fixed bug in ``python setup.py test --coverage`` on Windows machines. [#7673]

astropy.time
^^^^^^^^^^^^

- Avoid rounding errors when converting ``Quantity`` to ``TimeDelta``. [#7625]

astropy.visualization
^^^^^^^^^^^^^^^^^^^^^

- Fixed a bug that caused the position of the tick values in decimal mode
  to be incorrectly determined. [#7332]

astropy.wcs
^^^^^^^^^^^

- Fixed a bug that caused ``wcs_to_celestial_frame``, ``skycoord_to_pixel``, and
  ``pixel_to_skycoord`` to raise an error if the axes of the celestial WCS were
  swapped. [#7691]


2.0.7 (2018-06-01)
==================

Bug Fixes
---------

astropy.modeling
^^^^^^^^^^^^^^^^

- Fixed ``Tabular`` models to not change the shape of data. [#7411]

astropy.stats
^^^^^^^^^^^^^

- In ``freedman_bin_width``, if the data has too small IQR,
  raise ``ValueError``. [#7248, #7402]

astropy.table
^^^^^^^^^^^^^

- Fix a performance issue in ``MaskedColumn`` where initialization was
  extremely slow for large arrays with the default ``mask=None``. [#7422]

- Fix printing table row indexed with unsigned integer. [#7469]

- Fix copy of mask when copying a Table, as this is no more done systematically
  by Numpy since version 1.14. Also fixed a problem when MaskedColumn was
  initialized with ``mask=np.ma.nomask``. [#7486]

astropy.time
^^^^^^^^^^^^

- Fixed a bug in Time that raised an error when initializing a subclass of Time
  with a Time object. [#7453]

astropy.utils
^^^^^^^^^^^^^

- Fixed a bug that improperly handled unicode case of URL mirror in Python 2.
  [#7493]

astropy.visualization
^^^^^^^^^^^^^^^^^^^^^

- Fixed a bug that prevented legends from being added to plots done with
  units. [#7510]


Other Changes and Additions
---------------------------

- Bundled ``pytest-remotedata`` plugin is upgraded to 0.3. [#7493]


2.0.6 (2018-04-23)
==================

Bug Fixes
---------

astropy.convolution
^^^^^^^^^^^^^^^^^^^

- convolve(boundary=None) requires the kernel to be smaller than the image.
  This was never actually checked, it now is and an exception is raised.
  [#7313]

astropy.units
^^^^^^^^^^^^^

- ``u.quantity_input`` no longer errors if the return annotation for a
  function is ``None``. [#7336, #7380]

astropy.visualization
^^^^^^^^^^^^^^^^^^^^^

- Explicitly default to origin='lower' in WCSAxes. [#7331]

- Lists of units are now converted in the Matplotlib unit converter. This means
  that for Matplotlib versions later than 2.2, more plotting functions now work
  with units (e.g. errorbar). [#7037]


Other Changes and Additions
---------------------------

- Updated the bundled CFITSIO library to 3.44. This is to remedy another
  critical security vulnerability that was identified by NASA. See
  ``cextern/cfitsio/docs/changes.txt`` for additional information. [#7370]


2.0.5 (2018-03-12)
==================

Bug Fixes
---------

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- Add a workaround for a bug in the einsum function in Numpy 1.14.0. [#7187]

- Fix problems with printing ``Angle`` instances under numpy 1.14.1. [#7234]

astropy.io.fits
^^^^^^^^^^^^^^^

- Fixed the ``fitsdiff`` script for matching fits file with one in a
  directory path. [#7085]

- Make sure that lazily-loaded ``HDUList`` is automatically loaded when calling
  ``hdulist.pop``. [#7186]

astropy.modeling
^^^^^^^^^^^^^^^^

- Propagate weights to underlying fitter in ``FittingWithOutlierRemoval`` [#7249]

astropy.tests
^^^^^^^^^^^^^

- Support dotted package names as namespace packages when gathering test
  coverage. [#7170]

astropy.visualization
^^^^^^^^^^^^^^^^^^^^^

- Matplotlib axes have the ``axisbelow`` property to control the z-order of
  ticks, tick labels, and grid lines. WCSAxes will now respect this property.
  This is useful for drawing scale bars or inset boxes, which should have a
  z-order that places them above all ticks and gridlines. [#7098]


Other Changes and Additions
---------------------------

- Updated the bundled CFITSIO library to 3.430. This is to remedy a critical
  security vulnerability that was identified by NASA. See
  ``cextern/cfitsio/docs/changes.txt`` for additional information. [#7274, #7275]


2.0.4 (2018-02-06)
==================

Bug Fixes
---------

astropy.convolution
^^^^^^^^^^^^^^^^^^^

- Fixed IndexError when ``preserve_nan=True`` in ``convolve_fft``. Added
  testing with ``preserve_nan=True``. [#7000]

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- The ``sites.json`` file is now parsed explicitly with a UTF-8 encoding. This
  means that future revisions to the file with unicode observatory names can
  be done without breaking the site registry parser.  [#7082]

- Working around a bug in Numpy 1.14.0 that broke some coordinate
  transformations. [#7105]

- Fixed a bug where negative angles could be rounded wrongly when converting
  to a string with seconds omitted. [#7148]

astropy.io.fits
^^^^^^^^^^^^^^^

- When datafile is missing, fits.tabledump uses input file name to build
  output file name. Fixed how it gets input file name from HDUList. [#6976]

- Fix in-place updates to scaled columns. [#6956]

astropy.io.registry
^^^^^^^^^^^^^^^^^^^

- Fixed bug in identifying inherited registrations from multiple ancestors [#7156]

astropy.modeling
^^^^^^^^^^^^^^^^

- Fixed a bug in ``LevMarLSQFitter`` when fitting 2D models with constraints. [#6705]

astropy.utils
^^^^^^^^^^^^^

- ``download_file`` function will check for cache downloaded from mirror URL
  first before attempting actual download if primary URL is unavailable. [#6987]

astropy.visualization
^^^^^^^^^^^^^^^^^^^^^

- Fixed test failures for ``astropy.visualization.wcsaxes`` which were due to
  local matplotlibrc files being taken into account. [#7132]


Other Changes and Additions
---------------------------

- Fixed broken links in the documentation. [#6745]

- Substantial performance improvement (potentially >1000x for some cases) when
  converting non-scalar ``coordinates.Angle`` objects to strings. [#7004]


2.0.3 (2017-12-13)
==================

Bug Fixes
---------

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- Ecliptic frame classes now support attributes ``v_x``, ``v_y``, ``v_z`` when
  using with a Cartesian representation. [#6569]

- Added a nicer error message when accidentally calling ``frame.representation``
  instead of ``frame.data`` in the context of methods that use ``._apply()``.
  [#6561]

- Creating a new ``SkyCoord`` from a list of multiple ``SkyCoord`` objects now
  yield the correct type of frame, and works at all for non-equatorial frames.
  [#6612]

- Improved accuracy of velocity calculation in ``EarthLocation.get_gcrs_posvel``.
  [#6699]

- Improved accuracy of radial velocity corrections in
  ``SkyCoord.radial_velocity_correction```. [#6861]

- The precision of ecliptic frames is now much better, after removing the
  nutation from the rotation and fixing the computation of the position of the
  Sun. [#6508]

astropy.extern
^^^^^^^^^^^^^^

- Version 0.2.1 of ``pytest-astropy`` is included as an external package.
  [#6918]

astropy.io.fits
^^^^^^^^^^^^^^^

- Fix writing the result of ``fitsdiff`` to file with ``--output-file``. [#6621]

- Fix a minor bug where ``FITS_rec`` instances can not be indexed with tuples
  and other sequences that end up with a scalar. [#6955, #6966]

astropy.io.misc
^^^^^^^^^^^^^^^

- Fix ``ImportError`` when ``hdf5`` is imported first in a fresh Python
  interpreter in Python 3. [#6604, #6610]

astropy.nddata
^^^^^^^^^^^^^^

- Suppress errors during WCS creation in CCDData.read(). [#6500]

- Fixed a problem with ``CCDData.read`` when the extension wasn't given and the
  primary HDU contained no ``data`` but another HDU did. In that case the header
  were not correctly combined. [#6489]

astropy.stats
^^^^^^^^^^^^^

- Fixed an issue where the biweight statistics functions would
  sometimes cause runtime underflow/overflow errors for float32 input
  arrays. [#6905]

astropy.table
^^^^^^^^^^^^^

- Fixed a problem when printing a table when a column is deleted and
  garbage-collected, and the format function caching mechanism happens
  to re-use the same cache key. [#6714]

- Fixed a problem when comparing a unicode masked column (on left side) to
  a bytes masked column (on right side). [#6899]

- Fixed a problem in comparing masked columns in bytes and unicode when the
  unicode had masked entries. [#6899]

astropy.tests
^^^^^^^^^^^^^

- Fixed a bug that causes tests for rst files to not be run on certain
  platforms. [#6555, #6608]

- Fixed a bug that caused the doctestplus plugin to not work nicely with the
  hypothesis package. [#6605, #6609]

- Fixed a bug that meant that the data.astropy.org mirror could not be used when
  using --remote-data=astropy. [#6724]

- Support compatibility with new ``pytest-astropy`` plugins. [#6918]

- When testing, astropy (or the package being tested) is now installed to
  a temporary directory instead of copying the build. This allows
  entry points to work correctly. [#6890]

astropy.time
^^^^^^^^^^^^

- Initialization of Time instances now is consistent for all formats to
  ensure that ``-0.5 <= jd2 < 0.5``. [#6653]

astropy.units
^^^^^^^^^^^^^

- Ensure that ``Quantity`` slices can be set with objects that have a ``unit``
  attribute (such as ``Column``). [#6123]

astropy.utils
^^^^^^^^^^^^^

- ``download_files_in_parallel`` now respects the given ``timeout`` value.
  [#6658]

- Fixed bugs in remote data handling and also in IERS unit test related to path
  URL, and URI normalization on Windows. [#6651]

- Fixed a bug that caused ``get_pkg_data_fileobj`` to not work correctly when
  used with non-local data from inside packages. [#6724]

- Make sure ``get_pkg_data_fileobj`` fails if the URL can not be read, and
  correctly falls back on the mirror if necessary. [#6767]

- Fix the ``finddiff`` option in ``find_current_module`` to properly deal
  with submodules. [#6767]

- Fixed ``pyreadline`` import in ``utils.console.isatty`` for older IPython
  versions on Windows. [#6800]

astropy.visualization
^^^^^^^^^^^^^^^^^^^^^

- Fixed the vertical orientation of the ``fits2bitmap`` output bitmap
  image to match that of the FITS image. [#6844, #6969]

- Added a workaround for a bug in matplotlib so that the ``fits2bitmap``
  script generates the correct output file type. [#6969]


Other Changes and Additions
---------------------------

- No longer require LaTeX to build the documentation locally and
  use mathjax instead. [#6701]

- Ensured that all tests use the Astropy data mirror if needed. [#6767]


2.0.2 (2017-09-08)
==================

Bug Fixes
---------

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- Ensure transformations via ICRS also work for coordinates that use cartesian
  representations. [#6440]

- Fixed a bug that was preventing ``SkyCoord`` objects made from lists of other
  coordinate objects from being written out to ECSV files. [#6448]

astropy.io.fits
^^^^^^^^^^^^^^^

- Support the ``GZIP_2`` FITS image compression algorithm as claimed
  in docs. [#6486]

astropy.io.votable
^^^^^^^^^^^^^^^^^^

- Fixed a bug that wrote out VO table as version 1.2 instead of 1.3. [#6521]

astropy.table
^^^^^^^^^^^^^

- Fix a bug when combining unicode columns via join or vstack.  The character
  width of the output column was a factor of 4 larger than needed. [#6459]

astropy.tests
^^^^^^^^^^^^^

- Fixed running the test suite using --parallel. [#6415]

- Added error handling for attempting to run tests in parallel without having
  the ``pytest-xdist`` package installed. [#6416]

- Fixed issue running doctests with pytest>=3.2. [#6423, #6430]

- Fixed issue caused by antivirus software in response to malformed compressed
  files used for testing. [#6522]

- Updated top-level config file to properly ignore top-level directories.
  [#6449]

astropy.units
^^^^^^^^^^^^^

- Quantity._repr_latex_ now respects precision option from numpy
  printoptions. [#6412]

astropy.utils
^^^^^^^^^^^^^

- For the ``deprecated_renamed_argument`` decorator, refer to the deprecations
  caller instead of ``astropy.utils.decorators``, to makes it easier to find
  where the deprecation warnings comes from. [#6422]


2.0.1 (2017-07-30)
==================

Bug Fixes
---------

astropy.constants
^^^^^^^^^^^^^^^^^

- Fixed Earth radius to be the IAU2015 value for the equatorial radius.
  The polar value had erroneously been used in 2.0. [#6400]

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- Added old frame attribute classes back to top-level namespace of
  ``astropy.coordinates``. [#6357]

astropy.io.fits
^^^^^^^^^^^^^^^

- Scaling an image always uses user-supplied values when given. Added
  defaults for scaling when bscale/bzero are not present (float images).
  Fixed a small bug in when to reset ``_orig_bscale``. [#5955]

astropy.modeling
^^^^^^^^^^^^^^^^

- Fixed a bug in initializing compound models with units. [#6398]

astropy.nddata
^^^^^^^^^^^^^^

- Updating CCDData.read() to be more flexible with inputs, don't try to
  delete keywords that are missing from the header. [#6388]

astropy.tests
^^^^^^^^^^^^^
- Fixed the test command that is run from ``setuptools`` to allow it to
  gracefully handle keyboard interrupts and pass them on to the ``pytest``
  subprocess. This prompts ``pytest`` to teardown and display useful traceback
  and test information [#6369]

astropy.visualization
^^^^^^^^^^^^^^^^^^^^^

- Ticks and tick labels are now drawn in front of, rather than behind,
  gridlines in WCS axes. This improves legibility in situations where
  tick labels may be on the interior of the axes frame, such as the right
  ascension axis of an all-sky Aitoff or Mollweide projection. [#6361]

astropy.wcs
^^^^^^^^^^^

- Fix the missing wcskey part in _read_sip_kw, this will cause error when
  reading sip wcs while there is no default CRPIX1 CRPIX2 keywords and only
  CRPIX1n CRPIX2n in header. [#6372]



2.0 (2017-07-07)
================

New Features
------------

astropy.constants
^^^^^^^^^^^^^^^^^

- Constants are now organized into version modules, with physical CODATA
  constants in the ``codata2010`` and ``codata2014`` sub-modules,
  and astronomical constants defined by the IAU in the ``iau2012`` and
  ``iau2015`` sub-modules. The default constants in ``astropy.constants``
  in Astropy 2.0 have been updated from ``iau2012`` to ``iau2015`` and
  from ``codata2010`` to ``codata2014``. The constants for 1.3 can be
  accessed in the ``astropyconst13`` sub-module and the constants for 2.0
  (the default in ``astropy.constants``) can also be accessed in the
  ``astropyconst20`` sub-module [#6083]

- The GM mass parameters recommended by IAU 2015 Resolution B 3 have been
  added as ``GM_sun``, ``GM_jup``, and ``GM_earth``, for the Sun,
  Jupiter and the Earth. [#6083]

astropy.convolution
^^^^^^^^^^^^^^^^^^^

- Major change in convolution behavior and keyword arguments. Additional
  details are in the API section. [#5782]

- Convolution with un-normalized and un-normalizable kernels is now possible.
  [#5782]

- Add a new argument, ``normalization_rtol``, to ``convolve_fft``, allowing
  the user to specify the relative error tolerance in the normalization of
  the convolution kernel. [#5649, #5177]

- Models can now be convoluted using ``convolve`` or ``convolve_fft``,
  which generates a regular compound model. [#6015]

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- Frame attributes set on ``SkyCoord`` are now always validated, and any
  ndarray-like operation (like slicing) will also be done on those. [#5751]

- Caching of  all possible frame attributes was implemented. This greatly
  speeds up many ``SkyCoord`` operations. [#5703, #5751]

- A class hierarchy was added to allow the representation layer to store
  differentials (i.e., finite derivatives) of coordinates.  This is intended
  to enable support for velocities in coordinate frames. [#5871]

- ``replicate_without_data`` and ``replicate`` methods were added to
  coordinate frames that allow copying an existing frame object with various
  reference or copy behaviors and possibly overriding frame attributes. [#6182]

- The representation class instances can now contain differential objects.
  This is primarily useful for internal operations that will provide support
  for transforming velocity components in coordinate frames. [#6169]

- ``EarthLocation.to_geodetic()`` (and ``EarthLocation.geodetic``) now return
  namedtuples instead of regular tuples. [#6237]

- ``EarthLocation`` now has ``lat`` and ``lon`` properties (equivalent to, but
  preferred over, the previous ``latitude`` and ``longitude``). [#6237]

- Added a ``radial_velocity_correction`` method to ``SkyCoord`` to do compute
  barycentric and heliocentric velocity corrections. [#5752]

- Added a new ``AffineTransform`` class for coordinate frame transformations.
  This class supports matrix operations with vector offsets in position or
  any differential quantities (so far, only velocity is supported). The
  matrix transform classes now subclass from the base affine transform.
  [#6218]

- Frame objects now have experimental support for velocity components. Most
  frames default to accepting proper motion components and radial velocity,
  and the velocities transform correctly for any transformation that uses
  one of the ``AffineTransform``-type transformations.  For other
  transformations a finite-difference velocity transformation is available,
  although it is not as numerically stable as those that use
  ``AffineTransform``-type transformations. [#6219, #6226]

astropy.io.ascii
^^^^^^^^^^^^^^^^

- Allow to specify encoding in ``ascii.read``, only for Python 3 and with the
  pure-Python readers. [#5448]

- Writing latex tables with only a ``tabular`` environment is now possible by
  setting ``latexdict['tabletyle']`` to ``None``. [#6205]

- Allow ECSV format to support reading and writing mixin columns like
  ``Time``, ``SkyCoord``, ``Latitude``, and ``EarthLocation``. [#6181]

astropy.io.fits
^^^^^^^^^^^^^^^

- Checking available disk space before writing out file. [#5550, #4065]

- Change behavior to warn about units that are not FITS-compliant when
  writing a FITS file but not when reading. [#5675]

- Added absolute tolerance parameter when comparing FITS files. [#4729]

- New convenience function ``printdiff`` to print out diff reports. [#5759]

- Allow to instantiate a ``BinTableHDU`` directly from a ``Table`` object.
  [#6139]

astropy.io.misc
^^^^^^^^^^^^^^^

- YAML representer now also accepts numpy types. [#6077]

astropy.io.registry
^^^^^^^^^^^^^^^^^^^

- New functions to unregister readers, writers, and identifiers. [#6217]

astropy.modeling
^^^^^^^^^^^^^^^^

- Added ``SmoothlyBrokenPowerLaw1D`` model. [#5656]

- Add ``n_submodels`` shared method to single and compound models, which
  allows users to get the number of components of a given single (compound)
  model. [#5747]

- Added a ``name`` setter for instances of ``_CompoundModel``. [#5741]

- Added FWHM properties to Gaussian and Moffat models. [#6027]

- Added support for evaluating models and setting the results for inputs
  outside the bounding_box to a user specified ``fill_value``. This
  is controlled by a new optional boolean keyword ``with_bounding_box``. [#6081]

- Added infrastructure support for units on parameters and during
  model evaluation and fitting, added support for units on all
  functional, power-law, polynomial, and rotation models where this
  is appropriate. A new BlackBody1D model has been added.
  [#4855, #6183, #6204, #6235]

astropy.nddata
^^^^^^^^^^^^^^

- Added an image class, ``CCDData``. [#6173]

astropy.stats
^^^^^^^^^^^^^

- Added ``biweight_midcovariance`` function. [#5777]

- Added ``biweight_scale`` and ``biweight_midcorrelation``
  functions. [#5991]

- ``median_absolute_deviation`` and ``mad_std`` have ``ignore_nan`` option
  that will use ``np.ma.median`` with nans masked out or ``np.nanmedian``
  instead of ``np.median`` when computing the median. [#5232]

- Implemented statistical estimators for Ripley's K Function. [#5712]

- Added ``SigmaClip`` class. [#6206]

- Added ``std_ddof`` keyword option to ``sigma_clipped_stats``.
  [#6066, #6207]

astropy.table
^^^^^^^^^^^^^

- Issue a warning when assigning a string value to a column and
  the string gets truncated.  This can occur because numpy string
  arrays are fixed-width and silently drop characters which do not
  fit within the fixed width. [#5624, #5819]

- Added functionality to allow ``astropy.units.Quantity`` to be written
  as a normal column to FITS files. [#5910]

- Add support for Quantity columns (within a ``QTable``) in table
  ``join()``, ``hstack()`` and ``vstack()`` operations. [#5841]

- Allow unicode strings to be stored in a Table bytestring column in
  Python 3 using UTF-8 encoding.  Allow comparison and assignment of
  Python 3 ``str`` object in a bytestring column (numpy ``'S'`` dtype).
  If comparison with ``str`` instead of ``bytes`` is a problem
  (and ``bytes`` is really more logical), please open an issue on GitHub.
  [#5700]

- Added functionality to allow ``astropy.units.Quantity`` to be read
  from and written to a VOtable file. [#6132]

- Added support for reading and writing a table with mixin columns like
  ``Time``, ``SkyCoord``, ``Latitude``, and ``EarthLocation`` via the
  ASCII ECSV format. [#6181]

- Bug fix for ``MaskedColumn`` insert method, where ``fill_value`` attribute
  was not being passed along to the copy of the ``MaskedColumn`` that was
  returned. [#7585]

astropy.tests
^^^^^^^^^^^^^

- ``enable_deprecations_as_exceptions`` function now accepts additional
  user-defined module imports and warning messages to ignore. [#6223, #6334]

astropy.units
^^^^^^^^^^^^^

- The ``astropy.units.quantity_input`` decorator will now convert the output to
  the unit specified as a return annotation under Python 3. [#5606]

- Passing a logarithmic unit to the ``Quantity`` constructor now returns the
  appropriate logarithmic quantity class if ``subok=True``. For instance,
  ``Quantity(1, u.dex(u.m), subok=True)`` yields ``<Dex 1.0 dex(m)>``. [#5928]

- The ``quantity_input`` decorator now accepts a string physical type in
  addition to of a unit object to specify the expected input ``Quantity``'s
  physical type. For example, ``@u.quantity_input(x='angle')`` is now
  functionally the same as ``@u.quantity_input(x=u.degree)``. [#3847]

- The ``quantity_input`` decorator now also supports unit checking for
  optional keyword arguments and accepts iterables of units or physical types
  for specifying multiple valid equivalent inputs. For example,
  ``@u.quantity_input(x=['angle', 'angular speed'])`` or
  ``@u.quantity_input(x=[u.radian, u.radian/u.yr])`` would both allow either
  a ``Quantity`` angle or angular speed passed in to the argument ``x``.
  [#5653]

- Added a new equivalence ``molar_mass_amu`` between g/mol to
  atomic mass units. [#6040, #6113]

- ``Quantity`` has gained a new ``to_value`` method which returns the value
  of the quantity in a given unit. [#6127]

- ``Quantity`` now supports the ``@`` operator for matrix multiplication that
  was introduced in Python 3.5, for all supported versions of numpy. [#6144]

- ``Quantity`` supports the new ``__array_ufunc__`` protocol introduced in
  numpy 1.13.  As a result, operations that involve unit conversion will be
  sped up considerably (by up to a factor of two for costly operations such
  as trigonometric ones). [#2583]

astropy.utils
^^^^^^^^^^^^^

- Added a new ``dataurl_mirror`` configuration item in ``astropy.utils.data``
  that is used to indicate a mirror for the astropy data server. [#5547]

- Added a new convenience method ``get_cached_urls`` to ``astropy.utils.data``
  for getting a list of the URLs in your cache. [#6242]

astropy.wcs
^^^^^^^^^^^

- Upgraded the included wcslib to version 5.16. [#6225]

  The minimum required version of wcslib in is 5.14.


API Changes
-----------

astropy.analytic_functions
^^^^^^^^^^^^^^^^^^^^^^^^^^

- This entire sub-package is deprecated because blackbody has been moved to
  ``astropy.modeling.blackbody``. [#6191]

astropy.convolution
^^^^^^^^^^^^^^^^^^^

- Major change in convolution behavior and keyword arguments.
  ``astropy.convolution.convolve_fft`` replaced ``interpolate_nan`` with
  ``nan_treatment``, and ``astropy.convolution.convolve`` received a new
  ``nan_treatment`` argument. ``astropy.convolution.convolve`` also no longer
  double-interpolates interpolates over NaNs, although that is now available
  as a separate ``astropy.convolution.interpolate_replace_nans`` function. See
  `the backwards compatibility note
  <https://docs.astropy.org/en/v2.0.16/convolution/index.html#a-note-on-backward-compatibility-pre-v2-0>`_
  for more on how to get the old behavior (and why you probably don't want to.)
  [#5782]

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- The ``astropy.coordinates.Galactic`` frame previously was had the cartesian
  ordering 'w', 'u', 'v' (for 'x', 'y', and 'z', respectively).  This was an
  error and against the common convention.  The 'x', 'y', and 'z' axes now
  map to 'u', 'v', and 'w', following the right-handed ('u' points to
  the Galactic center) convention. [#6330]

- Removed deprecated ``angles.rotation_matrix`` and
  ``angles.angle_axis``. Use the routines in
  ``coordinates.matrix_utilities`` instead. [#6170]

- ``EarthLocation.latitude`` and ``EarthLocation.longitude`` are now
  deprecated in favor of ``EarthLocation.lat`` and ``EarthLocation.lon``.
  They former will be removed in a future version. [#6237]

- The ``FrameAttribute`` class and subclasses have been renamed to just contain
  ``Attribute``. For example, ``QuantityFrameAttribute`` is now
  ``QuantityAttribute``. [#6300]

astropy.cosmology
^^^^^^^^^^^^^^^^^

- Cosmological models do not include any contribution from neutrinos or photons
  by default -- that is, the default value of Tcmb0 is 0.  This does not affect
  built in models (such as WMAP or Planck). [#6112]

astropy.io.fits
^^^^^^^^^^^^^^^

- Remove deprecated ``NumCode`` and ``ImgCode`` properties on FITS
  ``_ImageBaseHDU``.  Use module-level constants ``BITPIX2DTYPE`` and
  ``DTYPE2BITPIX`` instead. [#4993]

- ``comments`` meta key (which is ``io.ascii``'s table convention) is output
  to ``COMMENT`` instead of ``COMMENTS`` header. Similarly, ``COMMENT``
  headers are read into ``comments`` meta [#6097]

- Remove compatibility code which forced loading all HDUs on close. The old
  behavior can be used with ``lazy_load_hdus=False``. Because of this change,
  trying to access the ``.data`` attribute from an HDU which is not loaded
  now raises a ``IndexError`` instead of a ``ValueError``. [#6082]

- Deprecated ``clobber`` keyword; use ``overwrite``. [#6203]

- Add EXTVER column to the output of ``HDUList.info()``. [#6124]

astropy.modeling
^^^^^^^^^^^^^^^^

- Removed deprecated ``Redshift`` model; Use ``RedshiftScaleFactor``. [#6053]

- Removed deprecated ``Pix2Sky_AZP.check_mu`` and ``Pix2Sky_SZP.check_mu``
  methods. [#6170]

- Deprecated ``GaussianAbsorption1D`` model, as it can be better represented
  by subtracting ``Gaussian1D`` from ``Const1D``. [#6200]

- Added method ``sum_of_implicit_terms`` to ``Model``, needed when performing
  a linear fit to a model that has built-in terms with no corresponding
  parameters (primarily the ``1*x`` term of ``Shift``). [#6174]

astropy.nddata
^^^^^^^^^^^^^^

- Removed deprecated usage of parameter ``propagate_uncertainties`` as a
  positional keyword. [#6170]

- Removed deprecated ``support_correlated`` attribute. [#6170]

- Removed deprecated ``propagate_add``, ``propagate_subtract``,
  ``propagate_multiply`` and ``propagate_divide`` methods. [#6170]

astropy.stats
^^^^^^^^^^^^^

- Removed the deprecated ``sig`` and ``varfunc`` keywords in the
  ``sigma_clip`` function. [#5715]

- Added ``modify_sample_size`` keyword to ``biweight_midvariance``
  function. [#5991]

astropy.table
^^^^^^^^^^^^^

- In Python 3, when getting an item from a bytestring Column it is now
  converted to ``str``.  This means comparing a single item to a ``bytes``
  object will always fail, and instead one must compare with a ``str``
  object. [#5700]

- Removed the deprecated ``data`` property of Row. [#5729]

- Removed the deprecated functions ``join``, ``hstack``, ``vstack`` and
  ``get_groups`` from np_utils. [#5729]

- Added ``name`` parameter to method ``astropy.table.Table.add_column`` and
  ``names`` parameter to method ``astropy.table.Table.add_columns``, to
  provide the flexibility to add unnamed columns, mixin objects and also to
  specify explicit names. Default names will be used if not
  specified. [#5996]

- Added optional ``axis`` parameter to ``insert`` method for ``Column`` and
  ``MaskedColumn`` classes. [#6092]

astropy.units
^^^^^^^^^^^^^

- Moved ``units.cgs.emu`` to ``units.deprecated.emu`` due to ambiguous
  definition of "emu". [#4918, #5906]

- ``jupiterMass``, ``earthMass``, ``jupiterRad``, and ``earthRad`` no longer
  have their prefixed units included in the standard units.  If needed, they
  can still  be found in ``units.deprecated``. [#5661]

- ``solLum``,``solMass``, and ``solRad`` no longer have  their prefixed units
  included in the standard units.  If needed, they can still be found in
  ``units.required_by_vounit``, and are enabled by default. [#5661]

- Removed deprecated ``Unit.get_converter``. [#6170]

- Internally, astropy replaced use of ``.to(unit).value`` with the new
  ``to_value(unit)`` method, since this is somewhat faster. Any subclasses
  that overwrote ``.to``, should also overwrite ``.to_value`` (or
  possibly just the private ``._to_value`` method.  (If you did this,
  please let us know what was lacking that made this necessary!). [#6137]

astropy.utils
^^^^^^^^^^^^^

- Removed the deprecated compatibility modules for Python 2.6 (``argparse``,
  ``fractions``, ``gzip``, ``odict``, ``subprocess``) [#5975,#6157,#6164]

- Removed the deprecated ``zest.releaser`` machinery. [#6282]

astropy.visualization
^^^^^^^^^^^^^^^^^^^^^

- Removed the deprecated ``scale_image`` function. [#6170]

astropy.vo
^^^^^^^^^^

- Cone Search now issues deprecation warning because it is moved to
  Astroquery 0.3.5 and will be removed from Astropy in a future version.
  [#5558, #5904]

- The ``astropy.vo.samp`` package has been moved to ``astropy.samp``, and no
  longer supports HTTPS/SSL. [#6201, #6213]

astropy.wcs
^^^^^^^^^^^

- Removed deprecated ``wcs.rotateCD``. [#6170]


Bug Fixes
---------

astropy.convolution
^^^^^^^^^^^^^^^^^^^

- Major change in convolution behavior and keyword arguments:
  ``astropy.convolution.convolve`` was not performing normalized convolution
  in earlier versions of astropy. [#5782]

- Direct convolution previously implemented the wrong definition of
  convolution.  This error only affects *asymmetric* kernels. [#6267]

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- The ``astropy.coordinates.Galactic`` frame had an incorrect ording for the
  'u', 'v', and 'w' cartesian coordinates. [#6330]

- The ``astropy.coordinates.search_around_sky``,
  ``astropy.coordinates.search_around_3d``, and ``SkyCoord`` equivalent methods
  now correctly yield an ``astropy.coordinates.Angle`` as the third return type
  even if there are no matches (previously it returned a raw Quantity). [#6347]

astropy.io.ascii
^^^^^^^^^^^^^^^^

- Fix an issue where the fast C-reader was dropping table comments for a
  table with no data lines. [#8274]

astropy.io.fits
^^^^^^^^^^^^^^^

- ``comments`` meta key (which is ``io.ascii``'s table convention) is output
  to ``COMMENT`` instead of ``COMMENTS`` header. Similarly, ``COMMENT``
  headers are read into ``comments`` meta [#6097]

- Use more sensible fix values for invalid NAXISj header values. [#5935]

- Close file on error to avoid creating a ``ResourceWarning`` warning
  about an unclosed file. [#6168, #6177]

astropy.modeling
^^^^^^^^^^^^^^^^

- Creating a compound model where one of the submodels is
  a compound model whose parameters were changed now uses the
  updated parameters and not the parameters of the original model. [#5741]

- Allow ``Mapping`` and ``Identity`` to be fittable. [#6018]

- Gaussian models now impose positive ``stddev`` in fitting. [#6019]

- OrthoPolynomialBase (Chebyshev2D / Legendre2D) models were being evaluated
  incorrectly when part of a compound model (using the parameters from the
  original model), which in turn caused fitting to fail as a no-op. [#6085]

- Allow ``Ring2D`` to be defined using ``r_out``. [#6192]

- Make ``LinearLSQFitter`` produce correct results with fixed model
  parameters and allow ``Shift`` and ``Scale`` to be fitted with
  ``LinearLSQFitter`` and ``LevMarLSQFitter``. [#6174]

astropy.stats
^^^^^^^^^^^^^

- Allow to choose which median function is used in ``mad_std`` and
  ``median_absolute_deviation``. And allow to use these functions with
  a multi-dimensional ``axis``. [#5835]

- Fixed ``biweight_midvariance`` so that by default it returns a
  variance that agrees with the standard definition. [#5991]

astropy.table
^^^^^^^^^^^^^

- Fix a problem with vstack for bytes columns in Python 3. [#5628]

- Fix QTable add/insert row for multidimensional Quantity. [#6092]

astropy.time
^^^^^^^^^^^^

- Fixed the initial condition of ``TimeFITS`` to allow scale, FITS scale
  and FITS realization to be checked and equated properly. [#6202]

astropy.visualization
^^^^^^^^^^^^^^^^^^^^^

- Fixed a bug that caused the default WCS to return coordinates offset by
  one. [#6339]

astropy.vo
^^^^^^^^^^

- Fixed a bug in vo.samp when stopping a hub for which a lockfile was
  not created. [#6211]


Other Changes and Additions
---------------------------

- Numpy 1.7 and 1.8 are no longer supported. [#6006]

- Python 3.3 is no longer supported. [#6020]

- The bundled ERFA was updated to version 1.4.0. [#6239]

- The bundled version of pytest has now been removed, but the
  astropy.tests.helper.pytest import will continue to work properly.
  Affiliated packages should nevertheless transition to importing pytest
  directly rather than from astropy.tests.helper. This also means that
  pytest is now a formal requirement for testing for both Astropy and
  for affiliated packages. [#5694]


1.3.3 (2017-05-29)
==================

Bug Fixes
---------

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- Fixed a bug where ``StaticMatrixTransform`` erroneously copied frame
  attributes from the input coordinate to the output frame. In practice, this
  didn't actually affect any transforms in Astropy but may change behavior for
  users who explicitly used the ``StaticMatrixTransform`` in their own code.
  [#6045]

- Fixed ``get_icrs_coordinates`` to loop through all the urls in case one
  raises an exception. [#5864]

astropy.io.fits
^^^^^^^^^^^^^^^

- Fix table header not written out properly when ``fits.writeto()``
  convenience function is used. [#6042]

- Fix writing out read-only arrays. [#6036]

- Extension headers are written out properly when the ``fits.update()``
  convenience function is used. [#6058]

- Angstrom, erg, G, and barn are no more reported as deprecated FITS units.
  [#5929]

astropy.table
^^^^^^^^^^^^^

- Fix problem with Table pprint/pformat raising an exception for
  non-UTF-8 compliant bytestring data. [#6117]

astropy.units
^^^^^^^^^^^^^

- Allow strings 'nan' and 'inf' as Quantity inputs. [#5958]

- Add support for ``positive`` and ``divmod`` ufuncs (new in numpy 1.13).
  [#5998, #6020, #6116]

astropy.utils
^^^^^^^^^^^^^

- On systems that do not have ``pkg_resources`` non-numerical additions to
  version numbers like ``dev`` or ``rc1`` are stripped in ``minversion`` to
  avoid a ``TypeError`` in ``distutils.version.LooseVersion`` [#5944]

- Fix ``auto_download`` setting ignored in ``Time.ut1``. [#6033]

astropy.visualization
^^^^^^^^^^^^^^^^^^^^^

- Fix bug in ManualInterval which caused the limits to be returned incorrectly
  if set to zero, and fix defaults for ManualInterval in the presence of NaNs.
  [#6088]

- Get rid of warnings that occurred when slicing a cube due to the tick
  locator trying to find ticks for the sliced axis. [#6104]

- Accept normal Matplotlib keyword arguments in set_xlabel and set_ylabel
  functions. [#5686, #5692, #6060]

- Fix a bug that caused labels to be missing from frames with labels that
  could change direction mid-axis, such as EllipticalFrame. Also ensure
  that empty tick labels do not cause any warnings. [#6063]


1.3.2 (2017-03-30)
==================

Bug Fixes
---------

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- Ensure that checking equivalence of ``SkyCoord`` objects works with
  non-scalar attributes [#5884, #5887]

- Ensure that transformation to frames with multi-dimensional attributes
  works as expected [#5890, #5897]

- Make sure all ``BaseRepresentation`` objects can be output as strings.
  [#5889, #5897]

astropy.units
^^^^^^^^^^^^^

- Add support for ``heaviside`` ufunc (new in numpy 1.13). [#5920]

astropy.utils
^^^^^^^^^^^^^

- Fix to allow the C-based _fast_iterparse() VOTable XML parser to
  relloc() its buffers instead of overflowing them. [#5824, #5869]


Other Changes and Additions
---------------------------

- File permissions are revised in the released source distribution. [#5912]


1.3.1 (2017-03-18)
==================

New Features
------------

astropy.utils
^^^^^^^^^^^^^

- The ``deprecated_renamed_argument`` decorator got a new ``pending``
  parameter to suppress the deprecation warnings. [#5761]

Bug Fixes
---------

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- Changed ``SkyCoord`` so that frame attributes which are not valid for the
  current ``frame`` (but are valid for other frames) are stored on the
  ``SkyCoord`` instance instead of the underlying ``frame`` instance (e.g.,
  setting ``relative_humidity`` on an ICRS ``SkyCoord`` instance.) [#5750]

- Ensured that ``position_angle`` and ``separation`` give correct answers for
  frames with different equinox (see #5722). [#5762]

astropy.io.fits
^^^^^^^^^^^^^^^

- Fix problem with padding bytes written for BinTable columns converted
  from unicode [#5280, #5287, #5288, #5296].

- Fix out-of-order TUNITn cards when writing tables to FITS. [#5720]

- Recognize PrimaryHDU when non boolean values are present for the
  'GROUPS' header keyword. [#5808]

- Fix the insertion of new keywords in compressed image headers
  (``CompImageHeader``). [#5866]

astropy.modeling
^^^^^^^^^^^^^^^^

- Fixed a problem with setting ``bounding_box`` on 1D models. [#5718]

- Fixed a broadcasting problem with weighted fitting of 2D models
  with ``LevMarLSQFitter``. [#5788]

- Fixed a problem with passing kwargs to fitters, specifically ``verblevel``. [#5815]

- Changed FittingWithOutlierRemoval to reject on the residual to the fit [#5831]

astropy.stats
^^^^^^^^^^^^^

- Fix the psd normalization for Lomb-Scargle periodograms in the presence
  of noise. [#5713]

- Fix bug in the autofrequency range when ``minimum_frequency`` is specified
  but ``maximum_frequency`` is not. [#5738]

- Ensure that a masked array is returned when sigma clipping fully masked
  data. [#5711]

astropy.table
^^^^^^^^^^^^^

- Fix problem where key for caching column format function was not
  sufficiently unique. [#5803]

- Handle sorting NaNs and masked values in jsviewer. [#4052, #5572]

- Ensure mixin columns can be added to a table using a scalar value for the
  right-hand side if the type supports broadcasting. E.g., for an existing
  ``QTable``, ``t['q'] = 3*u.m`` will now add a column as expected. [#5820]

- Fixes the bug of setting/getting values from rows/columns of a table using
  numpy array scalars. [#5772]

astropy.units
^^^^^^^^^^^^^

- Fixed problem where IrreducibleUnits could fail to unpickle. [#5868]

astropy.utils
^^^^^^^^^^^^^

- Avoid importing ``ipython`` in ``utils.console`` until it is necessary, to
  prevent deprecation warnings when importing, e.g., ``Column``. [#5755]

astropy.visualization
^^^^^^^^^^^^^^^^^^^^^

- Avoid importing matplotlib.pyplot when importing
  astropy.visualization.wcsaxes. [#5680, #5684]

- Ignore Numpy warnings that happen in coordinate transforms in WCSAxes.
  [#5792]

- Fix compatibility issues between WCSAxes and Matplotlib 2.x. [#5786]

- Fix a bug that caused WCSAxes frame visual properties to not be copied
  over when resetting the WCS. [#5791]

astropy.extern
^^^^^^^^^^^^^^

- Fixed a bug where PLY was overwriting its generated files. [#5728]

Other Changes and Additions
---------------------------

- Fixed a deprecation warning that occurred when running tests with
  astropy.test(). [#5689]

- The deprecation of the ``clobber`` argument (originally deprecated in 1.3.0)
  in the ``io.fits`` write functions was changed to a "pending" deprecation
  (without displaying warnings) for now. [#5761]

- Updated bundled astropy-helpers to v1.3.1. [#5880]


1.3 (2016-12-22)
================

New Features
------------

astropy.convolution
^^^^^^^^^^^^^^^^^^^

- The ``convolve`` and ``convolve_fft`` arguments now support a ``mask`` keyword,
  which allows them to also support ``NDData`` objects as inputs. [#5554]

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- Added an ``of_address`` classmethod to ``EarthLocation`` to enable fast creation of
  ``EarthLocation`` objects given an address by querying the Google maps API [#5154].

- A new routine, ``get_body_barycentric_posvel`` has been added that allows
  one to calculate positions as well as velocities for solar system bodies.
  For JPL kernels, this roughly doubles the execution time, so if one requires
  only the positions, one should use ``get_body_barycentric``. [#5231]

- Transformations between coordinate systems can use the more accurate JPL
  ephemerides. [#5273, #5436]

- Arithmetic on representations, such as addition of two representations,
  multiplication with a ``Quantity``, or calculating the norm via ``abs``,
  has now become possible. Furthermore, there are new methods ``mean``,
  ``sum``, ``dot``, and ``cross``. For all these, the representations are
  treated as vectors in cartesian space (temporarily converting to
  ``CartesianRepresentation`` if necessary).  [#5301]
  has now become possible. Furthermore, there are news methods ``mean``,
  ``sum``, ``dot``, and ``cross`` with obvious meaning. [#5301]
  multiplication with a ``Quantity`` has now become possible. Furthermore,
  there are new methods ``norm``, ``mean``, ``sum``, ``dot``, and ``cross``.
  In all operations, the representations are treated as vectors. They are
  temporarily converted to ``CartesianRepresentation`` if necessary.  [#5301]

- ``CartesianRepresentation`` can be initialized with plain arrays by passing
  in a ``unit``. Furthermore, for input with a vector array, the coordinates
  no longer have to be in the first dimension, but can be at any ``xyz_axis``.
  To complement the latter, a new ``get_xyz(xyz_axis)`` method allows one to
  get a vector array out along a given axis. [#5439]

astropy.io.ascii
^^^^^^^^^^^^^^^^

- Files with "Fortran-style" columns (i.e. double-precision scientific notation
  with a character other than "e", like ``1.495978707D+13``) can now be parsed by
  the fast reader natively. [#5552]

- Allow round-tripping masked data tables in most formats by using an
  empty string ``''`` as the default representation of masked values
  when writing. [#5347]

- Allow reading HTML tables with unicode column values in Python 2.7. [#5410]

- Check for self-consistency of ECSV header column names. [#5463]

- Produce warnings when writing an IPAC table from an astropy table that
  contains metadata not supported by the IPAC format. [#4700]

astropy.io.fits
^^^^^^^^^^^^^^^

- "Lazy" loading of HDUs now occurs - when an HDU is requested, the file is
  only read up to the point where that HDU is found.  This can mean a
  substantial speedup when accessing files that have many HDUs. [#5065]

astropy.io.misc
^^^^^^^^^^^^^^^

- Added ``io.misc.yaml`` module to support serializing core astropy objects
  using the YAML protocol. [#5486]

astropy.io.registry
^^^^^^^^^^^^^^^^^^^

- Added ``delay_doc_updates`` contextmanager to postpone the formatting of
  the documentation for the ``read`` and ``write`` methods of the class to
  optionally reduce the import time. [#5275]

astropy.modeling
^^^^^^^^^^^^^^^^

- Added a class to combine astropy fitters and functions to remove outliers
  e. g., sigma clip. [#4760]

- Added a ``Tabular`` model. [#5105]

- Added ``Hermite1D`` and ``Hermite2D`` polynomial models [#5242]

- Added the injection of EntryPoints into astropy.modeling.fitting if
  they inherit from Fitters class. [#5241]

- Added bounding box to ``Lorentz1D`` and ``MexicanHat1D`` models. [#5393]

- Added ``Planar2D`` functional model. [#5456]

- Updated ``Gaussian2D`` to accept no arguments (will use default x/y_stddev
  and theta). [#5537]

astropy.nddata
^^^^^^^^^^^^^^

- Added ``keep`` and ``**kwargs`` parameter to ``support_nddata``. [#5477]

astropy.stats
^^^^^^^^^^^^^

- Added ``axis`` keyword to ``biweight_location`` and
  ``biweight_midvariance``. [#5127, #5158]

astropy.table
^^^^^^^^^^^^^

- Allow renaming mixin columns. [#5469]

- Support generalized value formatting for mixin columns in tables. [#5274]

- Support persistence of table indices when pickling and copying table. [#5468]

astropy.tests
^^^^^^^^^^^^^

- Install both runtime and test dependencies when running the
  ./setup.py test command. These dependencies are specified by the
  install_requires and tests_require keywords via setuptools. [#5092]

- Enable easier subclassing of the TestRunner class. [#5505]

astropy.time
^^^^^^^^^^^^

- ``light_travel_time`` can now use more accurate JPL ephemerides. [#5273, #5436]

astropy.units
^^^^^^^^^^^^^

- Added ``pixel_scale`` and ``plate_scale`` equivalencies. [#4987]

- The ``spectral_density`` equivalency now supports transformations of
  luminosity density. [#5151]

- ``Quantity`` now accepts strings consisting of a number and unit such
  as '10 km/s'. [#5245]

astropy.utils
^^^^^^^^^^^^^

- Added a new decorator: ``deprecated_renamed_argument``. This can be used to
  rename a function argument, while it still allows for the use of the older
  argument name. [#5214]

astropy.visualization
^^^^^^^^^^^^^^^^^^^^^

- Added a ``make_lupton_rgb`` function to generate color images from three
  greyscale images, following the algorithm of Lupton et al. (2004). [#5535]

- Added ``data`` and ``interval`` inputs to the ``ImageNormalize``
  class. [#5206]

- Added a new ``simple_norm`` convenience function. [#5206]

- Added a default stretch for the ``Normalization`` class. [#5206].

- Added a default ``vmin/vmax`` for the ``ManualInterval`` class.
  [#5206].

- The ``wcsaxes`` subpackage has now been integrated in astropy as
  ``astropy.visualization.wcsaxes``.  This allows plotting of astronomical
  data/coordinate systems in Matplotlib. [#5496]

astropy.wcs
^^^^^^^^^^^

- Improved ``footprint_to_file``: allow to specify the coordinate system, and
  use by default the one from ``RADESYS``. Overwrite the file instead of
  appending to it. [#5494]


API Changes
-----------

astropy.convolution
^^^^^^^^^^^^^^^^^^^

- ``discretize_model`` now raises an exception if non-integer ranges are used.
  Previously it had incorrect behavior but did not raise an exception. [#5538]

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- ``SkyCoord``, ``ICRS``, and other coordinate objects, as well as the
  underlying representations such as ``SphericalRepresentation`` and
  ``CartesianRepresentation`` can now be reshaped using methods named like the
  numpy ones for ``ndarray`` (``reshape``, ``swapaxes``, etc.)
  [#4123, #5254, #5482]

- The ``obsgeoloc`` and ``obsgeovel`` attributes of ``GCRS`` and
  ``PrecessedGeocentric`` frames are now stored and returned as
  ``CartesianRepresentation`` objects, rather than ``Quantity`` objects.
  Similarly, ``EarthLocation.get_gcrs_posvel`` now returns a tuple of
  ``CartesianRepresentation`` objects. [#5253]

- ``search_around_3d`` and ``search_around_sky`` now return units
  for the distance matching their input argument when no match is
  found, instead of ``dimensionless_unscaled``. [#5528]

astropy.io.ascii
^^^^^^^^^^^^^^^^

- ASCII writers now accept an 'overwrite' argument.
  The default behavior is changed so that a warning will be
  issued when overwriting an existing file unless ``overwrite=True``.
  In a future version this will be changed from a warning to an
  exception to prevent accidentally overwriting a file. [#5007]

- The default representation of masked values when writing tables was
  changed from ``'--'`` to the empty string ``''``.  Previously any
  user-supplied ``fill_values`` parameter would overwrite the class
  default, but now the values are prepended to the class default. [#5347]

astropy.io.fits
^^^^^^^^^^^^^^^

- The old ``Header`` interface, deprecated since Astropy 0.1 (PyFITS 3.1), has
  been removed entirely. See :ref:`header-transition-guide` for explanations
  on this change and help on the transition. [#5310]

- The following functions, classes and methods have been removed:
  ``CardList``, ``Card.key``, ``Card.cardimage``, ``Card.ascardimage``,
  ``create_card``, ``create_card_from_string``, ``upper_key``,
  ``Header.ascard``, ``Header.rename_key``, ``Header.get_history``,
  ``Header.get_comment``, ``Header.toTxtFile``, ``Header.fromTxtFile``,
  ``new_table``, ``tdump``, ``tcreate``, ``BinTableHDU.tdump``,
  ``BinTableHDU.tcreate``.

- Removed ``txtfile`` argument to the ``Header`` constructor.

- Removed usage of ``Header.update`` with ``Header.update(keyword, value,
  comment)`` arguments.

- Removed ``startColumn`` and ``endColumn`` arguments to the ``FITS_record``
  constructor.

- The ``clobber`` argument in FITS writers has been renamed to
  ``overwrite``. This change affects the following functions and
  methods: ``tabledump``, ``writeto``, ``Header.tofile``,
  ``Header.totextfile``, ``_BaseDiff.report``,
  ``_BaseHDU.overwrite``, ``BinTableHDU.dump`` and
  ``HDUList.writeto``. [#5171]

- Added an optional ``copy`` parameter to ``fits.Header`` which controls if
  a copy is made when creating an ``Header`` from another ``Header``.
  [#5005, #5326]

astropy.io.registry
^^^^^^^^^^^^^^^^^^^

- ``.fts`` and ``.fts.gz`` files will be automatically identified as
  ``io.fits`` files if no explicit ``format`` is given. [#5211]

- Added an optional ``readwrite`` parameter for ``get_formats`` to filter
  formats for read or write. [#5275]

astropy.modeling
^^^^^^^^^^^^^^^^

- ``Gaussian2D`` now raises an error if ``theta`` is set at the same time as
  ``cov_matrix`` (previously ``theta`` was silently ignored). [#5537]

astropy.table
^^^^^^^^^^^^^

- Setting an existing table column (e.g. ``t['a'] = [1, 2, 3]``) now defaults
  to *replacing* the column with a column corresponding to the new value
  (using ``t.replace_column()``) instead of doing an in-place update.  Any
  existing meta-data in the column (e.g. the unit) is discarded.  An
  in-place update is still done when the new value is not a valid column,
  e.g. ``t['a'] = 0``.  To force an in-place update use the pattern
  ``t['a'][:] = [1, 2, 3]``. [#5556]

- Allow ``collections.Mapping``-like ``data`` attribute when initializing a
  ``Table`` object (``dict``-like was already possible). [#5213]

astropy.tests
^^^^^^^^^^^^^

- The inputs to the ``TestRunner.run_tests()`` method now must be
  keyword arguments (no positional arguments).  This applies to the
  ``astropy.test()`` function as well. [#5505]

astropy.utils
^^^^^^^^^^^^^

- Renamed ``ignored`` context manager in ``compat.misc`` to ``suppress``
  to be consistent with https://bugs.python.org/issue19266 . [#5003]

astropy.visualization
^^^^^^^^^^^^^^^^^^^^^

- Deprecated the ``scale_image`` function. [#5206]

- The ``mpl_normalize`` module (containing the ``ImageNormalize``
  class) is now automatically imported with the ``visualization``
  subpackage. [#5491]

astropy.vo
^^^^^^^^^^

- The ``clobber`` argument in ``VOSDatabase.to_json()`` has been
  renamed to ``overwrite``. [#5171]

astropy.wcs
^^^^^^^^^^^

- ``wcs.rotateCD()`` was deprecated without a replacement. [#5240]

Bug Fixes
---------

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- Transformations between CIRS and AltAz now correctly account for the
  location of the observer. [#5591]

- GCRS frames representing a location on Earth with multiple obstimes are now
  allowed. This means that the solar system routines ``get_body``,
  ``get_moon`` and ``get_sun`` now work with non-scalar times and a
  non-geocentric observer. [#5253]

astropy.io.ascii
^^^^^^^^^^^^^^^^

- Fix issue with units or other astropy core classes stored in table meta.
  [#5605]

astropy.io.fits
^^^^^^^^^^^^^^^

- Copying a ``fits.Header`` using ``copy`` or ``deepcopy`` from the ``copy``
  module will use ``Header.copy`` to ensure that modifying the copy will
  not alter the other original Header and vice-versa. [#4990, #5323]

- ``HDUList.info()`` no longer raises ``AttributeError`` in presence of
  ``BZERO``. [#5508]

- Avoid exceptions with numpy 1.10 and up when using scaled integer data
  where ``BZERO`` has float type but integer value. [#4639, #5527]

- Converting a header card to a string now calls ``self.verify('fix+warn')``
  instead of ``self.verify('fix')`` so headers with invalid keywords will
  not raise a ``VerifyError`` on printing. [#887,#5054]

- ``FITS_Record._convert_ascii`` now converts blank fields to 0 when a
  non-blank null column value is set. [#5134, #5394]

astropy.io.registry
^^^^^^^^^^^^^^^^^^^

- ``read`` now correctly raises an IOError if a file with an unknown
  extension can't be found, instead of raising IORegistryError:
  "Format could not be identified." [#4779]

astropy.time
^^^^^^^^^^^^

- Ensure ``Time`` instances holding a single ``delta_ut1_utc`` can be copied,
  flattened, etc. [#5225]

astropy.units
^^^^^^^^^^^^^

- Operations involving ``Angle`` or ``Distance``, or any other
  ``SpecificTypeQuantity`` instance, now also keep return an instance of the
  same type if the instance was the second argument (if the resulting unit
  is consistent with the specific type). [#5327]

- Inplace operations on ``Angle`` and ``Distance`` instances now raise an
  exception if the final unit is not equivalent to radian and meter, resp.
  Similarly, views as ``Angle`` and ``Distance`` can now only be taken
  from quantities with appropriate units, and views as ``Quantity`` can only
  be taken from logarithmic quanties such as ``Magnitude`` if the physical
  unit is dimensionless. [#5070]

- Conversion from quantities to logarithmic units now correctly causes a
  logarithmic quantity such as ``Magnitude`` to be returned. [#5183]


astropy.wcs
^^^^^^^^^^^

- SIP distortion for an alternate WCS is correctly initialized now by
  looking at the "CTYPE" values matching the alternate WCS. [#5443]

Other Changes and Additions
---------------------------

- The bundled ERFA was updated to version 1.3.0.  This includes the
  leap second planned for 2016 Dec 31.

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- Initialization of ``Angle`` has been sped up for ``Quantity`` and ``Angle``
  input. [#4970]

- The use of ``np.matrix`` instances in the transformations has been
  deprecated, since this class does not allow stacks of matrices.  As a
  result, the semi-public functions ``angles.rotation_matrix`` and
  ``angles.angle_axis`` are also deprecated, in favour of the new routines
  with the same name in ``coordinates.matrix_utilities``. [#5104]

- A new ``BaseCoordinateFrame.cache`` dictionary has been created to expose
  the internal cache. This is useful when modifying representation data
  in-place without using ``realize_frame``. Additionally, documentation for
  in-place operations on coordinates were added. [#5575]

- Coordinates and their representations are printed with a slightly different
  format, following how numpy >= 1.12 prints structured arrays. [#5423]

astropy.cosmology
^^^^^^^^^^^^^^^^^

- The default cosmological model has been changed to Planck 2015,
  and the citation strings have been updated. [#5372]

astropy.extern
^^^^^^^^^^^^^^

- Updated the bundled ``six`` module to version 1.10.0. [#5521]

- Updated the astropy shipped version of ``PLY`` to version 3.9. [#5526]

- Updated the astropy shipped version of jQuery to v3.3.1, and dataTables
  to v1.10.12. [#5564]

astropy.io.fits
^^^^^^^^^^^^^^^

- Performance improvements for tables with many columns. [#4985]

- Removed obsolete code that was previously needed to properly
  implement the append mode. [#4793]

astropy.io.registry
^^^^^^^^^^^^^^^^^^^

- Reduced the time spent in the ``get_formats`` function. This also reduces
  the time it takes to import astropy subpackages, i.e.
  ``astropy.coordinates``. [#5262]

astropy.units
^^^^^^^^^^^^^

- The functions ``add_enabled_units``, ``set_enabled_equivalencies`` and
  ``add_enabled_equivalencies`` have been sped up by copying the current
  ``_UnitRegistry`` instead of building it from scratch. [#5306]

- To build the documentation, the ``build_sphinx`` command has been deprecated
  in favor of ``build_docs``. [#5179]

- The ``--remote-data`` option to ``python setup.py test`` can now take
  different arguments: ``--remote-data=none`` is the same as not specifying
  ``--remote-data`` (skip all tests that require the internet),
  ``--remote-data=astropy`` skips all tests that need remote data except those
  that require only data from data.astropy.org, and ``--remote-data=any`` is
  the same as ``--remote-data`` (run all tests that use remote data). [#5506]

- The pytest ``recwarn`` fixture has been removed from the tests in favor of
  ``utils.catch_warnings``. [#5489]

- Deprecated escape sequences in strings (Python 3.6) have been removed. [#5489]


1.2.2 (2016-12-22)
==================

Bug Fixes
---------

astropy.io.ascii
^^^^^^^^^^^^^^^^

- Fix a bug where the ``fill_values`` parameter was ignored when writing a
  table to HTML format. [#5379]

astropy.io.fits
^^^^^^^^^^^^^^^

- Handle unicode FITS BinTable column names on Python 2 [#5204, #4805]

- Fix reading of float values from ASCII tables, that could be read as
  float32 instead of float64 (with the E and F formats). These values are now
  always read as float64. [#5362]

- Fixed memoryleak when using the compression module. [#5399, #5464]

- Able to insert and remove lower case HIERARCH keywords in a consistent
  manner [#5313, #5321]

astropy.stats
^^^^^^^^^^^^^

- Fixed broadcasting in ``sigma_clip`` when using negative ``axis``. [#4988]

astropy.table
^^^^^^^^^^^^^

- Assigning a logarithmic unit to a ``QTable`` column that did not have a
  unit yet now correctly turns it into the appropriate function quantity
  subclass (such as ``Magnitude`` or ``Dex``). [#5345]

- Fix default value for ``show_row_index`` in ``Table.show_in_browser``.
  [#5562]

astropy.units
^^^^^^^^^^^^^

- For inverse trig functions that operate on quantities, catch any warnings
  that occur from evaluating the function on the unscaled quantity value
  between __array_prepare__ and __array_wrap__. [#5153]

- Ensure ``!=`` also works for function units such as ``MagUnit`` [#5345]

astropy.wcs
^^^^^^^^^^^

- Fix use of the ``relax`` keyword in ``to_header`` when used to change the
  output precision. [#5164]

- ``wcs.to_header(relax=True)`` adds a "-SIP" suffix to ``CTYPE`` when SIP
  distortion is present in the WCS object. [#5239]

- Improved log messages in ``to_header``. [#5239]

Other Changes and Additions
---------------------------

- The bundled ERFA was updated to version 1.3.0.  This includes the
  leap second planned for 2016 Dec 31.

astropy.stats
^^^^^^^^^^^^^

- ``poisson_conf_interval`` with ``'kraft-burrows-nousek'`` interval is now
  faster and usable with SciPy versions < 0.14. [#5064, #5290]



1.2.1 (2016-06-22)
==================

Bug Fixes
---------

astropy.io.fits
^^^^^^^^^^^^^^^

- Fixed a bug that caused TFIELDS to not be in the correct position in
  compressed image HDU headers under certain circumstances, which created
  invalid FITS files. [#5118, #5125]

astropy.units
^^^^^^^^^^^^^

- Fixed an  ``ImportError`` that occurred whenever ``astropy.constants`` was
  imported before ``astropy.units``. [#5030, #5121]

- Magnitude zero points used to define ``STmag``, ``ABmag``, ``M_bol`` and
  ``m_bol`` are now collected in ``astropy.units.magnitude_zero_points``.
  They are not enabled as regular units by default, but can be included
  using ``astropy.units.magnitude_zero_points.enable()``. This makes it
  possible to round-trip magnitudes as originally intended.  [#5030]

1.2 (2016-06-19)
================

General
-------

- Astropy now requires Numpy 1.7.0 or later. [#4784]

New Features
------------

astropy.constants
^^^^^^^^^^^^^^^^^

- Add ``L_bol0``, the luminosity corresponding to absolute bolometric
  magnitude zero. [#4262]

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- ``CartesianRepresentation`` now includes a transform() method that can take
  a 3x3 matrix to transform coordinates. [#4860]

- Solar system and lunar ephemerides accessible via ``get_body``,
  ``get_body_barycentric`` and ``get_moon`` functions. [#4890]

- Added astrometric frames (i.e., a frame centered on a particular
  point/object specified in another frame). [#4909, #4941]

- Added ``SkyCoord.spherical_offsets_to`` method. [#4338]

- Recent Earth rotation (IERS) data are now auto-downloaded so that AltAz
  transformations for future dates now use the most accurate available
  rotation values. [#4436]

- Add support for heliocentric coordinate frames. [#4314]

astropy.cosmology
^^^^^^^^^^^^^^^^^

- ``angular_diameter_distance_z1z2`` now supports the computation of
  the angular diameter distance between a scalar and an array like
  argument. [#4593] The method now supports models with negative
  Omega_k0 (positive curvature universes) [#4661] and allows z2 < z1.

astropy.io.ascii
^^^^^^^^^^^^^^^^

- File name could be passed as ``Path`` object. [#4606]

- Check that columns in ``formats`` specifier exist in the output table
  when writing. [#4508, #4511]

- Allow trailing whitespace in the IPAC header lines. [#4758]

- Updated to filter out the default parser warning of BeautifulSoup.
  [#4551]

- Added support for reading and writing reStructuredText simple tables.
  [#4812]

astropy.io.fits
^^^^^^^^^^^^^^^

- File name could be passed as ``Path`` object. [#4606]

- Header allows a dictionary-like cards argument during creation. [#4663]

- New function ``convenience.table_to_hdu`` to allow creating a FITS
  HDU object directly from an astropy ``Table``. [#4778]

- New optional arguments ``ignore_missing`` and ``remove_all`` are added
  to ``astropy.io.fits.header.remove()``. [#5020]

astropy.io.registry
^^^^^^^^^^^^^^^^^^^

- Added custom ``IORegistryError``. [#4833]

astropy.io.votable
^^^^^^^^^^^^^^^^^^

- File name could be passed as ``Path`` object. [#4606]

astropy.modeling
^^^^^^^^^^^^^^^^

- Added the fittable=True attribute to the Scale and Shift models with tests. [#4718]

- Added example plots to docstrings for some built-in models. [#4008]

astropy.nddata
^^^^^^^^^^^^^^

- ``UnknownUncertainty`` new subclass of ``NDUncertainty`` that can be used to
  save uncertainties that cannot be used for error propagation. [#4272]

- ``NDArithmeticMixin``: ``add``, ``subtract``, ``multiply`` and ``divide``
  can be used as classmethods but require that two operands are given. These
  operands don't need to be NDData instances but they must be convertible to
  NDData. This conversion is done internally. Using it on the instance does
  not require (but also allows) two operands. [#4272, #4851]

- ``NDDataRef`` new subclass that implements ``NDData`` together with all
  currently available mixins. This class does not implement additional
  attributes, methods or a numpy.ndarray-like interface like ``NDDataArray``.
  attributes, methods or a numpy.ndarray-like interface like ``NDDataArray``.
  [#4797]

astropy.stats
^^^^^^^^^^^^^

- Added ``axis`` keyword for ``mad_std`` function. [#4688, #4689]

- Added Bayesian and Akaike Information Criteria. [#4716]

- Added Bayesian upper limits for Poisson count rates. [#4622]

- Added ``circstats``; a module for computing circular statistics. [#3705, #4472]

- Added ``jackknife`` resampling method. [#3708, #4439]

- Updated ``bootstrap`` to allow bootstrapping statistics with multiple
  outputs. [#3601]

- Added ``LombScargle`` class to compute Lomb-Scargle periodograms [#4811]

astropy.table
^^^^^^^^^^^^^

- ``Table.show_in_notebook`` and ``Table.show_in_browser(jsviewer=True)`` now
  yield tables with an "idx" column, allowing easy identification of the index
  of a row even when the table is re-sorted in the browser. [#4404]

- Added ``AttributeError`` when trying to set mask on non-masked table. [#4637]

- Allow to use a tuple of keys in ``Table.sort``.  [#4671]

- Added ``itercols``; a way to iterate through columns of a table. [#3805,
  #4888]

- ``Table.show_in_notebook`` and the default notebook display (i.e.,
  ``Table._repr_html_``) now use consistent table styles which can be set
  using the ``astropy.table.default_notebook_table_class`` configuration
  item. [#4886]

- Added interface to create ``Table`` directly from any table-like object
  that has an ``__astropy_table__`` method.  [#4885]

astropy.tests
^^^^^^^^^^^^^

- Enable test runner to obtain documentation source files from directory
  other than "docs". [#4748]

astropy.time
^^^^^^^^^^^^

- Added caching of scale and format transformations for improved performance.
  [#4422]

- Recent Earth rotation (IERS) data are now auto-downloaded so that UT1
  transformations for future times now work out of the box. [#4436]

- Add support for barycentric/heliocentric time corrections. [#4314]

astropy.units
^^^^^^^^^^^^^

- The option to use tuples to indicate fractional powers of units,
  deprecated in 0.3.1, has been removed. [#4449]

- Added slug to imperial units. [#4670]

- Added Earth radius (``R_earth``) and Jupiter radius (``R_jup``) to units.
  [#4818]

- Added a ``represents`` property to allow access to the definition of a
  named unit (e.g., ``u.kpc.represents`` yields ``1000 pc``). [#4806]

- Add bolometric absolute and apparent magnitudes, ``M_bol`` and ``m_bol``.
  [#4262]

astropy.utils
^^^^^^^^^^^^^

- ``Path`` object could be passed to ``get_readable_fileobj``. [#4606]

- Implemented a generic and extensible way of merging metadata. [#4459]

- Added ``format_doc`` decorator which allows to replace and/or format the
  current docstring of an object. [#4242]

- Added a new context manager ``set_locale`` to temporarily set the
  current locale. [#4363]

- Added new IERS_Auto class to auto-download recent IERS (Earth rotation)
  data when required by coordinate or time transformations. [#4436]

astropy.visualization
^^^^^^^^^^^^^^^^^^^^^

- Add zscale interval based on Numdisplay's implementation. [#4776]

API changes
-----------

astropy.config
^^^^^^^^^^^^^^

- The deprecated ``ConfigurationItem`` and ``ConfigAlias`` classes and the
  ``save_config``, ``get_config_items``, and ``generate_all_config_items``
  functions have now been removed. [#2767, #4446]

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- Removed compatibility layer for pre-v0.4 API. [#4447]

- Added ``copy`` keyword-only argument to allow initialization without
  copying the (possibly large) input coordinate arrays. [#4883]

astropy.cosmology
^^^^^^^^^^^^^^^^^

- Improve documentation of z validity range of cosmology objects [#4882, #4949]

astropy.io.ascii
^^^^^^^^^^^^^^^^

- Add a way to control HTML escaping when writing a table as an HTML file. [#4423]

astropy.io.fits
^^^^^^^^^^^^^^^

- Two optional boolean arguments ``ignore_missing`` and ``remove_all`` are
  added to ``Header.remove``. [#5020]

astropy.modeling
^^^^^^^^^^^^^^^^

- Renamed ``Redshift`` model to ``RedshiftScaleFactor``. [#3672]

- Inputs (``coords`` and ``out``) to ``render`` function in ``Model`` are
  converted to float. [#4697]

- ``RotateNative2Celestial`` and ``RotateCelestial2Native`` are now
  implemented as subclasses of ``EulerAngleRotation``. [#4881, #4940]

astropy.nddata
^^^^^^^^^^^^^^

- ``NDDataBase`` does not set the private uncertainty property anymore. This
  only affects you if you subclass ``NDDataBase`` directly. [#4270]

- ``NDDataBase``: the ``uncertainty``-setter is removed. A similar one is
  added in ``NDData`` so this also only affects you if you subclassed
  ``NDDataBase`` directly. [#4270]

- ``NDDataBase``: ``uncertainty``-getter returns ``None`` instead of the
  private uncertainty and is now abstract. This getter is moved to
  ``NDData`` so it only affects direct subclasses of ``NDDataBase``. [#4270]

- ``NDData`` accepts a Quantity-like data and an explicitly given unit.
  Before a ValueError was raised in this case. The final instance will use the
  explicitly given unit-attribute but doesn't check if the units are
  convertible and the data will not be scaled. [#4270]

- ``NDData`` : the given mask, explicit or implicit if the data was masked,
  will be saved by the setter. It will not be saved directly as the private
  attribute. [#4879]

- ``NDData`` accepts an additional argument ``copy`` which will copy every
  parameter before it is saved as attribute of the instance. [#4270]

- ``NDData``: added an ``uncertainty.getter`` that returns the private
  attribute. It is equivalent to the old ``NDDataBase.uncertainty``-getter.
  [#4270]

- ``NDData``: added an ``uncertainty.setter``. It is slightly modified with
  respect to the old ``NDDataBase.uncertainty``-setter. The changes include:

- if the uncertainty has no uncertainty_type an info message is printed
  instead of a TypeError and the uncertainty is saved as
  ``UnknownUncertainty`` except the uncertainty is None. [#4270]

- the requirement that the uncertainty_type of the uncertainty needs to be a
  string was removed. [#4270]

- if the uncertainty is a subclass of NDUncertainty the parent_nddata
  attribute will be set so the uncertainty knows to which data it belongs.
  This is also a Bugfix. [#4152, #4270]

- ``NDData``: added a ``meta``-getter, which will set and return an empty
  OrderedDict if no meta was previously set. [#4509, #4469]

- ``NDData``: added an ``meta``-setter. It requires that the meta is
  dictionary-like (it also accepts Headers or ordered dictionaries and others)
  or None. [#4509, #4469, #4921]

- ``NDArithmeticMixin``: The operand in arithmetic methods (``add``, ...)
  doesn't need to be a subclass of ``NDData``. It is sufficient if it can be
  converted to one. This conversion is done internally. [#4272]

- ``NDArithmeticMixin``: The arithmetic methods allow several new arguments to
  control how or if different attributes of the class will be processed during
  the operation. [#4272]

- ``NDArithmeticMixin``: Giving the parameter ``propagate_uncertainties`` as
  positional keyword is deprecated and will be removed in the future. You now
  need to specify it as keyword-parameter. Besides ``True`` and ``False`` also
  ``None`` is now a valid value for this parameter. [#4272, #4851]

- ``NDArithmeticMixin``: The wcs attribute of the operands is not compared and
  thus raises no ValueError if they differ, except if a ``compare_wcs``
  parameter is specified. [#4272]

- ``NDArithmeticMixin``: The arithmetic operation was split from a general
  ``_arithmetic`` method to different specialized private methods to allow
  subclasses more control on how the attributes are processed without
  overriding ``_arithmetic``. The ``_arithmetic`` method is now used to call
  these other methods. [#4272]

- ``NDSlicingMixin``: If the attempt at slicing the mask, wcs or uncertainty
  fails with a ``TypeError`` a Warning is issued instead of the TypeError. [#4271]

- ``NDUncertainty``: ``support_correlated`` attribute is deprecated in favor of
  ``supports_correlated`` which is a property. Also affects
  ``StdDevUncertainty``. [#4272]

- ``NDUncertainty``: added the ``__init__`` that was previously implemented in
  ``StdDevUncertainty`` and takes an additional ``unit`` parameter. [#4272]

- ``NDUncertainty``: added a ``unit`` property without setter that returns the
  set unit or if not set the unit of the parent. [#4272]

- ``NDUncertainty``: included a ``parent_nddata`` property similar to the one
  previously implemented in StdDevUncertainty. [#4272]

- ``NDUncertainty``: added an ``array`` property with setter. The setter will
  convert the value to a plain numpy array if it is a list or a subclass of a
  numpy array. [#4272]

- ``NDUncertainty``: ``propagate_multiply`` and similar were removed. Before
  they were abstract properties and replaced by methods with the same name but
  with a leading underscore. The entry point for propagation is a method
  called ``propagate``. [#4272]

- ``NDUncertainty`` and subclasses: implement a representation (``__repr__``).
  [#4787]

- ``StdDevUncertainty``: error propagation allows an explicitly given
  correlation factor, which may be a scalar or an array which will be taken
  into account during propagation.
  This correlation must be determined manually and is not done by the
  uncertainty! [#4272]

- ``StdDevUncertainty``: the ``array`` is converted to a plain numpy array
  only if it's a list or a subclass of numpy.ndarray. Previously it was always
  cast to a numpy array but also allowed subclasses. [#4272]

- ``StdDevUncertainty``: setting the ``parent_nddata`` does not compare if the
  shape of it's array is identical to the parents data shape. [#4272]

- ``StdDevUncertainty``: the ``array.setter`` doesn't compare if the array has
  the same shape as the parents data. [#4272]

- ``StdDevUncertainty``: deprecated ``support_correlated`` in favor of
  ``supports_correlated``. [#4272, #4828]

- ``StdDevUncertainty``: deprecated ``propagate_add`` and similar methods in
  favor of ``propagate``. [#4272, #4828]

- Allow ``data`` to be a named argument in ``NDDataArray``. [#4626]

astropy.table
^^^^^^^^^^^^^

- ``operations.unique`` now has a ``keep`` parameter, which allows
  one to select whether to keep the first or last row in a set of
  duplicate rows, or to remove all rows that are duplicates. [#4632]

- ``QTable`` now behaves more consistently by making columns act as a
  ``Quantity`` even if they are assigned a unit after the table is
  created. [#4497, #4884]

astropy.units
^^^^^^^^^^^^^

- Remove deprecated ``register`` argument for Unit classes. [#4448]

astropy.utils
^^^^^^^^^^^^^

- The astropy.utils.compat.argparse module has now been deprecated. Use the
  Python 'argparse' module directly instead. [#4462]

- The astropy.utils.compat.odict module has now been deprecated. Use the
  Python 'collections' module directly instead. [#4466]

- The astropy.utils.compat.gzip module has now been deprecated. Use the
  Python 'gzip' module directly instead. [#4464]

- The deprecated ``ScienceStateAlias`` class has been removed. [#2767, #4446]

- The astropy.utils.compat.subprocess module has now been deprecated. Use the
  Python 'subprocess' module instead. [#4483]

- The astropy.utils.xml.unescaper module now also unescapes ``'%2F'`` to
  ``'/'`` and ``'&&'`` to ``'&'`` in a given URL. [#4699]

- The astropy.utils.metadata.MetaData descriptor has now two optional
  parameters: doc and copy. [#4921]

- The default IERS (Earth rotation) data now is now auto-downloaded via a
  new class IERS_Auto.  When extrapolating UT1-UTC or polar motion values
  outside the available time range, the values are now clipped at the last
  available value instead of being linearly extrapolated. [#4436]

astropy.wcs
^^^^^^^^^^^

- WCS objects can now be initialized with an ImageHDU or
  PrimaryHDU object. [#4493, #4505]

- astropy.wcs now issues an INFO message when the header has SIP coefficients but
  "-SIP" is missing from CTYPE. [#4814]

Bug fixes
---------

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- Ameliorate a problem with ``get_sun`` not round-tripping due to
  approximations in the light deflection calculation. [#4952]

- Ensure that ``angle_utilities.position_angle`` accepts floats, as stated
  in the docstring. [#3800]

- Ensured that transformations for ``GCRS`` frames are correct for
  non-geocentric observers. [#4986]

- Fixed a problem with the ``Quantity._repr_latex_`` method causing errors
  when showing an ``EarthLocation`` in a Jupyter notebook. [#4542, #5068]

astropy.io.ascii
^^^^^^^^^^^^^^^^

- Fix a problem where the fast reader (with use_fast_converter=False) can
  fail on non-US locales. [#4363]

- Fix astropy.io.ascii.read handling of units for IPAC formatted files.
  Columns with no unit are treated as unitless not dimensionless.
  [#4867, #4947]

- Fix problems the header parsing in the sextractor reader. [#4603, #4910]

astropy.io.fits
^^^^^^^^^^^^^^^

- ``GroupsHDU.is_image`` property is now set to ``False``. [#4742]

- Ensure scaling keywords are removed from header when unsigned integer data
  is converted to signed type. [#4974, #5053]

- Made TFORMx keyword check more flexible in test of compressed images to
  enable compatibility of the test with cfitsio 3.380. [#4646, #4653]

astropy.io.votable
^^^^^^^^^^^^^^^^^^

- The astropy.io.votable.validator.html module is updated to handle division
  by zero when generating validation report. [#4699]

- KeyError when converting Table v1.2 numeric arrays fixed. [#4782]

astropy.modeling
^^^^^^^^^^^^^^^^

- Refactored ``AiryDisk2D``, ``Sersic1D``, and ``Sersic2D`` models
  to be able to combine them as classes as well as instances. [#4720]

- Modified the "LevMarLSQFitter" class to use the weights in the
  calculation of the Jacobian. [#4751]

astropy.nddata
^^^^^^^^^^^^^^

- ``NDData`` giving masked_Quantities as data-argument will use the
  implicitly passed mask, unit and value. [#4270]

- ``NDData`` using a subclass implementing ``NDData`` with
  ``NDArithmeticMixin`` now allows error propagation. [#4270]

- Fixed memory leak that happened when uncertainty of ``NDDataArray`` was
  set. [#4825, #4862]

- ``StdDevUncertainty``: During error propagation the unit of the uncertainty
  is taken into account. [#4272]

- ``NDArithmeticMixin``: ``divide`` and ``multiply`` yield correct
  uncertainties if only one uncertainty is set. [#4152, #4272]

astropy.stats
^^^^^^^^^^^^^

- Fix ``sigma_clipped_stats`` to use the ``axis`` argument. [#4726, #4808]

astropy.table
^^^^^^^^^^^^^

- Fixed bug where Tables created from existing Table objects were not
  inheriting the ``primary_key`` attribute. [#4672, #4930]

- Provide more detail in the error message when reading a table fails due to a
  problem converting column string values. [#4759]

astropy.units
^^^^^^^^^^^^^

- Exponentiation using a ``Quantity`` with a unit equivalent to dimensionless
  as base and an ``array``-like exponent yields the correct result. [#4770]

- Ensured that with ``spectral_density`` equivalency one could also convert
  between ``photlam`` and ``STmag``/``ABmag``. [#5017]

astropy.utils
^^^^^^^^^^^^^

- The astropy.utils.compat.fractions module has now been deprecated. Use the
  Python 'fractions' module directly instead. [#4463]

- Added ``format_doc`` decorator which allows to replace and/or format the
  current docstring of an object. [#4242]

- Attributes using the astropy.utils.metadata.MetaData descriptor are now
  included in the sphinx documentation. [#4921]

astropy.vo
^^^^^^^^^^

- Relaxed expected accuracy of Cone Search prediction test to reduce spurious
  failures. [#4382]

astropy.wcs
^^^^^^^^^^^

- astropy.wcs.to_header removes "-SIP" from CTYPE when SIP coefficients
  are not written out, i.e. ``relax`` is either ``False`` or ``None``.
  astropy.wcs.to_header appends "-SIP" to CTYPE when SIP coefficients
  are written out, i.e. ``relax=True``. [#4814]

- Made ``wcs.bounds_check`` call ``wcsprm_python2c``, which means it
  works even if ``wcs.set`` has not been called yet. [#4957, #4966].

- WCS objects can no longer be reverse-indexed, which was technically
  permitted but incorrectly implemented previously [#4962]

Other Changes and Additions
---------------------------

- Python 2.6 is no longer supported. [#4486]

- The bundled version of py.test has been updated to 2.8.3. [#4349]

- Reduce Astropy's import time (``import astropy``) by almost a factor 2. [#4649]

- Cython prerequisite for building changed to v0.19 in install.rst [#4705,
  #4710, #4719]

- All astropy.modeling functionality that was deprecated in Astropy 1.0 has
  been removed. [#4857]

- Added instructions for installing Astropy into CASA. [#4840]

- Added an example gallery to the docs demonstrating short
  snippets/examples. [#4734]


1.1.2 (2016-03-10)
==================

New Features
------------

astropy.wcs
^^^^^^^^^^^

- The ``astropy.wcs`` module now exposes ``WCSHDO_P*`` constants that can be
  used to allow more control over output precision when using the ``relax``
  keyword argument. [#4616]

Bug Fixes
---------

astropy.io.ascii
^^^^^^^^^^^^^^^^

- Fixed handling of CDS data file when no description is given and also
  included stripping out of markup for missing value from description. [#4437, #4474]

astropy.io.fits
^^^^^^^^^^^^^^^

- Fixed possible segfault during error handling in FITS tile
  compression. [#4489]

- Fixed crash on pickling of binary table columns with the 'X', 'P', or
  'Q' format. [#4514]

- Fixed memory / reference leak that could occur when copying a ``FITS_rec``
  object (the ``.data`` for table HDUs). [#520]

- Fixed a memory / reference leak in ``FITS_rec`` that occurred in a wide
  range of cases, especially after writing FITS tables to a file, but in
  other cases as well. [#4539]

astropy.modeling
^^^^^^^^^^^^^^^^

- Fix a bug to allow instantiation of a modeling class having a parameter
  with a custom setter that takes two parameters ``(value, model)`` [#4656]

astropy.table
^^^^^^^^^^^^^

- Fixed bug when replacing a table column with a mixin column like
  Quantity or Time. [#4601]

- Disable initial ordering in jsviewer (``show_in_browser``,
  ``show_in_notebook``) to respect the order from the Table. [#4628]

astropy.units
^^^^^^^^^^^^^

- Fixed sphinx issues on plotting quantities. [#4527]

astropy.utils
^^^^^^^^^^^^^

- Fixed latex representation of function units. [#4563]

- The ``zest.releaser`` hooks included in Astropy are now injected locally to
  Astropy, rather than being global. [#4650]

astropy.visualization
^^^^^^^^^^^^^^^^^^^^^

- Fixed ``fits2bitmap`` script to allow ext flag to contain extension
  names or numbers. [#4468]

- Fixed ``fits2bitmap`` default output filename generation for
  compressed FITS files. [#4468]

- Fixed ``quantity_support`` to ensure its conversion returns ndarray
  instances (needed for numpy >=1.10). [#4654]

astropy.wcs
^^^^^^^^^^^

- Fixed possible exception in handling of SIP headers that was introduced in
  v1.1.1. [#4492]

- Fixed a bug that caused WCS objects with a high dynamic range of values for
  certain parameters to lose precision when converted to a header. This
  occurred for example in cases of spectral cubes, where a spectral axis in
  Hz might have a CRVAL3 value greater than 1e10 but the spatial coordinates
  would have CRVAL1/2 values 8 to 10 orders of magnitude smaller. This bug
  was present in Astropy 1.1 and 1.1.1 but not 1.0.x. This has now been fixed
  by ensuring that all WCS keywords are output with 14 significant figures by
  default. [#4616]

Other Changes and Additions
---------------------------

- Updated bundled astropy-helpers to v1.1.2. [#4678]

- Updated bundled copy of WCSLIB to 5.14. [#4579]


1.1.1 (2016-01-08)
==================

New Features
------------

astropy.io.registry
^^^^^^^^^^^^^^^^^^^

- Allow ``pathlib.Path`` objects (available in Python 3.4 and later) for
  specifying the file name in registry read / write functions. [#4405]

astropy.utils
^^^^^^^^^^^^^

- ``console.human_file_size`` now accepts quantities with byte-equivalent
  units [#4373]

Bug Fixes
---------

astropy.analytic_functions
^^^^^^^^^^^^^^^^^^^^^^^^^^

- Fixed the blackbody functions' handling of overflows on some platforms
  (Windows with MSVC, older Linux versions) with a buggy ``expm1`` function.
  [#4393]

astropy.io.fits
^^^^^^^^^^^^^^^

- Fixed an bug where updates to string columns in FITS tables were not saved
  on Python 3. [#4452]

Other Changes and Additions
---------------------------

- Updated bundled astropy-helpers to v1.1.1. [#4413]


1.1 (2015-12-11)
================

New Features
------------

astropy.config
^^^^^^^^^^^^^^

- Added new tools ``set_temp_config`` and ``set_temp_cache`` which can be
  used either as function decorators or context managers to temporarily
  use alternative directories in which to read/write the Astropy config
  files and download caches respectively.  This is especially useful for
  testing, though ``set_temp_cache`` may also be used as a way to provide
  an alternative (application specific) download cache for large data files,
  rather than relying on the default cache location in users' home
  directories. [#3975]

astropy.constants
^^^^^^^^^^^^^^^^^

- Added the Thomson scattering cross-section. [#3839]

astropy.convolution
^^^^^^^^^^^^^^^^^^^

- Added Moffat2DKernel. [#3965]

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- Added ``get_constellation`` function and ``SkyCoord.get_constellation``
  convenience method to determine the constellation that a coordinate
  is in. [#3758]

- Added ``PrecessedGeocentric`` frame, which is based on GCRS, but precessed
  to a specific requested mean equinox. [#3758]

- Added ``Supergalactic`` frame to support de Vaucouleurs supergalactic
  coordinates. [#3892]

- ``SphericalRepresentation`` now has a ``._unit_representation`` class attribute to specify
  an equivalent UnitSphericalRepresentation. This allows subclasses of
  representations to pair up correctly. [#3757]

- Added functionality to support getting the locations of observatories by
  name. See ``astropy.coordinates.EarthLocation.of_site``. [#4042]

- Added ecliptic coordinates, including ``GeocentricTrueEcliptic``,
  ``BarycentricTrueEcliptic``, and ``HeliocentricTrueEcliptic``. [#3749]

astropy.cosmology
^^^^^^^^^^^^^^^^^

- Add Planck 2015 cosmology [#3476]

- Distance calculations now > 20-40x faster for the supplied
  cosmologies due to implementing Cython scalar versions of
  ``FLRW.inv_efunc``.[#4127]

astropy.io.ascii
^^^^^^^^^^^^^^^^

- Automatically use ``guess=False`` when reading if the file ``format`` is
  provided and the format parameters are uniquely specified.  This update
  also removes duplicate format guesses to improve performance. [#3418]

- Calls to ascii.read() for fixed-width tables may now omit one of the keyword
  arguments ``col_starts`` or ``col_ends``. Columns will be assumed to begin and
  end immediately adjacent to each other. [#3657]

- Add a function ``get_read_trace()`` that returns a traceback of the
  attempted read formats for the last call to ``astropy.io.ascii.read``. [#3688]

- Supports LZMA decompression via ``get_readable_fileobj`` [#3667]

- Allow ``-`` character is Sextractor format column names. [#4168]

- Improve DAOphot reader to read multi-aperture files [#3535, #4207]

astropy.io.fits
^^^^^^^^^^^^^^^

- Support reading and writing from bzip2 compressed files. i.e. ``.fits.bz2``
  files. [#3789]

- Included a new command-line script called ``fitsinfo`` to display
  a summary of the HDUs in one or more FITS files. [#3677]

astropy.io.misc
^^^^^^^^^^^^^^^

- Support saving all meta information, description and units of tables and columns
  in HDF5 files [#4103]

astropy.io.votable
^^^^^^^^^^^^^^^^^^

- A new method was added to ``astropy.io.votable.VOTable``,
  ``get_info_by_id`` to conveniently find an ``INFO`` element by its
  ``ID`` attribute. [#3633]

- Instances in the votable tree now have better ``__repr__`` methods. [#3639]

astropy.logger.py
^^^^^^^^^^^^^^^^^

- Added log levels (e.g., DEBUG, INFO, CRITICAL) to ``astropy.log`` [#3947]

astropy.modeling
^^^^^^^^^^^^^^^^

- Added a new ``Parameter.validator`` interface for setting a validation
  method on individual model parameters.  See the ``Parameter``
  documentation for more details. [#3910]

- The projection classes that are named based on the 3-letter FITS
  WCS projections (e.g. ``Pix2Sky_TAN``) now have aliases using
  longer, more descriptive names (e.g. ``Pix2Sky_Gnomonic``).
  [#3583]

- All of the standard FITS WCS projection types have been
  implemented in ``astropy.modeling.projections`` (by wrapping
  WCSLIB). [#3906]

- Added ``Sersic1D`` and ``Sersic2D`` model classes. [#3889]

- Added the Voigt profile to existing models. [#3901]

- Added ``bounding_box`` property and ``render_model`` function [#3909]

astropy.nddata
^^^^^^^^^^^^^^

- Added ``block_reduce`` and ``block_replicate`` functions. [#3453]

- ``extract_array`` now offers different options to deal with array
  boundaries [#3727]

- Added a new ``Cutout2D`` class to create postage stamp image cutouts
  with optional WCS propagation. [#3823]

astropy.stats
^^^^^^^^^^^^^

- Added ``sigma_lower`` and ``sigma_upper`` keywords to
  ``sigma_clip`` to allow for non-symmetric clipping. [#3595]

- Added ``cenfunc``, ``stdfunc``, and ``axis`` keywords to
  ``sigma_clipped_stats``. [#3792]

- ``sigma_clip`` automatically masks invalid input values (NaNs, Infs) before
  performing the clipping [#4051]

- Added the ``histogram`` routine, which is similar to ``np.histogram`` but
  includes several additional options for automatic determination of optimal
  histogram bins. Associated helper routines include ``bayesian_blocks``,
  ``friedman_bin_width``, ``scott_bin_width``, and ``knuth_bin_width``.
  This functionality was ported from the astroML library. [#3756]

- Added the ``bayesian_blocks`` routine, which implements a dynamic algorithm
  for locating change-points in various time series. [#3756]

- A new function ``poisson_conf_interval()`` was added to allow easy calculation
  of several standard formulae for the error bars on the mean of a Poisson variable
  estimated from a single sample.

astropy.table
^^^^^^^^^^^^^

- ``add_column()`` and ``add_columns()`` now have ``rename_duplicate``
  option to rename new column(s) rather than raise exception when its name
  already exists. [#3592]

- Added ``Table.to_pandas`` and ``Table.from_pandas`` for converting to/from
  pandas dataframes. [#3504]

- Initializing a ``Table`` with ``Column`` objects no longer requires
  that the column ``name`` attribute be defined. [#3781]

- Added an ``info`` property to ``Table`` objects which provides configurable
  summary information about the table and its columns. [#3731]

- Added an ``info`` property to column classes (``Column`` or mixins).  This
  serves a dual function of providing configurable summary information about
  the column, and acting as a manager of column attributes such as
  name, format, or description. [#3731]

- Updated table and column representation to use the ``dtype_info_name``
  function for the dtype value.  Removed the default "masked=False"
  from the table representation. [#3868, #3869]

- Updated row representation to be consistent with the corresponding
  table representation for that row.  Added HTML representation so a
  row displays nicely in IPython notebook.

- Added a new table indexing engine allowing for the creation of
  indices on one or more columns of a table using ``add_index``. These
  indices enable new functionality such as searching for rows by value
  using ``loc`` and ``iloc``, as well as increased performance for
  certain operations. [#3915, #4202]

- Added capability to include a structured array or recarray in a table
  as a mixin column.  This allows for an approximation of nested tables.
  [#3925]

- Added ``keep_byteorder`` option to ``Table.as_array()``.  See the
  "API Changes" section below. [#4080]

- Added a new method ``Table.replace_column()`` to replace an existing
  column with a new data column. [#4090]

- Added a ``tableclass`` option to ``Table.pformat()`` to allow specifying
  a list of CSS classes added to the HTML table. [#4131]

- New CSS for jsviewer table [#2917, #2982, #4174]

- Added a new ``Table.show_in_notebook`` method that shows an interactive view
  of a Table (similar to ``Table.show_in_browser(jsviewer=True)``) in an
  Python/Jupyter notebook. [#4197]

- Added column alignment formatting for better pprint viewing
  experience. [#3644]

astropy.tests
^^^^^^^^^^^^^

- Added new test config options, ``config_dir`` and ``cache_dir``  (these
  can be edited in ``setup.cfg`` or as extra command-line options to
  py.test) for setting the locations to use for the Astropy config files
  and download caches (see also the related ``set_temp_config/cache``
  features added in ``astropy.config``). [#3975]

astropy.time
^^^^^^^^^^^^

- Add support for FITS standard time strings. [#3547]

- Allow the ``format`` attribute to be updated in place to change the
  default representation of a ``Time`` object. [#3673]

- Add support for shape manipulation (reshape, ravel, etc.). [#3224]

- Add argmin, argmax, argsort, min, max, ptp, sort methods. [#3681]

- Add ``Time.to_datetime`` method for converting ``Time`` objects to
  timezone-aware datetimes. [#4119, #4124]

astropy.units
^^^^^^^^^^^^^

- Added furlong to imperial units. [#3529]

- Added mil to imperial units. [#3716]

- Added stone to imperial units. [#4192]

- Added Earth Mass (``M_earth``) and Jupiter mass (``M_jup``) to units [#3907]

- Added support for functional units, in particular the logarithmic ones
  ``Magnitude``, ``Decibel``, and ``Dex``. [#1894]

- Quantities now work with the unit support in matplotlib.  See
  :ref:`plotting-quantities`. [#3981]

- Clarified imperial mass measurements and added pound force (lbf),
  kilopound (kip), and pound per square inch (psi). [#3409]

astropy.utils
^^^^^^^^^^^^^

- Added new ``OrderedDescriptor`` and ``OrderedDescriptorContainer`` utility
  classes that make it easier to implement classes with declarative APIs,
  wherein class-level attributes have an inherit "ordering" to them that is
  specified by the order in which those attributes are defined in the class
  declaration (by defining them using special descriptors that have
  ``OrderedDescriptor`` as a base class).  See the API documentation for
  these classes for more details. Coordinate frames and models now use this
  interface. [#3679]

- The ``get_pkg_data_*`` functions now take an optional ``package`` argument
  which allows specifying any package to read package data filenames or
  content out of, as opposed to only being able to use data from the package
  that the function is called from. [#4079]

- Added function ``dtype_info_name`` to the ``data_info`` module to provide
  the name of a ``dtype`` for human-readable informational purposes. [#3868]

- Added ``classproperty`` decorator--this is to ``property`` as
  ``classmethod`` is to normal instance methods. [#3982]

- ``iers.open`` now handles network URLs, as well as local paths. [#3850]

- The ``astropy.utils.wraps`` decorator now takes an optional
  ``exclude_args`` argument not shared by the standard library ``wraps``
  decorator (as it is unique to the Astropy version's ability of copying
  the wrapped function's argument signature).  ``exclude_args`` allows
  certain arguments on the wrapped function to be excluded from the signature
  of the wrapper function.  This is particularly useful when wrapping an
  instance method as a function (to exclude the ``self`` argument). [#4017]

- ``get_readable_fileobj`` can automatically decompress LZMA ('.xz')
  files using the ``lzma`` module of Python 3.3+ or, when available, the
  ``backports.lzma`` package on earlier versions. [#3667]

- The ``resolve_name`` utility now accepts any number of additional
  positional arguments that are automatically dotted together with the
  first ``name`` argument. [#4083]

- Added ``is_url_in_cache`` for resolving paths to cached files via URLS
  and checking if files exist. [#4095]

- Added a ``step`` argument to the ``ProgressBar.map`` method to give
  users control over the update frequency of the progress bar. [#4191]

astropy.visualization
^^^^^^^^^^^^^^^^^^^^^

- Added a function / context manager ``quantity_support`` for enabling
  seamless plotting of ``Quantity`` instances in matplotlib. [#3981]

- Added the ``hist`` function, which is similar to ``plt.hist`` but
  includes several additional options for automatic determination of optimal
  histogram bins. This functionality was ported from the astroML library.
  [#3756]

astropy.wcs
^^^^^^^^^^^

- The included version of wcslib has been upgraded to 5.10. [#3992, #4239]

  The minimum required version of wcslib in the 4.x series remains 4.24.

  The minimum required version of wcslib in the 5.x series is
  5.8.  Building astropy against a wcslib 5.x prior to 5.8
  will raise an ``ImportError`` when ``astropy.wcs`` is imported.

  The wcslib changes relevant to astropy are:

- The FITS headers returned by ``astropy.wcs.WCS.to_header`` and
  ``astropy.wcs.WCS.to_header_string`` now include values with
  more precision.  This will result in numerical differences in
  your results if you convert ``astropy.wcs.WCS`` objects to FITS
  headers and use the results.

- ``astropy.wcs.WCS`` now recognises the ``TPV``, ``TPD``,
  ``TPU``, ``DSS``, ``TNX`` and ``ZPX`` polynomial distortions.

- Added relaxation flags to allow ``PC0i_0ja``, ``PV0j_0ma``, and
  ``PS0j_0ma`` (i.e. with leading zeroes on the index).

- Tidied up error reporting, particularly relating to translating
  status returns from lower-level functions.

- Changed output formatting of floating point values in
  ``to_header``.

- Enhanced text representation of ``WCS`` objects. [#3604]

- The ``astropy.tests.helper`` module is now part of the public API (and has a
  documentation page).  This module was in previous releases of astropy,
  but was not considered part of the public API until now. [#3890]

- There is a new function ``astropy.online_help`` to search the
  astropy documentation and display the result in a web
  browser. [#3642]

API changes
-----------

astropy.cosmology
^^^^^^^^^^^^^^^^^

- ``FLRW._tfunc`` and ``FLRW._xfunc`` are marked as deprecated.  Users
  should use the new public interfaces ``FLRW.lookback_time_integrand``
  and ``FLRW.abs_distance_integrand`` instead. [#3767]

astropy.io.ascii
^^^^^^^^^^^^^^^^

- The default header line processing was made to be consistent with data line
  processing in that it now ignores blank lines that may have whitespace
  characters.  Any code that explicitly specifies a ``header_start`` value
  for parsing a file with blank lines in the header containing whitespace will
  need to be updated. [#2654]

astropy.io.fits
^^^^^^^^^^^^^^^

- The ``uint`` argument to ``fits.open`` is now True by default; that is,
  arrays using the FITS unsigned integer convention will be detected, and
  read as unsigned integers by default.  A new config option for
  ``io.fits``, ``enable_uint``, can be changed to False to revert to the
  original behavior of ignoring the ``uint`` convention unless it is
  explicitly requested with ``uint=True``. [#3916]

- The ``ImageHDU.NumCode`` and ``ImageHDU.ImgCode`` attributes (and same
  for other classes derived from ``_ImageBaseHDU``) are deprecated.  Instead,
  the ``astropy.io.fits`` module-level constants ``BITPIX2DTYPE`` and
  ``DTYPE2BITPIX`` can be used. [#3916]

astropy.modeling
^^^^^^^^^^^^^^^^

- Note: Comparisons of model parameters with array-like values now
  yields a Numpy boolean array as one would get with normal Numpy
  array comparison.  Previously this returned a scalar True or False,
  with True only if the comparison was true for all elements compared,
  which could lead to confusing circumstances. [#3912]

- Using ``model.inverse = None`` to reset a model's inverse to its
  default is deprecated.  In the future this syntax will explicitly make
  a model not have an inverse (even if it has a default).  Instead, use
  ``del model.inverse`` to reset a model's inverse to its default (if it
  has a default, otherwise this just deletes any custom inverse that has
  been assigned to the model and is still equivalent to setting
  ``model.inverse = None``). [#4236]

- Adds a ``model.has_user_inverse`` attribute which indicates whether or not
  a user has assigned a custom inverse to ``model.inverse``.  This is just
  for informational purposes, for example, for software that introspects
  model objects. [#4236]

- Renamed the parameters of ``RotateNative2Celestial`` and
  ``RotateCelestial2Native`` from ``phi``, ``theta``, ``psi`` to
  ``lon``, ``lat`` and ``lon_pole``. [#3578]

- Deprecated the ``Pix2Sky_AZP.check_mu`` and ``Sky2Pix_AZP.check_mu``
  methods (these were obscure "accidentally public" methods that were
  probably not used by anyone). [#3910]

- Added a phase parameter to the Sine1D model. [#3807]

astropy.stats
^^^^^^^^^^^^^

- Renamed the ``sigma_clip`` ``sig`` keyword as ``sigma``. [#3595]

- Changed the ``sigma_clip`` ``varfunc`` keyword to ``stdfunc``. [#3595]

- Renamed the ``sigma_clipped_stats`` ``mask_val`` keyword to
  ``mask_value``. [#3595]

- Changed the default ``iters`` keyword value to 5 in both the
  ``sigma_clip`` and ``sigma_clipped_stats`` functions. [#4067]

astropy.table
^^^^^^^^^^^^^

- ``Table.as_array()`` always returns a structured array with each column in
  the system's native byte order.  The optional ``keep_byteorder=True``
  option will keep each column's data in its original byteorder. [#4080]

- ``Table.simple_table()`` now creates tables with int64 and float64 types
  instead of int32 and float64. [#4114]

- An empty table can now be initialized without a ``names`` argument as long
  as a valid ``dtype`` argument (with names embedded) is supplied. [#3977]

astropy.time
^^^^^^^^^^^^

- The ``astropy_time`` attribute and time format has been removed from the
  public interface.  Existing code that instantiates a new time object using
  ``format='astropy_time'`` can simply omit the ``format``
  specification. [#3857]

astropy.units
^^^^^^^^^^^^^

- Single-item ``Quantity`` instances with record ``dtype`` will now have
  their ``isscalar`` property return ``True``, consistent with behaviour for
  numpy arrays, where ``np.void`` records are considered scalar. [#3899]

- Three changes relating to the FITS unit format [#3993]:

- The FITS unit format will no longer parse an arbitrary number as a
  scale value.  It must be a power of 10 of the form ``10^^k``,
  ``10^k``, ``10+k``, ``10-k`` and ``10(k)``. [#3993]

- Scales that are powers of 10 can be written out.  Previously, any
  non-1.0 scale was rejected.

- The ``*`` character is accepted as a separator between the scale
  and the units.

- Unit formatter classes now require the ``parse`` and ``to_string``
  methods are now required to be classmethods (and the formatter
  classes themselves are assumed to be singletons that are not
  instantiated).  As unit formatters are mostly an internal implementation
  detail this is not likely to affect any users. [#4001]

- CGS E&M units are now defined separately from SI E&M units, and have
  distinct physical types. [#4255, #4355]

astropy.utils
^^^^^^^^^^^^^

- All of the ``get_pkg_data_*`` functions take an optional ``package``
  argument as their second positional argument.  So any code that previously
  passed other arguments to these functions as positional arguments might
  break.  Use keyword argument passing instead to mitigate this. [#4079]

- ``astropy.utils.iers`` now uses a ``QTable`` internally, which means that
  the numerical columns are stored as ``Quantity``, with full support for
  units.  Furthermore, the ``ut1_utc`` method now returns a ``Quantity``
  instead of a float or an array (as did ``pm_xy`` already). [#3223]

- ``astropy.utils.iers`` now throws an ``IERSRangeError``, a subclass
  of ``IndexError``, rather than a raw ``IndexError``.  This allows more
  fine-grained catching of situations where a ``Time`` is beyond the range
  of the loaded IERS tables. [#4302]

astropy.wcs
^^^^^^^^^^^

- When compiled with wcslib 5.9 or later, the FITS headers returned
  by ``astropy.wcs.WCS.to_header`` and
  ``astropy.wcs.WCS.to_header_string`` now include values with more
  precision.  This will result in numerical differences in your
  results if you convert ``astropy.wcs.WCS`` objects to FITS headers
  and use the results.

- If NAXIS1 or NAXIS2 is not passed with the header object to
  WCS.calc_footprint, a ValueError is raised. [#3557]

Bug fixes
---------

astropy.constants
^^^^^^^^^^^^^^^^^

- The constants ``Ry`` and ``u`` are now properly used inside the
  corresponding units.  The latter have changed slightly as a result. [#4229]

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- Internally, ``coordinates`` now consistently uses the appropriate time
  scales for using ERFA functions. [#4302]

astropy.io.ascii
^^^^^^^^^^^^^^^^

- Fix a segfault in the fast C parser when one of the column headers
  is empty [#3545].

- Fix several bugs that prevented the fast readers from being used
  when guessing the file format.  Also improved the read trace
  information to better understand format guessing. [#4115]

- Fix an underlying problem that resulted in an uncaught TypeError
  exception when reading a CDS-format file with guessing enabled. [#4120]

astropy.modeling
^^^^^^^^^^^^^^^^

- ``Simplex`` fitter now correctly passes additional keywords arguments to
  the scipy solver. [#3966]

- The keyword ``acc`` (for accuracy) is now correctly accepted by
  ``Simplex``. [#3966]

astropy.units
^^^^^^^^^^^^^

- The units ``Ryd`` and ``u`` are no longer hard-coded numbers, but depend
  on the appropriate values in the ``constants`` module.  As a result, these
  units now imply slightly different conversions.  [#4229]

Other Changes and Additions
---------------------------

- The ``./setup.py test`` command is now implemented in the ``astropy.tests``
  module again (previously its implementation had been moved into
  astropy-helpers).  However, that made it difficult to synchronize changes
  to the Astropy test runner with changes to the ``./setup.py test`` UI.
  astropy-helpers v1.1 and above will detect this implementation of the
  ``test`` command, when present, and use it instead of the old version that
  was included in astropy-helpers (most users will not notice any difference
  as a result of this change). [#4020]

- The repr for ``Table`` no longer displays ``masked=False`` since tables
  are not masked by default anyway. [#3869]

- The version of ``PLY`` that ships with astropy has been updated to 3.6.

- WCSAxes is now required for doc builds. [#4074]

- The migration guide from pre-v0.4 coordinates has been removed to avoid
  cluttering the ``astropy.coordinates`` documentation with increasingly
  irrelevant material.  To see the migration guide, we recommend you simply look
  to the archived documentation for previous versions, e.g.
  https://docs.astropy.org/en/v1.0/coordinates/index.html#migrating-from-pre-v0-4-coordinates
  [#4203]

- In ``astropy.coordinates``, the transformations between GCRS, CIRS,
  and ITRS have been adjusted to more logically reflect the order in
  which they actually apply.  This should not affect most coordinate
  transformations, but may affect code that is especially sensitive to
  machine precision effects that change when the order in which
  transformations occur is changed. [#4255]

- Astropy v1.1.0 will be the last release series to officially support
  Python 2.6.  A deprecation warning will now be issued when using Astropy
  in Python 2.6 (this warning can be disabled through the usual Python warning
  filtering mechanisms). [#3779]


1.0.13 (2017-05-29)
===================

Bug Fixes
---------

astropy.io.fits
^^^^^^^^^^^^^^^

- Fix use of quantize level parameter for ``CompImageHDU``. [#6029]

- Prevent crash when a header contains non-ASCII (e.g. UTF-8) characters, to
  allow fixing the problematic cards. [#6084]


1.0.12 (2017-03-05)
===================

Bug Fixes
---------

astropy.convolution
^^^^^^^^^^^^^^^^^^^

- Fixed bug in ``discretize_integrate_2D`` in which x and y coordinates
  where swapped. [#5634]

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- Fixed a bug where ``get_transform`` could sometimes produce confusing errors
  because of a typo in the input validation. [#5645]

astropy.io.fits
^^^^^^^^^^^^^^^

- Guard against extremely unlikely problems in compressed images, which
  could lead to memory unmapping errors. [#5775]

astropy.io.votable
^^^^^^^^^^^^^^^^^^

- Fixed a bug where stdlib ``realloc()`` was used instead of
  ``PyMem_Realloc()`` [#5696, #4739, #2100]

astropy.utils
^^^^^^^^^^^^^

- Fixed ImportError with NumPy < 1.7 and Python 3.x in
  ``_register_patched_dtype_reduce``. [#5848]


1.0.11 (2016-12-22)
===================

Bug Fixes
---------

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- Initialising a SkyCoord from a list containing a single SkyCoord no longer removes
  the distance from the coordinate. [#5270]

- Fix errors in the implementation of the conversion to and from FK4 frames
  without e-terms, which will have affected coordinates not on the unit
  sphere (i.e., with distances). [#4293]

- Fix bug where with cds units enabled it was no longer possible to initialize
  an ``Angle``. [#5483]

- Ensure that ``search_around_sky`` and ``search_around_3d`` return
  integer type index arrays for empty (non) matches. [#4877, #5083]

- Return an empty set of matches for ``search_around_sky`` and
  ``search_around_3d`` when one or both of the input coordinate
  arrays is empty. [#4875, #5083]

astropy.io.ascii
^^^^^^^^^^^^^^^^

- Fix a bug with empty value at end of tab-delimited table on Windows. [#5370]

- Fix reading of big ASCII tables (more than 2Gb) with the fast reader.
  [#5319]

- Fix segfault with FastCsv and row with too many columns. [#5534]

- Fix problem reading an AASTex format table that does not have ``\\``
  at the end of the last table row. [#5427]

astropy.io.fits
^^^^^^^^^^^^^^^

- Removed raising of AssertionError that could occur after closing or
  deleting compressed image data. [#4690, #4694, #4948]

- Fixed bug that caused an ignored exception to be displayed under certain
  conditions when terminating a script after using fits.getdata(). [#4977]

- Fixed usage of inplace operations that were raising an exception with
  recent versions of Numpy due to implicit casting. [#5250]

astropy.io.votable
^^^^^^^^^^^^^^^^^^

- Fixed bug of ``Resource.__repr__()`` having undefined attributes and
  variables. [#5382]

astropy.modeling
^^^^^^^^^^^^^^^^

- CompoundModel now correctly inherits _n_models, allowing the use of model sets [#5358]

astropy.units
^^^^^^^^^^^^^

- Fixed bug in Ci definition. [#5106]

- Non-ascii cds unit strings are now correctly represented using ``str`` also
  on python2. This solves bugs in parsing coordinates involving strings too.
  [#5355]

- Ensure ``Quantity`` supports ``np.float_power``, which is new in numpy 1.12.
  [#5480]

astropy.utils
^^^^^^^^^^^^^

- Fixed AttributeError when calling ``utils.misc.signal_number_to_name`` with
  Python3 [#5430].

astropy.wcs
^^^^^^^^^^^

- Update the ``_naxis{x}`` attributes when calling ``WCS.slice``. [#5411]


Other Changes and Additions
---------------------------

- The bundled ERFA was updated to version 1.3.0.  This includes the
  leap second planned for 2016 Dec 31. [#5418]

1.0.10 (2016-06-09)
===================

Bug Fixes
---------

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- ``SkyCoord`` objects created before a new frame which has frame attributes
  is created no longer raise ``AttributeError`` when the new attributes are
  accessed [#5021]

- Fix some errors in the implementation of aberration  for ``get_sun``. [#4979]

astropy.io.ascii
^^^^^^^^^^^^^^^^

- Fix problem reading a zero-length ECSV table with a bool type column. [#5010]

astropy.io.fits
^^^^^^^^^^^^^^^

- Fix convenience functions (``getdata``, ``getheader``, ``append``,
  ``update``) to close files. [#4786]

astropy.io.votable
^^^^^^^^^^^^^^^^^^

- The astropy.io.votable.validator.html module is updated to handle division
  by zero when generating validation report. [#4699]

astropy.table
^^^^^^^^^^^^^

- Fixed a bug where ``pprint()`` sometimes raises ``UnicodeDecodeError``
  in Python 2. [#4946]

- Fix bug when doing outer join on multi-dimensional columns. [#4060]

- Fixed bug where Tables created from existing Table objects were not
  inheriting the ``primary_key`` attribute. [#4672]

astropy.tests
^^^^^^^^^^^^^

- Fix coverage reporting in Python 3. [#4822]

astropy.units
^^^^^^^^^^^^^

- Duplicates between long and short names are now removed in the ``names``
  and ``aliases`` properties of units. [#5036]

astropy.utils
^^^^^^^^^^^^^

- The astropy.utils.xml.unescaper module now also unescapes ``'%2F'`` to
  ``'/'`` and ``'&&'`` to ``'&'`` in a given URL. [#4699]

- Fix two problems related to the download cache: clear_download_cache() does
  not work in Python 2.7 and downloading in Python 2.7 and then Python 3
  can result in an exception. [#4810]

astropy.vo
^^^^^^^^^^

- Cache option now properly caches both downloaded JSON database and XML VO
  tables. [#4699]

- The astropy.vo.validator.conf.conesearch_urls listing is updated to reflect
  external changes to some VizieR Cone Search services. [#4699]

- VOSDatabase decodes byte-string to UTF-8 instead of ASCII to avoid
  UnicodeDecodeError for some rare cases. Fixed a Cone Search test that is
  failing as a side-effect of #4699. [#4757]

Other Changes and Additions
---------------------------

- Updated ``astropy.tests`` test runner code to work with Coverage v4.0 when
  generating test coverage reports. [#4176]


1.0.9 (2016-03-10)
==================

New Features
------------

astropy.nddata
^^^^^^^^^^^^^^

- ``NDArithmeticMixin`` check for matching WCS now works with
  ``astropy.wcs.WCS`` objects [#4499, #4503]

Bug Fixes
---------

astropy.convolution
^^^^^^^^^^^^^^^^^^^

- Correct a bug in which ``psf_pad`` and ``fft_pad`` would be ignored [#4366]

astropy.io.ascii
^^^^^^^^^^^^^^^^

- Fixed addition of new line characters after last row of data in
  ascii.latex.AASTex. [#4561]

- Fixed reading of Latex tables where the ``\tabular`` tag is in the first
  line. [#4595]

- Fix use of plain format strings with the fast writer. [#4517]

- Fix bug writing space-delimited file when table has empty fields. [#4417]

astropy.io.fits
^^^^^^^^^^^^^^^

- Fixed possible segfault during error handling in FITS tile
  compression. [#4489]

- Fixed crash on pickling of binary table columns with the 'X', 'P', or
  'Q' format. [#4514]

- Fixed memory / reference leak that could occur when copying a ``FITS_rec``
  object (the ``.data`` for table HDUs). [#520]

- Fixed a memory / reference leak in ``FITS_rec`` that occurred in a wide
  range of cases, especially after writing FITS tables to a file, but in
  other cases as well. [#4539]

astropy.modeling
^^^^^^^^^^^^^^^^

- Fixed display of compound model expressions and components when printing
  compound model instances. [#4414, #4482]

astropy.stats
^^^^^^^^^^^^^

- the input for median_absolute_deviation will not be cast to plain numpy
  arrays when given subclasses of numpy arrays
  (like Quantity, numpy.ma.MaskedArray, etc.) [#4658]

- Fixed incorrect results when using median_absolute_deviation with masked
  arrays. [#4658]

astropy.utils
^^^^^^^^^^^^^

- The ``zest.releaser`` hooks included in Astropy are now injected locally to
  Astropy, rather than being global. [#4650]

astropy.visualization
^^^^^^^^^^^^^^^^^^^^^

- Fixed ``fits2bitmap`` script to allow ext flag to contain extension
  names or numbers. [#4468]

- Fixed ``fits2bitmap`` default output filename generation for
  compressed FITS files. [#4468]


1.0.8 (2016-01-08)
==================

Bug Fixes
---------

astropy.io.fits
^^^^^^^^^^^^^^^

- Fixed an bug where updates to string columns in FITS tables were not saved
  on Python 3. [#4452]

astropy.units
^^^^^^^^^^^^^

- In-place peak-to-peak calculations now work on ``Quantity``. [#4442]

astropy.utils
^^^^^^^^^^^^^

- Fixed ``find_api_page`` to work correctly on python 3.x [#4378, #4379]


1.0.7 (2015-12-04)
==================

Bug Fixes
---------

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- Pickling of ``EarthLocation`` instances now also works on Python 2. [#4304]

astropy.io.ascii
^^^^^^^^^^^^^^^^

- Fix fast writer so bytestring column output is not prefixed by 'b' in
  Python 3. [#4350]

astropy.io.fits
^^^^^^^^^^^^^^^

- Fixed a regression that could cause writes of large FITS files to be
  truncated. [#4307]

- Astropy v1.0.6 included a fix (#4228) for an obscure case where the TDIM
  of a table column is smaller than the repeat count of its data format.
  This updates that fix in such a way that it works with Numpy 1.10 as well.
  [#4266]

astropy.table
^^^^^^^^^^^^^

- Fix a bug when pickling a Table with mixin columns (e.g. Time). [#4098]

astropy.time
^^^^^^^^^^^^

- Fix incorrect ``value`` attribute for epoch formats like "unix"
  when ``scale`` is different from the class ``epoch_scale``. [#4312]

astropy.utils
^^^^^^^^^^^^^

- Fixed an issue where if ipython is installed but ipykernel is not
  installed then importing astropy from the ipython console gave an
  IPython.kernel deprecation warning. [#4279]

- Fixed crash that could occur in ``ProgressBar`` when ``astropy`` is
  imported in an IPython startup script. [#4274]

Other Changes and Additions
---------------------------

- Updated bundled astropy-helpers to v1.0.6. [#4372]


1.0.6 (2015-10-22)
==================

Bug Fixes
---------

astropy.analytic_functions
^^^^^^^^^^^^^^^^^^^^^^^^^^

- Fixed blackbody analytic functions to properly support arrays of
  temperatures. [#4251]

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- Fixed errors in transformations for objects within a few AU of the
  Earth.  Included substantive changes to transformation machinery
  that may change distances at levels ~machine precision for other
  objects. [#4254]

astropy.io.fits
^^^^^^^^^^^^^^^

- ``fitsdiff`` and related functions now do a better job reporting differences
  between values that are different types but have the same representation
  (ex: the string '0' versus the number 0). [#4122]

- Miscellaneous fixes for supporting Numpy 1.10. [#4228]

- Fixed an issue where writing a column of unicode strings to a FITS table
  resulted in a quadrupling of size of the column (i.e. the format of the
  FITS column was 4 characters for every one in the original strings).
  [#4228]

- Added support for an obscure case (but nonetheless allowed by the FITS
  standard) where a column has some TDIMn keyword, but a repeat count in
  the TFORMn column greater than the number of elements implied by the
  TDIMn.  For example TFORMn = 100I, but TDIMn = '(5,5)'.  In this case
  the TDIMn implies 5x5 arrays in the column, but the TFORMn implies
  a 100 element 1-D array in the column.  In this case the TDIM takes
  precedence, and the remaining bytes in the column are ignored. [#4228]

astropy.io.votable
^^^^^^^^^^^^^^^^^^

- Fixed crash with Python compiler optimization level = 2. [#4231]

astropy.vo
^^^^^^^^^^

- Fixed ``check_conesearch_sites`` with ``parallel=True`` on Python >= 3.3
  and on Windows (it was broken in both those cases for separate reasons).
  [#2970]

Other Changes and Additions
---------------------------

- All tests now pass against Numpy v1.10.x. This implies nominal support for
  Numpy 1.10.x moving forward (but there may still be unknown issues). For
  example, there is already a known performance issue with tables containing
  large multi-dimensional columns--for example, tables that contain entire
  images in one or more of their columns.  This is a known upstream issue in
  Numpy. [#4259]


1.0.5 (2015-10-05)
==================

Bug Fixes
---------

astropy.constants
^^^^^^^^^^^^^^^^^

- Rename units -> unit and error -> uncertainty in the ``repr`` and ``str``
  of constants to match attribute names. [#4147]

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- Fix string representation of ``SkyCoord`` objects transformed into
  the ``AltAz`` frame [#4055, #4057]

- Fix the ``search_around_sky`` function to allow ``storekdtree`` to be
  ``False`` as was intended. [#4082, #4212]

astropy.io.fits
^^^^^^^^^^^^^^^

- Fix bug when extending one header (without comments) with another
  (with comments). [#3967]

- Somewhat improved resource usage for FITS data--previously a new ``mmap``
  was opened for each HDU of a FITS file accessed through an ``HDUList``.
  Each ``mmap`` used up a single file descriptor, causing problems with
  system resource limits for some users.  Now only a single ``mmap`` is
  opened, and shared for the data of all HDUs.  Note: The problem still
  persists with using the "convenience" functions.  For example using
  ``fits.getdata`` will create one ``mmap`` per HDU read this way (as
  opposed to opening the file with ``fits.open`` and accessing the HDUs
  through the ``HDUList`` object). [#4097]

- Fix bug where reading a file without a newline failed with an
  unrelated / unhelpful exception. [#4160]

astropy.modeling
^^^^^^^^^^^^^^^^

- Cleaned up ``repr`` of models that have no parameters. [#4076]

astropy.nddata
^^^^^^^^^^^^^^

- Initializing ``NDDataArray`` from another instance now sets ``flags`` as
  expected and no longer fails when ``uncertainty`` is set [#4129].
  Initializing an ``NDData`` subclass from a parent instance
  (eg. ``NDDataArray`` from ``NDData``) now sets the attributes other than
  ``data`` as it should [#4130, #4137].

astropy.table
^^^^^^^^^^^^^

- Fix an issue with setting fill value when column dtype is changed. [#4088]

- Fix bug when unpickling a bare Column where the _parent_table
  attribute was not set.  This impacted the Column representation. [#4099]

- Fix issue with the web browser opening with an empty page, and ensure that
  the url is correctly formatted for Windows. [#4132]

- Fix NameError in table stack exception message. [#4213]

astropy.utils
^^^^^^^^^^^^^

- ``resolve_name`` no longer causes ``sys.modules`` to be cluttered with
  additional copies of modules under a package imported like
  ``resolve_name('numpy')``. [#4084]

- ``console`` was updated to support IPython 4.x and Jupyter 1.x.
  This should suppress a ShimWarning that was appearing at
  import of astropy with IPython 4.0 or later. [#4078]

- Temporary downloaded files created by ``get_readable_fileobj`` when passed
  a URL are now deleted immediately after the file is closed. [#4198]

astropy.visualization
^^^^^^^^^^^^^^^^^^^^^

- The color for axes labels was set to white. Since white labels on white
  background are hard to read, the label color has been changed to black.
  [#4143]

- ``ImageNormalize`` now automatically determines ``vmin``/``vmax``
  (via the ``autoscale_None`` method) when they have not been set
  explicitly. [#4117]

astropy.vo
^^^^^^^^^^

- Cone Search validation no longer crashes when the provider gives an
  incomplete test query. It also ensures search radius for a test query
  is not too large to avoid timeout. [#4158, #4159]

Other Changes and Additions
---------------------------

- Astropy now supports Python 3.5. [#4027]

- Updated bundled version of astropy-helpers to 1.0.5. [#4215]

- Updated tests to support py.test 2.7, and upgraded the bundled copy of
  py.test to v2.7.3. [#4027]


1.0.4 (2015-08-11)
==================

New Features
------------

astropy.convolution
^^^^^^^^^^^^^^^^^^^

- Modified Cython functions to release the GIL. This enables convolution
  to be parallelized effectively and gives large speedups when used with
  multithreaded task schedulers such as Dask. [#3949]

API Changes
-----------

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- Some transformations for an input coordinate that's a scalar now correctly
  return a scalar.  This was always the intended behavior, but it may break
  code that has been written to work-around this bug, so it may be viewed as
  an unplanned API change [#3920, #4039]

astropy.visualization
^^^^^^^^^^^^^^^^^^^^^

- The ``astropy_mpl_style`` no longer sets ``interactive`` to ``True``, but
  instead leaves it at the user preference.  This makes using the style
  compatible with building docs with Sphinx, and other non-interactive
  contexts. [#4030]

Bug Fixes
---------

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- Fix bug where coordinate representation setting gets reset to default value
  when coordinate array is indexed or sliced. [#3824]

- Fixed confusing warning message shown when using dates outside current IERS
  data. [#3844]

- ``get_sun`` now yields a scalar when the input time is a scalar (this was a
  regression in v1.0.3 from v1.0.2) [#3998, #4039]

- Fixed bug where some scalar coordinates were incorrectly being changed to
  length-1 array coordinates after transforming through certain frames.
  [#3920, #4039]

- Fixed bug causing the ``separation`` methods of ``SkyCoord`` and frame
  classes to fail due to infinite recursion [#4033, #4039]

- Made it so that passing in a list of ``SkyCoord`` objects that are in
  UnitSphericalRepresentation to the ``SkyCoord`` constructor appropriately
  yields a new object in UnitSphericalRepresentation [#3938, #4039]

astropy.cosmology
^^^^^^^^^^^^^^^^^

- Fixed wCDM to not ignore the Ob0 parameter on initialization. [#3934]

astropy.io.fits
^^^^^^^^^^^^^^^

- Fixed crash when updating data in a random groups HDU opened in update
  mode. [#3730]

- Fixed incorrect checksum / datasum being written when re-writing a scaled
  HDU (i.e. non-trivial BSCALE and/or BZERO) with
  ``do_not_scale_image_data=False``. [#3883]

- Fixed stray deprecation warning in ``BinTableHDU.copy()``. [#3798]

- Better handling of the ``BLANK`` keyword when auto-scaling scaled image
  data.  The ``BLANK`` keyword is now removed from the header after
  auto-scaling is applied, and it is restored properly (with floating point
  NaNs replaced by the filler value) when updating a file opened with the
  ``scale_back=True`` argument.  Invalid usage of the ``BLANK`` keyword is
  also better warned about during validation. [#3865]

- Reading memmaped scaled images won't fail when
  ``do_not_scale_image_data=True`` (that is, since we're just reading the raw
  / physical data there is no reason mmap can't be used). [#3766]

- Fixed a reference cycle that could sometimes cause FITS table-related
  objects (``BinTableHDU``, ``ColDefs``, etc.) to hang around in memory
  longer than expected. [#4012]

astropy.modeling
^^^^^^^^^^^^^^^^

- Improved support for pickling of compound models, including both compound
  model instances, and new compound model classes. [#3867]

- Added missing default values for ``Ellipse2D`` parameters. [#3903]

astropy.time
^^^^^^^^^^^^

- Fixed iteration of scalar ``Time`` objects so that ``iter()`` correctly
  raises a ``TypeError`` on them (while still allowing ``Time`` arrays to be
  iterated). [#4048]

astropy.units
^^^^^^^^^^^^^

- Added frequency-equivalency check when declaring doppler equivalencies
  [#3728]

- Define ``floor_divide`` (``//``) for ``Quantity`` to be consistent
  ``divmod``, such that it only works where the quotient is dimensionless.
  This guarantees that ``(q1 // q2) * q2 + (q1 % q2) == q1``. [#3817]

- Fixed the documentation of supported units to correctly report support for
  SI prefixes.  Previously the table of supported units incorrectly showed
  several derived unit as not supporting prefixes, when in fact they do.
  [#3835]

- Fix a crash when calling ``astropy.units.cds.enable()``.  This will now
  "set" rather than "add" units to the active set to avoid the namespace
  clash with the default units. [#3873]

- Ensure in-place operations on ``float32`` quantities work. [#4007]

astropy.utils
^^^^^^^^^^^^^

- The ``deprecated`` decorator did not correctly wrap classes that have a
  custom metaclass--the metaclass could be dropped from the deprecated
  version of the class. [#3997]

- The ``wraps`` decorator would copy the wrapped function's name to the
  wrapper function even when ``'__name__'`` is excluded from the ``assigned``
  argument. [#4016]

Misc
^^^^

- ``fitscheck`` no longer causes scaled image data to be rescaled when
  adding checksums to existing files. [#3884]

- Fixed an issue where running ``import astropy`` from within the source
  tree did not automatically build the extension modules if the source is
  from a source distribution (as opposed to a git repository). [#3932]

- Fixed multiple instances of a bug that prevented Astropy from being used
  when compiled with the ``python -OO`` flag, due to it causing all
  docstrings to be stripped out. [#3923]

- Removed source code template files that were being installed
  accidentally alongside installed Python modules. [#4014]

- Fixed a bug in the exception logging that caused a crash in the exception
  handler itself on Python 3 when exceptions do not include a message.
  [#4056]


1.0.3 (2015-06-05)
==================

New Features
------------

astropy.table
^^^^^^^^^^^^^

- Greatly improved the speed of printing a large table to the screen when
  only a few rows are being displayed. [#3796]

astropy.time
^^^^^^^^^^^^

- Add support for the 2015-Jun-30 leap second. [#3794]

API Changes
-----------

astropy.io.ascii
^^^^^^^^^^^^^^^^

- Note that HTML formatted tables will not always be found with guess mode
  unless it passes certain heuristics that strongly suggest the presence of
  HTML in the input.  Code that expects to read tables from HTML should
  specify ``format='html'`` explicitly. See bug fixes below for more
  details. [#3693]

Bug Fixes
---------

astropy.convolution
^^^^^^^^^^^^^^^^^^^

- Fix issue with repeated normalizations of ``Kernels``. [#3747]

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- Fixed ``get_sun`` to yield frames with the ``obstime`` set to what's passed into the function (previously it incorrectly always had J2000). [#3750]

- Fixed ``get_sun`` to account for aberration of light. [#3750]

- Fixed error in the GCRS->ICRS transformation that gave incorrect distances. [#3750]

astropy.io.ascii
^^^^^^^^^^^^^^^^

- Remove HTML from the list of automatically-guessed formats when reading if
  the file does not appear to be HTML.  This was necessary to avoid a
  commonly-encountered segmentation fault occurring in the libxml parser on
  MacOSX. [#3693]

astropy.io.fits
^^^^^^^^^^^^^^^

- Fixes to support the upcoming Numpy 1.10. [#3419]

astropy.modeling
^^^^^^^^^^^^^^^^

- Polynomials are now scaled when used in a compound model. [#3702]

- Fixed the ``Ellipse2D`` model to be consistent with ``Disk2D`` in
  how pixels are included. [#3736]

- Fixed crash when evaluating a model that accepts no inputs. [#3772]

astropy.testing
^^^^^^^^^^^^^^^

- The Astropy py.test plugins that disable unintentional internet access
  in tests were also blocking use of local UNIX sockets in tests, which
  prevented testing some multiprocessing code--fixed. [#3713]

astropy.units
^^^^^^^^^^^^^

- Supported full SI prefixes for the barn unit ("picobarn", "femtobarn",
  etc.)  [#3753]

- Fix loss of precision when multiplying non-whole-numbered powers
  of units together.  For example, before this change, ``(u.m **
  1.5) ** Fraction(4, 5)`` resulted in an inaccurate floating-point
  power of ``1.2000000000000002``.  After this change, the exact
  rational number of ``Fraction(6, 5)`` is maintained. [#3790]

- Fixed printing of object ndarrays containing multiple Quantity
  objects with differing / incompatible units. Note: Unit conversion errors
  now cause a ``UnitConversionError`` exception to be raised.  However, this
  is a subclass of the ``UnitsError`` exception used previously, so existing
  code that catches ``UnitsError`` should still work. [#3778]

Other Changes and Additions
---------------------------

- Added a new ``astropy.__bibtex__`` attribute which gives a citation
  for Astropy in bibtex format. [#3697]

- The bundled version of ERFA was updated to v1.2.0 to address leapsecond
  updates. [#3802]


0.4.6 (2015-05-29)
==================

Bug Fixes
---------

astropy.time
^^^^^^^^^^^^

- Fixed ERFA code to handle the 2015-Jun-30 leap second. [#3795]


1.0.2 (2015-04-16)
==================

New Features
------------

astropy.modeling
^^^^^^^^^^^^^^^^

- Added support for polynomials with degree 0 or degree greater than 15.
  [#3574, 3589]

Bug Fixes
---------

astropy.config
^^^^^^^^^^^^^^

- The pre-astropy-0.4 configuration API has been fixed. It was
  inadvertently broken in 1.0.1. [#3627]

astropy.io.fits
^^^^^^^^^^^^^^^

- Fixed a severe memory leak that occurred when reading tile compressed
  images. [#3680]

- Fixed bug where column data could be unintentionally byte-swapped when
  copying data from an existing FITS file to a new FITS table with a
  TDIMn keyword for that column. [#3561]

- The ``ColDefs.change_attrib``, ``ColDefs.change_name``, and
  ``ColDefs.change_unit`` methods now work as advertised.  It is also
  possible (and preferable) to update attributes directly on ``Column``
  objects (for example setting ``column.name``), and the change will be
  accurately reflected in any associated table data and its FITS header.
  [#3283, #1539, #2618]

- Fixes an issue with the ``FITS_rec`` interface to FITS table data, where a
  ``FITS_rec`` created by copying an existing FITS table but adding new rows
  could not be sliced or masked correctly.  [#3641]

- Fixed handling of BINTABLE with TDIMn of size 1. [#3580]

astropy.io.votable
^^^^^^^^^^^^^^^^^^

- Loading a ``TABLE`` element without any ``DATA`` now correctly
  creates a 0-row array. [#3636]

astropy.modeling
^^^^^^^^^^^^^^^^

- Added workaround to support inverses on compound models when one of the
  sub-models is itself a compound model with a manually-assigned custom
  inverse. [#3542]

- Fixed instantiation of polynomial models with constraints for parameters
  (constraints could still be assigned after instantiation, but not during).
  [#3606]

- Fixed fitting of 2D polynomial models with the ``LeVMarLSQFitter``. [#3606]

astropy.table
^^^^^^^^^^^^^

- Ensure ``QTable`` can be pickled [#3590]

- Some corner cases when instantiating an ``astropy.table.Table``
  with a Numpy array are handled [#3637]. Notably:

- a zero-length array is the same as passing ``None``

- a scalar raises a ``ValueError``

- a one-dimensional array is treated as a single row of a table.

- Ensure a ``Column`` without units is treated as an ``array``, not as an
  dimensionless ``Quantity``. [#3648]

astropy.units
^^^^^^^^^^^^^

- Ensure equivalencies that do more than just scale a ``Quantity`` are
  properly handled also in ``ufunc`` evaluations. [#2496, #3586]

- The LaTeX representation of the Angstrom unit has changed from
  ``\overset{\circ}{A}`` to ``\mathring{A}``, which should have
  better support across regular LaTeX, MathJax and matplotlib (as of
  version 1.5) [#3617]

astropy.vo
^^^^^^^^^^

- Using HTTPS/SSL for communication between SAMP hubs now works
  correctly on all supported versions of Python [#3613]

astropy.wcs
^^^^^^^^^^^

- When no ``relax`` argument is passed to ``WCS.to_header()`` and
  the result omits non-standard WCS keywords, a warning is
  emitted. [#3652]

Other Changes and Additions
---------------------------

astropy.vo
^^^^^^^^^^

- The number of retries for connections in ``astropy.vo.samp`` can now be
  configured by a ``n_retries`` configuration option. [#3612]

- Testing

- Running ``astropy.test()`` from within the IPython prompt has been
  provisionally re-enabled. [#3184]


1.0.1 (2015-03-06)
==================

Bug Fixes
---------

astropy.constants
^^^^^^^^^^^^^^^^^

- Ensure constants can be turned into ``Quantity`` safely. [#3537, #3538]

astropy.io.ascii
^^^^^^^^^^^^^^^^

- Fix a segfault in the fast C parser when one of the column headers
  is empty [#3545].

- Fixed support for reading inf and nan values with the fast reader in
  Windows.  Also fixed in the case of using ``use_fast_converter=True``
  with the fast reader. [#3525]

- Fixed use of mmap in the fast reader on Windows. [#3525]

- Fixed issue where commented header would treat comments defining the table
  (i.e. column headers) as purely information comments, leading to problems
  when trying to round-trip the table. [#3562]

astropy.modeling
^^^^^^^^^^^^^^^^

- Fixed propagation of parameter constraints ('fixed', 'bounds', 'tied')
  between compound models and their components.  There is may still be some
  difficulty defining 'tied' constraints properly for use with compound
  models, however. [#3481]

astropy.nddata
^^^^^^^^^^^^^^

- Restore several properties to the compatibility class ``NDDataArray`` that
  were inadvertently omitted [#3466].

astropy.time
^^^^^^^^^^^^

- Time objects now always evaluate to ``True``, except when empty. [#3530]

Miscellaneous
-------------

- The ERFA wrappers are now written directly in the Python/C API
  rather than using Cython, for greater performance. [#3521]

- Improve import time of astropy [#3488].

Other Changes and Additions
---------------------------

- Updated bundled astropy-helpers version to v1.0.1 to address installation
  issues with some packages that depend on Astropy. [#3541]


1.0 (2015-02-18)
================

General
-------

- Astropy now requires Numpy 1.6.0 or later.

New Features
------------

astropy.analytic_functions
^^^^^^^^^^^^^^^^^^^^^^^^^^

- The ``astropy.analytic_functions`` was added to contain analytic functions
  useful for astronomy [#3077].

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- ``astropy.coordinates`` now has a full stack of frames allowing
  transformations from ICRS or other celestial systems down to Alt/Az
  coordinates. [#3217]

- ``astropy.coordinates`` now has a ``get_sun`` function that gives
  the coordinates  of the Sun at a specified time. [#3217]

- ``SkyCoord`` now has ``to_pixel`` and ``from_pixel`` methods that convert
  between celestial coordinates as ``SkyCoord`` objects and pixel coordinates
  given an ``astropy.wcs.WCS`` object. [#3002]

- ``SkyCoord`` now has ``search_around_sky`` and ``search_around_3d``
  convenience methods that allow searching for all coordinates within
  a certain distance of another ``SkyCoord``. [#2953]

- ``SkyCoord`` can now accept a frame instance for the ``frame=`` keyword
  argument. [#3063]

- ``SkyCoord`` now has a ``guess_from_table`` method that can be used to
  quickly create ``SkyCoord`` objects from an ``astropy.table.Table``
  object. [#2951]

- ``astropy.coordinates`` now has a ``Galactocentric`` frame, a coordinate
  frame centered on a (user specified) center of the Milky Way. [#2761, #3286]

- ``SkyCoord`` now accepts more formats of the coordinate string when the
  representation has ``ra`` and ``dec`` attributes. [#2920]

- ``SkyCoord`` can now accept lists of ``SkyCoord`` objects, frame objects,
  or representation objects and will combine them into a single object.
  [#3285]

- Frames and ``SkyCoord`` instances now have a method ``is_equivalent_frame``
  that can be used to check that two frames are equivalent (ignoring the
  data).  [#3330]

- The ``__repr__`` of coordinate objects now shows scalar coordinates in the
  same format as vector coordinates. [#3350, 3448]

astropy.cosmology
^^^^^^^^^^^^^^^^^

- Added ``lookback_distance``, which is ``c * lookback_time``. [#3145]

- Add baryonic matter density and dark matter only density parameters
  to cosmology objects [#2757].

- Add a ``clone`` method to cosmology objects to allow copies
  of cosmological objects to be created with the specified variables
  modified [#2592].

- Increase default numerical precision of ``z_at_value`` following
  the accurate by default, fast by explicit request model [#3074].

- Cosmology functions that take a single (redshift) input now
  broadcast like numpy ufuncs.  So, passing an arbitrarily shaped
  array of inputs will produce an output of the same shape. [#3178, #3194]

astropy.io.ascii
^^^^^^^^^^^^^^^^

- Simplify the way new Reader classes are defined, allowing custom behavior
  entirely by overriding inherited class attributes instead of setting
  instance attributes in the Reader ``__init__`` method. [#2812]

- There is now a faster C/Cython engine available for reading and writing
  simple ASCII formats like CSV. Both are enabled by default, and fast
  reading will fall back on an ordinary reader in case of a parsing
  failure. Their behavior can be altered with the parameter ``fast_reader``
  in ``read`` and ``fast_writer`` in ``write``. [#2716]

- Make Latex/AASTex tables use unit attribute of Column for output. [#3064]

- Store comment lines encountered during reading in metadata of the
  output table via ``meta['comment_lines']``. [#3222]

- Write comment lines in Table metadata during output for all basic formats,
  IPAC, and fast writers. This functionality can be disabled with
  ``comment=False``. [#3255]

- Add reader / writer for the Enhanced CSV format which stores table and
  column meta data, in particular data type and unit. [#2319]

astropy.io.fits
^^^^^^^^^^^^^^^

- The ``fitsdiff`` script ignores some things by default when comparing fits
  files (e.g. empty header lines). This adds a ``--exact`` option where
  nothing is ignored. [#2782, #3110]

- The ``fitsheader`` script now takes a ``--keyword`` option to extract a
  specific keyword from the header of a FITS file, and a ``--table`` option
  to export headers into any of the data formats supported by
  ``astropy.table``. [#2555, #2588]

- ``Section`` now supports all advanced indexing features ``ndarray`` does
  (slices with any steps, integer arrays, boolean arrays, None, Ellipsis).
  It also properly returns scalars when this is appropriate. [#3148]

astropy.io.votable
^^^^^^^^^^^^^^^^^^

- ``astropy.io.votable.parse`` now takes a ``datatype_mapping``
  keyword argument to map invalid datatype names to valid ones in
  order to support non-compliant files. [#2675]

astropy.modeling
^^^^^^^^^^^^^^^^

- Added the capability of creating new "compound" models by combining
  existing models using arithmetic operators.  See the "What's New in 1.0"
  page in the Astropy documentation for more details. [#3231]

- A new ``custom_model`` decorator/factory function has been added for
  converting normal functions to ``Model`` classes that can work within
  the Astropy modeling framework.  This replaces the old ``custom_model_1d``
  function which is now deprecated.  The new function works the same as
  the old one but is less limited in the types of models it can be used to
  created.  [#1763]

- The ``Model`` and ``Fitter`` classes have ``.registry`` attributes which
  provide sets of all loaded ``Model`` and ``Fitter`` classes (this is
  useful for building UIs for models and fitting). [#2725]

- A dict-like ``meta`` member was added to ``Model``. it is to be used to
  store any optional information which is relevant to a project and is not
  in the standard ``Model`` class. [#2189]

- Added ``Ellipse2D`` model. [#3124]

astropy.nddata
^^^^^^^^^^^^^^

- New array-related utility functions in ``astropy.nddata.utils`` for adding
  and removing arrays from other arrays with different sizes/shapes. [#3201]

- New metaclass ``NDDataBase`` for enforcing the nddata interface in
  subclasses without restricting implementation of the data storage. [#2905]

- New mixin classes ``NDSlicingMixin`` for slicing, ``NDArithmeticMixin``
  for arithmetic operations, and ``NDIOMixin`` for input/output in NDData. [#2905]

- Added a decorator ``support_nddata`` that can be used to write functions
  that can either take separate arguments or NDData objects. [#2855]

astropy.stats
^^^^^^^^^^^^^

- Added ``mad_std()`` function. [#3208]

- Added ``gaussian_fwhm_to_sigma`` and ``gaussian_sigma_to_fwhm``
  constants. [#3208]

- New function ``sigma_clipped_stats`` which can be used to quickly get
  common statistics for an array, using sigma clipping at the same time.
  [#3201]

astropy.table
^^^^^^^^^^^^^

- Changed the internal implementation of the ``Table`` class changed so that
  it no longer uses numpy structured arrays as the core table data container.
  [#2790, #3179]

- Tables can now be written to an html file that includes interactive
  browsing capabilities. To write out to this format, use
  ``Table.write('filename.html', format='jsviewer')``. [#2875]

- A ``quantity`` property and ``to`` method were added to ``Table``
  columns that allow the column values to be easily converted to
  ``astropy.units.Quantity`` objects. [#2950]

- Add ``unique`` convenience method to table. [#3185]

astropy.tests
^^^^^^^^^^^^^

- Added a new Quantity-aware ``assert_quantity_allclose``. [#3273]

astropy.time
^^^^^^^^^^^^

- ``Time`` can now handle arbitrary array dimensions, with operations
  following standard numpy broadcasting rules. [#3138]

astropy.units
^^^^^^^^^^^^^

- Support for VOUnit has been updated to be compliant with version
  1.0 of the standard. [#2901]

- Added an ``insert`` method to insert values into a ``Quantity`` object.
  This is similar to the ``numpy.insert`` function. [#3049]

- When viewed in IPython, ``Quantity`` objects with array values now render
  using LaTeX and scientific notation. [#2271]

- Added ``units.quantity_input`` decorator to validate quantity inputs to a
  function for unit compatibility. [#3072]

- Added ``units.astronomical_unit`` as a long form for ``units.au``. [#3303]

astropy.utils
^^^^^^^^^^^^^

- Added a new decorator ``astropy.utils.wraps`` which acts as a replacement
  for the standard library's ``functools.wraps``, the only difference being
  that the decorated function also preserves the wrapped function's call
  signature. [#2849]

- ``astropy.utils.compat.numpy`` has been revised such that it can include
  patched versions of routines from newer ``numpy`` versions.  The first
  addition is a version of ``broadcast_arrays`` that can be used with
  ``Quantity`` and other ``ndarray`` subclasses (using the ``subok=True``
  flag). [#2327]

- Added ``astropy.utils.resolve_name`` which returns a member of a module
  or class given the fully qualified dotted name of that object as a
  string. [#3389]

- Added ``astropy.utils.minversion`` which can be used to check minimum
  version requirements of Python modules (to test for specific features and/
  or bugs and the like). [#3389]

astropy.visualization
^^^^^^^^^^^^^^^^^^^^^

- Created ``astropy.visualization`` module and added functionality relating
  to image normalization (i.e. stretching and scaling) as well as a new
  script ``fits2bitmap`` that can produce a bitmap image from a FITS file.
  [#3201]

- Added dictionary ``astropy.visualization.mpl_style.astropy_mpl_style``
  which can be used to set a uniform plotstyle specifically for tutorials
  that is improved compared to matplotlib defaults. [#2719, #2787, #3200]

astropy.wcs
^^^^^^^^^^^

- ``wcslib`` has been upgraded to version 4.25.  This brings a
  single new feature:

- ``equinox`` and ``radesys`` will now be given default values
  conforming with the WCS specification if ``EQUINOXa`` and
  ``RADESYSa``, respectively, are not present in the header.

- The minimum required version of ``wcslib`` is now 4.24. [#2503]

- Added a new function ``wcs_to_celestial_frame`` that can be used to find
  the astropy.coordinates celestial frame corresponding to a particular WCS.
  [#2730]

- ``astropy.wcs.WCS.compare`` now supports a ``tolerance`` keyword argument
  to allow for approximate comparison of floating-point values. [#2503]

- added ``pixel_scale_matrix``, ``celestial``, ``is_celestial``, and
  ``has_celestial`` convenience attributes. Added
  ``proj_plane_pixel_scales``, ``proj_plane_pixel_area``, and
  ``non_celestial_pixel_scales`` utility functions for retrieving WCS pixel
  scale and area information [#2832, #3304]

- Added two functions ``pixel_to_skycoord`` and
  ``skycoord_to_pixel`` that make it easy to convert between
  SkyCoord objects and pixel coordinates. [#2885]

- ``all_world2pix`` now uses a much more sophisticated and complete
  algorithm to iteratively compute the inverse WCS transform. [#2816]

- Add ability to use ``WCS`` object to define projections in Matplotlib,
  using the ``WCSAxes`` package. [#3183]

- Added ``is_proj_plane_distorted`` for testing if pixels are
  distorted. [#3329]

Misc
^^^^

- ``astropy._erfa`` was added as a new subpackage wrapping the functionality
  of the ERFA library in python.  This is primarily of use for other astropy
  subpackages, but the API may be made more public in the future. [#2992]


API Changes
-----------

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- Subclasses of ``BaseCoordinateFrame`` which define a custom ``repr`` should
  be aware of the format expected in ``SkyCoord.__repr__()``, which changed in
  this release. [#2704, #2882]

- The ``CartesianPoints`` class (deprecated in v0.4) has now been removed.
  [#2990]

- The previous ``astropy.coordinates.builtin_frames`` module is now a
  subpackage.  Everything that was in the
  ``astropy.coordinates.builtin_frames`` module is still accessible from the
  new package, but the classes are now in separate modules.  This should have
  no direct impact at the user level. [#3120]

- Support for passing a frame as a positional argument in the ``SkyCoord``
  class has now been deprecated, except in the case where a frame with data
  is passed as the sole positional argument. [#3152]

- Improved ``__repr__`` of coordinate objects representing a single
  coordinate point for the sake of easier copy/pasting. [#3350]

astropy.cosmology
^^^^^^^^^^^^^^^^^

- The functional interface to the cosmological routines as well as
  ``set_current`` and ``get_current`` (deprecated in v0.4) have now been
  removed. [#2990]

astropy.io.ascii
^^^^^^^^^^^^^^^^

- Added a new argument to ``htmldict`` in the HTML reader named
  ``parser``, which allows the user to specify which parser
  BeautifulSoup should use as a backend. [#2815]

- Add ``FixedWidthTwoLine`` reader to guessing. This will allows to read
  tables that a copied from screen output like ``print my_table`` to be read
  automatically. Discussed in #3025 and #3099 [#3109]

astropy.io.fits
^^^^^^^^^^^^^^^

- A new optional argument ``cache`` has been added to
  ``astropy.io.fits.open()``.  When opening a FITS file from a URL,
  ``cache`` is a boolean value specifying whether or not to save the
  file locally in Astropy's download cache (``True`` by default). [#3041]

astropy.modeling
^^^^^^^^^^^^^^^^

- Model classes should now specify ``inputs`` and ``outputs`` class
  attributes instead of the old ``n_inputs`` and ``n_outputs``.  These
  should be tuples providing human-readable *labels* for all inputs and
  outputs of the model.  The length of the tuple indicates the numbers
  of inputs and outputs.  See "What's New in Astropy 1.0" for more
  details. [#2835]

- It is no longer necessary to include ``__init__`` or ``__call__``
  definitions in ``Model`` subclasses if all they do is wrap the
  super-method in order to provide a nice call signature to the docs.
  The ``inputs`` class attribute is now used to generate a nice call
  signature, so these methods should only be overridden by ``Model``
  subclasses in order to provide new functionality. [#2835]

- Most models included in Astropy now have sensible default values for most
  or all of their parameters.  Call ``help(ModelClass)`` on any model to
  check what those defaults are.  Most of them time they should be
  overridden, but some of them are useful (for example spatial offsets are
  always set at the origin by default). Another rule of thumb is that, where
  possible, default parameters are set so that the model is a no-op, or
  close to it, by default. [#2932]

- The ``Model.inverse`` method has been changed to a *property*, so that
  now accessing ``model.inverse`` on a model returns a new model that
  implements that model's inverse, and *calling* ``model.inverse(...)``` on
  some independent variable computes the value of the inverse (similar to what
  the old ``Model.invert()`` method was meant to do).  [#3024]

- The ``Model.invert()`` method has been removed entirely (it was never
  implemented and there should not be any existing code that relies on it).
  [#3024]

- ``custom_model_1d`` is deprecated in favor of the new ``custom_model``
  (see "New Features" above).  [#1763]

- The ``Model.param_dim`` property (deprecated in v0.4) has now been removed.
  [#2990]

- The ``Beta1D`` and ``Beta2D`` models have been renamed to ``Moffat1D`` and
  ``Moffat2D``. [#3029]

astropy.nddata
^^^^^^^^^^^^^^

- ``flags``, ``shape``, ``size``, ``dtype`` and ``ndim`` properties removed
  from ``astropy.nddata.NDData``. [#2905]

- Arithmetic operations, uncertainty propagation, slicing and automatic
  conversion to a numpy array removed from ``astropy.nddata.NDData``. The
  class ``astropy.nddata.NDDataArray`` is functionally equivalent to the
  old ``NDData``.  [#2905]

astropy.table
^^^^^^^^^^^^^

- The ``Column.units`` property (deprecated in v0.3) has now been removed.
  [#2990]

- The ``Row.data`` and ``Table._data`` attributes have been deprecated
  related to the change in Table implementation.  They are replaced by
  ``Row.as_void()`` and ``Table.as_array()`` methods, respectively. [#2790]

- The ``Table.create_mask`` method has been removed.  This undocumented
  method was a development orphan and would cause corruption of the
  table if called. [#2790]

- The return type for integer item access to a Column (e.g. col[12] or
  t['a'][12]) is now always a numpy scalar, numpy ``ndarray``, or numpy
  ``MaskedArray``.  Previously if the column was multidimensional then a
  Column object would be returned. [#3095]

- The representation of Table and Column objects has been changed to
  be formatted similar to the print output. [#3239]

astropy.time
^^^^^^^^^^^^

- The ``Time.val`` and ``Time.vals`` properties (deprecated in v0.3) and the
  ``Time.lon``, and ``Time.lat`` properties (deprecated in v0.4) have now
  been removed. [#2990]

- Add ``decimalyear`` format that represents time as a decimal year. [#3265]

astropy.units
^^^^^^^^^^^^^

- Support for VOUnit has been updated to be compliant with version
  1.0 of the standard. This means that some VOUnit strings that were
  rejected before are now acceptable. [#2901] Notably:

- SI prefixes are supported on most units

- Binary prefixes are supported on "bits" and "bytes"

- Custom units can be defined "inline" by placing them between single
  quotes.

- ``Unit.get_converter`` has been deprecated.  It is not strictly
  necessary for end users, and it was confusing due to lack of
  support for ``Quantity`` objects. [#3456]

astropy.utils
^^^^^^^^^^^^^

- Some members of ``astropy.utils.misc`` were moved into new submodules.
  Specifically:

- ``deprecated``, ``deprecated_attribute``, and ``lazyproperty`` ->
  ``astropy.utils.decorators``

- ``find_current_module``, ``find_mod_objs`` ->
  ``astropy.utils.introspection``

  All of these functions can be imported directly from ``astropy.utils``
  which should be preferred over referencing individual submodules of
  ``astropy.utils``.  [#2857]

- The ProgressBar.iterate class method (deprecated in v0.3) has now been
  removed. [#2990]

- Updated ``astropy/utils/console.py`` ProgressBar() module to
  display output to IPython notebook with the addition of an
  ``interactive`` kwarg. [#2658, #2789]

astropy.wcs
^^^^^^^^^^^

- The ``WCS.calcFootprint`` method (deprecated in v0.4) has now been removed.
  [#2990]

- An invalid unit in a ``CUNITn`` keyword now displays a warning and
  returns a ``UnrecognizedUnit`` instance rather than raising an
  exception [#3190]

Bug Fixes
---------

astropy.convolution
^^^^^^^^^^^^^^^^^^^

- ``astropy.convolution.discretize_model`` now handles arbitrary callables
  correctly [#2274].

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- ``Angle.to_string`` now outputs unicode arrays instead of object arrays.
  [#2981]

- ``SkyCoord.to_string`` no longer gives an error when used with an array
  coordinate with more than one dimension. [#3340]

- Fixed support for subclasses of ``UnitSphericalRepresentation`` and
  ``SphericalRepresentation`` [#3354, #3366]

- Fixed latex display of array angles in IPython notebook. [#3480]

astropy.io.ascii
^^^^^^^^^^^^^^^^

- In the ``CommentedHeader`` the ``data_start`` parameter now defaults to
  ``0``, which is the first uncommented line. Discussed in #2692. [#3054]

- Position lines in ``FixedWidthTwoLine`` reader could consist of many characters.
  Now, only one character in addition to the delimiter is allowed. This bug was
  discovered as part of [#3109]

- The IPAC table writer now consistently uses the ``fill_values`` keyword to
  specify the output null values.  Previously the behavior was inconsistent
  or incorrect. [#3259]

- The IPAC table reader now correctly interprets abbreviated column types.
  [#3279]

- Tables that look almost, but not quite like DAOPhot tables could cause
  guessing to fail. [#3342]

astropy.io.fits
^^^^^^^^^^^^^^^

- Fixed the problem in ``fits.open`` of some filenames with colon (``:``) in
  the name being recognized as URLs instead of file names. [#3122]

- Setting ``memmap=True`` in ``fits.open`` and related functions now raises
  a ValueError if opening a file in memory-mapped mode is impossible. [#2298]

- CONTINUE cards no longer end the value of the final card in the series with
  an ampersand, per the specification of the CONTINUE card convention. [#3282]

- Fixed a crash that occurred when reading an ASCII table containing
  zero-precision floating point fields. [#3422]

- When a float field for an ASCII table has zero-precision a decimal point
  (with no digits following it) is still written to the field as long as
  there is space for it, as recommended by the FITS standard.  This makes it
  less ambiguous that these columns should be interpreted as floats. [#3422]

astropy.logger
^^^^^^^^^^^^^^

- Fix a bug that occurred when displaying warnings that produced an error
  message ``dictionary changed size during iteration``. [#3353]

astropy.modeling
^^^^^^^^^^^^^^^^

- Fixed a bug in ``SLSQPLSQFitter`` where the ``maxiter`` argument was not
  passed correctly to the optimizer. [#3339]

astropy.table
^^^^^^^^^^^^^

- Fix a problem where ``table.hstack`` fails to stack multiple references to
  the same table, e.g. ``table.hstack([t, t])``. [#2995]

- Fixed a problem where ``table.vstack`` and ``table.hstack`` failed to stack
  a single table, e.g. ``table.vstack([t])``. [#3313]

- Fix a problem when doing nested iterators on a single table. [#3358]

- Fix an error when an empty list, tuple, or ndarray is used for item access
  within a table.  This now returns the table with no rows. [#3442]

astropy.time
^^^^^^^^^^^^

- When creating a Time object from a datetime object the time zone
  info is now correctly used. [#3160]

- For Time objects, it is now checked that numerical input is finite. [#3396]

astropy.units
^^^^^^^^^^^^^

- Added a ``latex_inline`` unit format that returns the units in LaTeX math
  notation with negative exponents instead of fractions [#2622].

- When using a unit that is deprecated in a given unit format,
  non-deprecated alternatives will be suggested. [#2806] For
  example::

      >>> import astropy.units as u
      >>> u.Unit('Angstrom', format='fits')
      WARNING: UnitsWarning: The unit 'Angstrom' has been deprecated
      in the FITS standard. Suggested: nm (with data multiplied by
      0.1).  [astropy.units.format.utils]

astropy.utils
^^^^^^^^^^^^^

- ``treat_deprecations_as_exceptions`` has been fixed to recognize Astropy
  deprecation warnings. [#3015]

- Converted representation of progress bar units without suffix
  from float to int in console.human_file_size. [#2201, #2202, #2721, #3299]

astropy.wcs
^^^^^^^^^^^

- ``astropy.wcs.WCS.sub`` now accepts unicode strings as input on
  Python 2.x [#3356]

Misc
^^^^

- Some modules and tests that would crash upon import when using a non-final
  release of Numpy (e.g. 1.9.0rc1). [#3471]

Other Changes and Additions
---------------------------

- The bundled copy of astropy-helpers has been updated to v1.0. [#3515]

- Updated ``astropy.extern.configobj`` to Version 5. Version 5 uses ``six``
  and the same code covers both Python 2 and Python 3. [#3149]

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- The ``repr`` of ``SkyCoord`` and coordinate frame classes now separate
  frame attributes and coordinate information.  [#2704, #2882]

astropy.io.fits
^^^^^^^^^^^^^^^

- Overwriting an existing file using the ``clobber=True`` option no longer
  displays a warning message. [#1963]

- ``fits.open`` no longer catches ``OSError`` exceptions on missing or
  unreadable files-- instead it raises the standard Python exceptions in such
  cases. [#2756, #2785]

astropy.table
^^^^^^^^^^^^^

- Sped up setting of ``Column`` slices by an order of magnitude. [#2994, #3020]

- Updated the bundled ``six`` module to version 1.7.3 and made 1.7.3 the
  minimum acceptable version of ``six``. [#2814]

- The version of ERFA included with Astropy is now v1.1.1 [#2971]

- The code base is now fully Python 2 and 3 compatible and no longer requires
  2to3. [#2033]

- `funcsigs <https://pypi.org/project/funcsigs>`_ is included in
  utils.compat, but defaults to the inspect module components where available
  (3.3+) [#3151].

- The list of modules displayed in the pytest header can now be customized.
  [#3157]

- `jinja2 <http://jinja.pocoo.org/docs/dev/>`_>=2.7 is now required to build the
  source code from the git repository, in order to allow the ERFA wrappers to
  be generated. [#3166]


0.4.5 (2015-02-16)
==================

Bug Fixes
---------

- Fixed unnecessary attempt to run ``git`` when importing astropy.  In
  particular, fixed a crash in Python 3 that could result from this when
  importing Astropy when the the current working directory is an empty git
  repository. [#3475]

Other Changes and Additions
---------------------------

- Updated bundled copy of astropy-helpers to v0.4.6. [#3508]


0.4.4 (2015-01-21)
==================

Bug Fixes
---------

astropy.vo.samp
^^^^^^^^^^^^^^^

- ``astropy.vo.samp`` is now usable on Python builds that do not
  support the SSLv3 protocol (which depends both on the version of
  Python and the version of OpenSSL or LibreSSL that it is built
  against.) [#3308]

API Changes
-----------

astropy.vo.samp
^^^^^^^^^^^^^^^

- The default SSL protocol used is now determined from the default
  used in the Python ``ssl`` standard library.  This default may be
  different depending on the exact version of Python you are using.
  [#3308]

astropy.wcs
^^^^^^^^^^^

- WCS allows slices of the form slice(None, x, y), which previously resulted
  in an unsliced copy being returned (note: this was previously incorrectly
  reported as fixed in v0.4.3) [#2909]


0.4.3 (2015-01-15)
==================

Bug Fixes
---------

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- The ``Distance`` class has been fixed to no longer rely on the deprecated
  cosmology functions. [#2991]

- Ensure ``float32`` values can be used in coordinate representations. [#2983]

- Fix frame attribute inheritance in ``SkyCoord.transform_to()`` method so
  that the default attribute value (e.g. equinox) for the destination frame
  gets used if no corresponding value was explicitly specified. [#3106]

- ``Angle`` accepts hours:mins or deg:mins initializers (without
  seconds). In these cases float minutes are also accepted. [#2843]

- ``astropy.coordinates.SkyCoord`` objects are now copyable. [#2888]

- ``astropy.coordinates.SkyCoord`` object attributes are now
  immutable.  It is still technically possible to change the
  internal data for an array-valued coordinate object but this leads
  to inconsistencies [#2889] and should not be done. [#2888]

astropy.cosmology
^^^^^^^^^^^^^^^^^

- The ``ztol`` keyword argument to z_at_value now works correctly [#2993].

astropy.io.ascii
^^^^^^^^^^^^^^^^

- Fix a bug in Python 3 when guessing file format using a file object as
  input.  Also improve performance in same situation for Python 2. [#3132]

- Fix a problem where URL was being downloaded for each guess. [#2001]

astropy.io.fits
^^^^^^^^^^^^^^^

- The ``in`` operator now works correctly for checking if an extension
  is in an ``HDUList`` (as given via EXTNAME, (EXTNAME, EXTVER) tuples,
  etc.) [#3060]

- Added workaround for bug in MacOS X <= 10.8 that caused np.fromfile to
  fail. [#3078]

- Added support for the ``RICE_ONE`` compression type synonym. [#3115]

astropy.modeling
^^^^^^^^^^^^^^^^

- Fixed a test failure on Debian/PowerPC and Debian/s390x. [#2708]

- Fixed crash in evaluating models that have more outputs than inputs--this
  case may not be handled as desired for all conceivable models of this
  format (some may have to implement custom ``prepare_inputs`` and
  ``prepare_outputs`` methods).  But as long as all outputs can be assumed
  to have a shape determined from the broadcast of all inputs with all
  parameters then this can be used safely. [#3250]

astropy.table
^^^^^^^^^^^^^

- Fix a bug that caused join to fail for multi-dimensional columns. [#2984]

- Fix a bug where MaskedColumn attributes which had been changed since
  the object was created were not being carried through when slicing. [#3023]

- Fix a bug that prevented initializing a table from a structured array
  with multi-dimensional columns with copy=True. [#3034]

- Fixed unnecessarily large unicode columns when instantiating a table from
  row data on Python 3. [#3052]

- Improved the warning message when unable to aggregate non-numeric
  columns. [#2700]

astropy.units
^^^^^^^^^^^^^

- Operations on quantities with incompatible types now raises a much
  more informative ``TypeError``. [#2934]

- ``Quantity.tolist`` now overrides the ``ndarray`` method to give a
  ``NotImplementedError`` (by renaming the previous ``list`` method). [#3050]

- ``Quantity.round`` now always returns a ``Quantity`` (previously it
  returned an ``ndarray`` for ``decimals>0``). [#3062]

- Ensured ``np.squeeze`` always returns a ``Quantity`` (it only worked if
  no dimensions were removed). [#3045]

- Input to ``Quantity`` with a ``unit`` attribute no longer can get mangled
  with ``copy=False``. [#3051]

- Remove trailing space in ``__format__`` calls for dimensionless quantities.
  [#3097]

- Comparisons between units and non-unit-like objects now works
  correctly. [#3108]

- Units with fractional powers are now correctly multiplied together
  by using rational arithmetic.  [#3121]

- Removed a few entries from spectral density equivalencies which did not
  make sense. [#3153]

astropy.utils
^^^^^^^^^^^^^

- Fixed an issue with the ``deprecated`` decorator on classes that invoke
  ``super()`` in their ``__init__`` method. [#3004]

- Fixed a bug which caused the ``metadata_conflicts`` parameter to be
  ignored in the ``astropy.utils.metadata.merge`` function. [#3294]

astropy.vo
^^^^^^^^^^

- Fixed an issue with reconnecting to a SAMP Hub. [#2674]

astropy.wcs
^^^^^^^^^^^

- Invalid or out of range values passed to ``wcs_world2pix`` will
  now be correctly identified and returned as ``nan``
  values. [#2965]

- Fixed an issue which meant that Python thought ``WCS`` objects were
  iterable. [#3066]

Misc
^^^^

- Astropy will now work if your Python interpreter does not have the
  ``bz2`` module installed. [#3104]

- Fixed ``ResourceWarning`` for ``astropy/extern/bundled/six.py`` that could
  occur sometimes after using Astropy in Python 3.4. [#3156]

Other Changes and Additions
---------------------------

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- Improved the agreement of the FK5 <-> Galactic conversion with other
  codes, and with the FK5 <-> FK4 <-> Galactic route. [#3107]


0.4.2 (2014-09-23)
==================

Bug Fixes
---------

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- ``Angle`` accepts hours:mins or deg:mins initializers (without
  seconds). In these cases float minutes are also accepted.

- The ``repr`` for coordinate frames now displays the frame attributes
  (ex: ra, dec) in a consistent order.  It should be noted that as part of
  this fix, the ``BaseCoordinateFrame.get_frame_attr_names()`` method now
  returns an ``OrderedDict`` instead of just a ``dict``. [#2845]

astropy.io.fits
^^^^^^^^^^^^^^^

- Fixed a crash when reading scaled float data out of a FITS file that was
  loaded from a string (using ``HDUList.fromfile``) rather than from a file.
  [#2710]

- Fixed a crash when reading data from an HDU whose header contained in
  invalid value for the BLANK keyword (e.g., a string value instead of an
  integer as required by the FITS Standard). Invalid BLANK keywords are now
  warned about, but are otherwise ignored. [#2711]

- Fixed a crash when reading the header of a tile-compressed HDU if that
  header contained invalid duplicate keywords resulting in a ``KeyError``
  [#2750]

- Fixed crash when reading gzip-compressed FITS tables through the Astropy
  ``Table`` interface. [#2783]

- Fixed corruption when writing new FITS files through to gzipped files.
  [#2794]

- Fixed crash when writing HDUs made with non-contiguous data arrays to
  file-like objects. [#2794]

- It is now possible to create ``astropy.io.fits.BinTableHDU``
  objects with a table with zero rows. [#2916]

astropy.io.misc
^^^^^^^^^^^^^^^

- Fixed a bug that prevented h5py ``Dataset`` objects from being
  automatically recognized by ``Table.read``. [#2831]

astropy.modeling
^^^^^^^^^^^^^^^^

- Make ``LevMarLSQFitter`` work with ``weights`` keyword. [#2900]

astropy.table
^^^^^^^^^^^^^

- Fixed reference cycle in tables that could prevent ``Table`` objects
  from being freed from memory. [#2879]

- Fixed an issue where ``Table.pprint()`` did not print the header to
  ``stdout`` when ``stdout`` is redirected (say, to a file). [#2878]

- Fixed printing of masked values when a format is specified. [#1026]

- Ensured that numpy ufuncs that return booleans return plain ``ndarray``
  instances, just like the comparison operators. [#2963]

astropy.time
^^^^^^^^^^^^

- Ensure bigendian input to Time works on a little-endian machine
  (and vice versa).  [#2942]

astropy.units
^^^^^^^^^^^^^

- Ensure unit is kept when adding 0 to quantities. [#2968]

astropy.utils
^^^^^^^^^^^^^

- Fixed color printing on Windows with IPython 2.0. [#2878]

astropy.vo
^^^^^^^^^^

- Improved error message on Cone Search time out. [#2687]

Other Changes and Additions
---------------------------

- Fixed a couple issues with files being inappropriately included and/or
  excluded from the source archive distributions of Astropy. [#2843, #2854]

- As part of fixing the fact that masked elements of table columns could not be
  printed when a format was specified, the column format string options were
  expanded to allow simple specifiers such as ``'5.2f'``. [#2898]

- Ensure numpy 1.9 is supported. [#2917]

- Ensure numpy master is supported, by making ``np.cbrt`` work with quantities.
  [#2937]

0.4.1 (2014-08-08)
==================

Bug Fixes
---------

astropy.config
^^^^^^^^^^^^^^

- Fixed a bug where an unedited configuration file from astropy
  0.3.2 would not be correctly identified as unedited. [#2772] This
  resulted in the warning::

      WARNING: ConfigurationChangedWarning: The configuration options
      in astropy 0.4 may have changed, your configuration file was not
      updated in order to preserve local changes.  A new configuration
      template has been saved to
      '~/.astropy/config/astropy.0.4.cfg'. [astropy.config.configuration]

- Fixed the error message that is displayed when an old
  configuration item has moved.  Before, the destination
  section was wrong.  [#2772]

- Added configuration settings for ``io.fits``, ``io.votable`` and
  ``table.jsviewer`` that were missing from the configuration file
  template. [#2772]

- The configuration template is no longer rewritten on every import
  of astropy, causing race conditions. [#2805]

astropy.convolution
^^^^^^^^^^^^^^^^^^^

- Fixed the multiplication of ``Kernel`` with numpy floats. [#2174]

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- ``Distance`` can now take a list of quantities. [#2261]

- For in-place operations for ``Angle`` instances in which the result unit
  is not an angle, an exception is raised before the instance is corrupted.
  [#2718]

- ``CartesianPoints`` are now deprecated in favor of
  ``CartesianRepresentation``. [#2727]

astropy.io.misc
^^^^^^^^^^^^^^^

- An existing table within an HDF5 file can be overwritten without affecting
  other datasets in the same HDF5 file by simultaneously using
  ``overwrite=True`` and ``append=True`` arguments to the ``Table.write``
  method. [#2624]

astropy.logger
^^^^^^^^^^^^^^

- Fixed a crash that could occur in rare cases when (such as in bundled
  apps) where submodules of the ``email`` package are not importable. [#2671]

astropy.nddata
^^^^^^^^^^^^^^

- ``astropy.nddata.NDData()`` no longer raises a ``ValueError`` when passed
  a numpy masked array which has no masked entries. [#2784]

astropy.table
^^^^^^^^^^^^^

- When saving a table to a FITS file containing a unit that is not
  supported by the FITS standard, a warning rather than an exception
  is raised. [#2797]

astropy.units
^^^^^^^^^^^^^

- By default, ``Quantity`` and its subclasses will now convert to float also
  numerical types such as ``decimal.Decimal``, which are stored as objects
  by numpy. [#1419]

- The units ``count``, ``pixel``, ``voxel`` and ``dbyte`` now output
  to FITS, OGIP and VOUnit formats correctly. [#2798]

astropy.utils
^^^^^^^^^^^^^

- Restored missing information from deprecation warning messages
  from the ``deprecated`` decorator. [#2811]

- Fixed support for ``staticmethod`` deprecation in the ``deprecated``
  decorator. [#2811]

astropy.wcs
^^^^^^^^^^^

- Fixed a memory leak when ``astropy.wcs.WCS`` objects are copied
  [#2754]

- Fixed a crash when passing ``ra_dec_order=True`` to any of the
  ``*2world`` methods. [#2791]

Other Changes and Additions
---------------------------

- Bundled copy of astropy-helpers upgraded to v0.4.1. [#2825]

- General improvements to documentation and docstrings [#2722, #2728, #2742]

- Made it easier for third-party packagers to have Astropy use their own
  version of the ``six`` module (so long as it meets the minimum version
  requirement) and remove the copy bundled with Astropy.  See the
  astropy/extern/README file in the source tree.  [#2623]


0.4 (2014-07-16)
================

New Features
------------

astropy.constants
^^^^^^^^^^^^^^^^^

- Added ``b_wien`` to represent Wien wavelength displacement law constant.
  [#2194]

astropy.convolution
^^^^^^^^^^^^^^^^^^^

- Changed the input parameter in ``Gaussian1DKernel`` and
  ``Gaussian2DKernel`` from ``width`` to ``stddev`` [#2085].

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- The coordinates package has undergone major changes to implement
  `APE5 <https://github.com/astropy/astropy-APEs/blob/master/APE5.rst>`_ .
  These include backwards-incompatible changes, as the underlying framework
  has changed substantially. See the APE5 text and the package documentation
  for more details. [#2422]

- A ``position_angle`` method has been added to the new ``SkyCoord``. [#2487]

- Updated ``Angle.dms`` and ``Angle.hms`` to return ``namedtuple`` -s instead
  of regular tuples, and added ``Angle.signed_dms`` attribute that gives the
  absolute value of the ``d``, ``m``, and ``s`` along with the sign.  [#1988]

- By default, ``Distance`` objects are now required to be positive. To
  allow negative values, set ``allow_negative=True`` in the ``Distance``
  constructor when creating a ``Distance`` instance.

- ``Longitude`` (resp. ``Latitude``) objects cannot be used any more to
  initialize or set ``Latitude`` (resp. ``Longitude``) objects. An explicit
  conversion to ``Angle`` is now required. [#2461]

- The deprecated functions for pre-0.3 coordinate object names like
  ``ICRSCoordinates`` have been removed. [#2422]

- The ``rotation_matrix`` and ``angle_axis`` functions in
  ``astropy.coordinates.angles`` were made more numerically consistent and
  are now tested explicitly [#2619]

astropy.cosmology
^^^^^^^^^^^^^^^^^

- Added ``z_at_value`` function to find the redshift at which a cosmology
  function matches a desired value. [#1909]

- Added ``FLRW.differential_comoving_volume`` method to give the differential
  comoving volume at redshift z. [#2103]

- The functional interface is now deprecated in favor of the more-explicit
  use of methods on cosmology objects. [#2343]

- Updated documentation to reflect the removal of the functional
  interface. [#2507]

astropy.io.ascii
^^^^^^^^^^^^^^^^

- The ``astropy.io.ascii`` output formats ``latex`` and ``aastex`` accept a
  dictionary called ``latex_dict`` to specify options for LaTeX output.  It is
  now possible to specify the table alignment within the text via the
  ``tablealign`` keyword. [#1838]

- If ``header_start`` is specified in a call to ``ascii.get_reader`` or any
  method that calls ``get_reader`` (e.g. ``ascii.read``) but ``data_start``
  is not specified at the same time, then ``data_start`` is calculated so
  that the data starts after the header. Before this, the default was
  that the header line was read again as the first data line
  [#855 and #1844].

- A new ``csv`` format was added as a convenience for handling CSV (comma-
  separated values) data. [#1935]
  This format also recognises rows with an inconsistent number of elements.
  [#1562]

- An option was added to guess the start of data for CDS format files when
  they do not strictly conform to the format standard. [#2241]

- Added an HTML reader and writer to the ``astropy.io.ascii`` package.
  Parsing requires the installation of BeautifulSoup and is therefore
  an optional feature. [#2160]

- Added support for inputting column descriptions and column units
  with the ``io.ascii.SExtractor`` reader. [#2372]

- Allow the use of non-local ReadMe files in the CDS reader. [#2329]

- Provide a mechanism to select how masked values are printed. [#2424]

- Added support for reading multi-aperture daophot file. [#2656]

astropy.io.fits
^^^^^^^^^^^^^^^

- Included a new command-line script called ``fitsheader`` to display the
  header(s) of a FITS file from the command line. [#2092]

- Added new verification options ``fix+ignore``, ``fix+warn``,
  ``fix+exception``, ``silentfix+ignore``, ``silentfix+warn``, and
  ``silentfix+exception`` which give more control over how to report fixable
  errors as opposed to unfixable errors.

astropy.modeling
^^^^^^^^^^^^^^^^

- Prototype implementation of fitters that treat optimization algorithms
  separately from fit statistics, allowing new fitters to be created by
  mixing and matching optimizers and statistic functions. [#1914]

- Slight overhaul to how inputs to and outputs from models are handled with
  respect to array-valued parameters and variables, as well as sets of
  multiple models.  See the associated PR and the modeling section of the
  v0.4 documentation for more details. [#2634]

- Added a new ``SimplexLSQFitter`` which uses a downhill simplex optimizer
  with a least squares statistic. [#1914]

- Changed ``Gaussian2D`` model such that ``theta`` now increases
  counterclockwise. [#2199]

- Replaced the ``MatrixRotation2D`` model with a new model called simply
  ``Rotation2D`` which requires only an angle to specify the rotation.
  The new ``Rotation2D`` rotates in a counter-clockwise sense whereas
  the old ``MatrixRotation2D`` increased the angle clockwise.
  [#2266, #2269]

- Added a new ``AffineTransformation2D`` model which serves as a
  replacement for the capability of ``MatrixRotation2D`` to accept an
  arbitrary matrix, while also adding a translation capability. [#2269]

- Added ``GaussianAbsorption1D`` model. [#2215]

- New ``Redshift`` model [#2176].

astropy.nddata
^^^^^^^^^^^^^^

- Allow initialization ``NDData`` or ``StdDevUncertainty`` with a
  ``Quantity``. [#2380]

astropy.stats
^^^^^^^^^^^^^

- Added flat prior to binom_conf_interval and binned_binom_proportion

- Change default in ``sigma_clip`` from ``np.median`` to ``np.ma.median``.
  [#2582]

astropy.sphinx
^^^^^^^^^^^^^^

- Note, the following new features are included in astropy-helpers as well:

- The ``automodapi`` and ``automodsumm`` extensions now include sphinx
  configuration options to write out what ``automodapi`` and ``automodsumm``
  generate, mainly for debugging purposes. [#1975, #2022]

- Reference documentation now shows functions/class docstrings at the
  intended user-facing API location rather than the actual file where
  the implementation is found. [#1826]

- The ``automodsumm`` extension configuration was changed to generate
  documentation of class ``__call__`` member functions. [#1817, #2135]

- ``automodapi`` and ``automodsumm`` now have an ``:allowed-package-names:``
  option that make it possible to document functions and classes that
  are in a different namespace.  [#2370]

astropy.table
^^^^^^^^^^^^^

- Improved grouped table aggregation by using the numpy ``reduceat()`` method
  when possible. This can speed up the operation by a factor of at least 10
  to 100 for large unmasked tables and columns with relatively small
  group sizes.  [#2625]

- Allow row-oriented data input using a new ``rows`` keyword argument.
  [#850]

- Allow subclassing of ``Table`` and the component classes ``Row``, ``Column``,
  ``MaskedColumn``, ``TableColumns``, and ``TableFormatter``. [#2287]

- Fix to allow numpy integer types as valid indices into tables in
  Python 3.x [#2477]

- Remove transition code related to the order change in ``Column`` and
  ``MaskedColumn`` arguments ``name`` and ``data`` from Astropy 0.2
  to 0.3. [#2511]

- Change HTML table representation in IPython notebook to show all
  table columns instead of restricting to 80 column width.  [#2651]

astropy.time
^^^^^^^^^^^^

- Mean and apparent sidereal time can now be calculated using the
  ``sidereal_time`` method [#1418].

- The time scale now defaults to UTC if no scale is provided. [#2091]

- ``TimeDelta`` objects can have all scales but UTC, as well as, for
  consistency with time-like quantities, undefined scale (where the
  scale is taken from the object one adds to or subtracts from).
  This allows, e.g., to work consistently in TDB.  [#1932]

- ``Time`` now supports ISO format strings that end in "Z". [#2211, #2203]

astropy.units
^^^^^^^^^^^^^

- Support for the unit format `Office of Guest Investigator Programs (OGIP)
  FITS files
  <https://heasarc.gsfc.nasa.gov/docs/heasarc/ofwg/docs/general/ogip_93_001/>`__
  has been added. [#377]

- The ``spectral`` equivalency can now handle angular wave number. [#1306 and
  #1899]

- Added ``one`` as a shorthand for ``dimensionless_unscaled``. [#1980]

- Added ``dex`` and ``dB`` units. [#1628]

- Added ``temperature()`` equivalencies to support conversion between
  Kelvin, Celsius, and Fahrenheit. [#2209]

- Added ``temperature_energy()`` equivalencies to support conversion
  between electron-volt and Kelvin. [#2637]

- The runtime of ``astropy.units.Unit.compose`` is greatly improved
  (by a factor of 2 in most cases) [#2544]

- Added ``electron`` unit. [#2599]

astropy.utils
^^^^^^^^^^^^^

- ``timer.RunTimePredictor`` now uses ``astropy.modeling`` in its
  ``do_fit()`` method. [#1896]

astropy.vo
^^^^^^^^^^

- A new sub-package, ``astropy.vo.samp``, is now available (this was
  previously the SAMPy package, which has been refactored for use in
  Astropy). [#1907]

- Enhanced functionalities for ``VOSCatalog`` and ``VOSDatabase``. [#1206]

astropy.wcs
^^^^^^^^^^^

- astropy now requires wcslib version 4.23.  The version of wcslib
  included with astropy has been updated to version 4.23.

- Bounds checking is now performed on native spherical
  coordinates.  Any out-of-bounds values will be returned as
  ``NaN``, and marked in the ``stat`` array, if using the
  low-level ``wcslib`` interface such as
  ``astropy.wcs.Wcsprm.p2s``. [#2107]

- A new method, ``astropy.wcs.WCS.compare()``, compares two wcsprm
  structs for equality with varying degrees of strictness. [#2361]

- New ``astropy.wcs.utils`` module, with a handful of tools for manipulating
  WCS objects, including dropping, swapping, and adding axes.

Misc
^^^^

- Includes the new astropy-helpers package which separates some of Astropy's
  build, installation, and documentation infrastructure out into an
  independent package, making it easier for Affiliated Packages to depend on
  these features.  astropy-helpers replaces/deprecates some of the submodules
  in the ``astropy`` package (see API Changes below).  See also
  `APE 4 <https://github.com/astropy/astropy-APEs/blob/master/APE4.rst>`_
  for more details on the motivation behind and implementation of
  astropy-helpers.  [#1563]


API Changes
-----------

astropy.config
^^^^^^^^^^^^^^

- The configuration system received a major overhaul, as part of APE3.  It is
  no longer possible to save configuration items from Python, but instead
  users must edit the configuration file directly.  The locations of
  configuration items have moved, and some have been changed to science state
  values.  The old locations should continue to work until astropy 0.5, but
  deprecation warnings will be displayed.  See the `Configuration transition
  <https://docs.astropy.org/en/v0.4/config/config_0_4_transition.html>`_
  docs for a detailed description of the changes and how to update existing
  code. [#2094]

astropy.io.fits
^^^^^^^^^^^^^^^

- The ``astropy.io.fits.new_table`` function is now fully deprecated (though
  will not be removed for a long time, considering how widely it is used).

  Instead please use the more explicit ``BinTableHDU.from_columns`` to create
  a new binary table HDU, and the similar ``TableHDU.from_columns`` to create
  a new ASCII table.  These otherwise accept the same arguments as
  ``new_table`` which is now just a wrapper for these.

- The ``.fromstring`` classmethod of each HDU type has been simplified such
  that, true to its namesake, it only initializes an HDU from a string
  containing its header *and* data.

- Fixed an issue where header wildcard matching (for example
  ``header['DATE*']``) can be used to match *any* characters that might
  appear in a keyword.  Previously this only matched keywords containing
  characters in the set ``[0-9A-Za-z_]``.  Now this can also match a hyphen
  ``-`` and any other characters, as some conventions like ``HIERARCH`` and
  record-valued keyword cards allow a wider range of valid characters than
  standard FITS keywords.

- This will be the *last* release to support the following APIs that have
  been marked deprecated since Astropy v0.1/PyFITS v3.1:

- The ``CardList`` class, which was part of the old header implementation.

- The ``Card.key`` attribute.  Use ``Card.keyword`` instead.

- The ``Card.cardimage`` and ``Card.ascardimage`` attributes.  Use simply
  ``Card.image`` or ``str(card)`` instead.

- The ``create_card`` factory function.  Simply use the normal ``Card``
  constructor instead.

- The ``create_card_from_string`` factory function.  Use ``Card.fromstring``
  instead.

- The ``upper_key`` function.  Use ``Card.normalize_keyword`` method
  instead (this is not unlikely to be used outside of PyFITS itself, but it
  was technically public API).

- The usage of ``Header.update`` with ``Header.update(keyword, value,
  comment)`` arguments.  ``Header.update`` should only be used analogously
  to ``dict.update``.  Use ``Header.set`` instead.

- The ``Header.ascard`` attribute.  Use ``Header.cards`` instead for a list
  of all the ``Card`` objects in the header.

- The ``Header.rename_key`` method.  Use ``Header.rename_keyword`` instead.

- The ``Header.get_history`` method.  Use ``header['HISTORY']`` instead
  (normal keyword lookup).

- The ``Header.get_comment`` method.  Use ``header['COMMENT']`` instead.

- The ``Header.toTxtFile`` method.  Use ``header.totextfile`` instead.

- The ``Header.fromTxtFile`` method.  Use ``Header.fromtextfile`` instead.

- The ``tdump`` and ``tcreate`` functions.  Use ``tabledump`` and
  ``tableload`` respectively.

- The ``BinTableHDU.tdump`` and ``tcreate`` methods.  Use
  ``BinTableHDU.dump`` and ``BinTableHDU.load`` respectively.

- The ``txtfile`` argument to the ``Header`` constructor.  Use
  ``Header.fromfile`` instead.

- The ``startColumn`` and ``endColumn`` arguments to the ``FITS_record``
  constructor.  These are unlikely to be used by any user code.

  These deprecated interfaces will be removed from the development version of
  Astropy following the v0.4 release (they will still be available in any
  v0.4.x bugfix releases, however).

astropy.modeling
^^^^^^^^^^^^^^^^

- The method computing the derivative of the model with respect
  to parameters was renamed from ``deriv`` to ``fit_deriv``. [#1739]

- ``ParametricModel`` and the associated ``Parametric1DModel`` and
  ``Parametric2DModel`` classes have been renamed ``FittableModel``,
  ``Fittable1DModel``, and ``Fittable2DModel`` respectively.  The base
  ``Model`` class has subsumed the functionality of the old

  ``ParametricModel`` class so that all models support parameter constraints.
  The only distinction of ``FittableModel`` is that anything which subclasses
  it is assumed "safe" to use with Astropy fitters. [#2276]

- ``NonLinearLSQFitter`` has been renamed ``LevMarLSQFitter`` to emphasise
  that it uses the Levenberg-Marquardt optimization algorithm with a
  least squares statistic function. [#1914]

- The ``SLSQPFitter`` class has been renamed ``SLSQPLSQFitter`` to emphasize
  that it uses the Sequential Least Squares Programming optimization
  algorithm with a least squares statistic function. [#1914]

- The ``Fitter.errorfunc`` method has been renamed to the more general
  ``Fitter.objective_function``. [#1914]

astropy.nddata
^^^^^^^^^^^^^^

- Issue warning if unit is changed from a non-trivial value by directly
  setting ``NDData.unit``. [#2411]

- The ``mask`` and ``flag`` attributes of ``astropy.nddata.NDData`` can now
  be set with any array-like object instead of requiring that they be set
  with a ``numpy.ndarray``. [#2419]

astropy.sphinx
^^^^^^^^^^^^^^

- Use of the ``astropy.sphinx`` module is deprecated; all new development of
  this module is in ``astropy_helpers.sphinx`` which should be used instead
  (therefore documentation builds that made use of any of the utilities in
  ``astropy.sphinx`` now have ``astropy_helpers`` as a documentation
  dependency).

astropy.table
^^^^^^^^^^^^^

- The default table printing function now shows a table header row for units
  if any columns have the unit attribute set.  [#1282]

- Before, an unmasked ``Table`` was automatically converted to a masked
  table if generated from a masked Table or a ``MaskedColumn``.
  Now, this conversion is only done if explicitly requested or if any
  of the input values is actually masked. [#1185]

- The repr() function of ``astropy.table.Table`` now shows the units
  if any columns have the unit attribute set.  [#2180]

- The semantics of the config options ``table.max_lines`` and
  ``table.max_width`` has changed slightly.  If these values are not
  set in the config file, astropy will try to determine the size
  automatically from the terminal. [#2683]

astropy.time
^^^^^^^^^^^^

- Correct use of UT in TDB calculation [#1938, #1939].

- ``TimeDelta`` objects can have scales other than TAI [#1932].

- Location information should now be passed on via an ``EarthLocation``
  instance or anything that initialises it, e.g., a tuple containing
  either geocentric or geodetic coordinates. [#1928]

astropy.units
^^^^^^^^^^^^^

- ``Quantity`` now converts input to float by default, as this is physically
  most sensible for nearly all units [#1776].

- ``Quantity`` comparisons with ``==`` or ``!=`` now always return ``True``
  or ``False``, even if units do not match (for which case a ``UnitsError``
  used to be raised).  [#2328]

- Applying ``float`` or ``int`` to a ``Quantity`` now works for all
  dimensionless quantities; they are automatically converted to unscaled
  dimensionless. [#2249]

- The exception ``astropy.units.UnitException``, which was
  deprecated in astropy 0.2, has been removed.  Use
  ``astropy.units.UnitError`` instead [#2386]

- Initializing a ``Quantity`` with a valid number/array with a ``unit``
  attribute now interprets that attribute as the units of the input value.
  This makes it possible to initialize a ``Quantity`` from an Astropy
  ``Table`` column and have it correctly pick up the units from the column.
  [#2486]

astropy.wcs
^^^^^^^^^^^

- ``calcFootprint`` was deprecated. It is replaced by
  ``calc_footprint``.  An optional boolean keyword ``center`` was
  added to ``calc_footprint``.  It controls whether the centers or
  the corners of the pixels are used in the computation. [#2384]

- ``astropy.wcs.WCS.sip_pix2foc`` and
  ``astropy.wcs.WCS.sip_foc2pix`` formerly did not conform to the
  ``SIP`` standard: ``CRPIX`` was added to the ``foc`` result so
  that it could be used as input to "core FITS WCS".  As of astropy
  0.4, ``CRPIX`` is no longer added to the result, so the ``foc``
  space is correct as defined in the `SIP convention
  <https://ui.adsabs.harvard.edu/abs/2005ASPC..347..491S>`__. [#2360]

- ``astropy.wcs.UnitConverter``, which was deprecated in astropy
  0.2, has been removed.  Use the ``astropy.units`` module
  instead. [#2386]

- The following methods on ``astropy.wcs.WCS``, which were
  deprecated in astropy 0.1, have been removed [#2386]:

- ``all_pix2sky`` -> ``all_pix2world``

- ``wcs_pix2sky`` -> ``wcs_pix2world``

- ``wcs_sky2pix`` -> ``wcs_world2pix``

- The ``naxis1`` and ``naxis2`` attributes and the ``get_naxis``
  method of ``astropy.wcs.WCS``, which were deprecated in astropy
  0.2, have been removed.  Use the shape of the underlying FITS data
  array instead.  [#2386]

Misc
^^^^

- The ``astropy.setup_helpers`` and ``astropy.version_helpers`` modules are
  deprecated; any non-critical fixes and development to those modules should
  be in ``astropy_helpers`` instead.  Packages that use these modules in
  their ``setup.py`` should depend on ``astropy_helpers`` following the same
  pattern as in the Astropy package template.


Bug Fixes
---------

astropy.constants
^^^^^^^^^^^^^^^^^

- ``astropy.constants.Contant`` objects can now be deep
  copied. [#2601]

astropy.cosmology
^^^^^^^^^^^^^^^^^

- The distance modulus function in ``astropy.cosmology`` can now handle
  negative distances, which can occur in certain closed cosmologies. [#2008]

- Removed accidental imports of some extraneous variables in
  ``astropy.cosmology`` [#2025]

astropy.io.ascii
^^^^^^^^^^^^^^^^

- ``astropy.io.ascii.read`` would fail to read lists of strings where some of
  the strings consisted of just a newline ("\n"). [#2648]

astropy.io.fits
^^^^^^^^^^^^^^^

- Use NaN for missing values in FITS when using Table.write for float
  columns. Earlier the default fill value was close to 1e20.[#2186]

- Fixes for checksums on 32-bit platforms.  Results may be different
  if writing or checking checksums in "nonstandard" mode.  [#2484]

- Additional minor bug fixes ported from PyFITS.  [#2575]

astropy.io.votable
^^^^^^^^^^^^^^^^^^

- It is now possible to save an ``astropy.table.Table`` object as a
  VOTable with any of the supported data formats, ``tabledata``,
  ``binary`` and ``binary2``, by using the ``tabledata_format``
  kwarg. [#2138]

- Fixed a crash writing out variable length arrays. [#2577]

astropy.nddata
^^^^^^^^^^^^^^

- Indexing ``NDData`` in a way that results in a single element returns that
  element. [#2170]

- Change construction of result of arithmetic and unit conversion to allow
  subclasses to require the presence of attribute like unit. [#2300]

- Scale uncertainties to correct units in arithmetic operations and unit
  conversion. [#2393]

- Ensure uncertainty and mask members are copied in arithmetic and
  convert_unit_to. [#2394]

- Mask result of arithmetic if either of the operands is masked. [#2403]

- Copy all attributes of input object if ``astropy.nddata.NDData`` is
  initialized with an ``NDData`` object. [#2406]

- Copy ``flags`` to new object in ``convert_unit_to``. [#2409]

- Result of ``NDData`` arithmetic makes a copy of any WCS instead of using
  a reference. [#2410]

- Fix unit handling for multiplication/division and use
  ``astropy.units.Quantity`` for units arithmetic. [#2413]

- A masked ``NDData`` is now converted to a masked array when used in an
  operation or ufunc with a numpy array. [#2414]

- An unmasked ``NDData`` now uses an internal representation of its mask
  state that ``numpy.ma`` expects so that an ``NDData`` behaves as an
  unmasked array. [#2417]

astropy.sphinx
^^^^^^^^^^^^^^

- Fix crash in smart resolver when the resolution doesn't work. [#2591]

astropy.table
^^^^^^^^^^^^^

- The ``astropy.table.Column`` object can now use both functions and callable
  objects as formats. [#2313]

- Fixed a problem on 64 bit windows that caused errors
  "expected 'DTYPE_t' but got 'long long'" [#2490]

- Fix initialisation of ``TableColumns`` with lists or tuples.  [#2647]

- Fix removal of single column using ``remove_columns``. [#2699]

- Fix a problem that setting a row element within a masked table did not
  update the corresponding table element. [#2734]

astropy.time
^^^^^^^^^^^^

- Correct UT1->UTC->UT1 round-trip being off by 1 second if UT1 is
  on a leap second. [#2077]

astropy.units
^^^^^^^^^^^^^

- ``Quantity.copy`` now behaves identically to ``ndarray.copy``, and thus
  supports the ``order`` argument (for numpy >=1.6). [#2284]

- Composing base units into identical composite units now works. [#2382]

- Creating and composing/decomposing units is now substantially faster [#2544]

- ``Quantity`` objects now are able to be assigned NaN [#2695]

astropy.wcs
^^^^^^^^^^^

- Astropy now requires wcslib version 4.23.  The version of wcslib
  included with astropy has been updated to version 4.23.

- Bug fixes in the projection routines: in ``hpxx2s`` [the
  cartesian-to-spherical operation of the ``HPX`` projection]
  relating to bounds checking, bug introduced at wcslib 4.20; in
  ``parx2s`` and molx2s`` [the cartesion-to-spherical operation of
  the ``PAR`` and ``MOL`` projections respectively] relating to
  setting the stat vector; in ``hpxx2s`` relating to implementation
  of the vector API; and in ``xphx2s`` relating to setting an
  out-of-bounds value of *phi*.

- In the ``PCO`` projection, use alternative projection equations
  for greater numerical precision near theta == 0.  In the ``COP``
  projection, return an exact result for theta at the poles.
  Relaxed the tolerance for bounds checking a little in ``SFL``
  projection.

- Fix a bug allocating insufficient memory in
  ``astropy.wcs.WCS.sub`` [#2468]

- A new method, ``Wcsprm.bounds_check`` (corresponding to wcslib's
  ``wcsbchk``) has been added to control what bounds checking is performed by
  wcslib.

- ``WCS.to_header`` will now raise a more meaningful exception when the WCS
  information is invalid or inconsistent in some way. [#1854]

- In ``WCS.to_header``, ``RESTFRQ`` and ``RESTWAV`` are no longer
  rewritten if zero. [#2468]

- In ``WCS.to_header``, floating point values will now always be written
  with an exponent or fractional part, i.e. ``.0`` being appended if necessary
  to achieve this. [#2468]

- If the C extension for ``astropy.wcs`` was not built or fails to import for
  any reason, ``import astropy.wcs`` will result in an ``ImportError``,
  rather than getting obscure errors once the ``astropy.wcs`` is used.
  [#2061]

- When the C extension for ``astropy.wcs`` is built using a version of
  ``wscslib`` already present in the system, the package does not try
  to install ``wcslib`` headers under ``astropy/wcs/include``. [#2536]

- Fixes an unresolved external symbol error in the
  ``astropy.wcs._wcs`` C extension on Microsoft Windows when built
  with a Microsoft compiler. [#2478]

Misc
^^^^

- Running the test suite with ``python setup.py test`` now works if
  the path to the source contains spaces. [#2488]

- The version of ERFA included with Astropy is now v1.1.0 [#2497]

- Removed deprecated option from travis configuration and force use of
  wheels rather than allowing build from source. [#2576]

- The short option ``-n`` to run tests in parallel was broken
  (conflicts with the distutils built-in option of "dry-run").
  Changed to ``-j``. [#2566]

Other Changes and Additions
---------------------------

- python setup.py test --coverage will now give more accurate
  results, because the coverage analysis will include early imports of
  astropy.  There doesn't seem to be a way to get this to work when
  doing ``import astropy; astropy.test()``, so the ``coverage``
  keyword to ``astropy.test`` has been removed.  Coverage testing now
  depends only on `coverage.py
  <http://coverage.readthedocs.io/en/latest/>`__, not
  ``pytest-cov``. [#2112]

- The included version of py.test has been upgraded to 2.5.1. [#1970]

- The included version of six.py has been upgraded to 1.5.2. [#2006]

- Where appropriate, tests are now run both with and without the
  ``unicode_literals`` option to ensure that we support both cases. [#1962]

- Running the Astropy test suite from within the IPython REPL is disabled for
  now due to bad interaction between the test runner and IPython's logging
  and I/O handler.  For now, run the Astropy tests should be run in the basic
  Python interpreter. [#2684]

- Added support for numerical comparison of floating point values appearing in
  the output of doctests using a ``+FLOAT_CMP`` doctest flag. [#2087]

- A monkey patch is performed to fix a bug in Numpy version 1.7 and
  earlier where unicode fill values on masked arrays are not
  supported.  This may cause unintended side effects if your
  application also monkey patches ``numpy.ma`` or relies on the broken
  behavior.  If unicode support of masked arrays is important to your
  application, upgrade to Numpy 1.8 or later for best results. [#2059]

- The developer documentation has been extensively rearranged and
  rewritten. [#1712]

- The ``human_time`` function in ``astropy.utils`` now returns strings
  without zero padding. [#2420]

- The ``bdist_dmg`` command for ``setup.py`` has now been removed. [#2553]

- Many broken API links have been fixed in the documentation, and the
  ``nitpick`` Sphinx option is now used to avoid broken links in future.
  [#1221, #2019, #2109, #2161, #2162, #2192, #2200, #2296, #2448, #2456,
  #2460, #2467, #2476, #2508, #2509]


0.3.2 (2014-05-13)
==================

Bug Fixes
---------

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- if ``sep`` argument is specified to be a single character in
  ``sexagisimal_to_string``, it now includes separators only between
  items [#2183]

- Ensure comparisons involving ``Distance`` objects do not raise exceptions;
  also ensure operations that lead to units other than length return
  ``Quantity``. [#2206, #2250]

- Multiplication and division of ``Angle`` objects is now
  supported. [#2273]

- Fixed ``Angle.to_string`` functionality so that negative angles have the
  correct amount of padding when ``pad=True``. [#2337]

- Mixing strings and quantities in the ``Angle`` constructor now
  works.  For example: ``Angle(['1d', 1. * u.d])``.  [#2398]

- If ``Longitude`` is given a ``Longitude`` as input, use its ``wrap_angle``
  by default [#2705]

astropy.cosmology
^^^^^^^^^^^^^^^^^

- Fixed ``format()`` compatibility with Python 2.6. [#2129]

- Be more careful about converting to floating point internally [#1815, #1818]

astropy.io.ascii
^^^^^^^^^^^^^^^^

- The CDS reader in ``astropy.io.ascii`` can now handle multiple
  description lines in ReadMe files. [#2225]

- When reading a table with values that generate an overflow error during
  type conversion (e.g. overflowing the native C long type), fall through to
  using string. Previously this generated an exception [#2234].

- Recognize any string with one to four dashes as null value. [#1335]

astropy.io.fits
^^^^^^^^^^^^^^^

- Allow pickling of ``FITS_rec`` objects. [#1597]

- Improved behavior when writing large compressed images on OSX by removing
  an unnecessary check for platform architecture. [#2345]

- Fixed an issue where Astropy ``Table`` objects containing boolean columns
  were not correctly written out to FITS files. [#1953]

- Several other bug fixes ported from PyFITS v3.2.3 [#2368]

- Fixed a crash on Python 2.x when writing a FITS file directly to a
  ``StringIO.StringIO`` object. [#2463]

astropy.io.registry
^^^^^^^^^^^^^^^^^^^

- Allow readers/writers with the same name to be attached to different
  classes. [#2312]

astropy.io.votable
^^^^^^^^^^^^^^^^^^

- By default, floating point values are now written out using
  ``repr`` rather than ``str`` to preserve precision [#2137]

astropy.modeling
^^^^^^^^^^^^^^^^

- Fixed the ``SIP`` and ``InverseSIP`` models both so that they work in the
  first place, and so that they return results consistent with the SIP
  functions in ``astropy.wcs``. [#2177]

astropy.stats
^^^^^^^^^^^^^

- Ensure the ``axis`` keyword in ``astropy.stats.funcs`` can now be used for
  all axes. [#2173]

astropy.table
^^^^^^^^^^^^^

- Ensure nameless columns can be printed, using 'None' for the header. [#2213]

astropy.time
^^^^^^^^^^^^

- Fixed pickling of ``Time`` objects. [#2123]

astropy.units
^^^^^^^^^^^^^

- ``Quantity._repr_latex_()`` returns ``NotImplementedError`` for quantity
  arrays instead of an uninformative formatting exception. [#2258]

- Ensure ``Quantity.flat`` always returns ``Quantity``. [#2251]

- Angstrom unit renders better in MathJax [#2286]

astropy.utils
^^^^^^^^^^^^^

- Progress bars will now be displayed inside the IPython
  qtconsole. [#2230]

- ``data.download_file()`` now evaluates ``REMOTE_TIMEOUT()`` at runtime
  rather than import time. Previously, setting ``REMOTE_TIMEOUT`` after
  import had no effect on the function's behavior. [#2302]

- Progressbar will be limited to 100% so that the bar does not exceed the
  terminal width.  The numerical display can still exceed 100%, however.

astropy.vo
^^^^^^^^^^

- Fixed ``format()`` compatibility with Python 2.6. [#2129]

- Cone Search validation no longer raises ``ConeSearchError`` for positive RA.
  [#2240, #2242]

astropy.wcs
^^^^^^^^^^^

- Fixed a bug where calling ``astropy.wcs.Wcsprm.sub`` with
  ``WCSSUB_CELESTIAL`` may cause memory corruption due to
  underallocation of a temporary buffer. [#2350]

- Fixed a memory allocation bug in ``astropy.wcs.Wcsprm.sub`` and
  ``astropy.wcs.Wcsprm.copy``.  [#2439]

Misc
^^^^

- Fixes for compatibility with Python 3.4. [#1945]

- ``import astropy; astropy.test()`` now correctly uses the same test
  configuration as ``python setup.py test`` [#1811]


0.3.1 (2014-03-04)
==================

Bug Fixes
---------

astropy.config
^^^^^^^^^^^^^^

- Fixed a bug where ``ConfigurationItem.set_temp()`` does not reset to
  default value when exception is raised within ``with`` block. [#2117]

astropy.convolution
^^^^^^^^^^^^^^^^^^^

- Fixed a bug where ``_truncation`` was left undefined for ``CustomKernel``.
  [#2016]

- Fixed a bug with ``_normalization`` when ``CustomKernel`` input array
  sums to zero. [#2016]

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- Fixed a bug where using ``==`` on two array coordinates wouldn't
  work. [#1832]

- Fixed bug which caused ``len()`` not to work for coordinate objects and
  added a ``.shape`` property to get appropriately array-like behavior.
  [#1761, #2014]

- Fixed a bug where sexagesimal notation would sometimes include
  exponential notation in the last field. [#1908, #1913]

- ``CompositeStaticMatrixTransform`` no longer attempts to reference the
  undefined variable ``self.matrix`` during instantiation. [#1944]

- Fixed pickling of ``Longitude``, ensuring ``wrap_angle`` is preserved
  [#1961]

- Allow ``sep`` argument in ``Angle.to_string`` to be empty (resulting in no
  separators) [#1989]

astropy.io.ascii
^^^^^^^^^^^^^^^^

- Allow passing unicode delimiters when reading or writing tables.  The
  delimiter must be convertible to pure ASCII.  [#1949]

- Fix a problem when reading a table and renaming the columns to names that
  already exist. [#1991]

astropy.io.fits
^^^^^^^^^^^^^^^

- Ported all bug fixes from PyFITS 3.2.1.  See the PyFITS changelog at
  https://pyfits.readthedocs.io/en/v3.2.1/ [#2056]

astropy.io.misc
^^^^^^^^^^^^^^^

- Fixed issues in the HDF5 Table reader/writer functions that occurred on
  Windows. [#2099]

astropy.io.votable
^^^^^^^^^^^^^^^^^^

- The ``write_null_values`` kwarg to ``VOTable.to_xml``, when set to `False`
  (the default) would produce non-standard VOTable files.  Therefore, this
  functionality has been replaced by a better understanding that knows which
  fields in a VOTable may be left empty (only ``char``, ``float`` and
  ``double`` in VOTable 1.1 and 1.2, and all fields in VOTable 1.3).  The
  kwarg is still accepted but it will be ignored, and a warning is emitted.
  [#1809]

- Printing out a ``astropy.io.votable.tree.Table`` object using `repr` or
  `str` now uses the pretty formatting in ``astropy.table``, so it's possible
  to easily preview the contents of a ``VOTable``. [#1766]

astropy.modeling
^^^^^^^^^^^^^^^^

- Fixed bug in computation of model derivatives in ``LinearLSQFitter``.
  [#1903]

- Raise a ``NotImplementedError`` when fitting composite models. [#1915]

- Fixed bug in the computation of the ``Gaussian2D`` model. [#2038]

- Fixed bug in the computation of the ``AiryDisk2D`` model. [#2093]

astropy.sphinx
^^^^^^^^^^^^^^

- Added slightly more useful debug info for AstropyAutosummary. [#2024]

astropy.table
^^^^^^^^^^^^^

- The column string representation for n-dimensional cells with only
  one element has been fixed. [#1522]

- Fix a problem that caused ``MaskedColumn.__getitem__`` to not preserve
  column metadata. [#1471, #1872]

- With Numpy prior to version 1.6.2, tables with Unicode columns now
  sort correctly. [#1867]

- ``astropy.table`` can now print out tables with Unicode columns containing
  non-ascii characters. [#1864]

- Columns can now be named with Unicode strings, as long as they contain only
  ascii characters.  This makes using ``astropy.table`` easier on Python 2
  when ``from __future__ import unicode_literals`` is used. [#1864]

- Allow pickling of ``Table``, ``Column``, and ``MaskedColumn`` objects. [#792]

- Fix a problem where it was not possible to rename columns after sorting or
  adding a row. [#2039]

astropy.time
^^^^^^^^^^^^

- Fix a problem where scale conversion problem in TimeFromEpoch
  was not showing a useful error [#2046]

- Fix a problem when converting to one of the formats ``unix``, ``cxcsec``,
  ``gps`` or ``plot_date`` when the time scale is ``UT1``, ``TDB`` or ``TCB``
  [#1732]

- Ensure that ``delta_ut1_utc`` gets calculated when accessed directly,
  instead of failing and giving a rather obscure error message [#1925]

- Fix a bug when computing the TDB to TT offset.  The transform routine was
  using meters instead of kilometers for the Earth vector.  [#1929]

- Increase ``__array_priority__`` so that ``TimeDelta`` can convert itself
  to a ``Quantity`` also in reverse operations [#1940]

- Correct hop list from TCG to TDB to ensure that conversion is
  possible [#2074]

astropy.units
^^^^^^^^^^^^^

- ``Quantity`` initialisation rewritten for speed [#1775]

- Fixed minor string formatting issue for dimensionless quantities. [#1772]

- Fix error for inplace operations on non-contiguous quantities [#1834].

- The definition of the unit ``bar`` has been corrected to "1e5
  Pascal" from "100 Pascal" [#1910]

- For units that are close to known units, but not quite, for
  example due to differences in case, the exception will now include
  recommendations. [#1870]

- The generic and FITS unit parsers now accept multiple slashes in
  the unit string.  There are multiple ways to interpret them, but
  the approach taken here is to convert "m/s/kg" to "m s-1 kg-1".
  Multiple slashes are accepted, but discouraged, by the FITS
  standard, due to the ambiguity of parsing, so a warning is raised
  when it is encountered. [#1911]

- The use of "angstrom" (with a lower case "a") is now accepted in FITS unit
  strings, since it is in common usage.  However, since it is not officially
  part of the FITS standard, a warning will be issued when it is encountered.
  [#1911]

- Pickling unrecognized units will not raise a ``AttributeError``. [#2047]

- ``astropy.units`` now correctly preserves the precision of
  fractional powers. [#2070]

- If a ``Unit`` or ``Quantity`` is raised to a floating point power
  that is very close to a rational number with a denominator less
  than or equal to 10, it is converted to a ``Fraction`` object to
  preserve its precision through complex unit conversion operations.
  [#2070]

astropy.utils
^^^^^^^^^^^^^

- Fixed crash in ``timer.RunTimePredictor.do_fit``. [#1905]

- Fixed ``astropy.utils.compat.argparse`` for Python 3.1. [#2017]

astropy.wcs
^^^^^^^^^^^

- ``astropy.wcs.WCS``, ``astropy.wcs.WCS.fix`` and
  ``astropy.wcs.find_all_wcs`` now have a ``translate_units`` keyword
  argument that is passed down to ``astropy.wcs.Wcsprm.fix``.  This can be
  used to specify any unsafe translations of units from rarely used ones to
  more commonly used ones.

  Although ``"S"`` is commonly used to represent seconds, its translation to
  ``"s"`` is potentially unsafe since the standard recognizes ``"S"``
  formally as Siemens, however rarely that may be used.  The same applies to
  ``"H"`` for hours (Henry), and ``"D"`` for days (Debye).

  When these sorts of changes are performed, a warning is emitted.
  [#1854]

- When a unit is "fixed" by ``astropy.wcs.WCS.fix`` or
  ``astropy.wcs.Wcsprm.unitfix``, it now correctly reports the ``CUNIT``
  field that was changed. [#1854]

- ``astropy.wcs.Wcs.printwcs`` will no longer warn that ``cdelt`` is being
  ignored when none was present in the FITS file. [#1845]

- ``astropy.wcs.Wcsprm.set`` is called from within the ``astropy.wcs.WCS``
  constructor, therefore any invalid information in the keywords will be
  raised from the constructor, rather than on a subsequent call to a
  transformation method. [#1918]

- Fix a memory corruption bug when using ``astropy.wcs.Wcs.sub`` with
  ``astropy.wcs.WCSSUB_CELESTIAL``. [#1960]

- Fixed the ``AttributeError`` exception that was raised when using
  ``astropy.wcs.WCS.footprint_to_file``. [#1912]

- Fixed a ``NameError`` exception that was raised when using
  ``astropy.wcs.validate`` or the ``wcslint`` script. [#2053]

- Fixed a bug where named WCSes may be erroneously reported as ``' '`` when
  using ``astropy.wcs.validate`` or the ``wcslint`` script. [#2053]

- Fixed a bug where error messages about incorrect header keywords
  may not be propagated correctly, resulting in a "NULL error object
  in wcslib" message. [#2106]

Misc
^^^^

- There are a number of improvements to make Astropy work better on big
  endian platforms, such as MIPS, PPC, s390x and SPARC. [#1849]

- The test suite will now raise exceptions when a deprecated feature of
  Python or Numpy is used.  [#1948]

Other Changes and Additions
---------------------------

- A new function, ``astropy.wcs.get_include``, has been added to get the
  location of the ``astropy.wcs`` C header files. [#1755]

- The doctests in the ``.rst`` files in the ``docs`` folder are now
  tested along with the other unit tests.  This is in addition to the
  testing of doctests in docstrings that was already being performed.
  See ``docs/development/testguide.rst`` for more information. [#1771]

- Fix a problem where import fails on Python 3 if setup.py exists
  in current directory. [#1877]


0.3 (2013-11-20)
================

New Features
------------

- General

- A top-level configuration item, ``unicode_output`` has been added to
  control whether the Unicode string representation of certain
  objects will contain Unicode characters.  For example, when
  ``use_unicode`` is `False` (default)::

      >>> from astropy import units as u
      >>> print(unicode(u.degree))
      deg

  When ``use_unicode`` is `True`::

      >>> from astropy import units as u
      >>> print(unicode(u.degree))
      

  See `handling-unicode
  <https://docs.astropy.org/en/v0.3/development/codeguide.html#unicode-guidelines>`_
  for more information. [#1441]

- ``astropy.utils.misc.find_api_page`` is now imported into the top-level.
  This allows usage like ``astropy.find_api_page(astropy.units.Quantity)``.
  [#1779]

astropy.convolution
^^^^^^^^^^^^^^^^^^^

- New class-based system for generating kernels, replacing ``make_kernel``.
  [#1255] The ``astropy.nddata.convolution`` sub-package has now been moved
  to ``astropy.convolution``. [#1451]

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- Two classes ``astropy.coordinates.Longitude`` and
  ``astropy.coordinates.Latitude`` have been added.  These are derived from
  the new ``Angle`` class and used for all longitude-like (RA, azimuth,
  galactic L) and latitude-like coordinates (Dec, elevation, galactic B)
  respectively.  The ``Longitude`` class provides auto-wrapping capability
  and ``Latitude`` performs bounds checking.

- ``astropy.coordinates.Distance`` supports conversion to and from distance
  modulii. [#1472]

- ``astropy.coordinates.SphericalCoordinateBase`` and derived classes now
  support arrays of coordinates, enabling large speed-ups for some operations
  on multiple coordinates at the same time. These coordinates can also be
  indexed using standard slicing or any Numpy-compatible indexing. [#1535,
  #1615]

- Array coordinates can be matched to other array coordinates, finding the
  closest matches between the two sets of coordinates (see the
  ``astropy.coordinates.matching.match_coordinates_3d`` and
  ``astropy.coordinates.matching.match_coordinates_sky`` functions). [#1535]

astropy.cosmology
^^^^^^^^^^^^^^^^^

- Added support for including massive Neutrinos in the cosmology classes. The
  Planck (2013) cosmology has been updated to use this. [#1364]

- Calculations now use and return ``Quantity`` objects where appropriate.
  [#1237]

astropy.io.ascii
^^^^^^^^^^^^^^^^

- Added support for writing IPAC format tables [#1152].

astropy.io.fits
^^^^^^^^^^^^^^^

- Added initial support for table columns containing pseudo-unsigned
  integers.  This is currently enabled by using the ``uint=True`` option when
  opening files; any table columns with the correct BZERO value will be
  interpreted and returned as arrays of unsigned integers. [#906]

- Upgraded vendored copy of CFITSIO to v3.35, though backwards compatibility
  back to version v3.28 is maintained.

- Added support for reading and writing tables using the Q format for columns.
  The Q format is identical to the P format (variable-length arrays) except
  that it uses 64-bit integers for the data descriptors, allowing more than
  4 GB of variable-length array data in a single table.

- Some refactoring of the table and ``FITS_rec`` modules in order to better
  separate the details of the FITS binary and ASCII table data structures from
  the HDU data structures that encapsulate them.  Most of these changes should
  not be apparent to users (but see API Changes below).

astropy.io.votable
^^^^^^^^^^^^^^^^^^

- Updated to support the VOTable 1.3 draft. [#433]

- Added the ability to look up and group elements by their utype attribute.
  [#622]

- The format of the units of a VOTable file can be specified using the
  ``unit_format`` parameter.  Note that units are still always written out
  using the CDS format, to ensure compatibility with the standard.

astropy.modeling
^^^^^^^^^^^^^^^^

- Added a new framework for representing and evaluating mathematical models
  and for fitting data to models.  See "What's New in Astropy 0.3" in the
  documentation for further details. [#493]

astropy.stats
^^^^^^^^^^^^^

- Added robust statistics functions
  ``astropy.stats.funcs.median_absolute_deviation``,
  ``astropy.stats.funcs.biweight_location``, and
  ``astropy.stats.funcs.biweight_midvariance``. [#621]

- Added ``astropy.stats.funcs.signal_to_noise_oir_ccd`` for computing the
  signal to noise ratio for source being observed in the optical/IR using a
  CCD. [#870]

- Add ``axis=int`` option to ``stropy.stats.funcs.sigma_clip`` to allow
  clipping along a given axis for multidimensional data. [#1083]

astropy.table
^^^^^^^^^^^^^

- New columns can be added to a table via assignment to a non-existing
  column by name. [#726]

- Added ``join`` function to perform a database-like join on two tables. This
  includes support for inner, left, right, and outer joins as well as
  metadata merging.  [#903]

- Added ``hstack`` and ``vstack`` functions to stack two or more tables.
  [#937]

- Tables now have a ``.copy`` method and include support for ``copy`` and
  ``deepcopy``. [#1208]

- Added support for selecting and manipulating groups within a table with
  a database style ``group_by`` method. [#1424]

- Table ``read`` and ``write`` functions now include rudimentary support
  reading and writing of FITS tables via the unified reading/writing
  interface. [#591]

- The ``units`` and ``dtypes`` attributes and keyword arguments in Column,
  MaskedColumn, Row, and Table are now deprecated in favor of the
  single-tense ``unit`` and ``dtype``. [#1174]

- Setting a column from a Quantity now correctly sets the unit on the Column
  object. [#732]

- Add ``remove_row`` and ``remove_rows`` to remove table rows. [#1230]

- Added a new ``Table.show_in_browser`` method that opens a web browser
  and displays the table rendered as HTML. [#1342]

- New tables can now be instantiated using a single row from an existing
  table. [#1417]

astropy.time
^^^^^^^^^^^^

- New ``Time`` objects can be instantiated from existing ``Time`` objects
  (but with different format, scale, etc.) [#889]

- Added a ``Time.now`` classmethod that returns the current UTC time,
  similarly to Python's ``datetime.now``. [#1061]

- Update internal time manipulations so that arithmetic with Time and
  TimeDelta objects maintains sub-nanosecond precision over a time span
  longer than the age of the universe. [#1189]

- Use ``astropy.utils.iers`` to provide ``delta_ut1_utc``, so that
  automatic calculation of UT1 becomes possible. [#1145]

- Add ``datetime`` format which allows converting to and from standard
  library ``datetime.datetime`` objects. [#860]

- Add ``plot_date`` format which allows converting to and from the date
  representation used when plotting dates with matplotlib via the
  ``matplotlib.pyplot.plot_date`` function. [#860]

- Add ``gps`` format (seconds since 1980-01-01 00:00:00 UTC,
  including leap seconds) [#1164]

- Add array indexing to Time objects [#1132]

- Allow for arithmetic of multi-element and single-element Time and TimeDelta
  objects. [#1081]

- Allow multiplication and division of TimeDelta objects by
  constants and arrays, as well as changing sign (negation) and
  taking the absolute value of TimeDelta objects. [#1082]

- Allow comparisons of Time and TimeDelta objects. [#1171]

- Support interaction of Time and Quantity objects that represent a time
  interval. [#1431]

astropy.units
^^^^^^^^^^^^^

- Added parallax equivalency for length-angle. [#985]

- Added mass-energy equivalency. [#1333]

- Added a new-style format method which will use format specifiers
  (like ``0.03f``) in new-style format strings for the Quantity's value.
  Specifiers which can't be applied to the value will fall back to the
  entire string representation of the quantity. [#1383]

- Added support for complex number values in quantities. [#1384]

- Added new spectroscopic equivalencies for velocity conversions
  (relativistic, optical, and radio conventions are supported) [#1200]

- The ``spectral`` equivalency now also handles wave number.

- The ``spectral_density`` equivalency now also accepts a Quantity for the
  frequency or wavelength. It also handles additional flux units.

- Added Brightness Temperature (antenna gain) equivalency for conversion
  between :math:`T_B` and flux density. [#1327]

- Added percent unit, and allowed any string containing just a number to be
  interpreted as a scaled dimensionless unit. [#1409]

- New-style format strings can be used to set the unit output format.  For
  example, ``"{0:latex}".format(u.km)`` will print with the latex formatter.
  [#1462]

- The ``Unit.is_equivalent`` method can now take a tuple. In this case, the
  method returns ``True`` if the unit is equivalent to any of the units
  listed in the tuple. [#1521]

- ``def_unit`` can now take a 2-tuple of names of the form (short, long),
  where each entry is a list.  This allows for handling strange units that
  might have multiple short names. [#1543]

- Added ``dimensionless_angles`` equivalency, which allows conversion of any
  power of radian to dimensionless. [#1161]

- Added the ability to enable set of units, or equivalencies that are used by
  default.  Also provided context managers for these cases. [#1268]

- Imperial units are disabled by default. [#1593, #1662]

- Added an ``astropy.units.add_enabled_units`` context manager, which allows
  creating a temporary context with additional units temporarily enabled in
  the global units namespace. [#1662]

- ``Unit`` instances now have ``.si`` and ``.cgs`` properties a la
  ``Quantity``.  These serve as shortcuts for ``Unit.to_system(cgs)[0]``
  etc. [#1610]

astropy.vo
^^^^^^^^^^

- New package added to support Virtual Observatory Simple Cone Search query
  and service validation. [#552]

astropy.wcs
^^^^^^^^^^^

- Fixed attribute error in ``astropy.wcs.Wcsprm`` (lattype->lattyp) [#1463]

- Included a new command-line script called ``wcslint`` and accompanying API
  for validating the WCS in a given FITS file or header. [#580]

- Upgraded included version of WCSLIB to 4.19.

astropy.utils
^^^^^^^^^^^^^

- Added a new set of utilities in ``astropy.utils.timer`` for analyzing the
  runtime of functions and making runtime predections for larger inputs.
  [#743]

- ``ProgressBar`` and ``Spinner`` classes can now be used directly to return
  generator expressions. [#771]

- Added ``astropy.utils.iers`` which allows reading in of IERS A or IERS B
  bulletins and interpolation in UT1-UTC.

- Added a function ``astropy.utils.find_api_page``--given a class or object
  from the ``astropy`` package, this will open that class's API documentation
  in a web browser. [#663]

- Data download functions such as ``download_file`` now accept a
  ``show_progress`` argument to suppress console output, and a ``timeout``
  argument. [#865, #1258]

astropy.extern.six
^^^^^^^^^^^^^^^^^^

- Added `six <https://pypi.org/project/six/>`_ for python2/python3
  compatibility

- Astropy now uses the ERFA library instead of the IAU SOFA library for
  fundamental time transformation routines.  The ERFA library is derived, with
  permission, from the IAU SOFA library but is distributed under a BSD license.
  See ``license/ERFA.rst`` for details. [#1293]

astropy.logger
^^^^^^^^^^^^^^

- The Astropy logger now no longer catches exceptions by default, and also
  only captures warnings emitted by Astropy itself (prior to this change,
  following an import of Astropy, any warning got re-directed through the
  Astropy logger). Logging to the Astropy log file has also been disabled by
  default. However, users of Astropy 0.2 will likely still see the previous
  behavior with Astropy 0.3 for exceptions and logging to file since the
  default configuration file installed by 0.2 set the exception logging to be
  on by default. To get the new behavior, set the ``log_exceptions`` and
  ``log_to_file`` configuration items to ``False`` in the ``astropy.cfg``
  file. [#1331]

API Changes
-----------

- General

- The configuration option ``utils.console.use_unicode`` has been
  moved to the top level and renamed to ``unicode_output``.  It now
  not only affects console widgets, such as progress bars, but also
  controls whether calling `unicode` on certain classes will return a
  string containing unicode characters.

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- The ``astropy.coordinates.Angle`` class is now a subclass of
  ``astropy.units.Quantity``. This means it has all of the methods of a
  `numpy.ndarray`. [#1006]

- The ``astropy.coordinates.Distance`` class is now a subclass of
  ``astropy.units.Quantity``. This means it has all of the methods of a
  `numpy.ndarray`. [#1472]

- All angular units are now supported, not just ``radian``, ``degree`` and
  ``hour``, but now ``arcsecond`` and ``arcminute`` as well.  The object
  will retain its native unit, so when printing out a value initially
  provided in hours, its ``to_string()`` will, by default, also be
  expressed in hours.

- The ``Angle`` class now supports arrays of angles.

- To be consistent with ``units.Unit``, ``Angle.format`` has been
  deprecated and renamed to ``Angle.to_string``.

- To be consistent with ``astropy.units``, all plural forms of unit names
  have been removed.  Therefore, the following properties of
  ``astropy.coordinates.Angle`` should be renamed:

- ``radians`` -> ``radian``

- ``degrees`` -> ``degree``

- ``hours`` -> ``hour``

- Multiplication and division of two ``Angle`` objects used to raise
  ``NotImplementedError``.  Now they raise ``TypeError``.

- The ``astropy.coordinates.Angle`` class no longer has a ``bounds``
  attribute so there is no bounds-checking or auto-wrapping at this level.
  This allows ``Angle`` objects to be used in arbitrary arithmetic
  expressions (e.g. coordinate distance computation).

- The ``astropy.coordinates.RA`` and ``astropy.coordinates.Dec`` classes have
  been removed and replaced with ``astropy.coordinates.Longitude`` and
  ``astropy.coordinates.Latitude`` respectively.  These are now used for the
  components of Galactic and Horizontal (Alt-Az) coordinates as well instead
  of plain ``Angle`` objects.

- ``astropy.coordinates.angles.rotation_matrix`` and
  ``astropy.coordinates.angles.angle_axis`` now take a ``unit`` kwarg instead
  of ``degrees`` kwarg to specify the units of the angles.
  ``rotation_matrix`` will also take the unit from the given ``Angle`` object
  if no unit is provided.

- The ``AngularSeparation`` class has been removed.  The output of the
  coordinates ``separation()`` method is now an
  ``astropy.coordinates.Angle``.  [#1007]

- The coordinate classes have been renamed in a way that remove the
  ``Coordinates`` at the end of the class names.  E.g., ``ICRSCoordinates``
  from previous versions is now called ``ICRS``. [#1614]

- ``HorizontalCoordinates`` are now named ``AltAz``, to reflect more common
  terminology.

astropy.cosmology
^^^^^^^^^^^^^^^^^

- The Planck (2013) cosmology will likely give slightly different (and more
  accurate) results due to the inclusion of Neutrino masses. [#1364]

- Cosmology class properties now return ``Quantity`` objects instead of
  simple floating-point values. [#1237]

- The names of cosmology instances are now truly optional, and are set to
  ``None`` rather than the name of the class if the user does not provide
  them.  [#1705]

astropy.io.ascii
^^^^^^^^^^^^^^^^

- In the ``read`` method of ``astropy.io.ascii``, empty column values in an
  ASCII table are now treated as missing values instead of the previous
  treatment as a zero-length string "".  This now corresponds to the behavior
  of other table readers like ``numpy.genfromtxt``.  To restore the previous
  behavior set ``fill_values=None`` in the call to ``ascii.read()``. [#919]

- The ``read`` and ``write`` methods of ``astropy.io.ascii`` now have a
  ``format`` argument for specifying the file format.  This is the preferred
  way to choose the format instead of the ``Reader`` and ``Writer``
  arguments. [#961]

- The ``include_names`` and ``exclude_names`` arguments were removed from
  the ``BaseHeader`` initializer, and now instead handled by the reader and
  writer classes directly. [#1350]

- Allow numeric and otherwise unusual column names when reading a table
  where the ``format`` argument is specified, but other format details such
  as the delimiter or quote character are being guessed. [#1692]

- When reading an ASCII table using the ``Table.read()`` method, the default
  has changed from ``guess=False`` to ``guess=True`` to allow auto-detection
  of file format.  This matches the default behavior of ``ascii.read()``.

astropy.io.fits
^^^^^^^^^^^^^^^

- The ``astropy.io.fits.new_table`` function is marked "pending deprecation".
  This does not mean it will be removed outright or that its functionality
  has changed.  It will likely be replaced in the future for a function with
  similar, if not subtly different functionality.  A better, if not slightly
  more verbose approach is to use ``pyfits.FITS_rec.from_columns`` to create
  a new ``FITS_rec`` table--this has the same interface as
  ``pyfits.new_table``.  The difference is that it returns a plan
  ``FITS_rec`` array, and not an HDU instance.  This ``FITS_rec`` object can
  then be used as the data argument in the constructors for ``BinTableHDU``
  (for binary tables) or ``TableHDU`` (for ASCII tables).  This is analogous
  to creating an ``ImageHDU`` by passing in an image array.
  ``pyfits.FITS_rec.from_columns`` is just a simpler way of creating a
  FITS-compatible recarray from a FITS column specification.

- The ``updateHeader``, ``updateHeaderData``, and ``updateCompressedData``
  methods of the ``CompDataHDU`` class are pending deprecation and moved to
  internal methods.  The operation of these methods depended too much on
  internal state to be used safely by users; instead they are invoked
  automatically in the appropriate places when reading/writing compressed
  image HDUs.

- The ``CompDataHDU.compData`` attribute is pending deprecation in favor of
  the clearer and more PEP-8 compatible ``CompDataHDU.compressed_data``.

- The constructor for ``CompDataHDU`` has been changed to accept new keyword
  arguments.  The new keyword arguments are essentially the same, but are in
  underscore_separated format rather than camelCase format.  The old
  arguments are still pending deprecation.

- The internal attributes of HDU classes ``_hdrLoc``, ``_datLoc``, and
  ``_datSpan`` have been replaced with ``_header_offset``, ``_data_offset``,
  and ``_data_size`` respectively.  The old attribute names are still pending
  deprecation.  This should only be of interest to advanced users who have
  created their own HDU subclasses.

- The following previously deprecated functions and methods have been removed
  entirely: ``createCard``, ``createCardFromString``, ``upperKey``,
  ``ColDefs.data``, ``setExtensionNameCaseSensitive``, ``_File.getfile``,
  ``_TableBaseHDU.get_coldefs``, ``Header.has_key``, ``Header.ascardlist``.

- Interfaces that were pending deprecation are now fully deprecated.  These
  include: ``create_card``, ``create_card_from_string``, ``upper_key``,
  ``Header.get_history``, and ``Header.get_comment``.

- The ``.name`` attribute on HDUs is now directly tied to the HDU's header, so
  that if ``.header['EXTNAME']`` changes so does ``.name`` and vice-versa.

astropy.io.registry
^^^^^^^^^^^^^^^^^^^

- Identifier functions for reading/writing Table and NDData objects should
  now accept ``(origin, *args, **kwargs)`` instead of ``(origin, args,
  kwargs)``. [#591]

- Added a new ``astropy.io.registry.get_formats`` function for listing
  registered I/O formats and details about the their readers/writers. [#1669]

astropy.io.votable
^^^^^^^^^^^^^^^^^^

- Added a new option ``use_names_over_ids`` option to use when converting
  from VOTable objects to Astropy Tables. This can prevent a situation where
  column names are not preserved when converting from a VOTable. [#609]

astropy.nddata
^^^^^^^^^^^^^^

- The ``astropy.nddata.convolution`` sub-package has now been moved to
  ``astropy.convolution``, and the ``make_kernel`` function has been removed.
  (the kernel classes should be used instead) [#1451]

astropy.stats.funcs
^^^^^^^^^^^^^^^^^^^

- For ``sigma_clip``, the ``maout`` optional parameter has been removed, and
  the function now always returns a masked array.  A new boolean parameter
  ``copy`` can be used to indicated whether the input data should be copied
  (``copy=True``, default) or used by reference (``copy=False``) in the
  output masked array. [#1083]

astropy.table
^^^^^^^^^^^^^

- The first argument to the ``Column`` and ``MaskedColumn`` classes is now
  the data array--the ``name`` argument has been changed to an optional
  keyword argument. [#840]

- Added support for instantiating a ``Table`` from a list of dict, each one
  representing a single row with the keys mapping to column names. [#901]

- The plural 'units' and 'dtypes' have been switched to 'unit' and 'dtype'
  where appropriate. The original attributes are still present in this
  version as deprecated attributes, but will be removed in the next version.
  [#1174]

- The ``copy`` methods of ``Column`` and ``MaskedColumn`` were changed so
  that the first argument is now ``order='C'``.  This is required for
  compatibility with Numpy 1.8 which is currently in development. [#1250]

- Comparing a column (with == or !=) to a scalar, an array, or another column
  now always returns a boolean Numpy array (which is a masked array if either
  of the arguments in the comparison was masked). This is in contrast to the
  previous behavior, which in some cases returned a boolean Numpy array, and
  in some cases returned a boolean Column object. [#1446]

astropy.time
^^^^^^^^^^^^

- For consistency with ``Quantity``, the attributes ``val`` and
  ``is_scalar`` have been renamed to ``value`` and ``isscalar``,
  respectively, and the attribute ``vals`` has been dropped. [#767]

- The double-float64 internal representation of time is used more
  efficiently to enable better accuracy. [#366]

- Format and scale arguments are now allowed to be case-insensitive. [#1128]

astropy.units
^^^^^^^^^^^^^

- The ``Quantity`` class now inherits from the Numpy array class, and
  includes the following API changes [#929]:

- Using ``float(...)``, ``int(...)``, and ``long(...)`` on a quantity will
  now only work if the quantity is dimensionless and unscaled.

- All Numpy ufuncs should now treat units correctly (or raise an exception
  if not supported), rather than extract the value of quantities and
  operate on this, emitting a warning about the implicit loss of units.

- When using relevant Numpy ufuncs on dimensionless quantities (e.g.
  ``np.exp(h * nu / (k_B * T))``), or combining dimensionless quantities
  with Python scalars or plain Numpy arrays ``1 + v / c``, the
  dimensionless Quantity will automatically be converted to an unscaled
  dimensionless Quantity.

- When initializing a quantity from a value with no unit, it is now set to
  be dimensionless and unscaled by default. When initializing a Quantity
  from another Quantity and with no unit specified in the initializer, the
  unit is now taken from the unit of the Quantity being initialized from.

- Strings are no longer allowed as the values for Quantities. [#1005]

- Quantities are always comparable with zero regardless of their units.
  [#1254]

- The exception ``astropy.units.UnitsException`` has been renamed to
  ``astropy.units.UnitsError`` to be more consistent with the naming
  of built-in Python exceptions. [#1406]

- Multiplication with and division by a string now always returns a Unit
  (rather than a Quantity when the string was first) [#1408]

- Imperial units are disabled by default.

astropy.wcs
^^^^^^^^^^^

- For those including the ``astropy.wcs`` C headers in their project, they
  should now include it as:

  #include "astropy_wcs/astropy_wcs_api.h"

  instead of:

  #include "astropy_wcs_api.h"

  [#1631]

- The ``--enable-legacy`` option for ``setup.py`` has been removed. [#1493]

Bug Fixes
---------

astropy.io.ascii
^^^^^^^^^^^^^^^^

- The ``write()`` function was ignoring the ``fill_values`` argument. [#910]

- Fixed an issue in ``DefaultSplitter.join`` where the delimiter attribute
  was ignored when writing the CSV. [#1020]

- Fixed writing of IPAC tables containing null values. [#1366]

- When a table with no header row was read without specifying the format and
  using the ``names`` argument, then the first row could be dropped. [#1692]

astropy.io.fits
^^^^^^^^^^^^^^^

- Binary tables containing compressed images may, optionally, contain other
  columns unrelated to the tile compression convention. Although this is an
  uncommon use case, it is permitted by the standard.

- Reworked some of the file I/O routines to allow simpler, more consistent
  mapping between OS-level file modes ('rb', 'wb', 'ab', etc.) and the more
  "PyFITS-specific" modes used by PyFITS like "readonly" and "update".  That
  is, if reading a FITS file from an open file object, it doesn't matter as
  much what "mode" it was opened in so long as it has the right capabilities
  (read/write/etc.)  Also works around bugs in the Python io module in 2.6+
  with regard to file modes.

- Fixed a long-standing issue where writing binary tables did not correctly
  write the TFORMn keywords for variable-length array columns (they omitted
  the max array length parameter of the format).  This was thought fixed in
  an earlier version, but it was only fixed for compressed image HDUs and
  not for binary tables in general.

astropy.nddata
^^^^^^^^^^^^^^

- Fixed crash when trying to multiple or divide ``NDData`` objects with
  uncertainties. [#1547]

astropy.table
^^^^^^^^^^^^^

- Using a list of strings to index a table now correctly returns a new table
  with the columns named in the list. [#1454]

- Inequality operators now work properly with ``Column`` objects. [#1685]

astropy.time
^^^^^^^^^^^^

- ``Time`` scale and format attributes are now shown when calling ``dir()``
  on a ``Time`` object. [#1130]

astropy.wcs
^^^^^^^^^^^

- Fixed assignment to string-like WCS attributes on Python 3. [#956]

astropy.units
^^^^^^^^^^^^^

- Fixed a bug that caused the order of multiplication/division of plain
  Numpy arrays with Quantities to matter (i.e. if the plain array comes
  first the units were not preserved in the output). [#899]

- Directly instantiated ``CompositeUnits`` were made printable without
  crashing. [#1576]

Misc
^^^^

- Fixed various modules that hard-coded ``sys.stdout`` as default arguments
  to functions at import time, rather than using the runtime value of
  ``sys.stdout``. [#1648]

- Minor documentation fixes and enhancements [#922, #1034, #1210, #1217,
  #1491, #1492, #1498, #1582, #1608, #1621, #1646, #1670, #1756]

- Fixed a crash that could sometimes occur when running the test suite on
  systems with platform names containing non-ASCII characters. [#1698]

Other Changes and Additions
---------------------------

- General

- Astropy now follows the PSF Code of Conduct. [#1216]

- Astropy's test suite now tests all doctests in inline docstrings.  Support
  for running doctests in the reST documentation is planned to follow in
  v0.3.1.

- Astropy's test suite can be run on multiple CPUs in parallel, often
  greatly improving runtime, using the ``--parallel`` option. [#1040]

- A warning is now issued when using Astropy with Numpy < 1.5--much of
  Astropy may still work in this case but it shouldn't be expected to
  either. [#1479]

- Added automatic download/build/installation of Numpy during Astropy
  installation if not already found. [#1483]

- Handling of metadata for the ``NDData`` and ``Table`` classes has been
  unified by way of a common ``MetaData`` descriptor--it allows instantiating
  an object with metadata of any mapping type, and subsequently prevents
  replacing the mapping stored in the ``.meta`` attribute (only direct
  updates to that object are allowed). [#1686]

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- Angles containing out of bounds minutes or seconds (e.g. 60) can be
  parsed--the value modulo 60 is used with carry to the hours/minutes, and a
  warning is issued rather than raising an exception. [#990]

astropy.io.fits
^^^^^^^^^^^^^^^

- The new compression code also adds support for the ZQUANTIZ and ZDITHER0
  keywords added in more recent versions of this FITS Tile Compression spec.
  This includes support for lossless compression with GZIP. (#198) By default
  no dithering is used, but the ``SUBTRACTIVE_DITHER_1`` and
  ``SUBTRACTIVE_DITHER_2`` methods can be enabled by passing the correct
  constants to the ``quantize_method`` argument to the ``CompImageHDU``
  constructor.  A seed can be manually specified, or automatically generated
  using either the system clock or checksum-based methods via the
  ``dither_seed`` argument.  See the documentation for ``CompImageHDU`` for
  more details.

- Images compressed with the Tile Compression standard can now be larger than
  4 GB through support of the Q format.

- All HDUs now have a ``.ver`` ``.level`` attribute that returns the value of
  the EXTVAL and EXTLEVEL keywords from that HDU's header, if the exist.
  This was added for consistency with the ``.name`` attribute which returns
  the EXTNAME value from the header.

- Then ``Column`` and ``ColDefs`` classes have new ``.dtype`` attributes
  which give the Numpy dtype for the column data in the first case, and the
  full Numpy compound dtype for each table row in the latter case.

- There was an issue where new tables created defaulted the values in all
  string columns to '0.0'.  Now string columns are filled with empty strings
  by default--this seems a less surprising default, but it may cause
  differences with tables created with older versions of PyFITS or Astropy.

astropy.io.misc
^^^^^^^^^^^^^^^

- The HDF5 reader can now refer to groups in the path as well as datasets;
  if given a group, the first dataset in that group is read. [#1159]

astropy.nddata
^^^^^^^^^^^^^^

- ``NDData`` objects have more helpful, though still rudimentary ``__str__`
  and ``__repr__`` displays. [#1313]

astropy.units
^^^^^^^^^^^^^

- Added 'cycle' unit. [#1160]

- Extended units supported by the CDS formatter/parser. [#1468]

- Added unicode an LaTeX symbols for liter. [#1618]

astropy.wcs
^^^^^^^^^^^

- Redundant SCAMP distortion parameters are removed with SIP distortions are
  also present. [#1278]

- Added iterative implementation of ``all_world2pix`` that can be reliably
  inverted. [#1281]


0.2.5 (2013-10-25)
==================

Bug Fixes
---------

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- Fixed incorrect string formatting of Angles using ``precision=0``. [#1319]

- Fixed string formatting of Angles using ``decimal=True`` which ignored the
  ``precision`` argument. [#1323]

- Fixed parsing of format strings using appropriate unicode characters
  instead of the ASCII ``-`` for minus signs. [#1429]

astropy.io.ascii
^^^^^^^^^^^^^^^^

- Fixed a crash in the IPAC table reader when the ``include/exclude_names``
  option is set. [#1348]

- Fixed writing AASTex tables to honor the ``tabletype`` option. [#1372]

astropy.io.fits
^^^^^^^^^^^^^^^

- Improved round-tripping and preservation of manually assigned column
  attributes (``TNULLn``, ``TSCALn``, etc.) in table HDU headers. (Note: This
  issue was previously reported as fixed in Astropy v0.2.2 by mistake; it is
  not fixed until v0.3.) [#996]

- Fixed a bug that could cause a segfault when trying to decompress an
  compressed HDU whose contents are truncated (due to a corrupt file, for
  example). This still causes a Python traceback but better that than a
  segfault. [#1332]

- Newly created ``CompImageHDU`` HDUs use the correct value of the
  ``DEFAULT_COMPRESSION_TYPE`` module-level constant instead of hard-coding
  "RICE_1" in the header.

- Fixed a corner case where when extra memory is allocated to compress an
  image, it could lead to unnecessary in-memory copying of the compressed
  image data and a possible memory leak through Numpy.

- Fixed a bug where assigning from an mmap'd array in one FITS file over
  the old (also mmap'd) array in another FITS file failed to update the
  destination file. Corresponds to PyFITS issue 25.

- Some miscellaneous documentation fixes.

astropy.io.votable
^^^^^^^^^^^^^^^^^^

- Added a warning for when a VOTable 1.2 file contains no ``RESOURCES``
  elements (at least one should be present). [#1337]

- Fixed a test failure specific to MIPS architecture caused by an errant
  floating point warning. [#1179]

astropy.nddata.convolution
^^^^^^^^^^^^^^^^^^^^^^^^^^

- Prevented in-place modification of the input arrays to ``convolve()``.
  [#1153]

astropy.table
^^^^^^^^^^^^^

- Added HTML escaping for string values in tables when outputting the table
  as HTML. [#1347]

- Added a workaround in a bug in Numpy that could cause a crash when
  accessing a table row in a masked table containing ``dtype=object``
  columns. [#1229]

- Fixed an issue similar to the one in #1229, but specific to unmasked
  tables. [#1403]

astropy.units
^^^^^^^^^^^^^

- Improved error handling for unparsable units and fixed parsing CDS units
  without mantissas in the exponent. [#1288]

- Added a physical type for spectral flux density. [#1410]

- Normalized conversions that should result in a scale of exactly 1.0 to
  round off slight floating point imprecisions. [#1407]

- Added support in the CDS unit parser/formatter for unusual unit prefixes
  that are nonetheless required to be supported by that convention. [#1426]

- Fixed the parsing of ``sqrt()`` in unit format strings which was returning
  ``unit ** 2`` instead of ``unit ** 0.5``. [#1458]

astropy.wcs
^^^^^^^^^^^

- When passing a single array to the wcs transformation functions,
  (``astropy.wcs.Wcs.all_pix2world``, etc.), its second dimension must now
  exactly match the number of dimensions in the transformation. [#1395]

- Improved error message when incorrect arguments are passed to
  ``WCS.wcs_world2pix``. [#1394]

- Fixed a crash when trying to read WCS from FITS headers on Python 3.3
  in Windows. [#1363]

- Only headers that are required as part of the WCSLIB C API are installed
  by the package, per request of system packagers. [#1666]

Misc
^^^^

- Fixed crash when the ``COLUMNS`` environment variable is set to a
  non-integer value. [#1291]

- Fixed a bug in ``ProgressBar.map`` where ``multiprocess=True`` could cause
  it to hang on waiting for the process pool to be destroyed. [#1381]

- Fixed a crash on Python 3.2 when affiliated packages try to use the
  ``astropy.utils.data.get_pkg_data_*`` functions. [#1256]

- Fixed a minor path normalization issue that could occur on Windows in
  ``astropy.utils.data.get_pkg_data_filename``. [#1444]

- Fixed an annoyance where configuration items intended only for testing
  showed up in users' astropy.cfg files. [#1477]

- Prevented crashes in exception logging in unusual cases where no traceback
  is associated with the exception. [#1518]

- Fixed a crash when running the tests in unusual environments where
  ``sys.stdout.encoding`` is ``None``. [#1530]

- Miscellaneous documentation fixes and improvements [#1308, #1317, #1377,
  #1393, #1362, #1516]

Other Changes and Additions
---------------------------

- Astropy installation now requests setuptools >= 0.7 during build/installation
  if neither distribute or setuptools >= 0.7 is already installed.  In other
  words, if ``import setuptools`` fails, ``ez_setup.py`` is used to bootstrap
  the latest setuptools (rather than using ``distribute_setup.py`` to bootstrap
  the now obsolete distribute package). [#1197]

- When importing Astropy from a source checkout without having built the
  extension modules first an ``ImportError`` is raised rather than a
  ``SystemExit`` exception. [#1269]


0.2.4 (2013-07-24)
==================

Bug Fixes
---------

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- Fixed the angle parser to support parsing the string "1 degree". [#1168]

astropy.cosmology
^^^^^^^^^^^^^^^^^

- Fixed a crash in the ``comoving_volume`` method on non-flat cosmologies
  when passing it an array of redshifts.

astropy.io.ascii
^^^^^^^^^^^^^^^^

- Fixed a bug that prevented saving changes to the comment symbol when
  writing changes to a table. [#1167]

astropy.io.fits
^^^^^^^^^^^^^^^

- Added a workaround for a bug in 64-bit OSX that could cause truncation when
  writing files greater than 2^32 bytes in size. [#839]

astropy.io.votable
^^^^^^^^^^^^^^^^^^

- Fixed incorrect reading of tables containing multiple ``<RESOURCE>``
  elements. [#1223]

astropy.table
^^^^^^^^^^^^^

- Fixed a bug where ``Table.remove_column`` and ``Table.rename_column``
  could cause a masked table to lose its masking. [#1120]

- Fixed bugs where subclasses of ``Table`` did not preserver their class in
  certain operations. [#1142]

- Fixed a bug where slicing a masked table did not preserve the mask. [#1187]

astropy.units
^^^^^^^^^^^^^

- Fixed a bug where the ``.si`` and ``.cgs`` properties of dimensionless
  ``Quantity`` objects raised a ``ZeroDivisionError``. [#1150]

- Fixed a bug where multiple subsequent calls to the ``.decompose()`` method
  on array quantities applied a scale factor each time. [#1163]

Misc
^^^^

- Fixed an installation crash that could occur sometimes on Debian/Ubuntu
  and other \*NIX systems where ``pkg_resources`` can be installed without
  installing ``setuptools``. [#1150]

- Updated the ``distribute_setup.py`` bootstrapper to use setuptools >= 0.7
  when installing on systems that don't already have an up to date version
  of distribute/setuptools. [#1180]

- Changed the ``version.py`` template so that Astropy affiliated packages can
  (and they should) use their own ``cython_version.py`` and
  ``utils._compiler`` modules where appropriate. This issue only pertains to
  affiliated package maintainers. [#1198]

- Fixed a corner case where the default config file generation could crash
  if building with matplotlib but *not* Sphinx installed in a virtualenv.
  [#1225]

- Fixed a crash that could occur in the logging module on systems that
  don't have a default preferred encoding (in particular this happened
  in some versions of PyCharm). [#1244]

- The Astropy log now supports passing non-string objects (and calling
  ``str()`` on them by default) to the logging methods, in line with Python's
  standard logging API. [#1267]

- Minor documentation fixes [#582, #696, #1154, #1194, #1212, #1213, #1246,
  #1252]

Other Changes and Additions
---------------------------

astropy.cosmology
^^^^^^^^^^^^^^^^^

- Added a new ``Plank13`` object representing the Plank 2013 results. [#895]

astropy.units
^^^^^^^^^^^^^

- Performance improvements in initialization of ``Quantity`` objects with
  a large number of elements. [#1231]


0.2.3 (2013-05-30)
==================

Bug Fixes
---------

astropy.time
^^^^^^^^^^^^

- Fixed inaccurate handling of leap seconds when converting from UTC to UNIX
  timestamps. [#1118]

- Tightened required accuracy in many of the time conversion tests. [#1121]

Misc
^^^^

- Fixed a regression that was introduced in v0.2.2 by the fix to issue #992
  that was preventing installation of Astropy affiliated packages that use
  Astropy's setup framework. [#1124]


0.2.2 (2013-05-21)
==================

Bug Fixes
---------

astropy.io
^^^^^^^^^^

- Fixed issues in both the ``fits`` and ``votable`` sub-packages where array
  byte order was not being handled consistently, leading to possible crashes
  especially on big-endian systems. [#1003]

astropy.io.fits
^^^^^^^^^^^^^^^

- When an error occurs opening a file in fitsdiff the exception message will
  now at least mention which file had the error.

- Fixed a couple cases where creating a new table using TDIMn in some of the
  columns could cause a crash.

- Slightly refactored how tables containing variable-length array columns are
  handled to add two improvements: Fixes an issue where accessing the data
  after a call to the ``astropy.io.fits.getdata`` convenience function caused
  an exception, and allows the VLA data to be read from an existing mmap of
  the FITS file.

- Fixed a bug on Python 3 where attempting to open a non-existent file on
  Python 3 caused a seemingly unrelated traceback.

- Fixed an issue in the tests that caused some tests to fail if Astropy is
  installed with read-only permissions.

- Fixed a bug where instantiating a ``BinTableHDU`` from a numpy array
  containing boolean fields converted all the values to ``False``.

- Fixed an issue where passing an array of integers into the constructor of
  ``Column()`` when the column type is floats of the same byte width caused
  the column array to become garbled.

- Fixed inconsistent behavior in creating CONTINUE cards from byte strings
  versus unicode strings in Python 2--CONTINUE cards can now be created
  properly from unicode strings (so long as they are convertible to ASCII).

- Fixed a bug in parsing HIERARCH keywords that do not have a space after the
  first equals sign (before the value).

- Prevented extra leading whitespace on HIERARCH keywords from being treated
  as part of the keyword.

- Fixed a bug where HIERARCH keywords containing lower-case letters was
  mistakenly marked as invalid during header validation along with an
  ancillary issue where the ``Header.index()`` method id not work correctly
  with HIERARCH keywords containing lower-case letters.

- Disallowed assigning NaN and Inf floating point values as header values,
  since the FITS standard does not define a way to represent them in. Because
  this is undefined, the previous behavior did not make sense and produced
  invalid FITS files. [#954]

- Fixed an obscure issue that can occur on systems that don't have flush to
  memory-mapped files implemented (namely GNU Hurd). [#968]

astropy.io.votable
^^^^^^^^^^^^^^^^^^

- Stopped deprecation warnings from the ``astropy.io.votable`` package that
  could occur during setup. [#970]

- Fixed an issue where INFO elements were being incorrectly dropped when
  occurring inside a TABLE element. [#1000]

- Fixed obscure test failures on MIPS platforms. [#1010]

astropy.nddata.convolution
^^^^^^^^^^^^^^^^^^^^^^^^^^

- Fixed an issue in ``make_kernel()`` when using an Airy function kernel.
  Also removed the superfluous 'brickwall' option. [#939]

astropy.table
^^^^^^^^^^^^^

- Fixed a crash that could occur when adding a row to an empty (rowless)
  table with masked columns. [#973]

- Made it possible to assign to one table row from the value of another row,
  effectively making it easier to copy rows, for example. [#1019]

astropy.time
^^^^^^^^^^^^

- Added appropriate ``__copy__`` and ``__deepcopy__`` behavior; this
  omission caused a seemingly unrelated error in FK5 coordinate separation.
  [#891]

astropy.units
^^^^^^^^^^^^^

- Fixed an issue where the ``isiterable()`` utility returned ``True`` for
  quantities with scalar values.  Added an ``__iter__`` method for the
  ``Quantity`` class and fixed ``isiterable()`` to catch false positives.
  [#878]

- Fixed previously undefined behavior when multiplying a unit by a string.
  [#949]

- Added 'time' as a physical type--this was a simple omission. [#959]

- Fixed issues with pickling unit objects so as to play nicer with the
  multiprocessing module. [#974]

- Made it more difficult to accidentally override existing units with a new
  unit of the same name. [#1070]

- Added several more physical types and units that were previously omitted,
  including 'mass density', 'specific volume', 'molar volume', 'momentum',
  'angular momentum', 'angular speed', 'angular acceleration', 'electric
  current', 'electric current density', 'electric field strength', 'electric
  flux density', 'electric charge density', 'permittivity', 'electromagnetic
  field strength', 'radiant intensity', 'data quantity', 'bandwidth'; and
  'knots', 'nautical miles', 'becquerels', and 'curies' respectively. [#1072]

Misc
^^^^

- Fixed a permission error that could occur when running ``astropy.test()``
  on Python 3 when Astropy is installed as root. [#811]

- Made it easier to filter warnings from the ``convolve()`` function and
  from ``Quantity`` objects. [#853]

- Fixed a crash that could occur in Python 3 when generation of the default
  config file fails during setup. [#952]

- Fixed an unrelated error message that could occur when trying to import
  astropy from a source checkout without having build the extension modules
  first. This issue was claimed to be fixed in v0.2.1, but the fix itself had
  a bug. [#971]

- Fixed a crash that could occur when running the ``build_sphinx`` setup
  command in Python 3. [#977]

- Added a more helpful error message when trying to run the
  ``setup.py build_sphinx`` command when Sphinx is not installed. [#1027]

- Minor documentation fixes and restructuring.
  [#935, #967, #978, #1004, #1028, #1047]

Other Changes and Additions
---------------------------

- Some performance improvements to the ``astropy.units`` package, in particular
  improving the time it takes to import the sub-package. [#1015]


0.2.1 (2013-04-03)
==================

Bug Fixes
---------

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- Fixed encoding errors that could occur when formatting coordinate objects
  in code using ``from __future__ import unicode_literals``. [#817]

- Fixed a bug where the minus sign was dropped when string formatting dms
  coordinates with -0 degrees. [#875]

astropy.io.fits
^^^^^^^^^^^^^^^

- Properly supports the ZQUANTIZ keyword used to support quantization
  level--this includes working support for lossless GZIP compression of
  images.

- Fixed support for opening gzipped FITS files in a writeable mode. [#256]

- Added a more helpful exception message when trying to read invalid values
  from a table when the required ``TNULLn`` keyword is missing. [#309]

- More refactoring of the tile compression handling to work around a
  potential memory access violation that was particularly prevalent on
  Windows. [#507]

- Fixed an integer size mismatch in the compression module that could affect
  32-bit systems. [#786]

- Fixed malformatting of the ``TFORMn`` keywords when writing compressed
  image tables (they omitted the max array length parameter from the
  variable-length array format).

- Fixed a crash that could occur when writing a table containing multi-
  dimensional array columns from an existing file into a new file.

- Fixed a bug in fitsdiff that reported two header keywords containing NaN
  as having different values.

astropy.io.votable
^^^^^^^^^^^^^^^^^^

- Fixed links to the ``astropy.io.votable`` documentation in the VOTable
  validator output. [#806]

- When reading VOTables containing integers that are out of range for their
  column type, display a warning rather than raising an exception. [#825]

- Changed the default string format for floating point values for better
  round-tripping. [#856]

- Fixed opening VOTables through the ``Table.read()`` interface for tables
  that have no names. [#927]

- Fixed creation of VOTables from an Astropy table that does not have a data
  mask. [#928]

- Minor documentation fixes. [#932]

astropy.nddata.convolution
^^^^^^^^^^^^^^^^^^^^^^^^^^

- Added better handling of ``inf`` values to the ``convolve_fft`` family of
  functions. [#893]

astropy.table
^^^^^^^^^^^^^

- Fixed silent failure to assign values to a row on multiple columns. [#764]

- Fixed various buggy behavior when viewing a table after sorting by one of
  its columns. [#829]

- Fixed using ``numpy.where()`` with table indexing. [#838]

- Fixed a bug where opening a remote table with ``Table.read()`` could cause
  the entire table to be downloaded twice. [#845]

- Fixed a bug where ``MaskedColumn`` no longer worked if the column being
  masked is renamed. [#916]

astropy.units
^^^^^^^^^^^^^

- Added missing capability for array ``Quantity``\s to be initializable by
  a list of ``Quantity``\s. [#835]

- Fixed the definition of year and lightyear to be in terms of Julian year
  per the IAU definition. [#861]

- "degree" was removed from the list of SI base units. [#863]

astropy.wcs
^^^^^^^^^^^

- Fixed ``TypeError`` when calling ``WCS.to_header_string()``. [#822]

- Added new method ``WCS.all_world2pix`` for converting from world
  coordinates to pixel space, including inversion of the astrometric
  distortion correction. [#1066, #1281]

Misc
^^^^

- Fixed a minor issue when installing with ``./setup.py develop`` on a fresh
  git clone.  This is likely only of interest to developers on Astropy.
  [#725]

- Fixes a crash with ``ImportError: No module named 'astropy.version'`` when
  running setup.py from a source checkout for the first time on OSX with
  Python 3.3. [#820]

- Fixed an installation issue where running ``./setup.py install`` or when
  installing with pip the ``.astropy`` directory gets created in the home
  directory of the user running the command.  The user's ``.astropy``
  directory should only be created when they use Astropy, not when they
  install it. [#867]

- Fixed an exception when creating a ``ProgressBar`` with a "total" of 0.
  [#752]

- Added better documentation of behavior that can occur when trying to import
  the astropy package from within a source checkout without first building
  the extension modules. [#795, #864]

- Added link to the installation instructions in the README. [#797]

- Catches segfaults in xmllint which can occur sometimes and is otherwise out
  of our control. [#803]

- Minor changes to the documentation template. [#805]

- Fixed a minor exception handling bug in ``download_file()``. [#808]

- Added cleanup of any temporary files if an error occurs in
  ``download_file()``. [#857]

- Filesystem free space is checked for before attempting to download a file
  with ``download_file()``. [#858]

- Fixed package data locating to work across symlinks--required to work with
  some OS packaging layouts. [#827]

- Fixed a bug when building Cython extensions where hidden files containing
  ``.pyx`` extensions could cause the build to crash. This can be an issue
  with software and filesystems that autogenerate hidden files. [#834]

- Fixed bug that could cause a "script" called README.rst to be installed
  in a bin directory. [#852]

- Fixed some miscellaneous and mostly rare reference leaks caught by
  cpychecker. [#914]

Other Changes and Additions
---------------------------

- Added logo and branding for Windows binary installers. [#741]

- Upgraded included version libexpat to 2.1.0. [#781]

- ~25% performance improvement in unit composition/decomposition. [#836]

- Added previously missing LaTeX formatting for ``L_sun`` and ``R_sun``. [#841]

- ConfigurationItem\s now have a more useful and informative __repr__
  and improved documentation for how to use them. [#855]

- Added a friendlier error message when trying to import astropy from a source
  checkout without first building the extension modules inplace. [#864]

- py.test now outputs more system information for help in debugging issues
  from users. [#869]

- Added unit definitions "mas" and "uas" for "milliarcsecond" and
  "microarcsecond" respectively. [#892]


0.2 (2013-02-19)
================

New Features
------------

astropy.coordinates
^^^^^^^^^^^^^^^^^^^

- This new subpackage contains a representation of celestial coordinates,
  and provides a wide range of related functionality.  While
  fully-functional, it is a work in progress and parts of the API may
  change in subsequent releases.

astropy.cosmology
^^^^^^^^^^^^^^^^^

- Update to include cosmologies with variable dark energy equations of state.
  (This introduces some API incompatibilities with the older Cosmology
  objects).

- Added parameters for relativistic species (photons, neutrinos) to the
  astropy.cosmology classes. The current treatment assumes that neutrinos are
  massless. [#365]

- Add a WMAP9 object using the final (9-year) WMAP parameters from
  Hinshaw et al. 2013. It has also been made the default cosmology.
  [#629, #724]

- astropy.table I/O infrastructure for custom readers/writers
  implemented. [#305]

- Added support for reading/writing HDF5 files [#461]

- Added support for masked tables with missing or invalid data [#451]

- New ``astropy.time`` sub-package. [#332]

- New ``astropy.units`` sub-package that includes a class for units
  (``astropy.units.Unit``) and scalar quantities that have units
  (``astropy.units.Quantity``). [#370, #445]

  This has the following effects on other sub-packages:

- In ``astropy.wcs``, the ``wcs.cunit`` list now takes and returns
  ``astropy.units.Unit`` objects. [#379]

- In ``astropy.nddata``, units are now stored as ``astropy.units.Unit``
  objects. [#382]

- In ``astropy.table``, units on columns are now stored as
  ``astropy.units.Unit`` objects. [#380]

- In ``astropy.constants``, constants are now stored as
  ``astropy.units.Quantity`` objects. [#529]

astropy.io.ascii
^^^^^^^^^^^^^^^^

- Improved integration with the ``astropy.table`` Table class so that
  table and column metadata (e.g. keywords, units, description,
  formatting) are directly available in the output table object.  The
  CDS, DAOphot, and IPAC format readers now provide this type of
  integrated metadata.

- Changed to using ``astropy.table`` masked tables instead of NumPy
  masked arrays for tables with missing values.

- Added SExtractor table reader to ``astropy.io.ascii`` [#420]

- Removed the Memory reader class which was used to convert data input
  passed to the ``write`` function into an internal table.  Instead
  ``write`` instantiates an astropy Table object using the data
  input to ``write``.

- Removed the NumpyOutputter as the output of reading a table is now
  always a ``Table`` object.

- Removed the option of supplying a function as a column output
  formatter.

- Added a new ``strip_whitespace`` keyword argument to the ``write``
  function.  This controls whether whitespace is stripped from
  the left and right sides of table elements before writing.
  Default is True.

- Fixed a bug in reading IPAC tables with null values.

- Generalized I/O infrastructure so that ``astropy.nddata`` can also have
  custom readers/writers [#659]

astropy.wcs
^^^^^^^^^^^

- From updating the underlying wcslib 4.16:

- When ``astropy.wcs.WCS`` constructs a default coordinate representation
  it will give it the special name "DEFAULTS", and will not report "Found
  one coordinate representation".

Other Changes and Additions
---------------------------

- A configuration file with all options set to their defaults is now generated
  when astropy is installed.  This file will be pulled in as the users'
  astropy configuration file the first time they ``import astropy``.  [#498]

- Astropy doc themes moved into ``astropy.sphinx`` to allow affiliated packages
  to access them.

- Added expanded documentation for the ``astropy.cosmology`` sub-package.
  [#272]

- Added option to disable building of "legacy" packages (pyfits, vo, etc.).

- The value of the astronomical unit (au) has been updated to that adopted by
  IAU 2012 Resolution B2, and the values of the pc and kpc constants have been
  updated to reflect this. [#368]

- Added links to the documentation pages to directly edit the documentation on
  GitHub. [#347]

- Several updates merged from ``pywcs`` into ``astropy.wcs`` [#384]:

- Improved the reading of distortion images.

- Added a new option to choose whether or not to write SIP coefficients.

- Uses the ``relax`` option by default so that non-standard keywords are
  allowed. [#585]


- Added HTML representation of tables in IPython notebook [#409]

- Rewrote CFITSIO-based backend for handling tile compression of FITS files.
  It now uses a standard CFITSIO instead of heavily modified pieces of CFITSIO
  as before.  Astropy ships with its own copy of CFITSIO v3.30, but system
  packagers may choose instead to strip this out in favor of a
  system-installed version of CFITSIO.  This corresponds to PyFITS ticket 169.
  [#318]

- Moved ``astropy.config.data`` to ``astropy.utils.data`` and re-factored the
  I/O routines to separate out the generic I/O code that can be used to open
  any file or resource from the code used to access Astropy-related data. The
  'core' I/O routine is now ``get_readable_fileobj``, which can be used to
  access any local as well as remote data, supports caching, and can decompress
  gzip and bzip2 files on-the-fly. [#425]

- Added a classmethod to
  ``astropy.coordinates.coordsystems.SphericalCoordinatesBase`` that performs a
  name resolve query using Sesame to retrieve coordinates for the requested
  object. This works for any subclass of ``SphericalCoordinatesBase``, but
  requires an internet connection. [#556]

- astropy.nddata.convolution removed requirement of PyFFTW3; uses Numpy's
  FFT by default instead with the added ability to specify an FFT
  implementation to use. [#660]


Bug Fixes
---------

astropy.io.ascii
^^^^^^^^^^^^^^^^

- Fixed crash when pprinting a row with INDEF values. [#511]

- Fixed failure when reading DAOphot files with empty keyword values. [#666]

astropy.io.fits
^^^^^^^^^^^^^^^

- Improved handling of scaled images and pseudo-unsigned integer images in
  compressed image HDUs.  They now work more transparently like normal image
  HDUs with support for the ``do_not_scale_image_data`` and ``uint`` options,
  as well as ``scale_back`` and ``save_backup``.  The ``.scale()`` method
  works better too. Corresponds to PyFITS ticket 88.

- Permits non-string values for the EXTNAME keyword when reading in a file,
  rather than throwing an exception due to the malformatting.  Added
  verification for the format of the EXTNAME keyword when writing.
  Corresponds to PyFITS ticket 96.

- Added support for EXTNAME and EXTVER in PRIMARY HDUs.  That is, if EXTNAME
  is specified in the header, it will also be reflected in the ``.name``
  attribute and in ``fits.info()``.  These keywords used to be verboten in
  PRIMARY HDUs, but the latest version of the FITS standard allows them.
  Corresponds to PyFITS ticket 151.

- HCOMPRESS can again be used to compress data cubes (and higher-dimensional
  arrays) so long as the tile size is effectively 2-dimensional. In fact,
  compatible tile sizes will automatically be used even if they're not
  explicitly specified. Corresponds to PyFITS ticket 171.

- Fixed a bug that could cause a deadlock in the filesystem on OSX when
  reading the data from certain types of FITS files. This only occurred
  when used in conjunction with Numpy 1.7. [#369]

- Added support for the optional ``endcard`` parameter in the
  ``Header.fromtextfile()`` and ``Header.totextfile()`` methods.  Although
  ``endcard=False`` was a reasonable default assumption, there are still text
  dumps of FITS headers that include the END card, so this should have been
  more flexible. Corresponds to PyFITS ticket 176.

- Fixed a crash when running fitsdiff on two empty (that is, zero row) tables.
  Corresponds to PyFITS ticket 178.

- Fixed an issue where opening a FITS file containing a random group HDU in
  update mode could result in an unnecessary rewriting of the file even if
  no changes were made. This corresponds to PyFITS ticket 179.

- Fixed a crash when generating diff reports from diffs using the
  ``ignore_comments`` options. Corresponds to PyFITS ticket 181.

- Fixed some bugs with WCS distortion paper record-valued keyword cards:

- Cards that looked kind of like RVKCs but were not intended to be were
  over-permissively treated as such--commentary keywords like COMMENT and
  HISTORY were particularly affected. Corresponds to PyFITS ticket 183.

- Looking up a card in a header by its standard FITS keyword only should
  always return the raw value of that card.  That way cards containing
  values that happen to valid RVKCs but were not intended to be will still
  be treated like normal cards. Corresponds to PyFITS ticket 184.

- Looking up a RVKC in a header with only part of the field-specifier (for
  example "DP1.AXIS" instead of "DP1.AXIS.1") was implicitly treated as a
  wildcard lookup. Corresponds to PyFITS ticket 184.

- Fixed a crash when diffing two FITS files where at least one contains a
  compressed image HDU which was not recognized as an image instead of a
  table. Corresponds to PyFITS ticket 187.

- Fixed a bug where opening a file containing compressed image HDUs in
  'update' mode and then immediately closing it without making any changes
  caused the file to be rewritten unnecessarily.

- Fixed two memory leaks that could occur when writing compressed image data,
  or in some cases when opening files containing compressed image HDUs in
  'update' mode.

- Fixed a bug where ``ImageHDU.scale(option='old')`` wasn't working at
  all--it was not restoring the image to its original BSCALE and BZERO
  values.

- Fixed a bug when writing out files containing zero-width table columns,
  where the TFIELDS keyword would be updated incorrectly, leaving the table
  largely unreadable.

- Fixed a minor string formatting issue.

- Fixed bugs in the backwards compatibility layer for the ``CardList.index``
  and ``CardList.count`` methods. Corresponds to PyFITS ticket 190.

- Improved ``__repr__`` and text file representation of cards with long
  values that are split into CONTINUE cards. Corresponds to PyFITS ticket
  193.

- Fixed a crash when trying to assign a long (> 72 character) value to blank
  ('') keywords. This also changed how blank keywords are represented--there
  are still exactly 8 spaces before any commentary content can begin; this
  *may* affect the exact display of header cards that assumed there could be
  fewer spaces in a blank keyword card before the content begins. However,
  the current approach is more in line with the requirements of the FITS
  standard. Corresponds to PyFITS ticket 194.

astropy.io.votable
^^^^^^^^^^^^^^^^^^

- The ``Table`` class now maintains a single array object which is a
  Numpy masked array.  For variable-length columns, the object that
  is stored there is also a Numpy masked array.

- Changed the ``pedantic`` configuration option to be ``False`` by default
  due to the vast proliferation of non-compliant VO Tables. [#296]

- Renamed ``astropy.io.vo`` to ``astropy.io.votable``.

astropy.table
^^^^^^^^^^^^^

- Added a workaround for an upstream bug in Numpy 1.6.2 that could cause
  a maximum recursion depth RuntimeError when printing table rows. [#341]

astropy.wcs
^^^^^^^^^^^

- Updated to wcslib 4.15 [#418]

- Fixed a problem with handling FITS headers on locales that do not use
  dot as a decimal separator. This required an upstream fix to wcslib which
  is included in wcslib 4.14. [#313]

- Fixed some tests that could fail due to missing/incorrect logging
  configuration--ensures that tests don't have any impact on the default log
  location or contents. [#291]

- Various minor documentation fixes [#293 and others]

- Fixed a bug where running the tests with the ``py.test`` command still tried
  to replace the system-installed pytest with the one bundled with Astropy.
  [#454]

- Improved multiprocessing compatibility for file downloads. [#615]

- Fixed handling of Cython modules when building from a source checkout of a
  tagged release version. [#594]

- Added a workaround for a bug in Sphinx that could occur when using the
  ``:tocdepth:`` directive. [#595]

- Minor VOTable fixes [#596]

- Fixed how ``setup.py`` uses ``distribute_setup.py`` to prevent possible
  ``VersionConflict`` errors when an older version of distribute is already
  installed on the user's system. [#616, #640]

- Changed use of ``log.warn`` in the logging module to ``log.warning`` since
  the former is deprecated. [#624]


0.1 (2012-06-19)
================

- Initial release.
=======
Astropy
=======

|Actions Status| |CircleCI Status| |Azure Status| |Coverage Status| |PyPI Status| |Documentation Status| |Zenodo|

The Astropy Project (http://astropy.org/) is a community effort to develop a
single core package for Astronomy in Python and foster interoperability between
Python astronomy packages. This repository contains the core package which is
intended to contain much of the core functionality and some common tools needed
for performing astronomy and astrophysics with Python.

Releases are `registered on PyPI <https://pypi.org/project/astropy>`_,
and development is occurring at the
`project's GitHub page <http://github.com/astropy/astropy>`_.

For installation instructions, see the `online documentation <https://docs.astropy.org/>`_
or  `docs/install.rst <docs/install.rst>`_ in this source distribution.

Contributing Code, Documentation, or Feedback
---------------------------------------------

The Astropy Project is made both by and for its users, so we welcome and
encourage contributions of many kinds. Our goal is to keep this a positive,
inclusive, successful, and growing community by abiding with the
`Astropy Community Code of Conduct <http://www.astropy.org/about.html#codeofconduct>`_.

More detailed information on contributing to the project or submitting feedback
can be found on the `contributions <http://www.astropy.org/contribute.html>`_
page. A `summary of contribution guidelines <CONTRIBUTING.md>`_ can also be
used as a quick reference when you are ready to start writing or validating
code for submission.

Supporting the Project
----------------------

|NumFOCUS| |Donate|

The Astropy Project is sponsored by NumFOCUS, a 501(c)(3) nonprofit in the
United States. You can donate to the project by using the link above, and this
donation will support our mission to promote sustainable, high-level code base
for the astronomy community, open code development, educational materials, and
reproducible scientific research.

License
-------

Astropy is licensed under a 3-clause BSD style license - see the
`LICENSE.rst <LICENSE.rst>`_ file.

.. |Actions Status| image:: https://github.com/astropy/astropy/workflows/CI/badge.svg
    :target: https://github.com/astropy/astropy/actions
    :alt: Astropy's GitHub Actions CI Status

.. |CircleCI Status| image::  https://img.shields.io/circleci/build/github/astropy/astropy/main?logo=circleci&label=CircleCI
    :target: https://circleci.com/gh/astropy/astropy
    :alt: Astropy's CircleCI Status

.. |Azure Status| image:: https://dev.azure.com/astropy-project/astropy/_apis/build/status/astropy.astropy?repoName=astropy%2Fastropy&branchName=main
    :target: https://dev.azure.com/astropy-project/astropy
    :alt: Astropy's Azure Pipelines Status

.. |Coverage Status| image:: https://codecov.io/gh/astropy/astropy/branch/main/graph/badge.svg
    :target: https://codecov.io/gh/astropy/astropy
    :alt: Astropy's Coverage Status

.. |PyPI Status| image:: https://img.shields.io/pypi/v/astropy.svg
    :target: https://pypi.org/project/astropy
    :alt: Astropy's PyPI Status

.. |Zenodo| image:: https://zenodo.org/badge/DOI/10.5281/zenodo.4670728.svg
   :target: https://doi.org/10.5281/zenodo.4670728
   :alt: Zenodo DOI

.. |Documentation Status| image:: https://img.shields.io/readthedocs/astropy/latest.svg?logo=read%20the%20docs&logoColor=white&label=Docs&version=stable
    :target: https://docs.astropy.org/en/stable/?badge=stable
    :alt: Documentation Status

.. |NumFOCUS| image:: https://img.shields.io/badge/powered%20by-NumFOCUS-orange.svg?style=flat&colorA=E1523D&colorB=007D8A
    :target: http://numfocus.org
    :alt: Powered by NumFOCUS

.. |Donate| image:: https://img.shields.io/badge/Donate-to%20Astropy-brightgreen.svg
    :target: https://numfocus.salsalabs.org/donate-to-astropy/index.html


If you locally cloned this repo before 7 Apr 2021
-------------------------------------------------

The primary branch for this repo has been transitioned from ``master`` to
``main``.  If you have a local clone of this repository and want to keep your
local branch in sync with this repo, you'll need to do the following in your
local clone from your terminal::

   git fetch --all --prune
   # you can stop here if you don't use your local "master"/"main" branch
   git branch -m master main
   git branch -u origin/main main

If you are using a GUI to manage your repos you'll have to find the equivalent
commands as it's different for different programs. Alternatively, you can just
delete your local clone and re-clone!
External Packages/Libraries
===========================

This directory contains C libraries included with Astropy. Note that only C
libraries without python-specific code  should be included in this directory.
Cython or C code intended for use with Astropy or wrapper code should be in
the Astropy source tree.

.. _example-gallery:

Example gallery
===============

This gallery of examples shows a variety of relatively small snippets or
examples of tasks that can be done with the Astropy core package.
Contributions from the community are encouraged!

Longer-form tutorials (or tutorials for
`affiliated packages <http://affiliated.astropy.org>`_) belong at
https://learn.astropy.org (and can be submitted at
`the associated github repository <https://github.com/astropy/astropy-tutorials>`_).
astropy.extern
==============

This sub-package contains third-party Python packages/modules that are
required for some of Astropy's core functionality.  It also contains third-
party JavaScript libraries used for browser-based features.

In particular, this currently includes for Python:

- ConfigObj_: This provides the core config file handling for Astropy's
  configuration system.

- PLY_: This is a parser generator providing lex/yacc-like tools in Python.
  It is used for Astropy's unit parsing and angle/coordinate string parsing.

And for JavaScript:

- jQuery_: This is used currently for the browser-based table viewer feature.

- DataTables_: This is a plug-in for jQuery used also for the browser-based
  table viewer.

Notes for developers
--------------------

jQuery/DataTables
^^^^^^^^^^^^^^^^^

The minified files are the ones that are used in the table viewer feature, but
the non-minified versions are also present in the ``js/`` sub-directory for
packaging reasons. These files must also be distributed, to provide the source
files from which the minified ones can be compiled. This is a requirement for
Linux distributions such as Debian and Fedora.


Notes for third-party packagers
-------------------------------

Packagers preparing Astropy for inclusion in packaging frameworks have
different options for how to handle these third-party extern packages, if they
would prefer to use their system packages rather than the bundled versions.

jQuery/DataTables
^^^^^^^^^^^^^^^^^

Packagers may either use system copies of these JavaScript modules, or require
use of online versions (perhaps via URLs of cloud-hosted versions of these
modules).

It is possible to change the default urls for the remote versions of these
files by using the Astropy
`Configuration system <https://docs.astropy.org/en/stable/config/>`_. The default
configuration file (``$HOME/.astropy/config``) contains a commented section
``[table.jsviewer]`` with two items for jQuery and DataTables. It is also
possible to display the default value and modify it by importing the
configuration module::

    In [1]: from astropy.table.jsviewer import conf

    In [2]: conf.jquery_url
    Out[2]: u'https://code.jquery.com/jquery-1.11.3.min.js'

    In [3]: conf.jquery_url = '...'

Third-party packagers can override the defaults for these configuration items
(by modifying the configuration objects in ``astropy/table/jsviewer.py``, or
provide astropy config files that include the overrides appropriate for the
packaged version.  They would *also* need to set the default
``use_local_files`` option to ``False`` for these settings to be read.


Other
^^^^^

To replace any of the other Python modules included in this package, simply
remove them and update any imports in Astropy to import the system versions
rather than the bundled copies.


.. _ConfigObj: https://github.com/DiffSK/configobj
.. _PLY: http://www.dabeaz.com/ply/
.. _jQuery: http://jquery.com/
.. _DataTables: http://www.datatables.net/
.. _python-warnings:

**********************
Python warnings system
**********************

.. doctest-skip-all

Astropy uses the Python :mod:`warnings` module to issue warning messages.  The
details of using the warnings module are general to Python, and apply to any
Python software that uses this system.  The user can suppress the warnings
using the python command line argument ``-W"ignore"`` when starting an
interactive python session.  For example::

     $ python -W"ignore"

The user may also use the command line argument when running a python script as
follows::

     $ python -W"ignore" myscript.py

It is also possible to suppress warnings from within a python script.  For
instance, the warnings issued from a single call to the
`astropy.io.fits.writeto` function may be suppressed from within a Python
script using the `warnings.filterwarnings` function as follows::

     >>> import warnings
     >>> from astropy.io import fits
     >>> warnings.filterwarnings('ignore', category=UserWarning, append=True)
     >>> fits.writeto(filename, data, overwrite=True)

An equivalent way to insert an entry into the list of warning filter specifications
for simple call `warnings.simplefilter`::

    >>> warnings.simplefilter('ignore', UserWarning)

Astropy includes its own warning classes,
`~astropy.utils.exceptions.AstropyWarning` and
`~astropy.utils.exceptions.AstropyUserWarning`.  All warnings from Astropy are
based on these warning classes (see below for the distinction between them). One
can thus ignore all warnings from Astropy (while still allowing through
warnings from other libraries like Numpy) by using something like::

    >>> from astropy.utils.exceptions import AstropyWarning
    >>> warnings.simplefilter('ignore', category=AstropyWarning)

Warning filters may also be modified just within a certain context using the
`warnings.catch_warnings` context manager::

    >>> with warnings.catch_warnings():
    ...     warnings.simplefilter('ignore', AstropyWarning)
    ...     fits.writeto(filename, data, overwrite=True)

As mentioned above, there are actually *two* base classes for Astropy warnings.
The main distinction is that `~astropy.utils.exceptions.AstropyUserWarning` is
for warnings that are *intended* for typical users (e.g. "Warning: Ambiguous
unit", something that might be because of improper input).  In contrast,
`~astropy.utils.exceptions.AstropyWarning` warnings that are *not*
`~astropy.utils.exceptions.AstropyUserWarning` may be for lower-level warnings
more useful for developers writing code that *uses* Astropy (e.g., the
deprecation warnings discussed below).  So if you're a user that just wants to
silence everything, the code above will suffice, but if you are a developer and
want to hide development-related warnings from your users, you may wish to still
allow through `~astropy.utils.exceptions.AstropyUserWarning`.

Astropy also issues warnings when deprecated API features are used.  If you
wish to *squelch* deprecation warnings, you can start Python with
``-Wi::Deprecation``.  This sets all deprecation warnings to ignored.  There is
also an Astropy-specific `~astropy.utils.exceptions.AstropyDeprecationWarning`
which can be used to disable deprecation warnings from Astropy only.

See `the CPython documentation
<https://docs.python.org/3/using/cmdline.html#cmdoption-W>`__ for more
information on the -W argument.
*******************
LTS Backport Policy
*******************

Starting from astropy 5.0, backports to Long-Term Stable (LTS) releases
will only cover:

* Critical security fixes.
* Bug fixes where backporting is straightforward (e.g., no conflicts).
* More complex fixes are permissible only as resources allow, ideally with
  backports done by the original author, users, or institutions who are on LTS
  and need the fix.
* New "features" that are absolutely necessary to accomplish the above.
* Other critical additions absolutely necessary for users stuck to the LTS.
  In this case, the users themselves (e.g., developers from affected institutions)
  would perform the needed backports or implementation.

This is because LTS lasts for about 2 years. During that time frame, as the
LTS branch diverges from the development branch, backports will become
increasingly difficult due to merge conflicts. When conflicts arise,
automation is not possible, therefore driving up the maintenance cost.
************
Known Issues
************

.. contents::
   :local:
   :depth: 2

While most bugs and issues are managed using the `astropy issue
tracker <https://github.com/astropy/astropy/issues>`_, this document
lists issues that are too difficult to fix, may require some
intervention from the user to work around, or are caused by bugs in other
projects or packages.

Issues listed on this page are grouped into two categories: The first is known
issues and shortcomings in actual algorithms and interfaces that currently do
not have fixes or workarounds, and that users should be aware of when writing
code that uses ``astropy``. Some of those issues are still platform-specific,
while others are very general. The second category is of common issues that come
up when configuring, building, or installing ``astropy``. This also includes
cases where the test suite can report false negatives depending on the context/
platform on which it was run.

Known Deficiencies
==================

.. _quantity_issues:

Quantities Lose Their Units with Some Operations
------------------------------------------------

Quantities are subclassed from ``numpy``'s `~numpy.ndarray` and while we have
ensured that ``numpy`` functions will work well with them, they do not always
work in functions from ``scipy`` or other packages that use ``numpy``
internally, but ignore the subclass. Furthermore, at a few places in ``numpy``
itself we cannot control the behaviour. For instance, care must be taken when
setting array slices using Quantities::

    >>> import astropy.units as u
    >>> import numpy as np
    >>> a = np.ones(4)
    >>> a[2:3] = 2*u.kg
    >>> a # doctest: +FLOAT_CMP
    array([1., 1., 2., 1.])

::

    >>> a = np.ones(4)
    >>> a[2:3] = 1*u.cm/u.m
    >>> a # doctest: +FLOAT_CMP
    array([1., 1., 1., 1.])

Either set single array entries or use lists of Quantities::

    >>> a = np.ones(4)
    >>> a[2] = 1*u.cm/u.m
    >>> a # doctest: +FLOAT_CMP
    array([1.  , 1.  , 0.01, 1.  ])

::

    >>> a = np.ones(4)
    >>> a[2:3] = [1*u.cm/u.m]
    >>> a # doctest: +FLOAT_CMP
    array([1.  , 1.  , 0.01, 1.  ])

Both will throw an exception if units do not cancel, e.g.::

    >>> a = np.ones(4)
    >>> a[2] = 1*u.cm # doctest: +SKIP
    Traceback (most recent call last):
    ...
    TypeError: only dimensionless scalar quantities can be converted to Python scalars


See: https://github.com/astropy/astropy/issues/7582

Numpy array creation functions cannot be used to initialize Quantity
--------------------------------------------------------------------
Trying the following example will throw an UnitConversionError
on NumPy before version 1.20 and ignore the unit in later versions:

.. doctest-requires:: numpy<1.20

    >>> my_quantity = u.Quantity(1, u.m)
    >>> np.full(10, my_quantity)  # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
    ...
    UnitConversionError: 'm' (length) and '' (dimensionless) are not convertible

A workaround for this at the moment would be to do::

    >>> np.full(10, 1) << u.m
    <Quantity [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.] m>

As well as with `~numpy.full` one cannot do `~numpy.zeros`, `~numpy.ones`, and `~numpy.empty`.

The `~numpy.arange` function does not work either::

    >>> np.arange(0 * u.m, 10 * u.m, 1 * u.m)  # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
    ...
    TypeError: only dimensionless scalar quantities can be converted to Python scalars

Workarounds include moving the units outside of the call to
`~numpy.arange`::

    >>> np.arange(0, 10, 1) * u.m
    <Quantity [0., 1., 2., 3., 4., 5., 6., 7., 8., 9.] m>

Also, `~numpy.linspace` does work:

    >>> np.linspace(0 * u.m, 9 * u.m, 10)
    <Quantity [0., 1., 2., 3., 4., 5., 6., 7., 8., 9.] m>


Quantities Lose Their Units When Broadcasted
--------------------------------------------

When broadcasting Quantities, it is necessary to pass ``subok=True`` to
`~numpy.broadcast_to`, or else a bare `~numpy.ndarray` will be returned::

   >>> q = u.Quantity(np.arange(10.), u.m)
   >>> b = np.broadcast_to(q, (2, len(q)))
   >>> b # doctest: +FLOAT_CMP
   array([[0., 1., 2., 3., 4., 5., 6., 7., 8., 9.],
          [0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]])
   >>> b2 = np.broadcast_to(q, (2, len(q)), subok=True)
   >>> b2 # doctest: +FLOAT_CMP
   <Quantity [[0., 1., 2., 3., 4., 5., 6., 7., 8., 9.],
              [0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]] m>

This is analogous to the case of passing a Quantity to `~numpy.array`::

   >>> a = np.array(q)
   >>> a # doctest: +FLOAT_CMP
   array([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])
   >>> a2 = np.array(q, subok=True)
   >>> a2 # doctest: +FLOAT_CMP
   <Quantity [0., 1., 2., 3., 4., 5., 6., 7., 8., 9.] m>

See: https://github.com/astropy/astropy/issues/7832

mmap Support for ``astropy.io.fits`` on GNU Hurd
------------------------------------------------

On Hurd and possibly other platforms, ``flush()`` on memory-mapped files are not
implemented, so writing changes to a mmap'd FITS file may not be reliable and is
thus disabled. Attempting to open a FITS file in writeable mode with mmap will
result in a warning (and mmap will be disabled on the file automatically).

See: https://github.com/astropy/astropy/issues/968


Color Printing on Windows
-------------------------

Colored printing of log messages and other colored text does work in Windows,
but only when running in the IPython console. Colors are not currently
supported in the basic Python command-line interpreter on Windows.

``numpy.int64`` does not decompose input ``Quantity`` objects
-------------------------------------------------------------

Python's ``int()`` goes through ``__index__``
while ``numpy.int64`` or ``numpy.int_`` do not go through ``__index__``. This
means that an upstream fix in NumPy is required in order for
``astropy.units`` to control decomposing the input in these functions::

    >>> np.int64((15 * u.km) / (15 * u.imperial.foot))
    1
    >>> np.int_((15 * u.km) / (15 * u.imperial.foot))
    1
    >>> int((15 * u.km) / (15 * u.imperial.foot))
    3280

To convert a dimensionless `~astropy.units.Quantity` to an integer, it is
therefore recommended to use ``int(...)``.

Inconsistent behavior when converting complex numbers to floats
---------------------------------------------------------------

Attempting to use `float` or NumPy's ``numpy.float`` on a standard
complex number (e.g., ``5 + 6j``) results in a `TypeError`.  In
contrast, using `float` or ``numpy.float`` on a complex number from
NumPy (e.g., ``numpy.complex128``) drops the imaginary component and
issues a ``numpy.ComplexWarning``.  This inconsistency persists between
`~astropy.units.Quantity` instances based on standard and NumPy
complex numbers.  To get the real part of a complex number, it is
recommended to use ``numpy.real``.

.. _structured_unit_deserialization_segfault:

Structured units deserialization segfaults in big-endian
--------------------------------------------------------

Structured units deserialization with ``pickle`` may cause segmentation
fault in big-endian machine with ``numpy<1.21.1``.

Build/Installation/Test Issues
==============================

Anaconda Users Should Upgrade with ``conda``, Not ``pip``
---------------------------------------------------------

Upgrading ``astropy`` in the Anaconda Python distribution using ``pip`` can result
in a corrupted install with a mix of files from the old version and the new
version. Anaconda users should update with ``conda update astropy``. There
may be a brief delay between the release of ``astropy`` on PyPI and its release
via the ``conda`` package manager; users can check the availability of new
versions with ``conda search astropy``.


Locale Errors in MacOS X and Linux
----------------------------------

On MacOS X, you may see the following error when running ``pip``::

    ...
    ValueError: unknown locale: UTF-8

This is due to the ``LC_CTYPE`` environment variable being incorrectly set to
``UTF-8`` by default, which is not a valid locale setting.

On MacOS X or Linux (or other platforms) you may also encounter the following
error::

    ...
      stderr = stderr.decode(stdio_encoding)
    TypeError: decode() argument 1 must be str, not None

This also indicates that your locale is not set correctly.

To fix either of these issues, set this environment variable, as well as the
``LANG`` and ``LC_ALL`` environment variables to e.g. ``en_US.UTF-8`` using, in
the case of ``bash``::

    export LANG="en_US.UTF-8"
    export LC_ALL="en_US.UTF-8"
    export LC_CTYPE="en_US.UTF-8"

To avoid any issues in future, you should add this line to your e.g.
``~/.bash_profile`` or ``.bashrc`` file.

To test these changes, open a new terminal and type ``locale``, and you should
see something like::

    $ locale
    LANG="en_US.UTF-8"
    LC_COLLATE="en_US.UTF-8"
    LC_CTYPE="en_US.UTF-8"
    LC_MESSAGES="en_US.UTF-8"
    LC_MONETARY="en_US.UTF-8"
    LC_NUMERIC="en_US.UTF-8"
    LC_TIME="en_US.UTF-8"
    LC_ALL="en_US.UTF-8"

If so, you can go ahead and try running ``pip`` again (in the new
terminal).


Failing Logging Tests When Running the Tests in IPython
-------------------------------------------------------

When running the Astropy tests using ``astropy.test()`` in an IPython
interpreter, some of the tests in the ``astropy/tests/test_logger.py`` *might*
fail depending on the version of IPython or other factors.
This is due to mutually incompatible behaviors in IPython and pytest, and is
not due to a problem with the test itself or the feature being tested.

See: https://github.com/astropy/astropy/issues/717
*******************
Authors and Credits
*******************

Core Package Contributors
=========================

* Aaron Meisner
* Aarya Patil
* Abhinuv Nitin Pitale
* Abigail Stevens
* Adam Ginsburg
* Adele Plunkett
* Aditya Sharma
* Adrian Price-Whelan
* Akash Deshpande
* Akeem
* Akshat Dixit
* Akshat1Nar
* Al Niessner
* Albert Y. Shih
* Aleh Khvalko
* Alex Conley
* Alex de la Vega
* Alex Drlica-Wagner
* Alex Hagen
* Alex Rudy
* Alexander Bakanov
* Alexandre Beelen
* Amit Kumar
* Ana Posses
* Anany Shrey Jain
* Anchit Jain
* Andreas Baumbach
* Andrej Rode
* Andrew Hearin
* Aniket Kulkarni
* Aniket Sanghi
* Anirudh Katipally
* Anne Archibald
* Antetokounpo
* Anthony Horton
* Antony Lee
* Arfon Smith
* Arie Kurniawan
* Arne de Laat
* Arthur Eigenbrot
* Asish Panda
* Asra Nizami
* athul
* Austen Groener
* Axel Donath
* Azalee Bostroem
* Bastian Beischer
* Ben Greiner
* Benjamin Alan Weaver
* Benjamin Roulston
* Benjamin Winkel
* Bernardo Sulzbach
* Bernie Simon
* Bhavya Khandelwal
* Bili Dong
* Bogdan Nicula
* Bojan Nikolic
* Brett Morris
* Brian Soto
* Brigitta Sipcz
* britgit
* Bruce Merry
* Bruno Oliveira
* Bryce Kalmbach
* Bryce Nordgren
* Carl Osterwisch
* Carl Schaffer
* Chiara Marmo
* Chris Beaumont
* Chris Hanley
* Chris Osborne
* Chris Simpson
* Christian Clauss
* Christian Hettlage
* Christoph Deil
* Christoph Gohlke
* Christopher Bonnett
* Chun Ly
* Clara Brasseur
* Clare Shanahan
* Clment Robert
* Conor MacBride
* Cristian Ardelean
* Curtis McCully
* Dan Foreman-Mackey
* Dan P. Cunningham
* Dan Taranu
* Daniel Bell
* Daniel D'Avella
* Daniel Datsev
* Daniel Lenz
* Daniel Ruschel Dutra
* Daniel Ryan
* Danny Goldstein
* Daria Cara
* David Kirkby
* David M. Palmer
* David Prez-Surez
* David Shiga
* David Shupe
* David Stansby
* Demitri Muna
* Derek Homeier
* Devin Crichton
* Diego Alonso
* Diego Asterio de Zaballa
* disha
* Dominik Klaes
* Douglas Burke
* Drew Leonard
* Duncan Macleod
* Dylan Gregersen
* E\. Madison Bray
* E\. Rykoff
* Ed Slavich
* Edward Betts
* Edward Slavich
* Eero Vaher
* Eli Bressert
* Elijah Bernstein-Cooper
* Eloy Salinas
* Emily Deibert
* Emir
* Emma Hogan
* Eric Depagne
* Eric Jeschke
* Eric Koch
* Erik Tollerud
* Erin Allard
* Esteban Pardo Snchez
* Even Rouault
* Evert Rol
* Felix Yan
* fockez
* Francesco Biscani
* Francesco Montanari
* Francesco Montesano
* Frdric Chapoton
* Frdric Grollier
* Gabriel Brammer
* Gabriel Perren
* Geert Barentsen
* George Galvin
* Georgiana Ogrean
* Gerrit Schellenberger
* Giang Nguyen
* Giorgio Calderone
* Graham Kanarek
* Grant Jenks
* Gregory Dubois-Felsmann
* Gregory Simonian
* Griffin Hosseinzadeh
* Gustavo Bragana
* Gyanendra Shukla
* Hannes Breytenbach
* Hans Moritz Gnther
* Harry Ferguson
* Helen Sherwood-Taylor
* Himanshu Pathak
* homeboy445
* Hugo Buddelmeijer
* Humna Awan
* iamsoto
* ikkamens
* Inada Naoki
* J\. Goutin
* J\. Xavier Prochaska
* Jake VanderPlas
* Jakob Maljaars
* James Davies
* James Dearman
* James Noss
* James Taylor
* James Tocknell
* James Turner
* Jan Skowron
* Jane Rigby
* Jani umak
* Jason Segnini
* Javier Pascual Granado
* JC Hsu
* Jean Connelly
* Jeff Taylor
* Jeffrey McBeth
* Jero Bado
* jimboH
* Joanna Power
* Joe Hunkeler
* Joe Lyman
* Joe Philip Ninan
* John Fisher
* John Parejko
* Johnny Greco
* Jonas Groe Sundrup
* Jonathan Eisenhamer
* Jonathan Foster
* Jonathan Sick
* Jonathan Whitmore
* Jrg Dietrich
* Jose Sabater
* Joseph Jon Booker
* Joseph Long
* Joseph Ryan
* Joseph Schlitz
* Jose Sabater Montes
* Juan Luis Cano Rodrguez
* Juanjo Bazan
* Julien Woillez
* Jurien Huisman
* Kacper Kowalik
* Karan Grover
* Karl Gordon
* Karl Vyhmeister
* Karl Wessel
* Katrin Leinweber
* Kelle Cruz
* Kevin Gullikson
* Kevin Sooley
* Kewei Li
* Kieran Leschinski
* Kirill Tchernyshyov
* Kris Stern
* Kristin Berry
* Kyle Barbary
* Kyle Oman
* Larry Bradley
* Laura Hayes
* Laura Watkins
* Lauren Glattly
* Laurie Stephey
* Leah Fulmer
* Lee Spitler
* Lehman Garrison
* Lennard Kiehl
* Leo Singer
* Leonardo Ferreira
* Lia Corrales
* Lingyi Hu
* Lisa Martin
* Lisa Walter
* Ludwig Schwardt
* Luigi Paioro
* Luke G. Bouma
* Luke Kelley
* luz paz
* Lni Gauffier
* M Atakan Grkan
* M S R Dinesh
* Mabry Cervin
* Madhura Parikh
* Magali Mebsout
* maggiesam
* Maik Nijhuis
* Manas Satish Bedmutha
* Maneesh Yadav
* Mangala Gowri Krishnamoorthy
* Manish Biswas
* Manodeep Sinha
* Mark Fardal
* Mark Taylor
* Markus Demleitner
* Marten van Kerkwijk
* Martin Glatzle
* Matej Stuchlik
* Mathieu Servillat
* Matt Davis
* Matteo Bachetti
* Matthew Bourque
* Matthew Brett
* Matthew Craig
* Matthew Petroff
* Matthew Turk
* Matthias Bussonnier
* Mavani Bhautik
* Max Silbiger
* Max Voronkov
* Maximilian Nthe
* Mdric Boquien
* Megan Sosey
* Michael Brewer
* Michael Droettboom
* Michael Hirsch
* Michael Hoenig
* Michael Lindner-D'Addario
* Michael Mueller
* Michael Seifert
* Michael Wood-Vasey
* Michael Zhang
* Michele Costa
* Michele Mastropietro
* Miguel de Val-Borro
* Mihai Cara
* Mike Alexandersen
* Mike McCarty
* Mikhail Minin
* Mikoaj
* Miruna Oprescu
* Moataz Hisham
* Mohan Agrawal
* Molly Peeples
* Nabil Freij
* Nadia Dencheva
* Nathanial Hendler
* Nathaniel Starkman
* Neal McBurnett
* Neil Crighton
* Neil Parley
* Nicholas Earl
* Nicholas S. Kern
* Nicholas Saunders
* Nick Lloyd
* Nick Murphy
* Nicolas Tessore
* Nikita Saxena
* Nikita Tewary
* Nimit Bhardwaj
* Noah Zuckman
* Nora Luetzgendorf
* odidev
* Ole Streicher
* Orion Poplawski
* orionlee
* Param Patidar
* Parikshit Sakurikar
* Patricio Rojo
* Patti Carroll
* Paul Barrett
* Paul Hirst
* Paul Huwe
* Paul Price
* Paul Sladen
* Pauline Barmby
* Perry Greenfield
* Peter Cock
* Peter Teuben
* Peter Yoachim
* Pey Lian Lim
* Prasanth Nair
* Pratik Patel
* Pritish Chakraborty
* Pushkar Kopparla
* Ralf Gommers
* Rashid Khan
* Rasmus Handberg
* Ray Plante
* Rgis Terrier
* Ricardo Fonseca
* Ricardo Ogando
* Richard R
* Ricky O'Steen
* Rik van Lieshout
* Ritiek Malhotra
* Ritwick DSouza
* Roban Hultman Kramer
* Robel Geda
* Robert Cross
* Rocio Kiman
* Rohan Rajpal
* Rohit Kapoor
* Rohit Patil
* Roman Tolesnikov
* Roy Smart
* Rui Xue
* Ryan Abernathey
* Ryan Cooke
* Ryan Fox
* Sadie Bartholomew
* Sam Van Kooten
* Sam Verstocken
* Samuel Brice
* Sanjeev Dubey
* Sara Ogaz
* Sarah Graves
* Sarah Kendrew
* Sashank Mishra
* sashmish
* Saurav Sachidanand
* Scott Thomas
* Semyeong Oh
* Serge Montagnac
* Sergio Pascual
* Shailesh Ahuja
* Shankar Kulumani
* Shantanu Srivastava
* Shilpi Jain
* Shivan Sornarajah
* Shivansh Mishra
* Shresth Verma
* Shreyas Bapat
* Sigurd Nss
* Simon Conseil
* Simon Gibbons
* Simon Liedtke
* Simon Torres
* Sourabh Cheedella
* Srikrishna Sekhar
* srirajshukla
* Stefan Becker
* Stefan Nelson
* Stephen Portillo
* Steve Crawford
* Steve Guest
* Steven Bamford
* Stuart Littlefair
* Stuart Mumford
* Sudheesh Singanamalla
* Sushobhana Patra
* Suyog Garg
* Swapnil Sharma
* T\. Carl Beery
* Tanuj Rastogi
* Thomas Erben
* Thomas Robitaille
* Thompson Le Blanc
* Tiffany Jansen
* Tim Gates
* Tim Jenness
* Tim Plummer
* Tito Dal Canton
* Tom Aldcroft
* Tom Donaldson
* Tom J Wilson
* Tom Kooij
* Tomas Babej
* Tyler Finethy
* Vatsala Swaroop
* Victoria Dye
* Vinayak Mehta
* Vishnunarayan K I
* Vital Fernndez
* Volodymyr Savchenko
* VSN Reddy Janga
* Vctor Terrn
* Vctor Zabalza
* Wilfred Tyler Gee
* William Jamieson
* Wolfgang Kerzendorf
* Yannick Copin
* Yash Kumar
* Yash Sharma
* Yingqi Ying
* Zac Hatfield-Dodds
* Zach Edwards
* Zachary Kurtz
* Zeljko Ivezic
* Zhiyuan Ma
* Zlatan Vasovi
* Z Vinicius

Other Credits
=============

* Kyle Barbary for designing the Astropy logos and documentation themes.
* Andrew Pontzen and the `pynbody <https://github.com/pynbody/pynbody>`_ team
  (For code that grew into :mod:`astropy.units`)
* Everyone on the `astropy-dev mailing list`_ and the `Astropy mailing list`_
  for contributing to many discussions and decisions!

(If you have contributed to the ``astropy`` core package and your name is missing,
please send an email to the coordinators, or
`open a pull request for this page <https://github.com/astropy/astropy/edit/main/docs/credits.rst>`_
in the `astropy repository <https://github.com/astropy/astropy>`_)

For how to acknowledge Astropy, please see `the Acknowledging or Citing Astropy page <https://www.astropy.org/acknowledging.html>`_.
****************************************************
A description of the package (`astropy.packagename`)
****************************************************

When creating a new subpackage's documentation, this file should be
copied to a file "index.rst" in a directory corresponding to the name of
the package. E.g., ``docs/packagename/index.rst``. And don't forget to
delete this paragraph.

Introduction
============

Include general content that might be useful for understanding the
package here, as well as general scientific or mathematical background
that might be necessary for the "big-picture" of this package.


Getting Started
===============

Short tutorial-like examples of how to do common-tasks - should be
fairly quick, with any more detailed examples in the next section.


Using `packagename`
===================

.. THIS SECTION SHOULD BE EITHER


This section is for the detailed documentation.  For simpler packages, this
should either by paragraphs or sub-divided into sub-sections like:

Sub-topic 1
-----------

Content if needed

A Complex example
-----------------

Content if needed

Sub-sub topic 1
^^^^^^^^^^^^^^^^

Content if needed (note the use of ^^^^ at this level).

Sub-sub-sub topic 1
"""""""""""""""""""

Content if needed (note the use of """"" at this level).
This is probably the deepest level that is practical.  However,
just in case, the next levels of detail should use the +, :, and ~
characters respectively.


.. OR IF MORE COMPLICATED,

For more complicated packages that require multiple documents, this
should just be a table of contents referencing those documents:

.. toctree::
    subdoc1
    subdoc2
    subdoc3


Either a toctree or sub-sections should be used, *not* both.

For example, if your toctree looks like the above example, this document
should be ``docs/packagename/index.rst``, and the other documents should
be ``docs/packagename/subdoc1.rst``, ``docs/packagename/subdoc2.rst``,
and ``docs/packagename/subdoc3.rst``.

In the "more complicated" case of using ``subdoc.rst`` files, each of those
should likewise use the section character header order of ``* = - ^ " + : ~``.


See Also (optional)
===================

Include here any references to related packages, articles, or texts.


Reference/API
=============

.. automodapi:: packagename


Acknowledgments and Licenses (optional)
=======================================

Any acknowledgements or licenses needed for this package - remove the
section if none are necessary.
.. currentmodule:: astropy

****************
Astropy Glossary
****************

.. glossary::

   (`n`,)
      A parenthesized number followed by a comma denotes a tuple with one
      element. The trailing comma distinguishes a one-element tuple from a
      parenthesized ``n``.
      This is from NumPy; see https://numpy.org/doc/stable/glossary.html.

   number
      Any numeric type. eg float or int or any of the ``numpy.number``.

   -like
      Used to indicate on object of that type or that can instantiate the type.
      E.g. :class:`~astropy.units.Quantity`-like includes ``"2 * u.km"``
      because ``astropy.units.Quantity("2 * u.km")`` works.

   unit-like
      Must be an :class:`~astropy.units.UnitBase` (subclass) instance or a
      string or other instance parseable by :class:`~astropy.units.Unit`.

   quantity-like
      Must be an `~astropy.units.Quantity` (or subclass) instance or a string
      parseable by `~astropy.units.Quantity`.
      Note that the interpretation of units in strings depends on the class --
      ``Quantity("180d")`` is 180 **days**, while ``Angle("180d")`` is 180
      **degrees** -- so check the string parses as intended for ``Quantity``.

   ['physical type']
       The physical type of a quantity can be annotated in square brackets
       following a `~astropy.units.Quantity` (or similar :term:`quantity-like`).

       For example, ``distance : quantity-like ['length']``

   angle-like
      :term:`quantity-like`, but interpreted by an angular
      `~astropy.units.SpecificTypeQuantity`, like `~astropy.coordinates.Angle`
      or `~astropy.coordinates.Longitude` or `~astropy.coordinates.Latitude`.
      Note that the interpretation of units in strings depends on the class --
      ``Quantity("180d")`` is 180 days, while ``Angle("180d")`` is 180 degrees
      -- so make sure the string parses as intended for ``Angle``.

   length-like
      :term:`quantity-like`, but interpretable by
      :class:`~astropy.coordinates.Distance`.

   frame-like
      A :class:`~astropy.coordinates.BaseCoordinateFrame` subclass instance or a
      string that can be converted to a Frame by
      :class:`~astropy.coordinates.sky_coordinate_parsers._get_frame_class`.

   coordinate-like
      A Coordinate-type object such as a
      :class:`~astropy.coordinates.BaseCoordinateFrame` subclass instance or a
      :class:`~astropy.coordinates.SkyCoord` (or subclass) instance.

   table-like
      An astropy :class:`~astropy.table.Table` or any object that can
      initialize one. Anything marked as table-like will be processed through
      a :class:`~astropy.table.Table`.

   time-like
      :class:`~astropy.time.Time` or any valid initializer.

   buffer-like
      Anything that implements Python's buffer protocol. See
      https://docs.python.org/3/c-api/buffer.html#bufferobjects

   writable file-like object
      In the context of a :term:`python:file-like object` object, anything
      that supports writing with a method ``write``.

   readable file-like object
      In the context of a :term:`python:file-like object` object, anything
      that supports writing with a method ``read``.


***************************
Optional Packages' Glossary
***************************

.. currentmodule:: matplotlib.pyplot

.. glossary::

   color
      Any valid Matplotlib color.
**************************************
Importing ``astropy`` and Sub-packages
**************************************

In order to encourage consistency among users in importing and using Astropy
functionality, we have put together the following guidelines.

Since most of the functionality in Astropy resides in sub-packages, importing
``astropy`` as::

    >>> import astropy

is not very useful. Instead, it's best to import the desired sub-package
with the syntax::

    >>> from astropy import subpackage  # doctest: +SKIP

For example, to access the FITS-related functionality, you can import
`astropy.io.fits` with::

    >>> from astropy.io import fits
    >>> hdulist = fits.open('data.fits')  # doctest: +SKIP

In specific cases, we have recommended shortcuts in the documentation for
specific sub-packages. For example::

    >>> from astropy import units as u
    >>> from astropy import coordinates as coord
    >>> coord.SkyCoord(ra=10.68458*u.deg, dec=41.26917*u.deg, frame='icrs')  # doctest: +FLOAT_CMP
    <SkyCoord (ICRS): (ra, dec) in deg
        ( 10.68458,  41.26917)>

Finally, in some cases, most of the required functionality is contained in a
single class (or a few classes). In those cases, the class can be directly
imported::

    >>> from astropy.cosmology import WMAP7
    >>> from astropy.table import Table
    >>> from astropy.wcs import WCS

Note that for clarity, and to avoid any issues, we recommend **never**
importing any Astropy functionality using ``*``, for example::

    >>> from astropy.io.fits import *  # NOT recommended

Some components of Astropy started off as standalone packages (e.g. PyFITS,
PyWCS), so in cases where Astropy needs to be used as a drop-in replacement,
the following syntax is also acceptable::

    >>> from astropy.io import fits as pyfits

*********************************
Getting Started with Sub-packages
*********************************

Because different sub-packages have very different functionalities, each
sub-package has its own getting started guide. These can be found by browsing
the sections listed in the :ref:`user-docs`.

You can also look at docstrings for a particular package or object, or access
their documentation using the `~astropy.utils.misc.find_api_page` function. For
example, ::

    >>> from astropy import find_api_page
    >>> from astropy.units import Quantity
    >>> find_api_page(Quantity)  # doctest: +SKIP

will bring up the documentation for the `~astropy.units.Quantity` class
in your browser.
.. _testhelpers:

*********************
Astropy Testing Tools
*********************

This section is primarily a reference for developers that want to understand or
add to the Astropy testing machinery. See :doc:`/development/testguide` for an
overview of running or writing the tests.


`astropy.tests.helper` Module
=============================

To ease development of tests that work with Astropy, the
`astropy.tests.helper` module provides some utility functions to make
tests that use Astropy conventions or classes easier to work with, e.g.,
functions to test for near-equality of `~astropy.units.Quantity` objects.

The functionality here is not exhaustive, because
much of the useful tools are either in the standard
library, `pytest`_, or `numpy.testing
<https://numpy.org/doc/stable/reference/routines.testing.html>`_.  This module
contains primarily functionality specific to the astropy core package or
packages that follow the Astropy package template.

Conversion Guide
----------------

Some long-standing functionality has been deprecated/removed since ``astropy`` 5.1.
The following table maps them to what you should use instead.

========================================================== ===============================================
Deprecated                                                 Use this
========================================================== ===============================================
``astropy.io.ascii.tests.common.raises``                   ``pytest.raises``
``astropy.tests.helper.raises``                            ``pytest.raises``
``astropy.tests.helper.catch_warnings``                    ``pytest.warns``
``astropy.tests.helper.ignore_warnings``                   https://docs.pytest.org/en/stable/warnings.html
``astropy.tests.helper.enable_deprecations_as_exceptions`` https://docs.pytest.org/en/stable/warnings.html
``astropy.tests.helper.treat_deprecations_as_exceptions``  https://docs.pytest.org/en/stable/warnings.html
========================================================== ===============================================

========================================================== ===============================================
Removed                                                    Use this
========================================================== ===============================================
``astropy.tests.disable_internet``                         ``pytest_remotedata.disable_internet``
``astropy.tests.helper.remote_data``                       ``pytest.mark.remote_data``
``astropy.tests.plugins.display``                          ``pytest-astropy-header`` package
========================================================== ===============================================

Reference/API
-------------

.. module:: astropy.tests.helper

.. automodapi:: astropy.tests.helper
    :no-main-docstr:
    :no-inheritance-diagram:


Astropy Test Runner
===================

When executing tests with `astropy.test` the call to pytest is controlled
by the `astropy.tests.runner.TestRunner` class.

The `~astropy.tests.runner.TestRunner` class is used to generate the
`astropy.test` function, the test function generates a set of command line
arguments to pytest. The arguments to pytest are defined in the
``run_tests`` method, the arguments to
``run_tests`` and their respective logic are defined in methods of
`~astropy.tests.runner.TestRunner` decorated with the
`~astropy.tests.runner.keyword` decorator. For an example of this see
`~astropy.tests.runner.TestRunnerBase`. This design makes it easy for
packages to add or remove keyword arguments to their test runners, or define a
whole new set of arguments by subclassing from
`~astropy.tests.runner.TestRunnerBase`.

Reference/API
-------------

.. module:: astropy.tests.runner

.. autoclass:: astropy.tests.runner.keyword
    :no-undoc-members:

.. autoclass:: astropy.tests.runner.TestRunnerBase

.. autoclass:: astropy.tests.runner.TestRunner
:orphan:

****************************
Getting Started with Astropy
****************************

This page has been moved to :ref:`getting-started`. Please update your links.
:orphan:

********
Overview
********

This page has been removed. For an overview, see the `<http://www.astropy.org>`_ page.
**************
Logging system
**************

.. note::

    The Astropy logging system is meant for internal ``astropy`` usage. For use
    in other packages, we recommend implementing your own logger instead.

Overview
========

The Astropy logging system is designed to give users flexibility in deciding
which log messages to show, to capture them, and to send them to a file.

All messages printed by Astropy routines should use the built-in logging
facility (normal ``print()`` calls should only be done by routines that are
explicitly requested to print output). Messages can have one of several
levels:

* DEBUG: Detailed information, typically of interest only when diagnosing
  problems.

* INFO: An message conveying information about the current task, and
  confirming that things are working as expected

* WARNING: An indication that something unexpected happened, and that user
  action may be required.

* ERROR: indicates a more serious issue, including exceptions

By default, INFO, WARNING and ERROR messages are displayed, and are sent to a
log file located at ``~/.astropy/astropy.log`` (if the file is writeable).

Configuring the logging system
==============================

First, import the logger::

    from astropy import log

The threshold level (defined above) for messages can be set with e.g.::

    log.setLevel('DEBUG')

Color (enabled by default) can be disabled with::

    log.disable_color()

and enabled with::

    log.enable_color()

Warnings from ``warnings.warn`` can be logged with::

    log.enable_warnings_logging()

which can be disabled with::

    log.disable_warnings_logging()

and exceptions can be included in the log with::

    log.enable_exception_logging()

which can be disabled with::

    log.disable_exception_logging()

It is also possible to set these settings from the Astropy configuration file,
which also allows an overall log file to be specified. See
`Using the configuration file`_ for more information.

Context managers
================

In some cases, you may want to capture the log messages, for example to check
whether a specific message was output, or to log the messages from a specific
section of code to a file. Both of these are possible using context managers.

To add the log messages to a list, first import the logger if you have not
already done so::

    from astropy import log

then enclose the code in which you want to log the messages to a list in a
``with`` statement::

    with log.log_to_list() as log_list:
        # your code here

In the above example, once the block of code has executed, ``log_list`` will
be a Python list containing all the Astropy logging messages that were raised.
Note that messages continue to be output as normal.

Similarly, you can output the log messages of a specific section of code to a
file using::

    with log.log_to_file('myfile.log'):
        # your code here

which will add all the messages to ``myfile.log`` (this is in addition to the
overall log file mentioned in `Using the configuration file`_).

While these context managers will include all the messages emitted by the
logger (using the global level set by ``log.setLevel``), it is possible to
filter a subset of these using ``filter_level=``, and specifying one of
``'DEBUG'``, ``'INFO'``, ``'WARN'``, ``'ERROR'``. Note that if
``filter_level`` is a lower level than that set via ``setLevel``, only
messages with the level set by ``setLevel`` or higher will be included (i.e.
``filter_level`` is only filtering a subset of the messages normally emitted
by the logger).

Similarly, it is possible to filter a subset of the messages by origin by
specifying ``filter_origin=`` followed by a string. If the origin of a message
starts with that string, the message will be included in the context manager.
For example, ``filter_origin='astropy.wcs'`` will include only messages
emitted in the ``astropy.wcs`` sub-package.

Using the configuration file
============================

Options for the logger can be set in the ``[logger]`` section
of the Astropy configuration file::

    [logger]

    # Threshold for the logging messages. Logging messages that are less severe
    # than this level will be ignored. The levels are 'DEBUG', 'INFO', 'WARNING',
    # 'ERROR'
    log_level = 'INFO'

    # Whether to use color for the level names
    use_color = True

    # Whether to log warnings.warn calls
    log_warnings = False

    # Whether to log exceptions before raising them
    log_exceptions = False

    # Whether to always log messages to a log file
    log_to_file = True

    # The file to log messages to. If empty string is given, it defaults to a
    # file `astropy.log` in the astropy config directory.
    log_file_path = '~/.astropy/astropy.log'

    # Threshold for logging messages to log_file_path
    log_file_level = 'INFO'

    # Format for log file entries
    log_file_format = '%(asctime)s, %(origin)s, %(levelname)s, %(message)s'

    # The encoding (e.g., UTF-8) to use for the log file.  If empty string is
    # given, it defaults to the platform-preferred encoding.
    log_file_encoding = ""


Reference/API
=============

.. automodapi:: astropy.logger
    :no-inheritance-diagram:
.. Astropy documentation index file, created by
   sphinx-quickstart on Tue Jul 26 02:59:34 2011.
   You can adapt this file completely to your liking, but it should at least
   contain the root `toctree` directive.

:tocdepth: 3

.. the "raw" directive below is used to hide the title in favor of just the logo being visible
.. raw:: html

    <style media="screen" type="text/css">
      h1 { display:none; }
    </style>

#####################
Astropy Documentation
#####################

.. |logo_svg| image:: _static/astropy_banner.svg

.. |logo_png| image:: _static/astropy_banner_96.png

.. raw:: html

   <img src="_static/astropy_banner.svg" onerror="this.src='_static/astropy_banner_96.png'; this.onerror=null;" width="485"/>

.. only:: latex

    .. image:: _static/astropy_logo.pdf

The ``astropy`` package contains key functionality and common tools needed for
performing astronomy and astrophysics with Python.  It is at the core of the
`Astropy Project <http://www.astropy.org/about.html>`_, which aims to enable
the community to develop a robust ecosystem of `affiliated packages`_
covering a broad range of needs for astronomical research, data
processing, and data analysis.

.. Important:: If you use Astropy for work presented in a publication or talk
   please help the project via proper `citation or acknowledgement
   <https://www.astropy.org/acknowledging.html>`_.  This also applies to use of
   software or `affiliated packages`_ that depend on the astropy
   core package.

.. _getting-started:

***************
Getting Started
***************

.. toctree::
   :maxdepth: 1

   install
   whatsnew/5.1
   importing_astropy
   Example Gallery <generated/examples/index>
   Tutorials <https://learn.astropy.org/>
   Get Help <http://www.astropy.org/help.html>
   Contribute and Report Problems <http://www.astropy.org/contribute.html>
   About the Astropy Project <http://www.astropy.org/about.html>

.. _user-docs:

******************
User Documentation
******************

Data structures and transformations
-----------------------------------

.. toctree::
   :maxdepth: 1

   constants/index
   units/index
   nddata/index
   table/index
   time/index
   timeseries/index
   coordinates/index
   wcs/index
   modeling/index
   uncertainty/index

Files, I/O, and Communication
-----------------------------

.. toctree::
   :maxdepth: 1

   io/unified
   io/fits/index
   io/ascii/index
   io/votable/index
   io/misc
   samp/index

Computations and utilities
--------------------------

.. toctree::
   :maxdepth: 1

   cosmology/index
   convolution/index
   visualization/index
   stats/index

Nuts and bolts
--------------

.. toctree::
   :maxdepth: 1

   config/index
   io/registry
   logging
   warnings
   utils/index
   glossary

.. _developer-docs:

***********************
Developer Documentation
***********************

The developer documentation contains instructions for how to contribute to
Astropy or affiliated packages, install and test the development version,
as well as coding, documentation, and testing guidelines.

{% if is_development %}

For the guiding vision of this process and the project
as a whole, see :doc:`development/vision`.

.. toctree::
   :maxdepth: 1

   development/workflow/development_workflow
   development/workflow/get_devel_version
   development/when_to_rebase
   development/codeguide
   development/docguide
   development/style-guide
   development/testguide
   testhelpers
   development/scripts
   development/building
   development/ccython
   development/releasing
   development/workflow/maintainer_workflow
   development/astropy-package-template
   changelog

There are some additional tools, mostly of use for maintainers, in the
`astropy/astropy-tools repository
<https://github.com/astropy/astropy-tools>`__.

{%else%}

To read the developer documentation, you will need to go to the :ref:`latest
developer version of the documentation
<astropy-dev:developer-docs>`.

.. toctree::
   :maxdepth: 1

   changelog

{%endif%}

.. _project-details:

***************
Project details
***************

.. toctree::
   :maxdepth: 1

   whatsnew/index
   lts_policy
   known_issues
   credits
   license

*****
Index
*****

* :ref:`genindex`
* :ref:`modindex`
* :ref:`search`

.. _feedback@astropy.org: mailto:feedback@astropy.org
.. _affiliated packages: https://www.astropy.org/affiliated/
************
Installation
************

Installing ``astropy``
======================

If you are new to Python and/or do not have familiarity with `Python virtual
environments <https://docs.python.org/3/tutorial/venv.html>`_, then we recommend
starting by installing the `Anaconda Distribution
<https://www.anaconda.com/distribution/>`_. This works on all platforms (linux,
Mac, Windows) and installs a full-featured scientific Python in a user directory
without requiring root permissions.

Using pip
---------

.. warning::

    Users of the Anaconda Python distribution should follow the instructions
    for :ref:`anaconda_install`.

To install ``astropy`` with `pip`_, run::

    pip install astropy

If you want to make sure none of your existing dependencies get upgraded, you
can also do::

    pip install astropy --no-deps

On the other hand, if you want to install ``astropy`` along with recommended
or even all of the available optional :ref:`dependencies <astropy-main-req>`,
you can do::

    pip install astropy[recommended]

or::

    pip install astropy[all]

In most cases, this will install a pre-compiled version (called a *wheel*) of
astropy, but if you are using a very recent version of Python, if a new version
of astropy has just been released, or if you are building astropy for a platform
that is not common, astropy will be installed from a source file. Note that in
this case you will need a C compiler (e.g., ``gcc`` or ``clang``) to be installed
(see `Building from source`_ below) for the installation to succeed.

If you get a ``PermissionError`` this means that you do not have the required
administrative access to install new packages to your Python installation. In
this case you may consider using the ``--user`` option to install the package
into your home directory. You can read more about how to do this in the `pip
documentation <https://pip.pypa.io/en/stable/user_guide/#user-installs>`_.

Alternatively, if you intend to do development on other software that uses
``astropy``, such as an affiliated package, consider installing ``astropy``
into a :ref:`virtualenv <astropy-dev:virtual_envs>`.

Do **not** install ``astropy`` or other third-party packages using ``sudo``
unless you are fully aware of the risks.

.. _anaconda_install:

Using Conda
-----------

To install ``astropy`` using conda run::

    conda install astropy

``astropy`` is installed by default with the `Anaconda Distribution
<https://www.anaconda.com/distribution/>`_. To update to the latest version run::

    conda update astropy

There may be a delay of a day or two between when a new version of ``astropy``
is released and when a package is available for conda. You can check
for the list of available versions with ``conda search astropy``.

If you want to install ``astropy`` along with recommended or all of the
available optional :ref:`dependencies <astropy-main-req>`, you can do::

    conda install -c conda-forge -c defaults scipy matplotlib

or::

    conda install -c conda-forge -c defaults scipy matplotlib \
      h5py beautifulsoup4 html5lib bleach pandas sortedcontainers \
      pytz setuptools mpmath bottleneck jplephem asdf pyarrow

To also be able to run tests (see below) and support :ref:`builddocs` use the
following. We use ``pip`` for these packages to ensure getting the latest
releases which are compatible with the latest ``pytest`` and ``sphinx`` releases::

    pip install pytest-astropy sphinx-astropy

.. warning::

    Attempting to use `pip <https://pip.pypa.io>`__ to upgrade your installation
    of ``astropy`` itself may result in a corrupted installation.

.. _testing_installed_astropy:

Testing an Installed ``astropy``
--------------------------------

{% if is_development %}

The easiest way to test if your installed version of ``astropy`` is running
correctly is to use the :ref:`astropy.test()` function::

    import astropy
    astropy.test()

The tests should run and print out any failures, which you can report at
the `Astropy issue tracker <https://github.com/astropy/astropy/issues>`_.

This way of running the tests may not work if you do it in the ``astropy`` source
distribution. See :ref:`sourcebuildtest` for how to run the tests from the
source code directory, or :ref:`running-tests` for more details.

{%else%}

See the :ref:`latest documentation on how to test your installed version of
astropy <astropy-dev:testing_installed_astropy>`.

{%endif%}

.. _astropy-main-req:

Requirements
============

``astropy`` has the following strict requirements:

- `Python`_ |minimum_python_version| or later

- `Numpy`_ |minimum_numpy_version| or later

- `PyERFA`_ |minimum_pyerfa_version| or later

- `PyYAML <https://pyyaml.org>`_ |minimum_pyyaml_version| or later

- `packaging`_ |minimum_packaging_version| or later

``astropy`` also depends on a number of other packages for optional features.
The following are particularly recommended:

- `scipy`_ |minimum_scipy_version| or later: To power a variety of features
  in several modules.

- `matplotlib <https://matplotlib.org/>`_ |minimum_matplotlib_version| or later: To provide plotting
  functionality that `astropy.visualization` enhances.

The further dependencies provide more specific features:

- `h5py <http://www.h5py.org/>`_: To read/write
  :class:`~astropy.table.Table` objects from/to HDF5 files.

- `BeautifulSoup <https://www.crummy.com/software/BeautifulSoup/>`_: To read
  :class:`~astropy.table.table.Table` objects from HTML files.

- `html5lib <https://html5lib.readthedocs.io/en/stable/>`_: To read
  :class:`~astropy.table.table.Table` objects from HTML files using the
  `pandas <https://pandas.pydata.org/>`_ reader.

- `bleach <https://bleach.readthedocs.io/>`_: Used to sanitize text when
  disabling HTML escaping in the :class:`~astropy.table.Table` HTML writer.

- `xmllint <http://www.xmlsoft.org/>`_: To validate VOTABLE XML files.
  This is a command line tool installed outside of Python.

- `pandas <https://pandas.pydata.org/>`_: To convert
  :class:`~astropy.table.Table` objects from/to pandas DataFrame objects.
  Version 0.14 or higher is required to use the :ref:`table_io_pandas`
  I/O functions to read/write :class:`~astropy.table.Table` objects.

- `sortedcontainers <https://pypi.org/project/sortedcontainers/>`_ for faster
  ``SCEngine`` indexing engine with ``Table``, although this may still be
  slower in some cases than the default indexing engine.

- `pytz <https://pythonhosted.org/pytz/>`_: To specify and convert between
  timezones.

- `jplephem <https://pypi.org/project/jplephem/>`_: To retrieve JPL
  ephemeris of Solar System objects.

- `setuptools <https://setuptools.readthedocs.io>`_: Used for discovery of
  entry points which are used to insert fitters into `astropy.modeling.fitting`.

- `mpmath <http://mpmath.org/>`_: Used for the 'kraft-burrows-nousek'
  interval in `~astropy.stats.poisson_conf_interval`.

- `asdf <https://github.com/spacetelescope/asdf>`_ |minimum_asdf_version| or later: Enables the
  serialization of various Astropy classes into a portable, hierarchical,
  human-readable representation.

- `bottleneck <https://pypi.org/project/Bottleneck/>`_: Improves the performance
  of sigma-clipping and other functionality that may require computing
  statistics on arrays with NaN values.

- `certifi <https://pypi.org/project/certifi/>`_: Useful when downloading
  files from HTTPS or FTP+TLS sites in case Python is not able to locate
  up-to-date root CA certificates on your system; this package is usually
  already included in many Python installations (e.g., as a dependency of
  the ``requests`` package).

- `pyarrow <https://arrow.apache.org/docs/python/>`_ |minimum_pyarrow_version| or later:
  To read/write :class:`~astropy.table.Table` objects from/to Parquet files.

However, note that these packages require installation only if those particular
features are needed. ``astropy`` will import even if these dependencies are not
installed.

The following packages can optionally be used when testing:

- `pytest-astropy`_: See :ref:`sourcebuildtest`

- `pytest-xdist <https://pypi.org/project/pytest-xdist/>`_: Used for
  distributed testing.

- `pytest-mpl <https://github.com/matplotlib/pytest-mpl>`_: Used for testing
  with Matplotlib figures.

- `objgraph <https://mg.pov.lt/objgraph/>`_: Used only in tests to test for reference leaks.

- `IPython`_ |minimum_ipython_version| or later:
  Used for testing the notebook interface of `~astropy.table.Table`.

- `coverage <https://coverage.readthedocs.io/>`_: Used for code coverage
  measurements.

- `skyfield <https://rhodesmill.org/skyfield/>`_: Used for testing Solar System
  coordinates.

- `spgp4 <https://pypi.org/project/sgp4/>`_: Used for testing satellite positions.

- `tox <https://tox.readthedocs.io/en/latest/>`_: Used to automate testing
  and documentation builds.

Building from Source
====================

Prerequisites
-------------

You will need a compiler suite and the development headers for Python in order
to build ``astropy``. You do not need to install any other specific build
dependencies (such as `Cython <https://cython.org/>`_ or
`jinja2 <https://jinja.palletsprojects.com/en/master/>`_) since these are
declared in the ``pyproject.toml`` file and will be automatically installed into
a temporary build environment by pip.

Prerequisites for Linux
-----------------------

On Linux, using the package manager for your distribution will usually be the
easiest route to making sure you have the prerequisites to build ``astropy``. In
order to build from source, you will need the Python development
package for your Linux distribution, as well as pip.

For Debian/Ubuntu::

    sudo apt-get install python3-dev python3-numpy-dev python3-setuptools cython3 python3-jinja2 python3-pytest-astropy

For Fedora/RHEL::

    sudo yum install python3-devel python3-numpy python3-setuptools python3-Cython python3-jinja2 python3-pytest-astropy

.. note:: Building the developer version of ``astropy`` may require
          newer versions of the above packages than are available in
          your distribution's repository.  If so, you could either try
          a more up-to-date distribution (such as Debian ``testing``),
          or install more up-to-date versions of the packages using
          ``pip`` or ``conda`` in a virtual environment.

Prerequisites for Mac OS X
--------------------------

On MacOS X you will need the XCode command line tools which can be installed
using::

    xcode-select --install

Follow the onscreen instructions to install the command line tools required.
Note that you do **not** need to install the full XCode distribution (assuming
you are using MacOS X 10.9 or later).

The `instructions for building NumPy from source
<https://numpy.org/doc/stable/user/building.html>`_ are a good
resource for setting up your environment to build Python packages.

Obtaining the Source Packages
-----------------------------

Source Packages
^^^^^^^^^^^^^^^

The latest stable source package for ``astropy`` can be `downloaded here
<https://pypi.org/project/astropy>`_.

Development Repository
^^^^^^^^^^^^^^^^^^^^^^

The latest development version of ``astropy`` can be cloned from GitHub
using this command::

   git clone git://github.com/astropy/astropy.git

If you wish to participate in the development of ``astropy``, see the
:ref:`developer-docs`. The present document covers only the basics necessary to
installing ``astropy``.

Building and Installing
-----------------------

To build and install ``astropy`` (from the root of the source tree)::

    pip install .

If you install in this way and you make changes to the code, you will need to
re-run the install command for changes to be reflected. Alternatively, you can
use::

    pip install -e .

which installs ``astropy`` in develop/editable mode -- this then means that
changes in the code are immediately reflected in the installed version.

Troubleshooting
---------------

If you get an error mentioning that you do not have the correct permissions to
install ``astropy`` into the default ``site-packages`` directory, you can try
installing with::

    pip install . --user

which will install into a default directory in your home directory.

.. _external_c_libraries:

External C Libraries
^^^^^^^^^^^^^^^^^^^^

The ``astropy`` source ships with the C source code of a number of
libraries. By default, these internal copies are used to build
``astropy``. However, if you wish to use the system-wide installation of
one of those libraries, you can set environment variables with the
pattern ``ASTROPY_USE_SYSTEM_???`` to ``1`` when building/installing
the package.

For example, to build ``astropy`` using the system's expat parser
library, use::

    ASTROPY_USE_SYSTEM_EXPAT=1 pip install -e .

To build using all of the system libraries, use::

    ASTROPY_USE_SYSTEM_ALL=1 pip install -e .

The C libraries currently bundled with ``astropy`` include:

- `wcslib <https://www.atnf.csiro.au/people/mcalabre/WCS/>`_ see
  ``cextern/wcslib/README`` for the bundled version. To use the
  system version, set ``ASTROPY_USE_SYSTEM_WCSLIB=1``.

- `cfitsio <https://heasarc.gsfc.nasa.gov/fitsio/fitsio.html>`_ see
  ``cextern/cfitsio/changes.txt`` for the bundled version. To use the
  system version, set ``ASTROPY_USE_SYSTEM_CFITSIO=1``.

- `expat <https://libexpat.github.io/>`_ see ``cextern/expat/README`` for the
  bundled version. To use the system version, set ``ASTROPY_USE_SYSTEM_EXPAT=1``.


Installing ``astropy`` into CASA
--------------------------------

If you want to be able to use ``astropy`` inside `CASA
<https://casa.nrao.edu/>`_, the easiest way is to do so from inside CASA.

First, we need to make sure `pip <https://pip.pypa.io>`__ is
installed. Start up CASA as normal, and then type::

    CASA <2>: from setuptools.command import easy_install

    CASA <3>: easy_install.main(['--user', 'pip'])

Now, quit CASA and re-open it, then type the following to install ``astropy``::

    CASA <2>: import subprocess, sys

    CASA <3>: subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--user', 'astropy'])

Then close CASA again and open it, and you should be able to import ``astropy``::

    CASA <2>: import astropy

Any ``astropy`` affiliated package can be installed the same way (e.g. the
`spectral-cube <https://spectral-cube.readthedocs.io>`_ or other
packages that may be useful for radio astronomy).

.. note:: The above instructions have not been tested on all systems.
   We know of a few examples that do work, but that is not a guarantee
   that this will work on all systems. If you install ``astropy`` and begin to
   encounter issues with CASA, please look at the `known CASA issues
   <https://github.com/astropy/astropy/issues?q=+label%3ACASA-Installation+>`_
   and if you do not encounter your issue there, please post a new one.


Installing pre-built Development Versions of ``astropy``
--------------------------------------------------------

Most nights a development snapshot of ``astropy`` will be compiled.
This is useful if you want to test against a development version of astropy but
do not want to have to build it yourselves. You can see the
`available astropy dev snapshots page <https://dev.azure.com/astropy-project/astropy/_packaging?_a=package&feed=nightly&package=astropy&protocolType=PyPI&view=versions>`_
to find out what is currently being offered.

Installing these "nightlies" of ``astropy`` can be achieved by using ``pip``::

  $ pip install --extra-index-url=https://pkgs.dev.azure.com/astropy-project/astropy/_packaging/nightly/pypi/simple/ --pre astropy

The extra index URL tells ``pip`` to check the ``pip`` index on Azure Pipelines, where the
nightlies are built, and the ``--pre`` command tells ``pip`` to install pre-release
versions (in this case ``.dev`` releases).

.. _builddocs:

Building Documentation
----------------------

.. note::

    Building the documentation is in general not necessary unless you are
    writing new documentation or do not have internet access, because
    the latest (and archive) versions of Astropy's documentation should
    be available at `docs.astropy.org <https://docs.astropy.org>`_ .

Dependencies
^^^^^^^^^^^^

Building the documentation requires the ``astropy`` source code and some
additional packages. The easiest way to build the documentation is to use `tox
<https://tox.readthedocs.io/en/latest/>`_ as detailed in
:ref:`astropy-doc-building`. If you are happy to do this, you can skip the rest
of this section.

On the other hand, if you wish to call Sphinx manually to build the
documentation, you will need to make sure that a number of dependencies are
installed. If you use conda, the easiest way to install the dependencies is
with::

    conda install -c conda-forge sphinx-astropy

Without conda, you install the dependencies by specifying ``[docs]`` when
installing ``astropy`` with pip::

    pip install -e '.[docs]'

You can alternatively install the `sphinx-astropy
<https://github.com/astropy/sphinx-astropy>`_ package with pip::

    pip install sphinx-astropy

In addition to providing configuration common to packages in the Astropy
ecosystem, this package also serves as a way to automatically get the main
dependencies, including:

* `Sphinx <http://www.sphinx-doc.org>`_ - the main package we use to build
  the documentation
* `astropy-sphinx-theme <https://github.com/astropy/astropy-sphinx-theme>`_ -
  the default 'bootstrap' theme used by ``astropy`` and a number of affiliated
  packages
* `sphinx-automodapi <https://sphinx-automodapi.readthedocs.io>`_ - an extension
  that makes it easy to automatically generate API documentation
* `sphinx-gallery <https://sphinx-gallery.readthedocs.io/en/latest/>`_ - an
  extension to generate example galleries
* `numpydoc`_ - an extension to parse
  docstrings in NumPyDoc format
* `pillow <https://pillow.readthedocs.io>`_ - used in one of the examples
* `Graphviz <http://www.graphviz.org>`_ - generate inheritance graphs (available
  as a conda package or a system install but not in pip)

.. Note::
    Both of the ``pip`` install methods above do not include `Graphviz
    <http://www.graphviz.org>`_.  If you do not install this package separately
    then the documentation build process will produce a very large number of
    lengthy warnings (which can obscure bona fide warnings) and also not
    generate inheritance graphs.

.. _astropy-doc-building:

Building
^^^^^^^^

There are two ways to build the Astropy documentation. The easiest way is to
execute the following tox command (from the ``astropy`` source directory)::

    tox -e build_docs

If you do this, you do not need to install any of the documentation dependencies
as this will be done automatically. The documentation will be built in the
``docs/_build/html`` directory, and can be read by pointing a web browser to
``docs/_build/html/index.html``.

Alternatively, you can do::

    cd docs
    make html

And the documentation will be generated in the same location. Note that
this uses the installed version of astropy, so if you want to make sure
the current repository version is used, you will need to install it with
e.g.::

    pip install -e .[docs]

before changing to the ``docs`` directory.

In the second way, LaTeX documentation can be generated by using the command::

    make latex

The LaTeX file ``Astropy.tex`` will be created in the ``docs/_build/latex``
directory, and can be compiled using ``pdflatex``.

Reporting Issues/Requesting Features
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

As mentioned above, building the documentation depends on a number of Sphinx
extensions and other packages. Since it is not always possible to know which
package is causing issues or would need to have a new feature implemented, you
can open an issue in the `core astropy package issue
tracker <https://github.com/astropy/astropy/issues>`_. However, if you wish, you
can also open issues in the repositories for some of the dependencies:

* For requests/issues related to the appearance of the docs (e.g. related to
  the CSS), you can open an issue in the `astropy-sphinx-theme issue tracker
  <https://github.com/astropy/astropy-sphinx-theme/issues>`_.

* For requests/issues related to the auto-generated API docs which appear to
  be general issues rather than an issue with a specific docstring, you can use
  the `sphinx-automodapi issue tracker
  <https://github.com/astropy/sphinx-automodapi/issues>`_.

* For issues related to the default configuration (e.g which extensions are
  enabled by default), you can use the `sphinx-astropy issue tracker
  <https://github.com/astropy/sphinx-astropy/issues>`_.

.. _sourcebuildtest:

Testing a Source Code Build of ``astropy``
------------------------------------------

{% if is_development %}

The easiest way to run the tests in a source checkout of ``astropy``
is to use `tox <https://tox.readthedocs.io/en/latest/>`_::

    tox -e test-alldeps

There are also alternative methods of :ref:`running-tests` if you
would like more control over the testing process.

{%else%}

See the :ref:`latest documentation on how to run the tests in a source
checkout of astropy <astropy-dev:sourcebuildtest>`

{%endif%}
.. _changelog:

**************
Full Changelog
**************

.. changelog::
   :towncrier: ../
   :towncrier-skip-if-empty:
   :changelog_file: ../CHANGES.rst
.. _astropy-cosmology-units-and-equivalencies:

************************************
Cosmological Units and Equivalencies
************************************

.. currentmodule:: astropy.cosmology.units

This package defines and collects cosmological units and equivalencies.
We suggest importing this units package as

    >>> import astropy.cosmology.units as cu


To enable the main :mod:`astropy.units` to access these units when searching
for unit conversions and equivalencies, use
:func:`~astropy.units.add_enabled_units`.

    >>> import astropy.units as u
    >>> u.add_enabled_units(cu)  # doctest: +SKIP


About the Units
===============

.. doctest::
   :hide:

   >>> import astropy.units as u


.. _cosmological-redshift:

Cosmological Redshift and Dimensionless Equivalency
---------------------------------------------------

There are numerous measures of distance in cosmology -- luminosities, CMB
temperature, the universe's age, etc. -- but redshift is the principal measure
from which others are defined. In cosmology, distance measures are commonly
exasperating to follow in a derivation, because they are used interchangeably.
``astropy`` provides the ``redshift`` unit and associated equivalencies to
assist in these derivations and unify the distance measures.

Examples
^^^^^^^^

.. EXAMPLE START: Using redshift-dimensionless equivalency

To convert to or from dimensionless to "redshift" units:

    >>> import astropy.units as u
    >>> import astropy.cosmology.units as cu
    >>> z = 1100 * cu.redshift
    >>> z.to(u.dimensionless_unscaled, equivalencies=cu.dimensionless_redshift())
    <Quantity 1100.>

The equivalency works as part of a quantity with composite units

    >>> q = (2.7 * u.K) * z
    >>> q.to(u.K, equivalencies=cu.dimensionless_redshift())
    <Quantity 2970. K>

Since the redshift is not a true unit and is used so frequently, the
redshift / dimensionless equivalency is actually enabled by default.

    >>> z == 1100 * u.dimensionless_unscaled
    True

    >>> q.to(u.K)
    <Quantity 2970. K>

To temporarily remove the equivalency and enforce unit strictness, use
:func:`astropy.units.set_enabled_equivalencies` as a context.

    >>> with u.set_enabled_equivalencies([]):
    ...     try:
    ...         z.to(u.dimensionless_unscaled)
    ...     except u.UnitConversionError:
    ...         print("equivalency disabled")
    equivalency disabled

.. EXAMPLE END


.. EXAMPLE START: Using `with_redshift` equivalency

The other redshift equivalency is `~astropy.cosmology.units.with_redshift`,
enabling redshift to be converted to other units, like CMB temperature:

    >>> from astropy.cosmology import WMAP9
    >>> z = 1100 * cu.redshift
    >>> z.to(u.K, cu.with_redshift(WMAP9))
    <Quantity 3000.225 K>

or the Hubble parameter:

    >>> z.to(u.km / u.s / u.Mpc, cu.with_redshift(WMAP9))  # doctest: +FLOAT_CMP
    <Quantity 1565637.40154275 km / (Mpc s)>

    >>> z.to(cu.littleh, cu.with_redshift(WMAP9))  # doctest: +FLOAT_CMP
    <Quantity 15656.37401543 littleh>

or a physical distance (comoving, lookback, or luminosity):

.. doctest-requires:: scipy

    >>> z.to(u.Mpc, cu.with_redshift(WMAP9, distance="luminosity"))  # doctest: +FLOAT_CMP
    <Quantity 15418438.76317008 Mpc>

These conversions are cosmology dependent, so if the cosmology changes,
so too will the conversions.

    >>> excosmo = WMAP9.clone(Tcmb0=3.0)
    >>> z.to(u.K, cu.with_redshift(excosmo))
    <Quantity 3303. K>

If no argument is given (or the argument is `None`), this equivalency assumes
the current default |Cosmology|:

    >>> z.to(u.K, cu.with_redshift())
    <Quantity 3000.7755 K>

To use this equivalency in a larger block of code:

    >>> with u.add_enabled_equivalencies(cu.with_redshift()):
    ...     # long derivation here
    ...     z.to(u.K)
    <Quantity 3000.7755 K>

.. EXAMPLE END


.. _littleh-and-H0-equivalency:

Reduced Hubble Constant and "little-h" Equivalency
--------------------------------------------------

The dimensionless version of the Hubble constant  often known as "little h" 
is a frequently used quantity in extragalactic astrophysics. It is also widely
known as the bane of beginners' existence in such fields (See e.g., the title
of `this paper <https://doi.org/10.1017/pasa.2013.31>`__, which also provides
valuable advice on the use of little h). ``astropy`` provides the
:func:`~astropy.cosmology.units.with_H0` equivalency that helps keep this
straight in at least some of these cases, by providing a way to convert to/from
physical to "little h" units.

Examples
^^^^^^^^

.. EXAMPLE START: Using the "little h" Equivalency

To convert to or from physical to "little h" units:

.. code-block:: python

    >>> import astropy.units as u
    >>> import astropy.cosmology.units as cu
    >>> H0_70 = 70 * u.km/u.s/u.Mpc
    >>> distance = 70 * (u.Mpc/cu.littleh)
    >>> distance.to(u.Mpc, cu.with_H0(H0_70))  # doctest: +FLOAT_CMP
    <Quantity 100.0 Mpc>
    >>> luminosity = 0.49 * u.Lsun * cu.littleh**-2
    >>> luminosity.to(u.Lsun, cu.with_H0(H0_70))  # doctest: +FLOAT_CMP
    <Quantity 1.0 solLum>

Note the unit name ``littleh``: while this unit is usually expressed in the
literature as just ``h``, here it is ``littleh`` to avoid confusion with
"hours."

If no argument is given (or the argument is `None`), this equivalency assumes
the ``H0`` from the current default |Cosmology|:

.. code-block:: python

    >>> distance = 100 * (u.Mpc/cu.littleh)
    >>> distance.to(u.Mpc, cu.with_H0())  # doctest: +FLOAT_CMP
    <Quantity 147.79781259 Mpc>

This equivalency also allows a common magnitude formulation of little h
scaling:

.. code-block:: python

    >>> mag_quantity = 12 * (u.mag - u.MagUnit(cu.littleh**2))
    >>> mag_quantity  # doctest: +FLOAT_CMP
    <Magnitude 12. mag(1 / littleh2)>
    >>> mag_quantity.to(u.mag, cu.with_H0(H0_70))  # doctest: +FLOAT_CMP
    <Quantity 11.2254902 mag>

.. EXAMPLE END


Reference/API
=============

.. automodapi:: astropy.cosmology.units
   :inherited-members:
.. _astropy-cosmology:

***********************************************
Cosmological Calculations (`astropy.cosmology`)
***********************************************

.. |wCDM| replace:: :class:`~astropy.cosmology.wCDM`
.. |FlatwCDM| replace:: :class:`~astropy.cosmology.FlatwCDM`
.. |w0wzCDM| replace:: :class:`~astropy.cosmology.w0wzCDM`
.. |w0waCDM| replace:: :class:`~astropy.cosmology.w0waCDM`
.. |wpwaCDM| replace:: :class:`~astropy.cosmology.wpwaCDM`
.. |WMAP7| replace:: :ref:`WMAP7 <astropy:WMAP7>`
.. |WMAP9| replace:: :ref:`WMAP9 <astropy:WMAP9>`
.. |Planck13| replace:: :ref:`Planck13 <astropy:Planck13>`
.. |Planck15| replace:: :ref:`Planck15 <astropy:Planck15>`
.. |z_at_value| replace:: :func:`~astropy.cosmology.z_at_value`

Introduction
============

The :mod:`astropy.cosmology` sub-package contains classes for representing
cosmologies and utility functions for calculating commonly used quantities that
depend on a cosmological model. This includes distances, ages, and lookback
times corresponding to a measured redshift or the transverse separation
corresponding to a measured angular separation.

:mod:`astropy.cosmology.units` extends the :mod:`astropy.units` sub-package,
adding and collecting cosmological units and equivalencies, like :math:`h` for
keeping track of (dimensionless) factors of the Hubble constant.

For details on reading and writing cosmologies from files, see
:ref:`cosmology_io`.

For notes on building custom Cosmology classes and interfacing
:mod:`astropy.cosmology` with 3rd-party packages, see
:ref:`astropy-cosmology-for-developers`.


Getting Started
===============

Cosmological quantities are calculated using methods of a |Cosmology| object.

Examples
--------

..
  EXAMPLE START
  Calculating Cosmological Quantities

To calculate the Hubble constant at z=0 (i.e., ``H0``) and the number of
transverse proper kiloparsecs (kpc) corresponding to an arcminute at z=3::

  >>> from astropy.cosmology import WMAP9 as cosmo
  >>> cosmo.H(0)  # doctest: +FLOAT_CMP
  <Quantity 69.32 km / (Mpc s)>

.. doctest-requires:: scipy

  >>> cosmo.kpc_proper_per_arcmin(3)  # doctest: +FLOAT_CMP
  <Quantity 472.97709620405266 kpc / arcmin>

Here |WMAP9| is a built-in object describing a cosmology with the parameters
from the nine-year WMAP results. Several other built-in cosmologies are also
available (see `Built-in Cosmologies`_). The available methods of the cosmology
object are listed in the methods summary for the |FLRW| class.

All of these methods also accept an arbitrarily-shaped array of redshifts as
input:

.. doctest-requires:: scipy

  >>> import numpy as np
  >>> from astropy.cosmology import WMAP9 as cosmo
  >>> cosmo.comoving_distance(np.array([0.5, 1.0, 1.5]))  # doctest: +FLOAT_CMP
  <Quantity [1916.06941724, 3363.07062107, 4451.7475201 ] Mpc>

You can create your own FLRW-like cosmology using one of the cosmology
classes::

  >>> from astropy.cosmology import FlatLambdaCDM
  >>> cosmo = FlatLambdaCDM(H0=70, Om0=0.3, Tcmb0=2.725)
  >>> cosmo  # doctest: +FLOAT_CMP
  FlatLambdaCDM(H0=70.0 km / (Mpc s), Om0=0.3, Tcmb0=2.725 K,
                Neff=3.04, m_nu=[0. 0. 0.] eV, Ob0=None)

Note the presence of additional cosmological parameters (e.g., ``Neff``, the
number of effective neutrino species) with default values; these can also be
specified explicitly in the call to the constructor.

..
  EXAMPLE END

The cosmology sub-package makes use of :mod:`~astropy.units`, so in many cases
returns values with units attached. Consult the documentation for that
sub-package for more details, but briefly here we will show how to access the
floating point or array values::

  >>> from astropy.cosmology import WMAP9 as cosmo
  >>> H0 = cosmo.H(0)
  >>> H0.value, H0.unit  # doctest: +FLOAT_CMP
  (69.32, Unit("km / (Mpc s)"))


Using `astropy.cosmology`
=========================

More detailed information on using the package is provided on separate pages,
listed below.

.. toctree::
   :maxdepth: 1

   Reading and Writing <io>
   Units and Equivalencies <units>


Most of the functionality is enabled by the |FLRW| object. This represents a
homogeneous and isotropic cosmology (characterized by the
Friedmann-Lemaitre-Robertson-Walker metric, named after the people who solved
Einstein's field equation for this special case). However, you cannot work with
this class directly, as you must specify a dark energy model by using one of
its subclasses instead, such as |FlatLambdaCDM|.

Examples
--------

..
  EXAMPLE START
  Working with the FlatLambdaCDM Class

You can create a new |FlatLambdaCDM| object with arguments giving the Hubble
parameter and Omega matter (both at z=0)::

  >>> from astropy.cosmology import FlatLambdaCDM
  >>> cosmo = FlatLambdaCDM(H0=70, Om0=0.3)
  >>> cosmo
  FlatLambdaCDM(H0=70.0 km / (Mpc s), Om0=0.3, Tcmb0=0.0 K,
                Neff=3.04, m_nu=None, Ob0=None)

This can also be done more explicitly using units, which is recommended::

  >>> from astropy.cosmology import FlatLambdaCDM
  >>> import astropy.units as u
  >>> cosmo = FlatLambdaCDM(H0=70 * u.km / u.s / u.Mpc, Tcmb0=2.725 * u.K, Om0=0.3)


The predefined cosmologies described in the `Getting Started`_ section are
instances of |FlatLambdaCDM|, and have the same methods. So we can find the
luminosity distance to redshift 4 by:

.. doctest-requires:: scipy

  >>> cosmo.luminosity_distance(4)  # doctest: +FLOAT_CMP
  <Quantity 35842.353618623194 Mpc>

Or the age of the universe at z = 0:

.. doctest-requires:: scipy

  >>> cosmo.age(0)  # doctest: +FLOAT_CMP
  <Quantity 13.461701658024014 Gyr>

They also accept arrays of redshifts:

.. doctest-requires:: scipy

  >>> import astropy.cosmology.units as cu
  >>> cosmo.age([0.5, 1, 1.5] * cu.redshift)  # doctest: +FLOAT_CMP
  <Quantity [8.42128013, 5.74698021, 4.19645373] Gyr>

See the |FLRW| and |FlatLambdaCDM| object docstring for all of the methods and
attributes available.

..
  EXAMPLE END

..
  EXAMPLE START
  Working with Non-flat Universes with the LambdaCDM Class

In addition to flat universes, non-flat varieties are supported, such as
|LambdaCDM|. A variety of standard cosmologies with the parameters already
defined are also available (see `Built-in Cosmologies`_)
::

  >>> from astropy.cosmology import WMAP7   # WMAP 7-year cosmology
  >>> WMAP7.critical_density(0)  # critical density at z = 0  # doctest: +FLOAT_CMP
  <Quantity 9.31000324385361e-30 g / cm3>

You can see how the density parameters evolve with redshift as well::

  >>> import numpy as np
  >>> from astropy.cosmology import WMAP7   # WMAP 7-year cosmology
  >>> WMAP7.Om(np.array([0, 1.0, 2.0]))  # doctest: +FLOAT_CMP
  array([0.272     , 0.74898522, 0.90905234])
  >>> WMAP7.Ode(np.array([0., 1.0, 2.0]))  # doctest: +FLOAT_CMP
  array([0.72791572, 0.2505506 , 0.0901026 ])

Note that these do not quite add up to one, even though |WMAP7| assumes a flat
universe, because photons and neutrinos are included. Also note that the
density parameters are unitless and so are not |Quantity| objects.

It is possible to specify the baryonic matter density at redshift zero at class
instantiation by passing the keyword argument ``Ob0``::

  >>> from astropy.cosmology import FlatLambdaCDM
  >>> cosmo = FlatLambdaCDM(H0=70, Om0=0.3, Ob0=0.05)
  >>> cosmo
  FlatLambdaCDM(H0=70.0 km / (Mpc s), Om0=0.3, Tcmb0=0.0 K,
                Neff=3.04, m_nu=None, Ob0=0.05)

In this case the dark matter-only density at redshift 0 is available as class
attribute ``Odm0`` and the redshift evolution of dark and baryonic matter
densities can be computed using the methods ``Odm`` and ``Ob``, respectively.
If ``Ob0`` is not specified at class instantiation, it defaults to ``None`` and
any method relying on it being specified will raise a ``ValueError``:

  >>> from astropy.cosmology import FlatLambdaCDM
  >>> cosmo = FlatLambdaCDM(H0=70, Om0=0.3)
  >>> cosmo.Odm(1)
  Traceback (most recent call last):
  ...
  ValueError: Baryonic density not set for this cosmology, unclear
  meaning of dark matter density

Cosmological instances have an optional ``name`` attribute which can be used to
describe the cosmology::

  >>> from astropy.cosmology import FlatwCDM
  >>> cosmo = FlatwCDM(name='SNLS3+WMAP7', H0=71.58, Om0=0.262, w0=-1.016)
  >>> cosmo
  FlatwCDM(name="SNLS3+WMAP7", H0=71.58 km / (Mpc s), Om0=0.262,
           w0=-1.016, Tcmb0=0.0 K, Neff=3.04, m_nu=None, Ob0=None)

..
  EXAMPLE END

This is also an example with a different model for dark energy: a flat universe
with a constant dark energy equation of state, but not necessarily a
cosmological constant. A variety of additional dark energy models are also
supported (see `Specifying a dark energy model`_).

An important point is that the cosmological parameters of each instance are
immutable  that is, if you want to change, say, ``Om``, you need to make a new
instance of the class. To make this more convenient, a
:meth:`~astropy.cosmology.Cosmology.clone` operation is provided, which allows
you to make a copy with specified values changed. Note that you cannot change
the type of cosmology with this operation (e.g., flat to non-flat).

..
  EXAMPLE START
  Making New Cosmology Instances with the .clone() Method

To make a copy of a cosmological instance using the ``clone`` operation:

  >>> from astropy.cosmology import WMAP9
  >>> newcosmo = WMAP9.clone(name='WMAP9 modified', Om0=0.3141)
  >>> WMAP9.H0, newcosmo.H0  # some values unchanged  # doctest: +FLOAT_CMP
  (<Quantity 69.32 km / (Mpc s)>, <Quantity 69.32 km / (Mpc s)>)
  >>> WMAP9.Om0, newcosmo.Om0  # some changed  # doctest: +FLOAT_CMP
  (0.2865, 0.3141)
  >>> WMAP9.Ode0, newcosmo.Ode0  # Indirectly changed since this is flat  # doctest: +FLOAT_CMP
  (0.7134130719051658, 0.6858130719051657)

..
  EXAMPLE END

Finding the Redshift at a Given Value of a Cosmological Quantity
----------------------------------------------------------------

If you know a cosmological quantity and you want to know the redshift which it
corresponds to, you can use |z_at_value|.

Example
^^^^^^^

..
  EXAMPLE START
  Compute the Redshift at a Given Universe Age

To find the redshift using ``z_at_value``:

.. doctest-requires:: scipy

  >>> import astropy.units as u
  >>> from astropy.cosmology import Planck13, z_at_value
  >>> z_at_value(Planck13.age, 2 * u.Gyr)  # doctest: +FLOAT_CMP
  <Quantity 3.19812061 redshift>

..
  EXAMPLE END

For some quantities, there can be more than one redshift that satisfies a value.
In this case you can use the ``zmin`` and ``zmax`` keywords to restrict the
search range or set ``bracket`` to initialize it in the desired domain. See the
|z_at_value| docstring for more detailed usage examples.


Built-in Cosmologies
--------------------

A number of preloaded cosmologies are available from analyses using
the WMAP and Planck satellite data. For example:

.. doctest-requires:: scipy

  >>> from astropy.cosmology import Planck13  # Planck 2013
  >>> Planck13.lookback_time(2)  # lookback time in Gyr at z=2  # doctest: +FLOAT_CMP
  <Quantity 10.51184138 Gyr>

A full list of the predefined cosmologies is given by
``cosmology.realizations.available`` and summarized below:

===========  ============================== ====  ===== =======
Name         Source                         H0    Om    Flat
===========  ============================== ====  ===== =======
_`WMAP1`     Spergel et al. 2003            72.0  0.257 Yes
_`WMAP3`     Spergel et al. 2007            70.1  0.276 Yes
_`WMAP5`     Komatsu et al. 2009            70.2  0.277 Yes
_`WMAP7`     Komatsu et al. 2011            70.4  0.272 Yes
_`WMAP9`     Hinshaw et al. 2013            69.3  0.287 Yes
_`Planck13`  Planck Collab 2013, Paper XVI  67.8  0.307 Yes
_`Planck15`  Planck Collab 2015, Paper XIII 67.7  0.307 Yes
_`Planck18`  Planck Collab 2018, Paper VI   67.7  0.310 Yes
===========  ============================== ====  ===== =======

.. note::

  Unlike the Planck 2015 paper, the Planck 2018 paper includes massive
  neutrinos in ``Om0`` but the Planck18 object includes them in ``m_nu``
  instead for consistency. Hence, the ``Om0`` value in Planck18 differs
  slightly from the Planck 2018 paper but represents the same cosmological
  model.

Currently, all are instances of |FlatLambdaCDM|. More details about exactly
where each set of parameters comes from are available in the docstring for each
object::

  >>> from astropy.cosmology import WMAP7
  >>> print(WMAP7.__doc__)
  WMAP7 instance of FlatLambdaCDM cosmology
  (from Komatsu et al. 2011, ApJS, 192, 18, doi: 10.1088/0067-0049/192/2/18.
   Table 1 (WMAP + BAO + H0 ML).)


Specifying a Dark Energy Model
------------------------------

Along with the standard |FlatLambdaCDM| model described above, a number of
additional dark energy models are provided. |FlatLambdaCDM| and |LambdaCDM|
assume that dark energy is a cosmological constant, and should be the most
commonly used cases; the former assumes a flat universe, the latter allows for
spatial curvature. |FlatwCDM| and |wCDM| assume a constant dark energy equation
of state parameterized by :math:`w_{0}`. Two forms of a variable dark energy
equation of state are provided: the simple first order linear expansion
:math:`w(z) = w_{0} + w_{z} z` by |w0wzCDM|, as well as the common CPL form by
|w0waCDM|: :math:`w(z) = w_{0} + w_{a} (1 - a) = w_{0} + w_{a} z / (1 + z)`
and its generalization to include a pivot redshift by |wpwaCDM|:
:math:`w(z) = w_{p} + w_{a} (a_{p} - a)`.

Users can specify their own equation of state by subclassing |FLRW|. See the
provided subclasses for examples. It is advisable to stick to subclassing
|FLRW| rather than one of its subclasses, since some of them use internal
optimizations that also need to be propagated to any  subclasses. Users wishing
to use similar tricks (which can make distance calculations much faster) should
consult the cosmology module source code for details.

Photons and Neutrinos
---------------------

The cosmology classes (can) include the contribution to the energy density from
both photons and neutrinos. By default, the latter are assumed massless. The
three parameters controlling the properties of these species, which are
arguments to the initializers of all of the cosmological classes, are ``Tcmb0``
(the temperature of the cosmic microwave background at z=0), ``Neff`` (the
effective number of neutrino species), and ``m_nu`` (the rest mass of the
neutrino species). ``Tcmb0`` and ``m_nu`` should be expressed as unit
Quantities. All three have standard default values  0 K, 3.04, and 0 eV,
respectively. (The reason that ``Neff`` is not 3 has to do primarily with a
small bump in the neutrino energy spectrum due to electron-positron
annihilation, but is also affected by weak interaction physics.) Setting the
CMB temperature to 0 removes the contribution of both neutrinos and photons.
This is the default to ensure these components are excluded unless the user
explicitly requests them.

Massive neutrinos are treated using the approach described in the
WMAP seven-year cosmology paper (Komatsu et al. 2011, ApJS, 192, 18, section
3.3). This is not the simple
:math:`\Omega_{\nu 0} h^2 = \sum_i m_{\nu\, i} / 93.04\,\mathrm{eV}`
approximation. Also note that the values of :math:`\Omega_{\nu}(z)` include
both the kinetic energy and the rest mass energy components, and that the
|Planck13| and |Planck15| cosmologies include a single species of neutrinos
with non-zero mass (which is not included in :math:`\Omega_{m0}`).

Adding massive neutrinos can have significant performance implications. In
particular, the computation of distance measures and lookback times are factors
of three to four times slower than in the massless neutrino case. Therefore, if
you need to compute many distances in such a cosmology and performance is
critical, it is particularly useful to calculate them on a grid and use
interpolation.

Examples
^^^^^^^^

..
  EXAMPLE START
  Calculating the Contribution of Photons and Neutrinos to the Energy Density

The contribution of photons and neutrinos to the total mass-energy density can
be found as a function of redshift::

  >>> from astropy.cosmology import WMAP7   # WMAP 7-year cosmology
  >>> WMAP7.Ogamma0, WMAP7.Onu0  # Current epoch values  # doctest: +FLOAT_CMP
  (4.985694972799396e-05, 3.442154948307989e-05)
  >>> z = np.array([0, 1.0, 2.0])
  >>> WMAP7.Ogamma(z), WMAP7.Onu(z)  # doctest: +FLOAT_CMP
  (array([4.98603986e-05, 2.74593395e-04, 4.99915942e-04]),
   array([3.44239306e-05, 1.89580995e-04, 3.45145089e-04]))

If you want to exclude photons and neutrinos from your calculations, you can
set ``Tcmb0`` to 0 (which is also the default)::

  >>> from astropy.cosmology import FlatLambdaCDM
  >>> import astropy.units as u
  >>> cos = FlatLambdaCDM(70.4 * u.km / u.s / u.Mpc, 0.272, Tcmb0 = 0.0 * u.K)
  >>> cos.Ogamma0, cos.Onu0
  (0.0, 0.0)

You can include photons but exclude any contributions from neutrinos by setting
``Tcmb0`` to be non-zero (2.725 K is the standard value for our Universe) but
setting ``Neff`` to 0::

  >>> from astropy.cosmology import FlatLambdaCDM
  >>> cos = FlatLambdaCDM(70.4, 0.272, Tcmb0=2.725, Neff=0)
  >>> cos.Ogamma(np.array([0, 1, 2]))  # Photons are still present  # doctest: +FLOAT_CMP
  array([4.98603986e-05, 2.74642208e-04, 5.00086413e-04])
  >>> cos.Onu(np.array([0, 1, 2]))  # But not neutrinos  # doctest: +FLOAT_CMP
  array([0., 0., 0.])

The number of neutrino species is assumed to be the floor of ``Neff``, which in
the default case is ``Neff=3``. Therefore, if non-zero neutrino masses are
desired, then three masses should be provided. However, if only one value is
provided, all of the species are assumed to have the same mass. ``Neff`` is
assumed to be shared equally between each species.

::

  >>> from astropy.cosmology import FlatLambdaCDM
  >>> import astropy.units as u
  >>> H0 = 70.4 * u.km / u.s / u.Mpc
  >>> m_nu = 0 * u.eV
  >>> cosmo = FlatLambdaCDM(H0, 0.272, Tcmb0=2.725, m_nu=m_nu)
  >>> cosmo.has_massive_nu
  False
  >>> cosmo.m_nu  # doctest: +FLOAT_CMP
  <Quantity [0., 0., 0.] eV>
  >>> m_nu = [0.0, 0.05, 0.10] * u.eV
  >>> cosmo = FlatLambdaCDM(H0, 0.272, Tcmb0=2.725, m_nu=m_nu)
  >>> cosmo.has_massive_nu
  True
  >>> cosmo.m_nu  # doctest: +FLOAT_CMP
  <Quantity [0.  , 0.05, 0.1 ] eV>
  >>> cosmo.Onu(np.array([0, 1.0, 15.0]))  # doctest: +FLOAT_CMP
  array([0.00327011, 0.00896845, 0.01257946])
  >>> cosmo.Onu(1) * cosmo.critical_density(1)  # doctest: +FLOAT_CMP
  <Quantity 2.444380380370406e-31 g / cm3>

While these examples used |FlatLambdaCDM|, the above examples also apply for
all of the other cosmology classes.

..
  EXAMPLE END

See Also
========

* Hogg, "Distance measures in cosmology",
  https://arxiv.org/abs/astro-ph/9905116
* Linder, "Exploring the Expansion History of the Universe", https://arxiv.org/abs/astro-ph/0208512
* NASA's Legacy Archive for Microwave Background Data Analysis,
  https://lambda.gsfc.nasa.gov/

Range of Validity and Reliability
=================================

The code in this sub-package is tested against several widely used online
cosmology calculators and has been used to perform many calculations in
refereed papers. You can check the range of redshifts over which the code is
regularly tested in the module ``astropy.cosmology.tests.test_cosmology``. If
you find any bugs, please let us know by `opening an issue at the GitHub
repository <https://github.com/astropy/astropy/issues>`_!

A more difficult question is the range of redshifts over which the code is
expected to return valid results. This is necessarily model-dependent, but in
general you should not expect the numeric results to be well behaved for
redshifts more than a few times larger than the epoch of matter-radiation
equality (so, for typical models, not above z = 5-6,000, but for some models
much lower redshifts may be ill-behaved). In particular, you should pay
attention to warnings from the :mod:`scipy.integrate` package about integrals
failing to converge (which may only be issued once per session).

The built-in cosmologies use the parameters as listed in the respective papers.
These provide only a limited range of precision, and so you should not expect
derived quantities to match beyond that precision. For example, the Planck 2013
and 2015 results only provide the Hubble constant to four digits. Therefore,
they should not be expected to match the age quoted by the Planck team to
better than that, despite the fact that five digits are quoted in the papers.

Reference/API
=============

More detailed information on using the package is provided on separate pages,
listed below.

.. toctree::
   :maxdepth: 1

   Reading and Writing <io>
   For Developers <dev>


.. automodapi:: astropy.cosmology
   :inherited-members:
.. _cosmology_io:

Read, Write, and Convert Cosmology Objects
******************************************

For *temporary* storage an easy means to serialize and deserialize a Cosmology
object is using the :mod:`pickle` module. This is good for e.g. passing a
|Cosmology| between threads.

.. doctest-skip::

   >>> import pickle
   >>> from astropy.cosmology import Planck18
   >>> with open("planck18.pkl", mode="wb") as file:
   ...     pickle.dump(Planck18, file)
   >>> # and to read back
   >>> with open("planck18.pkl", mode="rb") as file:
   ...     cosmo = pickle.load(file)
   >>> cosmo
   FlatLambdaCDM(name="Planck18", ...

However this method has all the attendant drawbacks of :mod:`pickle`  security
vulnerabilities and non-human-readable files. Pickle files just generally don't
make for good persistent storage.

Solving both these issues, ``astropy`` provides a unified interface for reading
and writing data in different formats.


Getting Started
===============

The |Cosmology| class includes two methods, |Cosmology.read| and
|Cosmology.write|, that make it possible to read from and write to files.

Currently the only registered ``read`` / ``write`` format is "ascii.ecsv",
like for Table. Also, custom ``read`` / ``write`` formats may be registered
into the Astropy Cosmology I/O framework.

Writing a cosmology instance requires only the file location and optionally,
if the file format cannot be inferred, a keyword argument "format". Additional
positional arguments and keyword arguments are passed to the reader methods.

.. doctest-skip::

    >>> from astropy.cosmology import Planck18
    >>> Planck18.write("example_cosmology.ecsv", format="ascii.ecsv")

Reading back the cosmology is most safely done from |Cosmology|, the base
class, as it provides no default information and therefore requires the file
to have all necessary information to describe a cosmology.

.. doctest-skip::

    >>> from astropy.cosmology import Cosmology
    >>> cosmo = Cosmology.read("example_cosmology.ecsv", format="ascii.ecsv")
    >>> cosmo == Planck18
    True

To see a list of the available read/write file formats:

.. code-block:: python

    >>> from astropy.cosmology import Cosmology
    >>> Cosmology.read.list_formats()
      Format   Read Write Auto-identify
    ---------- ---- ----- -------------
    ascii.ecsv  Yes   Yes           Yes
      myformat  Yes   Yes           Yes

This list will include both built-in and registered 3rd-party formats.
"myformat" is from an `example 3rd-party package
<https://github.com/astropy/astropy/tree/main/astropy/cosmology/tests/mypackage>`_.

When a subclass of |Cosmology| is used to read a file, the subclass will provide
a keyword argument ``cosmology=<class>`` to the registered read method. The
method uses this cosmology class, regardless of the class indicated in the
file, and sets parameters' default values from the class' signature.

.. doctest-skip::

    >>> from astropy.cosmology import FlatLambdaCDM
    >>> cosmo = FlatLambdaCDM.read('<file name>')
    >>> cosmo == Planck18
    True

Reading and writing |Cosmology| objects go through intermediate
representations, often a dict or |QTable| instance. These intermediate
representations are accessible through the methods |Cosmology.to_format| /
|Cosmology.from_format|.

To see the a list of the available conversion formats:

.. code-block:: python

    >>> from astropy.cosmology import Cosmology
    >>> Cosmology.to_format.list_formats()
          Format      Read Write Auto-identify
    ----------------- ---- ----- -------------
    astropy.cosmology  Yes   Yes           Yes
        astropy.model  Yes   Yes           Yes
          astropy.row  Yes   Yes           Yes
        astropy.table  Yes   Yes           Yes
              mapping  Yes   Yes           Yes
            mypackage  Yes   Yes           Yes
                 yaml  Yes   Yes            No

This list will include both built-in and registered 3rd-party formats.
For instance, in the above, "mapping" is built-in while "mypackage" and
is from an `example 3rd-party package
<https://github.com/astropy/astropy/tree/main/astropy/cosmology/tests/mypackage>`_.

|Cosmology.to_format| / |Cosmology.from_format| parse a Cosmology to/from
another python object. This can be useful for e.g., iterating through an MCMC
of cosmological parameters or printing out a cosmological model to a journal
format, like latex or HTML. When 3rd party cosmology packages register with
Astropy's Cosmology I/O, ``to/from_format`` can be used to convert cosmology
instances between packages!

.. EXAMPLE START: Planck18 to mapping and back

.. code-block::

    >>> from astropy.cosmology import Planck18
    >>> cm = Planck18.to_format("mapping")
    >>> cm
    {'cosmology': <class 'astropy.cosmology.flrw.FlatLambdaCDM'>,
     'name': 'Planck18',
     'H0': <Quantity 67.66 km / (Mpc s)>,
     'Om0': 0.30966,
     ...

Now this dict can be used to load a new cosmological instance identical
to the |Planck18| cosmology from which it was created.

.. code-block::

    >>> from astropy.cosmology import Cosmology
    >>> cosmo = Cosmology.from_format(cm, format="mapping")
    >>> cosmo == Planck18
    True

.. EXAMPLE END

.. EXAMPLE START: Planck18 to QTable and back

Another pre-registered format is "table", for converting a |Cosmology| to and
from a |QTable|.

.. code-block::

    >>> ct = Planck18.to_format("astropy.table")
    >>> ct
    <QTable length=1>
      name        H0        Om0    Tcmb0    Neff    m_nu [3]    Ob0
             km / (Mpc s)            K                 eV
      str8     float64    float64 float64 float64   float64   float64
    -------- ------------ ------- ------- ------- ----------- -------
    Planck18        67.66 0.30966  2.7255   3.046 0.0 .. 0.06 0.04897

Cosmology supports the astropy Table-like protocol (see
:ref:`Table-like Objects`) to the same effect:

.. code-block::

    >>> from astropy.table import QTable
    >>> ct = QTable(Planck18)
    >>> ct
    <QTable length=1>
      name        H0        Om0    Tcmb0    Neff    m_nu [3]    Ob0
             km / (Mpc s)            K                 eV
      str8     float64    float64 float64 float64   float64   float64
    -------- ------------ ------- ------- ------- ----------- -------
    Planck18        67.66 0.30966  2.7255   3.046 0.0 .. 0.06 0.04897

Now this |QTable| can be used to load a new cosmological instance identical to
the |Planck18| cosmology from which it was created.

.. code-block::

    >>> cosmo = Cosmology.from_format(ct, format="astropy.table")
    >>> cosmo
    FlatLambdaCDM(name="Planck18", H0=67.66 km / (Mpc s), Om0=0.30966,
                  Tcmb0=2.7255 K, Neff=3.046, m_nu=[0. 0. 0.06] eV, Ob0=0.04897)

Perhaps more usefully, |QTable| can be saved to ``latex`` and ``html`` formats,
which can be copied into journal articles and websites, respectively.

.. EXAMPLE END

.. EXAMPLE START: Planck18 to Model and back

Using ``format="astropy.model"`` any redshift(s) method of a cosmology may be
turned into a :class:`astropy.modeling.Model`. Each |Cosmology|
:class:`~astropy.cosmology.Parameter` is converted to a
:class:`astropy.modeling.Model` :class:`~astropy.modeling.Parameter`
and the redshift-method to the model's ``__call__ / evaluate``.
Now you can fit cosmologies with data!

.. code-block::

    >>> model = Planck18.to_format("astropy.model", method="lookback_time")
    >>> model
    <FlatLambdaCDMCosmologyLookbackTimeModel(H0=67.66 km / (Mpc s), Om0=0.30966,
        Tcmb0=2.7255 K, Neff=3.046, m_nu=[0.  , 0.  , 0.06] eV, Ob0=0.04897,
        name='Planck18')>

Like for the other formats, the |Planck18| cosmology can be recovered with
|Cosmology.from_format|.


.. _custom_cosmology_converters:

Custom Cosmology To/From Formats
================================

Custom representation formats may also be registered into the Astropy Cosmology
I/O framework for use by these methods. For details of the framework see
:ref:`io_registry`. Note |Cosmology| ``to/from_format`` uses a custom registry,
available at ``Cosmology.<to/from>_format.registry``.

.. EXAMPLE START : custom to/from format

As an example, the following is an implementation of an |Row| converter. We can
and should use inbuilt parsers, like |QTable|, but to show a more complete
example we limit ourselves to only the "mapping" parser.

We start by defining the function to parse a |Row| into a |Cosmology|. This
function should take 1 positional argument, the row object, and 2 keyword
arguments, for how to handle extra metadata and which Cosmology class to use.
Details about metadata treatment are in
``Cosmology.from_format.help("mapping")``.

.. code-block:: python

    >>> import copy
    >>> from astropy.cosmology import Cosmology

    >>> def from_table_row(row, *, move_to_meta=False, cosmology=None):
    ...     # get name from column
    ...     name = row['name'] if 'name' in row.columns else None
    ...     meta = copy.deepcopy(row.meta)
    ...     # turn row into mapping (dict of the arguments)
    ...     mapping = dict(row)
    ...     mapping['name'] = name
    ...     mapping.setdefault("cosmology", meta.pop("cosmology", None))
    ...     mapping["meta"] = meta
    ...     # build cosmology from map
    ...     return Cosmology.from_format(mapping, move_to_meta=move_to_meta,
    ...                                  cosmology=cosmology)

The next step is a function to perform the reverse operation: parse a
|Cosmology| into a |Row|. This function requires only the cosmology object and
a ``*args`` to absorb unneeded information passed by
:class:`astropy.io.registry.UnifiedReadWrite` (which implements
|Cosmology.to_format|).

.. code-block:: python

    >>> from astropy.table import QTable

    >>> def to_table_row(cosmology, *args):
    ...     p = cosmology.to_format("mapping", cosmology_as_str=True)
    ...     meta = p.pop("meta")
    ...     # package parameters into lists for Table parsing
    ...     params = {k: [v] for k, v in p.items()}
    ...     return QTable(params, meta=meta)[0]  # return row

Last we write a function to help with format auto-identification and then
register everything into `astropy.io.registry`.

.. code-block:: python

    >>> from astropy.cosmology import Cosmology
    >>> from astropy.cosmology.connect import convert_registry
    >>> from astropy.table import Row

    >>> def row_identify(origin, format, *args, **kwargs):
    ...     """Identify if object uses the Table format."""
    ...     if origin == "read":
    ...         return isinstance(args[1], Row) and (format in (None, "astropy.row"))
    ...     return False

    >>> # These exact functions are already registered in astropy
    >>> # convert_registry.register_reader("astropy.row", Cosmology, from_table_row)
    >>> # convert_registry.register_writer("astropy.row", Cosmology, to_table_row)
    >>> # convert_registry.register_identifier("astropy.row", Cosmology, row_identify)

Now the registered functions can be used in |Cosmology.from_format| and
|Cosmology.to_format|.

.. code-block:: python

    >>> from astropy.cosmology import Planck18
    >>> row = Planck18.to_format("astropy.row")
    >>> row
    <Row index=0>
      cosmology     name        H0        Om0    Tcmb0    Neff    m_nu [3]    Ob0
                           km / (Mpc s)            K                 eV
        str13       str8     float64    float64 float64 float64   float64   float64
    ------------- -------- ------------ ------- ------- ------- ----------- -------
    FlatLambdaCDM Planck18        67.66 0.30966  2.7255   3.046 0.0 .. 0.06 0.04897

    >>> cosmo = Cosmology.from_format(row)
    >>> cosmo == Planck18  # test it round-trips
    True

.. EXAMPLE END


.. _custom_cosmology_readers_writers:

Custom Cosmology Readers/Writers
================================

Custom ``read`` / ``write`` formats may be registered into the Astropy
Cosmology I/O framework. For details of the framework see :ref:`io_registry`.
Note |Cosmology| ``read/write`` uses a custom registry, available at
``Cosmology.<read/write>.registry``.

.. EXAMPLE START : custom read/write

As an example, in the following we will fully work out a |Cosmology| <-> JSON
(de)serializer. Note that we can use other registered parsers -- here "mapping"
-- to make the implementation much simpler.

We start by defining the function to parse JSON into a |Cosmology|. This
function should take 1 positional argument, the file object or file path. We
will also pass kwargs through to |Cosmology.from_format|, which handles
metadata and which Cosmology class to use. Details of are in
``Cosmology.from_format.help("mapping")``.

.. code-block:: python

    >>> import json, os
    >>> import astropy.units as u
    >>> from astropy.cosmology import Cosmology

    >>> def read_json(filename, **kwargs):
    ...     # read file, from path-like or file-like
    ...     if isinstance(filename, (str, bytes, os.PathLike)):
    ...         with open(filename, "r") as file:
    ...             data = file.read()
    ...     else:  # file-like : this also handles errors in dumping
    ...         data = filename.read()
    ...     mapping = json.loads(data)  # parse json mappable to dict
    ...     # deserialize Quantity
    ...     for k, v in mapping.items():
    ...         if isinstance(v, dict) and "value" in v and "unit" in v:
    ...             mapping[k] = u.Quantity(v["value"], v["unit"])
    ...     for k, v in mapping.get("meta", {}).items():  # also the metadata
    ...         if isinstance(v, dict) and "value" in v and "unit" in v:
    ...             mapping["meta"][k] = u.Quantity(v["value"], v["unit"])
    ...     return Cosmology.from_format(mapping, **kwargs)


The next step is a function to write a |Cosmology| to JSON. This function
requires the cosmology object and a file object/path. We also require the
boolean flag "overwrite" to set behavior for existing files. Note that
|Quantity| is not natively compatible with JSON. In both the ``write`` and
``read`` methods we have to create custom parsers.

.. code-block:: python

    >>> def write_json(cosmology, file, *, overwrite=False, **kwargs):
    ...    data = cosmology.to_format("mapping", cosmology_as_str=True)  # start by turning into dict
    ...    # serialize Quantity
    ...    for k, v in data.items():
    ...        if isinstance(v, u.Quantity):
    ...            data[k] = {"value": v.value.tolist(), "unit": str(v.unit)}
    ...    for k, v in data.get("meta", {}).items():  # also serialize the metadata
    ...        if isinstance(v, u.Quantity):
    ...            data["meta"][k] = {"value": v.value.tolist(), "unit": str(v.unit)}
    ...
    ...    if isinstance(file, (str, bytes, os.PathLike)):
    ...        # check that file exists and whether to overwrite.
    ...        if os.path.exists(file) and not overwrite:
    ...            raise IOError(f"{file} exists. Set 'overwrite' to write over.")
    ...        with open(file, "w") as write_file:
    ...            json.dump(data, write_file)
    ...    else:
    ...        json.dump(data, file)

Last we write a function to help with format auto-identification and then
register everything into :mod:`astropy.io.registry`.

.. code-block:: python

    >>> from astropy.cosmology.connect import readwrite_registry

    >>> def json_identify(origin, filepath, fileobj, *args, **kwargs):
    ...     """Identify if object uses the JSON format."""
    ...     return filepath is not None and filepath.endswith(".json")

    >>> readwrite_registry.register_reader("json", Cosmology, read_json)
    >>> readwrite_registry.register_writer("json", Cosmology, write_json)
    >>> readwrite_registry.register_identifier("json", Cosmology, json_identify)

Now the registered functions can be used in |Cosmology.read| and
|Cosmology.write|.

.. doctest-skip:: win32

    >>> import tempfile
    >>> from astropy.cosmology import Planck18
    >>>
    >>> file = tempfile.NamedTemporaryFile()
    >>> Planck18.write(file.name, format="json", overwrite=True)
    >>> with open(file.name) as f: f.readlines()
    ['{"cosmology": "FlatLambdaCDM", "name": "Planck18",
       "H0": {"value": 67.66, "unit": "km / (Mpc s)"}, "Om0": 0.30966,
       ...
    >>>
    >>> cosmo = Cosmology.read(file.name, format="json")
    >>> file.close()
    >>> cosmo == Planck18  # test it round-trips
    True


.. doctest::
    :hide:

    >>> from astropy.io.registry import IORegistryError
    >>> readwrite_registry.unregister_reader("json", Cosmology)
    >>> readwrite_registry.unregister_writer("json", Cosmology)
    >>> readwrite_registry.unregister_identifier("json", Cosmology)
    >>> try:
    ...     readwrite_registry.get_reader("json", Cosmology)
    ... except IORegistryError:
    ...     pass

.. EXAMPLE END


Reference/API
=============

.. automodapi:: astropy.cosmology.connect

.. automodapi:: astropy.cosmology.io.mapping
.. _astropy-cosmology-for-developers:

Cosmology For Developers
************************

Cosmologies in Functions
========================

It is often useful to assume a default cosmology so that the exact cosmology
does not have to be specified every time a function or method is called. In
this case, it is possible to specify a "default" cosmology.

You can set the default cosmology to a predefined value by using the
"default_cosmology" option in the ``[cosmology.core]`` section of the
configuration file (see :ref:`astropy_config`). Alternatively, you can use the
:meth:`~astropy.cosmology.default_cosmology.set` function of
|default_cosmology| to set a cosmology for the current Python session. If you
have not set a default cosmology using one of the methods described above, then
the cosmology module will default to using the
``default_cosmology._value`` parameters.

You can override the default cosmology through the |default_cosmology| science
state object, using something like the following:

.. code-block:: python

    from astropy.cosmology import default_cosmology

    def myfunc(..., cosmo=None):
        if cosmo is None:
            cosmo = default_cosmology.get()

        ... function code here ...

This ensures that all code consistently uses the default cosmology unless
explicitly overridden.

.. note::

    If you are preparing a paper and thus need to ensure your code provides
    reproducible results, it is better to use an explicit cosmology (for
    example ``WMAP9.H(0)`` instead of ``default_cosmology.get().H(0)``).
    Use of the default cosmology should generally be reserved for code that
    allows for the global cosmology state to be changed; e.g. code included in
    ``astropy`` core or an affiliated package.


.. _astropy-cosmology-custom:

Custom Cosmologies
==================

In :mod:`astropy.cosmology` cosmologies are classes, so custom cosmologies may
be implemented by subclassing |Cosmology| (or more likely |FLRW|) and adding
details specific to that cosmology. Here we will review some of those details
and tips and tricks to building a performant cosmology class.

.. code-block:: python

    from astropy.cosmology import FLRW

    class CustomCosmology(FLRW):
        ...  # [details discussed below]


.. _astropy-cosmology-custom-parameters:

Parameters
----------

.. |Parameter| replace:: :class:`~astropy.cosmology.Parameter`

An astropy |Cosmology| is characterized by 1) its class, which encodes the
physics, and 2) its free parameter(s), which specify a cosmological realization.
When defining the former, all parameters must be declared using |Parameter| and
should have values assigned at instantiation.

A |Parameter| is a `descriptor <https://docs.python.org/3/howto/descriptor.html>`_.
When accessed from a class it transparently stores information, like the units
and accepted equivalencies, that might be opaquely contained in the constructor
signature or more deeply in the code. On a cosmology instance, the descriptor
will return the parameter value.

There are a number of best practices. For a reference, this is excerpted from
the definition of |FLRW|.

.. code-block:: python

    class FLRW(Cosmology):

        H0 = Parameter(doc="Hubble constant as an `~astropy.units.Quantity` at z=0",
                       unit="km/(s Mpc)", fvalidate="scalar")
        Om0 = Parameter(doc="Omega matter; matter density/critical density at z=0",
                        fvalidate="non-negative")
        Ode0 = Parameter(doc="Omega dark energy; dark energy density/critical density at z=0.",
                         fvalidate="float")
        Tcmb0 = Parameter(doc="Temperature of the CMB as `~astropy.units.Quantity` at z=0.",
                  unit="Kelvin", fmt="0.4g", fvalidate="scalar")
        Neff = Parameter(doc="Number of effective neutrino species.", fvalidate="non-negative")
        m_nu = Parameter(doc="Mass of neutrino species.",
                 unit="eV", equivalencies=u.mass_energy(), fmt="")
        Ob0 = Parameter(doc="Omega baryon; baryonic matter density/critical density at z=0.")

        def __init__(self, H0, Om0, Ode0, Tcmb0=0.0*u.K, Neff=3.04, m_nu=0.0*u.eV,
                     Ob0=None, *, name=None, meta=None):
            self.H0 = H0
            ...  # for each Parameter in turn

        @Ob0.validator
        def Ob0(self, param, value):
            """Validate baryon density to None or positive float > matter density."""
            if value is None:
                return value
            value = _validate_non_negative(self, param, value)
            if value > self.Om0:
                raise ValueError("baryonic density can not be larger than total matter density.")
            return value

First note that all the parameters are also arguments in ``__init__``. This is
not strictly necessary, but is good practice. If the parameter has units (and
related equivalencies) these must be specified on the Parameter, as seen in
:attr:`~astropy.cosmology.FLRW.H0` and :attr:`~astropy.cosmology.FLRW.m_nu`.

The next important thing to note is how the parameter value is set, in
``__init__``. |Parameter| allows for a value to be set once (before
auto-locking), so ``self.H0 = H0`` will use this setter and put the value on
"._H0". The advantage of this method over direct assignment to the private
attribute is the use of validators. |Parameter| allows for custom value
validators, using the method-decorator ``validator``, that can check a value's
validity and modify the value, e.g to assign units. If no custom ``validator``
is specified the default is to check if the |Parameter| has defined units and
if so, return the value as a |Quantity| with those units, using all enabled and
the parameter's unit equivalencies.

The last thing to note is pretty formatting for the |Cosmology|. Each
|Parameter| defaults to the `format specification
<https://docs.python.org/3/library/string.html#formatspec>`_ ".3g", but this
may be overridden, like :attr:`~astropy.cosmology.FLRW.Tcmb0` does.

If a new cosmology modifies an existing Parameter, then the
:meth:`~astropy.cosmology.Parameter.clone` method is useful to deep-copy the
parameter and change any constructor argument. For example, see
``FlatFLRWMixin`` in ``astropy.cosmology.flrw`` (also shown below).

.. code-block:: python

    class FlatFLRWMixin(FlatCosmologyMixin):
        ...

        Ode0 = FLRW.Ode0.clone(derived=True)  # now a derived param.

Mixins
------

`Mixins <https://en.wikipedia.org/wiki/Mixin>`_ are used in
:mod:`~astropy.cosmology` to reuse code across multiple classes in different
inheritance lines. We use the term loosely as mixins are meant to be strictly
orthogonal, but may not be, particularly in ``__init__``.

Currently the only mixin is |FlatCosmologyMixin| and its |FLRW|-specific
subclass |FlatFLRWMixin|. "Flat" cosmologies should use this mixin.
|FlatFLRWMixin| must precede the base class in the multiple-inheritance so that
this mixin's ``__init__`` proceeds the base class'.


.. _astropy-cosmology-fast-integrals:

Speeding up Integrals in Custom Cosmologies
-------------------------------------------

The supplied cosmology classes use a few tricks to speed up distance and time
integrals.  It is not necessary for anyone subclassing |FLRW| to use these
tricks -- but if they do, such calculations may be a lot faster.

The first, more basic, idea is that, in many cases, it's a big deal to provide
explicit formulae for :meth:`~astropy.cosmology.FLRW.inv_efunc` rather than
simply setting up ``de_energy_scale`` -- assuming there is a nice expression.
As noted above, almost all of the provided classes do this, and that template
can pretty much be followed directly with the appropriate formula changes.

The second, and more advanced, option is to also explicitly provide a scalar
only version of :meth:`~astropy.cosmology.FLRW.inv_efunc`. This results in a
fairly large speedup (>10x in most cases) in the distance and age integrals,
even if only done in python, because testing whether the inputs are iterable or
pure scalars turns out to be rather expensive. To take advantage of this, the
key thing is to explicitly set the instance variables
``self._inv_efunc_scalar`` and ``self._inv_efunc_scalar_args`` in the
constructor for the subclass, where the latter are all the arguments except
``z`` to ``_inv_efunc_scalar``. The provided classes do use this optimization,
and in fact go even further and provide optimizations for no radiation, and for
radiation with massless neutrinos coded in cython. Consult the |FLRW|
subclasses and ``scalar_inv_efuncs`` for the details.

However, the important point is that it is *not* necessary to do this.


Astropy Interoperability: I/O and your Cosmology Package
========================================================

If you are developing a package and want to be able to interoperate with
|Cosmology|, you're in the right place! Here we will discuss how to enable
Astropy to read and write your file formats, and convert your cosmology objects
to and from Astropy's |Cosmology|.

The following presumes knowledge of how Astropy structures I/O functions. For
a quick tutorial see :ref:`cosmology_io`.

Now that we know how to build and register functions into |Cosmology.read|,
|Cosmology.write|, |Cosmology.from_format|, |Cosmology.to_format|, we can do
this in your package.

Consider a package -- since this is mine, it's cleverly named ``mypackage`` --
with the following file structure: a module for cosmology codes and a module
for defining related input/output functions. In the cosmology module are
defined cosmology classes and a file format -- ``myformat`` -- and everything
should interoperate with astropy. The tests are done with :mod:`pytest` and are
integrated within the code structure.

.. code-block:: text
    :emphasize-lines: 7,8,9,13,14

    mypackage/
        __init__.py
        cosmology/
            __init__.py
            ...
        io/
            __init__.py
            astropy_convert.py
            astropy_io.py
            ...
            tests/
                __init__.py
                test_astropy_convert.py
                test_astropy_io.py
                ...

For a fully implemented example ``mypackage``, see
https://github.com/astropy/astropy/tree/main/astropy/cosmology/tests/mypackage


Converting Objects Between Packages
-----------------------------------

We want to enable conversion between cosmology objects from ``mypackage``
to/from |Cosmology|. All the Astropy interface code is defined in
``mypackage/io/astropy_convert.py``. The following is a rough outline of the
necessary functions and how to register them with astropy's unified I/O to be
automatically available to |Cosmology.from_format| and |Cosmology.to_format|.

.. literalinclude:: ../../astropy/cosmology/tests/mypackage/io/astropy_convert.py
   :language: python


Reading and Writing
-------------------

Everything Astropy read/write related is defined in
``mypackage/io/astropy_io.py``. The following is a rough outline of the read,
write, and identify functions and how to register them with astropy's unified
IO to be automatically available to |Cosmology.read| and |Cosmology.write|.

.. literalinclude:: ../../astropy/cosmology/tests/mypackage/io/astropy_io.py
   :language: python


If Astropy is an optional dependency
------------------------------------

The ``astropy_io`` and ``astropy_convert`` modules are written assuming Astropy
is installed. If in ``mypackage`` it is an optional dependency then it is
important to detect if Astropy is installed (and the correct version) before
importing ``astropy_io`` and ``astropy_convert``.
We do this in ``mypackage/io/__init__.py``:

.. literalinclude:: ../../astropy/cosmology/tests/mypackage/io/__init__.py
   :language: python


Astropy Interoperability Tests
------------------------------

Lastly, it's important to test that everything works. In this example package
all such tests are contained in ``mypackage/io/tests/test_astropy_io.py``.
These tests require Astropy and will be skipped if it is not installed (and
not the correct version), so at least one test in the test matrix should
include ``astropy >= 5.0``.

.. literalinclude:: ../../astropy/cosmology/tests/mypackage/io/tests/test_astropy_convert.py
   :language: python

.. literalinclude:: ../../astropy/cosmology/tests/mypackage/io/tests/test_astropy_io.py
   :language: python
.. _astropy_config_file:

Astropy's Default Configuration File
************************************

 .. generate_config:: astropy
.. _astropy_config:

***************************************
Configuration System (`astropy.config`)
***************************************

Introduction
============

The ``astropy`` configuration system is designed to give users control of
various parameters used in ``astropy`` or affiliated packages without delving
into the source code to make those changes.

.. note::
    * Before version 4.3 the configuration file was created by default
      when importing ``astropy``. Its existence was required, which is
      no longer the case.

Getting Started
===============

The ``astropy`` configuration options are most conveniently set by modifying
the configuration file. Since ``astropy`` 4.3 you need to create this file,
whereas before it was created automatically when importing ``astropy``. The
function :func:`~astropy.config.create_config_file` creates the file with all
of the default values commented out::

    >>> from astropy.config import create_config_file
    >>> create_config_file('astropy')  # doctest: +SKIP

The exact location of this file can be obtained with
:func:`~astropy.config.get_config_dir`::

    >>> from astropy.config import get_config_dir
    >>> get_config_dir()  # doctest: +SKIP

And you should see the location of your configuration directory. The standard
scheme generally puts your configuration directory in
``$HOME/.astropy/config``. It can be customized with the environment variable
``XDG_CONFIG_HOME`` in which case the ``$XDG_CONFIG_HOME/astropy`` directory
must exist. Note that ``XDG_CONFIG_HOME`` comes from a Linux-centric
specification (see `here
<https://wiki.archlinux.org/index.php/XDG_Base_Directory_support>`_ for more
details), but ``astropy`` will use this on any OS as a more general means to
know where user-specific configurations should be written.

.. note::
    See :ref:`astropy_config_file` for the content of this configuration file.

Once you have found the configuration file, open it with your favorite editor.
It should have all of the sections you might want, with descriptions and the
type of value that is accepted. Feel free to edit this as you wish, and
any of these changes will be reflected when you next start ``astropy``. Or call
the :func:`~astropy.config.reload_config` function if you want to see your
changes immediately in your current ``astropy`` session::

    >>> from astropy.config import reload_config
    >>> reload_config()

.. note::
    If for whatever reason your ``$HOME/.astropy`` directory is not accessible
    (i.e., you have ``astropy`` running somehow as root but you are not the root
    user), the best solution is to set the ``XDG_CONFIG_HOME`` and
    ``XDG_CACHE_HOME`` environment variables pointing to directories, and create
    an ``astropy`` directory inside each of those. Both the configuration and
    data download systems will then use those directories and never try to
    access the ``$HOME/.astropy`` directory.


Using `astropy.config`
======================

Accessing Values
----------------

By convention, configuration parameters live inside of objects called
``conf`` at the root of each sub-package. For example, configuration
parameters related to data files live in ``astropy.utils.data.conf``.
This object has properties for getting and setting individual
configuration parameters. For instance, to get the default URL for
``astropy`` remote data, do::

    >>> from astropy.utils.data import conf
    >>> conf.dataurl
    'http://data.astropy.org/'

Changing Values at Runtime
--------------------------

Changing the persistent state of configuration values is done by editing the
configuration file as described above. Values can also, however, be
modified in an active Python session by setting any of the properties
on a ``conf`` object, or using the
:meth:`~astropy.config.ConfigNamespace.set_temp` `context_manager
<https://docs.python.org/3/reference/datamodel.html#context-managers>`_, as
long as the new value complies with the `Item Types and Validation`_.

Example
^^^^^^^

..
  EXAMPLE START
  Changing the Persistent State of Configuration Values at Runtime

Suppose there is a part of your configuration file that looks like:

.. code-block:: ini

    [utils.data]

    # URL for astropy remote data site.
    dataurl = http://data.astropy.org/

    # Time to wait for remote data query (in seconds).
    remote_timeout = 10.0

If you wish to modify the ``remote_timeout`` value, but only for some small
section of your code, then :meth:`~astropy.config.ConfigNamespace.set_temp`
takes care of resetting the value you changed when you are done using it::

    >>> from astropy.utils.data import conf
    >>> conf.remote_timeout
    10.0
    >>> # Change remote_timeout, but only inside the with-statement.
    >>> with conf.set_temp('remote_timeout', 4.5):
    ...    conf.remote_timeout
    4.5
    >>> conf.remote_timeout
    10.0

You can also modify the values at runtime directly::

    >>> conf.dataurl
    'http://data.astropy.org/'
    >>> conf.dataurl = 'http://astropydata.mywebsite.com'
    >>> conf.dataurl
    'http://astropydata.mywebsite.com'
    >>> conf.remote_timeout
    10.0
    >>> conf.remote_timeout = 4.5
    >>> conf.remote_timeout
    4.5

..
  EXAMPLE END

Reloading Configuration
-----------------------

Instead of modifying the variables in Python, you can also modify the
configuration files and then use the
:meth:`~astropy.config.ConfigNamespace.reload` method.

Example
^^^^^^^

..
  EXAMPLE START
  Modifying and Reloading Configuration Files

If you modify the configuration file to say:

.. code-block:: ini

    [utils.data]

    # URL for astropy remote data site.
    dataurl = http://myotherdata.mywebsite.com/

    # Time to wait for remote data query (in seconds).
    remote_timeout = 6.3

And then run the following commands::

    >>> conf.reload('dataurl')
    >>> conf.reload('remote_timeout')

This should update the variables with the values from the configuration file:

.. doctest-skip::

    >>> conf.dataurl
    'http://myotherdata.mywebsite.com/'
    >>> conf.remote_timeout
    6.3

You can reload all configuration parameters of a ``conf`` object at
once by calling :meth:`~astropy.config.ConfigNamespace.reload` with no
parameters::

    >>> conf.reload()

Or if you want to reload all the configuration items at once, not just the ones
in the module ``conf`` belongs to, use the
:func:`~astropy.config.reload_config` function::

    >>> from astropy import config
    >>> config.reload_config()

You can also reset a configuration parameter back to its default value with the
:meth:`~astropy.config.ConfigNamespace.reset` method. Note that this is the
default value defined in the Python code, which might be different from the
value in the configuration file::

    >>> conf.reset('dataurl')
    >>> conf.dataurl
    'http://data.astropy.org/'

..
  EXAMPLE END

Exploring Configuration
-----------------------

To see what configuration parameters are defined for a given ``conf``::

    >>> from astropy.utils.iers import conf
    >>> list(conf)
    ['auto_download',
     'auto_max_age',
     ...,
     'ietf_leap_second_auto_url']
    >>> conf.auto_max_age
    30.0

You can also iterate through ``conf`` in a dictionary-like fashion::

    >>> list(conf.values())
    [<ConfigItem: name='auto_download' value=True at ...>,
     <ConfigItem: name='auto_max_age' value=30.0 at ...>,
     ...,
     <ConfigItem: name='ietf_leap_second_auto_url' value=...>]
    >>> for (key, cfgitem) in conf.items():
    ...     if key == 'auto_max_age':
    ...         print(f'{cfgitem.description} Value is {cfgitem()}')
    Maximum age (days) of predictive data before auto-downloading. See "Auto
    refresh behavior" in astropy.utils.iers documentation for details. Default
    is 30. Value is 30.0

Upgrading ``astropy``
---------------------

Each time you upgrade to a new major version of ``astropy``, the
configuration parameters may have changed. If you want to create a new
configuration file, you can run::

    >>> from astropy.config import create_config_file
    >>> create_config_file('astropy', overwrite=True)  # doctest: +SKIP

Note that this will overwrite the existing file, so if you modified it you
may want to report your changes in the new file. Another possibility is to
have a look at the :ref:`astropy_config` to see what has changed.

.. _config-developer:

Adding New Configuration Items
==============================

Configuration items should be used wherever an option or setting is
needed that is either tied to a system configuration or should persist
across sessions of ``astropy`` or an affiliated package. Options that may
affect the results of science calculations should not be configuration
items, but should instead be :class:`~astropy.utils.state.ScienceState`
instances, so it is possible to reproduce science results without them being
affected by configuration parameters set in a particular environment.
Admittedly, this is only a guideline, as the precise cases where a
configuration item is preferred over, say, a keyword option for a
function is somewhat personal preference. It is the preferred form of
persistent configuration, however, and ``astropy`` packages must all use
it (and it is recommended for affiliated packages).

The reference guide below describes the interface for creating a
``conf`` object with a number of configuration parameters. They
should be defined at the top level (i.e., in the ``__init__.py`` of
each sub-package that has configuration items)::

    """ This is the docstring at the beginning of a module
    """
    from astropy import config as _config

    class Conf(_config.ConfigNamespace):
        """
        Configuration parameters for my subpackage.
        """
        some_setting = _config.ConfigItem(
            1, 'Description of some_setting')
        another_setting = _config.ConfigItem(
            'string value', 'Description of another_setting')
        some_list = _config.ConfigItem(
            [], 'Description of some_list', cfgtype='list')
        another_list = _config.ConfigItem(
            ['value'], 'Description of another_setting', cfgtype='list')

    # Create an instance for the user
    conf = Conf()

    # implementation ...
    def some_func():
        # to get the value of some of these options, I might do:
        something = conf.some_setting + 2
        return conf.another_setting + ' Also, I added text.'

For an affiliated package called, for example, ``packagename``, the
configuration file can be generated with the
:func:`~astropy.config.create_config_file` function like this::

    >>> from astropy.config import create_config_file
    >>> create_config_file('packagename')  # doctest: +SKIP

The following content would be written to the config file template:

.. code-block:: ini

    [subpackage]
    ## Description of some_setting
    # some_setting = 1

    ## Description of another_setting
    # another_setting = foo

    ## Description of some_list
    # some_list = ,

    ## Description of another_setting
    # another_list = value,

Note that the key/value pairs are commented out. This will allow for
changing the default values in a future version of the package without
requiring the user to edit their configuration file to take advantage
of the new defaults. By convention, the descriptions of each
parameter are in comment lines starting with two hash characters
(``##``) to distinguish them from commented out key/value pairs.

Item Types and Validation
-------------------------

If not otherwise specified, a :class:`~astropy.config.ConfigItem` gets its type
from the type of the ``defaultvalue`` it is given when it is created. The item
can only ever have a value of this type, although in some cases a provided
value can be automatically converted. For example

::

    >>> conf.auto_download
    True
    >>> conf.auto_download = 0
    >>> conf.auto_download
    False

succeeds because the :class:`int` ``0`` can be safely converted to the
:class:`bool` `False`. On the other hand

::

    >>> from astropy.utils.data import conf
    >>> conf.compute_hash_block_size
    65536
    >>> conf.compute_hash_block_size = 65536.0
    Traceback (most recent call last):
    ...
    TypeError: Provided value for configuration item compute_hash_block_size
    not valid: the value "65536.0" is of the wrong type.

fails because converting a :class:`float` to an :class:`int` can lose
information.

Note that if you want the configuration item to be limited to a particular set
of options, you should pass in a :class:`list` as the ``defaultvalue`` option.
The first entry in the :class:`list` will be taken as the default, and the
:class:`list` as a whole gives all of the valid options. For example::

    an_option = ConfigItem(['a', 'b', 'c'],
                           "This option can be 'a', 'b', or 'c'")
    conf.an_option = 'b'  # succeeds
    conf.an_option = 'c'  # succeeds
    conf.an_option = 'd'  # fails!
    conf.an_option = 6    # fails!

Finally, a :class:`~astropy.config.ConfigItem` can be explicitly given a type
via the ``cfgtype`` option::

    an_int_setting = ConfigItem(
        1, 'A description.', cfgtype='integer')
    ...
    conf.an_int_setting = 3     # works fine
    conf.an_int_setting = 4.2   # fails!

If the default value's type does not match ``cfgtype``, the
:class:`~astropy.config.ConfigItem` cannot be created.

In summary, the default behavior (of automatically determining ``cfgtype``)
is usually what you want. The main exception is when you want your
configuration item to be a :class:`list`. The default behavior will treat that
as a *list of options* unless you explicitly tell it that the
:class:`~astropy.config.ConfigItem` itself is supposed to be a :class:`list`::

    # The setting must be 1, 2 or 3
    a_list_setting = ConfigItem([1, 2, 3], 'A description.')

    # The setting must be a list and is [1, 2, 3] by default
    a_list_setting = ConfigItem([1, 2, 3], 'A description.', cfgtype='list')

Details of all of the valid ``cfgtype`` items can be found in the
`validation section of the configobj manual
<https://configobj.readthedocs.io/en/latest/validate.html#the-standard-functions>`_.
Below is a list of the valid values here for quick reference:

* ``'integer'``
* ``'float'``
* ``'boolean'``
* ``'string'``
* ``'ip_addr'``
* ``'list'``
* ``'tuple'``
* ``'int_list'``
* ``'float_list'``
* ``'bool_list'``
* ``'string_list'``
* ``'ip_addr_list'``
* ``'mixed_list'``
* ``'option'``
* ``'pass'``

Usage Tips
----------

Keep in mind that :class:`~astropy.config.ConfigItem` objects can be
changed at runtime by users. So it is always recommended to read their
values immediately before use instead of storing their initial
value to some other variable (or used as a default for a
function). For example, the following is incorrect usage::

    >>> from astropy.utils.data import conf
    >>> conf.remote_timeout = 1.0
    >>> def some_func(val=conf.remote_timeout):
    ...     return val + 2

This works only as long as the user does not change the value of the
configuration item after the function has been defined, but if they do, the
function will not know about the change::

    >>> some_func()
    3.0
    >>> conf.remote_timeout = 3.0
    >>> some_func()  # naively should return 5.0, because 3 + 2 = 5
    3.0

There are two ways around this. The typical/intended way is::

    >>> def some_func():
    ...     """
    ...     The `SOME_SETTING` configuration item influences this output
    ...     """
    ...     return conf.remote_timeout + 2
    >>> some_func()
    5.0
    >>> conf.remote_timeout = 5.0
    >>> some_func()
    7.0

Or, if the option needs to be available as a function parameter::

    def some_func(val=None):
        """
        If not specified, `val` is set by the `SOME_SETTING` configuration item.
        """
        return (conf.remote_timeout if val is None else val) + 2


Customizing Config Location in Affiliated Packages
==================================================

The `astropy.config` package can be used by other packages. By default creating
a config object in another package will lead to a configuration file taking the
name of that package in the ``astropy`` config directory (i.e.,
``<astropy_config>/packagename.cfg``).

It is possible to configure this behavior so that the a custom configuration
directory is created for your package, for example,
``~/.packagename/packagename.cfg``. To do this, create a ``packagename.config``
subpackage and put the following into the ``__init__.py`` file::

  import astropy.config as astropyconfig


  class ConfigNamespace(astropyconfig.ConfigNamespace):
      rootname = 'packagename'


  class ConfigItem(astropyconfig.ConfigItem):
      rootname = 'packagename'

Then replace all imports of `astropy.config` with ``packagename.config``.


See Also
========

.. toctree::
   :maxdepth: 2

   astropy_config

:doc:`/logging` (overview of `astropy.logger`)


Reference/API
=============

.. automodapi:: astropy.config

.. testcleanup::

    >>> from astropy import config
    >>> config.reload_config()
    >>> from astropy.utils.iers import conf
    >>> conf.auto_download = False

.. _astropy-visualization-stretchnorm:

**********************************
Image stretching and normalization
**********************************

The `astropy.visualization` module provides a framework for
transforming values in images (and more generally any arrays),
typically for the purpose of visualization. Two main types of
transformations are provided:

* Normalization to the [0:1] range using lower and upper limits where
  :math:`x` represents the values in the original image:

.. math::

    y = \frac{x - v_{\rm min}}{v_{\rm max} - v_{\rm min}}

* *Stretching* of values in the [0:1] range to the [0:1] range using a
  linear or non-linear function:

.. math::

    z = f(y)

In addition, classes are provided in order to identify lower and upper
limits for a dataset based on specific algorithms (such as using
percentiles).

Identifying lower and upper limits, as well as re-normalizing, is
described in the `Intervals and Normalization`_ section, while
stretching is described in the `Stretching`_ section.

Intervals and Normalization
===========================

The Quick Way
-------------

``astropy`` provides a convenience
:func:`~astropy.visualization.mpl_normalize.simple_norm` function that can be
useful for quick interactive analysis:

.. plot::
    :include-source:
    :align: center

    import numpy as np
    import matplotlib.pyplot as plt
    from astropy.visualization import simple_norm

    # Generate a test image
    image = np.arange(65536).reshape((256, 256))

    # Create an ImageNormalize object
    norm = simple_norm(image, 'sqrt')

    # Display the image
    fig = plt.figure()
    ax = fig.add_subplot(1, 1, 1)
    im = ax.imshow(image, origin='lower', norm=norm)
    fig.colorbar(im)

This convenience function combines a :class:`Stretch
<astropy.visualization.stretch.BaseStretch>` object with an :class:`Interval
<astropy.visualization.interval.BaseInterval>` object.
We recommend using
:class:`~astropy.visualization.mpl_normalize.ImageNormalize` directly
in scripted programs instead of this convenience function.


The detailed way
----------------

Several classes are provided for determining intervals and for
normalizing values in this interval to the [0:1] range. One of the
simplest examples is the
:class:`~astropy.visualization.MinMaxInterval` which determines the
limits of the values based on the minimum and maximum values in the
array. The class is instantiated with no arguments::

    >>> from astropy.visualization import MinMaxInterval
    >>> interval = MinMaxInterval()

and the limits can be determined by calling the
:meth:`~astropy.visualization.MinMaxInterval.get_limits` method, which
takes the array of values::

    >>> interval.get_limits([1, 3, 4, 5, 6])
    (1, 6)

The ``interval`` instance can also be called like a function to
actually normalize values to the range::

    >>> interval([1, 3, 4, 5, 6])  # doctest: +FLOAT_CMP
    array([0. , 0.4, 0.6, 0.8, 1. ])

Other interval classes include
:class:`~astropy.visualization.ManualInterval`,
:class:`~astropy.visualization.PercentileInterval`,
:class:`~astropy.visualization.AsymmetricPercentileInterval`, and
:class:`~astropy.visualization.ZScaleInterval`. For these, values in
the array can fall outside of the limits given by the interval.  A
``clip`` argument is provided to control the behavior of the
normalization when values fall outside the limits::

    >>> from astropy.visualization import PercentileInterval
    >>> interval = PercentileInterval(50.)
    >>> interval.get_limits([1, 3, 4, 5, 6])
    (3.0, 5.0)
    >>> interval([1, 3, 4, 5, 6])  # default is clip=True  # doctest: +FLOAT_CMP
    array([0. , 0. , 0.5, 1. , 1. ])
    >>> interval([1, 3, 4, 5, 6], clip=False)  # doctest: +FLOAT_CMP
    array([-1. ,  0. ,  0.5,  1. ,  1.5])


Stretching
==========

In addition to classes that can scale values to the [0:1] range, a
number of classes are provided to 'stretch' the values using different
functions. These map a [0:1] range onto a transformed [0:1] range. A
simple example is the :class:`~astropy.visualization.SqrtStretch`
class::

    >>> from astropy.visualization import SqrtStretch
    >>> stretch = SqrtStretch()
    >>> stretch([0., 0.25, 0.5, 0.75, 1.])  # doctest: +FLOAT_CMP
    array([0.        , 0.5       , 0.70710678, 0.8660254 , 1.        ])

As for the intervals, values outside the [0:1] range can be treated
differently depending on the ``clip`` argument. By default, output
values are clipped to the [0:1] range::

    >>> stretch([-1., 0., 0.5, 1., 1.5])  # doctest: +FLOAT_CMP
    array([0.       , 0.        , 0.70710678, 1.        , 1.        ])

but this can be disabled::

    >>> stretch([-1., 0., 0.5, 1., 1.5], clip=False)  # doctest: +FLOAT_CMP
    array([       nan, 0.        , 0.70710678, 1.        , 1.22474487])

.. note::
    The stretch functions are similar but not always strictly
    identical to those used in e.g. `DS9
    <http://ds9.si.edu/site/Home.html>`_ (although they should have
    the same behavior). The equations for the DS9 stretches can be
    found `here <http://ds9.si.edu/doc/ref/how.html>`_ and can be
    compared to the equations for our stretches provided in the
    `astropy.visualization` API section. The main difference between
    our stretches and DS9 is that we have adjusted them so that the
    [0:1] range always maps exactly to the [0:1] range.


Combining transformations
=========================

Any intervals and stretches can be chained by using the ``+``
operator, which returns a new transformation. When combining intervals
and stretches, the stretch object must come before the interval
object. For example, to apply normalization based on a percentile
value, followed by a square root stretch, you can do::

    >>> transform = SqrtStretch() + PercentileInterval(90.)
    >>> transform([1, 3, 4, 5, 6])  # doctest: +FLOAT_CMP
    array([0.        , 0.60302269, 0.76870611, 0.90453403, 1.        ])

As before, the combined transformation can also accept a ``clip``
argument (which is `True` by default).

Matplotlib normalization
========================

Matplotlib allows a custom normalization and stretch to be used when
displaying data by passing a :class:`matplotlib.colors.Normalize`
object, e.g. to :meth:`~matplotlib.axes.Axes.imshow`. The
`astropy.visualization` module provides an
:class:`~astropy.visualization.mpl_normalize.ImageNormalize` class
that wraps the interval (see `Intervals and Normalization`_) and
stretch (see `Stretching`_) objects into an object Matplotlib
understands.

The inputs to the
:class:`~astropy.visualization.mpl_normalize.ImageNormalize` class are
the data and the interval and stretch objects:

.. plot::
    :include-source:
    :align: center

    import numpy as np
    import matplotlib.pyplot as plt

    from astropy.visualization import (MinMaxInterval, SqrtStretch,
                                       ImageNormalize)

    # Generate a test image
    image = np.arange(65536).reshape((256, 256))

    # Create an ImageNormalize object
    norm = ImageNormalize(image, interval=MinMaxInterval(),
                          stretch=SqrtStretch())

    # or equivalently using positional arguments
    # norm = ImageNormalize(image, MinMaxInterval(), SqrtStretch())

    # Display the image
    fig = plt.figure()
    ax = fig.add_subplot(1, 1, 1)
    im = ax.imshow(image, origin='lower', norm=norm)
    fig.colorbar(im)

As shown above, the colorbar ticks are automatically adjusted.

Please note that one should not use ``ax.imshow(norm(image))`` because
the colorbar ticks marks will represent normalized image values (on a
linear scale), not the actual image values.  Also, the image displayed
by ``ax.imshow(norm(image))`` is not exactly equivalent to
``ax.imshow(image, norm=norm)`` if the image contains ``NaN`` or
``inf`` values.  The exact equivalent is
``ax.imshow(norm(np.ma.masked_invalid(image))``.

The input image to
:class:`~astropy.visualization.mpl_normalize.ImageNormalize` is
typically the one to be displayed, so there is a convenience function
:func:`~astropy.visualization.mpl_normalize.imshow_norm` to ease this
use case:


.. plot::
    :include-source:
    :align: center

    import numpy as np
    import matplotlib.pyplot as plt

    from astropy.visualization import imshow_norm, MinMaxInterval, SqrtStretch

    # Generate a test image
    image = np.arange(65536).reshape((256, 256))

    # Display the exact same thing as the above plot
    fig = plt.figure()
    ax = fig.add_subplot(1, 1, 1)
    im, norm = imshow_norm(image, ax, origin='lower',
                           interval=MinMaxInterval(), stretch=SqrtStretch())
    fig.colorbar(im)

While this is the simplest case, it is also possible for a completely different
image to be used to establish the normalization (e.g. if one wants to display
several images with exactly the same normalization and stretch).

The inputs to the
:class:`~astropy.visualization.mpl_normalize.ImageNormalize` class can
also be the vmin and vmax limits, which you can determine from the
`Intervals and Normalization`_ classes, and the stretch object:

.. plot::
    :include-source:
    :align: center

    import numpy as np
    import matplotlib.pyplot as plt

    from astropy.visualization import (MinMaxInterval, SqrtStretch,
                                       ImageNormalize)

    # Generate a test image
    image = np.arange(65536).reshape((256, 256))

    # Create interval object
    interval = MinMaxInterval()
    vmin, vmax = interval.get_limits(image)

    # Create an ImageNormalize object using a SqrtStretch object
    norm = ImageNormalize(vmin=vmin, vmax=vmax, stretch=SqrtStretch())

    # Display the image
    fig = plt.figure()
    ax = fig.add_subplot(1, 1, 1)
    im = ax.imshow(image, origin='lower', norm=norm)
    fig.colorbar(im)


Combining stretches and Matplotlib normalization
================================================

Stretches can also be combined with other stretches, just like transformations.
The resulting :class:`~astropy.visualization.stretch.CompositeStretch` can be
used to normalize Matplotlib images like any other stretch. For example, a
composite stretch can stretch residual images with negative values:

.. plot::
    :include-source:
    :align: center

    import numpy as np
    import matplotlib.pyplot as plt
    from astropy.visualization.stretch import SinhStretch, LinearStretch
    from astropy.visualization import ImageNormalize

    # Transforms normalized values [0,1] to [-1,1] before stretch and then back
    stretch = LinearStretch(slope=0.5, intercept=0.5) + SinhStretch() + \
        LinearStretch(slope=2, intercept=-1)

    # Image of random Gaussian noise
    image = np.random.normal(size=(64, 64))
    fig = plt.figure()
    ax = fig.add_subplot(1, 1, 1)
    # ImageNormalize normalizes values to [0,1] before applying the stretch
    norm = ImageNormalize(stretch=stretch, vmin=-5, vmax=5)
    im = ax.imshow(image, origin='lower', norm=norm, cmap='gray')
    fig.colorbar(im)
.. _astropy-visualization-rgb:

*************************
Creating color RGB images
*************************

RGB images can be produced using matplotlib's ability to make three-color
images.  In general, an RGB image is an MxNx3 array, where M is the
y-dimension, N is the x-dimension, and the length-3 layer represents red,
green, and blue, respectively.  A fourth layer representing the alpha (opacity)
value can be specified.

Matplotlib has several tools for manipulating these colors at
`matplotlib.colors`.

Astropy's visualization tools can be used to change the stretch and scaling of
the individual layers of the RGB image.  Each layer must be on a scale of 0-1
for floats (or 0-255 for integers); values outside that range will be clipped.


**************************************************************
Creating color RGB images using the Lupton et al (2004) scheme
**************************************************************

`Lupton et al. (2004)`_ describe an "optimal" algorithm for producing red-green-
blue composite images from three separate high-dynamic range arrays. This method
is implemented in `~astropy.visualization.make_lupton_rgb` as a convenience
wrapper function and an associated set of classes to provide alternate scalings.
The SDSS SkyServer color images were made using a variation on this technique.
To generate a color PNG file with the default (arcsinh) scaling:

.. _Lupton et al. (2004): https://ui.adsabs.harvard.edu/abs/2004PASP..116..133L

.. plot::
    :include-source:
    :align: center

    import numpy as np
    import matplotlib.pyplot as plt
    from astropy.visualization import make_lupton_rgb
    image_r = np.random.random((100,100))
    image_g = np.random.random((100,100))
    image_b = np.random.random((100,100))
    image = make_lupton_rgb(image_r, image_g, image_b, stretch=0.5)
    plt.imshow(image)

This method requires that the three images be aligned and have the same pixel
scale and size. Changing ``minimum`` will change the black level, while
``stretch`` and ``Q`` will change how the values between black and white are
scaled.

For a more in-depth example, download the ``g``, ``r``, ``i`` SDSS frames
(they will serve as the blue, green and red channels respectively) of
the area around the Hickson 88 group and try the example below and compare
it with Figure 1 of `Lupton et al. (2004)`_:

.. plot::
   :context: reset
   :include-source:
   :align: center

   import matplotlib.pyplot as plt
   from astropy.visualization import make_lupton_rgb
   from astropy.io import fits
   from astropy.utils.data import get_pkg_data_filename

   # Read in the three images downloaded from here:
   g_name = get_pkg_data_filename('visualization/reprojected_sdss_g.fits.bz2')
   r_name = get_pkg_data_filename('visualization/reprojected_sdss_r.fits.bz2')
   i_name = get_pkg_data_filename('visualization/reprojected_sdss_i.fits.bz2')
   g = fits.open(g_name)[0].data
   r = fits.open(r_name)[0].data
   i = fits.open(i_name)[0].data

   rgb_default = make_lupton_rgb(i, r, g, filename="ngc6976-default.jpeg")
   plt.imshow(rgb_default, origin='lower')

The image above was generated with the default parameters. However using a
different scaling, e.g Q=10, stretch=0.5, faint features
of the galaxies show up. Compare with Fig. 1 of `Lupton et al. (2004)`_ or the
`SDSS Skyserver image`_.

.. plot::
   :context:
   :include-source:
   :align: center

   rgb = make_lupton_rgb(i, r, g, Q=10, stretch=0.5, filename="ngc6976.jpeg")
   plt.imshow(rgb, origin='lower')


.. _SDSS Skyserver image: http://skyserver.sdss.org/dr13/en/tools/chart/navi.aspx?ra=313.12381&dec=-5.74611
.. note that if this is changed from the default approach of using an *include*
   (in index.rst) to a separate performance page, the header needs to be changed
   from === to ***, the filename extension needs to be changed from .inc.rst to
   .rst, and a link needs to be added in the subpackage toctree

.. _astropy-visualization-performance:

.. Performance Tips
.. ================
..
.. Here we provide some tips and tricks for how to optimize performance of code
.. using `astropy.visualization`.
Plotting Astropy objects in Matplotlib
**************************************

.. _plotting-quantities:

Plotting quantities
===================

|Quantity| objects can be conveniently plotted using matplotlib.  This
feature needs to be explicitly turned on:

.. doctest-requires:: matplotlib

    >>> from astropy.visualization import quantity_support
    >>> quantity_support()  # doctest: +IGNORE_OUTPUT
    <astropy.visualization.units.MplQuantityConverter ...>

Then |Quantity| objects can be passed to matplotlib plotting
functions.  The axis labels are automatically labeled with the unit of
the quantity:

.. plot::
   :include-source:
   :context: reset

    from astropy import units as u
    from astropy.visualization import quantity_support
    quantity_support()
    from matplotlib import pyplot as plt
    plt.figure(figsize=(5,3))
    plt.plot([1, 2, 3] * u.m)

Quantities are automatically converted to the first unit set on a
particular axis, so in the following, the y-axis remains in ``m`` even
though the second line is given in ``cm``:

.. plot::
   :include-source:
   :context:

    plt.plot([1, 2, 3] * u.cm)

Plotting a quantity with an incompatible unit will raise an exception.
For example, calling ``plt.plot([1, 2, 3] * u.kg)`` (mass unit) to overplot
on the plot above that is displaying length units.

To make sure unit support is turned off afterward, you can use
`~astropy.visualization.quantity_support` with a ``with`` statement::

    with quantity_support():
        plt.plot([1, 2, 3] * u.m)

.. _plotting-times:

Plotting times
==============

Matplotlib natively provides a mechanism for plotting dates and times on one
or both of the axes, as described in
`Date tick labels <https://matplotlib.org/stable/gallery/text_labels_and_annotations/date.html>`_.
To make use of this, you can use the ``plot_date`` attribute of |Time| to get
values in the time system used by Matplotlib.

However, in many cases, you will probably want to have more control over the
precise scale and format to use for the tick labels, in which case you can make
use of the `~astropy.visualization.time_support` function. This feature needs to
be explicitly turned on:

.. doctest-requires:: matplotlib

    >>> from astropy.visualization import time_support
    >>> time_support()  # doctest: +IGNORE_OUTPUT
    <astropy.visualization.units.MplTimeConverter ...>

Once this is enabled, |Time| objects can be passed to matplotlib plotting
functions. The axis labels are then automatically labeled with times formatted
using the |Time| class:

.. plot::
   :include-source:
   :context: reset

    from matplotlib import pyplot as plt
    from astropy.time import Time
    from astropy.visualization import time_support

    time_support()

    plt.figure(figsize=(5,3))
    plt.plot(Time([58000, 59000, 62000], format='mjd'), [1.2, 3.3, 2.3])

By default, the format and scale used for the plots is taken from the first time
that Matplotlib encounters for a particular Axes instance. The format and scale
can also be explicitly controlled by passing arguments to ``time_support``:

.. plot::
   :nofigs:
   :context: reset

   from matplotlib import pyplot as plt
   from astropy.time import Time
   from astropy.visualization import time_support

.. plot::
   :include-source:
   :context:

    time_support(format='mjd', scale='tai')
    plt.figure(figsize=(5,3))
    plt.plot(Time([50000, 52000, 54000], format='mjd'), [1.2, 3.3, 2.3])

To make sure support for plotting times is turned off afterward, you can use
`~astropy.visualization.time_support` as a context manager::

    with time_support(format='mjd', scale='tai'):
        plt.figure(figsize=(5,3))
        plt.plot(Time([50000, 52000, 54000], format='mjd'))
.. _astropy-visualization:

********************************************
Data Visualization (`astropy.visualization`)
********************************************

Introduction
============

`astropy.visualization` provides functionality that can be helpful when
visualizing data. This includes a framework for plotting Astronomical images
with coordinates with Matplotlib (previously the standalone **wcsaxes**
package), functionality related to image normalization (including both scaling
and stretching), smart histogram plotting, RGB color image creation from
separate images, and custom plotting styles for Matplotlib.

Using `astropy.visualization`
=============================
.. toctree::
   :maxdepth: 2

   matplotlib_integration.rst
   wcsaxes/index.rst
   normalization.rst
   histogram.rst
   rgb.rst

.. _fits2bitmap:

Scripts
=======

This module includes a command-line script, ``fits2bitmap`` to convert FITS
images to bitmaps, including scaling and stretching of the image. To find out
more about the available options and how to use it, type::

    $ fits2bitmap --help

.. note that if this section gets too long, it should be moved to a separate
   doc page - see the top of performance.inc.rst for the instructions on how to do
   that
.. include:: performance.inc.rst

Reference/API
=============

.. automodapi:: astropy.visualization

.. automodapi:: astropy.visualization.mpl_normalize
.. _astropy-visualization-hist:

***********************
Choosing Histogram Bins
***********************

The :mod:`astropy.visualization` module provides the
:func:`~astropy.visualization.hist` function, which is a generalization of
matplotlib's histogram function which allows for more flexible specification
of histogram bins. For computing bins without the accompanying plot, see
:func:`astropy.stats.histogram`.

As a motivation for this, consider the following two histograms, which are
constructed from the same underlying set of 5000 points, the first with
matplotlib's default of 10 bins, the second with an arbitrarily chosen
200 bins:

.. plot::
   :align: center
   :include-source:

    import numpy as np
    import matplotlib.pyplot as plt

    # generate some complicated data
    rng = np.random.default_rng(0)
    t = np.concatenate([-5 + 1.8 * rng.standard_cauchy(500),
                        -4 + 0.8 * rng.standard_cauchy(2000),
                        -1 + 0.3 * rng.standard_cauchy(500),
                        2 + 0.8 * rng.standard_cauchy(1000),
                        4 + 1.5 * rng.standard_cauchy(1000)])

    # truncate to a reasonable range
    t = t[(t > -15) & (t < 15)]

    # draw histograms with two different bin widths
    fig, ax = plt.subplots(1, 2, figsize=(10, 4))

    fig.subplots_adjust(left=0.1, right=0.95, bottom=0.15)
    for i, bins in enumerate([10, 200]):
        ax[i].hist(t, bins=bins, histtype='stepfilled', alpha=0.2, density=True)
        ax[i].set_xlabel('t')
        ax[i].set_ylabel('P(t)')
        ax[i].set_title(f'plt.hist(t, bins={bins})',
                        fontdict=dict(family='monospace'))

Upon visual inspection, it is clear that each of these choices is suboptimal:
with 10 bins, the fine structure of the data distribution is lost, while with
200 bins, heights of individual bins are affected by sampling error.
The tried-and-true method employed by most scientists is a trial and error
approach that attempts to find a suitable midpoint between these.

Astropy's :func:`~astropy.visualization.hist` function addresses this by
providing several methods of automatically tuning the histogram bin size.
It has a syntax identical to matplotlib's ``plt.hist`` function, with the
exception of the ``bins`` parameter, which allows specification of one of
four different methods for automatic bin selection. These methods are
implemented in :func:`astropy.stats.histogram`, which has a similar syntax
to the ``np.histogram`` function.

Normal Reference Rules
======================
The simplest methods of tuning the number of bins are the normal reference
rules due to Scott (implemented in :func:`~astropy.stats.scott_bin_width`) and
Freedman & Diaconis (implemented in :func:`~astropy.stats.freedman_bin_width`).
These rules proceed by assuming the data is close to normally-distributed, and
applying a rule-of-thumb intended to minimize the difference between the
histogram and the underlying distribution of data.

The following figure shows the results of these two rules on the above dataset:

.. plot::
   :align: center
   :include-source:

    import numpy as np
    import matplotlib.pyplot as plt
    from astropy.visualization import hist

    # generate some complicated data
    rng = np.random.default_rng(0)
    t = np.concatenate([-5 + 1.8 * rng.standard_cauchy(500),
                        -4 + 0.8 * rng.standard_cauchy(2000),
                        -1 + 0.3 * rng.standard_cauchy(500),
                        2 + 0.8 * rng.standard_cauchy(1000),
                        4 + 1.5 * rng.standard_cauchy(1000)])

    # truncate to a reasonable range
    t = t[(t > -15) & (t < 15)]

    # draw histograms with two different bin widths
    fig, ax = plt.subplots(1, 2, figsize=(10, 4))

    fig.subplots_adjust(left=0.1, right=0.95, bottom=0.15)
    for i, bins in enumerate(['scott', 'freedman']):
        hist(t, bins=bins, ax=ax[i], histtype='stepfilled',
             alpha=0.2, density=True)
        ax[i].set_xlabel('t')
        ax[i].set_ylabel('P(t)')
        ax[i].set_title(f'hist(t, bins="{bins}")',
                        fontdict=dict(family='monospace'))


As we can see, both of these rules of thumb choose an intermediate number of
bins which provide a good trade-off between data representation and noise
suppression.

Bayesian Models
===============

Though rules-of-thumb like Scott's rule and the Freedman-Diaconis rule are
fast and convenient, their strong assumptions about the data make them
suboptimal for more complicated distributions. Other methods of bin selection
use fitness functions computed on the actual data to choose an optimal binning.
Astropy implements two of these examples: Knuth's rule (implemented in
:func:`~astropy.stats.knuth_bin_width`) and Bayesian Blocks (implemented in
:func:`~astropy.stats.bayesian_blocks`).

Knuth's rule chooses a constant bin size which minimizes the error of the
histogram's approximation to the data, while the Bayesian Blocks uses a more
flexible method which allows varying bin widths. Because both of these require
the minimization of a cost function across the dataset, they are more
computationally intensive than the rules-of-thumb mentioned above. Here are
the results of these procedures for the above dataset:

.. plot::
   :align: center
   :include-source:

    import warnings
    import numpy as np
    import matplotlib.pyplot as plt
    from astropy.visualization import hist

    # generate some complicated data
    rng = np.random.default_rng(0)
    t = np.concatenate([-5 + 1.8 * rng.standard_cauchy(500),
                        -4 + 0.8 * rng.standard_cauchy(2000),
                        -1 + 0.3 * rng.standard_cauchy(500),
                        2 + 0.8 * rng.standard_cauchy(1000),
                        4 + 1.5 * rng.standard_cauchy(1000)])

    # truncate to a reasonable range
    t = t[(t > -15) & (t < 15)]

    # draw histograms with two different bin widths
    fig, ax = plt.subplots(1, 2, figsize=(10, 4))

    fig.subplots_adjust(left=0.1, right=0.95, bottom=0.15)
    for i, bins in enumerate(['knuth', 'blocks']):
        hist(t, bins=bins, ax=ax[i], histtype='stepfilled',
                alpha=0.2, density=True)
        ax[i].set_xlabel('t')
        ax[i].set_ylabel('P(t)')
        ax[i].set_title(f'hist(t, bins="{bins}")',
                        fontdict=dict(family='monospace'))


Notice that both of these capture the shape of the distribution very
accurately, and that the ``bins='blocks'`` panel selects bin widths which vary
in width depending on the local structure in the data. Compared to standard
defaults, these Bayesian optimization methods provide a much more principled
means of choosing histogram binning.
********************************
Overplotting markers and artists
********************************

For the example in the following page we start from the example introduced in
:ref:`initialization`.

.. plot::
   :context: reset
   :nofigs:

    from astropy.wcs import WCS
    from astropy.io import fits
    from astropy.utils.data import get_pkg_data_filename
    import matplotlib.pyplot as plt

    filename = get_pkg_data_filename('galactic_center/gc_msx_e.fits')

    hdu = fits.open(filename)[0]
    wcs = WCS(hdu.header)

    ax = plt.subplot(projection=wcs)
    ax.imshow(hdu.data, vmin=-2.e-5, vmax=2.e-4, origin='lower')


Pixel coordinates
*****************

Apart from the handling of the ticks, tick labels, and grid lines, the
`~astropy.visualization.wcsaxes.WCSAxes` class behaves like a normal Matplotlib
``Axes`` instance, and methods such as
:meth:`~matplotlib.axes.Axes.imshow`,
:meth:`~matplotlib.axes.Axes.contour`,
:meth:`~matplotlib.axes.Axes.plot`,
:meth:`~matplotlib.axes.Axes.scatter`, and so on will work and plot the
data in **pixel coordinates** by default.

In the following example, the scatter markers and the rectangle will be plotted
in pixel coordinates:

.. plot::
   :context:
   :include-source:
   :align: center

    # The following line makes it so that the zoom level no longer changes,
    # otherwise Matplotlib has a tendency to zoom out when adding overlays.
    ax.set_autoscale_on(False)

    # Add a rectangle with bottom left corner at pixel position (30, 50) with a
    # width and height of 60 and 50 pixels respectively.
    from matplotlib.patches import Rectangle
    r = Rectangle((30., 50.), 60., 50., edgecolor='yellow', facecolor='none')
    ax.add_patch(r)

    # Add three markers at (40, 30), (100, 130), and (130, 60). The facecolor is
    # a transparent white (0.5 is the alpha value).
    ax.scatter([40, 100, 130], [30, 130, 60], s=100, edgecolor='white', facecolor=(1, 1, 1, 0.5))

World coordinates
*****************

All such Matplotlib commands allow a ``transform=`` argument to be passed,
which will transform the input from world to pixel coordinates before it is
passed to Matplotlib and plotted. For instance::

    ax.scatter(..., transform=...)

will take the values passed to :meth:`~matplotlib.axes.Axes.scatter` and will
transform them using the transformation passed to ``transform=``, in order to
end up with the final pixel coordinates.

The `~astropy.visualization.wcsaxes.WCSAxes` class includes a :meth:`~astropy.visualization.wcsaxes.WCSAxes.get_transform`
method that can be used to get the appropriate transformation object to convert
from various world coordinate systems to the final pixel coordinate system
required by Matplotlib. The :meth:`~astropy.visualization.wcsaxes.WCSAxes.get_transform` method can
take a number of different inputs, which are described in this and subsequent
sections. The two simplest inputs to this method are ``'world'`` and
``'pixel'``.

For example, if your WCS defines an image where the coordinate system consists of an angle in degrees and a wavelength in nanometers, you can do::

    ax.scatter([34], [3.2], transform=ax.get_transform('world'))

to plot a marker at (34deg, 3.2nm).

Using ``ax.get_transform('pixel')`` is equivalent to not using any
transformation at all (and things then behave as described in the `Pixel
coordinates`_ section).

Celestial coordinates
*********************

For the special case where the WCS represents celestial coordinates, a number
of other inputs can be passed to :meth:`~astropy.visualization.wcsaxes.WCSAxes.get_transform`. These
are:

* ``'fk4'``: B1950 FK4 equatorial coordinates
* ``'fk5'``: J2000 FK5 equatorial coordinates
* ``'icrs'``: ICRS equatorial coordinates
* ``'galactic'``: Galactic coordinates

In addition, any valid `astropy.coordinates` coordinate frame can be passed.

For example, you can add markers with positions defined in the FK5 system using:

.. plot::
   :context: reset
   :nofigs:

    from astropy.wcs import WCS
    from astropy.io import fits
    from astropy.utils.data import get_pkg_data_filename
    import matplotlib.pyplot as plt

    filename = get_pkg_data_filename('galactic_center/gc_msx_e.fits')

    hdu = fits.open(filename)[0]
    wcs = WCS(hdu.header)

    ax = plt.subplot(projection=wcs)
    ax.imshow(hdu.data, vmin=-2.e-5, vmax=2.e-4, origin='lower')

    ax.set_autoscale_on(False)

.. plot::
   :context:
   :include-source:
   :align: center

    ax.scatter(266.78238, -28.769255, transform=ax.get_transform('fk5'), s=300,
               edgecolor='white', facecolor='none')

In the case of :meth:`~matplotlib.axes.Axes.scatter` and :meth:`~matplotlib.axes.Axes.plot`, the positions of the center of the markers is transformed, but the markers themselves are drawn in the frame of reference of the image, which means that they will not look distorted.

Patches/shapes/lines
********************

Transformations can also be passed to Astropy or Matplotlib patches. For example, we can
use the :meth:`~astropy.visualization.wcsaxes.WCSAxes.get_transform` method above to plot a quadrangle
in FK5 equatorial coordinates:

.. plot::
   :context: reset
   :nofigs:

    from astropy.wcs import WCS
    from astropy.io import fits
    from astropy.utils.data import get_pkg_data_filename
    import matplotlib.pyplot as plt

    filename = get_pkg_data_filename('galactic_center/gc_msx_e.fits')
    hdu = fits.open(filename)[0]
    wcs = WCS(hdu.header)

    ax = plt.subplot(projection=wcs)
    ax.imshow(hdu.data, vmin=-2.e-5, vmax=2.e-4, origin='lower')

    ax.set_autoscale_on(False)

.. plot::
   :context:
   :include-source:
   :align: center

    from astropy import units as u
    from astropy.visualization.wcsaxes import Quadrangle

    r = Quadrangle((266.0, -28.9)*u.deg, 0.3*u.deg, 0.15*u.deg,
                   edgecolor='green', facecolor='none',
                   transform=ax.get_transform('fk5'))
    ax.add_patch(r)

In this case, the quadrangle will be plotted at FK5 J2000 coordinates (266deg, -28.9deg).
See the `Quadrangles`_ section for more information on `~astropy.visualization.wcsaxes.Quadrangle`.

However, it is **very important** to note that while the height will indeed be 0.15 degrees, the width will not strictly represent 0.3 degrees on the sky, but an interval of 0.3 degrees in longitude (which, depending on the latitude, will represent a different angle on the sky).
In other words, if the width and height are set to the same value, the resulting polygon will not be a square.
The same applies to the `~matplotlib.patches.Circle` patch, which will not actually produce a circle:

.. plot::
   :context:
   :include-source:
   :align: center

    from matplotlib.patches import Circle

    r = Quadrangle((266.4, -28.9)*u.deg, 0.3*u.deg, 0.3*u.deg,
                   edgecolor='cyan', facecolor='none',
                   transform=ax.get_transform('fk5'))
    ax.add_patch(r)

    c = Circle((266.4, -29.1), 0.15, edgecolor='yellow', facecolor='none',
               transform=ax.get_transform('fk5'))
    ax.add_patch(c)



.. important:: If what you are interested is simply plotting circles around
               sources to highlight them, then we recommend using
               :meth:`~matplotlib.axes.Axes.scatter`, since for the circular
               marker (the default), the circles will be guaranteed to be
               circles in the plot, and only the position of the center is
               transformed.

               To plot 'true' spherical circles, see the `Spherical patches`_
               section.

Quadrangles
***********

`~astropy.visualization.wcsaxes.Quadrangle` is the recommended patch for plotting a quadrangle, as opposed to Matplotlib's `~matplotlib.patches.Rectangle`.
The edges of a quadrangle lie on two lines of constant longitude and two lines of constant latitude (or the equivalent component names in the coordinate frame of interest, such as right ascension and declination).
The edges of `~astropy.visualization.wcsaxes.Quadrangle` will render as curved lines if appropriate for the WCS transformation.
In contrast, `~matplotlib.patches.Rectangle` will always have straight edges.
Here's a comparison of the two types of patches for plotting a quadrangle in `~astropy.coordinates.ICRS` coordinates on `~astropy.coordinates.Galactic` axes:

.. plot::
   :context: reset
   :nofigs:

    from astropy import units as u
    from astropy.wcs import WCS
    from astropy.io import fits
    from astropy.utils.data import get_pkg_data_filename
    from astropy.visualization.wcsaxes import Quadrangle
    import matplotlib.pyplot as plt

    filename = get_pkg_data_filename('galactic_center/gc_msx_e.fits')
    hdu = fits.open(filename)[0]
    wcs = WCS(hdu.header)

.. plot::
   :context:
   :include-source:
   :align: center

    from matplotlib.patches import Rectangle

    # Set the Galactic axes such that the plot includes the ICRS south pole
    ax = plt.subplot(projection=wcs)
    ax.set_xlim(0, 10000)
    ax.set_ylim(-10000, 0)

    # Overlay the ICRS coordinate grid
    overlay = ax.get_coords_overlay('icrs')
    overlay.grid(color='black', ls='dotted')

    # Add a quadrangle patch (100 degrees by 20 degrees)
    q = Quadrangle((255, -70)*u.deg, 100*u.deg, 20*u.deg,
                   label='Quadrangle', edgecolor='blue', facecolor='none',
                   transform=ax.get_transform('icrs'))
    ax.add_patch(q)

    # Add a rectangle patch (100 degrees by 20 degrees)
    r = Rectangle((255, -70), 100, 20,
                  label='Rectangle', edgecolor='red', facecolor='none', linestyle='--',
                  transform=ax.get_transform('icrs'))
    ax.add_patch(r)

    plt.legend(loc='upper right')

Contours
********

Overplotting contours is also simple using the
:meth:`~astropy.visualization.wcsaxes.WCSAxes.get_transform` method. For contours,
:meth:`~astropy.visualization.wcsaxes.WCSAxes.get_transform` should be given the WCS of the
image to plot the contours for:

.. plot::
   :context: reset
   :nofigs:

    from astropy.wcs import WCS
    from astropy.io import fits
    from astropy.utils.data import get_pkg_data_filename
    from matplotlib.patches import Rectangle
    import matplotlib.pyplot as plt

    filename = get_pkg_data_filename('galactic_center/gc_msx_e.fits')
    hdu = fits.open(filename)[0]
    wcs = WCS(hdu.header)

    ax = plt.subplot(projection=wcs)
    ax.imshow(hdu.data, vmin=-2.e-5, vmax=2.e-4, origin='lower')

    ax.set_autoscale_on(False)

.. plot::
   :context:
   :include-source:
   :align: center

    filename = get_pkg_data_filename('galactic_center/gc_bolocam_gps.fits')
    hdu = fits.open(filename)[0]
    ax.contour(hdu.data, transform=ax.get_transform(WCS(hdu.header)),
               levels=[1,2,3,4,5,6], colors='white')

Spherical patches
*****************

In the case where you are making a plot of a celestial image, and want to plot a circle that represents the area within a certain angle of a longitude/latitude,
the `~matplotlib.patches.Circle` patch is not appropriate, since it will result in a distorted shape (because longitude is not the same as the angle on the sky).
For this use case, you can instead use `~astropy.visualization.wcsaxes.SphericalCircle`, which takes a tuple of |Quantity| or a |SkyCoord| object as the input,
and a |Quantity| as the radius:

.. plot::
   :context: reset
   :nofigs:

    from astropy.wcs import WCS
    from astropy.io import fits
    from astropy.utils.data import get_pkg_data_filename
    import matplotlib.pyplot as plt

    filename = get_pkg_data_filename('galactic_center/gc_msx_e.fits')
    hdu = fits.open(filename)[0]
    wcs = WCS(hdu.header)

    ax = plt.subplot(projection=wcs)
    ax.imshow(hdu.data, vmin=-2.e-5, vmax=2.e-4, origin='lower')

    ax.set_autoscale_on(False)

.. plot::
   :context:
   :include-source:
   :align: center

    from astropy import units as u
    from astropy.coordinates import SkyCoord
    from astropy.visualization.wcsaxes import SphericalCircle


    r = SphericalCircle((266.4 * u.deg, -29.1 * u.deg), 0.15 * u.degree,
                         edgecolor='yellow', facecolor='none',
                         transform=ax.get_transform('fk5'))

    ax.add_patch(r)

    #The following lines show the usage of a SkyCoord object as the input.
    skycoord_object = SkyCoord(266.4 * u.deg, -28.7 * u.deg)
    s = SphericalCircle(skycoord_object, 0.15 * u.degree,
                        edgecolor='white', facecolor='none',
                        transform=ax.get_transform('fk5'))

    ax.add_patch(s)
.. _initialization:

****************************************
Initializing axes with world coordinates
****************************************

Basic initialization
********************

To make a plot using `~astropy.visualization.wcsaxes.WCSAxes`, we first read in
the data using `astropy.io.fits
<https://docs.astropy.org/en/stable/io/fits/index.html>`_ and parse the WCS
information. In this example, we will use an example FITS file from the
http://data.astropy.org server (the
:func:`~astropy.utils.data.get_pkg_data_filename` function downloads the file
and returns a filename):

.. plot::
   :context: reset
   :nofigs:
   :include-source:
   :align: center

    from astropy.wcs import WCS
    from astropy.io import fits
    from astropy.utils.data import get_pkg_data_filename

    filename = get_pkg_data_filename('galactic_center/gc_msx_e.fits')

    hdu = fits.open(filename)[0]
    wcs = WCS(hdu.header)

We then create a figure using Matplotlib and create the axes using the
:class:`~astropy.wcs.WCS` object created above. The following example shows how
to do this with the Matplotlib 'pyplot' interface, keeping a reference to the
axes object:

.. plot::
   :context:
   :include-source:
   :align: center

    import matplotlib.pyplot as plt
    ax = plt.subplot(projection=wcs)

The ``ax`` object created is an instance of the
:class:`~astropy.visualization.wcsaxes.WCSAxes` class. Note that if no WCS
transformation is specified, the transformation will default to identity,
meaning that the world coordinates will match the pixel coordinates.

The field of view shown is, as for standard matplotlib axes, 0 to 1 in both
directions, in pixel coordinates. As soon as you show an image (see
:doc:`images_contours`), the limits will be adjusted, but if you want you can
also adjust the limits manually. Adjusting the limits is done using the
same functions/methods as for a normal Matplotlib plot:

.. plot::
   :context:
   :include-source:
   :align: center

    ax.set_xlim(-0.5, hdu.data.shape[1] - 0.5)
    ax.set_ylim(-0.5, hdu.data.shape[0] - 0.5)

.. note:: If you use the pyplot interface, you can also replace ``ax.set_xlim`` and
          ``ax.set_ylim`` by ``plt.xlim`` and ``plt.ylim``.

Alternative methods
*******************

As in Matplotlib, there are in fact several ways you can initialize the
:class:`~astropy.visualization.wcsaxes.WCSAxes`.

As shown above, the simplest way is to make use of the :class:`~astropy.wcs.WCS`
class and pass this to ``plt.subplot``. If you normally use the (partially)
object-oriented interface of Matplotlib, you can also do::

    fig = plt.figure()
    ax = fig.add_subplot(1, 1, 1, projection=wcs)

Note that this also works with :meth:`~matplotlib.figure.Figure.add_axes` and
:func:`~matplotlib.pyplot.axes`, e.g.::

    ax = fig.add_axes([0.1, 0.1, 0.8, 0.8], projection=wcs)

or::

    plt.axes([0.1, 0.1, 0.8, 0.8], projection=wcs)

Any additional arguments passed to
:meth:`~matplotlib.figure.Figure.add_subplot`,
:meth:`~matplotlib.figure.Figure.add_axes`,
:func:`~matplotlib.pyplot.subplot`, or :func:`~matplotlib.pyplot.axes`, such
as ``slices`` or ``frame_class``, will be passed on to the
:class:`~astropy.visualization.wcsaxes.WCSAxes` class.

.. _initialize_alternative:

Directly initializing WCSAxes
*****************************

As an alternative to the above methods of initializing
:class:`~astropy.visualization.wcsaxes.WCSAxes`, you can also instantiate
:class:`~astropy.visualization.wcsaxes.WCSAxes` directly and add it to the
figure::

    from astropy.wcs import WCS
    from astropy.visualization.wcsaxes import WCSAxes
    import matplotlib.pyplot as plt

    wcs = WCS(...)

    fig = plt.figure()
    ax = WCSAxes(fig, [0.1, 0.1, 0.8, 0.8], wcs=wcs)
    fig.add_axes(ax)  # note that the axes have to be explicitly added to the figure
**********************************
Ticks, tick labels, and grid lines
**********************************

For the example in the following page we start from the example introduced in
:ref:`initialization`.

.. plot::
   :context: reset
   :nofigs:

    from astropy.wcs import WCS
    from astropy.io import fits
    from astropy.utils.data import get_pkg_data_filename

    filename = get_pkg_data_filename('galactic_center/gc_msx_e.fits')

    hdu = fits.open(filename)[0]
    wcs = WCS(hdu.header)

    import matplotlib.pyplot as plt

    ax = plt.subplot(projection=wcs)
    ax.imshow(hdu.data, vmin=-2.e-5, vmax=2.e-4, origin='lower')

.. _coordinateobjects:

Coordinate objects
******************

While for many images, the coordinate axes are aligned with the pixel axes,
this is not always the case, especially if there is any rotation in the world
coordinate system, or in coordinate systems with high curvature, where the
coupling between x- and y-axis to actual coordinates become less well-defined.

Therefore rather than referring to ``x`` and ``y`` ticks as Matplotlib does,
we use specialized objects to access the coordinates. The coordinates used in
the plot can be accessed using the ``coords`` attribute of the axes. As a
reminder, if you use the pyplot interface, you can grab a reference to the axes
when creating a subplot::

    ax = plt.subplot()

or you can call ``plt.gca()`` at any time to get the current active axes::

    ax = plt.gca()

If you use the object-oriented interface to Matplotlib, you should already
have a reference to the axes.

Once you have an axes object, the coordinates can either be accessed by index::

    lon = ax.coords[0]
    lat = ax.coords[1]

or, in the case of common coordinate systems, by their name:

.. plot::
   :context:
   :include-source:
   :nofigs:

    lon = ax.coords['glon']
    lat = ax.coords['glat']

In this example, the image is in Galactic coordinates, so the coordinates are
called ``glon`` and ``glat``. For an image in equatorial coordinates, you
would use ``ra`` and ``dec``. The names are only available for specific
celestial coordinate systems - for all other systems, you should use the index
of the coordinate (``0`` or ``1``).

Each coordinate is an instance of the
:class:`~astropy.visualization.wcsaxes.coordinate_helpers.CoordinateHelper` class, which can be used
to control the appearance of the ticks, tick labels, grid lines, and axis
labels associated with that coordinate.

Axis labels
***********

Axis labels can be added using the
:meth:`~astropy.visualization.wcsaxes.coordinate_helpers.CoordinateHelper.set_axislabel` method:

.. plot::
   :context:
   :include-source:
   :align: center

    lon.set_axislabel('Galactic Longitude')
    lat.set_axislabel('Galactic Latitude')

The padding of the axis label with respect to the axes can also be adjusted by
using the ``minpad`` option. The default value for ``minpad`` is 1 and is in
terms of the font size of the axis label text. Negative values are also
allowed.

.. plot::
   :context:
   :include-source:
   :align: center

    lon.set_axislabel('Galactic Longitude', minpad=0.3)
    lat.set_axislabel('Galactic Latitude', minpad=-0.4)


.. plot::
   :context:
   :nofigs:

    lon.set_axislabel('Galactic Longitude', minpad=1)
    lat.set_axislabel('Galactic Latitude', minpad=1)

.. note:: Note that, as shown in :ref:`wcsaxes-getting-started`, it is also
          possible to use the normal ``plt.xlabel`` or ``ax.set_xlabel``
          notation to set the axis labels in the case where they do appear on
          the x and y axis.

.. _tick_label_format:

Tick label format
*****************

The format of the tick labels can be specified with a string describing the
format:

.. plot::
   :context:
   :include-source:
   :align: center

    lon.set_major_formatter('dd:mm:ss.s')
    lat.set_major_formatter('dd:mm')

The syntax for the format string is the following:

==================== ====================
       format              result
==================== ====================
``'dd'``              ``'15d'``
``'dd:mm'``           ``'15d24m'``
``'dd:mm:ss'``        ``'15d23m32s'``
``'dd:mm:ss.s'``      ``'15d23m32.0s'``
``'dd:mm:ss.ssss'``   ``'15d23m32.0316s'``
``'hh'``              ``'1h'``
``'hh:mm'``           ``'1h02m'``
``'hh:mm:ss'``        ``'1h01m34s'``
``'hh:mm:ss.s'``      ``'1h01m34.1s'``
``'hh:mm:ss.ssss'``   ``'1h01m34.1354s'``
``'d'``               ``'15'``
``'d.d'``             ``'15.4'``
``'d.dd'``            ``'15.39'``
``'d.ddd'``           ``'15.392'``
``'m'``               ``'924'``
``'m.m'``             ``'923.5'``
``'m.mm'``            ``'923.53'``
``'s'``               ``'55412'``
``'s.s'``             ``'55412.0'``
``'s.ss'``            ``'55412.03'``
``'x.xxxx'``          ``'15.3922'``
``'%.2f'``            ``'15.39'``
``'%.3f'``            ``'15.392'``
``'%d'``              ``'15'``
==================== ====================

All the ``h...``, ``d...``, ``m...``, and ``s...`` formats can be used for
angular coordinate axes, while the ``x...`` format or valid Python formats
(see `String Formatting Operations
<https://docs.python.org/3/library/stdtypes.html#string-formatting>`_) should
be used for non-angular coordinate axes.

The separators for angular coordinate tick labels can also be set by
specifying a string or a tuple.

.. plot::
   :context:
   :include-source:
   :align: center

    lon.set_separator(('d', "'", '"'))
    lat.set_separator(':-s')


Tick/label spacing and properties
*********************************

The spacing of ticks/tick labels should have a sensible default, but you may
want to be able to manually specify the spacing. This can be done using the
:meth:`~astropy.visualization.wcsaxes.coordinate_helpers.CoordinateHelper.set_ticks` method. There
are different options that can be used:

* Set the tick positions manually as an Astropy :class:`~astropy.units.quantity.Quantity`::

      from astropy import units as u
      lon.set_ticks([242.2, 242.3, 242.4] * u.degree)

* Set the spacing between ticks also as an Astropy :class:`~astropy.units.quantity.Quantity`::

      lon.set_ticks(spacing=5. * u.arcmin)

* Set the approximate number of ticks::

      lon.set_ticks(number=4)

In the case of angular axes, specifying the spacing as an Astropy
:class:`~astropy.units.quantity.Quantity` avoids roundoff errors. The
:meth:`~astropy.visualization.wcsaxes.coordinate_helpers.CoordinateHelper.set_ticks` method can also
be used to set the appearance (color and size) of the ticks, using the
``color=`` and ``size=`` options.

The :meth:`~astropy.visualization.wcsaxes.coordinate_helpers.CoordinateHelper.set_ticklabel` method can be used
to change settings for the tick labels, such as color, font, size, and so on::

    lon.set_ticklabel(color='red', size=12)

In addition, this method has an option ``exclude_overlapping=True`` to prevent
overlapping tick labels from being displayed.

We can apply this to the previous example:

.. plot::
   :context:
   :include-source:
   :align: center

    from astropy import units as u
    lon.set_ticks(spacing=10 * u.arcmin, color='white')
    lat.set_ticks(spacing=10 * u.arcmin, color='white')
    lon.set_ticklabel(exclude_overlapping=True)
    lat.set_ticklabel(exclude_overlapping=True)

Minor ticks
***********

WCSAxes does not display minor ticks by default but these can be shown by
using the
:meth:`~astropy.visualization.wcsaxes.coordinate_helpers.CoordinateHelper.display_minor_ticks`
method. The default frequency of minor ticks is 5 but this can also be
specified.

.. plot::
   :context:
   :include-source:
   :align: center

    lon.display_minor_ticks(True)
    lat.display_minor_ticks(True)
    lat.set_minor_frequency(10)

Tick, tick label, and axis label position
*****************************************

By default, the tick and axis labels for the first coordinate are shown on the
x-axis, and the tick and axis labels for the second coordinate are shown on
the y-axis. In addition, the ticks for both coordinates are shown on all axes.
This can be customized using the
:meth:`~astropy.visualization.wcsaxes.coordinate_helpers.CoordinateHelper.set_ticks_position` and
:meth:`~astropy.visualization.wcsaxes.coordinate_helpers.CoordinateHelper.set_ticklabel_position` methods, which each
take a string that can contain any or several of ``l``, ``b``, ``r``, or ``t``
(indicating the ticks or tick labels should be shown on the left, bottom,
right, or top axes respectively):

.. plot::
   :context:
   :include-source:
   :align: center

    lon.set_ticks_position('bt')
    lon.set_ticklabel_position('bt')
    lon.set_axislabel_position('bt')
    lat.set_ticks_position('lr')
    lat.set_ticklabel_position('lr')
    lat.set_axislabel_position('lr')

We can set the defaults back using:

.. plot::
   :context:
   :include-source:
   :align: center

    lon.set_ticks_position('all')
    lon.set_ticklabel_position('b')
    lon.set_axislabel_position('b')
    lat.set_ticks_position('all')
    lat.set_ticklabel_position('l')
    lat.set_axislabel_position('l')

On plots with elliptical frames, three alternate tick positions are supported:
``c`` for the outer circular or elliptical border, ``h`` for the horizontal
axis (which is usually the major axis of the ellipse), and ``v`` for the
vertical axis (which is usually the minor axis of the ellipse).


Hiding ticks and tick labels
****************************

Sometimes it's desirable to hide ticks and tick labels. A common scenario
is where WCSAxes is being used in a grid of subplots and the tick labels
are redundant across rows or columns. Tick labels and ticks can be hidden with
the :meth:`~astropy.visualization.wcsaxes.coordinate_helpers.CoordinateHelper.set_ticklabel_visible`
and :meth:`~astropy.visualization.wcsaxes.coordinate_helpers.CoordinateHelper.set_ticks_visible`
methods, respectively:

.. plot::
   :context:
   :include-source:
   :align: center

    lon.set_ticks_visible(False)
    lon.set_ticklabel_visible(False)
    lat.set_ticks_visible(False)
    lat.set_ticklabel_visible(False)
    lon.set_axislabel('')
    lat.set_axislabel('')

And we can restore the ticks and tick labels again using:

.. plot::
   :context:
   :include-source:
   :align: center

    lon.set_ticks_visible(True)
    lon.set_ticklabel_visible(True)
    lat.set_ticks_visible(True)
    lat.set_ticklabel_visible(True)
    lon.set_axislabel('Galactic Longitude')
    lat.set_axislabel('Galactic Latitude')


Coordinate grid
***************

Since the properties of a coordinate grid are linked to the properties of the
ticks and labels, grid lines 'belong' to the coordinate objects described
above. For example, you can show a grid with yellow lines for RA and orange lines
for declination with:

.. plot::
   :context:
   :include-source:
   :align: center

    lon.grid(color='yellow', alpha=0.5, linestyle='solid')
    lat.grid(color='orange', alpha=0.5, linestyle='solid')

For convenience, you can also simply draw a grid for all the coordinates in
one command:

.. plot::
   :context:
   :include-source:
   :align: center

    ax.coords.grid(color='white', alpha=0.5, linestyle='solid')

.. note:: If you use the pyplot interface, you can also plot the grid using
          ``plt.grid()``.
*******************************************
Initializing WCSAxes with custom transforms
*******************************************

In :ref:`initialization`, we saw how to make plots using
:class:`~astropy.wcs.WCS` objects. However, the
:class:`~astropy.visualization.wcsaxes.WCSAxes` class can also be initialized
with more general transformations that don't have to be represented by the
:class:`~astropy.wcs.WCS` class. Instead, you can initialize
:class:`~astropy.visualization.wcsaxes.WCSAxes` using a Matplotlib
:class:`~matplotlib.transforms.Transform` object and a dictionary
(``coord_meta``) that provides metadata on how to interpret the transformation.

The :class:`~matplotlib.transforms.Transform` should represent the conversion
from pixel to world coordinates, and should have ``input_dims=2`` and can have
``output_dims`` set to any positive integer. In addition, ``has_inverse`` should
be set to `True` and the ``inverted`` method should be implemented.

The ``coord_meta`` dictionary should include the following keys:

* ``name``: an iterable of strings giving the names for each dimension
* ``type``: an iterable of strings that should be either ``'longitude'``,
  ``'latitude'``, or ``'scalar'`` (for anything that isn't a longitude or latitude).
* ``wrap``: an iterable of values which indicate for longitudes at which
  angle (in degrees) to wrap the coordinates. This should be `None` unless
  ``type`` is ``'longitude'``.
* ``unit``: an iterable of :class:`~astropy.units.Unit` objects giving the
  units of the world coordinates returned by the
  :class:`~matplotlib.transforms.Transform`.
* ``format_unit``: an iterable of :class:`~astropy.units.Unit` objects
  giving the units to use for the formatting of the labels. These can be set to
  `None` to default to the units given in ``unit``, but can be set for example
  if the :class:`~matplotlib.transforms.Transform` returns values in degrees
  and you want the labels to be formatted in hours.

In addition the ``coord_meta`` can optionally include the following keys:

* ``default_axislabel_position``: an iterable of strings giving for each world
  coordinates the spine of the frame on which to show the axis label for the
  coordinate. Each string should be such that it could be used as input to
  :meth:`~astropy.visualization.wcsaxes.coordinate_helpers.CoordinateHelper.set_axislabel_position`.

* ``default_ticklabel_position``: an iterable of strings giving for each world
  coordinates the spine of the frame on which to show the tick labels for the
  coordinate. Each string should be such that it could be used as input to
  :meth:`~astropy.visualization.wcsaxes.coordinate_helpers.CoordinateHelper.set_ticklabel_position`.

* ``default_ticks_position``: an iterable of strings giving for each world
  coordinates the spine of the frame on which to show the ticks for the
  coordinate. Each string should be such that it could be used as input to
  :meth:`~astropy.visualization.wcsaxes.coordinate_helpers.CoordinateHelper.set_ticks_position`.

The following example illustrates a custom projection using a transform and
``coord_meta``:

.. plot::
   :context: reset
   :include-source:
   :align: center

    from astropy import units as u
    import matplotlib.pyplot as plt
    from matplotlib.transforms import Affine2D
    from astropy.visualization.wcsaxes import WCSAxes

    # Set up an affine transformation
    transform = Affine2D()
    transform.scale(0.01)
    transform.translate(40, -30)
    transform.rotate(0.3)  # radians

    # Set up metadata dictionary
    coord_meta = {}
    coord_meta['name'] = 'lon', 'lat'
    coord_meta['type'] = 'longitude', 'latitude'
    coord_meta['wrap'] = 180, None
    coord_meta['unit'] = u.deg, u.deg
    coord_meta['format_unit'] = None, None

    fig = plt.figure()
    ax = WCSAxes(fig, [0.1, 0.1, 0.8, 0.8], aspect='equal',
                 transform=transform, coord_meta=coord_meta)
    fig.add_axes(ax)
    ax.set_xlim(-0.5, 499.5)
    ax.set_ylim(-0.5, 399.5)
    ax.grid()
    ax.coords['lon'].set_axislabel('Longitude')
    ax.coords['lat'].set_axislabel('Latitude')
******************
Controlling Axes
******************

Changing Axis Units
*******************

WCSAxes also allows users to change the units of the axes of an image. In the
example in :doc:`slicing_datacubes`, the x axis represents velocity in m/s. We
can change the unit to an equivalent one by:


.. plot::
   :context: reset
   :nofigs:

    from astropy.wcs import WCS
    from astropy.io import fits
    from astropy.utils.data import get_pkg_data_filename

    filename = get_pkg_data_filename('l1448/l1448_13co.fits')
    hdu = fits.open(filename)[0]
    wcs = WCS(hdu.header)

    import matplotlib.pyplot as plt

    ax = plt.subplot(projection=wcs, slices=(50, 'y', 'x'))
    ax.imshow(hdu.data[:, :, 50].transpose())

.. plot::
   :context:
   :include-source:
   :align: center

    import astropy.units as u
    ax.coords[2].set_major_formatter('x.x') # Otherwise values round to the nearest whole number
    ax.coords[2].set_format_unit(u.km / u.s)


Disabling Automatic Labelling
*****************************

By default WCSAxes adds labels to the axes to indicate what world coordinate is
being represented on that axis, and what unit is being used to display it. If
you want to disable this behavior you can either set an explicit label for that
axis with `~astropy.visualization.wcsaxes.CoordinateHelper.set_axislabel` or you
can disable the feature per coordinate with::

  ax = plt.subplot(projection=wcs)  # doctest: +SKIP
  ax.coords[0].set_auto_axislabel(False)  # doctest: +SKIP


Changing Axis Directions
************************

Sometimes astronomy FITS files don't follow the convention of having the longitude increase to the left,
so we want to flip an axis so that it goes in the opposite direction. To do this on our example image:

.. plot::
   :context:
   :include-source:
   :align: center

    ax.invert_xaxis()
*****************************
Overlaying coordinate systems
*****************************

For the example in the following page we start from the example introduced in
:ref:`initialization`.

.. plot::
   :context: reset
   :nofigs:

    from astropy.wcs import WCS
    from astropy.io import fits
    from astropy.utils.data import get_pkg_data_filename

    filename = get_pkg_data_filename('galactic_center/gc_msx_e.fits')

    hdu = fits.open(filename)[0]
    wcs = WCS(hdu.header)

    import matplotlib.pyplot as plt

    ax = plt.subplot(projection=wcs)
    ax.imshow(hdu.data, vmin=-2.e-5, vmax=2.e-4, origin='lower')

The coordinates shown by default in a plot will be those derived from the WCS
or transformation passed to the :class:`~astropy.visualization.wcsaxes.WCSAxes` class.
However, it is possible to overlay different coordinate systems using the
:meth:`~astropy.visualization.wcsaxes.WCSAxes.get_coords_overlay` method:

.. plot::
   :context:
   :include-source:
   :align: center

    overlay = ax.get_coords_overlay('fk5')

The object returned is a :class:`~astropy.visualization.wcsaxes.coordinates_map.CoordinatesMap`, the
same type of object as ``ax.coord``. It can therefore be used in the same way
as ``ax.coord`` to set the ticks, tick labels, and axis labels properties:

.. plot::
   :context:
   :include-source:
   :align: center

    ax.coords['glon'].set_ticks(color='white')
    ax.coords['glat'].set_ticks(color='white')

    ax.coords['glon'].set_axislabel('Galactic Longitude')
    ax.coords['glat'].set_axislabel('Galactic Latitude')

    ax.coords.grid(color='yellow', linestyle='solid', alpha=0.5)

    overlay['ra'].set_ticks(color='white')
    overlay['dec'].set_ticks(color='white')

    overlay['ra'].set_axislabel('Right Ascension')
    overlay['dec'].set_axislabel('Declination')

    overlay.grid(color='white', linestyle='solid', alpha=0.5)
****************************
Plotting images and contours
****************************

For the example in the following page we start from the example introduced in
:ref:`initialization`.

.. plot::
   :context: reset
   :nofigs:

    from astropy.wcs import WCS
    from astropy.io import fits
    from astropy.utils.data import get_pkg_data_filename

    filename = get_pkg_data_filename('galactic_center/gc_msx_e.fits')

    hdu = fits.open(filename)[0]
    wcs = WCS(hdu.header)

    import matplotlib.pyplot as plt

    ax = plt.subplot(projection=wcs)
    ax.imshow(hdu.data, vmin=-2.e-5, vmax=2.e-4, origin='lower')

Plotting images as bitmaps or contours should be done via the usual matplotlib
methods such as :meth:`~matplotlib.axes.Axes.imshow` or
:meth:`~matplotlib.axes.Axes.contour`. For example, continuing from the
example in :ref:`initialization`, you can do:

.. plot::
   :context:
   :include-source:
   :align: center

    ax.imshow(hdu.data, vmin=-2.e-5, vmax=2.e-4, origin='lower')

and we can also add contours corresponding to the same image using:

.. plot::
   :context:
   :include-source:
   :align: center

    import numpy as np
    ax.contour(hdu.data, levels=np.logspace(-4.7, -3., 10), colors='white', alpha=0.5)

To show contours for an image in a different coordinate system, see
:doc:`overlays`.

.. note:: If you like using the pyplot interface, you can also call
          ``plt.imshow`` and ``plt.contour`` instead of ``ax.imshow`` and
          ``ax.contour``.
********************
Using a custom frame
********************

By default, `~astropy.visualization.wcsaxes.WCSAxes` will make use of a rectangular
frame for a plot, but this can be changed to provide any custom frame. The
following example shows how to use the built-in
:class:`~astropy.visualization.wcsaxes.frame.EllipticalFrame` class, which is an ellipse which extends to the same limits as the built-in rectangular frame:

.. plot::
   :context: reset
   :include-source:
   :align: center

    from astropy.wcs import WCS
    from astropy.io import fits
    from astropy.utils.data import get_pkg_data_filename
    from astropy.visualization.wcsaxes.frame import EllipticalFrame
    import matplotlib.pyplot as plt

    filename = get_pkg_data_filename('galactic_center/gc_msx_e.fits')
    hdu = fits.open(filename)[0]
    wcs = WCS(hdu.header)

    ax = plt.subplot(projection=wcs, frame_class=EllipticalFrame)

    ax.coords.grid(color='white')

    im = ax.imshow(hdu.data, vmin=-2.e-5, vmax=2.e-4, origin='lower')

    # Clip the image to the frame
    im.set_clip_path(ax.coords.frame.patch)

The :class:`~astropy.visualization.wcsaxes.frame.EllipticalFrame` class is especially useful for
all-sky plots such as Aitoff projections:

.. plot::
   :context: reset
   :include-source:
   :align: center

    from astropy.wcs import WCS
    from astropy.io import fits
    from astropy.utils.data import get_pkg_data_filename
    from astropy.visualization.wcsaxes.frame import EllipticalFrame
    from matplotlib import patheffects
    import matplotlib.pyplot as plt

    filename = get_pkg_data_filename('allsky/allsky_rosat.fits')
    hdu = fits.open(filename)[0]
    wcs = WCS(hdu.header)

    ax = plt.subplot(projection=wcs, frame_class=EllipticalFrame)

    path_effects=[patheffects.withStroke(linewidth=3, foreground='black')]
    ax.coords.grid(color='white')
    ax.coords['glon'].set_ticklabel(color='white', path_effects=path_effects)

    im = ax.imshow(hdu.data, vmin=0., vmax=300., origin='lower')

    # Clip the image to the frame
    im.set_clip_path(ax.coords.frame.patch)

However, you can also write your own frame class. The idea is to set up any
number of connecting spines that define the frame. You can define a frame as a
spine, but if you define it as multiple spines you will be able to control on
which spine the tick labels and ticks should appear.

The following example shows how you could for example define a hexagonal frame:

.. plot::
   :context: reset
   :include-source:
   :nofigs:

    import numpy as np
    from astropy.visualization.wcsaxes.frame import BaseFrame

    class HexagonalFrame(BaseFrame):

        spine_names = 'abcdef'

        def update_spines(self):

            xmin, xmax = self.parent_axes.get_xlim()
            ymin, ymax = self.parent_axes.get_ylim()

            ymid = 0.5 * (ymin + ymax)
            xmid1 = (xmin + xmax) / 4.
            xmid2 = (xmin + xmax) * 3. / 4.

            self['a'].data = np.array(([xmid1, ymin], [xmid2, ymin]))
            self['b'].data = np.array(([xmid2, ymin], [xmax, ymid]))
            self['c'].data = np.array(([xmax, ymid], [xmid2, ymax]))
            self['d'].data = np.array(([xmid2, ymax], [xmid1, ymax]))
            self['e'].data = np.array(([xmid1, ymax], [xmin, ymid]))
            self['f'].data = np.array(([xmin, ymid], [xmid1, ymin]))

which we can then use:

.. plot::
    :context:
    :include-source:
    :align: center

     from astropy.wcs import WCS
     from astropy.io import fits
     from astropy.utils.data import get_pkg_data_filename
     import matplotlib.pyplot as plt

     filename = get_pkg_data_filename('galactic_center/gc_msx_e.fits')
     hdu = fits.open(filename)[0]
     wcs = WCS(hdu.header)

     ax = plt.subplot(projection=wcs, frame_class=HexagonalFrame)

     ax.coords.grid(color='white')

     im = ax.imshow(hdu.data, vmin=-2.e-5, vmax=2.e-4, origin='lower')

     # Clip the image to the frame
     im.set_clip_path(ax.coords.frame.patch)


Frame properties
****************

The color and linewidth of the frame can also be set by

.. plot::
    :context:
    :include-source:
    :align: center

    ax.coords.frame.set_color('red')
    ax.coords.frame.set_linewidth(2)
*****************************
Slicing Multidimensional Data
*****************************

WCSAxes can either plot one or two dimensional data. If we have a dataset with
higher dimensionality than the plot we want to make, we have to select which
dimensions to use for the x or x and y axes of the plot. This example will show
how to slice a FITS data cube and plot an image from it.

Slicing the WCS object
**********************

Like the example introduced in :ref:`initialization`, we will read in the
data using `astropy.io.fits
<https://docs.astropy.org/en/stable/io/fits/index.html>`_ and parse the WCS
information. The original FITS file can be downloaded from `here
<http://www.astropy.org/astropy-data/l1448/l1448_13co.fits>`_.

.. plot::
   :context: reset
   :include-source:
   :align: center
   :nofigs:

    import matplotlib.pyplot as plt
    from astropy.wcs import WCS
    from astropy.io import fits
    from astropy.utils.data import get_pkg_data_filename
    filename = get_pkg_data_filename('l1448/l1448_13co.fits')
    hdu = fits.open(filename)[0]
    wcs = WCS(hdu.header)
    image_data = hdu.data

This is a three-dimensional dataset which you can check by looking at the
header information by::

    >>> hdu.header  # doctest: +SKIP
    ...
    NAXIS = 3 /number of axes
    CTYPE1  = 'RA---SFL'           /
    CTYPE2  = 'DEC--SFL'           /
    CTYPE3  = 'VELO-LSR'           /
    ...

The header keyword 'NAXIS' gives the number of dimensions of the dataset. The keywords 'CTYPE1', 'CTYPE2' and 'CTYPE3' give the data type of these dimensions to be right ascension, declination and velocity respectively.

We then instantiate the `~astropy.visualization.wcsaxes.WCSAxes` using the
:class:`~astropy.wcs.WCS` object and select the slices we want to plot:

.. plot::
   :context:
   :include-source:
   :align: center
   :nofigs:

    import matplotlib.pyplot as plt
    ax = plt.subplot(projection=wcs, slices=(50, 'y', 'x'))

By setting ``slices=(50, 'y', 'x')``, we have chosen to plot the second
dimension on the y-axis and the third dimension on the x-axis. Even though we
are not plotting the all the dimensions, we have to specify which slices to
select for the dimensions that are not shown. In this example, we are not
plotting the first dimension so we have selected the slice 50 to display. You
can experiment with this by changing the selected slice and looking at how the
plotted image changes.

Plotting the image
******************

We then add the axes to the image and plot it using the method
:meth:`~matplotlib.axes.Axes.imshow`.

.. plot::
   :context:
   :include-source:
   :align: center

    ax.coords[2].set_ticklabel(exclude_overlapping=True)
    ax.imshow(image_data[:, :, 50].transpose())

Here, ``image_data`` is an :class:`~numpy.ndarray` object. In Numpy, the order
of the axes is reversed so the first dimension in the FITS file appears last,
the last dimension appears first and so on. Therefore the index passed to
:meth:`~matplotlib.axes.Axes.imshow` should be the same as passed to
``slices`` but in reversed order. We also need to
:meth:`~numpy.ndarray.transpose` ``image_data`` as we have reversed the
dimensions plotted on the x and y axes in the slice.

If we don't want to reverse the dimensions plotted, we can simply do:

.. plot::
   :context: reset
   :align: center
   :nofigs:

    import astropy.units as u
    from astropy.wcs import WCS
    from astropy.io import fits
    from astropy.utils.data import get_pkg_data_filename
    filename = get_pkg_data_filename('l1448/l1448_13co.fits')
    hdu = fits.open(filename)[0]
    wcs = WCS(hdu.header)
    image_data = hdu.data

.. plot::
   :context:
   :include-source:
   :align: center

    import matplotlib.pyplot as plt
    ax = plt.subplot(projection=wcs, slices=(50, 'x', 'y'))
    ax.imshow(image_data[:, :, 50])


Plotting one dimensional data
*****************************

If we wanted to plot the spectral axes for one pixel we can do this by slicing
down to one dimension.

.. plot::
   :context:
   :include-source:
   :align: center
   :nofigs:

    import matplotlib.pyplot as plt
    ax = plt.subplot(projection=wcs, slices=(50, 50, 'x'))


Here we have selected the 50 pixel in the first and second dimensions and will
use the third dimension as our x axis.

We can now plot the spectral axis for this pixel. Note that we are plotting
against pixel coordinates in the call to ``ax.plot``, ``WCSAxes`` will display
the world coordinates for us.

.. plot::
   :context:
   :include-source:
   :align: center
   :nofigs:

   ax.plot(image_data[:, 50, 50])

As this is still a ``WCSAxes`` plot, we can set the display units for the x-axis

.. plot::
   :context:
   :include-source:
   :align: center

   ra, dec, vel = ax.coords
   vel.set_format_unit(u.km/u.s)


If we wanted to plot a one dimensional plot along a spatial dimension, i.e.
intensity along a row in the image, ``WCSAxes`` defaults to displaying both the
world coordinates for this plot. We can customise the colors and add grid lines
for each of the spatial axes.

.. plot::
   :context:
   :include-source:
   :align: center
   :nofigs:

    import matplotlib.pyplot as plt
    ax = plt.subplot(projection=wcs, slices=(50, 'x', 0))

.. plot::
   :context:
   :include-source:
   :align: center

   ax.plot(image_data[0, :, 50])

   ra, dec, wave = ax.coords
   ra.set_ticks(color="red")
   ra.set_ticklabel(color="red")
   ra.grid(color="red")

   dec.set_ticks(color="blue")
   dec.set_ticklabel(color="blue")
   dec.grid(color="blue")
.. _wcsaxes:

*********************************************
Making plots with world coordinates (WCSAxes)
*********************************************

WCSAxes is a framework for making plots of Astronomical data in `Matplotlib`_.
It was previously distributed as a standalone package (``wcsaxes``), but is now included in
:ref:`astropy.visualization <astropy-visualization>`.

.. _wcsaxes-getting-started:

Getting started
===============

The following is a very simple example of plotting an image with the WCSAxes
package:

.. plot::
   :context: reset
   :include-source:
   :align: center

    import matplotlib.pyplot as plt

    from astropy.wcs import WCS
    from astropy.io import fits
    from astropy.utils.data import get_pkg_data_filename

    filename = get_pkg_data_filename('galactic_center/gc_msx_e.fits')

    hdu = fits.open(filename)[0]
    wcs = WCS(hdu.header)

    plt.subplot(projection=wcs)
    plt.imshow(hdu.data, vmin=-2.e-5, vmax=2.e-4, origin='lower')
    plt.grid(color='white', ls='solid')
    plt.xlabel('Galactic Longitude')
    plt.ylabel('Galactic Latitude')

This example uses the :mod:`matplotlib.pyplot` interface to Matplotlib, but WCSAxes
can be used with any of the other ways of using Matplotlib (some examples of which
are given in :ref:`initialization`). For example, using the partially object-oriented
interface, you can do::

    ax = plt.subplot(projection=wcs)
    ax.imshow(hdu.data, vmin=-2.e-5, vmax=2.e-4, origin='lower')
    ax.grid(color='white', ls='solid')
    ax.set_xlabel('Galactic Longitude')
    ax.set_ylabel('Galactic Latitude')

However, the axes object is needed to access some of the more advanced functionality
of WCSAxes.  An example of this usage is:

.. plot::
   :context:
   :include-source:
   :align: center

    ax = plt.subplot(projection=wcs, label='overlays')

    ax.imshow(hdu.data, vmin=-2.e-5, vmax=2.e-4, origin='lower')

    ax.coords.grid(True, color='white', ls='solid')
    ax.coords[0].set_axislabel('Galactic Longitude')
    ax.coords[1].set_axislabel('Galactic Latitude')

    overlay = ax.get_coords_overlay('fk5')
    overlay.grid(color='white', ls='dotted')
    overlay[0].set_axislabel('Right Ascension (J2000)')
    overlay[1].set_axislabel('Declination (J2000)')

In the rest of this documentation we will assume that you have kept a reference
to the axes object, which we will refer to as ``ax``. However, we also note
when something can be done directly with the pyplot interface.

WCSAxes supports a number of advanced plotting options, including the ability to
control which axes to show labels on for which coordinates, overlaying contours
from data with different coordinate systems, overlaying grids for different
coordinate systems, dealing with plotting slices from data with more dimensions
than the plot, and defining custom (non-rectangular) frames.

Using WCSAxes
=============

.. toctree::
   :maxdepth: 1

   initializing_axes
   images_contours
   ticks_labels_grid
   overlays
   overlaying_coordinate_systems
   slicing_datacubes
   controlling_axes
   generic_transforms
   custom_frames

Reference/API
=============

.. automodapi:: astropy.visualization.wcsaxes
   :no-inheritance-diagram:

.. automodapi:: astropy.visualization.wcsaxes.frame
   :no-inheritance-diagram:
********************************************************************
Miscellaneous: HDF5, YAML, ASDF, Parquet, pickle (`astropy.io.misc`)
********************************************************************

The `astropy.io.misc` module contains miscellaneous input/output routines that
do not fit elsewhere, and are often used by other ``astropy`` sub-packages. For
example, `astropy.io.misc.hdf5` contains functions to read/write
:class:`~astropy.table.Table` objects from/to HDF5 files, but these
should not be imported directly by users. Instead, users can access this
functionality via the :class:`~astropy.table.Table` class itself (see
:ref:`table_io`). Routines that are intended to be used directly by users are
listed in the `astropy.io.misc` section.

.. automodapi:: astropy.io.misc
   :headings: =-

.. automodapi:: astropy.io.misc.hdf5
   :headings: =-

.. automodapi:: astropy.io.misc.yaml
   :headings: =-

.. automodapi:: astropy.io.misc.parquet
   :headings: =-

astropy.io.misc.asdf Package
============================

The **asdf** sub-package contains code that is used to serialize ``astropy``
types so that they can be represented and stored using the Advanced Scientific
Data Format (ASDF).

If both **asdf** and **astropy** are installed, no further configuration is
required in order to process ASDF files that contain **astropy** types. The
**asdf** package has been designed to automatically detect the presence of the
tags defined by **astropy**.

For convenience, users can write `~astropy.table.Table` objects to ASDF files
using the :ref:`table_io`. See :ref:`asdf_io` below.

Documentation on the ASDF Standard can be found `here
<https://asdf-standard.readthedocs.io>`__. Documentation on the ASDF Python
module can be found `here <https://asdf.readthedocs.io>`__. Additional details
for Astropy developers can be found in :ref:`asdf_dev`.

.. _asdf_io:

Using ASDF With Table I/O
-------------------------

ASDF provides readers and writers for `~astropy.table.Table` using the
:ref:`table_io`. This makes it convenient to read and write ASDF files with
`~astropy.table.Table` data.

Basic Usage
^^^^^^^^^^^

Given a table, it is possible to write it out to an ASDF file::

    from astropy.table import Table

    # Create a simple table
    t = Table(dtype=[('a', 'f4'), ('b', 'i4'), ('c', 'S2')])
    # Write the table to an ASDF file
    t.write('table.asdf')

The I/O registry automatically selects the appropriate writer function to use
based on the ``.asdf`` extension of the output file.

Reading a file generated in this way is also possible using
`~astropy.table.Table.read`::

    t2 = Table.read('table.asdf')

The I/O registry automatically selects the appropriate reader function based on
the extension of the input file.

In the case of both reading and writing, if the file extension is not ``.asdf``
it is possible to explicitly specify the reader/writer function to be used::

    t3 = Table.read('table.zxcv', format='asdf')

Advanced Usage
^^^^^^^^^^^^^^

The fundamental ASDF data structure is the tree, which is a nested
combination of basic data structures (see `this
<https://asdf.readthedocs.io/en/latest/asdf/features.html#data-model>`_
for a more detailed description). At the top level, the tree is a `dict`.

The consequence of this is that a `~astropy.table.Table` object (or any object,
for that matter) can be stored at any arbitrary location within an ASDF tree.
The basic writer use case described above stores the given
`~astropy.table.Table` at the top of the tree using a default key. The basic
reader case assumes that a `~astropy.table.Table` is stored in the same place.

However, it may sometimes be useful for users to specify a different top-level
key to be used for storage and retrieval of a `~astropy.table.Table` from an
ASDF file. For this reason, the ASDF I/O interface provides ``data_key`` as an
optional keyword when writing and reading::

    from astropy.table import Table

    t = Table(dtype=[('a', 'f4'), ('b', 'i4'), ('c', 'S2')])
    # Write the table to an ASDF file using a non-default key
    t.write('foo.asdf', data_key='foo')

A `~astropy.table.Table` stored using a custom data key can be retrieved by
passing the same argument to `~astropy.table.Table.read`::

    foo = Table.read('foo.asdf', data_key='foo')

The ``data_key`` option only applies to `~astropy.table.Table` objects that are
stored at the top of the ASDF tree. For full generality, users may pass a
callback when writing or reading ASDF files to define precisely where the
`~astropy.table.Table` object should be placed in the tree. The option for the
write case is ``make_tree``. The function callback should accept exactly one
argument, which is the `~astropy.table.Table` object, and should return a
`dict` representing the tree to be stored::

    def make_custom_tree(table):
        # Return a nested tree where the table is stored at the second level
        return dict(foo=dict(bar=table))

    t = Table(dtype=[('a', 'f4'), ('b', 'i4'), ('c', 'S2')])
    # Write the table to an ASDF file using a non-default key
    t.write('foobar.asdf', make_tree=make_custom_tree)

Similarly, when reading an ASDF file, the user can pass a custom callback to
locate the table within the ASDF tree. The option in this case is
``find_table``. The callback should accept exactly one argument, which is an
`dict` representing the ASDF tree, and it should return a
`~astropy.table.Table` object::

    def find_table(tree):
        # This returns the Table that was stored by the example above
        return tree['foo']['bar']

    foo = Table.read('foobar.asdf', find_table=find_table)

.. _asdf_dev:

Details
-------

The **asdf** sub-package defines classes, referred to as **tags**, that
implement the logic for serialization and deserialization of ``astropy`` types.
Users should never need to refer to tag implementations directly. Their
presence should be entirely transparent when processing ASDF files.

ASDF makes use of abstract data type definitions called **schemas**. The tag
classes provided here are specific implementations of particular schemas. Some
of the tags in ``astropy`` (e.g., those related to transforms) implement schemas
that are defined by the ASDF Standard. In other cases, both the tags and
schemas are defined within ``astropy`` (e.g., those related to many of the
coordinate frames). Documentation of the individual schemas defined by
``astropy`` can be found below in the :ref:`asdf_schemas` section.

Not all ``astropy`` types are currently serializable by ASDF. Attempting to
write unsupported types to an ASDF file will lead to a ``RepresenterError``. In
order to support new types, new tags and schemas must be created. See `Writing
ASDF Extensions <https://asdf.readthedocs.io/en/latest/asdf/extending/legacy.html>`_
for additional details, as well as the following example.

Example: Adding a New Object to the Astropy ASDF Extension
----------------------------------------------------------

In this example, we will show how to implement serialization for a new
`~astropy.modeling.Model` object, but the basic principles apply to
serialization of other ``astropy`` objects. As mentioned, adding a new object
to the ``astropy``  ASDF extension requires both a tag and a schema.

All schemas for transforms are currently defined within the ASDF standard.
Any new serializable transforms must have a corresponding new
schema here. Let's consider a new model called ``MyModel``, a new model in
``astropy.modeling.functional_models`` that has two parameters ``amplitude``
and ``x_0``. We would like to strictly require both of these parameters be set.
We would also like to specify that these parameters can either be numeric type,
or ``astropy.units.quantity`` type. A schema describing this
model would look like::

    %YAML 1.1
    ---
    $schema: "http://stsci.edu/schemas/yaml-schema/draft-01"
    id: "http://stsci.edu/schemas/asdf/transform/mymodel-1.0.0"
    tag: "tag:stsci.edu:asdf/transform/mymodel-1.0.0"
    title: >
      Example new model.

    description: >
      Example new model, which describes the distribution of ABC.

    allOf:
      - $ref: "transform-1.2.0"
      - type: object
        properties:
          amplitude:
            anyOf:
              - $ref: "../unit/quantity-1.1.0"
              - type: number
            description: Amplitude of distribution.
          x_0:
            anyOf:
              - $ref: "../unit/quantity-1.1.0"
              - type: number
            description: X center position.

        required: ['amplitude', 'x_0]
    ...

All new transform schemas reference the base transform schema of the latest
type. This schema describes the other model attributes that are common to all
or many models, so that individual schemas only handle the parameters specific
to that model. Additionally, this schema references the latest version
of the ``quantity`` schema, so that models can retain information about units
and quantities. References allow previously defined objects to be used inside
new custom types.

The next component is the tag class. This class must have a ``to_tree`` method
in which the required attributes of the object in question are obtained, and a
``from_tree`` method which reconstructs the object based on the parameters
written to the ASDF file. ``astropy`` Models inherit from the
``TransformType`` base class tag, which takes care of attributes (e.g ``name``,
``bounding_box``, ``n_inputs``) that are common to all or many Model classes to
limit redundancy in individual tags. Each individual model tag then only has
to obtain and set model-specific parameters::

    from .basic import TransformType
    from . import _parameter_to_value

    class MyModelType(TransformType):
    name = 'transform/mymodel'
    version = '1.0.0'
    types = ['astropy.modeling.functional_models.MyModel']

    @classmethod
    def from_tree_transform(cls, node, ctx):
        return functional_models.MyModel(amplitude=node['amplitude'],
                                         x_0=node['x_0'])

    @classmethod
    def to_tree_transform(cls, model, ctx):
        node = {'amplitude': _parameter_to_value(amplitude),
                'x_0': _parameter_to_value(x_0)}
        return node

This tag class contains all the machinery to deconstruct objects to and
reconstruct them from ASDF files. The tag class - by convention named by the
object name appended with 'Type' - references the schema and version, and the
object in ``astropy.modeling.functional_models``. The basic model parameters
are handled in the ``to_tree_transform`` and ``from_tree_transform`` of the
base ``TransformType`` class, while model-specific parameters are handled here
in ``MyModelType``. Since this model can take units and quantities with input
parameters, the imported``_parameter_to_value`` allows this to flexibly work
with both basic numeric values as well as quantities.


Schemas
-------

Documentation for each of the individual ASDF schemas defined by ``astropy``
can be found below.

.. toctree::
   :maxdepth: 2

   asdf-schemas
.. _table_io:

Unified File Read/Write Interface
*********************************

``astropy`` provides a unified interface for reading and writing data in
different formats. For many common cases this will streamline the process of
file I/O and reduce the need to learn the separate details of all of the I/O
packages within ``astropy``. For details on the implementation see
:ref:`io_registry`.

Getting Started with Image I/O
==============================

Reading and writing image data in the unified I/O interface is supported
though the `~astropy.nddata.CCDData` class using FITS file format:

.. doctest-skip::

    >>> # Read CCD image
    >>> ccd = CCDData.read('image.fits')

.. doctest-skip::

    >>> # Write back CCD image
    >>> ccd.write('new_image.fits')

Note that the unit is stored in the ``BUNIT`` keyword in the header on saving,
and is read from the header if it is present.

Detailed help on the available keyword arguments for reading and writing
can be obtained via the ``help()`` method as follows:

.. doctest-skip::

    >>> CCDData.read.help('fits')  # Get help on the CCDData FITS reader
    >>> CCDData.writer.help('fits')  # Get help on the CCDData FITS writer

Getting Started with Table I/O
==============================

The :class:`~astropy.table.Table` class includes two methods,
:meth:`~astropy.table.Table.read` and
:meth:`~astropy.table.Table.write`, that make it possible to read from
and write to files. A number of formats are automatically supported (see
`Built-in table readers/writers`_) and new file formats and extensions can be
registered with the :class:`~astropy.table.Table` class (see
:ref:`io_registry`).

Examples
--------

..
  EXAMPLE START
  Reading a DAOPhot Table

To use this interface, first import the :class:`~astropy.table.Table` class,
then call the :class:`~astropy.table.Table`
:meth:`~astropy.table.Table.read` method with the name of the file and
the file format, for instance ``'ascii.daophot'``:

.. doctest-skip::

    >>> from astropy.table import Table
    >>> t = Table.read('photometry.dat', format='ascii.daophot')

..
  EXAMPLE END

..
  EXAMPLE START
  Reading a Table Directly from the Internet

It is possible to load tables directly from the Internet using URLs. For
example, download tables from Vizier catalogues in CDS format
(``'ascii.cds'``)::

    >>> t = Table.read("ftp://cdsarc.u-strasbg.fr/pub/cats/VII/253/snrs.dat",
    ...         readme="ftp://cdsarc.u-strasbg.fr/pub/cats/VII/253/ReadMe",
    ...         format="ascii.cds")  # doctest: +SKIP

For certain file formats the format can be automatically detected, for
example, from the filename extension::

    >>> t = Table.read('table.tex')  # doctest: +SKIP

..
  EXAMPLE END

..
  EXAMPLE START
  Writing a LaTeX Table

For writing a table, the format can be explicitly specified::

    >>> t.write(filename, format='latex')  # doctest: +SKIP

As for the :meth:`~astropy.table.Table.read` method, the format may
be automatically identified in some cases.

The underlying file handler will also automatically detect various
compressed data formats and transparently uncompress them as far as
supported by the Python installation (see
:meth:`~astropy.utils.data.get_readable_fileobj`).

For writing, you can also specify details about the `Table serialization
methods`_ via the ``serialize_method`` keyword argument. This allows
fine control of the way to write out certain columns, for instance
writing an ISO format Time column as a pair of JD1/JD2 floating
point values (for full resolution) or as a formatted ISO date string.

..
  EXAMPLE END

Getting Help on Readers and Writers
-----------------------------------

Each file format is handled by a specific reader or writer, and each of those
functions will have its own set of arguments. For examples of
this see the section `Built-in table readers/writers`_. This section also
provides the full list of choices for the ``format`` argument.

To get help on the available arguments for each format, use the ``help()``
method of the `~astropy.table.Table.read` or `~astropy.table.Table.write`
methods. Each of these calls prints a long help document which is divided
into two sections, the generic read/write documentation (common to any
call) and the format-specific documentation. For ASCII tables, the
format-specific documentation includes the generic `astropy.io.ascii` package
interface and then a description of the particular ASCII sub-format.

In the examples below we do not show the long output:

.. doctest-skip::

    >>> Table.read.help('fits')
    >>> Table.read.help('ascii')
    >>> Table.read.help('ascii.latex')
    >>> Table.write.help('hdf5')
    >>> Table.write.help('csv')

Command-Line Utility
--------------------

For convenience, the command-line tool ``showtable`` can be used to print the
content of tables for the formats supported by the unified I/O interface.

Example
^^^^^^^

..
  EXAMPLE START
  Viewing the Contents of a Table on the Command Line

To view the contents of a table on the command line::

    $ showtable astropy/io/fits/tests/data/table.fits

     target V_mag
    ------- -----
    NGC1001  11.1
    NGC1002  12.3
    NGC1003  15.2

To get full documentation on the usage and available options, do ``showtable
--help``.

..
  EXAMPLE END

.. _built_in_readers_writers:

Built-In Table Readers/Writers
==============================

The :class:`~astropy.table.Table` class has built-in support for various input
and output formats including :ref:`table_io_ascii`,
-:ref:`table_io_fits`, :ref:`table_io_hdf5`, :ref:`table_io_pandas`,
:ref:`table_io_parquet`, and :ref:`table_io_votable`.

A full list of the supported formats and corresponding classes is shown in the
table below. The ``Write`` column indicates those formats that support write
functionality, and the ``Suffix`` column indicates the filename suffix
indicating a particular format. If the value of ``Suffix`` is ``auto``, the
format is auto-detected from the file itself. Not all formats support auto-
detection.

===========================  =====  ======  ============================================================================================
           Format            Write  Suffix                                          Description
===========================  =====  ======  ============================================================================================
                      ascii    Yes          ASCII table in any supported format (uses guessing)
               ascii.aastex    Yes          :class:`~astropy.io.ascii.AASTex`: AASTeX deluxetable used for AAS journals
                ascii.basic    Yes          :class:`~astropy.io.ascii.Basic`: Basic table with custom delimiters
                  ascii.cds     No          :class:`~astropy.io.ascii.Cds`: CDS format table
     ascii.commented_header    Yes          :class:`~astropy.io.ascii.CommentedHeader`: Column names in a commented line
                  ascii.csv    Yes    .csv  :class:`~astropy.io.ascii.Csv`: Basic table with comma-separated values
              ascii.daophot     No          :class:`~astropy.io.ascii.Daophot`: IRAF DAOphot format table
                 ascii.ecsv    Yes   .ecsv  :class:`~astropy.io.ascii.Ecsv`: Basic table with Enhanced CSV (supporting metadata)
          ascii.fixed_width    Yes          :class:`~astropy.io.ascii.FixedWidth`: Fixed width
ascii.fixed_width_no_header    Yes          :class:`~astropy.io.ascii.FixedWidthNoHeader`: Fixed width with no header
 ascii.fixed_width_two_line    Yes          :class:`~astropy.io.ascii.FixedWidthTwoLine`: Fixed width with second header line
                 ascii.html    Yes   .html  :class:`~astropy.io.ascii.HTML`: HTML table
                 ascii.ipac    Yes          :class:`~astropy.io.ascii.Ipac`: IPAC format table
                ascii.latex    Yes    .tex  :class:`~astropy.io.ascii.Latex`: LaTeX table
                  ascii.mrt    Yes          :class:`~astropy.io.ascii.Mrt`: AAS Machine-Readable Table format
            ascii.no_header    Yes          :class:`~astropy.io.ascii.NoHeader`: Basic table with no headers
                  ascii.qdp    Yes    .qdp   :class:`~astropy.io.ascii.QDP`: Quick and Dandy Plotter files
                  ascii.rdb    Yes    .rdb  :class:`~astropy.io.ascii.Rdb`: Tab-separated with a type definition header line
                  ascii.rst    Yes    .rst  :class:`~astropy.io.ascii.RST`: reStructuredText simple format table
           ascii.sextractor     No          :class:`~astropy.io.ascii.SExtractor`: SExtractor format table
                  ascii.tab    Yes          :class:`~astropy.io.ascii.Tab`: Basic table with tab-separated values
                       fits    Yes    auto  :mod:`~astropy.io.fits`: Flexible Image Transport System file
                       hdf5    Yes    auto  HDF5_: Hierarchical Data Format binary file
                    parquet    Yes    auto  Parquet_: Apache Parquet binary file
                 pandas.csv    Yes          Wrapper around ``pandas.read_csv()`` and ``pandas.to_csv()``
                 pandas.fwf     No          Wrapper around ``pandas.read_fwf()`` (fixed width format)
                pandas.html    Yes          Wrapper around ``pandas.read_html()`` and ``pandas.to_html()``
                pandas.json    Yes          Wrapper around ``pandas.read_json()`` and ``pandas.to_json()``
                    votable    Yes    auto  :mod:`~astropy.io.votable`: Table format used by Virtual Observatory (VO) initiative
===========================  =====  ======  ============================================================================================

.. _table_io_ascii:

ASCII Formats
-------------

The :meth:`~astropy.table.Table.read` and
:meth:`~astropy.table.Table.write` methods can be used to read and write formats
supported by `astropy.io.ascii`.

Use ``format='ascii'`` in order to interface to the generic
:func:`~astropy.io.ascii.read` and :func:`~astropy.io.ascii.write`
functions from `astropy.io.ascii`. When reading a table, this means
that all supported ASCII table formats will be tried in order to successfully
parse the input.

Examples
^^^^^^^^

..
  EXAMPLE START
  Reading and Writing ASCII Formats

To read and write formats supported by `astropy.io.ascii`:

.. doctest-skip::

  >>> t = Table.read('astropy/io/ascii/tests/t/latex1.tex', format='ascii')
  >>> print(t)
  cola colb colc
  ---- ---- ----
     a    1    2
     b    3    4

When writing a table with ``format='ascii'`` the output is a basic
character-delimited file with a single header line containing the
column names.

All additional arguments are passed to the `astropy.io.ascii`
:func:`~astropy.io.ascii.read` and :func:`~astropy.io.ascii.write`
functions. Further details are available in the sections on
:ref:`io_ascii_read_parameters` and :ref:`io_ascii_write_parameters`. For
example, to change the column delimiter and the output format for the ``colc``
column use:

.. doctest-skip::

  >>> t.write(sys.stdout, format='ascii', delimiter='|', formats={'colc': '%0.2f'})
  cola|colb|colc
  a|1|2.00
  b|3|4.00


.. note::

   When specifying an ASCII table format using the unified interface, the
   format name is prefixed with ``ascii`` in order to identify the format as
   ASCII-based. Compare the table above to the `astropy.io.ascii` list of
   :ref:`supported formats <supported_formats>` where the prefix is not
   needed. Therefore the following are equivalent:

.. doctest-skip::

     >>> dat = ascii.read('file.dat', format='daophot')
     >>> dat = Table.read('file.dat', format='ascii.daophot')

.. attention:: **ECSV is recommended**

   For writing and reading tables to ASCII in a way that fully reproduces the
   table data, types, and metadata (i.e., the table will "round-trip"), we
   highly recommend using the :ref:`ecsv_format`. This writes the actual data
   in a space-delimited format (the ``basic`` format) that any ASCII table
   reader can parse, but also includes metadata encoded in a comment block that
   allows full reconstruction of the original columns. This includes support
   for :ref:`ecsv_format_mixin_columns` (such as
   `~astropy.coordinates.SkyCoord` or `~astropy.time.Time`) and
   :ref:`ecsv_format_masked_columns`.

..
  EXAMPLE END

.. _table_io_fits:

FITS
----

Reading and writing tables in `FITS <https://fits.gsfc.nasa.gov/>`_ format is
supported with ``format='fits'``. In most cases, existing FITS files should be
automatically identified as such based on the header of the file, but if not,
or if writing to disk, then the format should be explicitly specified.

Reading
^^^^^^^

If a FITS table file contains only a single table, then it can be read in
with:

.. doctest-skip::

    >>> from astropy.table import Table
    >>> t = Table.read('data.fits')

If more than one table is present in the file, you can select the HDU
as follows::

    >>> t = Table.read('data.fits', hdu=3)  # doctest: +SKIP

In this case if the ``hdu`` argument is omitted, then the first table found
will be read in and a warning will be emitted::

    >>> t = Table.read('data.fits')  # doctest: +SKIP
    WARNING: hdu= was not specified but multiple tables are present, reading in first available table (hdu=1) [astropy.io.fits.connect]

You can also read a table from the HDUs of an in-memory FITS file. This will
round-trip any :ref:`mixin_columns` that were written to that HDU, using the
header information to reconstruct them::

    >>> hdulist = astropy.io.fits.open('data.fits') # doctest: +SKIP
    >>> t = Table.read(hdulist[1])  # doctest: +SKIP

Writing
^^^^^^^

To write a table ``t`` to a new file::

    >>> t.write('new_table.fits')  # doctest: +SKIP

If the file already exists and you want to overwrite it, then set the
``overwrite`` keyword::

    >>> t.write('existing_table.fits', overwrite=True)  # doctest: +SKIP

If you want to append a table to an existing file, set the ``append``
keyword::

    >>> t.write('existing_table.fits', append=True)  # doctest: +SKIP

Alternatively, you can use the convenience function
:func:`~astropy.io.fits.table_to_hdu` to create a single
binary table HDU and insert or append that to an existing
:class:`~astropy.io.fits.HDUList`.

There is support for writing a table which contains :ref:`mixin_columns` such
as `~astropy.time.Time` or `~astropy.coordinates.SkyCoord`. This uses FITS
``COMMENT`` cards to capture additional information needed order to fully
reconstruct the mixin columns when reading back from FITS. The information is a
Python `dict` structure which is serialized using YAML.

Keywords
^^^^^^^^

The FITS keywords associated with an HDU table are represented in the ``meta``
ordered dictionary attribute of a :ref:`Table <astropy-table>`. After reading
a table you can view the available keywords in a readable format using:

.. doctest-skip::

  >>> for key, value in t.meta.items():
  ...     print(f'{key} = {value}')

This does not include the "internal" FITS keywords that are required to specify
the FITS table properties (e.g., ``NAXIS``, ``TTYPE1``). ``HISTORY`` and
``COMMENT`` keywords are treated specially and are returned as a list of
values.

Conversely, the following shows examples of setting user keyword values for a
table ``t``:

.. doctest-skip::

  >>> t.meta['MY_KEYWD'] = 'my value'
  >>> t.meta['COMMENT'] = ['First comment', 'Second comment', 'etc']
  >>> t.write('my_table.fits', overwrite=True)

The keyword names (e.g., ``MY_KEYWD``) will be automatically capitalized prior
to writing.

At this time, the ``meta`` attribute of the :class:`~astropy.table.Table` class
is an ordered dictionary and does not fully represent the structure of a
FITS header (for example, keyword comments are dropped).

.. _fits_astropy_native:


TDISPn Keyword
^^^^^^^^^^^^^^

TDISPn FITS keywords will map to and from the `~astropy.table.Column` ``format``
attribute if the display format is convertible to and from a Python display
format. Below are the rules used for both conversion directions.

TDISPn to Python format string
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

TDISPn format characters are defined in the table below.

============   ================================================================
   Format                              Description
============   ================================================================
Aw             Character
Lw             Logical
Iw.m           Integer
Bw.m           Binary, integers only
Ow.m           Octal, integers only
Zw.m           Hexadecimal, integers only
Fw.d           Floating-point, fixed decimal notation
Ew.dEe         Floating-point, exponential notation
ENw.d          Engineering; E format with exponent multiple of three
ESw.d          Scientific; same as EN but non-zero leading digit if not zero
Gw.dEe         General; appears as F if significance not lost, also E
Dw.dEe         Floating-point, exponential notation, double precision
============   ================================================================

Where w is the width in characters of displayed values, m is the minimum number
of digits displayed, d is the number of digits to the right of decimal, and e
is the number of digits in the exponent. The .m and Ee fields are optional.

The A (character), L (logical), F (floating point), and G (general) display
formats can be directly translated to Python format strings. The other formats
need to be modified to match Python display formats.

For the integer formats (I, B, O, and Z), the width (w) value is used to add
space padding to the left of the column value. The minimum number (m) value is
not used. For the E, G, D, EN, and ES formats (floating point exponential) the
width (w) and precision (d) are both used, but the exponential (e) is not used.

Python format string to TDISPn
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The conversion from Python format strings back to TDISPn is slightly more
complicated.

Python strings map to the TDISP format A if the Python formatting string does
not contain right space padding. It will accept left space padding. The same
applies to the logical format L.

The integer formats (decimal integer, binary, octal, hexadecimal) map to the
I, B, O, and Z TDISP formats respectively. Integer formats do not accept a
zero padded format string or a format string with no left padding defined (a
width is required in the TDISP format standard for the Integer formats).

For all float and exponential values, zero padding is not accepted. There
must be at least a width or precision defined. If only a width is defined,
there is no precision set for the TDISPn format. If only a precision is
defined, the width is set to the precision plus an extra padding value
depending on format type, and both are set in the TDISPn format. Otherwise,
if both a width and precision are present they are both set in the TDISPn
format. A Python ``f`` or ``F`` map to TDISP F format. The Python ``g`` or
``G`` map to TDISP G format. The Python ``e`` and ``E`` map to TDISP E format.

Masked Columns
^^^^^^^^^^^^^^

Tables that contain `~astropy.table.MaskedColumn` columns can be written to
FITS. By default this will replace the masked data elements with certain
sentinel values according to the FITS standard:

- ``NaN`` for float columns.
- Value of ``TNULLn`` for integer columns, as defined by the column
  ``fill_value`` attribute.
- Null string for string columns (not currently implemented).

When the file is read back those elements are marked as masked in the returned
table, but see `issue #4708 <https://github.com/astropy/astropy/issues/4708>`_
for problems in all three cases.

The FITS standard has a few limitations:

- Not all data types are supported (e.g., logical / boolean).
- Integer columns require picking one value as the NULL indicator. If
  all possible values are represented in valid data (e.g., an unsigned
  int columns with all 256 possible values in valid data), then there
  is no way to represent missing data.
- The masked data values are permanently lost, precluding the possibility
  of later unmasking the values.

``astropy`` provides a work-around for this limitation that users can choose to
use. The key part is to use the ``serialize_method='data_mask'`` keyword
argument when writing the table. This tells the FITS writer to split each masked
column into two separate columns, one for the data and one for the mask.
When it gets read back that process is reversed and the two columns are
merged back into one masked column.

.. doctest-skip::

  >>> from astropy.table.table_helpers import simple_table
  >>> t = simple_table(masked=True)
  >>> t['d'] = [False, False, True]
  >>> t['d'].mask = [True, False, False]
  >>> t
  <Table masked=True length=3>
    a      b     c     d
  int64 float64 str1  bool
  ----- ------- ---- -----
     --     1.0    c    --
      2     2.0   -- False
      3      --    e  True

.. doctest-skip::

  >>> t.write('data.fits', serialize_method='data_mask', overwrite=True)
  >>> Table.read('data.fits')
  <Table masked=True length=3>
    a      b      c      d
  int64 float64 bytes1  bool
  ----- ------- ------ -----
     --     1.0      c    --
      2     2.0     -- False
      3      --      e  True

.. warning:: This option goes outside of the established FITS standard for
   representing missing data, so users should be careful about choosing this
   option, especially if other (non-``astropy``) users will be reading the
   file(s). Behind the scenes, ``astropy`` is converting the masked columns
   into two distinct data and mask columns, then writing metadata into
   ``COMMENT`` cards to allow reconstruction of the original data.

``astropy`` Native Objects (Mixin Columns)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

It is possible to store not only standard `~astropy.table.Column` objects to a
FITS table HDU, but also any ``astropy`` native objects
(:ref:`mixin_columns`) within a `~astropy.table.Table` or
`~astropy.table.QTable`. This includes `~astropy.time.Time`,
`~astropy.units.Quantity`, `~astropy.coordinates.SkyCoord`, and many others.

In general, a mixin column may contain multiple data components as well as
object attributes beyond the standard Column attributes like ``format`` or
``description``. Abiding by the rules set by the FITS standard requires the
mapping of these data components and object attributes to the appropriate FITS
table columns and keywords. Thus, a well defined protocol has been developed
to allow the storage of these mixin columns in FITS while allowing the object to
"round-trip" through the file with no loss of data or attributes.

Quantity
~~~~~~~~

A `~astropy.units.Quantity` mixin column in a `~astropy.table.QTable` is
represented in a FITS table using the ``TUNITn`` FITS column keyword to
incorporate the unit attribute of Quantity. For example:

.. doctest-skip::

    >>> from astropy.table import QTable
    >>> import astropy.units as u
    >>> t = QTable([[1, 2] * u.angstrom)])
    >>> t.write('my_table.fits', overwrite=True)
    >>> qt = QTable.read('my_table.fits')
    >>> qt
    <QTable length=2>
      col0
    Angstrom
    float64
    --------
         1.0
         2.0

Time
~~~~

``astropy`` provides the following features for reading and writing ``Time``:

- Writing and reading `~astropy.time.Time` Table columns to and from FITS
  tables.
- Reading time coordinate columns in FITS tables (compliant with the time
  standard) as `~astropy.time.Time` Table columns.

Writing and reading ``astropy`` Time columns
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

By default, a `~astropy.time.Time` mixin column within a `~astropy.table.Table`
or `~astropy.table.QTable` will be written to FITS in full precision. This will
be done using the FITS time standard by setting the necessary FITS header
keywords.

The default behavior for reading a FITS table into a `~astropy.table.Table`
has historically been to convert all FITS columns to `~astropy.table.Column`
objects, which have closely matching properties. For some columns, however,
closer native ``astropy`` representations are possible, and you can indicate
these should be used by passing ``astropy_native=True`` (for backwards
compatibility, this is not done by default). This will convert columns
conforming to the FITS time standard to `~astropy.time.Time` instances,
avoiding any loss of precision.

Example
~~~~~~~

..
  EXAMPLE START
  Writing and Reading Time Columns to/from FITS Tables

To read a FITS table into `~astropy.table.Table`:

.. doctest-skip::

    >>> from astropy.time import Time
    >>> from astropy.table import Table
    >>> from astropy.coordinates import EarthLocation
    >>> t = Table()
    >>> t['a'] = Time([100.0, 200.0], scale='tt', format='mjd',
    ...               location=EarthLocation(-2446354, 4237210, 4077985, unit='m'))
    >>> t.write('my_table.fits', overwrite=True)
    >>> tm = Table.read('my_table.fits', astropy_native=True)
    >>> tm['a']
    <Time object: scale='tt' format='jd' value=[ 2400100.5  2400200.5]>
    >>> tm['a'].location
    <EarthLocation (-2446354.,  4237210.,  4077985.) m>
    >>> all(tm['a'] == t['a'])
    True

The same will work with ``QTable``.

..
  EXAMPLE END

In addition to binary table columns, various global time informational FITS
keywords are treated specially with ``astropy_native=True``. In particular,
the keywords ``DATE``, ``DATE-*`` (ISO 8601 datetime strings), and the ``MJD-*``
(MJD date values) will be returned as ``Time`` objects in the Table ``meta``.
For more details regarding the FITS time paper and the implementation,
refer to :ref:`fits_time_column`.

Since not all FITS readers are able to use the FITS time standard, it is also
possible to store `~astropy.time.Time` instances using the `_time_format`.
For this case, none of the special header keywords associated with the
FITS time standard will be set. When reading this back into ``astropy``, the
column will be an ordinary Column instead of a `~astropy.time.Time` object.
See the `Details`_ section below for an example.

Reading FITS standard compliant time coordinate columns in binary tables
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Reading FITS files which are compliant with the FITS time standard is supported
by ``astropy`` by following the multifarious rules and conventions set by the
standard. The standard was devised in order to describe time coordinates in
an unambiguous and comprehensive manner and also to provide flexibility for its
multiple use cases. Thus, while reading time coordinate columns in FITS-
compliant files, multiple aspects of the standard are taken into consideration.

Time coordinate columns strictly compliant with the two-vector JD subset of the
standard (described in the `Details`_ section below) can be read as native
`~astropy.time.Time` objects. The other subsets of the standard are also
supported by ``astropy``; a thorough examination of the FITS standard time-
related keywords is done and the time data is interpreted accordingly.

The standard describes the various components in the specification of time:

- Time coordinate frame
- Time unit
- Corrections, errors, etc.
- Durations

The keywords used to specify times define these components. Using these
keywords, time coordinate columns are identified and read as
`~astropy.time.Time` objects. Refer to :ref:`fits_time_column` for the
specification of these keywords and their description.

There are two aspects of the standard that require special attention due to the
subtleties involved while handling them. These are:

* Column named TIME with time unit

A common convention found in existing FITS files is that a FITS binary
table column with ``TTYPEn = TIME`` represents a time coordinate column.
Many astronomical data files, including official data products from major
observatories, follow this convention that predates the FITS standard.
The FITS time standard states that such a column will be controlled by
the global time reference frame keywords, and this will still be compliant
with the present standard.

Using this convention which has been incorporated into the standard, ``astropy``
can read time coordinate columns from all such FITS tables as native
`~astropy.time.Time` objects. Common examples of FITS files following
this convention are Chandra, XMM, and HST files.

Examples
~~~~~~~~

..
  EXAMPLE START
  Reading FITS Standard Compliant Time Coordinate Columns in Binary Tables

The following is an example of a Header extract of a Chandra event list:

.. parsed-literal::

    COMMENT      ---------- Globally valid key words ----------------
    DATE    = '2016-01-27T12:34:24' / Date and time of file creation
    TIMESYS = 'TT      '           / Time system
    MJDREF  =  5.0814000000000E+04 / [d] MJD zero point for times
    TIMEUNIT= 's       '           / Time unit
    TIMEREF = 'LOCAL   '           / Time reference (barycenter/local)

    COMMENT      ---------- Time Column -----------------------
    TTYPE1  = 'time    '           / S/C TT corresponding to mid-exposure
    TFORM1  = '1D      '           / format of field
    TUNIT1  = 's       '

When reading such a FITS table with ``astropy_native=True``, ``astropy`` checks
whether the name of a column is "TIME"/ "time" (``TTYPEn = TIME``) and
whether its unit is a FITS recognized time unit (``TUNITn`` is a time unit).

For example, reading a Chandra event list which has the above mentioned header
and the time coordinate column ``time`` as ``[1, 2]`` will give::

    >>> from astropy.table import Table
    >>> from astropy.time import Time, TimeDelta
    >>> from astropy.utils.data import get_pkg_data_filename
    >>> chandra_events = get_pkg_data_filename('data/chandra_time.fits',
    ...                                        package='astropy.io.fits.tests')
    >>> native = Table.read(chandra_events, astropy_native=True)  # doctest: +IGNORE_WARNINGS
    >>> native['time']  # doctest: +FLOAT_CMP
    <Time object: scale='tt' format='mjd' value=[57413.76033393 57413.76033393]>
    >>> non_native = Table.read(chandra_events)
    >>> # MJDREF  =  5.0814000000000E+04, TIMESYS = 'TT'
    >>> ref_time = Time(non_native.meta['MJDREF'], format='mjd',
    ...                 scale=non_native.meta['TIMESYS'].lower())
    >>> # TTYPE1  = 'time', TUNIT1 = 's'
    >>> delta_time = TimeDelta(non_native['time'])
    >>> all(ref_time + delta_time == native['time'])
    True

By default, FITS table columns will be read as standard `~astropy.table.Column`
objects without taking the FITS time standard into consideration.

..
  EXAMPLE END

* String time column in ISO 8601 Datetime format

FITS uses a subset of ISO 8601 (which in itself does not imply a particular
timescale) for several time-related keywords, such as DATE-xxx. Following the
FITS standard, its values must be written as a character string in the
following ``datetime`` format:

.. parsed-literal::

    [+/-C]CCYY-MM-DD[Thh:mm:ss[.s...]]

A time coordinate column can be constructed using this representation of time.
The following is an example of an ISO 8601 ``datetime`` format time column:

.. parsed-literal::

    TIME
    ----
    1999-01-01T00:00:00
    1999-01-01T00:00:40
    1999-01-01T00:01:06
    .
    .
    .
    1999-01-20T01:10:00

The criteria for identifying a time coordinate column in ISO 8601 format is as
follows:

A time column is identified using the time coordinate frame keywords as
described in :ref:`fits_time_column`. Once it has been identified, its datatype
is checked in order to determine its representation format. Since ISO 8601
``datetime`` format is the only string representation of time, a time
coordinate column having string datatype will be automatically read as a
`~astropy.time.Time` object with ``format='fits'`` ('fits' represents the FITS
ISO 8601 format).

As this format does not imply a particular timescale, it is determined using
the timescale keywords in the header (``TCTYP`` or ``TIMESYS``) or their
defaults. The other time coordinate information is also determined in the same
way, using the time coordinate frame keywords. All ISO 8601 times are relative
to a globally accepted zero point (year 0 corresponds to 1 BCE) and are thus
not relative to the reference time keywords (MJDREF, JDREF, or DATEREF).
Hence, these keywords will be ignored while dealing with ISO 8601 time columns.

.. note::

   Reading FITS files with time coordinate columns *may* fail. ``astropy``
   supports a large subset of these files, but there are still some FITS files
   which are not compliant with any aspect of the standard. If you have such a
   file, please do not hesitate to let us know (by opening an issue in the
   `issue tracker <https://github.com/astropy/astropy/issues>`_).

   Also, reading a column having ``TTYPEn = TIME`` as `~astropy.time.Time`
   will fail if ``TUNITn`` for the column is not a FITS-recognized time unit.

Details
~~~~~~~

Time as a dimension in astronomical data presents challenges in its
representation in FITS files. The standard has therefore been extended to
describe rigorously the time coordinate in the ``World Coordinate System``
framework. Refer to `FITS WCS paper IV
<https://ui.adsabs.harvard.edu/abs/2015A%26A...574A..36R/>`_ for details.

Allowing ``Time`` columns to be written as time coordinate
columns in FITS tables thus involves storing time values in a way that
ensures retention of precision and mapping the associated metadata to the
relevant FITS keywords.

In accordance with the standard, which states that in binary tables one may use
pairs of doubles, the ``astropy`` Time column is written in such a table as a
vector of two doubles ``(TFORMn = 2D) (jd1, jd2)`` where ``JD = jd1 + jd2``.
This reproduces the time values to double-double precision and is the
"lossless" version, exploiting the higher precision provided in binary tables.
Note that ``jd1`` is always a half-integer or integer, while ``abs(jd2) < 1``.
"Round-tripping" of ``astropy``-written FITS binary tables containing time
coordinate columns has been partially achieved by mapping selected metadata,
``scale`` and singular ``location`` of `~astropy.time.Time`, to corresponding
keywords. Note that the arbitrary metadata allowed in `~astropy.table.Table`
objects within the ``meta`` dict is not written and will be lost.

Examples
~~~~~~~~

..
  EXAMPLE START
  Time Columns in FITS Files

Consider the following Time column:

    >>> t['a'] = Time([100.0, 200.0], scale='tt', format='mjd')  # doctest: +SKIP

The FITS standard requires an additional translation layer back into
the desired format. The Time column ``t['a']`` will undergo the translation
``Astropy Time --> FITS --> Astropy Time`` which corresponds to the format
conversion ``mjd --> (jd1, jd2) --> jd``. Thus, the final conversion from
``(jd1, jd2)`` will require a software implementation which is fully compliant
with the FITS time standard.

Taking this into consideration, the functionality to read/write Time
from/to FITS can be explicitly turned off, by opting to store the time
representation values in the format specified by the ``format`` attribute
of the `~astropy.time.Time` column, instead of the ``(jd1, jd2)`` format, with
no extra metadata in the header. This is the "lossy" version, but can help
with portability. For the above example, the FITS column corresponding
to ``t['a']`` will then store ``[100.0 200.0]`` instead of
``[[ 2400100.5, 0. ], [ 2400200.5, 0. ]]``. This is done by setting the
`Table serialization methods`_ for Time columns when writing, as in the
following example:

.. doctest-skip::

    >>> from astropy.time import Time
    >>> from astropy.table import Table
    >>> from astropy.coordinates import EarthLocation
    >>> t = Table()
    >>> t['a'] = Time([100.0, 200.0], scale='tt', format='mjd')
    >>> t.write('my_table.fits', overwrite=True,
    ...         serialize_method={Time: 'formatted_value'})
    >>> tm = Table.read('my_table.fits')
    >>> tm['a']
    <Column name='a' dtype='float64' length=2>
    100.0
    200.0
    >>> all(tm['a'] == t['a'].value)
    True

By default, ``serialize_method`` for Time columns is equal to
``'jd1_jd2'``, that is, Time columns will be written in full precision.

.. note::

   The ``astropy`` `~astropy.time.Time` object does not precisely map to the
   FITS time standard.

   * FORMAT

     The FITS format considers only three formats: ISO 8601, JD, and MJD.
     ``astropy`` Time allows for many other formats like ``unix`` or ``cxcsec``
     for representing the values.

     Hence, the ``format`` attribute of Time is not stored. After reading from
     FITS the user must set the ``format`` as desired.

   * LOCATION

     In the FITS standard, the reference position for a time coordinate is a
     scalar expressed via keywords. However, vectorized reference position or
     location can be supported by the `Green Bank Keyword Convention
     <https://fits.gsfc.nasa.gov/registry/greenbank.html>`_ which is a
     Registered FITS Convention. In ``astropy`` Time, location can be an array
     which is broadcastable to the Time values.

     Hence, vectorized ``location`` attribute of Time is stored and read
     following this convention.

..
  EXAMPLE END

.. doctest-skip-all

.. _table_io_hdf5:

HDF5
----

.. _HDF5: https://www.hdfgroup.org/HDF5/
.. _h5py: http://www.h5py.org/

Reading/writing from/to HDF5_ files is supported with ``format='hdf5'`` (this
requires h5py_ to be installed). However, the ``.hdf5`` file extension is
automatically recognized when writing files, and HDF5 files are automatically
identified (even with a different extension) when reading in (using the first
few bytes of the file to identify the format), so in most cases you will not
need to explicitly specify ``format='hdf5'``.

Since HDF5 files can contain multiple tables, the full path to the table
should be specified via the ``path=`` argument when reading and writing.

Examples
^^^^^^^^

..
  EXAMPLE START
  Reading from and Writing to HDF5 Files

To read a table called ``data`` from an HDF5 file named ``observations.hdf5``,
you can do::

    >>> t = Table.read('observations.hdf5', path='data')

To read a table nested in a group in the HDF5 file, you can do::

    >>> t = Table.read('observations.hdf5', path='group/data')

To write a table to a new file, the path should also be specified::

    >>> t.write('new_file.hdf5', path='updated_data')

It is also possible to write a table to an existing file using ``append=True``::

    >>> t.write('observations.hdf5', path='updated_data', append=True)

As with other formats, the ``overwrite=True`` argument is supported for
overwriting existing files. To overwrite only a single table within an HDF5
file that has multiple datasets, use *both* the ``overwrite=True`` and
``append=True`` arguments.

Finally, when writing to HDF5 files, the ``compression=`` argument can be
used to ensure that the data is compressed on disk::

    >>> t.write('new_file.hdf5', path='updated_data', compression=True)

..
  EXAMPLE END

.. doctest-skip-all

.. _table_io_parquet:

Parquet
-------

.. _Parquet: https://parquet.apache.org/
.. _pyarrow: https://arrow.apache.org/docs/python/

Reading and writing Parquet_ files is supported with ``format='parquet'``
if the pyarrow_ package is installed. For writing, the file extensions ``.parquet`` or
``.parq`` will automatically imply the ``'parquet'`` format. For reading,
Parquet files are automatically identified regardless of the extension
if the first four bytes of the file are ``b'PAR1'``.
In many cases you do not need to explicitly specify ``format='parquet'``,
but it may be a good idea anyway if there is any ambiguity about the
file format.

Multiple-file Parquet datasets are not supported for reading and writing.

Examples
^^^^^^^^

..
  EXAMPLE START
  Reading from and Writing to Parquet Files

To read a table from a Parquet file named ``observations.parquet``, you can do::

    >>> t = Table.read('observations.parquet')

To write a table to a new file, simply do::

    >>> t.write('new_file.parquet')

As with other formats, the ``overwrite=True`` argument is supported for
overwriting existing files.

One big advantage of the Parquet files is that each column is stored independently,
and thus reading a subset of columns is fast and efficient.  To find out which
columns are stored in a table without reading the data, use the ``schema_only=True``
as shown below. This returns a zero-length table with the appropriate columns::

    >>> schema = Table.read('observations.parquet', schema_only=True)

To read only a subset of the columns, use the ``include_names`` and/or ``exclude_names`` keywords::

    >>> t_sub = Table.read('observations.parquet', include_names=['mjd', 'airmass'])

..
  EXAMPLE END

Metadata and Mixin Columns
^^^^^^^^^^^^^^^^^^^^^^^^^^

``astropy`` tables can contain metadata, both in the table ``meta`` attribute
(which is an ordered dictionary of arbitrary key/value pairs), and within the
columns, which each have attributes ``unit``, ``format``, ``description``,
and ``meta``.

By default, when writing a table to HDF5 the code will attempt to store each
key/value pair within the table ``meta`` as HDF5 attributes of the table
dataset. This will fail if the values within ``meta`` are not objects that can
be stored as HDF5 attributes. In addition, if the table columns being stored
have defined values for any of the above-listed column attributes, these
metadata will *not* be stored and a warning will be issued.

serialize_meta
~~~~~~~~~~~~~~

To enable storing all table and column metadata to the HDF5 file, call
the ``write()`` method with ``serialize_meta=True``. This will store metadata
in a separate HDF5 dataset, contained in the same file, which is named
``<path>.__table_column_meta__``. Here ``path`` is the argument provided in
the call to ``write()``::

    >>> t.write('observations.hdf5', path='data', serialize_meta=True)

The table metadata are stored as a dataset of strings by serializing the
metadata in YAML following the `ECSV header format
<https://github.com/astropy/astropy-APEs/blob/main/APE6.rst#header-details>`_
definition. Since there are YAML parsers for most common languages, one can
easily access and use the table metadata if reading the HDF5 in a non-astropy
application.

As of ``astropy`` 3.0, by specifying ``serialize_meta=True`` one can also store
to HDF5 tables that contain :ref:`mixin_columns` such as `~astropy.time.Time` or
`~astropy.coordinates.SkyCoord` columns.

.. _table_io_pandas:

Pandas
------

.. _pandas: https://pandas.pydata.org/pandas-docs/stable/index.html

``astropy`` `~astropy.table.Table` supports the ability to read or write tables
using some of the `I/O methods <https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html>`_
available within pandas_. This interface thus provides convenient wrappers to
the following functions / methods:

.. csv-table::
    :header: "Format name", "Data Description", "Reader", "Writer"
    :widths: 25, 25, 25, 25
    :delim: ;

    ``pandas.csv``;`CSV <https://en.wikipedia.org/wiki/Comma-separated_values>`__;`read_csv() <https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#io-read-csv-table>`_;`to_csv() <https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#io-store-in-csv>`_
    ``pandas.json``;`JSON <http://www.json.org/>`__;`read_json() <https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#io-json-reader>`_;`to_json() <https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#io-json-writer>`_
    ``pandas.html``;`HTML <https://en.wikipedia.org/wiki/HTML>`__;`read_html() <https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#io-read-html>`_;`to_html() <https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#io-html>`_
    ``pandas.fwf``;Fixed Width;`read_fwf() <https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_fwf.html#pandas.read_fwf>`_;

**Notes**:

- There is no fixed-width writer in pandas_.
- Reading HTML requires `BeautifulSoup4 <https://pypi.org/project/beautifulsoup4/>`_ and
  `html5lib <https://pypi.org/project/html5lib/>`_ to be installed.

When reading or writing a table, any keyword arguments apart from the
``format`` and file name are passed through to pandas, for instance:

.. doctest-skip::

  >>> t.write('data.csv', format='pandas.csv', sep=' ', header=False)
  >>> t2 = Table.read('data.csv', format='pandas.csv', sep=' ', names=['a', 'b', 'c'])

.. _table_io_jsviewer:

JSViewer
--------

Provides an interactive HTML export of a Table, like the
:class:`~astropy.io.ascii.HTML` writer but using the DataTables_ library, which
allow to visualize interactively an HTML table (with columns sorting, search,
and pagination).

Example
^^^^^^^

..
  EXAMPLE START
  JSViewer to Provide an Interactive HTML Export of a Table

To write a table ``t`` to a new file::

    >>> t.write('new_table.html', format='jsviewer')

Several additional parameters can be used:

- *table_id*: the HTML ID of the ``<table>`` tag, defaults to ``'table{id}'``
  where ``id`` is the ID of the Table object.
- *max_lines*: maximum number of lines.
- *table_class*: HTML classes added to the ``<table>`` tag, can be useful to
  customize the style of the table.
- *jskwargs*: additional arguments passed to :class:`~astropy.table.JSViewer`.
- *css*: CSS style, default to ``astropy.table.jsviewer.DEFAULT_CSS``.
- *htmldict*: additional arguments passed to :class:`~astropy.io.ascii.HTML`.

.. _Datatables: https://www.datatables.net/

..
  EXAMPLE END

.. _table_io_votable:

VO Tables
---------

Reading/writing from/to `VO table <http://www.ivoa.net/documents/VOTable/>`_
files is supported with ``format='votable'``. In most cases, existing VO
tables should be automatically identified as such based on the header of the
file, but if not, or if writing to disk, then the format should be explicitly
specified.

Examples
^^^^^^^^

..
  EXAMPLE START
  Reading from and Writing to VO Tables

If a VO table file contains only a single table, then it can be read in with::

    >>> t = Table.read('aj285677t3_votable.xml')

If more than one table is present in the file, an error will be raised,
unless the table ID is specified via the ``table_id=`` argument::

    >>> t = Table.read('catalog.xml')
    Traceback (most recent call last):
    ...
    ValueError: Multiple tables found: table id should be set via the table_id= argument. The available tables are twomass, spitzer

    >>> t = Table.read('catalog.xml', table_id='twomass')

To write to a new file, the ID of the table should also be specified (unless
``t.meta['ID']`` is defined)::

    >>> t.write('new_catalog.xml', table_id='updated_table', format='votable')

When writing, the ``compression=True`` argument can be used to force
compression of the data on disk, and the ``overwrite=True`` argument can be
used to overwrite an existing file.

..
  EXAMPLE END

.. _table_serialization_methods:

Table Serialization Methods
===========================

``astropy`` supports fine-grained control of the way to write out (serialize)
the columns in a Table. For instance, if you are writing an ISO format
Time column to an ECSV ASCII table file, you may want to write this as a pair
of JD1/JD2 floating point values for full resolution (perfect "round-trip"),
or as a formatted ISO date string so that the values are easily readable by
your other applications.

The default method for serialization depends on the format (FITS, ECSV, HDF5).
For instance HDF5 is a binary format and so it would make sense to store a Time
object as JD1/JD2, while ECSV is a flat ASCII format and commonly you
would want to see the date in the same format as the Time object. The defaults
also reflect an attempt to minimize compatibility issues between ``astropy``
versions. For instance, it was possible to write Time columns to ECSV as
formatted strings in a version prior to the ability to write as JD1/JD2
pairs, so the current default for ECSV is to write as formatted strings.

The two classes which have configurable serialization methods are
`~astropy.time.Time` and `~astropy.table.MaskedColumn`. See the sections
on Time `Details`_ and `Masked columns`_, respectively, for additional
information. The defaults for each format are listed below:

====== ==================== ===============
Format    Time                MaskedColumn
====== ==================== ===============
FITS    ``jd1_jd2``          ``null_value``
ECSV    ``formatted_value``  ``null_value``
HDF5    ``jd1_jd2``          ``data_mask``
YAML    ``jd2_jd2``            ---
====== ==================== ===============

Examples
--------

..
  EXAMPLE START
  Table Serialization Methods in astropy.io

Start by making a table with a Time column and masked column:

  >>> import sys
  >>> from astropy.time import Time
  >>> from astropy.table import Table, MaskedColumn

  >>> t = Table(masked=True)
  >>> t['tm'] = Time(['2000-01-01', '2000-01-02'])
  >>> t['mc1'] = MaskedColumn([1.0, 2.0], mask=[True, False])
  >>> t['mc2'] = MaskedColumn([3.0, 4.0], mask=[False, True])
  >>> t
  <Table masked=True length=2>
             tm             mc1     mc2
           object         float64 float64
  ----------------------- ------- -------
  2000-01-01 00:00:00.000      --     3.0
  2000-01-02 00:00:00.000     2.0      --

Now specify that you want all `~astropy.time.Time` columns written as JD1/JD2
and the ``mc1`` column written as a data/mask pair and write to ECSV:

.. doctest-skip::

  >>> serialize_method = {Time: 'jd1_jd2', 'mc1': 'data_mask'}
  >>> t.write(sys.stdout, format='ascii.ecsv', serialize_method=serialize_method)
  # %ECSV 0.9
   ...
  # schema: astropy-2.0
   tm.jd1    tm.jd2  mc1  mc1.mask  mc2
  2451544.0    0.5   1.0   True     3.0
  2451546.0   -0.5   2.0   False     ""

(Spaces added for clarity)

Notice that the ``tm`` column has been replaced by the ``tm.jd1`` and ``tm.jd2``
columns, and likewise a new column ``mc1.mask`` has appeared and it explicitly
contains the mask values. When this table is read back with the ``ascii.ecsv``
reader then the original columns are reconstructed.

The ``serialize_method`` argument can be set in two different ways:

- As a single string like ``data_mask``. This value then applies to every
  column, and is a convenient strategy for a masked table with no Time columns.
- As a `dict`, where the key can be either a single column name or a class (as
  shown in the example above), and the value is the corresponding serialization
  method.

..
  EXAMPLE END
.. _io_registry:

************************************
I/O Registry (`astropy.io.registry`)
************************************

.. note::

   The I/O registry is only meant to be used directly by users who want to
   define their own custom readers/writers. Users who want to find out more
   about what built-in formats are supported by :class:`~astropy.table.Table`
   by default should see :ref:`table_io`.
   Likewise :ref:`cosmology_io` for built-in formats supported by
   :class:`~astropy.cosmology.Cosmology`.
   No built-in formats are currently defined for
   :class:`~astropy.nddata.NDData`, but this will be added in future.

Introduction
============

The I/O registry is a submodule used to define the readers/writers available
for the :class:`~astropy.table.Table`, :class:`~astropy.nddata.NDData`,
and :class:`~astropy.cosmology.Cosmology` classes.


Custom Read/Write Functions
===========================

This section demonstrates how to create a custom reader/writer. A reader is
written as a function that can take any arguments except ``format`` (which is
needed when manually specifying the format  see below) and returns an
instance of the :class:`~astropy.table.Table` or
:class:`~astropy.nddata.NDData` classes (or subclasses).


Examples
--------

..
  EXAMPLE START
  Using astropy.io.registry to Create a Custom Reader/Writer

Here we assume that we are trying to write a reader/writer for the
:class:`~astropy.table.Table` class::

    >>> from astropy.table import Table

    >>> def my_table_reader(filename, some_option=1):
    ...     # Read in the table by any means necessary
    ...     return table  # should be an instance of Table

Such a function can then be registered with the I/O registry::

    from astropy.io import registry
    registry.register_reader('my-table-format', Table, my_table_reader)

where the first argument is the name of the format, the second argument is the
class that the function returns an instance for, and the third argument is the
reader itself.

We can then read in a table with::

    d = Table.read('my_table_file.mtf', format='my-table-format')

In practice, it would be nice to have the ``read`` method automatically
identify that this file is in the ``my-table-format`` format, so we can
construct a function that can recognize these files, which we refer to here as
an *identifier* function.

An identifier function should take a first argument that is a string
which indicates whether the identifier is being called from ``read`` or
``write``, and should then accept an arbitrary number of positional and keyword
arguments via ``*args`` and ``**kwargs``, which are the arguments passed to
the ``read`` method.

In the above case, we can write a function that only looks at
filenames (but in practice, this function could even look at the first few
bytes of the file, for example). The only requirement for the identifier
function is that it return a boolean indicating whether the input matches that
expected for the format. In our example, we want to automatically recognize
files with filenames ending in ``.mtf`` as being in the ``my-table-format``
format::

    import os

    def identify_mtf(origin, *args, **kwargs):
        return (isinstance(args[0], str) and
                os.path.splitext(args[0].lower())[1] == '.mtf')

.. note::

    Identifier functions should be prepared for arbitrary input  in
    particular, the first argument may not be a filename or file object, so it
    should not assume that this is the case.

We then register this identifier function, similarly to the reader function::

    registry.register_identifier('my-table-format', Table, identify_mtf)

Having registered this function, we can then do::

    t = Table.read('catalog.mtf')

If multiple formats match the current input, then an exception is
raised, and similarly if no format matches the current input. In that
case, the format should be explicitly given with the ``format=``
keyword argument.

It is also possible to create custom writers. To go with our custom reader
above, we can write a custom writer::

   def my_table_writer(table, filename, overwrite=False):
       ...  # Write the table out to a file
       return ...  # generally None, but other values are not forbidden.

Writer functions should take a dataset object (either an instance of the
:class:`~astropy.table.Table` or :class:`~astropy.nddata.NDData`
classes or subclasses), and any number of subsequent positional and keyword
arguments  although as for the reader, the ``format`` keyword argument cannot
be used.

We then register the writer::

   registry.register_writer('my-custom-format', Table, my_table_writer)

We can write the table out to a file::

   t.write('catalog_new.mtf', format='my-table-format')

Since we have already registered the identifier function, we can also do::

   t.write('catalog_new.mtf')

..
  EXAMPLE END


Registries, local and default
=============================

.. versionchanged:: 5.0

As of Astropy 5.0 the I/O registry submodule has switched to a class-based
architecture, allowing for the creation of custom registries.
The three supported registry types are read-only --
:class:`~astropy.io.registry.UnifiedInputRegistry` --
write-only -- :class:`~astropy.io.registry.UnifiedOutputRegistry` --
and read/write -- :class:`~astropy.io.registry.UnifiedIORegistry`.

    >>> from astropy.io.registry import UnifiedIORegistry
    >>> example_reg = UnifiedIORegistry()
    >>> print([m for m in dir(example_reg) if not m.startswith("_")])
    ['available_registries', 'delay_doc_updates', 'get_formats', 'get_reader',
     'get_writer', 'identify_format', 'read', 'register_identifier',
     'register_reader', 'register_writer', 'unregister_identifier',
     'unregister_reader', 'unregister_writer', 'write']

For backward compatibility all the methods on this registry have corresponding
module-level functions, which work with the default global read/write registry.
These functions were used in the previous examples. This new registry is empty.

    >>> example_reg.get_formats()
    <Table length=0>
    Data class  Format   Read   Write  Auto-identify
     float64   float64 float64 float64    float64
    ---------- ------- ------- ------- -------------

We can register read / write / identify methods with this registry object:

    >>> example_reg.register_reader('my-table-format', Table, my_table_reader)
    >>> example_reg.get_formats()
    <Table length=1>
    Data class      Format     Read Write Auto-identify
       str5         str15      str3  str2      str2
    ---------- --------------- ---- ----- -------------
         Table my-table-format  Yes    No            No


What is the use of a custom registries?

    1. To make read-only or write-only registries.
    2. To allow for different readers for the same format.
    3. To allow for an object to have different *kinds* of readers and writers.
       E.g. |Cosmology| which supports both file I/O and object conversion.


Reference/API
=============

.. automodapi:: astropy.io.registry
.. _asdf_schemas:

Schemas
=======

Documentation for each of the individual ASDF schemas defined by ``astropy`` can
be found at the links below.

Documentation for the schemas defined in the ASDF Standard can be found `here
<https://asdf-standard.readthedocs.io/en/latest/schemas/index.html>`__.

.. contents::

Coordinates
-----------

The following schemas are associated with ``astropy`` types from the
:ref:`astropy-coordinates` submodule:

coordinates/angle-1.0.0
^^^^^^^^^^^^^^^^^^^^^^^

.. literalinclude:: ../../astropy/io/misc/asdf/data/schemas/astropy.org/astropy/coordinates/angle-1.0.0.yaml
   :language: yaml

coordinates/earthlocation-1.0.0
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. literalinclude:: ../../astropy/io/misc/asdf/data/schemas/astropy.org/astropy/coordinates/earthlocation-1.0.0.yaml
   :language: yaml

coordinates/latitude-1.0.0
^^^^^^^^^^^^^^^^^^^^^^^^^^

.. literalinclude:: ../../astropy/io/misc/asdf/data/schemas/astropy.org/astropy/coordinates/latitude-1.0.0.yaml
   :language: yaml

coordinates/longitude-1.0.0
^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. literalinclude:: ../../astropy/io/misc/asdf/data/schemas/astropy.org/astropy/coordinates/longitude-1.0.0.yaml
   :language: yaml

coordinates/representation-1.0.0
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. literalinclude:: ../../astropy/io/misc/asdf/data/schemas/astropy.org/astropy/coordinates/representation-1.0.0.yaml
   :language: yaml

coordinates/skycoord-1.0.0
^^^^^^^^^^^^^^^^^^^^^^^^^^

.. literalinclude:: ../../astropy/io/misc/asdf/data/schemas/astropy.org/astropy/coordinates/skycoord-1.0.0.yaml
   :language: yaml

FITS
----

The following schemas are associated with ``astropy`` types from the
:ref:`astropy-io-fits` submodule:

fits/fits-1.0.0
^^^^^^^^^^^^^^^

.. literalinclude:: ../../astropy/io/misc/asdf/data/schemas/astropy.org/astropy/fits/fits-1.0.0.yaml
   :language: yaml


Table
-----

The following schemas are associated with ``astropy`` types from the
:ref:`astropy-table` submodule:

table/table-1.0.0
^^^^^^^^^^^^^^^^^

.. literalinclude:: ../../astropy/io/misc/asdf/data/schemas/astropy.org/astropy/table/table-1.0.0.yaml
   :language: yaml


Time
----

The following schemas are associated with ``astropy`` types from the
:ref:`astropy-time` submodule:

time/timedelta-1.0.0
^^^^^^^^^^^^^^^^^^^^

.. literalinclude:: ../../astropy/io/misc/asdf/data/schemas/astropy.org/astropy/time/timedelta-1.0.0.yaml
   :language: yaml


Units
-----

The following schemas are associated with ``astropy`` types from the
:ref:`astropy-units` submodule:

units/equivalency-1.0.0
^^^^^^^^^^^^^^^^^^^^^^^

.. literalinclude:: ../../astropy/io/misc/asdf/data/schemas/astropy.org/astropy/units/equivalency-1.0.0.yaml
   :language: yaml
.. include:: references.txt

`astropy.io.votable.exceptions`
*******************************

.. contents::

.. automodule:: astropy.io.votable.exceptions

Exception Utilities
===================

.. currentmodule:: astropy.io.votable.exceptions

.. autoclass:: Conf
   :members:

.. autofunction:: warn_or_raise

.. autofunction:: vo_raise

.. autofunction:: vo_reraise

.. autofunction:: vo_warn

.. autofunction:: parse_vowarning

.. autoclass:: VOWarning
   :show-inheritance:

.. autoclass:: VOTableChangeWarning
   :show-inheritance:

.. autoclass:: VOTableSpecWarning
   :show-inheritance:

.. autoclass:: UnimplementedWarning
   :show-inheritance:

.. autoclass:: IOWarning
   :show-inheritance:

.. autoclass:: VOTableSpecError
   :show-inheritance:
.. note that if this is changed from the default approach of using an *include*
   (in index.rst) to a separate performance page, the header needs to be changed
   from === to ***, the filename extension needs to be changed from .inc.rst to
   .rst, and a link needs to be added in the subpackage toctree

.. _astropy-io-votable-performance:

.. Performance Tips
.. ================
..
.. Here we provide some tips and tricks for how to optimize performance of code
.. using `astropy.io.votable`.
.. doctest-skip-all

.. include:: references.txt

.. _astropy-io-votable:

*******************************************
VOTable XML Handling (`astropy.io.votable`)
*******************************************

Introduction
============

The `astropy.io.votable` sub-package converts VOTable XML files to and
from ``numpy`` record arrays. This subpackage was originally developed
as ``vo.table``.

Getting Started
===============

This section provides a quick introduction of using :mod:`astropy.io.votable`. The
goal is to demonstrate the package's basic features without getting into too
much detail.

.. note::

    If you want to read or write a single table in VOTable format, the
    recommended method is via the high-level :ref:`table_io`. In particular
    see the :ref:`Unified I/O VOTables <table_io_votable>` section.

Reading a VOTable File
----------------------

To read in a VOTable file, pass a file path to
`~astropy.io.votable.parse`::

    from astropy.io.votable import parse
    votable = parse("votable.xml")

``votable`` is a `~astropy.io.votable.tree.VOTableFile` object, which
can be used to retrieve and manipulate the data and save it back out
to disk.

VOTable files are made up of nested ``RESOURCE`` elements, each of
which may contain one or more ``TABLE`` elements. The ``TABLE``
elements contain the arrays of data.

To get at the ``TABLE`` elements, you can write a loop over the
resources in the ``VOTABLE`` file::

    for resource in votable.resources:
        for table in resource.tables:
            # ... do something with the table ...
            pass

However, if the nested structure of the resources is not important,
you can use `~astropy.io.votable.tree.VOTableFile.iter_tables` to
return a flat list of all tables::

    for table in votable.iter_tables():
        # ... do something with the table ...
        pass

Finally, if you expect only one table in the file, it might be most convenient
to use `~astropy.io.votable.tree.VOTableFile.get_first_table`::

  table = votable.get_first_table()

Alternatively, there is a convenience method to parse a VOTable file and
return the first table all in one step::

  from astropy.io.votable import parse_single_table
  table = parse_single_table("votable.xml")

From a `~astropy.io.votable.tree.Table` object, you can get the data itself
in the ``array`` member variable::

  data = table.array

This data is a ``numpy`` record array.

The columns get their names from both the ``ID`` and ``name``
attributes of the ``FIELD`` elements in the ``VOTABLE`` file.

Examples
^^^^^^^^

..
  EXAMPLE START
  Reading a VOTable File with astropy.io.votable

Suppose we had a ``FIELD`` specified as follows:

.. code-block:: xml

   <FIELD ID="Dec" name="dec_targ" datatype="char" ucd="POS_EQ_DEC_MAIN"
          unit="deg">
    <DESCRIPTION>
     representing the ICRS declination of the center of the image.
    </DESCRIPTION>
   </FIELD>

.. note::

    The mapping from VOTable ``name`` and ``ID`` attributes to ``numpy``
    dtype ``names`` and ``titles`` is highly confusing.

    In VOTable, ``ID`` is guaranteed to be unique, but is not
    required. ``name`` is not guaranteed to be unique, but is
    required.

    In ``numpy`` record dtypes, ``names`` are required to be unique and
    are required. ``titles`` are not required, and are not required
    to be unique.

    Therefore, VOTable's ``ID`` most closely maps to ``numpy``'s
    ``names``, and VOTable's ``name`` most closely maps to ``numpy``'s
    ``titles``. However, in some cases where a VOTable ``ID`` is not
    provided, a ``numpy`` ``name`` will be generated based on the VOTable
    ``name``. Unfortunately, VOTable fields do not have an attribute
    that is both unique and required, which would be the most
    convenient mechanism to uniquely identify a column.

    When converting from an `astropy.io.votable.tree.Table` object to
    an `astropy.table.Table` object, you can specify whether to give
    preference to ``name`` or ``ID`` attributes when naming the
    columns. By default, ``ID`` is given preference. To give
    ``name`` preference, pass the keyword argument
    ``use_names_over_ids=True``::

      >>> votable.get_first_table().to_table(use_names_over_ids=True)

This column of data can be extracted from the record array using::

  >>> table.array['dec_targ']
  array([17.15153360566, 17.15153360566, 17.15153360566, 17.1516686826,
         17.1516686826, 17.1516686826, 17.1536197136, 17.1536197136,
         17.1536197136, 17.15375479055, 17.15375479055, 17.15375479055,
         17.1553884541, 17.15539736932, 17.15539752176,
         17.25736014763,
         # ...
         17.2765703], dtype=object)

or equivalently::

  >>> table.array['Dec']
  array([17.15153360566, 17.15153360566, 17.15153360566, 17.1516686826,
         17.1516686826, 17.1516686826, 17.1536197136, 17.1536197136,
         17.1536197136, 17.15375479055, 17.15375479055, 17.15375479055,
         17.1553884541, 17.15539736932, 17.15539752176,
         17.25736014763,
         # ...
         17.2765703], dtype=object)

..
  EXAMPLE END

Building a New Table from Scratch
---------------------------------

It is also possible to build a new table, define some field datatypes,
and populate it with data.

Example
^^^^^^^

..
  EXAMPLE START
  Building a New Table from a VOTable File

To build a new table from a VOTable file::

  from astropy.io.votable.tree import VOTableFile, Resource, Table, Field

  # Create a new VOTable file...
  votable = VOTableFile()

  # ...with one resource...
  resource = Resource()
  votable.resources.append(resource)

  # ... with one table
  table = Table(votable)
  resource.tables.append(table)

  # Define some fields
  table.fields.extend([
          Field(votable, name="filename", datatype="char", arraysize="*"),
          Field(votable, name="matrix", datatype="double", arraysize="2x2")])

  # Now, use those field definitions to create the numpy record arrays, with
  # the given number of rows
  table.create_arrays(2)

  # Now table.array can be filled with data
  table.array[0] = ('test1.xml', [[1, 0], [0, 1]])
  table.array[1] = ('test2.xml', [[0.5, 0.3], [0.2, 0.1]])

  # Now write the whole thing to a file.
  # Note, we have to use the top-level votable file object
  votable.to_xml("new_votable.xml")

..
  EXAMPLE END

Outputting a VOTable File
-------------------------

This section describes writing table data in the VOTable format using the
`~astropy.io.votable` package directly. For some cases, however, the high-level
:ref:`table_io` will often suffice and is somewhat more convenient to use. See
the :ref:`Unified I/O VOTable <table_io_votable>` section for details.

To save a VOTable file, call the
`~astropy.io.votable.tree.VOTableFile.to_xml` method. It accepts
either a string or Unicode path, or a Python file-like object::

  votable.to_xml('output.xml')

There are a number of data storage formats supported by
`astropy.io.votable`. The ``TABLEDATA`` format is XML-based and
stores values as strings representing numbers. The ``BINARY`` format
is more compact, and stores numbers in base64-encoded binary. VOTable
version 1.3 adds the ``BINARY2`` format, which allows for masking of
any data type, including integers and bit fields which cannot be
masked in the older ``BINARY`` format. The storage format can be set
on a per-table basis using the `~astropy.io.votable.tree.Table.format`
attribute, or globally using the
`~astropy.io.votable.tree.VOTableFile.set_all_tables_format` method::

  votable.get_first_table().format = 'binary'
  votable.set_all_tables_format('binary')
  votable.to_xml('binary.xml')

Using `astropy.io.votable`
==========================

Standard Compliance
-------------------

`astropy.io.votable.tree.Table` supports the `VOTable Format Definition
Version 1.1
<https://www.ivoa.net/documents/REC/VOTable/VOTable-20040811.html>`_,
`Version 1.2
<https://www.ivoa.net/documents/VOTable/20091130/REC-VOTable-1.2.html>`_,
`Version 1.3
<https://www.ivoa.net/documents/VOTable/20130920/REC-VOTable-1.3-20130920.html>`_,
and `Version 1.4
<https://www.ivoa.net/documents/VOTable/20191021/REC-VOTable-1.4-20191021.html>`_.
Some flexibility is provided to support the 1.0 draft version and
other nonstandard usage in the wild, see :ref:`verifying-votables` for more
details.

.. note::

  Each warning and VOTABLE-specific exception emitted has a number and
  is documented in more detail in :ref:`warnings` and
  :ref:`exceptions`.

Output always conforms to the 1.1, 1.2, 1.3, or 1.4 spec, depending on the
input.

.. _verifying-votables:

Verifying VOTables
^^^^^^^^^^^^^^^^^^

Many VOTable files in the wild do not conform to the VOTable specification. You
can set what should happen when a violation is encountered with the ``verify``
keyword, which can take three values:

    * ``'ignore'`` - Attempt to parse the VOTable silently. This is the default
      setting.
    * ``'warn'`` - Attempt to parse the VOTable, but raise appropriate
      :ref:`warnings`. It is possible to limit the number of warnings of the
      same type to a maximum value using the
      `astropy.io.votable.exceptions.conf.max_warnings
      <astropy.io.votable.exceptions.Conf.max_warnings>` item in the
      :ref:`astropy_config`.
    * ``'exception'`` - Do not parse the VOTable and raise an exception.

The ``verify`` keyword can be used with the :func:`~astropy.io.votable.parse`
or :func:`~astropy.io.votable.parse_single_table` functions::

  from astropy.io.votable import parse
  votable = parse("votable.xml", verify='warn')

It is possible to change the default ``verify`` value through the
`astropy.io.votable.conf.verify <astropy.io.votable.Conf.verify>` item in the
:ref:`astropy_config`.

Note that ``'ignore'`` or ``'warn'``  mean that ``astropy`` will attempt to
parse the VOTable, but if the specification has been violated then success
cannot be guaranteed.

It is good practice to report any errors to the author of the application that
generated the VOTable file to bring the file into compliance with the
specification.

Missing Values
--------------

Any value in the table may be "missing". `astropy.io.votable` stores
a  ``numpy`` masked array in each `~astropy.io.votable.tree.Table`
instance. This behaves like an ordinary ``numpy`` masked array, except
for variable-length fields. For those fields, the datatype of the
column is "object" and another ``numpy`` masked array is stored there.
Therefore, operations on variable-length columns will not work  this
is because variable-length columns are not directly supported
by ``numpy`` masked arrays.

Datatype Mappings
-----------------

The datatype specified by a ``FIELD`` element is mapped to a ``numpy``
type according to the following table:

  ================================ =========================
  VOTABLE type                     NumPy type
  ================================ =========================
  boolean                          b1
  -------------------------------- -------------------------
  bit                              b1
  -------------------------------- -------------------------
  unsignedByte                     u1
  -------------------------------- -------------------------
  char (*variable length*)         O - A ``bytes()`` object.
  -------------------------------- -------------------------
  char (*fixed length*)            S
  -------------------------------- -------------------------
  unicodeChar (*variable length*)  O - A `str` object
  -------------------------------- -------------------------
  unicodeChar (*fixed length*)     U
  -------------------------------- -------------------------
  short                            i2
  -------------------------------- -------------------------
  int                              i4
  -------------------------------- -------------------------
  long                             i8
  -------------------------------- -------------------------
  float                            f4
  -------------------------------- -------------------------
  double                           f8
  -------------------------------- -------------------------
  floatComplex                     c8
  -------------------------------- -------------------------
  doubleComplex                    c16
  ================================ =========================

If the field is a fixed-size array, the data is stored as a ``numpy``
fixed-size array.

If the field is a variable-size array (that is, ``arraysize`` contains
a '*'), the cell will contain a Python list of ``numpy`` values. Each
value may be either an array or scalar depending on the ``arraysize``
specifier.

Examining Field Types
---------------------

To look up more information about a field in a table, you can use the
`~astropy.io.votable.tree.Table.get_field_by_id` method, which returns
the `~astropy.io.votable.tree.Field` object with the given ID.

Example
^^^^^^^

..
  EXAMPLE START
  Examining Field Types in VOTables with astropy.io.votable

To look up more information about a field::

  >>> field = table.get_field_by_id('Dec')
  >>> field.datatype
  'char'
  >>> field.unit
  'deg'

.. note::
   Field descriptors should not be mutated. To change the set of
   columns, convert the Table to an `astropy.table.Table`, make the
   changes, and then convert it back.

..
  EXAMPLE END

.. _votable-serialization:

Data Serialization Formats
--------------------------

VOTable supports a number of different serialization formats.

- `TABLEDATA
  <http://www.ivoa.net/documents/VOTable/20130920/REC-VOTable-1.3-20130920.html#ToC36>`__
  stores the data in pure XML, where the numerical values are written
  as human-readable strings.

- `BINARY
  <http://www.ivoa.net/documents/VOTable/20130920/REC-VOTable-1.3-20130920.html#ToC38>`__
  is a binary representation of the data, stored in the XML as an
  opaque ``base64``-encoded blob.

- `BINARY2
  <http://www.ivoa.net/documents/VOTable/20130920/REC-VOTable-1.3-20130920.html#ToC39>`__
  was added in VOTable 1.3, and is identical to "BINARY", except that
  it explicitly records the position of missing values rather than
  identifying them by a special value.

- `FITS
  <http://www.ivoa.net/documents/VOTable/20130920/REC-VOTable-1.3-20130920.html#ToC37>`__
  stores the data in an external FITS file. This serialization is not
  supported by the `astropy.io.votable` writer, since it requires
  writing multiple files.

The serialization format can be selected in two ways:

    1) By setting the ``format`` attribute of a
    `astropy.io.votable.tree.Table` object::

        votable.get_first_table().format = "binary"
        votable.to_xml("new_votable.xml")

    2) By overriding the format of all tables using the
    ``tabledata_format`` keyword argument when writing out a VOTable
    file::

        votable.to_xml("new_votable.xml", tabledata_format="binary")

Converting to/from an `astropy.table.Table`
-------------------------------------------

The VOTable standard does not map conceptually to an
`astropy.table.Table`. However, a single table within the ``VOTable``
file may be converted to and from an `astropy.table.Table`::

  from astropy.io.votable import parse_single_table
  table = parse_single_table("votable.xml").to_table()

As a convenience, there is also a function to create an entire VOTable
file with just a single table::

  from astropy.io.votable import from_table, writeto
  votable = from_table(table)
  writeto(votable, "output.xml")

.. note::

  By default, ``to_table`` will use the ``ID`` attribute from the files to
  create the column names for the `~astropy.table.Table` object. However,
  it may be that you want to use the ``name`` attributes instead. For this,
  set the ``use_names_over_ids`` keyword to `True`. Note that since field
  ``names`` are not guaranteed to be unique in the VOTable specification,
  but column names are required to be unique in ``numpy`` structured arrays (and
  thus `astropy.table.Table` objects), the names may be renamed by appending
  numbers to the end in some cases.

Performance Considerations
--------------------------

File reads will be moderately faster if the ``TABLE`` element includes
an nrows_ attribute. If the number of rows is not specified, the
record array must be resized repeatedly during load.

.. _nrows: http://www.ivoa.net/documents/REC/VOTable/VOTable-20040811.html#ToC10

See Also
========

- `VOTable Format Definition Version 1.1
  <https://www.ivoa.net/documents/REC/VOTable/VOTable-20040811.html>`_

- `VOTable Format Definition Version 1.2
  <https://www.ivoa.net/documents/VOTable/20091130/REC-VOTable-1.2.html>`_

- `VOTable Format Definition Version 1.3
  <https://www.ivoa.net/documents/VOTable/20130920/REC-VOTable-1.3-20130920.html>`_

- `VOTable Format Definition Version 1.4
  <https://www.ivoa.net/documents/VOTable/20191021/REC-VOTable-1.4-20191021.html>`_

.. note that if this section gets too long, it should be moved to a separate
   doc page - see the top of performance.inc.rst for the instructions on how to do
   that
.. include:: performance.inc.rst

Reference/API
=============

.. automodapi:: astropy.io.votable
   :no-inheritance-diagram:
   :skip: VOWarning
   :skip: VOTableChangeWarning
   :skip: VOTableSpecWarning
   :skip: UnimplementedWarning
   :skip: IOWarning
   :skip: VOTableSpecError

.. automodapi:: astropy.io.votable.tree
   :no-inheritance-diagram:

.. automodapi:: astropy.io.votable.converters
   :no-inheritance-diagram:

.. automodapi:: astropy.io.votable.ucd
   :no-inheritance-diagram:

.. automodapi:: astropy.io.votable.util
   :no-inheritance-diagram:

.. automodapi:: astropy.io.votable.validator
   :no-inheritance-diagram:

.. automodapi:: astropy.io.votable.xmlutil
   :no-inheritance-diagram:


astropy.io.votable.exceptions Module
------------------------------------

.. toctree::
   :maxdepth: 1

   api_exceptions.rst
.. note that if this is changed from the default approach of using an *include*
   (in index.rst) to a separate performance page, the header needs to be changed
   from === to ***, the filename extension needs to be changed from .inc.rst to
   .rst, and a link needs to be added in the subpackage toctree

.. _astropy-io-ascii-performance:

Performance Tips
================

By default, when trying to read a file the reader will guess the format, which
involves trying to read it with many different readers. For better performance
when dealing with large tables, it is recommended to specify the format and any
options explicitly, and turn off guessing as well.

Example
-------

..
  EXAMPLE START
  Performance Tips for Reading Large Tables with astropy.io.ascii

If you are reading a simple CSV file with a one-line header with column names,
the following::

    read('example.csv', format='basic', delimiter=',', guess=False)  # doctest: +SKIP

can be at least an order of magnitude faster than::

    read('example.csv')  # doctest: +SKIP

..
  EXAMPLE END
.. include:: references.txt

.. _astropy.io.ascii_write:

Writing Tables
==============

:mod:`astropy.io.ascii` is able to write ASCII tables out to a file or file-like
object using the same class structure and basic user interface as for reading
tables.

The |write| function provides a way to write a data table as a
formatted ASCII table.

Examples
--------

..
  EXAMPLE START
  Writing ASCII Tables Using astropy.io.ascii

To write a formatted ASCII table using the |write| function::

  >>> import numpy as np
  >>> from astropy.io import ascii
  >>> from astropy.table import Table
  >>> data = Table()
  >>> data['x'] = np.array([1, 2, 3], dtype=np.int32)
  >>> data['y'] = data['x'] ** 2
  >>> ascii.write(data, 'values.dat', overwrite=True)  # doctest: +SKIP

The ``values.dat`` file will then contain::

  x y
  1 1
  2 4
  3 9

It is also possible and encouraged to use the write functionality from
:mod:`astropy.io.ascii` through a higher level interface in the :ref:`Data
Tables <astropy-table>` package (see :ref:`table_io` for more details). For
example::

  >>> data.write('values.dat', format='ascii', overwrite=True)  # doctest: +SKIP

For a more reproducible ASCII version of your table, we recommend using the
:ref:`ecsv_format`. This stores all the table meta-data (in particular the
column types and units) to a comment section at the beginning while still
maintaining compatibility with most plain CSV readers. It also allows storing
richer data like `~astropy.coordinates.SkyCoord` or multidimensional or
variable-length columns. For our simple example::

  >>> data.write('values.ecsv', overwrite=True)  # doctest: +SKIP

The ``.ecsv`` extension is recognized and implies using ECSV (equivalent to
``format='ascii.ecsv'``). The ``values.ecsv`` file will then contain::

  # %ECSV 1.0
  # ---
  # datatype:
  # - {name: x, datatype: int32}
  # - {name: y, datatype: int32}
  # schema: astropy-2.0
  x y
  1 1
  2 4
  3 9

Most of the input table :ref:`supported_formats` for
reading are also available for writing. This provides a great deal of
flexibility in the format for writing. The example below writes the data as a
LaTeX table, using the option to send the output to ``sys.stdout`` instead of a
file::

  >>> ascii.write(data, format='latex')  # doctest: +SKIP
  \begin{table}
  \begin{tabular}{cc}
  x & y \\
  1 & 1 \\
  2 & 4 \\
  3 & 9 \\
  \end{tabular}
  \end{table}

There is also a faster Cython engine for writing simple formats,
which is enabled by default for these formats (see :ref:`fast_ascii_io`).
To disable this engine, use the parameter ``fast_writer``::

   >>> ascii.write(data, 'values.csv', format='csv', fast_writer=False)  # doctest: +SKIP

..
  EXAMPLE END

.. Note::

   For most supported formats one can write a masked table and then read it back
   without losing information about the masked table entries. This is
   accomplished by using a blank string entry to indicate a masked (missing)
   value. See the :ref:`replace_bad_or_missing_values` section for more
   information.

.. _io_ascii_write_parameters:

Parameters for ``write()``
--------------------------

The |write| function accepts a number of parameters that specify the detailed
output table format. Each of the :ref:`supported_formats` is handled by a
corresponding Writer class that can define different defaults, so the
descriptions below sometimes mention "typical" default values. This refers to
the :class:`~astropy.io.ascii.Basic` writer and other similar Writer classes.

Some output format Writer classes (e.g., :class:`~astropy.io.ascii.Latex` or
:class:`~astropy.io.ascii.AASTex`) accept additional keywords that can
customize the output further. See the documentation of these classes for
details.

**output**: output specifier
  There are two ways to specify the output for the write operation:

  - Name of a file (string)
  - File-like object (from open(), StringIO, etc.)

**table**: input table
  Any value that is supported for initializing a |Table| object (see
  :ref:`construct_table`). This includes a table with a list of columns, a
  dictionary of columns, or from `numpy` arrays (either structured or
  homogeneous).

**format**: output format (default='basic')
  This specifies the format of the ASCII table to be written, such as a basic
  character delimited table, fixed-format table, or a CDS-compatible table,
  etc. The value of this parameter must be one of the :ref:`supported_formats`.

**delimiter**: column delimiter string
  A one-character string used to separate fields which typically defaults to
  the space character. Other common values might be "," or "|" or "\\t".

**comment**: string defining start of a comment line in output table
  For the :class:`~astropy.io.ascii.Basic` Writer this defaults to "# ".
  Which comments are written and how depends on the format chosen.
  The comments are defined as a list of strings in the input table
  ``meta['comments']`` element. Comments in the metadata of the given
  |Table| will normally be written before the header, although
  :class:`~astropy.io.ascii.CommentedHeader` writes table comments after the
  commented header. To disable writing comments, set ``comment=False``.

**formats**: dict of data type converters
  For each key (column name) use the given value to convert the column data to
  a string. If the format value is string-like, then it is used as a Python
  format statement (e.g., '%0.2f' % value). If it is a callable function, then
  that function is called with a single argument containing the column value to
  be converted. Example::

    astropy.io.ascii.write(table, sys.stdout, formats={'XCENTER': '%12.1f',
                                                 'YCENTER': lambda x: round(x, 1)},

**names**: list of output column names
  Define the complete list of output column names to write for the data table,
  overriding the existing column names.

**include_names**: list of names to include in output
  From the list of column names found from the data table or the ``names``
  parameter, select for output only columns within this list. If not supplied
  then include all names.

**exclude_names**: list of names to exclude from output
  Exclude these names from the list of output columns. This is applied *after*
  the ``include_names`` filtering. If not specified then no columns are excluded.

**fill_values**: list of fill value specifiers
  This can be used to fill missing values in the table or replace values with special meaning.

  See the :ref:`replace_bad_or_missing_values` section for more information on
  the syntax. The syntax is almost the same as when reading a table.
  There is a special value ``astropy.io.ascii.masked`` that is used to say
  "output this string for all masked values in a masked table" (the default is
  to use an empty string ``""``)::

      >>> import sys
      >>> from astropy.table import Table, Column, MaskedColumn
      >>> from astropy.io import ascii
      >>> t = Table([(1, 2), (3, 4)], names=('a', 'b'), masked=True)
      >>> t['a'].mask = [True, False]
      >>> ascii.write(t, sys.stdout)
      a b
      "" 3
      2 4
      >>> ascii.write(t, sys.stdout, fill_values=[(ascii.masked, 'N/A')])
      a b
      N/A 3
      2 4

  Note that when writing a table, all values are converted to strings before
  any value is replaced. Because ``fill_values`` only replaces cells that
  are an exact match to the specification, you need to provide the string
  representation (stripped of whitespace) for each value. For example, in
  the following commands ``-99`` is formatted with two digits after the
  comma, so we need to replace ``-99.00`` and not ``-99``::

      >>> t = Table([(-99, 2), (3, 4)], names=('a', 'b'))
      >>> ascii.write(t, sys.stdout, fill_values = [('-99.00', 'no data')],
      ...             formats={'a': '%4.2f'})
      a b
      "no data" 3
      2.00 4

  Similarly, if you replace a value in a column that has a fixed length format
  (e.g., ``'f4.2'``), then the string you want to replace must have the same
  number of characters. In the example above, ``fill_values=[(' nan',' N/A')]``
  would work.

**fill_include_names**: list of column names, which are affected by ``fill_values``
  If not supplied, then ``fill_values`` can affect all columns.

**fill_exclude_names**: list of column names, which are not affected by ``fill_values``
  If not supplied, then ``fill_values`` can affect all columns.

**fast_writer**: whether to use the fast Cython writer
  If this parameter is ``None`` (which it is by default), |write| will attempt
  to use the faster writer (described in :ref:`fast_ascii_io`) if possible.
  Specifying ``fast_writer=False`` disables this behavior.

**Writer** : Writer class (*deprecated* in favor of ``format``)
  This specifies the top-level format of the ASCII table to be written, such as
  a basic character delimited table, fixed-format table, or a CDS-compatible
  table, etc. The value of this parameter must be a Writer class. For basic
  usage this means one of the built-in :ref:`extension_reader_classes`.
  Note that Reader classes and Writer classes are synonymous; in other
  words, Reader classes can also write, but for historical reasons they are
  often called Reader classes.

.. _cds_mrt_format:

Machine-Readable Table Format
-----------------------------

The American Astronomical Society Journals' `Machine-Readable Table (MRT)
<https://journals.aas.org/mrt-standards/>`_ format consists of single file with
the table description header and the table data itself. MRT is similar to the
`CDS <http://vizier.u-strasbg.fr/doc/catstd.htx>`_ format standard, but differs
in the table description sections and the lack of a separate ``ReadMe`` file.
Astropy does not support writing in the CDS format.

The :class:`~astropy.io.ascii.Mrt` writer supports writing tables to MRT format.

.. note::

    The metadata of the table, apart from column ``unit``, ``name`` and
    ``description``, are not written in the output file. Placeholders for
    the title, authors, and table name fields are put into the output file and
    can be edited after writing.

Examples
""""""""

..
  EXAMPLE START
  Writing MRT Format Tables Using astropy.io.ascii

The command ``ascii.write(format='mrt')`` writes an ``astropy`` `~astropy.table.Table`
to the MRT format. Section dividers ``---`` and ``===`` are used to divide the table
into different sections, with the last section always been the actual data.

As the MRT standard requires,
for columns that have a ``unit`` attribute not set to ``None``,
the unit names are tabulated in the Byte-By-Byte
description of the column. When columns do not contain any units, ``---`` is put instead.
A ``?`` is prefixed to the column description in the Byte-By-Byte for ``Masked``
columns or columns that have null values, indicating them as such.

The example below initializes a table with columns that have a ``unit`` attribute and
has masked values.

  >>> from astropy.io import ascii
  >>> from astropy.table import Table, Column, MaskedColumn
  >>> from astropy import units as u
  >>> table = Table()
  >>> table['Name'] = ['ASASSN-15lh', 'ASASSN-14li']
  >>> # MRT Standard requires all quantities in SI units.
  >>> temperature = [0.0334, 0.297] * u.K
  >>> table['Temperature'] = temperature.to(u.keV, equivalencies=u.temperature_energy())
  >>> table['nH'] = Column([0.025, 0.0188], unit=u.Unit(10**22))
  >>> table['Flux'] = ([2.044 * 10**-11] * u.erg * u.cm**-2).to(u.Jy * u.Unit(10**12))
  >>> table['Flux'] = MaskedColumn(table['Flux'], mask=[True, False])
  >>> table['magnitude'] = [u.Magnitude(25), u.Magnitude(-9)]

Note that for columns with `~astropy.time.Time`, `~astropy.time.TimeDelta` and related values,
the writer does not do any internal conversion or modification. These columns should be
converted to regular columns with proper ``unit`` and ``name`` attribute before writing
the table. Thus::

  >>> from astropy.time import Time, TimeDelta
  >>> from astropy.timeseries import TimeSeries
  >>> ts = TimeSeries(time_start=Time('2019-01-01'), time_delta=2*u.day, n_samples=1)
  >>> table['Obs'] = Column(ts.time.decimalyear, description='Time of Observation')
  >>> table['Cadence'] = Column(TimeDelta(100.0, format='sec').datetime.seconds,
  ...                           unit=u.s)

Columns that are `~astropy.coordinates.SkyCoord` objects or columns with
values that are such objects are recognized as such, and some predefined labels and
description is used for them. Coordinate columns that have `~astropy.coordinates.SphericalRepresentation`
are additionally sub-divided into their coordinate component columns. Representations that have
``ra`` and ``dec`` components are divided into their ``hour``-``min``-``sec``
and ``deg``-``arcmin``-``arcsec`` components respectively. Whereas columns with
``SkyCoord`` objects in the ``Galactic`` or any of the ``Ecliptic`` frames are divided
into their latitude(``ELAT``/``GLAT``) and longitude components (``ELON``/``GLAT``) only.
The original table remains accessible as such, while the file is written from a modified
copy of the table. The new coordinate component columns are appended to the end of the table.

It should be noted that the default precision of the latitude, longitude and seconds (of arc)
columns is set at a default number of 12, 10 and 9 digits after the decimal for ``deg``, ``sec``
and ``arcsec`` values, respectively. This default is set to match a machine precision of 1e-15
relative to the original ``SkyCoord`` those columns were extracted from.
As all other columns, the format can be expliclty set by passing the ``formats`` keyword to the
``write`` function or by setting the ``format`` attribute of individual columns (the latter
will only work for columns that are not decomposed).
To customize the number of significant digits, presicions should therefore be specified in the
``formats`` dictionary for the *output* column names, such as
``formats={'RAs': '07.4f', 'DEs': '06.3f'}`` or ``formats={'GLAT': '+10.6f', 'GLON': '9.6f'}``
for milliarcsecond accuracy. Note that the forms with leading zeros for the seconds and
including the sign for latitudes are recommended for better consistency and readability.

The following code illustrates the above.

  >>> from astropy.coordinates import SkyCoord
  >>> table['coord'] = [SkyCoord.from_name('ASASSN-15lh'),
  ...                   SkyCoord.from_name('ASASSN-14li')]  # doctest: +REMOTE_DATA
  >>> table.write('coord_cols.dat', format='ascii.mrt')     # doctest: +SKIP
  >>> table['coord'] = table['coord'].geocentrictrueecliptic  # doctest: +REMOTE_DATA
  >>> table['Temperature'].format = '.5E' # Set default column format.
  >>> table.write('ecliptic_cols.dat', format='ascii.mrt')    # doctest: +SKIP

After execution, the contents of ``coords_cols.dat`` will be::

  Title:
  Authors:
  Table:
  ================================================================================
  Byte-by-byte Description of file: table.dat
  --------------------------------------------------------------------------------
   Bytes Format Units  Label     Explanations
  --------------------------------------------------------------------------------
   1-11  A11     ---    Name        Description of Name
  13-23  E11.6   keV    Temperature [0.0/0.01] Description of Temperature
  25-30  F6.4    10+22  nH          [0.01/0.03] Description of nH
  32-36  F5.3   10+12Jy Flux        ? Description of Flux
  38-42  E5.1    mag    magnitude   [0.0/3981.08] Description of magnitude
  44-49  F6.1    ---    Obs         [2019.0/2019.0] Time of Observation
  51-53  I3      s      Cadence     [100] Description of Cadence
  55-56  I2     h      RAh           Right Ascension (hour)
  58-59  I2     min    RAm           Right Ascension (minute)
  61-73  F13.10 s      RAs           Right Ascension (second)
     75  A1     ---    DE-           Sign of Declination
  76-77  I2     deg    DEd           Declination (degree)
  79-80  I2     arcmin DEm           Declination (arcmin)
  82-93  F12.9  arcsec DEs           Declination (arcsec)
  --------------------------------------------------------------------------------
  Notes:
  --------------------------------------------------------------------------------
  ASASSN-15lh 2.87819e-09 0.0250       1e-10 2019.0 100 22 02 15.4500000000 -61 39 34.599996000
  ASASSN-14li 2.55935e-08 0.0188 2.044 4e+03 2019.0 100 12 48 15.2244072000 +17 46 26.496624000

And the file ``ecliptic_cols.dat`` will look like::

  Title:
  Authors:
  Table:
  ================================================================================
  Byte-by-byte Description of file: table.dat
  --------------------------------------------------------------------------------
   Bytes Format Units  Label     Explanations
  --------------------------------------------------------------------------------
   1- 11  A11     ---    Name        Description of Name
  13- 23  E11.6   keV    Temperature [0.0/0.01] Description of Temperature
  25- 30  F6.4    10+22  nH          [0.01/0.03] Description of nH
  32- 36  F5.3   10+12Jy Flux        ? Description of Flux
  38- 42  E5.1    mag    magnitude   [0.0/3981.08] Description of magnitude
  44- 49  F6.1    ---    Obs         [2019.0/2019.0] Time of Observation
  51- 53  I3      s      Cadence     [100] Description of Cadence
  55- 70  F16.12  deg    ELON        Ecliptic Longitude (geocentrictrueecliptic)
  72- 87  F16.12  deg    ELAT        Ecliptic Latitude (geocentrictrueecliptic)
  --------------------------------------------------------------------------------
  Notes:
  --------------------------------------------------------------------------------
  ASASSN-15lh 2.87819e-09 0.0250       1e-10 2019.0 100 306.224208650096 -45.621789850825
  ASASSN-14li 2.55935e-08 0.0188 2.044 4e+03 2019.0 100 183.754980099243  21.051410763027

Finally, MRT has some specific naming conventions for columns
(`<https://journals.aas.org/mrt-labels/#reflab>`_). For example, if a column contains
the mean error for the data in a column named ``label``, then this column should be named ``e_label``.
These kinds of relative column naming cannot be enforced by the MRT writer
because it does not know what the column data means and thus, the relation between the
columns cannot be figured out. Therefore, it is up to the user to use ``Table.rename_columns``
to appropriately rename any columns before writing the table to MRT format.
The following example shows a similar situation, using the option to send the output to
``sys.stdout`` instead of a file::

  >>> table['error'] = [1e4, 450] * u.Jy  # Error in the Flux values.
  >>> outtab = table.copy()  # So that changes don't affect the original table.
  >>> outtab.rename_column('error', 'e_Flux')
  >>> # re-order so that related columns are placed next to eachother.
  >>> outtab = outtab['Name', 'Obs', 'coord', 'Cadence', 'nH', 'magnitude',
  ...                 'Temperature', 'Flux', 'e_Flux']  # doctest: +REMOTE_DATA

  >>> ascii.write(outtab, format='mrt')  # doctest: +SKIP
  Title:
  Authors:
  Table:
  ================================================================================
  Byte-by-byte Description of file: table.dat
  --------------------------------------------------------------------------------
   Bytes Format Units  Label     Explanations
  --------------------------------------------------------------------------------
   1- 11  A11     ---    Name        Description of Name
  13- 18  F6.1    ---    Obs         [2019.0/2019.0] Time of Observation
  20- 22  I3      s      Cadence     [100] Description of Cadence
  24- 29  F6.4    10+22  nH          [0.01/0.03] Description of nH
  31- 35  E5.1    mag    magnitude   [0.0/3981.08] Description of magnitude
  37- 47  E11.6   keV    Temperature [0.0/0.01] Description of Temperature
  49- 53  F5.3   10+12Jy Flux        ? Description of Flux
  55- 61  F7.1    Jy     e_Flux      [450.0/10000.0] Description of e_Flux
  63- 78  F16.12  deg    ELON        Ecliptic Longitude (geocentrictrueecliptic)
  80- 95  F16.12  deg    ELAT        Ecliptic Latitude (geocentrictrueecliptic)
  --------------------------------------------------------------------------------
  Notes:
  --------------------------------------------------------------------------------
  ASASSN-15lh 2019.0 100 0.0250 1e-10 2.87819e-09       10000.0 306.224208650096 -45.621789850825
  ASASSN-14li 2019.0 100 0.0188 4e+03 2.55935e-08 2.044   450.0 183.754980099243  21.051410763027

..
  EXAMPLE END

.. attention::

    The MRT writer currently supports automatic writing of a single coordinate column
    in ``Tables``. For tables with more than one coordinate column of a given kind
    (e.g. equatorial, galactic or ecliptic), only the first found coordinate column
    will be decomposed into its component columns, and the rest of the coordinate
    columns of the same type will be converted to string columns. Thus users should take
    care that the additional coordinate columns are dealt with (e.g. by converting them
    to unique ``float``-valued columns) before using ``SkyCoord`` methods.
.. include:: references.txt

.. _astropy.io.ascii_read:

Reading Tables
**************

The majority of commonly encountered ASCII tables can be read with the |read|
function::

  >>> from astropy.io import ascii
  >>> data = ascii.read(table)  # doctest: +SKIP

Here ``table`` is the name of a file, a string representation of a table, or a
list of table lines. The return value (``data`` in this case) is a :ref:`Table
<astropy-table>` object.

By default, |read| will try to `guess the table format <#guess-table-format>`_
by trying all of the supported formats.

.. Warning::

   Guessing the file format is often slow for large files because the reader
   tries parsing the file with every allowed format until one succeeds.
   For large files it is recommended to disable guessing with ``guess=False``.

..
  EXAMPLE START
  Reading ASCII Tables Using astropy.io.ascii

For unusually formatted tables where guessing does not work, give additional
hints about the format::

   >>> lines = ['objID                   & osrcid            & xsrcid       ',
   ...          '----------------------- & ----------------- & -------------',
   ...          '              277955213 & S000.7044P00.7513 & XS04861B6_005',
   ...          '              889974380 & S002.9051P14.7003 & XS03957B7_004']
   >>> data = ascii.read(lines, data_start=2, delimiter='&')
   >>> print(data)
     objID         osrcid          xsrcid
   --------- ----------------- -------------
   277955213 S000.7044P00.7513 XS04861B6_005
   889974380 S002.9051P14.7003 XS03957B7_004

Other examples are as follows::

   >>> data = astropy.io.ascii.read('data/nls1_stackinfo.dbout', data_start=2, delimiter='|')  # doctest: +SKIP
   >>> data = astropy.io.ascii.read('data/simple.txt', quotechar="'")  # doctest: +SKIP
   >>> data = astropy.io.ascii.read('data/simple4.txt', format='no_header', delimiter='|')  # doctest: +SKIP
   >>> data = astropy.io.ascii.read('data/tab_and_space.txt', delimiter=r'\s')  # doctest: +SKIP

If the format of a file is known (e.g., it is a fixed-width table or an IPAC
table), then it is more efficient and reliable to provide a value for the
``format`` argument from one of the values in the :ref:`supported_formats`. For
example::

   >>> data = ascii.read(lines, format='fixed_width_two_line', delimiter='&')

See the :ref:`guess_formats` section for additional details on format guessing.

..
  EXAMPLE END

For simpler formats such as CSV, |read| will automatically try reading with the
Cython/C parsing engine, which is significantly faster than the ordinary Python
implementation (described in :ref:`fast_ascii_io`). If the fast engine fails,
|read| will fall back on the Python reader by default. The argument
``fast_reader`` can be specified to control this behavior. For example, to
disable the fast engine::

   >>> data = ascii.read(lines, format='csv', fast_reader=False)

For reading very large tables see the section on :ref:`chunk_reading` or
use `pandas <https://pandas.pydata.org/>`_ (see Note below).

.. Note::

   Reading a table which contains unicode characters is supported with the
   pure Python readers by specifying the ``encoding`` parameter. The fast
   C-readers do not support unicode. For large data files containing unicode,
   we recommend reading the file using `pandas <https://pandas.pydata.org/>`_
   and converting to a :ref:`Table <astropy-table>` via the :ref:`Table -
   Pandas interface <pandas>`.

The |read| function accepts a number of parameters that specify the detailed
table format. Different formats can define different defaults, so the
descriptions below sometimes mention "typical" default values. This refers to
the :class:`~astropy.io.ascii.Basic` format reader and other similar character-separated formats.

.. _io_ascii_read_parameters:

Parameters for ``read()``
=========================

**table** : input table
  There are four ways to specify the table to be read:

  - Path to a file (string)
  - Single string containing all table lines separated by newlines
  - File-like object with a callable read() method
  - List of strings where each list element is a table line

  The first two options are distinguished by the presence of a newline in the
  string. This assumes that valid file names will not normally contain a
  newline, and a valid table input will at least contain two rows.
  Note that a table read in ``no_header`` format can legitimately consist
  of a single row; in this case passing the string as a list with a single
  item will ensure that it is not interpreted as a file name.

**format** : file format (default='basic')
  This specifies the top-level format of the ASCII table; for example,
  if it is a basic character delimited table, fixed format table, or
  a CDS-compatible table, etc. The value of this parameter must
  be one of the :ref:`supported_formats`.

**guess** : try to guess table format (default=None)
  If set to True, then |read| will try to guess the table format by cycling
  through a number of possible table format permutations and attempting to read
  the table in each case. See the `Guess table format`_ section for further details.

**delimiter** : column delimiter string
  A one-character string used to separate fields which typically defaults to
  the space character. Other common values might be "\\s" (whitespace), "," or
  "|" or "\\t" (tab). A value of "\\s" allows any combination of the tab and
  space characters to delimit columns.

**comment** : regular expression defining a comment line in table
  If the ``comment`` regular expression matches the beginning of a table line
  then that line will be discarded from header or data processing.  For the
  ``basic`` format this defaults to "\\s*#" (any whitespace followed by #).

**quotechar** : one-character string to quote fields containing special characters
  This specifies the quote character and will typically be either the single or
  double quote character. This is can be useful for reading text fields with
  spaces in a space-delimited table. The default is typically the double quote.

**header_start** : line index for the header line
  This includes only significant non-comment lines and counting starts at 0. If
  set to None this indicates that there is no header line and the column names
  will be auto-generated. See `Specifying header and data location`_ for more
  details.

**data_start** : line index for the start of data counting
  This includes only significant non-comment lines and counting starts at 0.
  See `Specifying header and data location`_ for more details.

**data_end** : line index for the end of data
  This includes only significant non-comment lines and can be negative to count
  from end. See `Specifying header and data location`_ for more details.

**encoding**: encoding to read the file (``default=None``)
  When `None` use `locale.getpreferredencoding` as an encoding. This matches
  the default behavior of the built-in `open` when no ``mode`` argument is
  provided.

**converters** : ``dict`` of data type converters
  See the `Converters`_ section for more information.

**names** : list of names corresponding to each data column
  Define the complete list of names for each data column. This will override
  names found in the header (if it exists). If not supplied then
  use names from the header or auto-generated names if there is no header.

**include_names** : list of names to include in output
  From the list of column names found from the header or the ``names``
  parameter, select for output only columns within this list. If not supplied,
  then include all names.

**exclude_names** : list of names to exclude from output
  Exclude these names from the list of output columns. This is applied *after*
  the ``include_names`` filtering. If not specified then no columns are excluded.

**fill_values** : list of fill value specifiers
  Specify input table entries which should be masked in the output table
  because they are bad or missing. See the `Bad or missing values`_ section
  for more information and examples. The default is that any blank table
  values are treated as missing.

**fill_include_names** : list of column names affected by ``fill_values``
  This is a list of column names (found from the header or the ``names``
  parameter) for all columns where values will be filled. `None` (the default) will
  apply ``fill_values`` to all columns.

**fill_exclude_names** : list of column names not affected by ``fill_values``
  This is a list of column names (found from the header or the ``names``
  parameter) for all columns where values will be **not** be filled.
  This parameter takes precedence over ``fill_include_names``.  A value
  of `None` (default) does not exclude any columns.

**Outputter** : Outputter class
  This converts the raw data tables value into the
  output object that gets returned by |read|. The default is
  :class:`~astropy.io.ascii.TableOutputter`, which returns a
  :class:`~astropy.table.Table` object (see :ref:`Data Tables <astropy-table>`).

**Inputter** : Inputter class
  This is generally not specified.

**data_Splitter** : Splitter class to split data columns

**header_Splitter** : Splitter class to split header columns

**fast_reader** : whether to use the C engine
  This can be ``True`` or ``False``, and also be a ``dict`` with options.
  (see :ref:`fast_ascii_io`)

**Reader** : Reader class (*deprecated* in favor of ``format``)
  This specifies the top-level format of the ASCII table; for example,
  if it is a basic character delimited table, fixed format table, or
  a CDS-compatible table, etc. The value of this parameter must
  be a Reader class. For basic usage this means one of the
  built-in :ref:`extension_reader_classes`.

Specifying Header and Data Location
===================================

The three parameters ``header_start``, ``data_start``, and ``data_end`` make it
possible to read a table file that has extraneous non-table data included.
This is a case where you need to help out `astropy.io.ascii` and tell it where
to find the header and data.

When a file is processed into a header and data components, any blank lines
(which might have whitespace characters) and commented lines (starting with the
comment character, typically ``#``) are stripped out *before* the header and
data parsing code sees the table content.

Example
-------

..
  EXAMPLE START
  Specifying Header and Data Location for ASCII Tables

To use the parameters ``header_start``, ``data_start``, and ``data_end``
to read a table with non-table data included, take the file below. The column
on the left is not part of the file but instead shows how `astropy.io.ascii` is
viewing each line and the line count index.  ::

  Index    Table content
  ------ ----------------------------------------------------------------
     -  | # This is the start of my data file
     -  |
     0  | Automatically generated by my_script.py at 2012-01-01T12:13:14
     1  | Run parameters: None
     2  | Column header line:
     -  |
     3  | x y z
     -  |
     4  | Data values section:
     -  |
     5  | 1 2 3
     6  | 4 5 6
     -  |
     7  | Run completed at 2012:01-01T12:14:01

In this case you would have ``header_start=3``, ``data_start=5``, and
``data_end=7``. The convention for ``data_end`` follows the normal Python
slicing convention where to select data rows 5 and 6 you would do
``rows[5:7]``. For ``data_end`` you can also supply a negative index to
count backward from the end, so ``data_end=-1`` (like ``rows[5:-1]``) would
work in this case.

..
  EXAMPLE END

.. _replace_bad_or_missing_values:

Bad or Missing Values
=====================

ASCII data tables can contain bad or missing values. A common case is when a
table contains blank entries with no available data.

Examples
--------

..
  EXAMPLE START
  ASCII Tables with Bad or Missing Values

Take this example of a table with blank entries::

  >>> weather_data = """
  ...   day,precip,type
  ...   Mon,1.5,rain
  ...   Tues,,
  ...   Wed,1.1,snow
  ...   """

By default, |read| will interpret blank entries as being bad/missing and output
a masked Table with those entries masked out by setting the corresponding mask
value set to ``True``::

  >>> dat = ascii.read(weather_data)
  >>> print(dat)
  day  precip type
  ---- ------ ----
   Mon    1.5 rain
  Tues     --   --
   Wed    1.1 snow

If you want to replace the masked (missing) values with particular values, set
the masked column ``fill_value`` attribute and then get the "filled" version of
the table. This looks like the following::

  >>> dat['precip'].fill_value = -999
  >>> dat['type'].fill_value = 'N/A'
  >>> print(dat.filled())
  day  precip type
  ---- ------ ----
   Mon    1.5 rain
  Tues -999.0  N/A
   Wed    1.1 snow

ASCII tables may have other indicators of bad or missing data as well. For
example, a table may contain string values that are not a valid representation
of a number (e.g., ``"..."``), or a table may have special values like ``-999``
that are chosen to indicate missing data. The |read| function has a flexible
system to accommodate these cases by marking specified character sequences in
the input data as "missing data" during the conversion process. Whenever
missing data is found the output will be a masked table.

This is done with the ``fill_values`` keyword argument, which can be set to a
single missing-value specification ``<missing_spec>`` or a list of ``<missing_spec>`` tuples::

  fill_values = <missing_spec> | [<missing_spec1>, <missing_spec2>, ...]
  <missing_spec> = (<match_string>, '0', <optional col name 1>, <optional col name 2>, ...)

When reading a table, the second element of a ``<missing_spec>`` should always
be the string ``'0'``, otherwise you may get unexpected behavior [#f1]_. By
default, the ``<missing_spec>`` is applied to all columns unless column name
strings are supplied. An alternate way to limit the columns is via the
``fill_include_names`` and ``fill_exclude_names`` keyword arguments in |read|.

In the example below we read back the weather table after filling the missing
values in with typical placeholders::

  >>> table = ['day   precip  type',
  ...          ' Mon     1.5  rain',
  ...          'Tues  -999.0   N/A',
  ...          ' Wed     1.1  snow']
  >>> t = ascii.read(table, fill_values=[('-999.0', '0', 'precip'), ('N/A', '0', 'type')])
  >>> print(t)
  day  precip type
  ---- ------ ----
   Mon    1.5 rain
  Tues     --   --
   Wed    1.1 snow

.. note::

   The default in |read| is ``fill_values=('','0')``. This marks blank entries as being
   missing for any data type (int, float, or string). If ``fill_values`` is explicitly
   set in the call to |read| then the default behavior of marking blank entries as missing
   no longer applies. For instance setting ``fill_values=None`` will disable this
   auto-masking without setting any other fill values. This can be useful for a string
   column where one of values happens to be ``""``.


.. [#f1] The requirement to put the ``'0'`` there is the legacy of an old
         interface which is maintained for backward compatibility and also to
         match the format of ``fill_value`` for reading with the format of
         ``fill_value`` used for writing tables. On reading, the second
         element of the ``<missing_spec>`` tuple can actually be an arbitrary
         string value which replaces occurrences of the ``<match_string>``
         string in the input stream prior to type conversion. This ends up
         being the value "behind the mask", which should never be directly
         accessed. Only the value ``'0'`` is neutral when attempting to detect
         the column data type and perform type conversion. For instance if you
         used ``'nan'`` for the ``<match_string>`` value then integer columns
         would wind up as float.

..
  EXAMPLE END

Selecting columns for masking
-----------------------------
The |read| function provides the parameters ``fill_include_names`` and ``fill_exclude_names``
to select which columns will be used in the ``fill_values`` masking process described above.

..
  EXAMPLE START
  Using the ``fill_include_names`` and ``fill_exclude_names`` parameters for ASCII tables

The use of these parameters is not common but in some cases can considerably simplify
the code required to read a table. The following gives a simple example to illustrate how
``fill_include_names`` and ``fill_exclude_names`` can be used
in the most basic and typical cases::

  >>> from astropy.io import ascii
  >>> lines = ['a,b,c,d', '1.0,2.0,3.0,4.0', ',,,']
  >>> ascii.read(lines)
  <Table length=2>
     a       b       c       d
  float64 float64 float64 float64
  ------- ------- ------- -------
      1.0     2.0     3.0     4.0
       --      --      --      --

  >>> ascii.read(lines, fill_include_names=['a', 'c'])
  <Table length=2>
     a     b      c     d
  float64 str3 float64 str3
  ------- ---- ------- ----
      1.0  2.0     3.0  4.0
       --           --

  >>> ascii.read(lines, fill_exclude_names=['a', 'c'])
  <Table length=2>
   a      b     c      d
  str3 float64 str3 float64
  ---- ------- ---- -------
   1.0     2.0  3.0     4.0
            --           --

..
  EXAMPLE END

.. _guess_formats:

Guess Table Format
==================

If the ``guess`` parameter in |read| is set to True, then
|read| will try to guess the table format by cycling through a number of
possible table format permutations and attempting to read the table in each
case. The first format which succeeds and will be used to read the table. To
succeed, the table must be successfully parsed by the Reader and satisfy the
following column requirements:

 * At least two table columns.
 * No column names are a float or int number.
 * No column names begin or end with space, comma, tab, single quote, double
   quote, or a vertical bar (|).

These requirements reduce the chance for a false positive where a table is
successfully parsed with the wrong format. A common situation is a table
with numeric columns but no header row, and in this case `astropy.io.ascii` will
auto-assign column names because of the restriction on column names that
look like a number.

Guess Order
-----------

The order of guessing is shown by this Python code, where ``Reader`` is the
class which actually implements reading the different file formats::

  for Reader in (Ecsv, FixedWidthTwoLine, Rst, FastBasic, Basic,
                 FastRdb, Rdb, FastTab, Tab, Cds, Daophot, SExtractor,
                 Ipac, Latex, AASTex):
      read(Reader=Reader)

  for Reader in (CommentedHeader, FastBasic, Basic, FastNoHeader, NoHeader):
      for delimiter in ("|", ",", " ", "\\s"):
          for quotechar in ('"', "'"):
              read(Reader=Reader, delimiter=delimiter, quotechar=quotechar)

Note that the :class:`~astropy.io.ascii.FixedWidth` derived-readers are not
included in the default guess sequence (this causes problems), so to read such
tables you must explicitly specify the format with the ``format`` keyword. Also
notice that formats compatible with the fast reading engine attempt to use the
fast engine before the ordinary reading engine.

If none of the guesses succeed in reading the table (subject to the column
requirements), a final try is made using just the user-supplied parameters but
without checking the column requirements. In this way, a table with only one
column or column names that look like a number can still be successfully read.

The guessing process respects any values of the Reader, delimiter, and
quotechar parameters as well as options for the fast reader that were
supplied to the read() function. Any guesses that would conflict are
skipped. For example, the call::

 >>> data = ascii.read(table, Reader=ascii.NoHeader, quotechar="'")

would only try the four delimiter possibilities, skipping all the conflicting
Reader and quotechar combinations. Similarly, with any setting of
``fast_reader`` that requires use of the fast engine, only the fast
variants in the Reader list above will be tried.

Disabling
---------

Guessing can be disabled in two ways::

  import astropy.io.ascii
  data = astropy.io.ascii.read(table)               # guessing enabled by default
  data = astropy.io.ascii.read(table, guess=False)  # disable for this call
  astropy.io.ascii.set_guess(False)                 # set default to False globally
  data = astropy.io.ascii.read(table)               # guessing disabled

Debugging
---------

In order to get more insight into the guessing process and possibly debug if
something is not working as expected, use the
`~astropy.io.ascii.get_read_trace()` function. This returns a traceback of the
attempted read formats for the last call to `~astropy.io.ascii.read()`.

Comments and Metadata
=====================

Any comment lines detected during reading are inserted into the output table
via the ``comments`` key in the table's ``.meta`` dictionary.

Example
-------

..
  EXAMPLE START
  Comments and Metadata in ASCII Tables

Comment lines detected during reading are inserted into the output table as
such::

 >>> table='''# TELESCOPE = 30 inch
 ...          # TARGET = PV Ceph
 ...          # BAND = V
 ...          MJD mag
 ...          55555 12.3
 ...          55556 12.4'''
 >>> dat = ascii.read(table)
 >>> print(dat.meta['comments'])
 ['TELESCOPE = 30 inch', 'TARGET = PV Ceph', 'BAND = V']

While :mod:`astropy.io.ascii` will not do any post-processing on comment lines,
custom post-processing can be accomplished by rereading with the metadata line
comments. Here is one example, where comments are of the form "# KEY = VALUE"::

 >>> header = ascii.read(dat.meta['comments'], delimiter='=',
 ...                     format='no_header', names=['key', 'val'])
 >>> print(header)
    key      val
 --------- -------
 TELESCOPE 30 inch
    TARGET PV Ceph
      BAND       V

..
  EXAMPLE END

Converters
==========

:mod:`astropy.io.ascii` converts the raw string values from the table into
numeric data types by using converter functions such as the Python ``int`` and
``float`` functions. For example, ``int("5.0")`` will fail while float("5.0")
will succeed and return 5.0 as a Python float.

The default converters are::

    default_converters = [astropy.io.ascii.convert_numpy(numpy.int),
                          astropy.io.ascii.convert_numpy(numpy.float),
                          astropy.io.ascii.convert_numpy(numpy.str)]

These take advantage of the :func:`~astropy.io.ascii.convert_numpy`
function which returns a two-element tuple ``(converter_func, converter_type)``
as described in the previous section. The type provided to
:func:`~astropy.io.ascii.convert_numpy` must be a valid `NumPy type
<https://numpy.org/doc/stable/user/basics.types.html>`_ such as
``numpy.int``, ``numpy.uint``, ``numpy.int8``, ``numpy.int64``,
``numpy.float``, ``numpy.float64``, or ``numpy.str``.

The default converters for each column can be overridden with the
``converters`` keyword::

  >>> import numpy as np
  >>> converters = {'col1': [ascii.convert_numpy(np.uint)],
  ...               'col2': [ascii.convert_numpy(np.float32)]}
  >>> ascii.read('file.dat', converters=converters)  # doctest: +SKIP

In addition to single column names you can use wildcards via `fnmatch` to
select multiple columns. For example, we can set the format for all columns
with a name starting with "col" to an unsigned integer while applying default
converters to all other columns in the table::

  >>> import numpy as np
  >>> converters = {'col*': [ascii.convert_numpy(np.uint)]}
  >>> ascii.read('file.dat', converters=converters)  # doctest: +SKIP


.. _fortran_style_exponents:

Fortran-Style Exponents
=======================

The :ref:`fast converter <fast_conversion_opts>` available with the C
input parser provides an ``exponent_style`` option to define a custom
character instead of the standard ``'e'`` for exponential formats in
the input file, to read, for example, Fortran-style double precision
numbers like ``'1.495978707D+13'``:

  >>> ascii.read('double.dat', format='basic', guess=False,
  ...            fast_reader={'exponent_style': 'D'})  # doctest: +SKIP

The special setting ``'fortran'`` is provided to allow for the
auto-detection of any valid Fortran exponent character (``'E'``,
``'D'``, ``'Q'``), as well as of triple-digit exponents prefixed with no
character at all (e.g., ``'2.1127123261674622-107'``).
All values and exponent characters in the input data are
case-insensitive; any value other than the default ``'E'`` implies the
automatic setting of ``'use_fast_converter': True``.

Advanced Customization
======================

Here we provide a few examples that demonstrate how to extend the base
functionality to handle special cases. To go beyond these examples, the
best reference is to read the code for the existing
:ref:`extension_reader_classes`.

Examples
--------

..
  EXAMPLE START
  Advanced Customization to Extend Base Functionality of astropy.io.ascii

For special cases, these examples demonstrate how to extend the base
functionality of `astropy.io.ascii`.

**Define custom readers by class inheritance**

The most useful way to define a new reader class is by inheritance.
This is the way all of the built-in readers are defined, so there are plenty
of examples in the code.

In most cases, you will define one class to handle the header,
one class that handles the data, and a reader class that ties it all together.
Here is an example from the code that defines a reader that is just like
the basic reader, but header and data start in different lines of the file::

  # Note: NoHeader is already included in astropy.io.ascii for convenience.
  class NoHeaderHeader(BasicHeader):
      '''Reader for table header without a header

      Set the start of header line number to `None`, which tells the basic
      reader there is no header line.
      '''
      start_line = None

  class NoHeaderData(BasicData):
      '''Reader for table data without a header

      Data starts at first uncommented line since there is no header line.
      '''
      start_line = 0

  class NoHeader(Basic):
      """Read a table with no header line.  Columns are autonamed using
      header.auto_format which defaults to "col%d".  Otherwise this reader
      the same as the :class:`Basic` class from which it is derived.  Example::

        # Table data
        1 2 "hello there"
        3 4 world
      """
      _format_name = 'no_header'
      _description = 'Basic table with no headers'
      header_class = NoHeaderHeader
      data_class = NoHeaderData

In a slightly more involved case, the implementation can also override some of
the methods in the base class::

  # Note: CommentedHeader is already included in astropy.io.ascii for convenience.
  class CommentedHeaderHeader(BasicHeader):
      """Header class for which the column definition line starts with the
      comment character.  See the :class:`CommentedHeader` class  for an example.
      """
      def process_lines(self, lines):
          """Return only lines that start with the comment regexp.  For these
          lines strip out the matching characters."""
          re_comment = re.compile(self.comment)
          for line in lines:
              match = re_comment.match(line)
              if match:
                  yield line[match.end():]

      def write(self, lines):
          lines.append(self.write_comment + self.splitter.join(self.colnames))


  class CommentedHeader(Basic):
      """Read a file where the column names are given in a line that begins with
      the header comment character. ``header_start`` can be used to specify the
      line index of column names, and it can be a negative index (for example -1
      for the last commented line).  The default delimiter is the <space>
      character.::

        # col1 col2 col3
        # Comment line
        1 2 3
        4 5 6
      """
      _format_name = 'commented_header'
      _description = 'Column names in a commented line'

      header_class = CommentedHeaderHeader
      data_class = NoHeaderData


**Define a custom reader functionally**

Instead of defining a new class, it is also possible to obtain an instance
of a reader, and then to modify the properties of this one reader instance
in a function::

   def read_rdb_table(table):
       reader = astropy.io.ascii.Basic()
       reader.header.splitter.delimiter = '\t'
       reader.data.splitter.delimiter = '\t'
       reader.header.splitter.process_line = None
       reader.data.splitter.process_line = None
       reader.data.start_line = 2

       return reader.read(table)


**Create a custom splitter.process_val function**
::

   # The default process_val() normally just strips whitespace.
   # In addition have it replace empty fields with -999.
   def process_val(x):
       """Custom splitter process_val function: Remove whitespace at the beginning
       or end of value and substitute -999 for any blank entries."""
       x = x.strip()
       if x == '':
           x = '-999'
       return x

   # Create an RDB reader and override the splitter.process_val function
   rdb_reader = astropy.io.ascii.get_reader(Reader=astropy.io.ascii.Rdb)
   rdb_reader.data.splitter.process_val = process_val

..
  EXAMPLE END

.. _chunk_reading:

Reading Large Tables in Chunks
==============================

The default process for reading ASCII tables is not memory efficient and may
temporarily require much more memory than the size of the file (up to a factor
of 5 to 10). In cases where the temporary memory requirement exceeds available
memory this can cause significant slowdown when disk cache gets used.

In this situation, there is a way to read the table in smaller chunks which are
limited in size. There are two possible ways to do this:

- Read the table in chunks and aggregate the final table along the way. This
  uses only somewhat more memory than the final table requires.
- Use a Python generator function to return a `~astropy.table.Table` object for
  each chunk of the input table. This allows for scanning through arbitrarily
  large tables since it never returns the final aggregate table.

The chunk reading functionality is most useful for very large tables, so this is
available only for the :ref:`fast_ascii_io` readers. The following formats are
supported: ``tab``, ``csv``, ``no_header``, ``rdb``, and ``basic``. The
``commented_header`` format is not directly supported, but as a workaround one
can read using the ``no_header`` format and explicitly supply the column names
using the ``names`` argument.

In order to read a table in chunks you must provide the ``fast_reader`` keyword
argument with a ``dict`` that includes the ``chunk_size`` key with the value
being the approximate size (in bytes) of each chunk of the input table to read.
In addition, if you provide a ``chunk_generator`` key which is set to
``True``, then instead of returning a single table for the whole input it
returns an iterator that provides a table for each chunk of the input.

Examples
--------

..
  EXAMPLE START
  Reading Large Tables in Chunks with astropy.io.ascii

To read an entire table while limiting peak memory usage:
::

  # Read a large CSV table in 100 Mb chunks.

  tbl = ascii.read('large_table.csv', format='csv', guess=False,
                   fast_reader={'chunk_size': 100 * 1000000})

To read the table in chunks with an iterator, we iterate over a CSV table and
select all rows where the ``Vmag`` column is less than 8.0 (e.g., all stars in
table brighter than 8.0 mag). We collect all of these subtables and then stack
them at the end.
::

  from astropy.table import vstack

  # tbls is an iterator over the chunks (no actual reading done yet)
  tbls = ascii.read('large_table.csv', format='csv', guess=False,
                    fast_reader={'chunk_size': 100 * 1000000,
                                 'chunk_generator': True})

  out_tbls = []

  # At this point the file is actually read in chunks.
  for tbl in tbls:
      bright = tbl['Vmag'] < 8.0
      if np.count_nonzero(bright):
          out_tbls.append(tbl[bright])

  out_tbl = vstack(out_tbls)

.. Note:: **Performance**

  Specifying the ``format`` explicitly and using ``guess=False`` is a good idea
  for large tables. This prevents unnecessary guessing in the typical case
  where the format is already known.

  The ``chunk_size`` should generally be set to the largest value that is
  reasonable given available system memory. There is overhead associated
  with processing each chunk, so the fewer chunks the better.

  ..
    EXAMPLE END
.. include:: references.txt

.. _io-ascii:

*********************************
ASCII Tables (`astropy.io.ascii`)
*********************************

Introduction
============

`astropy.io.ascii` provides methods for reading and writing a wide range of
ASCII data table formats via built-in :ref:`extension_reader_classes`. The
emphasis is on flexibility and convenience of use, although readers can
optionally use a less flexible C-based engine for reading and writing for
improved performance. This subpackage was originally developed as ``asciitable``.

The following shows a few of the ASCII formats that are available, while the
section on `Supported formats`_ contains the full list.

* :class:`~astropy.io.ascii.Basic`: basic table with customizable delimiters and header configurations
* :class:`~astropy.io.ascii.Cds`: `CDS format table <http://vizier.u-strasbg.fr/doc/catstd.htx>`_ (also Vizier)
* :class:`~astropy.io.ascii.Daophot`: table from the IRAF DAOphot package
* :class:`~astropy.io.ascii.Ecsv`: :ref:`ecsv_format` for lossless round-trip of data tables (**recommended**)
* :class:`~astropy.io.ascii.FixedWidth`: table with fixed-width columns (see also :ref:`fixed_width_gallery`)
* :class:`~astropy.io.ascii.Ipac`: `IPAC format table <https://irsa.ipac.caltech.edu/applications/DDGEN/Doc/ipac_tbl.html>`_
* :class:`~astropy.io.ascii.HTML`: HTML format table contained in a <table> tag
* :class:`~astropy.io.ascii.Latex`: LaTeX table with datavalue in the ``tabular`` environment
* :class:`~astropy.io.ascii.Mrt`: AAS `Machine-Readable Tables (MRT) <https://journals.aas.org/mrt-standards/>`_)
* :class:`~astropy.io.ascii.SExtractor`: `SExtractor format table <https://sextractor.readthedocs.io/en/latest/>`_

The strength of `astropy.io.ascii` is the support for astronomy-specific
formats (often with metadata) and specialized data types such as
:ref:`SkyCoord <astropy-coordinates-high-level>`, :ref:`Time
<astropy-time>`, and :ref:`Quantity <quantity>`. For reading or writing large
data tables in a generic format such as CSV, using the :ref:`Table - Pandas
interface <pandas>` is an option to consider.

.. note::

    It is also possible and encouraged to use the functionality from
    :mod:`astropy.io.ascii` through a higher level interface in the
    :ref:`Data Tables <astropy-table>` package. See :ref:`table_io` for more details.

Getting Started
===============

Reading Tables
--------------

The majority of commonly encountered ASCII tables can be read with the
|read| function. Assume you have a file named ``sources.dat`` with the
following contents::

  obsid redshift  X      Y     object
  3102  0.32      4167  4085   Q1250+568-A
  877   0.22      4378  3892   "Source 82"

This table can be read with the following::

  >>> from astropy.io import ascii
  >>> data = ascii.read("sources.dat")  # doctest: +SKIP
  >>> print(data)                       # doctest: +SKIP
  obsid redshift  X    Y      object
  ----- -------- ---- ---- -----------
   3102     0.32 4167 4085 Q1250+568-A
    877     0.22 4378 3892   Source 82

The first argument to the |read| function can be the name of a file, a string
representation of a table, or a list of table lines. The return value
(``data`` in this case) is a :ref:`Table <astropy-table>` object.

By default, |read| will try to :ref:`guess the table format <guess_formats>`
by trying all of the `supported formats`_.

.. Warning::

   Guessing the file format is often slow for large files because the reader
   tries parsing the file with every allowed format until one succeeds.
   For large files it is recommended to disable guessing with ``guess=False``.

If guessing the format does not work, as in the case for unusually formatted
tables, you may need to give `astropy.io.ascii` additional hints about
the format.

Writing Tables
--------------

The |write| function provides a way to write a data table as a formatted ASCII
table.  Most of the input table :ref:`supported_formats` for reading are also
available for writing. This provides a great deal of flexibility in the format
for writing.

..
  EXAMPLE START
  Writing Data Tables as Formatted ASCII Tables

The following shows how to write a formatted ASCII table using the |write|
function::

  >>> import numpy as np
  >>> from astropy.io import ascii
  >>> from astropy.table import Table
  >>> data = Table()
  >>> data['x'] = np.array([1, 2, 3], dtype=np.int32)
  >>> data['y'] = data['x'] ** 2
  >>> ascii.write(data, 'values.dat', overwrite=True)

The ``values.dat`` file will then contain::

  x y
  1 1
  2 4
  3 9

It is also possible and encouraged to use the write functionality from
:mod:`astropy.io.ascii` through a higher level interface in the :ref:`Data
Tables <astropy-table>` package (see :ref:`table_io` for more details). For
example::

  >>> data.write('values.dat', format='ascii', overwrite=True)

.. attention:: **ECSV is recommended**

   For a reproducible ASCII version of your table, we recommend using the
   :ref:`ecsv_format`. This stores all the table meta-data (in particular the
   column types and units) to a comment section at the beginning while
   maintaining compatibility with most plain CSV readers. It also allows storing
   richer data like `~astropy.coordinates.SkyCoord` or multidimensional or
   variable-length columns. ECSV is also supported in java by `STIL
   <http://www.star.bristol.ac.uk/~mbt/stil/>`_ and `TOPCAT
   <http://www.star.bris.ac.uk/~mbt/topcat/>`_,

To write our simple example table to ECSV we use::

  >>> data.write('values.ecsv', overwrite=True)  # doctest: +SKIP

The ``.ecsv`` extension is recognized and implies using ECSV (equivalent to
``format='ascii.ecsv'``). The ``values.ecsv`` file will then contain::

  # %ECSV 1.0
  # ---
  # datatype:
  # - {name: x, datatype: int32}
  # - {name: y, datatype: int32}
  # schema: astropy-2.0
  x y
  1 1
  2 4
  3 9

..
  EXAMPLE END

.. _supported_formats:

Supported Formats
=================

A full list of the supported ``format`` values and corresponding format types
for ASCII tables is given below. The ``Write`` column indicates which formats
support write functionality, and the ``Fast`` column indicates which formats
are compatible with the fast Cython/C engine for reading and writing.

========================= ===== ==== ============================================================================================
           Format         Write Fast                                          Description
========================= ===== ==== ============================================================================================
``aastex``                  Yes      :class:`~astropy.io.ascii.AASTex`: AASTeX deluxetable used for AAS journals
``basic``                   Yes  Yes :class:`~astropy.io.ascii.Basic`: Basic table with custom delimiters
``cds``                     Yes      :class:`~astropy.io.ascii.Cds`: CDS format table
``commented_header``        Yes  Yes :class:`~astropy.io.ascii.CommentedHeader`: Column names in a commented line
``csv``                     Yes  Yes :class:`~astropy.io.ascii.Csv`: Basic table with comma-separated values
``daophot``                          :class:`~astropy.io.ascii.Daophot`: IRAF DAOphot format table
``ecsv``                    Yes      :class:`~astropy.io.ascii.Ecsv`: Enhanced CSV format (**recommended**)
``fixed_width``             Yes      :class:`~astropy.io.ascii.FixedWidth`: Fixed width
``fixed_width_no_header``   Yes      :class:`~astropy.io.ascii.FixedWidthNoHeader`: Fixed-width with no header
``fixed_width_two_line``    Yes      :class:`~astropy.io.ascii.FixedWidthTwoLine`: Fixed-width with second header line
``html``                    Yes      :class:`~astropy.io.ascii.HTML`: HTML format table
``ipac``                    Yes      :class:`~astropy.io.ascii.Ipac`: IPAC format table
``latex``                   Yes      :class:`~astropy.io.ascii.Latex`: LaTeX table
``mrt``                     Yes      :class:`~astropy.io.ascii.Mrt`: AAS Machine-Readable Table format
``no_header``               Yes  Yes :class:`~astropy.io.ascii.NoHeader`: Basic table with no headers
``qdp``                     Yes      :class:`~astropy.io.ascii.QDP`: Quick and Dandy Plotter files
``rdb``                     Yes  Yes :class:`~astropy.io.ascii.Rdb`: Tab-separated with a type definition header line
``rst``                     Yes      :class:`~astropy.io.ascii.RST`: reStructuredText simple format table
``sextractor``                       :class:`~astropy.io.ascii.SExtractor`: SExtractor format table
``tab``                     Yes  Yes :class:`~astropy.io.ascii.Tab`: Basic table with tab-separated values
========================= ===== ==== ============================================================================================


Using `astropy.io.ascii`
========================

The details of using `astropy.io.ascii` are provided in the following sections:

Reading tables
---------------

.. toctree::
   :maxdepth: 2

   read

Writing tables
---------------

.. toctree::
   :maxdepth: 2

   write

ECSV Format
-----------

.. toctree::
   :maxdepth: 2

   ecsv

Fixed-Width Gallery
--------------------

.. toctree::
   :maxdepth: 2

   fixed_width_gallery

Fast ASCII Engine
-----------------

.. toctree::
   :maxdepth: 2

   fast_ascii_io

Base Class Elements
-------------------

.. toctree::
   :maxdepth: 2

   base_classes

Extension Reader Classes
------------------------

.. toctree::
   :maxdepth: 2

   extension_classes

.. note that if this section gets too long, it should be moved to a separate
   doc page - see the top of performance.inc.rst for the instructions on how to do
   that
.. include:: performance.inc.rst

Reference/API
=============

.. automodapi:: astropy.io.ascii
.. include:: references.txt

.. _base_class_elements:

Base Class Elements
*******************

The key elements in :mod:`astropy.io.ascii` are:

* :class:`~astropy.io.ascii.Column`: internal storage of column properties and data.
* :class:`Reader <astropy.io.ascii.BaseReader>`: base class to handle reading and writing tables.
* :class:`Inputter <astropy.io.ascii.BaseInputter>`: gets the lines from the table input.
* :class:`Splitter <astropy.io.ascii.BaseSplitter>`: splits the lines into string column values.
* :class:`Header <astropy.io.ascii.BaseHeader>`: initializes output columns based on the table header or user input.
* :class:`Data <astropy.io.ascii.BaseData>`: populates column data from the table.
* :class:`Outputter <astropy.io.ascii.BaseOutputter>`: converts column data to the specified output format (e.g., ``numpy`` structured array).

Each of these elements is an inheritable class with attributes that control the
corresponding functionality. In this way, the large number of tunable
parameters are modularized into manageable groups. In certain places these
attributes are actually functions for handling special cases.
.. include:: references.txt

.. _fast_ascii_io:

Fast ASCII I/O
**************

While :mod:`astropy.io.ascii` was designed with flexibility and extensibility
in mind, there is also a less flexible but significantly faster Cython/C engine
for reading and writing ASCII files. By default, |read| and |write| will
attempt to use this engine when dealing with compatible formats. The following
formats are currently compatible with the fast engine:

 * ``basic``
 * ``commented_header``
 * ``csv``
 * ``no_header``
 * ``rdb``
 * ``tab``

The fast engine can also be enabled through the format parameter by prefixing
a compatible format with "fast" and then an underscore. In this case, or
when enforcing the fast engine by either setting ``fast_reader='force'``
or explicitly setting any of the :ref:`fast_conversion_opts`, |read|
will not fall back on an ordinary reader if fast reading fails.

Examples
--------

..
  EXAMPLE START
  Read and Write a CSV File Using Fast ASCII

To open a CSV file and write it back out::

   >>> from astropy.table import Table
   >>> t = ascii.read('file.csv', format='fast_csv')  # doctest: +SKIP
   >>> t.write('output.csv', format='ascii.fast_csv')  # doctest: +SKIP

To disable the fast engine, specify ``fast_reader=False`` or
``fast_writer=False``. For example::

   >>> t = ascii.read('file.csv', format='csv', fast_reader=False) # doctest: +SKIP
   >>> t.write('file.csv', format='csv', fast_writer=False) # doctest: +SKIP

.. Note:: Guessing and Fast reading

   By default |read| will try to guess the format of the input data by
   successively trying different formats until one succeeds
   (see the section on :ref:`guess_formats`). For each supported
   format it will first try the fast, then the slow version of that
   reader. Without any additional options this means that both some pure
   Python readers with no fast implementation and the Python versions
   of some readers will be tried before getting to some of the fast
   readers. To bypass them entirely, a fast reader should be explicitly
   requested as above.

   **For optimum performance** however, it is recommended to turn off
   guessing entirely (``guess=False``) or narrow down the format options
   as much as possible by specifying the format (e.g., ``format='csv'``)
   and/or other options such as the delimiter.

..
  EXAMPLE END

Reading
=======

Since the fast engine is not part of the ordinary :mod:`astropy.io.ascii`
infrastructure, fast readers raise an error when passed certain
parameters which are not implemented in the fast reader infrastructure.
In this case |read| will fall back on the ordinary reader, unless the
fast reader has been explicitly requested (see above).
These parameters are:

 * Negative ``header_start`` (except for commented-header format)
 * Negative ``data_start``
 * ``data_start=None``
 * ``comment`` string not of length 1
 * ``delimiter`` string not of length 1
 * ``quotechar`` string not of length 1
 * ``converters``
 * ``Outputter``
 * ``Inputter``
 * ``data_Splitter``
 * ``header_Splitter``

.. _fast_conversion_opts:

Parallel and Fast Conversion Options
------------------------------------

In addition to ``True`` and ``False``, the parameter ``fast_reader`` can also
be a ``dict`` specifying any of three additional parameters, ``parallel``,
``use_fast_converter`` and ``exponent_style``.

Example
=======

..
  EXAMPLE START
  Parallel and Fast Conversion Options for Faster Table Reading

To specify additional parameters using ``fast_reader``::

   >>> ascii.read('data.txt', format='basic',
   ...            fast_reader={'parallel': True, 'use_fast_converter': True}) # doctest: +SKIP

..
  EXAMPLE END

These options allow for even faster table reading when enabled, but both are
disabled by default because they come with some caveats.

The ``parallel`` parameter can be used to enable multiprocessing via
the ``multiprocessing`` module, and can either be set to a number (the number
of processes to use) or ``True``, in which case the number of processes will be
``multiprocessing.cpu_count()``. Note that this can cause issues within the
IPython Notebook and so enabling multiprocessing in this context is discouraged.

Setting ``use_fast_converter`` to be ``True`` enables a faster but
slightly imprecise conversion method for floating-point values, as described
below.

The ``exponent_style`` parameter allows to define a different character
from the default ``'e'`` for exponential formats in the input file.
The special setting ``'fortran'`` enables auto-detection of any valid
exponent character under Fortran notation. For details see the section on
:ref:`fortran_style_exponents`.

Fast Converter
--------------

Input floating-point values should ideally be converted to the
nearest possible floating-point approximation; that is, the conversion
should be correct within half of the distance between the two closest
representable values, or 0.5 `ULP
<https://en.wikipedia.org/wiki/Unit_in_the_last_place>`__. The ordinary readers,
as well as the default fast reader, are guaranteed to convert floating-point
values within 0.5 ULP, but there is also a faster and less accurate
conversion method accessible via ``use_fast_converter``. If the input
data has less than about fifteen significant figures, or if accuracy is
relatively unimportant, this converter might be the best option in
performance-critical scenarios.

For values with a reasonably small number of
significant figures, the fast converter is guaranteed to produce an optimal
conversion (within 0.5 ULP). Once the number of significant figures exceeds
the precision of 64-bit floating-point values, the fast converter is no
longer guaranteed to be within 0.5 ULP, but about 60% of values end up
within 0.5 ULP and about 90% within 1.0 ULP.

Reading Large Tables
--------------------

For reading very large tables using the fast reader, see the section on
:ref:`chunk_reading`.

Writing
=======

The fast engine supports the same functionality as the ordinary writing engine
and is generally about two to four times faster than the ordinary engine.
The speed advantage of the faster engine is greatest for integer data and least
for floating-point data; the fast engine is around 3.6 times faster for a
sample file including a mixture of floating-point, integer, and text data.
Also note that stripping string values slows down the writing process, so
specifying ``strip_whitespace=False`` can improve performance.

Speed Gains
===========

The fast ASCII engine was designed based on the general parsing strategy
used in the `Pandas <https://pandas.pydata.org/>`__ data analysis library, so
its performance is generally comparable (although slightly slower by
default) to the Pandas ``read_csv`` method.

The ``genfromtxt`` and the ordinary :mod:`astropy.io.ascii` reader
are very similar in terms of speed, while ``read_csv`` is slightly faster
than the fast engine for integer and floating-point data; for pure
floating-point data, enabling the fast converter yields a speedup of about
50%. Also note that Pandas uses the exact same method as the fast
converter in Astropy when converting floating-point data.

The difference in performance between the fast engine and Pandas for
text data depends on the extent to which data values are repeated, as
Pandas is almost twice as fast as the fast engine when every value is
identical and the reverse is true when values are randomized. This is
because the fast engine uses fixed-size NumPy string arrays for
text data, while Pandas uses variable-size object arrays and uses an
underlying set to avoid copying repeated values.

Overall, the fast engine tends to be around four or five times faster than
the ordinary ASCII engine. If the input data is very large (generally
about 100,000 rows or greater), and particularly if the data does not
contain primarily integer data or repeated string values, specifying
``parallel`` as ``True`` can yield further performance gains. Although
IPython does not work well with ``multiprocessing``, there is a
`script <https://github.com/mdmueller/ascii-profiling/blob/master/parallel.py>`__
available for testing the performance of the fast engine in parallel,
and a sample result may be viewed `here
<http://mdmueller.github.io/ascii-profiling/>`__. This profile uses the
fast converter for both the serial and parallel Astropy
readers.

Another point worth noting is that the fast engine uses memory mapping
if a filename is supplied as input. If you want to avoid this for whatever
reason, supply an open file object instead. However, this will generally
be less efficient from both a time and a memory perspective, as the entire
file input will have to be read at once.
.. _ecsv_format:

ECSV Format
===========

The `Enhanced Character-Separated Values (ECSV) format
<https://github.com/astropy/astropy-APEs/blob/main/APE6.rst>`_ can be used to
write ``astropy`` `~astropy.table.Table` or `~astropy.table.QTable` datasets to
a text-only human readable data file and then read the table back without loss
of information. The format stores column specifications like unit and data type
along with table metadata by using a YAML header data structure. The
actual tabular data are stored in a standard character separated values (CSV)
format, giving compatibility with a wide variety of non-specialized CSV table
readers.

.. attention::

    The ECSV format is the recommended way to store Table data in a
    human-readable ASCII file. This includes use cases from informal
    use in science research to production pipelines and data systems.

    In addition to Python, ECSV is supported in `TOPCAT
    <http://www.star.bris.ac.uk/~mbt/topcat/>`_ and in the java `STIL
    <http://www.star.bris.ac.uk/~mbt/topcat/sun253/inEcsv.html>`_ library. .

Usage
-----

When writing in the ECSV format there are only two choices for the delimiter,
either space or comma, with space being the default. Any other value of
``delimiter`` will give an error. For reading the delimiter is specified within
the file itself.

Apart from the delimiter, the only other applicable read/write arguments are
``names``, ``include_names``, and ``exclude_names``. All other arguments will be
either ignored or raise an error.

Simple Table
------------
..
  EXAMPLE START
  Writing Data Tables as ECSV: Simple Table

The following writes a table as a simple space-delimited file. The
ECSV format is auto-selected due to ``.ecsv`` suffix::

  >>> import numpy as np
  >>> from astropy.table import Table
  >>> data = Table()
  >>> data['a'] = np.array([1, 2], dtype=np.int8)
  >>> data['b'] = np.array([1, 2], dtype=np.float32)
  >>> data['c'] = np.array(['hello', 'world'])
  >>> data.write('my_data.ecsv')  # doctest: +SKIP

The contents of ``my_data.ecsv`` are shown below::

  # %ECSV 1.0
  # ---
  # datatype:
  # - {name: a, datatype: int8}
  # - {name: b, datatype: float32}
  # - {name: c, datatype: string}
  # schema: astropy-2.0
  a b c
  1 1.0 hello
  2 2.0 world

The ECSV header is the section prefixed by the ``#`` comment character. An ECSV
file must start with the ``%ECSV <version>`` line. The ``datatype`` element
defines the list of columns and the ``schema`` relates to astropy-specific
extensions that are used for writing `Mixin Columns`_.

..
  EXAMPLE END

Masked Data
-----------

You can write masked (or "missing") data in the ECSV format in two different
ways, either using an empty string to represent missing values or by splitting
the masked columns into separate data and mask columns.

Empty String
""""""""""""

The first (default) way uses an empty string as a marker in place of
masked values. This is a bit more common outside of ``astropy`` and does not
require any astropy-specific extensions.

  >>> from astropy.table import MaskedColumn
  >>> t = Table()
  >>> t['x'] = MaskedColumn([1.0, 2.0, 3.0], unit='m', dtype='float32')
  >>> t['x'][1] = np.ma.masked
  >>> t['y'] = MaskedColumn([False, True, False], dtype='bool')
  >>> t['y'][0] = np.ma.masked

  >>> t.write('my_data.ecsv', format='ascii.ecsv', overwrite=True)  # doctest: +SKIP

The contents of ``my_data.ecsv`` are shown below::

  # %ECSV 1.0
  # ---
  # datatype:
  # - {name: x, unit: m, datatype: float32}
  # - {name: y, datatype: bool}
  # schema: astropy-2.0
  x y
  1.0 ""
  "" True
  3.0 False

To read this back, you would run the following::

  >>> Table.read('my_data.ecsv')  # doctest: +SKIP
  <Table length=3>
     x      y
     m
  float32  bool
  ------- -----
      1.0    --
       --  True
      3.0 False

Data + Mask
"""""""""""

The second way is to tell the writer to break any masked column into a data
column and a mask column by supplying the ``serialize_method='data_mask'``
argument::

  >>> t.write('my_data.ecsv', serialize_method='data_mask', overwrite=True)  # doctest: +SKIP

There are two main reasons you might want to do this:

- Storing the data "under the mask" instead of replacing it with an empty string.
- Writing a string column that contains empty strings which are not masked.

The contents of ``my_data.ecsv`` are shown below. First notice that there are
two new columns ``x.mask`` and ``y.mask`` that have been added, and these explicitly
record the mask values for those columns. Next notice now that the ECSV
header is a bit more complex and includes the astropy-specific extensions that
tell the reader how to interpret the plain CSV columns ``x, x.mask, y, y.mask``
and reassemble them back into the appropriate masked columns.
::

  # %ECSV 1.0
  # ---
  # datatype:
  # - {name: x, unit: m, datatype: float32}
  # - {name: x.mask, datatype: bool}
  # - {name: y, datatype: bool}
  # - {name: y.mask, datatype: bool}
  # meta: !!omap
  # - __serialized_columns__:
  #     x:
  #       __class__: astropy.table.column.MaskedColumn
  #       data: !astropy.table.SerializedColumn {name: x}
  #       mask: !astropy.table.SerializedColumn {name: x.mask}
  #     y:
  #       __class__: astropy.table.column.MaskedColumn
  #       data: !astropy.table.SerializedColumn {name: y}
  #       mask: !astropy.table.SerializedColumn {name: y.mask}
  # schema: astropy-2.0
  x x.mask y y.mask
  1.0 False False True
  2.0 True True False
  3.0 False False False

.. note::

   For the security minded, the ``__class__`` value must within an allowed list
   of astropy classes that are trusted by the reader. You cannot use an
   arbitrary class here.

..
  EXAMPLE START
  Using ECSV Format to Write Astropy Tables with Masked or Missing Data

Per-column control
@@@@@@@@@@@@@@@@@@

In rare cases it may be necessary to specify the serialization method for each
column individually. This is shown in the example below::

  >>> from astropy.table.table_helpers import simple_table
  >>> t = simple_table(masked=True)
  >>> t['c'][0] = ""  # Valid empty string in data
  >>> t
  <Table masked=True length=3>
    a      b     c
  int64 float64 str1
  ----- ------- ----
     --     1.0
      2     2.0   --
      3      --    e

Now we tell ECSV writer to output separate data and mask columns for the
string column ``'c'``:

.. doctest-skip::

  >>> t['c'].info.serialize_method['ecsv'] = 'data_mask'
  >>> ascii.write(t, format='ecsv')
  # %ECSV 1.0
  # ---
  # datatype:
  # - {name: a, datatype: int64}
  # - {name: b, datatype: float64}
  # - {name: c, datatype: string}
  # - {name: c.mask, datatype: bool}
  # meta: !!omap
  # - __serialized_columns__:
  #     c:
  #       __class__: astropy.table.column.MaskedColumn
  #       data: !astropy.table.SerializedColumn {name: c}
  #       mask: !astropy.table.SerializedColumn {name: c.mask}
  # schema: astropy-2.0
  a b c c.mask
  "" 1.0 "" False
  2 2.0 d True
  3 "" e False

When you read this back in, both the empty (zero-length) string and the masked
``'d'`` value in the column ``'c'`` will be preserved.

..
  EXAMPLE END

.. _ecsv_format_mixin_columns:

Mixin Columns
-------------

It is possible to store not only standard `~astropy.table.Column` and
`~astropy.table.MaskedColumn` objects to ECSV but also the following
:ref:`mixin_columns`:

- `astropy.time.Time`
- `astropy.time.TimeDelta`
- `astropy.units.Quantity`
- `astropy.coordinates.Latitude`
- `astropy.coordinates.Longitude`
- `astropy.coordinates.Angle`
- `astropy.coordinates.Distance`
- `astropy.coordinates.EarthLocation`
- `astropy.coordinates.SkyCoord`
- `astropy.table.NdarrayMixin`
- Coordinate representation types such as `astropy.coordinates.SphericalRepresentation`

In general, a mixin column may contain multiple data components as well as
object attributes beyond the standard `~astropy.table.Column` attributes like
``format`` or ``description``. Storing such mixin columns is done by replacing
the mixin column with column(s) representing the underlying data component(s)
and then inserting metadata which informs the reader of how to reconstruct the
original column. For example, a `~astropy.coordinates.SkyCoord` mixin column in
``'spherical'`` representation would have data attributes ``ra``, ``dec``,
``distance``, along with object attributes like ``representation_type`` or
``frame``.

..
  EXAMPLE START
  Writing a Table with a SkyCoord Column in ECSV Format

This example demonstrates writing a `~astropy.table.QTable` that has `~astropy.time.Time`
and `~astropy.coordinates.SkyCoord` mixin columns::

  >>> from astropy.coordinates import SkyCoord
  >>> import astropy.units as u
  >>> from astropy.table import QTable

  >>> sc = SkyCoord(ra=[1, 2] * u.deg, dec=[3, 4] * u.deg)
  >>> sc.info.description = 'flying circus'
  >>> q = [1, 2] * u.m
  >>> q.info.format = '.2f'
  >>> t = QTable()
  >>> t['c'] = [1, 2]
  >>> t['q'] = q
  >>> t['sc'] = sc

  >>> t.write('my_data.ecsv')  # doctest: +SKIP

The contents of ``my_data.ecsv`` are below::

  # %ECSV 1.0
  # ---
  # datatype:
  # - {name: c, datatype: int64}
  # - {name: q, unit: m, datatype: float64, format: .2f}
  # - {name: sc.ra, unit: deg, datatype: float64}
  # - {name: sc.dec, unit: deg, datatype: float64}
  # meta: !!omap
  # - __serialized_columns__:
  #     q:
  #       __class__: astropy.units.quantity.Quantity
  #       __info__: {format: .2f}
  #       unit: !astropy.units.Unit {unit: m}
  #       value: !astropy.table.SerializedColumn {name: q}
  #     sc:
  #       __class__: astropy.coordinates.sky_coordinate.SkyCoord
  #       __info__: {description: flying circus}
  #       dec: !astropy.table.SerializedColumn
  #         __class__: astropy.coordinates.angles.Latitude
  #         unit: &id001 !astropy.units.Unit {unit: deg}
  #         value: !astropy.table.SerializedColumn {name: sc.dec}
  #       frame: icrs
  #       ra: !astropy.table.SerializedColumn
  #         __class__: astropy.coordinates.angles.Longitude
  #         unit: *id001
  #         value: !astropy.table.SerializedColumn {name: sc.ra}
  #         wrap_angle: !astropy.coordinates.Angle
  #           unit: *id001
  #           value: 360.0
  #       representation_type: spherical
  # schema: astropy-2.0
  c q sc.ra sc.dec
  1 1.0 1.0 3.0
  2 2.0 2.0 4.0

The ``'__class__'`` keyword gives the fully-qualified class name and must be
one of the specifically allowed ``astropy`` classes. There is no option to add
user-specified allowed classes. The ``'__info__'`` keyword contains values for
standard `~astropy.table.Column` attributes like ``description`` or ``format``,
for any mixin columns that are represented by more than one serialized column.

..
  EXAMPLE END

.. _ecsv_format_masked_columns:

Multidimensional Columns
------------------------

Using ECSV it is possible to write a table that contains multidimensional
columns (both masked and unmasked). This is done by encoding each element as a
string using JSON. This functionality works for all column types that are
supported by ECSV including :ref:`mixin_columns`. This capability is added in
astropy 4.3 and ECSV version 1.0.

..
  EXAMPLE START
  Using ECSV Format to Write Astropy Tables with Multidimensional Columns

We start by defining a table with 2 rows where each element in the second column
``'b'`` is itself a 3x2 array::

  >>> t = Table()
  >>> t['a'] = ['x', 'y']
  >>> t['b'] = np.arange(12, dtype=np.float64).reshape(2, 3, 2)
  >>> t
  <Table length=2>
   a     b [3,2]
  str1   float64
  ---- -----------
     x  0.0 .. 5.0
     y 6.0 .. 11.0

  >>> t['b'][0]
  array([[0., 1.],
        [2., 3.],
        [4., 5.]])

Now we can write this to ECSV and observe how the N-d column ``'b'`` has been
written as a string with ``datatype: string``. Notice also that the column
descriptor for the column includes the new ``subtype: float64[3,2]`` attribute
specifying the type and shape of each item.

.. doctest-skip::

  >>> ascii.write(t, format='ecsv')  # doctest: +SKIP
  # %ECSV 1.0
  # ---
  # datatype:
  # - {name: a, datatype: string}
  # - {name: b, datatype: string, subtype: 'float64[3,2]'}
  # schema: astropy-2.0
  a b
  x [[0.0,1.0],[2.0,3.0],[4.0,5.0]]
  y [[6.0,7.0],[8.0,9.0],[10.0,11.0]]

When you read this back in, the sequence of JSON-encoded column items are then
decoded using JSON back into the original N-d column.

..
  EXAMPLE END

Variable-length arrays
----------------------

ECSV supports storing multidimensional columns is when the length of each array
element may vary. This data structure is supported in the `FITS standard
<https://fits.gsfc.nasa.gov/fits_standard.html>`_. While ``numpy`` does not
natively support variable-length arrays, it is possible to represent such a
structure using an object-type array of typed ``np.ndarray`` objects. This is how
the ``astropy`` FITS reader outputs a variable-length array.

This capability is added in astropy 4.3 and ECSV version 1.0.

Most commonly variable-length arrays have a 1-d array in each cell of the
column. You might a column with 1-d ``np.ndarray`` cells having lengths of 2, 5,
and 3 respectively.

The ECSV standard and ``astropy`` also supports arbitrary N-d arrays in each
cell, where all dimensions except the last one must match. For instance you
could have a column with ``np.ndarray`` cells having shapes of ``(4,4,2)``,
``(4,4,5)``, and ``(4,4,3)`` respectively.

..
  EXAMPLE START
  Using ECSV Format to Write Astropy Tables with Variable-Length Arrays

The example below shows writing a variable-length 1-d array to ECSV. Notice the
new ECSV column attribute ``subtype: 'int64[null]'``. The ``[null]`` indicates a
variable length for the one dimension. If we had been writing the N-d example
above the subtype would have been ``int64[4,4,null]``.

.. doctest-skip::

  >>> t = Table()
  >>> t['a'] = np.empty(3, dtype=object)
  >>> t['a'] = [np.array([1, 2], dtype=np.int64),
  ...           np.array([3, 4, 5], dtype=np.int64),
  ...           np.array([6, 7, 8, 9], dtype=np.int64)]
  >>> ascii.write(t, format='ecsv')
  # %ECSV 1.0
  # ---
  # datatype:
  # - {name: a, datatype: string, subtype: 'int64[null]'}
  # schema: astropy-2.0
  a
  [1,2]
  [3,4,5]
  [6,7,8,9]

..
  EXAMPLE END

Object arrays
-------------

ECSV can store object-type columns with simple Python objects consisting of
``dict``, ``list``, ``str``, ``int``, ``float``, ``bool`` and ``None`` elements.
More precisely, any object that can be serialized to `JSON
<https://www.json.org/>`__ using the standard library `json
<https://docs.python.org/3/library/json.html>`__ package is supported.

..
  EXAMPLE START
  Using ECSV Format to Write Astropy Tables with Object Arrays

The example below shows writing an object array to ECSV. Because JSON requires
a double-quote around strings, and because ECSV requires ``""`` to represent
a double-quote within a string, one tends to get double-double quotes in this
representation.

.. doctest-skip::

  >>> t = Table()
  >>> t['a'] = np.array([{'a': 1},
  ...                    {'b': [2.5, None]},
  ...                    True], dtype=object)
  >>> ascii.write(t, format='ecsv')
  # %ECSV 1.0
  # ---
  # datatype:
  # - {name: a, datatype: string, subtype: json}
  # schema: astropy-2.0
  a
  "{""a"":1}"
  "{""b"":[2.5,null]}"
  true

..
  EXAMPLE END
.. include:: references.txt

.. _extension_reader_classes:

Extension Reader Classes
************************

The following classes extend the base :class:`~astropy.io.ascii.BaseReader` functionality to handle reading and writing
different table formats. Some, such as the :class:`~astropy.io.ascii.Basic` Reader class
are fairly general and include a number of configurable attributes. Others
such as :class:`~astropy.io.ascii.Cds` or :class:`~astropy.io.ascii.Daophot` are specialized to read certain
well-defined but idiosyncratic formats.

* :class:`~astropy.io.ascii.AASTex`: AASTeX `deluxetable <https://fits.gsfc.nasa.gov/standard30/deluxetable.sty>`_ used for AAS journals.
* :class:`~astropy.io.ascii.Basic`: basic table with customizable delimiters and header configurations.
* :class:`~astropy.io.ascii.Cds`: `CDS format table <http://vizier.u-strasbg.fr/doc/catstd.htx>`_ (also Vizier and ApJ machine readable tables).
* :class:`~astropy.io.ascii.CommentedHeader`: column names given in a line that begins with the comment character.
* :class:`~astropy.io.ascii.Csv`: comma-separated values.
* :class:`~astropy.io.ascii.Daophot`: table from the IRAF DAOphot package.
* :class:`~astropy.io.ascii.FixedWidth`: table with fixed-width columns (see also :ref:`fixed_width_gallery`).
* :class:`~astropy.io.ascii.FixedWidthNoHeader`: table with fixed-width columns and no header.
* :class:`~astropy.io.ascii.FixedWidthTwoLine`: table with fixed-width columns and a two-line header.
* :class:`~astropy.io.ascii.HTML`: HTML format table contained in a <table> tag.
* :class:`~astropy.io.ascii.Ipac`: `IPAC format table <https://irsa.ipac.caltech.edu/applications/DDGEN/Doc/ipac_tbl.html>`_.
* :class:`~astropy.io.ascii.Latex`: LaTeX table with datavalue in the ``tabular`` environment.
* :class:`~astropy.io.ascii.Mrt`: `AAS Machine-Readable Table format <https://journals.aas.org/mrt-standards/>`_.
* :class:`~astropy.io.ascii.NoHeader`: basic table with no header where columns are auto-named.
* :class:`~astropy.io.ascii.Rdb`: tab-separated values with an extra line after the column definition line.
* :class:`~astropy.io.ascii.RST`: `reStructuredText simple format table <https://docutils.sourceforge.io/docs/ref/rst/restructuredtext.html#simple-tables>`_.
* :class:`~astropy.io.ascii.SExtractor`: `SExtractor format table <https://sextractor.readthedocs.io/en/latest/>`_.
* :class:`~astropy.io.ascii.Tab`: tab-separated values.
.. include:: references.txt

.. _fixed_width_gallery:

Fixed-Width Gallery
*******************

Fixed-width tables are those where each column has the same width for every row
in the table. This is commonly used to make tables easy to read for humans or
Fortran codes. It also reduces issues with quoting and special characters,
for example::

  Col1   Col2    Col3 Col4
  ---- --------- ---- ----
   1.2   "hello"    1    a
   2.4 's worlds    2    2

There are a number of common variations in the formatting of fixed-width tables
which :mod:`astropy.io.ascii` can read and write. The most significant
difference is whether there is no header line (:class:`~astropy.io.ascii.FixedWidthNoHeader`), one
header line (:class:`~astropy.io.ascii.FixedWidth`), or two header lines
(:class:`~astropy.io.ascii.FixedWidthTwoLine`). Next, there are variations in
the delimiter character, like whether the delimiter appears on either end
("bookends"), or if there is padding around the delimiter.

Details are available in the class API documentation, but the easiest way to
understand all of the options and their interactions is by example.

Reading
=======

..
  EXAMPLE START
  Reading Fixed-Width Tables

FixedWidth
----------

**Nice, typical, fixed-format table:**
::

  >>> from astropy.io import ascii
  >>> table = """
  ... # comment (with blank line above)
  ... |  Col1  |  Col2   |
  ... |  1.2   | "hello" |
  ... |  2.4   |'s worlds|
  ... """
  >>> ascii.read(table, format='fixed_width')
  <Table length=2>
    Col1     Col2
  float64    str9
  ------- ---------
      1.2   "hello"
      2.4 's worlds

**Typical fixed-format table with col names provided:**
::

  >>> table = """
  ... # comment (with blank line above)
  ... |  Col1  |  Col2   |
  ... |  1.2   | "hello" |
  ... |  2.4   |'s worlds|
  ... """
  >>> ascii.read(table, format='fixed_width', names=['name1', 'name2'])
  <Table length=2>
   name1    name2
  float64    str9
  ------- ---------
      1.2   "hello"
      2.4 's worlds

**Weird input table with data values chopped by col extent:**
::

  >>> table = """
  ...   Col1  |  Col2 |
  ...   1.2       "hello"
  ...   2.4   sdf's worlds
  ... """
  >>> ascii.read(table, format='fixed_width')
  <Table length=2>
    Col1    Col2
  float64   str7
  ------- -------
      1.2    "hel
      2.4 df's wo

**Table with double delimiters:**
::

  >>> table = """
  ... || Name ||   Phone ||         TCP||
  ... |  John  | 555-1234 |192.168.1.10X|
  ... |  Mary  | 555-2134 |192.168.1.12X|
  ... |   Bob  | 555-4527 | 192.168.1.9X|
  ... """
  >>> ascii.read(table, format='fixed_width')
  <Table length=3>
  Name  Phone       TCP
  str4   str8      str12
  ---- -------- ------------
  John 555-1234 192.168.1.10
  Mary 555-2134 192.168.1.12
   Bob 555-4527  192.168.1.9

**Table with space delimiter:**
::

  >>> table = """
  ...  Name  --Phone-    ----TCP-----
  ...  John  555-1234    192.168.1.10
  ...  Mary  555-2134    192.168.1.12
  ...   Bob  555-4527     192.168.1.9
  ... """
  >>> ascii.read(table, format='fixed_width', delimiter=' ')
  <Table length=3>
  Name --Phone- ----TCP-----
  str4   str8      str12
  ---- -------- ------------
  John 555-1234 192.168.1.10
  Mary 555-2134 192.168.1.12
   Bob 555-4527  192.168.1.9

**Table with no header row and auto-column naming:**

Use ``header_start`` and ``data_start`` keywords to indicate no header line.
::

  >>> table = """
  ... |  John  | 555-1234 |192.168.1.10|
  ... |  Mary  | 555-2134 |192.168.1.12|
  ... |   Bob  | 555-4527 | 192.168.1.9|
  ... """
  >>> ascii.read(table, format='fixed_width',
  ...            header_start=None, data_start=0)
  <Table length=3>
  col1   col2       col3
  str4   str8      str12
  ---- -------- ------------
  John 555-1234 192.168.1.10
  Mary 555-2134 192.168.1.12
   Bob 555-4527  192.168.1.9

**Table with no header row and with col names provided:**

Second and third rows also have hanging spaces after final "|". Use
header_start and data_start keywords to indicate no header line.
::

  >>> table = ["|  John  | 555-1234 |192.168.1.10|",
  ...          "|  Mary  | 555-2134 |192.168.1.12|  ",
  ...          "|   Bob  | 555-4527 | 192.168.1.9|  "]
  >>> ascii.read(table, format='fixed_width',
  ...            header_start=None, data_start=0,
  ...            names=('Name', 'Phone', 'TCP'))
  <Table length=3>
  Name  Phone       TCP
  str4   str8      str12
  ---- -------- ------------
  John 555-1234 192.168.1.10
  Mary 555-2134 192.168.1.12
   Bob 555-4527  192.168.1.9


FixedWidthNoHeader
------------------

**Table with no header row and auto-column naming. Use the
``fixed_width_no_header`` format for convenience:**
::

  >>> table = """
  ... |  John  | 555-1234 |192.168.1.10|
  ... |  Mary  | 555-2134 |192.168.1.12|
  ... |   Bob  | 555-4527 | 192.168.1.9|
  ... """
  >>> ascii.read(table, format='fixed_width_no_header')
  <Table length=3>
  col1   col2       col3
  str4   str8      str12
  ---- -------- ------------
  John 555-1234 192.168.1.10
  Mary 555-2134 192.168.1.12
   Bob 555-4527  192.168.1.9

**Table with no delimiter with column start and end values specified:**

This uses the col_starts and col_ends keywords. Note that the
col_ends values are inclusive so a position range of zero to five
will select the first six characters.
::

  >>> table = """
  ... #    5   9     17  18      28    <== Column start / end indexes
  ... #    |   |       ||         |    <== Column separation positions
  ...   John   555- 1234 192.168.1.10
  ...   Mary   555- 2134 192.168.1.12
  ...    Bob   555- 4527  192.168.1.9
  ... """
  >>> ascii.read(table, format='fixed_width_no_header',
  ...                 names=('Name', 'Phone', 'TCP'),
  ...                 col_starts=(0, 9, 18),
  ...                 col_ends=(5, 17, 28),
  ...                 )
  <Table length=3>
  Name   Phone      TCP
  str4    str9     str10
  ---- --------- ----------
  John 555- 1234 192.168.1.
  Mary 555- 2134 192.168.1.
   Bob 555- 4527  192.168.1

**Table with no delimiter with only column start or end values specified:**

If only the col_starts keyword is given, it is assumed that each column
ends where the next column starts, and the final column ends at the same
position as the longest line of data.

Conversely, if only the col_ends keyword is given, it is assumed that the first
column starts at position zero and that each successive column starts
immediately after the previous one.

The two examples below read the same table and produce the same result.
::

  >>> table = """
  ... #1       9        19                <== Column start indexes
  ... #|       |         |                <== Column start positions
  ... #<------><--------><------------->  <== Inferred column positions
  ...   John   555- 1234 192.168.1.10
  ...   Mary   555- 2134 192.168.1.123
  ...    Bob   555- 4527  192.168.1.9
  ...    Bill  555-9875  192.255.255.255
  ... """
  >>> ascii.read(table,
  ...                 format='fixed_width_no_header',
  ...                 names=('Name', 'Phone', 'TCP'),
  ...                 col_starts=(1, 9, 19),
  ...                 )
  <Table length=4>
  Name   Phone         TCP
  str4    str9        str15
  ---- --------- ---------------
  John 555- 1234    192.168.1.10
  Mary 555- 2134   192.168.1.123
   Bob 555- 4527     192.168.1.9
  Bill  555-9875 192.255.255.255

  >>> ascii.read(table,
  ...                 format='fixed_width_no_header',
  ...                 names=('Name', 'Phone', 'TCP'),
  ...                 col_ends=(8, 18, 32),
  ...                 )
  <Table length=4>
  Name   Phone        TCP
  str4    str9       str14
  ---- --------- --------------
  John 555- 1234   192.168.1.10
  Mary 555- 2134  192.168.1.123
   Bob 555- 4527    192.168.1.9
  Bill  555-9875 192.255.255.25


FixedWidthTwoLine
-----------------

**Typical fixed-format table with two header lines with some cruft:**
::

  >>> table = """
  ...   Col1    Col2
  ...   ----  ---------
  ...    1.2xx"hello"
  ...   2.4   's worlds
  ... """
  >>> ascii.read(table, format='fixed_width_two_line')
  <Table length=2>
    Col1     Col2
  float64    str9
  ------- ---------
      1.2   "hello"
      2.4 's worlds

..
  EXAMPLE END

..
  EXAMPLE START
  Reading a reStructuredText Table

**reStructuredText table:**
::

  >>> table = """
  ... ======= ===========
  ...   Col1    Col2
  ... ======= ===========
  ...   1.2   "hello"
  ...   2.4   's worlds
  ... ======= ===========
  ... """
  >>> ascii.read(table, format='fixed_width_two_line',
  ...                 header_start=1, position_line=2, data_end=-1)
  <Table length=2>
    Col1     Col2
  float64    str9
  ------- ---------
      1.2   "hello"
      2.4 's worlds

..
  EXAMPLE END

**Text table designed for humans and test having position line before the header line:**
::

  >>> table = """
  ... +------+----------+
  ... | Col1 |   Col2   |
  ... +------|----------+
  ... |  1.2 | "hello"  |
  ... |  2.4 | 's worlds|
  ... +------+----------+
  ... """
  >>> ascii.read(table, format='fixed_width_two_line', delimiter='+',
  ...                 header_start=1, position_line=0, data_start=3, data_end=-1)
  <Table length=2>
    Col1     Col2
  float64    str9
  ------- ---------
      1.2   "hello"
      2.4 's worlds

Writing
=======

..
  EXAMPLE START
  Writing Fixed-Width Tables

FixedWidth
----------

**Define input values ``dat`` for all write examples:**
::

  >>> table = """
  ... | Col1 |  Col2     |  Col3 | Col4 |
  ... | 1.2  | "hello"   |  1    | a    |
  ... | 2.4  | 's worlds |  2    | 2    |
  ... """
  >>> dat = ascii.read(table, format='fixed_width')

**Write a table as a normal fixed-width table:**
::

  >>> ascii.write(dat, format='fixed_width')
  | Col1 |      Col2 | Col3 | Col4 |
  |  1.2 |   "hello" |    1 |    a |
  |  2.4 | 's worlds |    2 |    2 |

**Write a table as a fixed-width table with no padding:**
::

  >>> ascii.write(dat, format='fixed_width', delimiter_pad=None)
  |Col1|     Col2|Col3|Col4|
  | 1.2|  "hello"|   1|   a|
  | 2.4|'s worlds|   2|   2|

**Write a table as a fixed-width table with no bookend:**
::

  >>> ascii.write(dat, format='fixed_width', bookend=False)
  Col1 |      Col2 | Col3 | Col4
   1.2 |   "hello" |    1 |    a
   2.4 | 's worlds |    2 |    2

**Write a table as a fixed-width table with no delimiter:**
::

  >>> ascii.write(dat, format='fixed_width', bookend=False, delimiter=None)
  Col1       Col2  Col3  Col4
   1.2    "hello"     1     a
   2.4  's worlds     2     2

**Write a table as a fixed-width table with no delimiter and formatting:**
::

  >>> ascii.write(dat, format='fixed_width',
  ...                  formats={'Col1': '%-8.3f', 'Col2': '%-15s'})
  |     Col1 |            Col2 | Col3 | Col4 |
  | 1.200    | "hello"         |    1 |    a |
  | 2.400    | 's worlds       |    2 |    2 |

FixedWidthNoHeader
------------------

**Write a table as a normal fixed-width table:**
::

  >>> ascii.write(dat, format='fixed_width_no_header')
  | 1.2 |   "hello" | 1 | a |
  | 2.4 | 's worlds | 2 | 2 |

**Write a table as a fixed-width table with no padding:**
::

  >>> ascii.write(dat, format='fixed_width_no_header', delimiter_pad=None)
  |1.2|  "hello"|1|a|
  |2.4|'s worlds|2|2|

**Write a table as a fixed-width table with no bookend:**
::

  >>> ascii.write(dat, format='fixed_width_no_header', bookend=False)
  1.2 |   "hello" | 1 | a
  2.4 | 's worlds | 2 | 2

**Write a table as a fixed-width table with no delimiter:**
::

  >>> ascii.write(dat, format='fixed_width_no_header', bookend=False,
  ...                  delimiter=None)
  1.2    "hello"  1  a
  2.4  's worlds  2  2

FixedWidthTwoLine
-----------------

**Write a table as a normal fixed-width table:**
::

  >>> ascii.write(dat, format='fixed_width_two_line')
  Col1      Col2 Col3 Col4
  ---- --------- ---- ----
   1.2   "hello"    1    a
   2.4 's worlds    2    2

**Write a table as a fixed width table with space padding and '=' position_char:**
::

  >>> ascii.write(dat, format='fixed_width_two_line',
  ...                  delimiter_pad=' ', position_char='=')
  Col1        Col2   Col3   Col4
  ====   =========   ====   ====
   1.2     "hello"      1      a
   2.4   's worlds      2      2

**Write a table as a fixed-width table with no bookend:**
::

  >>> ascii.write(dat, format='fixed_width_two_line', bookend=True, delimiter='|')
  |Col1|     Col2|Col3|Col4|
  |----|---------|----|----|
  | 1.2|  "hello"|   1|   a|
  | 2.4|'s worlds|   2|   2|

..
  EXAMPLE END
.. note that if this is changed from the default approach of using an *include*
   (in index.rst) to a separate performance page, the header needs to be changed
   from === to ***, the filename extension needs to be changed from .inc.rst to
   .rst, and a link needs to be added in the subpackage toctree

.. _astropy-io-fits-performance:

Performance Tips
================

It is possible to set the data array for :class:`~astropy.io.fits.PrimaryHDU`
and :class:`~astropy.io.fits.ImageHDU` to a `dask <https://dask.org/>`_ array.
If this is written to disk, the dask array will be computed as it is being
written, which will avoid using excessive memory:

.. doctest-requires:: dask

    >>> import dask.array as da
    >>> array = da.random.random((1000, 1000))
    >>> from astropy.io import fits
    >>> hdu = fits.PrimaryHDU(data=array)
    >>> hdu.writeto('test_dask.fits')

.. TODO: determine whether the following is quantitatively true, and either
.. uncomment or remove.

.. Performance Tips
.. ================
..
.. By default, :func:`astropy.io.fits.open` will open files using memory-mapping,
.. which means that the data is not necessarily read into memory until it is
.. needed. While memory-efficient, if memory is not a concern for you, you may
.. find that you can get better performance by turning memory mapping off, which
.. forces the data to be read into memory immediately:
..
.. .. doctest-skip::
..
..     >>> fits.open('example.fits', memmap=False)
.. currentmodule:: astropy.io.fits

.. _astropy-io-fits:

**************************************
FITS File Handling (`astropy.io.fits`)
**************************************

Introduction
============

The :mod:`astropy.io.fits` package provides access to FITS files. FITS
(Flexible Image Transport System) is a portable file standard widely used in
the astronomy community to store images and tables. This subpackage was
originally developed as PyFITS.

.. _tutorial:

Getting Started
===============

This section provides a quick introduction of using :mod:`astropy.io.fits`. The
goal is to demonstrate the package's basic features without getting into too
much detail. If you are a first time user or have never used ``astropy`` or
PyFITS, this is where you should start. See also the :ref:`FAQ <io-fits-faq>`
for answers to common questions and issues.

.. note::

    If you want to read or write a single table in FITS format, the
    recommended method is via the high-level :ref:`table_io`. In particular
    see the :ref:`Unified I/O FITS <table_io_fits>` section.

Reading and Updating Existing FITS Files
----------------------------------------

Opening a FITS File
^^^^^^^^^^^^^^^^^^^

.. note::

    The ``astropy.io.fits.util.get_testdata_filepath()`` function,
    used in the examples here, is for accessing data shipped with ``astropy``.
    To work with your own data instead, please use :func:`astropy.io.fits.open`,
    which takes either the relative or absolute path.

Once the `astropy.io.fits` package is loaded using the standard convention
[#f1]_, we can open an existing FITS file::

    >>> from astropy.io import fits
    >>> fits_image_filename = fits.util.get_testdata_filepath('test0.fits')

    >>> hdul = fits.open(fits_image_filename)

The :func:`open` function has several optional arguments which will be
discussed in a later chapter. The default mode, as in the above example, is
"readonly". The open function returns an object called an :class:`HDUList`
which is a `list`-like collection of HDU objects. An HDU (Header Data Unit) is
the highest level component of the FITS file structure, consisting of a header
and (typically) a data array or table.

After the above open call, ``hdul[0]`` is the primary HDU, ``hdul[1]`` is
the first extension HDU, etc. (if there are any extensions), and so on. It
should be noted that ``astropy`` uses zero-based indexing when referring to
HDUs and header cards, though the FITS standard (which was designed with
Fortran in mind) uses one-based indexing.

The :class:`HDUList` has a useful method :meth:`HDUList.info`, which
summarizes the content of the opened FITS file:

    >>> hdul.info()
    Filename: ...test0.fits
    No.    Name      Ver    Type      Cards   Dimensions   Format
      0  PRIMARY       1 PrimaryHDU     138   ()
      1  SCI           1 ImageHDU        61   (40, 40)   int16
      2  SCI           2 ImageHDU        61   (40, 40)   int16
      3  SCI           3 ImageHDU        61   (40, 40)   int16
      4  SCI           4 ImageHDU        61   (40, 40)   int16

After you are done with the opened file, close it with the
:meth:`HDUList.close` method:

    >>> hdul.close()

You can avoid closing the file manually by using :func:`open` as context
manager::

    >>> with fits.open(fits_image_filename) as hdul:
    ...     hdul.info()
    Filename: ...test0.fits
    No.    Name      Ver    Type      Cards   Dimensions   Format
      0  PRIMARY       1 PrimaryHDU     138   ()
      1  SCI           1 ImageHDU        61   (40, 40)   int16
      2  SCI           2 ImageHDU        61   (40, 40)   int16
      3  SCI           3 ImageHDU        61   (40, 40)   int16
      4  SCI           4 ImageHDU        61   (40, 40)   int16

After exiting the ``with`` scope the file will be closed automatically. That is
(generally) the preferred way to open a file in Python, because it will close
the file even if an exception happens.

If the file is opened with ``lazy_load_hdus=False``, all of the headers will
still be accessible after the HDUList is closed. The headers and data may or
may not be accessible depending on whether the data are touched and if they
are memory-mapped; see later chapters for detail.

.. _fits-large-files:

Working with large files
""""""""""""""""""""""""

The :func:`open` function supports a ``memmap=True`` argument that allows the
array data of each HDU to be accessed with mmap, rather than being read into
memory all at once. This is particularly useful for working with very large
arrays that cannot fit entirely into physical memory. Here ``memmap=True`` by
default, and this value is obtained from the configuration item ``astropy.io.fits.Conf.use_memmap``.

This has minimal impact on smaller files as well, though some operations, such
as reading the array data sequentially, may incur some additional overhead. On
32-bit systems, arrays larger than 2 to 3 GB cannot be mmap'd (which is fine,
because by that point you are likely to run out of physical memory anyways), but
64-bit systems are much less limited in this respect.

.. warning::
    When opening a file with ``memmap=True``, because of how mmap works this
    means that when the HDU data is accessed (i.e., ``hdul[0].data``) another
    handle to the FITS file is opened by mmap. This means that even after
    calling ``hdul.close()`` the mmap still holds an open handle to the data so
    that it can still be accessed by unwary programs that were built with the
    assumption that the .data attribute has all of the data in-memory.

    In order to force the mmap to close, either wait for the containing
    ``HDUList`` object to go out of scope, or manually call
    ``del hdul[0].data``. (This works so long as there are no other references
    held to the data array.)

Unsigned integers
"""""""""""""""""

Due to the FITS format's Fortran origins, FITS does not natively support
unsigned integer data in images or tables. However, there is a common
convention to store unsigned integers as signed integers, along with a
*shift* instruction (a ``BZERO`` keyword with value ``2 ** (BITPIX - 1)``) to
shift up all signed integers to unsigned integers. For example, when writing
the value ``0`` as an unsigned 32-bit integer, it is stored in the FITS
file as ``-32768``, along with the header keyword ``BZERO = 32768``.

``astropy`` recognizes and applies this convention by default, so that all data
that looks like it should be interpreted as unsigned integers is automatically
converted (this applies to both images and tables).

Even with ``uint=False``, the ``BZERO`` shift is still applied, but the
returned array is of "float64" type. To disable scaling/shifting entirely, use
``do_not_scale_image_data=True`` (see :ref:`fits-scaled-data-faq` in the FAQ
for more details).

Working with compressed files
"""""""""""""""""""""""""""""

.. note::

    Files that use compressed HDUs within the FITS file are discussed
    in :ref:`Compressed Image Data <astropy-io-fits-compressedImageData>`.


The :func:`open` function will seamlessly open FITS files that have been
compressed with gzip, bzip2 or pkzip. Note that in this context we are talking
about a FITS file that has been compressed with one of these utilities (e.g., a
.fits.gz file).

There are some limitations when working with compressed files. For example,
with Zip files that contain multiple compressed files, only the first file will
be accessible. Also bzip2 does not support the append or update access modes.

When writing a file (e.g., with the :func:`writeto` function), compression will
be determined based on the filename extension given, or the compression used in
a pre-existing file that is being written to.


Working with non-standard files
"""""""""""""""""""""""""""""""
When `astropy.io.fits` reads a FITS file which does not conform to the FITS
standard it will try to make an educated interpretation of non-compliant fields.
This may not always succeed and may trigger warnings when accessing headers or
exceptions when writing to file. Verification of fields written to an output
file can be controlled with the ``output_verify`` parameter of :func:`open`.
Files opened for reading can be verified and fixed with method
``HDUList.verify``. This method is typically invoked after opening the file
but before accessing any headers or data::

    >>> with fits.open(fits_image_filename) as hdul:
    ...    hdul.verify('fix')
    ...    data = hdul[1].data

In the above example, the call to ``hdul.verify("fix")`` requests that `astropy.io.fits`
fix non-compliant fields and print informative messages. Other options in addition to ``"fix"``
are described under FITS :ref:`fits_io_verification`

.. seealso:: FITS :ref:`fits_io_verification`.

Working with FITS Headers
^^^^^^^^^^^^^^^^^^^^^^^^^

As mentioned earlier, each element of an :class:`HDUList` is an HDU object with
``.header`` and ``.data`` attributes, which can be used to access the header
and data portions of the HDU.

For those unfamiliar with FITS headers, they consist of a list of 80 byte
"cards", where a card contains a keyword, a value, and a comment. The keyword
and comment must both be strings, whereas the value can be a string or an
integer, floating point number, complex number, or ``True``/``False``. Keywords
are usually unique within a header, except in a few special cases.

The header attribute is a Header instance, another ``astropy`` object. To get
the value associated with a header keyword, do ( la Python dicts)::

    >>> hdul = fits.open(fits_image_filename)
    >>> hdul[0].header['DATE']
    '01/04/99'

to get the value of the keyword "DATE", which is a string '01/04/99'.

Although keyword names are always in upper case inside the FITS file,
specifying a keyword name with ``astropy`` is case-insensitive for the user's
convenience. If the specified keyword name does not exist, it will raise a
`KeyError` exception.

We can also get the keyword value by indexing ( la Python lists)::

    >>> hdul[0].header[7]
    32768.0

This example returns the eighth (like Python lists, it is 0-indexed) keyword's
value  a float  32768.0.

Similarly, it is possible to update a keyword's value in ``astropy``, either
through keyword name or index::

    >>> hdr = hdul[0].header
    >>> hdr['targname'] = 'NGC121-a'
    >>> hdr[27] = 99

Please note however that almost all application code should update header
values via their keyword name and not via their positional index. This is
because most FITS keywords may appear at any position in the header.

It is also possible to update both the value and comment associated with a
keyword by assigning them as a tuple::

    >>> hdr = hdul[0].header
    >>> hdr['targname'] = ('NGC121-a', 'the observation target')
    >>> hdr['targname']
    'NGC121-a'
    >>> hdr.comments['targname']
    'the observation target'

Like a dict, you may also use the above syntax to add a new keyword/value pair
(and optionally a comment as well). In this case the new card is appended to
the end of the header (unless it is a commentary keyword such as COMMENT or
HISTORY, in which case it is appended after the last card with that keyword).

Another way to either update an existing card or append a new one is to use the
:meth:`Header.set` method::

    >>> hdr.set('observer', 'Edwin Hubble')

Comment or history records are added like normal cards, though in their case a
new card is always created, rather than updating an existing HISTORY or COMMENT
card::

    >>> hdr['history'] = 'I updated this file 2/26/09'
    >>> hdr['comment'] = 'Edwin Hubble really knew his stuff'
    >>> hdr['comment'] = 'I like using HST observations'
    >>> hdr['history']
    I updated this file 2/26/09
    >>> hdr['comment']
    Edwin Hubble really knew his stuff
    I like using HST observations

Note: Be careful not to confuse COMMENT cards with the comment value for normal
cards.

To update existing COMMENT or HISTORY cards, reference them by index::

    >>> hdr['history'][0] = 'I updated this file on 2/27/09'
    >>> hdr['history']
    I updated this file on 2/27/09
    >>> hdr['comment'][1] = 'I like using JWST observations'
    >>> hdr['comment']
    Edwin Hubble really knew his stuff
    I like using JWST observations


To see the entire header as it appears in the FITS file (with the END card and
padding stripped), enter the header object by itself, or
``print(repr(hdr))``::

    >>> hdr  # doctest: +ELLIPSIS
    SIMPLE  =                    T / file does conform to FITS standard
    BITPIX  =                   16 / number of bits per data pixel
    NAXIS   =                    0 / number of data axes
    ...
    >>> print(repr(hdr))  # doctest: +ELLIPSIS
    SIMPLE  =                    T / file does conform to FITS standard
    BITPIX  =                   16 / number of bits per data pixel
    NAXIS   =                    0 / number of data axes
    ...

Entering only ``print(hdr)`` will also work, but may not be very legible
on most displays, as this displays the header as it is written in the FITS file
itself, which means there are no line breaks between cards. This is a common
source of confusion for new users.

It is also possible to view a slice of the header::

   >>> hdr[:2]
   SIMPLE  =                    T / file does conform to FITS standard
   BITPIX  =                   16 / number of bits per data pixel

Only the first two cards are shown above.

To get a list of all keywords, use the :meth:`Header.keys` method just as you
would with a dict::

    >>> list(hdr.keys())  # doctest: +ELLIPSIS
    ['SIMPLE', 'BITPIX', 'NAXIS', ...]

.. topic:: Examples:

    See also :ref:`sphx_glr_generated_examples_io_modify-fits-header.py`.

.. _structural_keywords:

Structural Keywords
"""""""""""""""""""

FITS keywords mix up both metadata and critical information about the file structure
that is needed to parse the file. These *structural* keywords are managed internally by
:mod:`astropy.io.fits` and, in general, should not be touched by the user. Instead one
should use  the related attributes of the `astropy.io.fits` classes (see examples below).

The specific set of structural keywords used by the FITS standard varies with HDU type.
The following table lists which keywords are associated with each HDU type:

.. csv-table:: Structural Keywords
   :header: "HDU Type", "Structural Keywords"
   :widths: 20, 20

   "All", "``SIMPLE``, ``BITPIX``, ``NAXIS``"
   ":class:`PrimaryHDU`", "``EXTEND``"
   ":class:`ImageHDU`, :class:`TableHDU`, :class:`BinTableHDU`",  "``PCOUNT``, ``GCOUNT``"
   ":class:`GroupsHDU`", "``NAXIS1``, ``GCOUNT``, ``PCOUNT``, ``GROUPS``"
   ":class:`TableHDU`, :class:`BinTableHDU`", "``TFIELDS``, ``TFORM``, ``TBCOL``"

There are many other reserved keywords, for instance for the data scaling, or for table's column
attributes, as described in the  `FITS Standard <https://fits.gsfc.nasa.gov/fits_standard.html>`__.
Most of these are accessible via attributes of the :class:`Column` or HDU objects, for instance
``hdu.name`` to set ``EXTNAME``, or ``hdu.ver`` for ``EXTVER``. Structural keywords are checked
and/or updated as a consequence of common operations. For example, when:

1. Setting the data. The ``NAXIS*`` keywords are set from the data shape (``.data.shape``), and ``BITPIX``
   from the data type (``.data.dtype``).
2. Setting the header. Its keywords are updated based on the data properties (as above).
3. Writing a file. All the necessary keywords are deleted, updated or added to the header.
4. Calling an HDU's verify method (e.g., :func:`PrimaryHDU.verify`). Some keywords can be fixed automatically.

In these cases any hand-written values users might assign to those keywords will be overwrittten.

Working with Image Data
^^^^^^^^^^^^^^^^^^^^^^^

If an HDU's data is an image, the data attribute of the HDU object will return
a ``numpy`` `~numpy.ndarray` object. Refer to the ``numpy`` documentation for
details on manipulating these numerical arrays::

    >>> data = hdul[1].data

Here, ``data`` points to the data object in the second HDU (the first HDU,
``hdul[0]``, being the primary HDU) which corresponds to the 'SCI'
extension. Alternatively, you can access the extension by its extension name
(specified in the EXTNAME keyword)::

    >>> data = hdul['SCI'].data

If there is more than one extension with the same EXTNAME, the EXTVER value
needs to be specified along with the EXTNAME as a tuple; for example::

    >>> data = hdul['sci',2].data

Note that the EXTNAME is also case-insensitive.

The returned ``numpy`` object has many attributes and methods for a user to get
information about the array, for example::

    >>> data.shape
    (40, 40)
    >>> data.dtype.name
    'int16'

Since image data is a ``numpy`` object, we can slice it, view it, and perform
mathematical operations on it. To see the pixel value at x=5, y=2::

    >>> print(data[1, 4])
    348

Note that, like C (and unlike Fortran), Python is 0-indexed and the indices
have the slowest axis first and fastest changing axis last; that is, for a 2D
image, the fast axis (X-axis) which corresponds to the FITS NAXIS1 keyword, is
the second index. Similarly, the 1-indexed subsection of x=11 to 20
(inclusive) and y=31 to 40 (inclusive) would be given in Python as::

    >>> data[30:40, 10:20]
    array([[350, 349, 349, 348, 349, 348, 349, 347, 350, 348],
           [348, 348, 348, 349, 348, 349, 347, 348, 348, 349],
           [348, 348, 347, 349, 348, 348, 349, 349, 349, 349],
           [349, 348, 349, 349, 350, 349, 349, 347, 348, 348],
           [348, 348, 348, 348, 349, 348, 350, 349, 348, 349],
           [348, 347, 349, 349, 350, 348, 349, 348, 349, 347],
           [347, 348, 347, 348, 349, 349, 350, 349, 348, 348],
           [349, 349, 350, 348, 350, 347, 349, 349, 349, 348],
           [349, 348, 348, 348, 348, 348, 349, 347, 349, 348],
           [349, 349, 349, 348, 350, 349, 349, 350, 348, 350]], dtype=int16)

To update the value of a pixel or a subsection::

    >>> data[30:40, 10:20] = data[1, 4] = 999

This example changes the values of both the pixel \[1, 4] and the subsection
\[30:40, 10:20] to the new value of 999. See the `Numpy documentation`_ for
more details on Python-style array indexing and slicing.

The next example of array manipulation is to convert the image data from counts
to flux::

    >>> photflam = hdul[1].header['photflam']
    >>> exptime = hdr['exptime']
    >>> data = data * photflam / exptime
    >>> hdul.close()

Note that performing an operation like this on an entire image requires holding
the entire image in memory. This example performs the multiplication in-place
so that no copies are made, but the original image must first be able to fit in
main memory. For most observations this should not be an issue on modern
personal computers.

If at this point you want to preserve all of the changes you made and write it
to a new file, you can use the :meth:`HDUList.writeto` method (see below).

.. _Numpy documentation: https://numpy.org/doc/stable/reference/arrays.indexing.html

.. topic:: Examples:

    See also :ref:`sphx_glr_generated_examples_io_plot_fits-image.py`.

Working with Table Data
^^^^^^^^^^^^^^^^^^^^^^^

This section describes reading and writing table data in the FITS format using
the `~astropy.io.fits` package directly. For some cases, however, the
high-level :ref:`table_io` will often suffice and is somewhat more convenient
to use. See the :ref:`Unified I/O FITS <table_io_fits>` section for details.

Like images, the data portion of a FITS table extension is in the ``.data``
attribute::

    >>> fits_table_filename = fits.util.get_testdata_filepath('tb.fits')
    >>> hdul = fits.open(fits_table_filename)
    >>> data = hdul[1].data # assuming the first extension is a table
    >>> hdul.close()

If you are familiar with ``numpy`` `~numpy.recarray` (record array) objects, you
will find the table data is basically a record array with some extra
properties. But familiarity with record arrays is not a prerequisite for this
guide.

To see the first row of the table::

    >>> print(data[0])
    (1, 'abc', 3.7000000715255736, False)

Each row in the table is a :class:`FITS_record` object which looks like a
(Python) tuple containing elements of heterogeneous data types. In this
example: an integer, a string, a floating point number, and a Boolean value. So
the table data are just an array of such records. More commonly, a user is
likely to access the data in a column-wise way. This is accomplished by using
the :meth:`~FITS_rec.field` method. To get the first column (or "field" in
NumPy parlance  it is used here interchangeably with "column") of the table,
use::

    >>> data.field(0)
    array([1, 2]...)

A ``numpy`` object with the data type of the specified field is returned.

Like header keywords, a column can be referred either by index, as above, or by
name::

    >>> data.field('c1')
    array([1, 2]...)

When accessing a column by name, dict-like access is also possible (and even
preferable)::

    >>> data['c1']
    array([1, 2]...)

In most cases it is preferable to access columns by their name, as the column
name is entirely independent of its physical order in the table. As with
header keywords, column names are case-insensitive.

But how do we know what columns we have in a table? First, we will introduce
another attribute of the table HDU: the :attr:`~BinTableHDU.columns`
attribute::

    >>> cols = hdul[1].columns

This attribute is a :class:`ColDefs` (column definitions) object. If we use the
:meth:`ColDefs.info` method from the interactive prompt::

    >>> cols.info()
    name:
        ['c1', 'c2', 'c3', 'c4']
    format:
        ['1J', '3A', '1E', '1L']
    unit:
        ['', '', '', '']
    null:
        [-2147483647, '', '', '']
    bscale:
        ['', '', 3, '']
    bzero:
        ['', '', 0.4, '']
    disp:
        ['I11', 'A3', 'G15.7', 'L6']
    start:
        ['', '', '', '']
    dim:
        ['', '', '', '']
    coord_type:
        ['', '', '', '']
    coord_unit:
        ['', '', '', '']
    coord_ref_point:
        ['', '', '', '']
    coord_ref_value:
        ['', '', '', '']
    coord_inc:
        ['', '', '', '']
    time_ref_pos:
        ['', '', '', '']

it will show the attributes of all columns in the table, such as their names,
formats, bscales, bzeros, etc. A similar output that will display the column
names and their formats can be printed from within a script with::

    >>> hdul[1].columns
    ColDefs(
        name = 'c1'; format = '1J'; null = -2147483647; disp = 'I11'
        name = 'c2'; format = '3A'; disp = 'A3'
        name = 'c3'; format = '1E'; bscale = 3; bzero = 0.4; disp = 'G15.7'
        name = 'c4'; format = '1L'; disp = 'L6'
    )

We can also get these properties individually; for example::

    >>> cols.names
    ['c1', 'c2', 'c3', 'c4']

returns a (Python) list of field names.

Since each field is a ``numpy`` object, we will have the entire arsenal of
``numpy`` tools to use. We can reassign (update) the values::

    >>> data['c4'][:] = 0

take the mean of a column::

    >>> data['c3'].mean()  # doctest: +FLOAT_CMP
    5.19999989271164

and so on.

.. topic:: Examples:

    See also :ref:`sphx_glr_generated_examples_io_fits-tables.py`.

Save File Changes
^^^^^^^^^^^^^^^^^

As mentioned earlier, after a user opened a file, made a few changes to either
header or data, the user can use :meth:`HDUList.writeto` to save the changes.
This takes the version of headers and data in memory and writes them to a new
FITS file on disk. Subsequent operations can be performed to the data in memory
and written out to yet another different file, all without recopying the
original data to (more) memory:

.. code:: python

    hdul.writeto('newtable.fits')

will write the current content of ``hdulist`` to a new disk file newfile.fits.
If a file was opened with the update mode, the :meth:`HDUList.flush` method can
also be used to write all of the changes made since :func:`open`, back to the
original file. The :meth:`~HDUList.close` method will do the same for a FITS
file opened with update mode:

.. code:: python

    with fits.open('original.fits', mode='update') as hdul:
        # Change something in hdul.
        hdul.flush()  # changes are written back to original.fits

    # closing the file will also flush any changes and prevent further writing


Creating a New FITS File
------------------------

Creating a New Image File
^^^^^^^^^^^^^^^^^^^^^^^^^

So far we have demonstrated how to read and update an existing FITS file. But
how about creating a new FITS file from scratch? Such tasks are very convenient
in ``astropy`` for an image HDU. We will first demonstrate how to create a FITS
file consisting of only the primary HDU with image data.

First, we create a ``numpy`` object for the data part::

    >>> import numpy as np
    >>> n = np.arange(100.0) # a simple sequence of floats from 0.0 to 99.9

Next, we create a :class:`PrimaryHDU` object to encapsulate the data::

    >>> hdu = fits.PrimaryHDU(n)

We then create an :class:`HDUList` to contain the newly created primary HDU, and write to
a new file::

    >>> hdul = fits.HDUList([hdu])
    >>> hdul.writeto('new1.fits')

That is it! In fact, ``astropy`` even provides a shortcut for the last two
lines to accomplish the same behavior::

    >>> hdu.writeto('new2.fits')

This will write a single HDU to a FITS file without having to manually
encapsulate it in an :class:`HDUList` object first.


Creating a New Table File
^^^^^^^^^^^^^^^^^^^^^^^^^

.. note::

    If you want to create a **binary** FITS table with no other HDUs,
    you can use :class:`~astropy.table.Table` instead and then write to FITS.
    This is less complicated than "lower-level" FITS interface::

    >>> from astropy.table import Table
    >>> t = Table([[1, 2], [4, 5], [7, 8]], names=('a', 'b', 'c'))
    >>> t.write('table1.fits', format='fits')

    The equivalent code using ``astropy.io.fits`` would look like this:

    >>> from astropy.io import fits
    >>> import numpy as np
    >>> c1 = fits.Column(name='a', array=np.array([1, 2]), format='K')
    >>> c2 = fits.Column(name='b', array=np.array([4, 5]), format='K')
    >>> c3 = fits.Column(name='c', array=np.array([7, 8]), format='K')
    >>> t = fits.BinTableHDU.from_columns([c1, c2, c3])
    >>> t.writeto('table2.fits')

To create a table HDU is a little more involved than an image HDU, because a
table's structure needs more information. First of all, tables can only be an
extension HDU, not a primary. There are two kinds of FITS table extensions:
ASCII and binary. We will use binary table examples here.

To create a table from scratch, we need to define columns first, by
constructing the :class:`Column` objects and their data. Suppose we have two
columns, the first containing strings, and the second containing floating point
numbers::

    >>> import numpy as np
    >>> a1 = np.array(['NGC1001', 'NGC1002', 'NGC1003'])
    >>> a2 = np.array([11.1, 12.3, 15.2])
    >>> col1 = fits.Column(name='target', format='20A', array=a1)
    >>> col2 = fits.Column(name='V_mag', format='E', array=a2)

.. note::

    It is not necessary to create a :class:`Column` object explicitly
    if the data is stored in a
    `structured array <https://numpy.org/doc/stable/user/basics.rec.html>`_.

Next, create a :class:`ColDefs` (column-definitions) object for all columns::

    >>> cols = fits.ColDefs([col1, col2])

Now, create a new binary table HDU object by using the
:func:`BinTableHDU.from_columns` function::

    >>> hdu = fits.BinTableHDU.from_columns(cols)

This function returns (in this case) a :class:`BinTableHDU`.

The data structure used to represent FITS tables is called a :class:`FITS_rec`
and is derived from the :class:`numpy.recarray` interface. When creating
a new table HDU the individual column arrays will be assembled into a single
:class:`FITS_rec` array.

You can create a :class:`BinTableHDU` more concisely without creating intermediate
variables for the individual columns and without manually creating a
:class:`ColDefs` object::

    >>> hdu = fits.BinTableHDU.from_columns(
    ...     [fits.Column(name='target', format='20A', array=a1),
    ...      fits.Column(name='V_mag', format='E', array=a2)])

Now you may write this new table HDU directly to a FITS file like so::

    >>> hdu.writeto('table3.fits')

This shortcut will automatically create a minimal primary HDU with no data and
prepend it to the table HDU to create a valid FITS file. If you require
additional data or header keywords in the primary HDU you may still create a
:class:`PrimaryHDU` object and build up the FITS file manually using an
:class:`HDUList`, as described in the next section.

Creating a File with Multiple Extensions
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

In the previous examples we created files with a single meaningful extension (a
:class:`PrimaryHDU` or :class:`BinTableHDU`). To create a file with multiple
extensions we need to create extension HDUs and append them to an :class:`HDUList`.

First, we create some data for Image extensions::

    >>> import numpy as np
    >>> n = np.ones((3, 3))
    >>> n2 = np.ones((100, 100))
    >>> n3 = np.ones((10, 10, 10))

Note that the data shapes of the different extensions do not need to be the same.
Next, place the data into separate :class:`PrimaryHDU` and :class:`ImageHDU`
objects::

    >>> primary_hdu = fits.PrimaryHDU(n)
    >>> image_hdu = fits.ImageHDU(n2)
    >>> image_hdu2 = fits.ImageHDU(n3)

A multi-extension FITS file is not constrained to be only imaging or table data, we
can mix them. To show this we'll use the example from the previous section to make a
:class:`BinTableHDU`::

    >>> c1 = fits.Column(name='a', array=np.array([1, 2]), format='K')
    >>> c2 = fits.Column(name='b', array=np.array([4, 5]), format='K')
    >>> c3 = fits.Column(name='c', array=np.array([7, 8]), format='K')
    >>> table_hdu = fits.BinTableHDU.from_columns([c1, c2, c3])

Now when we create the :class:`HDUList` we list all extensions we want to
include::

    >>> hdul = fits.HDUList([primary_hdu, image_hdu, table_hdu])

Because :class:`HDUList` acts like a :class:`list` we can also append, for example,
an :class:`ImageHDU` to an already existing :class:`HDUList`::

    >>> hdul.append(image_hdu2)

Multi-extension :class:`HDUList` are treated just like those with only a
:class:`PrimaryHDU`, so to save the file use :func:`HDUList.writeto` as shown above.

.. note::

    The FITS standard enforces all files to have exactly one :class:`PrimaryHDU` that
    is the first HDU present in the file. This standard is enforced during the call to
    :func:`HDUList.writeto` and an error will be raised if it is not met. See the
    ``output_verify`` option in :func:`HDUList.writeto` for ways to fix or ignore these
    warnings.

In the previous example the :class:`PrimaryHDU` contained actual data. In some cases it
is desirable to have a minimal :class:`PrimaryHDU` with only basic header information.
To do this, first create a new :class:`Header` object to encapsulate any keywords you want
to include in the primary HDU, then as before create a :class:`PrimaryHDU`::

    >>> hdr = fits.Header()
    >>> hdr['OBSERVER'] = 'Edwin Hubble'
    >>> hdr['COMMENT'] = "Here's some commentary about this FITS file."
    >>> empty_primary = fits.PrimaryHDU(header=hdr)

When we create a new primary HDU with a custom header as in the above example,
this will automatically include any additional header keywords that are
*required* by the FITS format (keywords such as ``SIMPLE`` and ``NAXIS`` for
example). In general, users should not have to manually manage such keywords,
and should only create and modify observation-specific informational keywords.

We then create an HDUList containing both the primary HDU and any other HDUs want::

    >>> hdul = fits.HDUList([empty_primary, image_hdu2, table_hdu])

.. topic:: Examples:

    See also :ref:`sphx_glr_generated_examples_io_create-mef.py`.

Convenience Functions
---------------------

`astropy.io.fits` also provides several high-level ("convenience") functions.
Such a convenience function is a "canned" operation to achieve one task.
By using these "convenience" functions, a user does not have to worry about
opening or closing a file; all of the housekeeping is done implicitly.

.. warning::

    These functions are useful for interactive Python sessions and less complex
    analysis scripts, but should not be used for application code, as they
    are highly inefficient. For example, each call to :func:`getval`
    requires re-parsing the entire FITS file. Code that makes repeated use
    of these functions should instead open the file with :func:`open`
    and access the data structures directly.

The first of these functions is :func:`getheader`, to get the header of an HDU.
Here are several examples of getting the header. Only the file name is required
for this function. The rest of the arguments are optional and flexible to
specify which HDU the user wants to access::

    >>> from astropy.io.fits import getheader
    >>> hdr = getheader(fits_image_filename)  # get default HDU (=0), i.e. primary HDU's header
    >>> hdr = getheader(fits_image_filename, 0)  # get primary HDU's header
    >>> hdr = getheader(fits_image_filename, 2)  # the second extension
    >>> hdr = getheader(fits_image_filename, 'sci')  # the first HDU with EXTNAME='SCI'
    >>> hdr = getheader(fits_image_filename, 'sci', 2)  # HDU with EXTNAME='SCI' and EXTVER=2
    >>> hdr = getheader(fits_image_filename, ('sci', 2))  # use a tuple to do the same
    >>> hdr = getheader(fits_image_filename, ext=2)  # the second extension
    >>> hdr = getheader(fits_image_filename, extname='sci')  # first HDU with EXTNAME='SCI'
    >>> hdr = getheader(fits_image_filename, extname='sci', extver=2)

Ambiguous specifications will raise an exception::

    >>> getheader(fits_image_filename, ext=('sci', 1), extname='err', extver=2)
    Traceback (most recent call last):
        ...
    TypeError: Redundant/conflicting extension arguments(s): ...

After you get the header, you can access the information in it, such as getting
and modifying a keyword value::

    >>> fits_image_2_filename = fits.util.get_testdata_filepath('o4sp040b0_raw.fits')
    >>> hdr = getheader(fits_image_2_filename, 0)    # get primary hdu's header
    >>> filter = hdr['filter']                       # get the value of the keyword "filter'
    >>> val = hdr[10]                                # get the 11th keyword's value
    >>> hdr['filter'] = 'FW555'                      # change the keyword value

For the header keywords, the header is like a dictionary, as well as a list.
The user can access the keywords either by name or by numeric index, as
explained earlier in this chapter.

If a user only needs to read one keyword, the  :func:`getval` function can
further simplify to just one call, instead of two as shown in the above
examples::

    >>> from astropy.io.fits import getval
    >>> # get 0th extension's keyword FILTER's value
    >>> flt = getval(fits_image_2_filename, 'filter', 0)
    >>> flt
    'Clear'

    >>> # get the 2nd sci extension's 11th keyword's value
    >>> val = getval(fits_image_2_filename, 10, 'sci', 2)
    >>> val
    False

The function :func:`getdata` gets the data of an HDU. Similar to
:func:`getheader`, it only requires the input FITS file name while the
extension is specified through the optional arguments. It does have one extra
optional argument header. If header is set to True, this function will return
both data and header, otherwise only data is returned::

    >>> from astropy.io.fits import getdata
    >>> # get 3rd sci extension's data:
    >>> data = getdata(fits_image_filename, 'sci', 3)
    >>> # get 1st extension's data AND header:
    >>> data, hdr = getdata(fits_image_filename, 1, header=True)

The functions introduced above are for reading. The next few functions
demonstrate convenience functions for writing::

    >>> fits.writeto('out.fits', data, hdr)

The :func:`writeto` function uses the provided data and an optional header to
write to an output FITS file.

::

    >>> fits.append('out.fits', data, hdr)

The :func:`append` function will use the provided data and the optional header
to append to an existing FITS file. If the specified output file does not
exist, it will create one.

.. code:: python

    from astropy.io.fits import update
    update(filename, dat, hdr, 'sci')         # update the 'sci' extension
    update(filename, dat, 3)                  # update the 3rd extension
    update(filename, dat, hdr, 3)             # update the 3rd extension
    update(filename, dat, 'sci', 2)           # update the 2nd SCI extension
    update(filename, dat, 3, header=hdr)      # update the 3rd extension
    update(filename, dat, header=hdr, ext=5)  # update the 5th extension

The :func:`update` function will update the specified extension with the input
data/header. The third argument can be the header associated with the data. If
the third argument is not a header, it (and other positional arguments) are
assumed to be the extension specification(s). Header and extension specs can
also be keyword arguments.

The :func:`printdiff` function will print a difference report of two FITS files,
including headers and data. The first two arguments must be two FITS
filenames or FITS file objects with matching data types (i.e., if using strings
to specify filenames, both inputs must be strings). The third
argument is an optional extension specification, with the same call format
of :func:`getheader` and :func:`getdata`. In addition you can add any keywords
accepted by the :class:`FITSDiff` class.

.. code:: python

    from astropy.io.fits import printdiff
    # get a difference report of ext 2 of inA and inB
    printdiff('inA.fits', 'inB.fits', ext=2)
    # ignore HISTORY and COMMENT keywords
    printdiff('inA.fits', 'inB.fits', ignore_keywords=('HISTORY','COMMENT')

Finally, the :func:`info` function will print out information of the specified
FITS file::

    >>> fits.info(fits_image_filename)
    Filename: ...test0.fits
    No.    Name      Ver    Type      Cards   Dimensions   Format
      0  PRIMARY       1 PrimaryHDU     138   ()
      1  SCI           1 ImageHDU        61   (40, 40)   int16
      2  SCI           2 ImageHDU        61   (40, 40)   int16
      3  SCI           3 ImageHDU        61   (40, 40)   int16
      4  SCI           4 ImageHDU        61   (40, 40)   int16

This is one of the most useful convenience functions for getting an overview of
what a given file contains without looking at any of the details.


Using `astropy.io.fits`
=======================
.. toctree::
   :maxdepth: 2

   usage/headers
   usage/image
   usage/table
   usage/verification
   usage/unfamiliar
   usage/scripts
   usage/misc

Command-Line Utilities
======================

For convenience, several of ``astropy``'s sub-packages install utility programs
on your system which allow common tasks to be performed without having
to open a Python interpreter. These utilities include:

- `~astropy.io.fits.scripts.fitsheader`: prints the headers of a FITS file.

- `~astropy.io.fits.scripts.fitscheck`: verifies and optionally rewrites
  the CHECKSUM and DATASUM keywords of a FITS file.

- :ref:`fitsdiff`: compares two FITS files and reports the differences.

- :ref:`fits2bitmap`: converts FITS images to bitmaps, including scaling and
  stretching.

- :ref:`wcslint <wcslint>`: checks the :ref:`WCS <astropy-wcs>` keywords in a
  FITS file for compliance against the standards.

Other Information
=================

.. toctree::
    :maxdepth: 1

    appendix/faq
    appendix/header_transition
    appendix/history

.. note that if this section gets too long, it should be moved to a separate
   doc page - see the top of performance.inc.rst for the instructions on how to do
   that

.. include:: performance.inc.rst

Reference/API
=============

.. automodule:: astropy.io.fits

.. toctree::
    :maxdepth: 3

    api/files.rst
    api/hdulists.rst
    api/hdus.rst
    api/headers.rst
    api/cards.rst
    api/tables.rst
    api/images.rst
    api/diff.rst
    api/verification.rst

.. rubric:: Footnotes

.. [#f1]  For legacy code only that already depends on PyFITS, it's acceptable to continue using "from astropy.io import fits as pyfits".
.. currentmodule:: astropy.io.fits

Miscellaneous Features
**********************

This section describes some of the miscellaneous features of :mod:`astropy.io.fits`.

.. _io-fits-differs:

Differs
=======

The :mod:`astropy.io.fits.diff` module contains several facilities for
generating and reporting the differences between two FITS files, or two
components of a FITS file.

The :class:`FITSDiff` class can be used to generate and represent the
differences between either two FITS files on disk, or two existing
:class:`HDUList` objects (or some combination thereof).

Likewise, the :class:`HeaderDiff` class can be used to find the differences
just between two :class:`Header` objects. Other available differs include
:class:`HDUDiff`, :class:`ImageDataDiff`, :class:`TableDataDiff`, and
:class:`RawDataDiff`.

Each of these classes are instantiated with two instances of the objects that
they diff. The returned diff instance has a number of attributes starting with
``.diff_`` that describe differences between the two objects.

Example
-------

..
  EXAMPLE START
  Generating Differences Between FITS Files Using astropy.io.fits.diff

The :class:`HeaderDiff` class can be used to find the differences
between two :class:`Header` objects like so::

    >>> from astropy.io import fits
    >>> header1 = fits.Header([('KEY_A', 1), ('KEY_B', 2)])
    >>> header2 = fits.Header([('KEY_A', 3), ('KEY_C', 4)])
    >>> diff = fits.diff.HeaderDiff(header1, header2)
    >>> diff.identical
    False
    >>> diff.diff_keywords
    (['KEY_B'], ['KEY_C'])
    >>> diff.diff_keyword_values
    defaultdict(..., {'KEY_A': [(1, 3)]})

See the API documentation for details on the different differ classes.

..
  EXAMPLE END
.. currentmodule:: astropy.io.fits

FITS Headers
************

In the next three chapters, more detailed information including examples will
be explained for manipulating FITS headers, image/array data, and table data
respectively.


Header of an HDU
================

Every Header Data Unit (HDU) normally has two components: header and data. In
``astropy`` these two components are accessed through the two attributes of the
HDU, ``hdu.header`` and ``hdu.data``.

While an HDU may have empty data (i.e., the ``.data`` attribute is `None`), any
HDU will always have a header. When an HDU is created with a constructor (e.g.,
``hdu = PrimaryHDU(data, header)``), the user may supply the header value from
an existing HDU's header and the data value from a ``numpy`` array. If the
defaults (None) are used, the new HDU will have the minimal required keywords
for an HDU of that type::

    >>> from astropy.io import fits
    >>> hdu = fits.PrimaryHDU()
    >>> hdu.header  # show the all of the header cards
    SIMPLE  =                    T / conforms to FITS standard
    BITPIX  =                    8 / array data type
    NAXIS   =                    0 / number of array dimensions
    EXTEND  =                    T

A user can use any header and any data to construct a new HDU. ``astropy`` will
strip any keywords that describe the data structure leaving only your
informational keywords. Later it will add back in the required structural
keywords for compatibility with the new HDU and any data added to it. So, a
user can use a table HDU's header to construct an image HDU and vice versa. The
constructor will also ensure the data type and dimension information in the
header agree with the data.


The Header Attribute
====================

Value Access, Updating, and Creating
------------------------------------

As shown in the :ref:`Getting Started <tutorial>` tutorial, keyword values can
be accessed via keyword name or index of an HDU's header attribute. You can
also use the wildcard character ``*`` to get the keyword value pairs that match
your search string. Here is a quick summary::

    >>> fits_image_filename = fits.util.get_testdata_filepath('test0.fits')
    >>> hdul = fits.open(fits_image_filename)  # open a FITS file
    >>> hdr = hdul[0].header  # the primary HDU header
    >>> print(hdr[34])  # get the 2nd keyword's value
    96
    >>> hdr[34] = 20  # change its value
    >>> hdr['DARKCORR']  # get the value of the keyword 'darkcorr'
    'OMIT'
    >>> hdr['DARKCOR*']  # get keyword values using wildcard matching
    DARKCORR= 'OMIT              ' / Do dark correction: PERFORM, OMIT, COMPLETE
    >>> hdr['darkcorr'] = 'PERFORM'  # change darkcorr's value

Keyword names are case-insensitive except in a few special cases (see the
sections on HIERARCH card and record-valued cards). Thus, ``hdr['abc']``,
``hdr['ABC']``, or ``hdr['aBc']`` are all equivalent.

As with Python's :class:`dict` type, new keywords can also be added to the
header using assignment syntax::

    >>> hdr = hdul[1].header
    >>> 'DARKCORR' in hdr  # Check for existence
    False
    >>> hdr['DARKCORR'] = 'OMIT'  # Add a new DARKCORR keyword

You can also add a new value *and* comment by assigning them as a tuple::

    >>> hdr['DARKCORR'] = ('OMIT', 'Dark Image Subtraction')

.. note::

    An important point to note when adding new keywords to a header is that by
    default they are not appended *immediately* to the end of the file.
    Rather, they are appended to the last non-commentary keyword. This is in
    order to support the common use case of always having all HISTORY keywords
    grouped together at the end of a header. A new non-commentary keyword will
    be added at the end of the existing keywords, but before any
    HISTORY/COMMENT keywords at the end of the header.

    There are a couple of ways to override this functionality:

    * Use the :meth:`Header.append` method with the ``end=True`` argument:

        >>> hdr.append(('DARKCORR', 'OMIT', 'Dark Image Subtraction'), end=True)

      This forces the new keyword to be added at the actual end of the header.

    * The :meth:`Header.insert` method will always insert a new keyword exactly
      where you ask for it:

        >>> del hdr['DARKCORR']  # Delete previous insertion for doctest
        >>> hdr.insert(20, ('DARKCORR', 'OMIT', 'Dark Image Subtraction'))

      This inserts the DARKCORR keyword before the 20th keyword in the
      header no matter what it is.

A keyword (and its corresponding card) can be deleted using the same index/name
syntax::

    >>> del hdr[3]  # delete the 2nd keyword
    >>> del hdr['DARKCORR']  # delete the value of the keyword 'DARKCORR'

Note that, like a regular Python list, the indexing updates after each delete,
so if ``del hdr[3]`` is done two times in a row, the fourth and fifth keywords
are removed from the original header. Likewise, ``del hdr[-1]`` will delete
the last card in the header.

It is also possible to delete an entire range of cards using the slice syntax::

    >>> del hdr[3:5]

The method :meth:`Header.set` is another way to update the value or comment
associated with an existing keyword, or to create a new keyword. Most of its
functionality can be duplicated with the dict-like syntax shown above. But in
some cases it might be more clear. It also has the advantage of allowing a user
to either move cards within the header or specify the location of a new card
relative to existing cards::

    >>> hdr.set('target', 'NGC1234', 'target name')
    >>> # place the next new keyword before the 'TARGET' keyword
    >>> hdr.set('newkey', 666, before='TARGET')  # comment is optional
    >>> # place the next new keyword after the 21st keyword
    >>> hdr.set('newkey2', 42.0, 'another new key', after=20)

In FITS headers, each keyword may also have a comment associated with it
explaining its purpose. The comments associated with each keyword are accessed
through the :attr:`~Header.comments` attribute::

    >>> hdr['NAXIS']
    2
    >>> hdr.comments['NAXIS']
    'number of data axes'
    >>> hdr.comments['NAXIS'] = 'The number of image axes'  # Update
    >>> hdul.close()  # close the HDUList again

Comments can be accessed in all of the same ways that values are accessed,
whether by keyword name or card index. Slices are also possible. The only
difference is that you go through ``hdr.comments`` instead of just ``hdr`` by
itself.


COMMENT, HISTORY, and Blank Keywords
------------------------------------

Most keywords in a FITS header have unique names. If there are more than two
cards sharing the same name, it is the first one accessed when referred by
name. The duplicates can only be accessed by numeric indexing.

There are three special keywords (their associated cards are sometimes referred
to as commentary cards), which commonly appear in FITS headers more than once.
They are (1) blank keyword, (2) HISTORY, and (3) COMMENT. Unlike other
keywords, when accessing these keywords they are returned as a list::

    >>> filename = fits.util.get_testdata_filepath('history_header.fits')
    >>> with fits.open(filename) as hdul:  # open a FITS file
    ...     hdr = hdul[0].header

    >>> hdr['HISTORY']
    I updated this file on 02/03/2011
    I updated this file on 02/04/2011

These lists can be sliced like any other list. For example, to display just the
last HISTORY entry, use ``hdr['history'][-1]``. Existing commentary cards
can also be updated by using the appropriate index number for that card.

New commentary cards can be added like any other card by using the dict-like
keyword assignment syntax, or by using the :meth:`Header.set` method. However,
unlike with other keywords, a new commentary card is always added and appended
to the last commentary card with the same keyword, rather than to the end of
the header.

Example
^^^^^^^

..
  EXAMPLE START
  Manipulating FITS Headers in astropy.io.fits

To add a new commentary card::

    >>> hdu.header['HISTORY'] = 'history 1'
    >>> hdu.header[''] = 'blank 1'
    >>> hdu.header['COMMENT'] = 'comment 1'
    >>> hdu.header['HISTORY'] = 'history 2'
    >>> hdu.header[''] = 'blank 2'
    >>> hdu.header['COMMENT'] = 'comment 2'

and the part in the modified header becomes:

.. parsed-literal::

    HISTORY history 1
    HISTORY history 2
            blank 1
            blank 2
    COMMENT comment 1
    COMMENT comment 2


Users can also directly control exactly where in the header to add a new
commentary card by using the :meth:`Header.insert` method.

.. note::

    Ironically, there is no comment in a commentary card, only a string
    value.

..
  EXAMPLE END

Undefined Values
----------------

FITS headers can have undefined values and these are represented in Python
with the special value `None`. `None` can be used when assigning values
to a `~astropy.io.fits.Header` or `~astropy.io.fits.Card`.

    >>> hdr = fits.Header()
    >>> hdr['UNDEF'] = None
    >>> hdr['UNDEF'] is None
    True
    >>> repr(hdr)
    'UNDEF   =                                                                       '
    >>> hdr.append('UNDEF2')
    >>> hdr['UNDEF2'] is None
    True
    >>> hdr.append(('UNDEF3', None, 'Undefined value'))
    >>> str(hdr.cards[-1])
    'UNDEF3  =  / Undefined value                                                    '


Card Images
===========

A FITS header consists of card images.

A card image in a FITS header consists of a keyword name, a value, and
optionally a comment. Physically, it takes 80 columns (bytes)  without carriage
return  in a FITS file's storage format. In ``astropy``, each card image is
manifested by a :class:`Card` object. There are also special kinds of cards:
commentary cards (see above) and card images taking more than one 80-column
card image. The latter will be discussed later.

Most of the time the details of dealing with cards are handled by the
:class:`Header` object, and it is not necessary to directly manipulate cards.
In fact, most :class:`Header` methods that accept a ``(keyword, value)`` or
``(keyword, value, comment)`` tuple as an argument can also take a
:class:`Card` object as an argument. :class:`Card` objects are just wrappers
around such tuples that provide the logic for parsing and formatting individual
cards in a header. There is usually nothing gained by manually using a
:class:`Card` object, except to examine how a card might appear in a header
before actually adding it to the header.

A new Card object is created with the :class:`Card` constructor:
``Card(key, value, comment)``.

Example
-------

..
  EXAMPLE START
  Card Images in FITS Headers in astropy.io.fits

To create a new Card object::

    >>> c1 = fits.Card('TEMP', 80.0, 'temperature, floating value')
    >>> c2 = fits.Card('DETECTOR', 1)  # comment is optional
    >>> c3 = fits.Card('MIR_REVR', True,
    ...                'mirror reversed? Boolean value')
    >>> c4 = fits.Card('ABC', 2+3j, 'complex value')
    >>> c5 = fits.Card('OBSERVER', 'Hubble', 'string value')

    >>> print(c1); print(c2); print(c3); print(c4); print(c5)  # show the cards
    TEMP    =                 80.0 / temperature, floating value
    DETECTOR=                    1
    MIR_REVR=                    T / mirror reversed? Boolean value
    ABC     =           (2.0, 3.0) / complex value
    OBSERVER= 'Hubble  '           / string value

Cards have the attributes ``.keyword``, ``.value``, and ``.comment``. Both
``.value`` and ``.comment`` can be changed but not the ``.keyword`` attribute.
In other words, once a card is created, it is created for a specific, immutable
keyword.

The :meth:`Card` constructor will check if the arguments given are conforming
to the FITS standard and has a fixed card image format. If the user wants to
create a card with a customized format or even a card which is not conforming
to the FITS standard (e.g., for testing purposes), the :meth:`Card.fromstring`
class method can be used.

Cards can be verified with :meth:`Card.verify`. The nonstandard card ``c2`` in
the example below is flagged by such verification. More about verification in
``astropy`` will be discussed in a later chapter.

::

    >>> c1 = fits.Card.fromstring('ABC     = 3.456D023')
    >>> c2 = fits.Card.fromstring("P.I. ='Hubble'")
    >>> print(c1)
    ABC     = 3.456D023
    >>> print(c2)  # doctest: +SKIP
    P.I. ='Hubble'
    >>> c2.verify()  # doctest: +SKIP
    Output verification result:
    Unfixable error: Illegal keyword name 'P.I.'

A list of the :class:`Card` objects underlying a :class:`Header` object can be
accessed with the :attr:`Header.cards` attribute. This list is only meant for
observing, and should not be directly manipulated. In fact, it is only a
copy  modifications to it will not affect the header from which it came. Use
the methods provided by the :class:`Header` class instead.

..
  EXAMPLE END


CONTINUE Cards
==============

The fact that the FITS standard only allows up to eight characters for the
keyword name and 80 characters to contain the keyword, the value, and the
comment is restrictive for certain applications. To allow long string values
for keywords, a proposal was made in:

    https://heasarc.gsfc.nasa.gov/docs/heasarc/ofwg/docs/ofwg_recomm/r13.html

by using the CONTINUE keyword after the regular 80 column containing the
keyword. ``astropy`` does support this convention, which is a part of the FITS
standard since version 4.0.

Examples
--------

..
  EXAMPLE START
  CONTINUE Cards for Long String Values in astropy.io.fits

The examples below show that the use of CONTINUE is automatic for long
string values::

    >>> hdr = fits.Header()
    >>> hdr['abc'] = 'abcdefg' * 20
    >>> hdr
    ABC     = 'abcdefgabcdefgabcdefgabcdefgabcdefgabcdefgabcdefgabcdefgabcdefgabcd&'
    CONTINUE  'efgabcdefgabcdefgabcdefgabcdefgabcdefgabcdefgabcdefgabcdefgabcdefga&'
    CONTINUE  'bcdefg'
    >>> hdr['abc']
    'abcdefgabcdefgabcdefgabcdefgabcdefgabcdefgabcdefgabcdefgabcdefgabcdefgabcdefgabcdefgabcdefgabcdefgabcdefgabcdefgabcdefgabcdefgabcdefgabcdefg'
    >>> # both value and comments are long
    >>> hdr['abc'] = ('abcdefg' * 10, 'abcdefg' * 10)
    >>> hdr
    ABC     = 'abcdefgabcdefgabcdefgabcdefgabcdefgabcdefgabcdefgabcdefgabcdefgabcd&'
    CONTINUE  'efg&'
    CONTINUE  '&' / abcdefgabcdefgabcdefgabcdefgabcdefgabcdefgabcdefgabcdefgabcdefga
    CONTINUE  '' / bcdefg

Note that when a CONTINUE card is used, at the end of each 80-character card
image, an ampersand is present. The ampersand is not part of the string value.
Also, there is no "=" at the ninth column after CONTINUE. In the first example,
the entire 240 characters is treated by ``astropy`` as a single card. So, if it
is the nth card in a header, the (n+1)th card refers to the next keyword, not
the next CONTINUE card. As such, CONTINUE cards are transparently handled by
``astropy`` as a single logical card, and it is generally not necessary to worry
about the details of the format. Keywords that resolve to a set of CONTINUE
cards can be accessed and updated just like regular keywords.

..
  EXAMPLE END


HIERARCH Cards
==============

For keywords longer than eight characters, there is a convention originated at
the European Southern Observatory (ESO) to facilitate such use. It uses a
special keyword HIERARCH with the actual long keyword following. ``astropy``
supports this convention as well.

If a keyword contains more than eight characters ``astropy`` will automatically
use a HIERARCH card, but will also issue a warning in case this is in error.
However, you may explicitly request a HIERARCH card by prepending the keyword
with 'HIERARCH ' (just as it would appear in the header). For example,
``hdr['HIERARCH abcdefghi']`` will create the keyword ``abcdefghi`` without
displaying a warning. Once created, HIERARCH keywords can be accessed like any
other: ``hdr['abcdefghi']``, without prepending 'HIERARCH' to the keyword.

Examples
--------

..
  EXAMPLE START
  HIERARCH Cards for Keywords Longer than Eight Characters in astropy.io.fits

``astropy`` will use a HIERARCH card and issue a warning when keywords contain
more than eight characters::

    >>> # this will result in a Warning because a HIERARCH card is implicitly created
    >>> c = fits.Card('abcdefghi', 10)  # doctest: +SKIP
    >>> print(c)  # doctest: +SKIP
    HIERARCH abcdefghi = 10
    >>> c = fits.Card('hierarch abcdefghi', 10)
    >>> print(c)
    HIERARCH abcdefghi = 10
    >>> hdu = fits.PrimaryHDU()
    >>> hdu.header['hierarch abcdefghi'] =  99
    >>> hdu.header['abcdefghi']
    99
    >>> hdu.header['abcdefghi'] = 10
    >>> hdu.header['abcdefghi']
    10
    >>> hdu.header
    SIMPLE  =                    T / conforms to FITS standard
    BITPIX  =                    8 / array data type
    NAXIS   =                    0 / number of array dimensions
    EXTEND  =                    T
    HIERARCH abcdefghi = 10

..
  EXAMPLE END

.. note::

    A final point to keep in mind about the :class:`Header` class is that much
    of its design is intended to abstract away quirks about the FITS format.
    This is why, for example, it will automatically create CONTINUE and
    HIERARCH cards. The Header is just a data structure, and as a user you
    should not have to worry about how it ultimately gets serialized to a header
    in a FITS file.

    Though there are some areas where it is almost impossible to hide away the
    quirks of the FITS format, ``astropy`` tries to make it so that you have to
    think about it as little as possible. If there are any areas that are left
    vague or difficult to understand about how the header is constructed, please
    let us know, as there are probably areas where this can be
    improved on even more.

.. currentmodule:: astropy.io.fits

Table Data
**********

In this chapter, we will discuss the data component in a table HDU. A table will
always be in an extension HDU, never in a primary HDU.

There are two kinds of tables in the FITS standard: binary tables and ASCII
tables. Binary tables are more economical in storage and faster in data access
and manipulation. ASCII tables store the data in a "human readable" form and
therefore take up more storage space as well as more processing time since the
ASCII text needs to be parsed into numerical values.

.. note::

    If you want to read or write a single table in FITS format then the
    most convenient method is often via the high-level :ref:`table_io`. In
    particular see the :ref:`Unified I/O FITS <table_io_fits>` section.

Table Data as a Record Array
============================


What is a Record Array?
-----------------------

A record array is an array which contains records (i.e., rows) of heterogeneous
data types. Record arrays are available through the records module in the NumPy
library.

Here is a sample record array::

    >>> import numpy as np
    >>> bright = np.rec.array([(1,'Sirius', -1.45, 'A1V'),
    ...                        (2,'Canopus', -0.73, 'F0Ib'),
    ...                        (3,'Rigil Kent', -0.1, 'G2V')],
    ...                       formats='int16,a20,float32,a10',
    ...                       names='order,name,mag,Sp')

In this example, there are three records (rows) and four fields (columns). The
first field is a short integer, the second a character string (of length 20),
the third a floating point number, and the fourth a character string (of length
10). Each record has the same (heterogeneous) data structure.

The underlying data structure used for FITS tables is a class called
:class:`FITS_rec` which is a specialized subclass of `numpy.recarray`. A
:class:`FITS_rec` can be instantiated directly using the same initialization
format presented for plain recarrays as in the example above. You may also
instantiate a new :class:`FITS_rec` from a list of `astropy.io.fits.Column`
objects using the :meth:`FITS_rec.from_columns` class method. This has the
exact same semantics as :meth:`BinTableHDU.from_columns` and
:meth:`TableHDU.from_columns`, except that it only returns an actual FITS_rec
array and not a whole HDU object.


Metadata of a Table
-------------------

The data in a FITS table HDU is basically a record array with added
attributes. The metadata (i.e., information about the table data) are stored in
the header. For example, the keyword TFORM1 contains the format of the first
field, TTYPE2 the name of the second field, etc. NAXIS2 gives the number of
records (rows) and TFIELDS gives the number of fields (columns). For FITS
tables, the maximum number of fields is 999. The data type specified in TFORM
is represented by letter codes for binary tables and a Fortran-like format
string for ASCII tables. Note that this is different from the format
specifications when constructing a record array.


Reading a FITS Table
--------------------

Like images, the ``.data`` attribute of a table HDU contains the data of the
table.

Example
^^^^^^^

..
  EXAMPLE START
  Reading a FITS Table with astropy.io.fits

To read a FITS Table::


    >>> from astropy.io import fits
    >>> fits_table_filename = fits.util.get_testdata_filepath('btable.fits')

    >>> hdul = fits.open(fits_table_filename)  # open a FITS file
    >>> data = hdul[1].data  # assume the first extension is a table
    >>> # show the first two rows
    >>> first_two_rows = data[:2]
    >>> first_two_rows  # doctest: +SKIP
    [(1, 'Sirius', -1.45000005, 'A1V') (2, 'Canopus', -0.73000002, 'F0Ib')]
    >>> # show the values in field "mag"
    >>> magnitudes = data['mag']
    >>> magnitudes  # doctest: +SKIP
    array([-1.45000005, -0.73000002, -0.1       ], dtype=float32)
    >>> # columns can be referenced by index too
    >>> names = data.field(1)
    >>> names.tolist() # doctest: +SKIP
    ['Sirius', 'Canopus', 'Rigil Kent']
    >>> hdul.close()

Note that in ``astropy``, when using the ``field()`` method, it is 0-indexed
while the suffixes in header keywords such as TFORM is 1-indexed. So,
``data.field(0)`` is the data in the column with the name specified in TTYPE1
and format in TFORM1.

.. warning::

    The FITS format allows table columns with a zero-width data format, such as
    ``'0D'``. This is probably intended as a space-saving measure on files in
    which that column contains no data. In such files, the zero-width columns
    are omitted when accessing the table data, so the indexes of fields might
    change when using the ``field()`` method. For this reason, if you expect
    to encounter files containing zero-width columns it is recommended to access
    fields by name rather than by index.

..
  EXAMPLE END


Table Operations
================


Selecting Records in a Table
----------------------------

Like image data, we can use the same "mask array" idea to pick out desired
records from a table and make a new table out of it.

Examples
^^^^^^^^

..
  EXAMPLE START
  Selecting Records in a Table Using a "Mask Array"

Assuming the table's second field as having the name 'magnitude', an output
table containing all the records of magnitude > -0.5 from the input table is
generated::

    >>> with fits.open(fits_table_filename) as hdul:
    ...     data = hdul[1].data
    ...     mask = data['mag'] > -0.5
    ...     newdata = data[mask]
    ...     hdu = fits.BinTableHDU(data=newdata)
    ...     hdu.writeto('newtable.fits')

It is also possible to update the data from the HDU object in-place::

    >>> with fits.open(fits_table_filename) as hdul:
    ...     hdu = hdul[1]
    ...     mask = hdu.data['mag'] > -0.5
    ...     hdu.data = hdu.data[mask]
    ...     hdu.writeto('newtable2.fits')

..
  EXAMPLE END

Merging Tables
--------------

Merging different tables is very convenient in ``astropy``.

Examples
^^^^^^^^

..
  EXAMPLE START
  Merging FITS Tables

To merge the column definitions of the input tables::

    >>> fits_other_table_filename = fits.util.get_testdata_filepath('table.fits')

    >>> with fits.open(fits_table_filename) as hdul1:
    ...     with fits.open(fits_other_table_filename) as hdul2:
    ...         new_columns = hdul1[1].columns + hdul2[1].columns
    ...         new_hdu = fits.BinTableHDU.from_columns(new_columns)
    >>> new_columns
    ColDefs(
            name = 'order'; format = 'I'
            name = 'name'; format = '20A'
            name = 'mag'; format = 'E'
            name = 'Sp'; format = '10A'
            name = 'target'; format = '20A'
            name = 'V_mag'; format = 'E'
        )

The number of fields in the output table will be the sum of numbers of fields
of the input tables. Users have to make sure the input tables do not share any
common field names. The number of records in the output table will be the
largest number of records of all input tables. The expanded slots for the
originally shorter table(s) will be zero (or blank) filled.

Another version of this example can be used to append a new column to a
table. Updating an existing table with a new column is generally more
difficult than it is worth, but you can "append" a column to a table by creating
a new table with columns from the existing table plus the new column(s)::

    >>> with fits.open(fits_table_filename) as hdul:
    ...     orig_table = hdul[1].data
    ...     orig_cols = orig_table.columns
    >>> new_cols = fits.ColDefs([
    ...     fits.Column(name='NEWCOL1', format='D',
    ...                 array=np.zeros(len(orig_table))),
    ...     fits.Column(name='NEWCOL2', format='D',
    ...                 array=np.zeros(len(orig_table)))])
    >>> hdu = fits.BinTableHDU.from_columns(orig_cols + new_cols)

Now ``newtable.fits`` contains a new table with the original table, plus the
two new columns filled with zeros.

..
  EXAMPLE END

Appending Tables
----------------

Appending one table after another is slightly trickier, since the two tables
may have different field attributes.

Examples
^^^^^^^^

..
  EXAMPLE START
  Appending to FITS Tables

Here, the first example is to append by field indices, and the second one is to
append by field names. In both cases, the output table will inherit the column
attributes (name, format, etc.) of the first table::

    >>> with fits.open(fits_table_filename) as hdul1:
    ...     with fits.open(fits_table_filename) as hdul2:
    ...         nrows1 = hdul1[1].data.shape[0]
    ...         nrows2 = hdul2[1].data.shape[0]
    ...         nrows = nrows1 + nrows2
    ...         hdu = fits.BinTableHDU.from_columns(hdul1[1].columns, nrows=nrows)
    ...         for colname in hdul1[1].columns.names:
    ...             hdu.data[colname][nrows1:] = hdul2[1].data[colname]

..
  EXAMPLE END

Scaled Data in Tables
=====================

A table field's data, like an image, can also be scaled. Scaling in a table has
a more generalized meaning than in images. In images, the physical data is a
simple linear transformation from the storage data. The table fields do have
such a construct too, where BSCALE and BZERO are stored in the header as TSCALn
and TZEROn. In addition, boolean columns and ASCII tables' numeric fields are
also generalized "scaled" fields, but without TSCAL and TZERO.

All scaled fields, like the image case, will take extra memory space as well as
processing. So, if high performance is desired, try to minimize the use of
scaled fields.

All of the scalings are done for the user, so the user only sees the physical
data. Thus, there is no need to worry about scaling back and forth between the
physical and storage column values.


Creating a FITS Table
=====================

.. _column_creation:

Column Creation
---------------

To create a table from scratch, it is necessary to create individual columns
first. A :class:`Column` constructor needs the minimal information of column
name and format. Here is a summary of all allowed formats for a binary table:

.. parsed-literal::

    **FITS format code         Description                     8-bit bytes**

    L                        logical (Boolean)               1
    X                        bit                             \*
    B                        Unsigned byte                   1
    I                        16-bit integer                  2
    J                        32-bit integer                  4
    K                        64-bit integer                  8
    A                        character                       1
    E                        single precision float (32-bit) 4
    D                        double precision float (64-bit) 8
    C                        single precision complex        8
    M                        double precision complex        16
    P                        array descriptor                8
    Q                        array descriptor                16

We will concentrate on binary tables in this chapter. ASCII tables will be
discussed in a later chapter. The less frequently used X format (bit array) and
P format (used in variable length tables) will also be discussed in a later
chapter.

Besides the required name and format arguments in constructing a
:class:`Column`, there are many optional arguments which can be used in
creating a column. Here is a list of these arguments and their corresponding
header keywords and descriptions:

.. parsed-literal::

    **Argument        Corresponding         Description**
    **in Column()     header keyword**

    name            TTYPE                 column name
    format          TFORM                 column format
    unit            TUNIT                 unit
    null            TNULL                 null value (only for B, I, and J)
    bscale          TSCAL                 scaling factor for data
    bzero           TZERO                 zero point for data scaling
    disp            TDISP                 display format
    dim             TDIM                  multi-dimensional array spec
    start           TBCOL                 starting position for ASCII table
    coord_type      TCTYP                 coordinate/axis type
    coord_unit      TCUNI                 coordinate/axis unit
    coord_ref_point TCRPX                 pixel coordinate of the reference point
    coord_ref_value TCRVL                 coordinate value at reference point
    coord_inc       TCDLT                 coordinate increment at reference point
    time_ref_pos    TRPOS                 reference position for a time coordinate column
    ascii                                 specifies a column for an ASCII table
    array                                 the data of the column

Examples
^^^^^^^^

..
  EXAMPLE START
  Creating a FITS Table

Here are a few Columns using various combinations of the optional arguments::

    >>> counts = np.array([312, 334, 308, 317])
    >>> names = np.array(['NGC1', 'NGC2', 'NGC3', 'NGC4'])
    >>> values = np.arange(2*2*4).reshape(4, 2, 2)
    >>> col1 = fits.Column(name='target', format='10A', array=names)
    >>> col2 = fits.Column(name='counts', format='J', unit='count', array=counts)
    >>> col3 = fits.Column(name='notes', format='A10')
    >>> col4 = fits.Column(name='spectrum', format='10E')
    >>> col5 = fits.Column(name='flag', format='L', array=[True, False, True, True])
    >>> col6 = fits.Column(name='intarray', format='4I', dim='(2, 2)', array=values)

In this example, formats are specified with the FITS letter codes. When there
is a number (>1) preceding a (numeric type) letter code, it means each cell in
that field is a one-dimensional array. In the case of column "col4", each cell
is an array (a NumPy array) of 10 elements. And in the case of column "col6",
with the use of the "dim" argument, each cell is a multi-dimensional array of
2x2 elements.

For character string fields, the number should be to the *left* of the letter
'A' when creating binary tables, and should be to the *right* when creating
ASCII tables. However, as this is a common confusion, both formats are
understood when creating binary tables (note, however, that upon writing to a
file the correct format will be written in the header). So, for columns "col1"
and "col3", they both have 10 characters in each of their cells. For numeric
data type, the dimension number must be before the letter code, not after.

After the columns are constructed, the :meth:`BinTableHDU.from_columns` class
method can be used to construct a table HDU. We can either go through the
column definition object::

    >>> coldefs = fits.ColDefs([col1, col2, col3, col4, col5, col6])
    >>> hdu = fits.BinTableHDU.from_columns(coldefs)
    >>> coldefs
    ColDefs(
        name = 'target'; format = '10A'
        name = 'counts'; format = 'J'; unit = 'count'
        name = 'notes'; format = '10A'
        name = 'spectrum'; format = '10E'
        name = 'flag'; format = 'L'
        name = 'intarray'; format = '4I'; dim = '(2, 2)'
    )

or directly use the :meth:`BinTableHDU.from_columns` method::

    >>> hdu = fits.BinTableHDU.from_columns([col1, col2, col3, col4, col5, col6])
    >>> hdu.columns
    ColDefs(
        name = 'target'; format = '10A'
        name = 'counts'; format = 'J'; unit = 'count'
        name = 'notes'; format = '10A'
        name = 'spectrum'; format = '10E'
        name = 'flag'; format = 'L'
        name = 'intarray'; format = '4I'; dim = '(2, 2)'
    )

.. note::

    Users familiar with older versions of ``astropy`` will wonder what
    happened to ``astropy.io.fits.new_table``. :meth:`BinTableHDU.from_columns`
    and its companion for ASCII tables :meth:`TableHDU.from_columns` are the
    same in the arguments they accept and their behavior, but make it
    more explicit as to what type of table HDU they create.

A look at the newly created HDU's header will show that relevant keywords are
properly populated::

    >>> hdu.header
    XTENSION= 'BINTABLE'           / binary table extension
    BITPIX  =                    8 / array data type
    NAXIS   =                    2 / number of array dimensions
    NAXIS1  =                   73 / length of dimension 1
    NAXIS2  =                    4 / length of dimension 2
    PCOUNT  =                    0 / number of group parameters
    GCOUNT  =                    1 / number of groups
    TFIELDS =                    6 / number of table fields
    TTYPE1  = 'target  '
    TFORM1  = '10A     '
    TTYPE2  = 'counts  '
    TFORM2  = 'J       '
    TUNIT2  = 'count   '
    TTYPE3  = 'notes   '
    TFORM3  = '10A     '
    TTYPE4  = 'spectrum'
    TFORM4  = '10E     '
    TTYPE5  = 'flag    '
    TFORM5  = 'L       '
    TTYPE6  = 'intarray'
    TFORM6  = '4I      '
    TDIM6   = '(2, 2)  '

.. warning::

    It should be noted that when creating a new table with
    :meth:`BinTableHDU.from_columns`, an in-memory copy of all of the input
    column arrays is created. This is because it is not guaranteed that the
    columns are arranged contiguously in memory in row-major order (in fact,
    they are most likely not), so they have to be combined into a new array.

However, if the array data *is* already contiguous in memory, such as in an
existing record array, a kludge can be used to create a new table HDU without
any copying. First, create the Columns as before, but without using the
``array=`` argument::

    >>> col1 = fits.Column(name='target', format='10A')

Then call :meth:`BinTableHDU.from_columns`::

    >>> hdu = fits.BinTableHDU.from_columns([col1, col2, col3, col4, col5])

This will create a new table HDU as before, with the correct column
definitions, but an empty data section. Now you can assign your array directly
to the HDU's data attribute:

.. doctest-skip::

    >>> hdu.data = mydata

In a future version of ``astropy``, table creation will be simplified and this
process will not be necessary.

..
  EXAMPLE END

.. _fits_time_column:

FITS Tables with Time Columns
=============================

The `FITS Time standard paper
<https://ui.adsabs.harvard.edu/abs/2015A%26A...574A..36R/>`_ defines the formats
and keywords used to represent timing information in FITS files. The ``astropy``
FITS package provides support for reading and writing native
`~astropy.time.Time` columns and objects using this format. This is done
within the :ref:`table_io_fits` unified I/O interface and examples of usage can
be found in the :ref:`fits_astropy_native` section. The support is not
complete and only a subset of the full standard is implemented.

Example
-------

..
  EXAMPLE START
  FITS Tables with Time Columns

The following is an example of a Header extract of a binary table (event list)
with a time column:

.. parsed-literal::

    COMMENT      ---------- Globally valid key words ----------------
    TIMESYS = TT                / Time system
    MJDREF  = 50814.000000000000  / MJD zero point for (native) TT (= 1998-01-01)
    MJD-OBS = 53516.257939301     / MJD for observation in (native) TT

    COMMENT      ---------- Time Column -----------------------
    TTYPE1  = Time              / S/C TT corresponding to mid-exposure
    TFORM1  = 2D                / format of field
    TUNIT1  = s       
    TCTYP1  = TT      
    TCNAM1  = Terrestrial Time  / This is TT
    TCUNI1  = s       

..
  EXAMPLE END

However, the FITS standard and the ``astropy`` Time object are not perfectly
mapped and some compromises must be made. To help the user understand how the
``astropy`` code deals with these situations, the following text describes the
approach that ``astropy`` takes in some detail.

To create FITS columns which adhere to the FITS Time standard, we have taken
into account the following important points stated in the `FITS Time paper
<https://ui.adsabs.harvard.edu/abs/2015A%26A...574A..36R/>`_.

The strategy used to store `~astropy.time.Time` columns in FITS tables is to
create a `~astropy.io.fits.Header` with the appropriate time coordinate
global reference keywords and the column-specific override keywords. The
module ``astropy.io.fits.fitstime`` deals with the reading and writing of
Time columns.

The following keywords set the Time Coordinate Frame:

* TIME SCALE

  The most important of all of the metadata is the time scale which is a
  specification for measuring time.

  .. parsed-literal::

      **TIMESYS** (string-valued)
      Time scale; default UTC

      **TCTYPn** (string-valued)
      Column-specific override keyword

  The global time scale may be overridden by a time scale recorded in the table
  equivalent keyword ``TCTYPn`` for time coordinates in FITS table columns.
  ``TCTYna`` is used for alternate coordinates.

* TIME REFERENCE

  The reference point in time to which all times in the HDU are relative.
  Since there are no context-specific reference times in case there are
  multiple time columns in the same table, we need to adjust the reference
  times for the columns using some other keywords.

  The reference point in time shall be specified through one of the three
  following keywords, which are listed in decreasing order of preference:

  .. parsed-literal::

      **MJDREF** (floating-valued)
      Reference time in MJD

      **JDREF** (floating-valued)
      Reference time in JD

      **DATEREF** (datetime-valued)
      Reference time in ISO-8601

  The time reference keywords (MJDREF, JDREF, DATEREF) are interpreted using the
  time scale specified in ``TIMESYS``.

  .. note::

     If none of the three keywords are present, there is no problem as long as
     all times in the HDU are expressed in ISO-8601 ``Datetime Strings`` format:
     ``CCYY-MM-DD[Thh:mm:ss[.s...]]`` (e.g., ``"2015-04-05T12:22:33.8"``);
     otherwise MJDREF = 0.0 must be assumed.

     The value of the reference time has global validity for all time values,
     but it does not have a particular time scale associated with it. Thus we
     need to use ``TCRVLn`` (time coordinate reference value) keyword to
     compensate for the time scale differences.

* TIME REFERENCE POSITION

  The reference position, specified by the keyword ``TREFPOS``, specifies the
  spatial location at which the time is valid, either where the observation was
  made or the point in space for which light-time corrections have been applied.
  This may be a standard location (such as ``GEOCENTER`` or ``TOPOCENTER``) or
  a point in space defined by specific coordinates.

  .. parsed-literal::

      **TREFPOS** (string-valued)
      Time reference position; default TOPOCENTER

      **TRPOSn** (string-valued)
      Column-specific override keyword

  .. note::

     For TOPOCENTER, we need to specify the observatory location
     (ITRS Cartesian coordinates or geodetic latitude/longitude/height) in the
     ``OBSGEO-*`` keywords.

* TIME REFERENCE DIRECTION

  If any pathlength corrections have been applied to the time stamps (i.e., if
  the reference position is not ``TOPOCENTER`` for observational data), the
  reference direction that is used in calculating the pathlength delay should
  be provided in order to maintain a proper analysis trail of the data.
  However, this is useful only if there is also information available on the
  location from where the observation was made (the observatory location).

  The reference direction is indicated through a reference to specific keywords.
  These keywords may explicitly hold the direction or indicate columns holding
  the coordinates.

  .. parsed-literal::

      **TREFDIR** (string-valued)
      Pointer to time reference direction

      **TRDIRn** (string-valued)
      Column-specific override keyword

* TIME UNIT

  The FITS standard recommends the time unit to be one of the allowed ones
  in the specification.

  .. parsed-literal::

      **TIMEUNIT** (string-valued)
      Time unit; default s

      **TCUNIn** (string-valued)
      Column-specific override

* TIME OFFSET

  It is sometimes convenient to be able to apply a uniform clock correction
  in bulk by putting that number in a single keyword. A second use
  for a time offset is to set a zero offset to a relative time series,
  allowing zero-relative times, or higher precision, in the time stamps.
  Its default value is zero.

  .. parsed-literal::

      **TIMEOFFS** (floating-valued)
      This has global validity

* The absolute, relative errors and time resolution, time binning can be used
  when needed.


The following keywords define the global time informational keywords:

* DATE and DATE-* keywords

  These define the date of HDU creation and observation in ISO-8601.
  ``DATE`` is in UTC if the file is constructed on the Earths surface
  and others are in the time scale given by ``TIMESYS``.

* MJD-* keywords

  These define the same as above, but in ``MJD`` (Modified Julian Date).

The implementation writes a subset of the above FITS keywords, which map
to the Time metadata. Time is intrinsically a coordinate and hence shares
keywords with the ``World Coordinate System`` specification for spatial
coordinates. Therefore, while reading FITS tables with time columns,
the verification that a coordinate column is indeed time is done using
the FITS WCS standard rules and suggestions.
.. currentmodule:: astropy.io.fits

Image Data
**********

In this chapter, we will discuss the data component in an image HDU.


Image Data as an Array
======================

A FITS primary HDU or an image extension HDU may contain image data. The
following discussions apply to both of these HDU classes. For most cases in
``astropy``, it is a ``numpy`` array, having the shape specified by the NAXIS
keywords and the data type specified by the BITPIX keyword  unless the data is
scaled, in which case see the next section. Here is a quick cross reference
between allowed BITPIX values in FITS images and the ``numpy`` data types:

.. parsed-literal::

    **BITPIX**    **Numpy Data Type**
    8         numpy.uint8 (note it is UNsigned integer)
    16        numpy.int16
    32        numpy.int32
    64        numpy.int64
    -32       numpy.float32
    -64       numpy.float64

To recap, in ``numpy`` the arrays are 0-indexed and the axes are
ordered from slow to fast. So, if a FITS image has NAXIS1=300 and NAXIS2=400,
the ``numpy`` array of its data will have the shape of (400, 300).

Examples
--------

..
  EXAMPLE START
  Image Data as an Array in astropy.io.fits

Here is a summary of reading and updating image data values::

    >>> from astropy.io import fits
    >>> fits_image_filename = fits.util.get_testdata_filepath('test0.fits')

    >>> with fits.open(fits_image_filename) as hdul:  # open a FITS file
    ...     data = hdul[1].data  # assume the first extension is an image
    >>> print(data[1, 4])   # get the pixel value at x=5, y=2
    313
    >>> # get values of the subsection from x=11 to 20, y=31 to 40 (inclusive)
    >>> data[30:40, 10:20]
    array([[314, 314, 313, 312, 313, 313, 313, 313, 313, 312],
           [314, 314, 312, 313, 313, 311, 313, 312, 312, 314],
           [314, 315, 313, 313, 313, 313, 315, 312, 314, 312],
           [314, 313, 313, 314, 311, 313, 313, 313, 313, 313],
           [313, 314, 312, 314, 312, 314, 314, 315, 313, 313],
           [312, 311, 311, 312, 312, 312, 312, 313, 311, 312],
           [314, 314, 314, 314, 312, 313, 314, 314, 314, 311],
           [314, 313, 312, 313, 313, 314, 312, 312, 311, 314],
           [313, 313, 313, 314, 313, 313, 315, 313, 312, 313],
           [314, 313, 313, 314, 313, 312, 312, 314, 310, 314]], dtype=int16)
    >>> data[1,4] = 999  # update a pixel value
    >>> data[30:40, 10:20] = 0  # update values of a subsection
    >>> data[3] = data[2]    # copy the 3rd row to the 4th row

Here are some more complicated examples by using the concept of the "mask
array." The first example is to change all negative pixel values in ``data`` to
zero. The second one is to take logarithm of the pixel values which are
positive::

    >>> data[data < 0] = 0
    >>> import numpy as np
    >>> data[data > 0] = np.log(data[data > 0])

These examples show the concise nature of ``numpy`` array operations.

..
  EXAMPLE END


Scaled Data
===========

Sometimes an image is scaled; that is, the data stored in the file is not the
image's physical (true) values, but linearly transformed according to the
equation:

.. parsed-literal::

    physical value = BSCALE \* (storage value) + BZERO

BSCALE and BZERO are stored as keywords of the same names in the header of the
same HDU. The most common use of a scaled image is to store unsigned 16-bit
integer data because the FITS standard does not allow it. In this case, the
stored data is signed 16-bit integer (BITPIX=16) with BZERO=32768
(:math:`2^{15}`), BSCALE=1.


Reading Scaled Image Data
-------------------------

Images are scaled only when either of the BSCALE/BZERO keywords are present in
the header and either of their values is not the default value (BSCALE=1,
BZERO=0).

For unscaled data, the data attribute of an HDU in ``astropy`` is a ``numpy``
array of the same data type specified by the BITPIX keyword. For a scaled
image, the ``.data`` attribute will be the physical data (i.e., already
transformed from the storage data and may not be the same data type as
prescribed in BITPIX). This means an extra step of copying is needed and thus
the corresponding memory requirement. This also means that the advantage of
memory mapping is reduced for scaled data.

For floating point storage data, the scaled data will have the same data type.
For integer data type, the scaled data will always be single precision floating
point (``numpy.float32``).

Example
^^^^^^^

..
  EXAMPLE START
  Reading Scaled Image Data with astropy.io.fits

Here is an example of what happens to scaled data, before and after the data is
touched::

    >>> fits_scaledimage_filename = fits.util.get_testdata_filepath('scale.fits')

    >>> hdul = fits.open(fits_scaledimage_filename)
    >>> hdu = hdul[0]
    >>> hdu.header['bitpix']
    16
    >>> hdu.header['bzero']
    1500.0
    >>> hdu.data[0, 0]  # once data is touched, it is scaled  #  doctest: +FLOAT_CMP
    557.7563
    >>> hdu.data.dtype.name
    'float32'
    >>> hdu.header['bitpix']  # BITPIX is also updated
    -32
    >>> # BZERO and BSCALE are removed after the scaling
    >>> hdu.header['bzero']
    Traceback (most recent call last):
        ...
    KeyError: "Keyword 'BZERO' not found."

.. warning::

    An important caveat to be aware of when dealing with scaled data in
    ``astropy``, is that when accessing the data via the ``.data`` attribute,
    the data is automatically scaled with the BZERO and BSCALE parameters. If
    the file was opened in "update" mode, it will be saved with the rescaled
    data. This surprising behavior is a compromise to err on the side of not
    losing data: if some floating point calculations were made on the data,
    rescaling it when saving could result in a loss of information.

    To prevent this automatic scaling, open the file with the
    ``do_not_scale_image_data=True`` argument to ``fits.open()``. This is
    especially useful for updating some header values, while ensuring that the
    data is not modified.

    You may also manually reapply scale parameters by using ``hdu.scale()``
    (see below). Alternately, you may open files with the ``scale_back=True``
    argument. This assures that the original scaling is preserved when saving
    even when the physical values are updated. In other words, it reapplies
    the scaling to the new physical values upon saving.

..
  EXAMPLE END


Writing Scaled Image Data
-------------------------

With the extra processing and memory requirement, we discourage the use of
scaled data as much as possible. However, ``astropy`` does provide ways to
write scaled data with the `~ImageHDU.scale` method.

Examples
^^^^^^^^

..
  EXAMPLE START
  Writing Scaled Image Data in astropy.io.fits

To write scaled data with the `~ImageHDU.scale` method::

    >>> # scale the data to Int16 with user specified bscale/bzero
    >>> hdu.scale('int16', bzero=32768)
    >>> # scale the data to Int32 with the min/max of the data range, emits
    >>> # RuntimeWarning: overflow encountered in short_scalars
    >>> hdu.scale('int32', 'minmax')  # doctest: +SKIP
    >>> # scale the data, using the original BSCALE/BZERO, emits
    >>> # RuntimeWarning: invalid value encountered in add
    >>> hdu.scale('int32', 'old')  # doctest: +SKIP
    >>> hdul.close()

The first example above shows how to store an unsigned short integer array.

Caution must be exercised when using the :meth:`~ImageHDU.scale` method.
The :attr:`~ImageHDU.data` attribute of an image HDU, after the
:meth:`~ImageHDU.scale` call, will become the storage values, not the physical
values. So, only call :meth:`~ImageHDU.scale` just before writing out to FITS
files (i.e., calls of :meth:`~HDUList.writeto`, :meth:`~HDUList.flush`, or
:meth:`~HDUList.close`). No further use of the data should be exercised. Here is
an example of what happens to the :attr:`~ImageHDU.data` attribute after the
:meth:`~ImageHDU.scale` call::

    >>> hdu = fits.PrimaryHDU(np.array([0., 1, 2, 3]))
    >>> print(hdu.data)  # doctest: +FLOAT_CMP
    [0. 1. 2. 3.]
    >>> hdu.scale('int16', bzero=32768)
    >>> print(hdu.data)  # now the data has storage values
    [-32768 -32767 -32766 -32765]
    >>> hdu.writeto('new.fits')

..
  EXAMPLE END

.. _data-sections:

Data Sections
=============

When a FITS image HDU's :attr:`~ImageHDU.data` is accessed, either the whole
data is copied into memory (in cases of NOT using memory mapping or if the data
is scaled) or a virtual memory space equivalent to the data size is allocated
(in the case of memory mapping of non-scaled data). If there are several very
large image HDUs being accessed at the same time, the system may run out of
memory.

If a user does not need the entire image(s) at the same time (e.g., processing
the images(s) ten rows at a time), the :attr:`~ImageHDU.section` attribute of an
HDU can be used to alleviate such memory problems.

With ``astropy``'s improved support for memory-mapping, the sections feature is
not as necessary as it used to be for handling very large images. However, if
the image's data is scaled with non-trivial BSCALE/BZERO values, accessing the
data in sections may still be necessary under the current implementation.
Memmap is also insufficient for loading images larger than 2 to 4 GB on a 32-bit
system  in such cases it may be necessary to use sections.

Example
-------

..
  EXAMPLE START
  Data Sections in astropy.io.fits

Here is an example of getting the median image from three input images of the
size 5000x5000.

.. code:: python

    hdul1 = fits.open('file1.fits')
    hdul2 = fits.open('file2.fits')
    hdul3 = fits.open('file3.fits')
    output = np.zeros((5000, 5000))
    for i in range(50):
        j = i * 100
        k = j + 100
        x1 = hdul1[0].section[j:k,:]
        x2 = hdul2[0].section[j:k,:]
        x3 = hdul3[0].section[j:k,:]
        output[j:k, :] = np.median([x1, x2, x3], axis=0)

Data in each :attr:`~ImageHDU.section` does not need to be contiguous for
memory savings to be possible. ``astropy`` will do its best to join together
discontiguous sections of the array while reading as little as possible into
main memory.

Sections cannot currently be assigned. Any modifications made to a data
section are not saved back to the original file.

..
  EXAMPLE END
.. currentmodule:: astropy.io.fits

Less Familiar Objects
*********************

In this chapter, we will discuss less frequently used FITS data structures. They
include ASCII tables, variable length tables, and random access group FITS
files.


ASCII Tables
============

FITS standard supports both binary and ASCII tables. In ASCII tables, all of the
data are stored in a human-readable text form, so it takes up more space and
extra processing to parse the text for numeric data. Depending on how the
columns are formatted, floating point data may also lose precision.

In ``astropy``, the interface for ASCII tables and binary tables is basically
the same (i.e., the data is in the ``.data`` attribute and the ``field()``
method is used to refer to the columns and returns a ``numpy`` array). When
reading the table, ``astropy`` will automatically detect what kind of table it
is.

::

    >>> from astropy.io import fits
    >>> filename = fits.util.get_testdata_filepath('ascii.fits')
    >>> hdul = fits.open(filename)
    >>> hdul[1].data[:1]  # doctest: +SKIP
    FITS_rec([(10.123, 37)],
             dtype=(numpy.record, {'names':['a','b'], 'formats':['S10','S5'], 'offsets':[0,11], 'itemsize':16}))
    >>> hdul[1].data['a']
    array([  10.123,    5.2  ,   15.61 ,    0.   ,  345.   ])
    >>> hdul[1].data.formats
    ['E10.4', 'I5']
    >>> hdul.close()

Note that the formats in the record array refer to the raw data which are ASCII
strings (therefore 'a11' and 'a5'), but the ``.formats`` attribute of data
retains the original format specifications ('E10.4' and 'I5').

.. _creating_ascii_table:

Creating an ASCII Table
-----------------------

Creating an ASCII table from scratch is similar to creating a binary table. The
difference is in the Column definitions. The columns/fields in an ASCII table
are more limited than in a binary table. It does not allow more than one
numerical value in a cell. Also, it only supports a subset of what is allowed
in a binary table, namely character strings, integer, and (single and double
precision) floating point numbers. Boolean and complex numbers are not allowed.

The format syntax (the values of the TFORM keywords) is different from that of a
binary table. They are:

.. parsed-literal::

    Aw         Character string
    Iw         (Decimal) Integer
    Fw.d       Double precision real
    Ew.d       Double precision real, in exponential notation
    Dw.d       Double precision real, in exponential notation

where w is the width, and d the number of digits after the decimal point. The
syntax difference between ASCII and binary tables can be confusing. For example,
a field of 3-character string is specified as '3A' in a binary table and as
'A3' in an ASCII table.

The other difference is the need to specify the table type when using the
:meth:`TableHDU.from_columns` method, and that `Column` should be provided the
``ascii=True`` argument in order to be unambiguous.

.. note::

    Although binary tables are more common in most FITS files, earlier versions
    of the FITS format only supported ASCII tables. That is why the class
    :class:`TableHDU` is used for representing ASCII tables specifically,
    whereas :class:`BinTableHDU` is more explicit that it represents a binary
    table. These names come from the value ``XTENSION`` keyword in the tables'
    headers, which is ``TABLE`` for ASCII tables and ``BINTABLE`` for binary
    tables.

:meth:`TableHDU.from_columns` can be used like so::

    >>> import numpy as np

    >>> a1 = np.array(['abcd', 'def'])
    >>> r1 = np.array([11., 12.])
    >>> col1 = fits.Column(name='abc', format='A3', array=a1, ascii=True)
    >>> col2 = fits.Column(name='def', format='E', array=r1, bscale=2.3,
    ...                    bzero=0.6, ascii=True)
    >>> col3 = fits.Column(name='t1', format='I', array=[91, 92, 93], ascii=True)
    >>> hdu = fits.TableHDU.from_columns([col1, col2, col3])
    >>> hdu.data
    FITS_rec([('abc', 11.0, 91), ('def', 12.0, 92), ('', 0.0, 93)],
             dtype=(numpy.record, [('abc', 'S3'), ('def', 'S15'), ('t1', 'S10')]))

It should be noted that when the formats of the columns are unambiguously
specific to ASCII tables it is not necessary to specify ``ascii=True`` in
the :class:`ColDefs` constructor. In this case there *is* ambiguity because
the format code ``'I'`` represents a 16-bit integer in binary tables, while in
ASCII tables it is not technically a valid format. ASCII table format codes
technically require a character width for each column, such as ``'I10'`` to
create a column that can hold integers up to 10 characters wide.

However, ``astropy`` allows the width specification to be omitted in some cases.
When it is omitted from ``'I'`` format columns the minimum width needed to
accurately represent all integers in the column is used. The only problem with
using this shortcut is its ambiguity with the binary table ``'I'`` format, so
specifying ``ascii=True`` is a good practice (though ``astropy`` will still
figure out what you meant in most cases).


Variable Length Array Tables
============================

The FITS standard also supports variable length array tables. The basic idea is
that sometimes it is desirable to have tables with cells in the same field
(column) that have the same data type but have different lengths/dimensions.
Compared with the standard table data structure, the variable length table can
save storage space if there is a large dynamic range of data lengths in
different cells.

A variable length array table can have one or more fields (columns) which are
variable length. The rest of the fields (columns) in the same table can still
be regular, fixed-length ones. ``astropy`` will automatically detect what kind
of field it is during reading; no special action is needed from the user. The
data type specification (i.e., the value of the TFORM keyword) uses an extra
letter 'P' and the format is:

.. parsed-literal::

    rPt(max)

where ``r`` may be 0 or 1 (typically omitted, as it is not applicable to
variable length arrays), ``t`` is one of the letter codes for basic data types
(L, B, I, J, etc.; currently, the X format is not supported for variable length
array field in ``astropy``), and ``max`` is the maximum number of elements of
any array in the column. So, for a variable length field of int16, the
corresponding format spec is, for example, 'PJ(100)'.

Example
-------

..
  EXAMPLE START
  Accessing Variable Length Array Columns in FITS Tables

This example shows a variable length array field of data type int16::

    >>> filename = fits.util.get_testdata_filepath('variable_length_table.fits')
    >>> hdul = fits.open(filename)
    >>> hdul[1].header['tform1']
    'PI(3)'
    >>> print(hdul[1].data.field(0))
    [array([45, 56], dtype=int16) array([11, 12, 13], dtype=int16)]
    >>> hdul.close()

In this field the first row has one element, the second row has two elements,
etc. Accessing variable length fields is almost identical to regular fields,
except that operations on the whole field simultaneously are usually not
possible. A user has to process the field row by row as though they are
independent arrays.

..
  EXAMPLE END


Creating a Variable Length Array Table
--------------------------------------

Creating a variable length table is almost identical to creating a regular
table. The only difference is in the creation of field definitions which are
variable length arrays. First, the data type specification will need the 'P'
letter, and secondly, the field data must be an objects array (as included in
the ``numpy`` module).

Example
^^^^^^^

..
  EXAMPLE START
  Creating a Variable Length Array Column in a FITS Table

Here is an example of creating a table with two fields; one is regular and the
other a variable length array::

    >>> col1 = fits.Column(
    ...    name='var', format='PI()',
    ...    array=np.array([[45, 56], [11, 12, 13]], dtype=np.object_))
    >>> col2 = fits.Column(name='xyz', format='2I', array=[[11, 3], [12, 4]])
    >>> hdu = fits.BinTableHDU.from_columns([col1, col2])
    >>> data = hdu.data
    >>> data  # doctest: +SKIP
    FITS_rec([([45, 56], [11,  3]), ([11, 12, 13], [12,  4])],
             dtype=(numpy.record, [('var', '<i4', (2,)), ('xyz', '<i2', (2,))]))
    >>> hdu.writeto('variable_length_table.fits')
    >>> with fits.open('variable_length_table.fits') as hdul:
    ...     print(repr(hdul[1].header))
    XTENSION= 'BINTABLE'           / binary table extension
    BITPIX  =                    8 / array data type
    NAXIS   =                    2 / number of array dimensions
    NAXIS1  =                   12 / length of dimension 1
    NAXIS2  =                    2 / length of dimension 2
    PCOUNT  =                   10 / number of group parameters
    GCOUNT  =                    1 / number of groups
    TFIELDS =                    2 / number of table fields
    TTYPE1  = 'var     '
    TFORM1  = 'PI(3)   '
    TTYPE2  = 'xyz     '
    TFORM2  = '2I      '

..
  EXAMPLE END

.. _random-groups:

Random Access Groups
====================

Another less familiar data structure supported by the FITS standard is the
random access group. This convention was established before the binary table
extension was introduced. In most cases its use can now be superseded by the
binary table. It is mostly used in radio interferometry.

Like primary HDUs, a Random Access Group HDU is always the first HDU of a FITS
file. Its data has one or more groups. Each group may have any number
(including 0) of parameters, together with an image. The parameters and the
image have the same data type.

All groups in the same HDU have the same data structure, that is, same data type
(specified by the keyword BITPIX, as in image HDU), same number of parameters
(specified by PCOUNT), and the same size and shape (specified by NAXISn
keywords) of the image data. The number of groups is specified by GCOUNT and
the keyword NAXIS1 is always 0. Thus the total data size for a Random Access
Group HDU is:

.. parsed-literal::

    \|BITPIX\| \* GCOUNT \* (PCOUNT + NAXIS2 \* NAXIS3 \* ... \* NAXISn)


Header and Summary
------------------

Accessing the header of a Random Access Group HDU is no different from any
other HDU; you can use the .header attribute.

The content of the HDU can similarly be summarized by using the
:meth:`HDUList.info` method::

    >>> filename = fits.util.get_testdata_filepath('group.fits')
    >>> hdul = fits.open(filename)
    >>> hdul[0].header['groups']
    True
    >>> hdul[0].header['gcount']
    10
    >>> hdul[0].header['pcount']
    3
    >>> hdul.info()
    Filename: ...group.fits
    No.    Name      Ver    Type      Cards   Dimensions   Format
      0  PRIMARY       1 GroupsHDU       15   (5, 3, 1, 1)   float32   10 Groups  3 Parameters


Data: Group Parameters
----------------------

The data part of a Random Access Group HDU is, like other HDUs, in the
``.data`` attribute. It includes both parameter(s) and image array(s).

Examples
^^^^^^^^

..
  EXAMPLE START
  Group Parameters in Random Access Group HDUs

To show the contents of the third group, including parameters and data::

    >>> hdul[0].data[2]  # doctest: +FLOAT_CMP
    (2.0999999, 42.0, 42.0, array([[[[30., 31., 32., 33., 34.],
             [35., 36., 37., 38., 39.],
             [40., 41., 42., 43., 44.]]]], dtype=float32))

The data first lists all of the parameters, then the image array, for the
specified group(s). As a reminder, the image data in this file has the shape of
(1,1,1,4,3) in Python or C convention, or (3,4,1,1,1) in IRAF or Fortran
convention.

To access the parameters, first find out what the parameter names are, with the
``.parnames`` attribute::

    >>> hdul[0].data.parnames # get the parameter names
    ['abc', 'xyz', 'xyz']

The group parameter can be accessed by the :meth:`~GroupData.par` method. Like
the table :meth:`~FITS_rec.field` method, the argument can be either index or
name::

    >>> hdul[0].data.par(0)[8]  # Access group parameter by name or by index  # doctest: +FLOAT_CMP
    8.1
    >>> hdul[0].data.par('abc')[8]  # doctest: +FLOAT_CMP
    8.1

Note that the parameter name 'xyz' appears twice. This is a feature in the
random access group, and it means to add the values together. Thus::

    >>> hdul[0].data.parnames  # get the parameter names
    ['abc', 'xyz', 'xyz']
    >>> hdul[0].data.par(1)[8]  # Duplicate parameter name 'xyz'
    42.0
    >>> hdul[0].data.par(2)[8]
    42.0
    >>> # When accessed by name, it adds the values together if the name is
    >>> # shared by more than one parameter
    >>> hdul[0].data.par('xyz')[8]
    84.0

The :meth:`~GroupData.par` is a method for either the entire data object or one
data item (a group). So there are two possible ways to get a group parameter
for a certain group, this is similar to the situation in table data (with its
:meth:`~FITS_rec.field` method)::

    >>> hdul[0].data.par(0)[8]  # doctest: +FLOAT_CMP
    8.1
    >>> hdul[0].data[8].par(0)  # doctest: +FLOAT_CMP
    8.1

On the other hand, to modify a group parameter, we can either assign the new
value directly (if accessing the row/group number last) or use the
:meth:`~Group.setpar` method (if accessing the row/group number first). The
method :meth:`~Group.setpar` is also needed for updating by name if the
parameter is shared by more than one parameters::

    >>> # Update group parameter when selecting the row (group) number last
    >>> hdul[0].data.par(0)[8] = 99.
    >>> # Update group parameter when selecting the row (group) number first
    >>> hdul[0].data[8].setpar(0, 99.)  # or:
    >>> hdul[0].data[8].setpar('abc', 99.)
    >>> # Update group parameter by name when the name is shared by more than
    >>> # one parameters, the new value must be a tuple of constants or
    >>> # sequences
    >>> hdul[0].data[8].setpar('xyz', (2445729., 0.3))
    >>> hdul[0].data[8:].par('xyz')  # doctest: +FLOAT_CMP
    array([2.44572930e+06, 8.40000000e+01])

..
  EXAMPLE END

Data: Image Data
----------------

The image array of the data portion is accessible by the
:attr:`~GroupData.data` attribute of the data object. A ``numpy`` array is
returned::

    >>> print(hdul[0].data.data[8])  # doctest: +FLOAT_CMP
    [[[[120. 121. 122. 123. 124.]
       [125. 126. 127. 128. 129.]
       [130. 131. 132. 133. 134.]]]]
    >>> hdul.close()


Creating a Random Access Group HDU
----------------------------------

To create a Random Access Group HDU from scratch, use :class:`GroupData` to
encapsulate the data into the group data structure, and use :class:`GroupsHDU`
to create the HDU itself.

Example
^^^^^^^

..
  EXAMPLE START
  Creating a Random Access Group HDU in a FITS File

To create a Random Access Group HDU::

    >>> # Create the image arrays. The first dimension is the number of groups.
    >>> imdata = np.arange(150.0).reshape(10, 1, 1, 3, 5)
    >>> # Next, create the group parameter data, we'll have two parameters.
    >>> # Note that the size of each parameter's data is also the number of
    >>> # groups.
    >>> # A parameter's data can also be a numeric constant.
    >>> pdata1 = np.arange(10) + 0.1
    >>> pdata2 = 42
    >>> # Create the group data object, put parameter names and parameter data
    >>> # in lists assigned to their corresponding arguments.
    >>> # If the data type (bitpix) is not specified, the data type of the
    >>> # image will be used.
    >>> x = fits.GroupData(imdata, bitpix=-32,
    ...                    parnames=['abc', 'xyz', 'xyz'],
    ...                    pardata=[pdata1, pdata2, pdata2])
    >>> # Now, create the GroupsHDU and write to a FITS file.
    >>> hdu = fits.GroupsHDU(x)
    >>> hdu.writeto('test_group.fits')
    >>> hdu.header
    SIMPLE  =                    T / conforms to FITS standard
    BITPIX  =                  -32 / array data type
    NAXIS   =                    5 / number of array dimensions
    NAXIS1  =                    0
    NAXIS2  =                    5
    NAXIS3  =                    3
    NAXIS4  =                    1
    NAXIS5  =                    1
    EXTEND  =                    T
    GROUPS  =                    T / has groups
    PCOUNT  =                    3 / number of parameters
    GCOUNT  =                   10 / number of groups
    PTYPE1  = 'abc     '
    PTYPE2  = 'xyz     '
    PTYPE3  = 'xyz     '
    >>> data = hdu.data
    >>> hdu.data  # doctest: +SKIP
    GroupData([ (0.1       , 42., 42., [[[[  0.,   1.,   2.,   3.,   4.], [  5.,   6.,   7.,   8.,   9.], [ 10.,  11.,  12.,  13.,  14.]]]]),
               (1.10000002, 42., 42., [[[[ 15.,  16.,  17.,  18.,  19.], [ 20.,  21.,  22.,  23.,  24.], [ 25.,  26.,  27.,  28.,  29.]]]]),
               (2.0999999 , 42., 42., [[[[ 30.,  31.,  32.,  33.,  34.], [ 35.,  36.,  37.,  38.,  39.], [ 40.,  41.,  42.,  43.,  44.]]]]),
               (3.0999999 , 42., 42., [[[[ 45.,  46.,  47.,  48.,  49.], [ 50.,  51.,  52.,  53.,  54.], [ 55.,  56.,  57.,  58.,  59.]]]]),
               (4.0999999 , 42., 42., [[[[ 60.,  61.,  62.,  63.,  64.], [ 65.,  66.,  67.,  68.,  69.], [ 70.,  71.,  72.,  73.,  74.]]]]),
               (5.0999999 , 42., 42., [[[[ 75.,  76.,  77.,  78.,  79.], [ 80.,  81.,  82.,  83.,  84.], [ 85.,  86.,  87.,  88.,  89.]]]]),
               (6.0999999 , 42., 42., [[[[ 90.,  91.,  92.,  93.,  94.], [ 95.,  96.,  97.,  98.,  99.], [100., 101., 102., 103., 104.]]]]),
               (7.0999999 , 42., 42., [[[[105., 106., 107., 108., 109.], [110., 111., 112., 113., 114.], [115., 116., 117., 118., 119.]]]]),
               (8.10000038, 42., 42., [[[[120., 121., 122., 123., 124.], [125., 126., 127., 128., 129.], [130., 131., 132., 133., 134.]]]]),
               (9.10000038, 42., 42., [[[[135., 136., 137., 138., 139.], [140., 141., 142., 143., 144.], [145., 146., 147., 148., 149.]]]])],
               dtype=(numpy.record, [('abc', '<f4'), ('xyz', '<f4'), ('_xyz', '<f4'), ('DATA', '<f4', (1, 1, 3, 5))]))

..
  EXAMPLE END

Compressed Image Data
=====================
.. _astropy-io-fits-compressedImageData:

A general technique has been developed for storing compressed image data in
FITS binary tables. The principle used in this convention is to first divide
the n-dimensional image into a rectangular grid of sub-images or 'tiles'.
Each tile is then compressed as a continuous block of data, and the resulting
compressed byte stream is stored in a row of a variable length column in a
FITS binary table. Several commonly used algorithms for compressing image
tiles are supported. These include Gzip, Rice, IRAF Pixel List (PLIO), and
Hcompress.

For more details, reference "A FITS Image Compression Proposal" from:

    https://www.adass.org/adass/proceedings/adass99/P2-42/

and "Registered FITS Convention, Tiled Image Compression Convention":

    https://fits.gsfc.nasa.gov/registry/tilecompression.html

Compressed image data is accessed, in ``astropy``, using the optional
``astropy.io.fits.compression`` module contained in a C shared library
(compression.so). If an attempt is made to access an HDU containing compressed
image data when the compression module is not available, the user is notified
of the problem and the HDU is treated like a standard binary table HDU. This
notification will only be made the first time compressed image data is
encountered. In this way, the compression module is not required in order for
``astropy`` to work.


Header and Summary
------------------

In ``astropy``, the header of a compressed image HDU appears to the user like
any image header. The actual header stored in the FITS file is that of a binary
table HDU with a set of special keywords, defined by the convention, to
describe the structure of the compressed image. The conversion between binary
table HDU header and image HDU header is all performed behind the scenes.
Since the HDU is actually a binary table, it may not appear as a primary HDU in
a FITS file.

Example
^^^^^^^

..
  EXAMPLE START
  Accessing Compressed FITS Image HDU Headers

The content of the HDU header may be accessed using the ``.header`` attribute::

    >>> filename = fits.util.get_testdata_filepath('compressed_image.fits')

    >>> hdul = fits.open(filename)
    >>> hdul[1].header
    XTENSION= 'IMAGE   '           / Image extension
    BITPIX  =                   16 / data type of original image
    NAXIS   =                    2 / dimension of original image
    NAXIS1  =                   10 / length of original image axis
    NAXIS2  =                   10 / length of original image axis
    PCOUNT  =                    0 / number of parameters
    GCOUNT  =                    1 / number of groups

The contents of the corresponding binary table HDU may be accessed using the
hidden ``._header`` attribute. However, all user interface with the HDU header
should be accomplished through the image header (the ``.header`` attribute)::

    >>> hdul[1]._header
    XTENSION= 'BINTABLE'           / binary table extension
    BITPIX  =                    8 / array data type
    NAXIS   =                    2 / number of array dimensions
    NAXIS1  =                    8 / width of table in bytes
    NAXIS2  =                   10 / number of rows in table
    PCOUNT  =                   60 / number of group parameters
    GCOUNT  =                    1 / number of groups
    TFIELDS =                    1 / number of fields in each row
    TTYPE1  = 'COMPRESSED_DATA'    / label for field 1
    TFORM1  = '1PB(6)  '           / data format of field: variable length array
    ZIMAGE  =                    T / extension contains compressed image
    ZTENSION= 'IMAGE   '           / Image extension
    ZBITPIX =                   16 / data type of original image
    ZNAXIS  =                    2 / dimension of original image
    ZNAXIS1 =                   10 / length of original image axis
    ZNAXIS2 =                   10 / length of original image axis
    ZPCOUNT =                    0 / number of parameters
    ZGCOUNT =                    1 / number of groups
    ZTILE1  =                   10 / size of tiles to be compressed
    ZTILE2  =                    1 / size of tiles to be compressed
    ZCMPTYPE= 'RICE_1  '           / compression algorithm
    ZNAME1  = 'BLOCKSIZE'          / compression block size
    ZVAL1   =                   32 / pixels per block
    ZNAME2  = 'BYTEPIX '           / bytes per pixel (1, 2, 4, or 8)
    ZVAL2   =                    2 / bytes per pixel (1, 2, 4, or 8)
    EXTNAME = 'COMPRESSED_IMAGE'   / name of this binary table extension

The contents of the HDU can be summarized by using either the :func:`info`
convenience function or method::

    >>> fits.info(filename)
    Filename: ...compressed_image.fits
    No.    Name      Ver    Type      Cards   Dimensions   Format
      0  PRIMARY       1 PrimaryHDU       4   ()
      1  COMPRESSED_IMAGE    1 CompImageHDU      7   (10, 10)   int16

    >>> hdul.info()
    Filename: ...compressed_image.fits
    No.    Name      Ver    Type      Cards   Dimensions   Format
      0  PRIMARY       1 PrimaryHDU       4   ()
      1  COMPRESSED_IMAGE    1 CompImageHDU      7   (10, 10)   int16

..
  EXAMPLE END

Data
----

As with the header, the data of a compressed image HDU appears to the user as
standard uncompressed image data. The actual data is stored in the FITS file
as Binary Table data containing at least one column (COMPRESSED_DATA). Each
row of this variable length column contains the byte stream that was generated
as a result of compressing the corresponding image tile. Several optional
columns may also appear. These include UNCOMPRESSED_DATA to hold the
uncompressed pixel values for tiles that cannot be compressed, ZSCALE and ZZERO
to hold the linear scale factor and zero point offset which may be needed to
transform the raw uncompressed values back to the original image pixel values,
and ZBLANK to hold the integer value used to represent undefined pixels (if
any) in the image.

Example
^^^^^^^

..
  EXAMPLE START
  Accessing Compressed FITS Image HDU Data

The contents of the uncompressed HDU data may be accessed using the ``.data``
attribute::

    >>> hdul[1].data
    array([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9],
           [10, 11, 12, 13, 14, 15, 16, 17, 18, 19],
           [20, 21, 22, 23, 24, 25, 26, 27, 28, 29],
           [30, 31, 32, 33, 34, 35, 36, 37, 38, 39],
           [40, 41, 42, 43, 44, 45, 46, 47, 48, 49],
           [50, 51, 52, 53, 54, 55, 56, 57, 58, 59],
           [60, 61, 62, 63, 64, 65, 66, 67, 68, 69],
           [70, 71, 72, 73, 74, 75, 76, 77, 78, 79],
           [80, 81, 82, 83, 84, 85, 86, 87, 88, 89],
           [90, 91, 92, 93, 94, 95, 96, 97, 98, 99]], dtype=int16)
    >>> hdul.close()

The compressed data can be accessed via the ``.compressed_data`` attribute, but
this rarely needs be accessed directly. It may be useful for performing direct
copies of the compressed data without needing to decompress it first.

..
  EXAMPLE END


Creating a Compressed Image HDU
-------------------------------

To create a compressed image HDU from scratch, construct a
:class:`CompImageHDU` object from an uncompressed image data array and its
associated image header. From there, the HDU can be treated just like any
other image HDU.

Example
^^^^^^^

..
  EXAMPLE START
  Creating a Compressed FITS Image HDU

To create a compressed image HDU::

    >>> imageData = np.arange(100).astype('i2').reshape(10, 10)
    >>> imageHeader = fits.Header()
    >>> hdu = fits.CompImageHDU(imageData, imageHeader)
    >>> hdu.writeto('compressed_image.fits')

The API documentation for the :class:`CompImageHDU` initializer method
describes the possible options for constructing a :class:`CompImageHDU` object.

..
  EXAMPLE END
Executable Scripts
******************

``astropy`` installs a couple of useful utility programs on your system that are
built with ``astropy``.

fitsinfo
========
.. automodule:: astropy.io.fits.scripts.fitsinfo

fitsheader
==========
.. automodule:: astropy.io.fits.scripts.fitsheader

fitscheck
=========
.. automodule:: astropy.io.fits.scripts.fitscheck

With ``astropy`` installed, please run ``fitscheck --help`` to see the full
program usage documentation.

.. _fitsdiff:

fitsdiff
========

.. currentmodule:: astropy.io.fits

``fitsdiff`` provides a thin command-line wrapper around the :class:`FITSDiff`
interface. It outputs the report from a :class:`FITSDiff` of two FITS files,
and like common diff-like commands returns a 0 status code if no differences
were found, and 1 if differences were found:

With ``astropy`` installed, please run ``fitsdiff --help`` to see the full
program usage documentation.
.. currentmodule:: astropy.io.fits

..  _fits_io_verification:

Verification
************

``astropy`` has built in a flexible scheme to verify FITS data conforming to
the FITS standard. The basic verification philosophy in ``astropy`` is to be
tolerant with input and strict with output.

When ``astropy`` reads a FITS file which does not conform to FITS standard, it
will not raise an error and exit. It will try to make the best educated
interpretation and only gives up when the offending data is accessed and no
unambiguous interpretation can be reached.

On the other hand, when writing to an output FITS file, the content to be
written must be strictly compliant to the FITS standard by default. This
default behavior can be overwritten by several other options, so the user will
not be held up because of a minor standard violation.


FITS Standard
=============

Since FITS standard is a "loose" standard, there are many places the violation
can occur and to enforce them all will be almost impossible. It is not uncommon
for major observatories to generate data products which are not 100% FITS
compliant. Some observatories have also developed their own nonstandard
dialect and some of these are so prevalent that they have become de facto
standards. Examples include the long string value and the use of the CONTINUE
card.

The violation of the standard can happen at different levels of the data
structure. ``astropy``'s verification scheme is developed on these hierarchical
levels. Here are the three ``astropy`` verification levels:

1. The HDU List

2. Each HDU

3. Each Card in the HDU Header

These three levels correspond to the three categories of objects:
:class:`HDUList`, any HDU (e.g., :class:`PrimaryHDU`, :class:`ImageHDU`, etc.),
and :class:`Card`. They are the only objects having the ``verify()`` method.
Most other classes in `astropy.io.fits` do not have a ``verify()`` method.

If ``verify()`` is called at the HDU List level, it verifies standard
compliance at all three levels, but a call of ``verify()`` at the Card level
will only check the compliance of that Card. Since ``astropy`` is tolerant when
reading a FITS file, no ``verify()`` is called on input. On output,
``verify()`` is called with the most restrictive option as the default.


Verification Options
====================

There are several options accepted by all verify(option) calls in ``astropy``.
In addition, they available for the ``output_verify`` argument of the following
methods: ``close()``, ``writeto()``, and ``flush()``. In these cases, they are
passed to a ``verify()`` call within these methods. The available options are:

**exception**

This option will raise an exception if any FITS standard is violated. This is
the default option for output (i.e., when ``writeto()``, ``close()``, or
``flush()`` is called). If a user wants to overwrite this default on output, the
other options listed below can be used.

**warn**

This option is the same as the ignore option but will send warning messages. It
will not try to fix any FITS standard violations whether fixable or not.

**ignore**

This option will ignore any FITS standard violation. On output, it will write
the HDU List content to the output FITS file, whether or not it is conforming
to the FITS standard.

The ignore option is useful in the following situations:

1. An input FITS file with nonstandard formatting is read and the user wants
   to copy or write out to an output file. The nonstandard formatting will be
   preserved in the output file.

2. A user wants to create a nonstandard FITS file on purpose, possibly for
   testing or consistency.

No warning message will be printed out. This is like a silent warning option
(see below).

**fix**

This option will try to fix any FITS standard violations. It is not always
possible to fix such violations. In general, there are two kinds of FITS
standard violations: fixable and non-fixable. For example, if a keyword has a
floating number with an exponential notation in lower case 'e' (e.g., 1.23e11)
instead of the upper case 'E' as required by the FITS standard, it is a fixable
violation. On the other hand, a keyword name like 'P.I.' is not fixable, since
it will not know what to use to replace the disallowed periods. If a violation
is fixable, this option will print out a message noting it is fixed. If it is
not fixable, it will throw an exception.

The principle behind fixing is to do no harm. For example, it is plausible to
'fix' a Card with a keyword name like 'P.I.' by deleting it, but ``astropy``
will not take such action to hurt the integrity of the data.

Not all fixes may be the "correct" fix, but at least ``astropy`` will try to
make the fix in such a way that it will not throw off other FITS readers.

**silentfix**

Same as fix, but will not print out informative messages. This may be useful in
a large script where the user does not want excessive harmless messages. If the
violation is not fixable, it will still throw an exception.

In addition the following combined options are available:

 * **fix+ignore**
 * **fix+warn**
 * **fix+exception**
 * **silentfix+ignore**
 * **silentfix+warn**
 * **silentfix+exception**

These options combine the semantics of the basic options. For example,
``silentfix+exception`` is actually equivalent to just ``silentfix`` in that
fixable errors will be fixed silently, but any unfixable errors will raise an
exception. On the other hand, ``silentfix+warn`` will issue warnings for
unfixable errors, but will stay silent about any fixed errors.


Verifications at Different Data Object Levels
=============================================

We will examine what ``astropy``'s verification does at the three different
levels:


Verification at HDUList
-----------------------

At the HDU List level, the verification is only for two simple cases:

1. Verify that the first HDU in the HDU list is a primary HDU. This is a
   fixable case. The fix is to insert a minimal primary HDU into the HDU list.

2. Verify the second or later HDU in the HDU list is not a primary HDU.
   Violation will not be fixable.


Verification at Each HDU
------------------------

For each HDU, the mandatory keywords, their locations in the header, and their
values will be verified. Each FITS HDU has a fixed set of required keywords in
a fixed order. For example, the primary HDU's header must at least have the
following keywords:

.. parsed-literal::

    SIMPLE =                     T /
    BITPIX =                     8 /
    NAXIS  =                     0

If any of the mandatory keywords are missing or in the wrong order, the fix
option will fix them::

    >>> from astropy.io import fits
    >>> filename = fits.util.get_testdata_filepath('verify.fits')
    >>> hdul = fits.open(filename)
    >>> hdul[0].header
    SIMPLE  =                    T / conforms to FITS standard
    NAXIS   =                    0 / NUMBER OF AXES
    BITPIX  =                    8 / BITS PER PIXEL
    >>> hdul[0].verify('fix') # doctest: +SHOW_WARNINGS
    VerifyWarning: Verification reported errors:
    VerifyWarning: 'BITPIX' card at the wrong place (card 2).
      Fixed by moving it to the right place (card 1).
    VerifyWarning: Note: astropy.io.fits uses zero-based indexing.
    >>> hdul[0].header           # voila!
    SIMPLE  =                    T / conforms to FITS standard
    BITPIX  =                    8 / BITS PER PIXEL
    NAXIS   =                    0 / NUMBER OF AXES
    >>> hdul.close()

Verification at Each Card
-------------------------

The lowest level, the Card, also has the most complicated verification
possibilities.

Examples
^^^^^^^^

..
  EXAMPLE START
  Verification at Each Card in astropy.io.fits

Here is a list of fixable and not fixable Cards:

Fixable Cards:

1. Floating point numbers with lower case 'e' or 'd'::

    >>> from astropy.io import fits
    >>> c = fits.Card.fromstring('FIX1    = 2.1e23')
    >>> c.verify('silentfix')
    >>> print(c)
    FIX1    =               2.1E23

2. The equal sign is before column nine in the card image::

    >>> c = fits.Card.fromstring('FIX2= 2')
    >>> c.verify('silentfix')
    >>> print(c)
    FIX2    =                    2

3. String value without enclosing quotes::

    >>> c = fits.Card.fromstring('FIX3    = string value without quotes')
    >>> c.verify('silentfix')
    >>> print(c)
    FIX3    = 'string value without quotes'

4. Missing equal sign before column nine in the card image.

5. Space between numbers and E or D in floating point values::

    >>> c = fits.Card.fromstring('FIX5    = 2.4 e 03')
    >>> c.verify('silentfix')
    >>> print(c)
    FIX5    =               2.4E03

6. Unparsable values will be "fixed" as a string::

    >>> c = fits.Card.fromstring('FIX6    = 2 10 ')
    >>> c.verify('fix+warn') # doctest: +SHOW_WARNINGS
    VerifyWarning: Verification reported errors:
    VerifyWarning: Card 'FIX6' is not FITS standard
     (invalid value string: '2 10').
       Fixed 'FIX6' card to meet the FITS standard.
    VerifyWarning: Note: astropy.io.fits uses zero-based indexing.
    >>> print(c)
    FIX6    = '2 10    '

Unfixable Cards:

1. Illegal characters in keyword name.

We will summarize the verification with a "life-cycle" example::

    >>> h = fits.PrimaryHDU()  # create a PrimaryHDU
    >>> # Try to add an non-standard FITS keyword 'P.I.' (FITS does no allow
    >>> # '.' in the keyword), if using the update() method - doesn't work!
    >>> h.header['P.I.'] = 'Hubble' # doctest: +SHOW_WARNINGS
    VerifyWarning: Keyword name 'P.I.' is greater than 8 characters or
     contains characters not allowed by the FITS standard;
      a HIERARCH card will be created.
    >>> # Have to do it the hard way (so a user will not do this by accident)
    >>> # First, create a card image and give verbatim card content (including
    >>> # the proper spacing, but no need to add the trailing blanks)
    >>> c = fits.Card.fromstring("P.I. = 'Hubble'")
    >>> h.header.append(c)  # then append it to the header
    >>> # Now if we try to write to a FITS file, the default output
    >>> # verification will not take it.
    >>> h.writeto('pi.fits')  # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
     ...
    VerifyError: HDU 0:
        Card 5:
            Card 'P.I. ' is not FITS standard (equal sign not at column 8).
            Illegal keyword name 'P.I. '
    >>> # Must set the output_verify argument to 'ignore', to force writing a
    >>> # non-standard FITS file
    >>> h.writeto('pi.fits', output_verify='ignore')
    >>> # Now reading a non-standard FITS file
    >>> # astropy.io.fits is magnanimous in reading non-standard FITS files
    >>> hdul = fits.open('pi.fits')
    >>> hdul[0].header # doctest: +SHOW_WARNINGS
    SIMPLE  =            T / conforms to FITS standard
    BITPIX  =            8 / array data type
    NAXIS   =            0 / number of array dimensions
    EXTEND  =            T
    HIERARCH P.I. = 'Hubble  '
    P.I.    = 'Hubble  '
    VerifyWarning: Verification reported errors:
    VerifyWarning: Card 'P.I. ' is not FITS standard (equal sign
     not at column 8).  Fixed 'P.I. ' card to meet the FITS standard.
    VerifyWarning: Unfixable error: Illegal keyword name 'P.I. '
    VerifyWarning: Note: astropy.io.fits uses zero-based indexing.
    >>> # even when you try to access the offending keyword, it does NOT
    >>> # complain
    >>> hdul[0].header['p.i.']
    'Hubble'
    >>> # But if you want to make sure if there is anything wrong/non-standard,
    >>> # use the verify() method
    >>> hdul.verify() # doctest: +SHOW_WARNINGS
    VerifyWarning: Verification reported errors:
    VerifyWarning: HDU 0:
    VerifyWarning:     Card 5:
    VerifyWarning:         Illegal keyword name 'P.I. '
    VerifyWarning: Note: astropy.io.fits uses zero-based indexing.
    >>> hdul.close()

..
  EXAMPLE END

Verification Using the FITS Checksum Keyword Convention
=======================================================

The North American FITS committee has reviewed the FITS Checksum Keyword
Convention for possible adoption as a FITS Standard. This convention provides
an integrity check on information contained in FITS HDUs. The convention
consists of two header keyword cards: CHECKSUM and DATASUM. The CHECKSUM
keyword is defined as an ASCII character string whose value forces the 32-bit
1's complement checksum accumulated over all the 2880-byte FITS logical records
in the HDU to equal negative zero. The DATASUM keyword is defined as a
character string containing the unsigned integer value of the 32-bit 1's
complement checksum of the data records in the HDU. Verifying the
accumulated checksum is still equal to negative zero provides a fairly reliable
way to determine that the HDU has not been modified by subsequent data
processing operations or corrupted while copying or storing the file on
physical media.

In order to avoid any impact on performance, by default ``astropy`` will not
verify HDU checksums when a file is opened or generate checksum values when a
file is written. In fact, CHECKSUM and DATASUM cards are automatically removed
from HDU headers when a file is opened, and any CHECKSUM or DATASUM cards are
stripped from headers when an HDU is written to a file. In order to verify the
checksum values for HDUs when opening a file, the user must supply the checksum
keyword argument in the call to the open convenience function with a value of
True. When this is done, any checksum verification failure will cause a
warning to be issued (via the warnings module). If checksum verification is
requested in the open, and no CHECKSUM or DATASUM cards exist in the HDU
header, the file will open without comment. Similarly, in order to output the
CHECKSUM and DATASUM cards in an HDU header when writing to a file, the user
must supply the checksum keyword argument with a value of True in the call to
the ``writeto()`` function. It is possible to write only the DATASUM card to the
header by supplying the checksum keyword argument with a value of 'datasum'.

Examples
--------

..
  EXAMPLE START
  Verification Using the FITS Checksum Keyword Convention

To verify the checksum values for HDUs when opening a file::

    >>> # Open the file checksum.fits verifying the checksum values for all HDUs
    >>> filename = fits.util.get_testdata_filepath('checksum.fits')
    >>> hdul = fits.open(filename, checksum=True)
    >>> hdul.close()
    >>> # Open the file in.fits where checksum verification fails
    >>> filename = fits.util.get_testdata_filepath('checksum_false.fits')
    >>> hdul = fits.open(filename, checksum=True) # doctest: +SHOW_WARNINGS
    AstropyUserWarning: Checksum verification failed for HDU ('PRIMARY', 1).
    AstropyUserWarning: Datasum verification failed for HDU ('PRIMARY', 1).
    AstropyUserWarning: Checksum verification failed for HDU ('RATE', 1).
    AstropyUserWarning: Datasum verification failed for HDU ('RATE', 1).
    >>> # Create file out.fits containing an HDU constructed from data
    >>> # containing both CHECKSUM and DATASUM cards.
    >>> data = hdul[0].data
    >>> fits.writeto('out.fits', data=data, checksum=True)
    >>> hdun = fits.open('out.fits', checksum=True)
    >>> hdun.close()

    >>> # Create file out.fits containing all the HDUs in the HDULIST
    >>> # hdul with each HDU header containing only the DATASUM card
    >>> hdul.writeto('out2.fits', checksum='datasum')

    >>> # Create file out.fits containing the HDU hdu with both CHECKSUM
    >>> # and DATASUM cards in the header
    >>> hdu = hdul[1]
    >>> hdu.writeto('out3.fits', checksum=True)

    >>> # Append a new HDU constructed from array data to the end of
    >>> # the file existingfile.fits with only the appended HDU
    >>> # containing both CHECKSUM and DATASUM cards.
    >>> fits.append('out3.fits', data, checksum=True)
    >>> hdul.close()

..
  EXAMPLE END
.. currentmodule:: astropy.io.fits

.. _images:

Images
******

`ImageHDU`
==========

.. autoclass:: ImageHDU
   :members:
   :inherited-members:
   :show-inheritance:

`CompImageHDU`
==============

.. autoclass:: CompImageHDU
   :members:
   :inherited-members:
   :show-inheritance:

`Section`
---------

.. autoclass:: Section
   :members:
   :inherited-members:
   :show-inheritance:
Differs
*******

.. automodule:: astropy.io.fits.diff
.. currentmodule:: astropy.io.fits

:class:`FITSDiff`
=================
.. autoclass:: FITSDiff
   :members:
   :inherited-members:
   :show-inheritance:

:class:`HDUDiff`
================
.. autoclass:: HDUDiff
   :members:
   :inherited-members:
   :show-inheritance:

:class:`HeaderDiff`
===================
.. autoclass:: HeaderDiff
   :members:
   :inherited-members:
   :show-inheritance:

:class:`ImageDataDiff`
======================
.. autoclass:: ImageDataDiff
   :members:
   :inherited-members:
   :show-inheritance:

:class:`RawDataDiff`
====================
.. autoclass:: RawDataDiff
   :members:
   :inherited-members:
   :show-inheritance:

:class:`TableDataDiff`
======================
.. autoclass:: TableDataDiff
   :members:
   :inherited-members:
   :show-inheritance:
.. currentmodule:: astropy.io.fits

Headers
*******

:class:`Header`
===============

.. autoclass:: Header
   :members:
   :inherited-members:
   :undoc-members:
   :show-inheritance:
.. currentmodule:: astropy.io.fits

Cards
*****

:class:`Card`
=============

.. autoclass:: Card
   :members:
   :inherited-members:
   :undoc-members:
   :show-inheritance:
.. currentmodule:: astropy.io.fits

.. _tables:

Tables
******

:class:`BinTableHDU`
====================
.. autoclass:: BinTableHDU
   :members:
   :inherited-members:
   :show-inheritance:

:class:`TableHDU`
=================
.. autoclass:: TableHDU
   :members:
   :inherited-members:
   :show-inheritance:

:class:`Column`
===============
.. autoclass:: Column
   :members:
   :inherited-members:
   :show-inheritance:

:class:`ColDefs`
================
.. autoclass:: ColDefs
   :members:
   :inherited-members:
   :show-inheritance:

:class:`FITS_rec`
=================
.. autoclass:: FITS_rec
   :members:
   :show-inheritance:

:class:`FITS_record`
====================
.. autoclass:: FITS_record
   :members:
   :inherited-members:
   :show-inheritance:


Table Functions
===============

:func:`tabledump`
-----------------
.. autofunction:: tabledump

:func:`tableload`
-----------------
.. autofunction:: tableload

:func:`table_to_hdu`
--------------------
.. autofunction:: table_to_hdu
.. currentmodule:: astropy.io.fits

File Handling and Convenience Functions
***************************************

:func:`open`
============
.. autofunction:: open

:func:`writeto`
===============
.. autofunction:: writeto

:func:`info`
============
.. autofunction:: info

:func:`printdiff`
=================
.. autofunction:: printdiff

:func:`append`
==============
.. autofunction:: append

:func:`update`
==============
.. autofunction:: update

:func:`getdata`
===============
.. autofunction:: getdata

:func:`getheader`
=================
.. autofunction:: getheader

:func:`getval`
==============
.. autofunction:: getval

:func:`setval`
==============
.. autofunction:: setval

:func:`delval`
==============
.. autofunction:: delval
.. currentmodule:: astropy.io.fits

Header Data Unit
****************

Header Data Units are the fundamental container structure of the FITS format
consisting of a ``data`` member and its associated metadata in a ``header``.
They are defined in ``astropy.io.fits.hdu``.

The :class:`ImageHDU` and :class:`CompImageHDU` classes are discussed in the
section on :ref:`Images`.

The :class:`TableHDU` and :class:`BinTableHDU` classes are discussed in the
section on :ref:`Tables`.

:class:`PrimaryHDU`
===================
.. autoclass:: PrimaryHDU
   :members:
   :inherited-members:
   :show-inheritance:

:class:`GroupsHDU`
==================
.. autoclass:: GroupsHDU
   :members:
   :inherited-members:
   :show-inheritance:

:class:`GroupData`
==================
.. autoclass:: GroupData
   :members:
   :show-inheritance:

:class:`Group`
--------------
.. autoclass:: Group
   :members:
   :show-inheritance:

:class:`StreamingHDU`
=====================
.. autoclass:: StreamingHDU
   :members:
   :inherited-members:
   :show-inheritance:
.. currentmodule:: astropy.io.fits

HDU Lists
*********

.. inheritance-diagram:: HDUList

:class:`HDUList`
================

.. autoclass:: HDUList
   :members:
   :undoc-members:
   :show-inheritance:
.. currentmodule:: astropy.io.fits

.. _verify:

Verification Options
********************

There are five options for the ``output_verify`` argument of the following
methods of :class:`HDUList`: :meth:`~HDUList.close`, :meth:`~HDUList.writeto`,
and :meth:`~HDUList.flush`, or the ``_BaseHDU.writeto`` method on any HDU
object. In these cases, the verification option is passed to a ``verify``
call within these methods.

``'exception'``
===============

This option will raise an exception if any FITS standard is violated. This is
the default option for output (i.e., when :meth:`~HDUList.writeto`,
:meth:`~HDUList.close`, or :meth:`~HDUList.flush` is called). If a user wants to
overwrite this default on output, the other options listed below can be used.

``'ignore'``
============

This option will ignore any FITS standard violation. On output, it will write
the HDU List content to the output FITS file, whether or not it is conforming
to FITS standard.

The ``ignore`` option is useful in these situations, for example:

  1. An input FITS file with non-standard is read and the user wants to copy or
     write out after some modification to an output file. The non-standard will
     be preserved in such output file.

  2. A user wants to create a non-standard FITS file on purpose, possibly for
     testing purpose.

No warning message will be printed out. This is like a silent warn (see below)
option.

``'fix'``
=========

This option will try to fix any FITS standard violations. It is not always
possible to fix such violations. In general, there are two kinds of FITS
standard violations: fixable and not fixable. For example, if a keyword has a
floating number with an exponential notation in lower case 'e' (e.g., 1.23e11)
instead of the upper case 'E' as required by the FITS standard, it is a fixable
violation. On the other hand, a keyword name like ``P.I.`` is not fixable,
since it will not know what to use to replace the disallowed periods. If a
violation is fixable, this option will print out a message noting it is fixed.
If it is not fixable, it will throw an exception.

The principle behind the fixing is do no harm. For example, it is plausible to
'fix' a :class:`Card` with a keyword name like ``P.I.`` by deleting it, but
``astropy`` will not take such action to hurt the integrity of the data.

Not all fixes may be the "correct" fix, but at least ``astropy`` will try to
make the fix in such a way that it will not throw off other FITS readers.

``'silentfix'``
===============

Same as fix, but will not print out informative messages. This may be useful in
a large script where the the user does not want excessive harmless messages. If
the violation is not fixable, it will still throw an exception.

``'warn'``
==========

This option is the same as the ignore option but will send warning messages. It
will not try to fix any FITS standard violations whether fixable or not.
.. doctest-skip-all

astropy.io.fits History
***********************

Prior to its inclusion in Astropy, the `astropy.io.fits` package was a stand-
alone package called `PyFITS`_.  PyFITS is no longer actively maintained, and
its development is now solely in Astropy.
This page documents the release history of PyFITS prior to its merge into
Astropy.

.. contents:: PyFITS Changelog
   :depth: 2
   :local:


3.4.0 (2016-01-29)
==================

This is the last released version of PyFITS as a standalone package.


3.3.0 (2014-07-17)
==================

New Features
------------

- Added new verification options ``fix+ignore``, ``fix+warn``,
  ``fix+exception``, ``silentfix+ignore``, ``silentfix+warn``, and
  ``silentfix+exception`` which give more control over how to report fixable
  errors as opposed to unfixable errors.  See the "Verification" section in
  the PyFITS documentation for more details.

API Changes
-----------

- The ``pyfits.new_table`` function is now fully deprecated (though will not
  be removed for a long time, considering how widely it is used).

  Instead please use the more explicit ``pyfits.BinTableHDU.from_columns`` to
  create a new binary table HDU, and the similar
  ``pyfits.TableHDU.from_columns`` to create a new ASCII table.  These
  otherwise accept the same arguments as ``pyfits.new_table`` which is now
  just a wrapper for these.

- The ``.fromstring`` classmethod of each HDU type has been simplified such
  that, true to its namesake, it only initializes an HDU from a string
  containing its header *and* data. (spacetelescope/PyFITS#64)

- Fixed an issue where header wildcard matching (for example
  ``header['DATE*']``) can be used to match *any* characters that might appear
  in a keyword.  Previously this only matched keywords containing characters
  in the set ``[0-9A-Za-z_]``.  Now this can also match a hyphen ``-`` and any
  other characters, as some conventions like ``HIERARCH`` and record-valued
  keyword cards allow a wider range of valid characters than standard FITS
  keywords.

- This will be the *last* release to support the following APIs that have been
  marked deprecated since PyFITS v3.1:

  - The ``CardList`` class, which was part of the old header implementation.

  - The ``Card.key`` attribute.  Use ``Card.keyword`` instead.

  - The ``Card.cardimage`` and ``Card.ascardimage`` attributes.  Use simply
    ``Card.image`` or ``str(card)`` instead.

  - The ``create_card`` factory function.  Simply use the normal ``Card``
    constructor instead.

  - The ``create_card_from_string`` factory function.  Use ``Card.fromstring``
    instead.

  - The ``upper_key`` function.  Use ``Card.normalize_keyword`` method instead
    (this is not unlikely to be used outside of PyFITS itself, but it was
    technically public API).

  - The usage of ``Header.update`` with ``Header.update(keyword, value,
    comment)`` arguments.  ``Header.update`` should only be used analogously
    to ``dict.update``.  Use ``Header.set`` instead.

  - The ``Header.ascard`` attribute.  Use ``Header.cards`` instead for a list
    of all the ``Card`` objects in the header.

  - The ``Header.rename_key`` method.  Use ``Header.rename_keyword`` instead.

  - The ``Header.get_history`` method.  Use ``header['HISTORY']`` instead
    (normal keyword lookup).

  - The ``Header.get_comment`` method.  Use ``header['COMMENT']`` instead.

  - The ``Header.toTxtFile`` method.  Use ``header.totextfile`` instead.

  - The ``Header.fromTxtFile`` method.  Use ``Header.fromtextfile`` instead.

  - The ``pyfits.tdump`` and ``tcreate`` functions.  Use ``pyfits.tabledump``
    and ``pyfits.tableload`` respectively.

  - The ``BinTableHDU.tdump`` and ``tcreate`` methods.  Use
    ``BinTableHDU.dump`` and ``BinTableHDU.load`` respectively.

  - The ``txtfile`` argument to the ``Header`` constructor.  Use
    ``Header.fromfile`` instead.

  - The ``startColumn`` and ``endColumn`` arguments to the ``FITS_record``
    constructor.  These are unlikely to be used by any user code.

  These deprecated interfaces will be removed from the development version of
  PyFITS following the v3.3 release (they will still be available in any
  v3.3.x bugfix releases, however).

Other Changes and Additions
---------------------------

- PyFITS has switched to a unified code base which supports Python 2.5 through
  3.4 simultaneously without translation.  This *shouldn't* have any
  significant performance impacts, but please report if anything seems
  noticeably slower.  As a reminder, support for Python 2.5 will be ended
  after PyFITS 3.3.x.

- Warnings for deprecated APIs in PyFITS are now always displayed by default.
  This is in line with a similar change made recently to Astropy:
  https://github.com/astropy/astropy/pull/1871
  To disable PyFITS deprecation warnings in scripts one may call
  ``pyfits.ignore_deprecation_warnings()`` after importing PyFITS.

- ``Card`` objects have a new ``is_blank`` attribute which returns ``True`` if
  the card represents a blank card (no keyword, value, or comment) and
  ``False`` otherwise.

Bug Fixes
---------

- Fixed a regression where it was not possible to save an empty "compressed"
  image to a file (in this case there is nothing to compress, hence the
  quotes, but trying to do so caused a crash). (spacetelescope/PyFITS#69)

- Fixed a regression that may have been introduced in v3.2.1 with writing
  compressed image HDUs, particularly compressed images using a non-empty
  GZIP_COMPRESSED_DATA column. (spacetelescope/#71)


3.2.4 (2014-06-02)
==================

- Fixed a regression where multiple consecutive calls of the ``writeto``
  method on the same HDU but to different files could lead to corrupt data or
  crashes on the subsequent calls after the first. (spacetelescope/PyFITS#40)


3.2.3 (2014-05-14)
==================

- Nominal support for Python 3.4.

- Fixed a bug with using the ``tabledump`` and ``tableload`` functions with
  tables containing array columns (columns in which each element is an array
  instead of a single scalar value). (spacetelescope/PyFITS#22)

- Fixed an issue where PyFITS allowed newline characters in header values and
  comments. (spacetelescope/PyFITS#51)

- Fixed pickling of ``FITS_rec`` (table data) objects.
  (spacetelescope/PyFITS#53)

- Improved behavior when writing large compressed images on OSX by removing an
  unnecessary check for platform architecture. (spacetelescope/PyFITS#57)

- Allow reading FITS files from file-like objects that do not have a
  ``.closed`` attribute (and as such may not even have an "open" vs. "closed"
  concept). (spacetelescope/PyFITS#56)

- Fixed duplicate insertion of commentary keywords on compressed image
  headers. (spacetelescope/PyFITS#58)

- Fixed minor issue with comparison of header commentary card values.
  (spacetelescope/PyFITS#59)


3.1.6 (2014-05-14)
==================

- Nominal support for Python 3.4.

- Fixed a bug with using the ``tabledump`` and ``tableload`` functions with
  tables containing array columns (columns in which each element is an array
  instead of a single scalar value). (Backported from 3.2.3)

- Fixed an issue where PyFITS allowed newline characters in header values and
  comments. (Backported from 3.2.3)

- Fixed pickling of ``FITS_rec`` (table data) objects.
  (Backported from 3.2.3)

- Improved behavior when writing large compressed images on OSX by removing an
  unnecessary check for platform architecture. (Backported from 3.2.3)

- Allow reading FITS files from file-like objects that do not have a
  ``.closed`` attribute (and as such may not even have an "open" vs. "closed"
  concept). (Backported from 3.2.3)

- Fixed minor issue with comparison of header commentary card values.
  (Backported from 3.2.3)


3.2.2 (2014-03-25)
==================

- Fixed a regression on deletion of record-valued keyword cards using
  the Header wildcard syntax.  This was intended to be fixed before the
  v3.2.1 release.


3.1.5 (2014-03-25)
==================

- Fixed a regression on deletion of record-valued keyword cards using
  the Header wildcard syntax.  This was intended to be fixed before the
  v3.1.4 release.


3.2.1 (2014-03-04)
==================

- Nominal support for the upcoming Python 3.4.

- Added missing features from the ``Header.insert()`` method that were
  intended for inclusion in the original 3.1 release:  In addition to
  accepting an integer index as the first argument, it also supports supplying
  a keyword name as the first argument for insertion relative to a specific
  keyword.  It also now supports an optional ``after`` argument.  If
  ``after=True`` the insertion is made below the insertion point instead
  of above it. (spacetelescope/PyFITS#12)

- Fixed support for broadcasting of values assigned to table columns.
  (spacetelescope/PyFITS#48)

- A grab bag of minor performance improvements in headers.
  (spacetelescope/PyFITS#46)

- Fix an unrelated error that occurred when instantiating a ``ColDefs`` object
  with invalid input.

- Fixed an issue where opening an image containing pseudo-unsigned integers
  and immediately writing it to a new file using the ``writeto`` method would
  drop the scale factors that identified the data as unsigned.

- Fixed a bug where writing a file with ``checksum=True`` did not add the
  checksum on new files. (spacetelescope/PyFITS#8)

- Fixed an issue where validating an HDU's checksums removed the checksum from
  that HDU's header entirely (even if it was valid.)

- Fixed checksums on compressed images, so that the ``ZHECKSUM`` and
  ``ZDATASUM`` contain a checksum of the original image HDU, while
  ``CHECKSUM`` and ``DATASUM`` contain checksums of the compressed image HDU.
  This feature was supposed to be supported in 3.2, but the support was buggy.

- Fixed an issue where the size of the heap was sometimes not computed
  properly when writing an existing table containing variable-length array
  columns to a new FITS file.  This could result in corruption in the new FITS
  file. (spacetelescope/PyFITS#47)

- Fixed issue with updates to the header of ``CompImageHDU`` objects not being
  preserved on save. (spacetelescope/PyFITS#23)

- Fixed a bug where a boolean value of ``True`` in a header could not be
  replaced with the integer 1, and likewise for ``False`` and 0 and vice
  versa.

- Fixed an issue similar to the above one but for numeric values--now
  replacing a header value with an equivalent numeric value will up/downcast
  that value.  For example replacing '0' with '0.0' will write '0.0' to the
  header so that it is returned as a floating point value.  Likewise a float
  can be downcast to an integer. (spacetelescope/PyFITS#49)

- A handful of Python 3 compatibility fixes, especially for compatibility
  with the upcoming Python 3.4.

- Fixed unrelated crash when a header contains an invalid END card (for
  example "END = ").  This resulted in a cryptic traceback.  Now headers like
  this will detect "clearly intended" END cards and produce a warning about
  their invalidity and fix them. (#217)

- Allowed a sequence of ``Column`` objects to be passed in as the main
  argument to ``FITS_rec.from_columns`` as the documentation suggests should
  be possible.

- Fixed a display formatting issue with fitsdiff where sometimes it did not
  show the difference between two floating point numbers if they were the same
  up to some low number of digits. (spacetelescope/PyFITS#21)

- Fixed an issue where Python 2 sometimes allowed non-ASCII strings to be
  assigned as header values if they were assigned as old-style ``str`` objects
  and not ``unicode`` objects. (spacetelescope/PyFITS#37)


3.1.4 (2014-03-04)
==================

- Added missing features from the ``Header.insert()`` method that were
  intended for inclusion in the original 3.1 release:  In addition to
  accepting an integer index as the first argument, it also supports supplying
  a keyword name as the first argument for insertion relative to a specific
  keyword.  It also now supports an optional ``after`` argument.  If
  ``after=True`` the insertion is made below the insertion point instead
  of above it. (Backported from 3.2.1)

- A grab bag of minor performance improvements in headers.
  (Backported from 3.2.1)

- Fixed an issue where opening an image containing pseudo-unsigned integers
  and immediately writing it to a new file using the ``writeto`` method would
  drop the scale factors that identified the data as unsigned.
  (Backported from 3.2.1)

- Fixed a bug where writing a file with ``checksum=True`` did not add the
  checksum on new files. (Backported from 3.2.1)

- Fixed an issue where validating an HDU's checksums removed the checksum from
  that HDU's header entirely (even if it was valid.)
  (Backported from 3.2.1)

- Fixed an issue where the size of the heap was sometimes not computed
  properly when writing an existing table containing variable-length array
  columns to a new FITS file.  This could result in corruption in the new FITS
  file. (Backported from 3.2.1)

- Fixed a bug where a boolean value of ``True`` in a header could not be
  replaced with the integer 1, and likewise for ``False`` and 0 and vice
  versa. (Backported from 3.2.1)

- Fixed an issue similar to the above one but for numeric values--now
  replacing a header value with an equivalent numeric value will up/downcast
  that value.  For example replacing '0' with '0.0' will write '0.0' to the
  header so that it is returned as a floating point value.  Likewise a float
  can be downcast to an integer. (Backported from 3.2.1)

- Fixed unrelated crash when a header contains an invalid END card (for
  example "END = ").  This resulted in a cryptic traceback.  Now headers like
  this will detect "clearly intended" END cards and produce a warning about
  their invalidity and fix them. (Backported from 3.2.1)

- Fixed a display formatting issue with fitsdiff where sometimes it did not
  show the difference between two floating point numbers if they were the same
  up to some low number of digits. (Backported from 3.2.1)

- Fixed an issue where Python 2 sometimes allowed non-ASCII strings to be
  assigned as header values if they were assigned as old-style ``str`` objects
  and not ``unicode`` objects. (Backported from 3.2.1)


3.0.13 (2014-03-04)
===================

- Fixed a bug where writing a file with ``checksum=True`` did not add the
  checksum on new files. (Backported from 3.2.1)

- Fixed an issue where validating an HDU's checksums removed the checksum from
  that HDU's header entirely (even if it was valid.)
  (Backported from 3.2.1)


3.2 (2013-11-26)
================

Highlights
----------

- Rewrote CFITSIO-based backend for handling tile compression of FITS files.
  It now uses a standard CFITSIO instead of heavily modified pieces of CFITSIO
  as before.  PyFITS ships with its own copy of CFITSIO v3.35 which supports
  the latest version of the Tiled Image Convention (v2.3), but system
  packagers may choose instead to strip this out in favor of a
  system-installed version of CFITSIO.  Earlier versions may work, but nothing
  earlier than 3.28 has been tested yet. (#169)

- Added support for reading and writing tables using the Q format for columns.
  The Q format is identical to the P format (variable-length arrays) except
  that it uses 64-bit integers for the data descriptors, allowing more than
  4 GB of variable-length array data in a single table. (#160)

- Added initial support for table columns containing pseudo-unsigned integers.
  This is currently enabled by using the ``uint=True`` option when opening
  files; any table columns with the correct BZERO value will be interpreted
  and returned as arrays of unsigned integers.

- Some refactoring of the table and ``FITS_rec`` modules in order to better
  separate the details of the FITS binary and ASCII table data structures from
  the HDU data structures that encapsulate them.  Most of these changes should
  not be apparent to users (but see API Changes below).


API Changes
-----------

- Assigning to values in ``ColDefs.names``, ``ColDefs.formats``,
  ``ColDefs.nulls`` and other attributes of ``ColDefs`` instances that return
  lists of column properties is no longer supported.  Assigning to those lists
  will no longer update the corresponding columns.  Instead, please just
  modify the ``Column`` instances directly (``Column.name``, ``Column.null``,
  etc.)

- The ``pyfits.new_table`` function is marked "pending deprecation".  This
  does not mean it will be removed outright or that its functionality has
  changed.  It will likely be replaced in the future for a function with
  similar, if not subtly different functionality.  A better, if not slightly
  more verbose approach is to use ``pyfits.FITS_rec.from_columns`` to create
  a new ``FITS_rec`` table--this has the same interface as
  ``pyfits.new_table``.  The difference is that it returns a plan ``FITS_rec``
  array, and not an HDU instance.  This ``FITS_rec`` object can then be used
  as the data argument in the constructors for ``BinTableHDU`` (for binary
  tables) or ``TableHDU`` (for ASCII tables).  This is analogous to creating
  an ``ImageHDU`` by passing in an image array.
  ``pyfits.FITS_rec.from_columns`` is just a simpler way of creating a
  FITS-compatible recarray from a FITS column specification.

- The ``updateHeader``, ``updateHeaderData``, and ``updateCompressedData``
  methods of the ``CompDataHDU`` class are pending deprecation and moved to
  internal methods.  The operation of these methods depended too much on
  internal state to be used safely by users; instead they are invoked
  automatically in the appropriate places when reading/writing compressed image
  HDUs.

- The ``CompDataHDU.compData`` attribute is pending deprecation in favor of
  the clearer and more PEP-8 compatible ``CompDataHDU.compressed_data``.

- The constructor for ``CompDataHDU`` has been changed to accept new keyword
  arguments.  The new keyword arguments are essentially the same, but are in
  underscore_separated format rather than camelCase format.  The old arguments
  are still pending deprecation.

- The internal attributes of HDU classes ``_hdrLoc``, ``_datLoc``, and
  ``_datSpan`` have been replaced with ``_header_offset``, ``_data_offset``,
  and ``_data_size`` respectively.  The old attribute names are still pending
  deprecation.  This should only be of interest to advanced users who have
  created their own HDU subclasses.

- The following previously deprecated functions and methods have been removed
  entirely: ``createCard``, ``createCardFromString``, ``upperKey``,
  ``ColDefs.data``, ``setExtensionNameCaseSensitive``, ``_File.getfile``,
  ``_TableBaseHDU.get_coldefs``, ``Header.has_key``, ``Header.ascardlist``.

  If you run your code with a previous version of PyFITS (>= 3.0, < 3.2) with
  the ``python -Wd`` argument, warnings for all deprecated interfaces still in
  use will be displayed.

- Interfaces that were pending deprecation are now fully deprecated.  These
  include: ``create_card``, ``create_card_from_string``, ``upper_key``,
  ``Header.get_history``, and ``Header.get_comment``.

- The ``.name`` attribute on HDUs is now directly tied to the HDU's header, so
  that if ``.header['EXTNAME']`` changes so does ``.name`` and vice-versa.

- The ``pyfits.file.PYTHON_MODES`` constant dict was renamed to
  ``pyfits.file.PYFITS_MODES`` which better reflects its purpose.  This is
  rarely used by client code, however.  Support for the old name will be
  removed by PyFITS 3.4.


Other Changes and Additions
---------------------------

- The new compression code also adds support for the ZQUANTIZ and ZDITHER0
  keywords added in more recent versions of this FITS Tile Compression spec.
  This includes support for lossless compression with GZIP. (#198) By default
  no dithering is used, but the ``SUBTRACTIVE_DITHER_1`` and
  ``SUBTRACTIVE_DITHER_2`` methods can be enabled by passing the correct
  constants to the ``quantize_method`` argument to the ``CompImageHDU``
  constructor.  A seed can be manually specified, or automatically generated
  using either the system clock or checksum-based methods via the
  ``dither_seed`` argument.  See the documentation for ``CompImageHDU`` for
  more details. (#198) (spacetelescope/PYFITS#32)

- Images compressed with the Tile Compression standard can now be larger than
  4 GB through support of the Q format. (#159)

- All HDUs now have a ``.ver`` ``.level`` attribute that returns the value of
  the EXTVAL and EXTLEVEL keywords from that HDU's header, if the exist.  This
  was added for consistency with the ``.name`` attribute which returns the
  EXTNAME value from the header.

- Then ``Column`` and ``ColDefs`` classes have new ``.dtype`` attributes
  which give the Numpy dtype for the column data in the first case, and the
  full Numpy compound dtype for each table row in the latter case.

- There was an issue where new tables created defaulted the values in all
  string columns to '0.0'.  Now string columns are filled with empty strings
  by default--this seems a less surprising default, but it may cause
  differences with tables created with older versions of PyFITS.

- Improved round-tripping and preservation of manually assigned column
  attributes (``TNULLn``, ``TSCALn``, etc.) in table HDU headers.
  (astropy/astropy#996)


Bug Fixes
---------

- Binary tables containing compressed images may, optionally, contain other
  columns unrelated to the tile compression convention. Although this is an
  uncommon use case, it is permitted by the standard. (#159)

- Reworked some of the file I/O routines to allow simpler, more consistent
  mapping between OS-level file modes ('rb', 'wb', 'ab', etc.) and the more
  "PyFITS-specific" modes used by PyFITS like "readonly" and "update".
  That is, if reading a FITS file from an open file object, it doesn't matter
  as much what "mode" it was opened in so long as it has the right
  capabilities (read/write/etc.)  Also works around bugs in the Python io
  module in 2.6+ with regard to file modes. (spacetelescope/PyFITS#33)

- Fixed an obscure issue that can occur on systems that don't have flush to
  memory-mapped files implemented (namely GNU Hurd). (astropy/astropy#968)


3.1.3 (2013-11-26)
==================

- Disallowed assigning NaN and Inf floating point values as header values,
  since the FITS standard does not define a way to represent them in. Because
  this is undefined, the previous behavior did not make sense and produced
  invalid FITS files. (spacetelescope/PyFITS#11)

- Added a workaround for a bug in 64-bit OSX that could cause truncation when
  writing files greater than 2^32 bytes in size. (spacetelescope/PyFITS#28)

- Fixed a long-standing issue where writing binary tables did not correctly
  write the TFORMn keywords for variable-length array columns (they omitted
  the max array length parameter of the format).  This was thought fixed in
  v3.1.2, but it was only fixed there for compressed image HDUs and not for
  binary tables in general.

- Fixed an obscure issue that can occur on systems that don't have flush to
  memory-mapped files implemented (namely GNU Hurd). (Backported from 3.2)


3.0.12 (2013-11-26)
===================

- Disallowed assigning NaN and Inf floating point values as header values,
  since the FITS standard does not define a way to represent them in. Because
  this is undefined, the previous behavior did not make sense and produced
  invalid FITS files. (Backported from 3.1.3)

- Added a workaround for a bug in 64-bit OSX that could cause truncation when
  writing files greater than 2^32 bytes in size. (Backported from 3.1.3)

- Fixed a long-standing issue where writing binary tables did not correctly
  write the TFORMn keywords for variable-length array columns (they omitted
  the max array length parameter of the format).  This was thought fixed in
  v3.1.2, but it was only fixed there for compressed image HDUs and not for
  binary tables in general. (Backported from 3.1.3)

- Fixed an obscure issue that can occur on systems that don't have flush to
  memory-mapped files implemented (namely GNU Hurd). (Backported from 3.2)


3.1.3 (unreleased)
==================

- Disallowed assigning NaN and Inf floating point values as header values,
  since the FITS standard does not define a way to represent them in. Because
  this is undefined, the previous behavior did not make sense and produced
  invalid FITS files. (spacetelescope/PyFITS#11)


3.0.12 (unreleased)
===================

- Disallowed assigning NaN and Inf floating point values as header values,
  since the FITS standard does not define a way to represent them in. Because
  this is undefined, the previous behavior did not make sense and produced
  invalid FITS files. (Backported from 3.1.3)

- Added a workaround for a bug in 64-bit OSX that could cause truncation when
  writing files greater than 2^32 bytes in size. (Backported from 3.1.3)


3.1.2 (2013-04-22)
==================

- When an error occurs opening a file in fitsdiff the exception message will
  now at least mention which file had the error. (#168)

- Fixed support for opening gzipped FITS files by filename in a writeable mode
  (PyFITS has supported writing to gzip files for some time now, but only
  enabled it when GzipFile objects were passed to ``pyfits.open()`` due to
  some legacy code preventing full gzip support. (#195)

- Added a more helpful error message in the case of malformatted FITS files
  that contain non-float NULL values in an ASCII table but are missing the
  required TNULLn keywords in the header. (#197)

- Fixed an (apparently long-standing) issue where writing compressed images
  did not correctly write the TFORMn keywords for variable-length array
  columns (they omitted the max array length parameter of the format). (#199)

- Slightly refactored how tables containing variable-length array columns are
  handled to add two improvements: Fixes an issue where accessing the data
  after a call to the ``pyfits.getdata`` convenience function caused an
  exception, and allows the VLA data to be read from an existing mmap of the
  FITS file. (#200)

- Fixed a bug that could occur when opening a table containing
  multi-dimensional columns (i.e. via the TDIMn keyword) and then writing it
  out to a new file. (#201)

- Added use of the console_scripts entry point to install the fitsdiff and
  fitscheck scripts, which if nothing else provides better Windows support.
  The generated scripts now override the ones explicitly defined in the
  scripts/ directory (which were just trivial stubs to begin with). (#202)

- Fixed a bug on Python 3 where attempting to open a non-existent file on
  Python 3 caused a seemingly unrelated traceback. (#203)

- Fixed a bug in fitsdiff that reported two header keywords containing NaN
  as value as different. (#204)

- Fixed an issue in the tests that caused some tests to fail if pyfits is
  installed with read-only permissions. (#208)

- Fixed a bug where instantiating a ``BinTableHDU`` from a numpy array
  containing boolean fields converted all the values to ``False``. (#215)

- Fixed an issue where passing an array of integers into the constructor of
  ``Column()`` when the column type is floats of the same byte width caused the
  column array to become garbled. (#218)

- Fixed inconsistent behavior in creating CONTINUE cards from byte strings
  versus Unicode strings in Python 2--CONTINUE cards can now be created
  properly from Unicode strings (so long as they are convertible to ASCII).
  (spacetelescope/PyFITS#1)

- Fixed a couple cases where creating a new table using TDIMn in some of the
  columns could caused a crash. (spacetelescope/PyFITS#3)

- Fixed a bug in parsing HIERARCH keywords that do not have a space after
  the first equals sign (before the value). (spacetelescope/PyFITS#5)

- Prevented extra leading whitespace on HIERARCH keywords from being treated
  as part of the keyword. (spacetelescope/PyFITS#6)

- Fixed a bug where HIERARCH keywords containing lower-case letters was
  mistakenly marked as invalid during header validation.
  (spacetelescope/PyFITS#7)

- Fixed an issue that was ancillary to (spacetelescope/PyFITS#7) where the
  ``Header.index()`` method did not work correctly with HIERARCH keywords
  containing lower-case letters.


3.0.11 (2013-04-17)
===================

- Fixed support for opening gzipped FITS files by filename in a writeable mode
  (PyFITS has supported writing to gzip files for some time now, but only
  enabled it when GzipFile objects were passed to ``pyfits.open()`` due to
  some legacy code preventing full gzip support. Backported from 3.1.2. (#195)

- Added a more helpful error message in the case of malformatted FITS files
  that contain non-float NULL values in an ASCII table but are missing the
  required TNULLn keywords in the header. Backported from 3.1.2. (#197)

- Fixed an (apparently long-standing) issue where writing compressed images did
  not correctly write the TFORMn keywords for variable-length array columns
  (they omitted the max array length parameter of the format). Backported from
  3.1.2. (#199)

- Slightly refactored how tables containing variable-length array columns are
  handled to add two improvements: Fixes an issue where accessing the data
  after a call to the ``pyfits.getdata`` convenience function caused an
  exception, and allows the VLA data to be read from an existing mmap of the
  FITS file. Backported from 3.1.2. (#200)

- Fixed a bug that could occur when opening a table containing
  multi-dimensional columns (i.e. via the TDIMn keyword) and then writing it
  out to a new file. Backported from 3.1.2. (#201)

- Fixed a bug on Python 3 where attempting to open a non-existent file on
  Python 3 caused a seemingly unrelated traceback. Backported from 3.1.2.
  (#203)

- Fixed a bug in fitsdiff that reported two header keywords containing NaN
  as value as different. Backported from 3.1.2. (#204)

- Fixed an issue in the tests that caused some tests to fail if pyfits is
  installed with read-only permissions. Backported from 3.1.2. (#208)

- Fixed a bug where instantiating a ``BinTableHDU`` from a numpy array
  containing boolean fields converted all the values to ``False``. Backported
  from 3.1.2. (#215)

- Fixed an issue where passing an array of integers into the constructor of
  ``Column()`` when the column type is floats of the same byte width caused the
  column array to become garbled. Backported from 3.1.2. (#218)

- Fixed a couple cases where creating a new table using TDIMn in some of the
  columns could caused a crash. Backported from 3.1.2.
  (spacetelescope/PyFITS#3)


3.1.1 (2013-01-02)
==================

This is a bug fix release for the 3.1.x series.

Bug Fixes
---------

- Improved handling of scaled images and pseudo-unsigned integer images in
  compressed image HDUs.  They now work more transparently like normal image
  HDUs with support for the ``do_not_scale_image_data`` and ``uint`` options,
  as well as ``scale_back`` and ``save_backup``.  The ``.scale()`` method
  works better too. (#88)

- Permits non-string values for the EXTNAME keyword when reading in a file,
  rather than throwing an exception due to the malformatting.  Added
  verification for the format of the EXTNAME keyword when writing. (#96)

- Added support for EXTNAME and EXTVER in PRIMARY HDUs.  That is, if EXTNAME
  is specified in the header, it will also be reflected in the ``.name``
  attribute and in ``pyfits.info()``.  These keywords used to be verboten in
  PRIMARY HDUs, but the latest version of the FITS standard allows them.
  (#151)

- HCOMPRESS can again be used to compress data cubes (and higher-dimensional
  arrays) so long as the tile size is effectively 2-dimensional. In fact,
  PyFITS will automatically use compatible tile sizes even if they're not
  explicitly specified. (#171)

- Added support for the optional ``endcard`` parameter in the
  ``Header.fromtextfile()`` and ``Header.totextfile()`` methods.  Although
  ``endcard=False`` was a reasonable default assumption, there are still text
  dumps of FITS headers that include the END card, so this should have been
  more flexible. (#176)

- Fixed a crash when running fitsdiff on two empty (that is, zero row) tables.
  (#178)

- Fixed an issue where opening files containing random groups HDUs in update
  mode could cause an unnecessary rewrite of the file even if none of the
  data is modified. (#179)

- Fixed a bug that could caused a deadlock in the filesystem on OSX if PyFITS
  is used with Numpy 1.7 in some cases. (#180)

- Fixed a crash when generating diff reports from diffs using the
  ``ignore_comments`` options. (#181)

- Fixed some bugs with FITS WCS distortion paper record-valued keyword cards:

  - Cards that looked kind of like RVKCs but were not intended to be were
    over-permissively treated as such--commentary keywords like COMMENT and
    HISTORY were particularly affected. (#183)

  - Looking up a card in a header by its standard FITS keyword only should
    always return the raw value of that card.  That way cards containing
    values that happen to valid RVKCs but were not intended to be will still
    be treated like normal cards. (#184)

  - Looking up a RVKC in a header with only part of the field-specifier (for
    example "DP1.AXIS" instead of "DP1.AXIS.1") was implicitly treated as a
    wildcard lookup. (#184)

- Fixed a crash when diffing two FITS files where at least one contains a
  compressed image HDU which was not recognized as an image instead of a
  table. (#187)

- Fixed bugs in the backwards compatibility layer for the ``CardList.index``
  and ``CardList.count`` methods. (#190)

- Improved ``__repr__`` and text file representation of cards with long values
  that are split into CONTINUE cards. (#193)

- Fixed a crash when trying to assign a long (> 72 character) value to blank
  ('') keywords. This also changed how blank keywords are represented--there
  are still exactly 8 spaces before any commentary content can begin; this
  *may* affect the exact display of header cards that assumed there could be
  fewer spaces in a blank keyword card before the content begins. However, the
  current approach is more in line with the requirements of the FITS standard.
  (#194)


3.0.10 (2013-01-02)
===================

- Improved handling of scaled images and pseudo-unsigned integer images in
  compressed image HDUs.  They now work more transparently like normal image
  HDUs with support for the ``do_not_scale_image_data`` and ``uint`` options,
  as well as ``scale_back`` and ``save_backup``.  The ``.scale()`` method
  works better too.  Backported from 3.1.1. (#88)

- Permits non-string values for the EXTNAME keyword when reading in a file,
  rather than throwing an exception due to the malformatting.  Added
  verification for the format of the EXTNAME keyword when writing.  Backported
  from 3.1.1. (#96)

- Added support for EXTNAME and EXTVER in PRIMARY HDUs.  That is, if EXTNAME
  is specified in the header, it will also be reflected in the ``.name``
  attribute and in ``pyfits.info()``.  These keywords used to be verbotten in
  PRIMARY HDUs, but the latest version of the FITS standard allows them.
  Backported from 3.1.1. (#151)

- HCOMPRESS can again be used to compress data cubes (and higher-dimensional
  arrays) so long as the tile size is effectively 2-dimensional. In fact,
  PyFITS will not automatically use compatible tile sizes even if they're not
  explicitly specified.  Backported from 3.1.1. (#171)

- Fixed a bug when writing out files containing zero-width table columns,
  where the TFIELDS keyword would be updated incorrectly, leaving the table
  largely unreadable.  Backported from 3.1.0. (#174)

- Fixed an issue where opening files containing random groups HDUs in update
  mode could cause an unnecessary rewrite of the file even if none of the
  data is modified.  Backported from 3.1.1. (#179)

- Fixed a bug that could caused a deadlock in the filesystem on OSX if PyFITS
  is used with Numpy 1.7 in some cases. Backported from 3.1.1. (#180)


3.1 (2012-08-08)
================

Highlights
----------

- The ``Header`` object has been significantly reworked, and ``CardList``
  objects are now deprecated (their functionality folded into the ``Header``
  class).  See API Changes below for more details.

- Memory maps are now used by default to access HDU data.  See API Changes
  below for more details.

- Now includes a new version of the ``fitsdiff`` program for comparing two
  FITS files, and a new FITS comparison API used by ``fitsdiff``.  See New
  Features below.

API Changes
-----------

- The ``Header`` class has been rewritten, and the ``CardList`` class is
  deprecated.  Most of the basic details of working with FITS headers are
  unchanged, and will not be noticed by most users.  But there are differences
  in some areas that will be of interest to advanced users, and to application
  developers.  For full details of the changes, see the "Header Interface
  Transition Guide" section in the PyFITS documentation.  See ticket #64 on
  the PyFITS Trac for further details and background. Some highlights are
  listed below:

  * The Header class now fully implements the Python dict interface, and can
    be used interchangeably with a dict, where the keys are header keywords.

  * New keywords can be added to the header using normal keyword assignment
    (previously it was necessary to use ``Header.update`` to add new
    keywords).  For example::

        >>> header['NAXIS'] = 2

    will update the existing 'FOO' keyword if it already exists, or add a new
    one if it doesn't exist, just like a dict.

  * It is possible to assign both a value and a comment at the same time using
    a tuple::

        >>> header['NAXIS'] = (2, 'Number of axes')

  * To add/update a new card and ensure it's added in a specific location, use
    ``Header.set()``::

        >>> header.set('NAXIS', 2, 'Number of axes', after='BITPIX')

    This works the same as the old ``Header.update()``.  ``Header.update()``
    still works in the old way too, but is deprecated.

  * Although ``Card`` objects still exist, it generally is not necessary to
    work with them directly.  ``Header.ascardlist()``/``Header.ascard`` are
    deprecated and should not be used.  To directly access the ``Card``
    objects in a header, use ``Header.cards``.

  * To access card comments, it is still possible to either go through the
    card itself, or through ``Header.comments``.  For example::

       >>> header.cards['NAXIS'].comment
       Number of axes
       >>> header.comments['NAXIS']
       Number of axes

  * ``Card`` objects can now be used interchangeably with
    ``(keyword, value, comment)`` 3-tuples.  They still have ``.value`` and
    ``.comment`` attributes as well.  The ``.key`` attribute has been renamed
    to ``.keyword`` for consistency, though ``.key`` is still supported (but
    deprecated).

- Memory mapping is now used by default to access HDU data.  That is,
  ``pyfits.open()`` uses ``memmap=True`` as the default.  This provides better
  performance in the majority of use cases--there are only some I/O intensive
  applications where it might not be desirable.  Enabling mmap by default also
  enabled finding and fixing a large number of bugs in PyFITS' handling of
  memory-mapped data (most of these bug fixes were backported to PyFITS
  3.0.5). (#85)

  * A new ``pyfits.USE_MEMMAP`` global variable was added.  Set
    ``pyfits.USE_MEMMAP = False`` to change the default memmap setting for
    opening files.  This is especially useful for controlling the behavior in
    applications where pyfits is deeply embedded.

  * Likewise, a new ``PYFITS_USE_MEMMAP`` environment variable is supported.
    Set ``PYFITS_USE_MEMMAP = 0`` in your environment to change the default
    behavior.

- The ``size()`` method on HDU objects is now a ``.size`` property--this
  returns the size in bytes of the data portion of the HDU, and in most cases
  is equivalent to ``hdu.data.nbytes`` (#83)

- ``BinTableHDU.tdump`` and ``BinTableHDU.tcreate`` are deprecated--use
  ``BinTableHDU.dump`` and ``BinTableHDU.load`` instead.  The new methods
  output the table data in a slightly different format from previous versions,
  which places quotes around each value.  This format is compatible with data
  dumps from previous versions of PyFITS, but not vice-versa due to a parsing
  bug in older versions.

- Likewise the ``pyfits.tdump`` and ``pyfits.tcreate`` convenience function
  versions of these methods have been renamed ``pyfits.tabledump`` and
  ``pyfits.tableload``.  The old deprecated, but currently retained for
  backwards compatibility. (r1125)

- A new global variable ``pyfits.EXTENSION_NAME_CASE_SENSITIVE`` was added.
  This serves as a replacement for ``pyfits.setExtensionNameCaseSensitive``
  which is not deprecated and may be removed in a future version.  To enable
  case-sensitivity of extension names (i.e. treat 'sci' as distinct from 'SCI')
  set ``pyfits.EXTENSION_NAME_CASE_SENSITIVE = True``.  The default is
  ``False``. (r1139)

- A new global configuration variable ``pyfits.STRIP_HEADER_WHITESPACE`` was
  added.  By default, if a string value in a header contains trailing
  whitespace, that whitespace is automatically removed when the value is read.
  Now if you set ``pyfits.STRIP_HEADER_WHITESPACE = False`` all whitespace is
  preserved. (#146)

- The old ``classExtensions`` extension mechanism (which was deprecated in
  PyFITS 3.0) is removed outright.  To our knowledge it was no longer used
  anywhere. (r1309)

- Warning messages from PyFITS issued through the Python warnings API are now
  output to stderr instead of stdout, as is the default.  PyFITS no longer
  modifies the default behavior of the warnings module with respect to which
  stream it outputs to. (r1319)

- The ``checksum`` argument to ``pyfits.open()`` now accepts a value of
  'remove', which causes any existing CHECKSUM/DATASUM keywords to be ignored,
  and removed when the file is saved.

New Features
------------

- Added support for the proposed "FITS" extension HDU type. FITS
  HDUs contain an entire FITS file embedded in their data section.  ``FitsHDU``
  objects work like other HDU types in PyFITS.  Their ``.data`` attribute
  returns the raw data array.  However, they have a special ``.hdulist``
  attribute which processes the data as a FITS file and returns it as an
  in-memory HDUList object.  FitsHDU objects also support a
  ``FitsHDU.fromhdulist()`` classmethod which returns a new ``FitsHDU`` object
  that embeds the supplied HDUList. (#80)

- Added a new ``.is_image`` attribute on HDU objects, which is True if the HDU
  data is an 'image' as opposed to a table or something else.  Here the
  meaning of 'image' is fairly loose, and mostly just means a Primary or Image
  extension HDU, or possibly a compressed image HDU (#71)

- Added an ``HDUList.fromstring`` classmethod which can parse a FITS file
  already in memory and instantiate and ``HDUList`` object from it.  This
  could be useful for integrating PyFITS with other libraries that work on
  FITS file, such as CFITSIO.  It may also be useful in streaming
  applications.  The name is a slight misnomer, in that it actually accepts
  any Python object that implements the buffer interface, which includes
  ``bytes``, ``bytearray``, ``memoryview``, ``numpy.ndarray``, etc. (#90)

- Added a new ``pyfits.diff`` module which contains facilities for comparing
  FITS files.  One can use the ``pyfits.diff.FITSDiff`` class to compare two
  FITS files in their entirety.  There is also a ``pyfits.diff.HeaderDiff``
  class for just comparing two FITS headers, and other similar interfaces.
  See the PyFITS Documentation for more details on this interface.  The
  ``pyfits.diff`` module powers the new ``fitsdiff`` program installed with
  PyFITS.  After installing PyFITS, run ``fitsdiff --help`` for usage details.

- ``pyfits.open()`` now accepts a ``scale_back`` argument.  If set to
  ``True``, this automatically scales the data using the original BZERO and
  BSCALE parameters the file had when it was first opened, if any, as well as
  the original BITPIX.  For example, if the original BITPIX were 16, this
  would be equivalent to calling ``hdu.scale('int16', 'old')`` just before
  calling ``flush()`` or ``close()`` on the file.  This option applies to all
  HDUs in the file. (#120)

- ``pyfits.open()`` now accepts a ``save_backup`` argument.  If set to
  ``True``, this automatically saves a backup of the original file before
  flushing any changes to it (this of course only applies to update and append
  mode).  This may be especially useful when working with scaled image data.
  (#121)

Changes in Behavior
-------------------

- Warnings from PyFITS are not output to stderr by default, instead of stdout
  as it has been for some time.  This is contrary to most users' expectations
  and makes it more difficult for them to separate output from PyFITS from the
  desired output for their scripts. (r1319)

Bug Fixes
---------

- Fixed ``pyfits.tcreate()`` (now ``pyfits.tableload()``) to be more robust
  when encountering blank lines in a column definition file (#14)

- Fixed a fairly rare crash that could occur in the handling of CONTINUE cards
  when using Numpy 1.4 or lower (though 1.4 is the oldest version supported by
  PyFITS). (r1330)

- Fixed ``_BaseHDU.fromstring`` to actually correctly instantiate an HDU
  object from a string/buffer containing the header and data of that HDU.
  This allowed for the implementation of ``HDUList.fromstring`` described
  above. (#90)

- Fixed a rare corner case where, in some use cases, (mildly, recoverable)
  malformatted float values in headers were not properly returned as floats.
  (#137)

- Fixed a corollary to the previous bug where float values with a leading zero
  before the decimal point had the leading zero unnecessarily removed when
  saving changes to the file (eg. "0.001" would be written back as ".001" even
  if no changes were otherwise made to the file). (#137)

- When opening a file containing CHECKSUM and/or DATASUM keywords in update
  mode, the CHECKSUM/DATASUM are updated and preserved even if the file was
  opened with checksum=False.  This change in behavior prevents checksums from
  being unintentionally removed. (#148)

- Fixed a bug where ``ImageHDU.scale(option='old')`` wasn't working at all--it
  was not restoring the image to its original BSCALE and BZERO values. (#162)

- Fixed a bug when writing out files containing zero-width table columns,
  where the TFIELDS keyword would be updated incorrectly, leaving the table
  largely unreadable.  This fix will be backported to the 3.0.x series in
  version 3.0.10.  (#174)


3.0.9 (2012-08-06)
==================

This is a bug fix release for the 3.0.x series.

Bug Fixes
---------

- Fixed ``Header.values()``/``Header.itervalues()`` and ``Header.items()``/
  ``Header.iteritems()`` to correctly return the different values for
  duplicate keywords (particularly commentary keywords like HISTORY and
  COMMENT).  This makes the old Header implementation slightly more compatible
  with the new implementation in PyFITS 3.1. (#127)

  .. note::
      This fix did not change the existing behavior from earlier PyFITS
      versions where ``Header.keys()`` returns all keywords in the header with
      duplicates removed.  PyFITS 3.1 changes that behavior, so that
      ``Header.keys()`` includes duplicates.

- Fixed a bug where ``ImageHDU.scale(option='old')`` wasn't working at all--it
  was not restoring the image to its original BSCALE and BZERO values. (#162)

- Fixed a bug where opening a file containing compressed image HDUs in
  'update' mode and then immediately closing it without making any changes
  caused the file to be rewritten unnecessarily. (#167)

- Fixed two memory leaks that could occur when writing compressed image data,
  or in some cases when opening files containing compressed image HDUs in
  'update' mode. (#168)


3.0.8 (2012-06-04)
==================

Changes in Behavior
-------------------

- Prior to this release, image data sections did not work with scaled
  data--that is, images with non-trivial BSCALE and/or BZERO values.
  Previously, in order to read such images in sections, it was necessary to
  manually apply the BSCALE+BZERO to each section.  It's worth noting that
  sections *did* support pseudo-unsigned ints (flakily).  This change just
  extends that support for general BSCALE+BZERO values.

Bug Fixes
---------

- Fixed a bug that prevented updates to values in boolean table columns from
  being saved.  This turned out to be a symptom of a deeper problem that could
  prevent other table updates from being saved as well. (#139)

- Fixed a corner case in which a keyword comment ending with the string "END"
  could, in some circumstances, cause headers (and the rest of the file after
  that point) to be misread. (#142)

- Fixed support for scaled image data and pseudo-unsigned ints in image data
  sections (``hdu.section``).  Previously this was not supported at all.  At
  some point support was supposedly added, but it was buggy and incomplete.
  Now the feature seems to work much better. (#143)

- Fixed the documentation to point out that image data sections *do* support
  non-contiguous slices (and have for a long time).  The documentation was
  never updated to reflect this, and misinformed users that only contiguous
  slices were supported, leading to some confusion. (#144)

- Fixed a bug where creating an ``HDUList`` object containing multiple PRIMARY
  HDUs caused an infinite recursion when validating the object prior to
  writing to a file. (#145)

- Fixed a rare but serious case where saving an update to a file that
  previously had a CHECKSUM and/or DATASUM keyword, but removed the checksum
  in saving, could cause the file to be slightly corrupted and unreadable.
  (#147)

- Fixed problems with reading "non-standard" FITS files with primary headers
  containing SIMPLE = F.  PyFITS has never made many guarantees as to how such
  files are handled.  But it should at least be possible to read their
  headers, and the data if possible.  Saving changes to such a file should not
  try to prepend an unwanted valid PRIMARY HDU. (#157)

- Fixed a bug where opening an image with ``disable_image_compression = True``
  caused compression to be disabled for all subsequent ``pyfits.open()`` calls.
  (r1651)


3.0.7 (2012-04-10)
==================

Changes in Behavior
-------------------

- Slices of GroupData objects now return new GroupData objects instead of
  extended multi-row _Group objects. This is analogous to how PyFITS 3.0 fixed
  FITS_rec slicing, and should have been fixed for GroupData at the same time.
  The old behavior caused bugs where functions internal to Numpy expected that
  slicing an ndarray would return a new ndarray.  As this is a rare use case
  with a rare feature most users are unlikely to be affected by this change.

- The previously internal _Group object for representing individual group
  records in a GroupData object are renamed Group and are now a public
  interface.  However, there's almost no good reason to create Group objects
  directly, so it shouldn't be considered a "new feature".

- An annoyance from PyFITS 3.0.6 was fixed, where the value of the EXTEND
  keyword was always being set to F if there are not actually any extension
  HDUs.  It was unnecessary to modify this value.

Bug Fixes
---------

- Fixed GroupData objects to return new GroupData objects when sliced instead
  of _Group record objects.  See "Changes in behavior" above for more details.

- Fixed slicing of Group objects--previously it was not possible to slice
  slice them at all.

- Made it possible to assign ``np.bool_`` objects as header values. (#123)

- Fixed overly strict handling of the EXTEND keyword; see "Changes in
  behavior" above. (#124)

- Fixed many cases where an HDU's header would be marked as "modified" by
  PyFITS and rewritten, even when no changes to the header are necessary.
  (#125)

- Fixed a bug where the values of the PTYPEn keywords in a random groups HDU
  were forced to be all lower-case when saving the file. (#130)

- Removed an unnecessary inline import in ``ExtensionHDU.__setattr__`` that was
  causing some slowdown when opening files containing a large number of
  extensions, plus a few other small (but not insignificant) performance
  improvements thanks to Julian Taylor. (#133)

- Fixed a regression where header blocks containing invalid end-of-header
  padding (i.e. null bytes instead of spaces) couldn't be parsed by PyFITS.
  Such headers can be parsed again, but a warning is raised, as such headers
  are not valid FITS. (#136)

- Fixed a memory leak where table data in random groups HDUs weren't being
  garbage collected. (#138)


3.0.6 (2012-02-29)
==================

Highlights
----------

The main reason for this release is to fix an issue that was introduced in
PyFITS 3.0.5 where merely opening a file containing scaled data (that is, with
non-trivial BSCALE and BZERO keywords) in 'update' mode would cause the data
to be automatically rescaled--possibly converting the data from ints to
floats--as soon as the file is closed, even if the application did not touch
the data.  Now PyFITS will only rescale the data in an extension when the data
is actually accessed by the application.  So opening a file in 'update' mode
in order to modify the header or append new extensions will not cause any
change to the data in existing extensions.

This release also fixes a few Windows-specific bugs found through more
extensive Windows testing, and other miscellaneous bugs.

Bug Fixes
---------

- More accurate error messages when opening files containing invalid header
  cards. (#109)

- Fixed a possible reference cycle/memory leak that was caught through more
  extensive testing on Windows. (#112)

- Fixed 'ostream' mode to open the underlying file in 'wb' mode instead of 'w'
  mode. (#112)

- Fixed a Windows-only issue where trying to save updates to a resized FITS
  file could result in a crash due to there being open mmaps on that file.
  (#112)

- Fixed a crash when trying to create a FITS table (i.e. with new_table())
  from a Numpy array containing bool fields. (#113)

- Fixed a bug where manually initializing an ``HDUList`` with a list of of
  HDUs wouldn't set the correct EXTEND keyword value on the primary HDU.
  (#114)

- Fixed a crash that could occur when trying to deepcopy a Header in Python <
  2.7. (#115)

- Fixed an issue where merely opening a scaled image in 'update' mode would
  cause the data to be converted to floats when the file is closed. (#119)


3.0.5 (2012-01-30)
==================

- Fixed a crash that could occur when accessing image sections of files
  opened with memmap=True. (r1211)

- Fixed the inconsistency in the behavior of files opened in 'readonly' mode
  when memmap=True vs. when memmap=False.  In the latter case, although
  changes to array data were not saved to disk, it was possible to update the
  array data in memory.  On the other hand with memmap=True, 'readonly' mode
  prevented even in-memory modification to the data.  This is what
  'copyonwrite' mode was for, but difference in behavior was confusing.  Now
  'readonly' is equivalent to 'copyonwrite' when using memmap.  If the old
  behavior of denying changes to the array data is necessary, a new
  'denywrite' mode may be used, though it is only applicable to files opened
  with memmap. (r1275)

- Fixed an issue where files opened with memmap=True would return image data
  as a raw numpy.memmap object, which can cause some unexpected
  behaviors--instead memmap object is viewed as a numpy.ndarray. (r1285)

- Fixed an issue in Python 3 where a workaround for a bug in Numpy on Python 3
  interacted badly with some other software, namely to vo.table package (and
  possibly others). (r1320, r1337, and #110)

- Fixed buggy behavior in the handling of SIGINTs (i.e. Ctrl-C keyboard
  interrupts) while flushing changes to a FITS file.  PyFITS already prevented
  SIGINTs from causing an incomplete flush, but did not clean up the signal
  handlers properly afterwards, or reraise the keyboard interrupt once the
  flush was complete. (r1321)

- Fixed a crash that could occur in Python 3 when opening files with checksum
  checking enabled. (r1336)

- Fixed a small bug that could cause a crash in the ``StreamingHDU`` interface
  when using Numpy below version 1.5.

- Fixed a crash that could occur when creating a new ``CompImageHDU`` from an
  array of big-endian data. (#104)

- Fixed a crash when opening a file with extra zero padding at the end.
  Though FITS files should not have such padding, it's not explicitly forbidden
  by the format either, and PyFITS shouldn't stumble over it. (#106)

- Fixed a major slowdown in opening tables containing large columns of string
  values.  (#111)


3.0.4 (2011-11-22)
==================

- Fixed a crash when writing HCOMPRESS compressed images that could happen on
  Python 2.5 and 2.6. (r1217)

- Fixed a crash when slicing an table in a file opened in 'readonly' mode with
  memmap=True. (r1230)

- Writing changes to a file or writing to a new file verifies the output in
  'fix' mode by default instead of 'exception'--that is, PyFITS will
  automatically fix common FITS format errors rather than raising an
  exception. (r1243)

- Fixed a bug where convenience functions such as getval() and getheader()
  crashed when specifying just 'PRIMARY' as the extension to use (r1263).

- Fixed a bug that prevented passing keyword arguments (beyond the standard
  data and header arguments) as positional arguments to the constructors of
  extension HDU classes.

- Fixed some tests that were failing on Windows--in this case the tests
  themselves failed to close some temp files and Windows refused to delete them
  while there were still open handles on them. (r1295)

- Fixed an issue with floating point formatting in header values on Python 2.5
  for Windows (and possibly other platforms).  The exponent was zero-padded to
  3 digits; although the FITS standard makes no specification on this, the
  formatting is now normalized to always pad the exponent to two digits.
  (r1295)

- Fixed a bug where long commentary cards (such as HISTORY and COMMENT) were
  broken into multiple CONTINUE cards.  However, commentary cards are not
  expected to be found in CONTINUE cards.  Instead these long cards are broken
  into multiple commentary cards. (#97)

- GZIP/ZIP-compressed FITS files can be detected and opened regardless of
  their filename extension. (#99)

- Fixed a serious bug where opening scaled images in 'update' mode and then
  closing the file without touching the data would cause the file to be
  corrupted. (#101)


3.0.3 (2011-10-05)
==================

- Fixed several small bugs involving corner cases in record-valued keyword
  cards (#70)

- In some cases HDU creation failed if the first keyword value in the header
  was not a string value (#89)

- Fixed a crash when trying to compute the HDU checksum when the data array
  contains an odd number of bytes (#91)

- Disabled an unnecessary warning that was displayed on opening compressed
  HDUs with disable_image_compression = True (#92)

- Fixed a typo in code for handling HCOMPRESS compressed images.


3.0.2 (2011-09-23)
==================

- The ``BinTableHDU.tcreate`` method and by extension the ``pyfits.tcreate``
  function don't get tripped up by blank lines anymore (#14)

- The presence, value, and position of the EXTEND keyword in Primary HDUs is
  verified when reading/writing a FITS file (#32)

- Improved documentation (in warning messages as well as in the handbook) that
  PyFITS uses zero-based indexing (as one would expect for C/Python code, but
  contrary to the PyFITS standard which was written with FORTRAN in mind)
  (#68)

- Fixed a bug where updating a header card comment could cause the value to be
  lost if it had not already been read from the card image string.

- Fixed a related bug where changes made directly to Card object in a header
  (i.e. assigning directly to card.value or card.comment) would not propagate
  when flushing changes to the file (#69) [Note: This and the bug above it
  were originally reported as being fixed in version 3.0.1, but the fix was
  never included in the release.]

- Improved file handling, particularly in Python 3 which had a few small file
  I/O-related bugs (#76)

- Fixed a bug where updating a FITS file would sometimes cause it to lose its
  original file permissions (#79)

- Fixed the handling of TDIMn keywords; 3.0 added support for them, but got
  the axis order backwards (they were treated as though they were row-major)
  (#82)

- Fixed a crash when a FITS file containing scaled data is opened and
  immediately written to a new file without explicitly viewing the data first
  (#84)

- Fixed a bug where creating a table with columns named either 'names' or
  'formats' resulted in an infinite recursion (#86)


3.0.1 (2011-09-12)
==================

- Fixed a bug where updating a header card comment could cause the value to be
  lost if it had not already been read from the card image string.

- Changed ``_TableBaseHDU.data`` so that if the data contain an empty table a
  ``FITS_rec`` object with zero rows is returned rather than ``None``.

- The ``.key`` attribute of ``RecordValuedKeywordCards`` now returns the full
  keyword+field-specifier value, instead of just the plain keyword (#46)

- Fixed a related bug where changes made directly to Card object in a header
  (i.e. assigning directly to card.value or card.comment) would not propagate
  when flushing changes to the file (#69)

- Fixed a bug where writing a table with zero rows could fail in some cases
  (#72)

- Miscellaneous small bug fixes that were causing some tests to fail,
  particularly on Python 3 (#74, #75)

- Fixed a bug where creating a table column from an array in non-native byte
  order would not preserve the byte order, thus interpreting the column array
  using the wrong byte order (#77)


3.0.0 (2011-08-23)
====================

- Contains major changes, bumping the version to 3.0

- Large amounts of refactoring and reorganization of the code; tried to
  preserve public API backwards-compatibility with older versions (private API
  has many changes and is not guaranteed to be backwards-compatible).  There
  are a few small public API changes to be aware of:

  * The pyfits.rec module has been removed completely.  If your version of
    numpy does not have the numpy.core.records module it is too old to be used
    with PyFITS.

  * The ``Header.ascardlist()`` method is deprecated--use the ``.ascard``
    attribute instead.

  * ``Card`` instances have a new ``.cardimage`` attribute that should be used
    rather than ``.ascardimage()``, which may become deprecated.

  * The ``Card.fromstring()`` method is now a classmethod.  It returns a new
    ``Card`` instance rather than modifying an existing instance.

  * The ``req_cards()`` method on HDU instances has changed:  The ``pos``
    argument is not longer a string.  It is either an integer value (meaning
    the card's position must match that value) or it can be a function that
    takes the card's position as it's argument, and returns True if the
    position is valid.  Likewise, the ``test`` argument no longer takes a
    string, but instead a function that validates the card's value and returns
    True or False.

  * The ``get_coldefs()`` method of table HDUs is deprecated.  Use the
    ``.columns`` attribute instead.

  * The ``ColDefs.data`` attribute is deprecated--use ``ColDefs.columns``
    instead (though in general you shouldn't mess with it directly--it might
    become internal at some point).

  * ``FITS_record`` objects take ``start`` and ``end`` as arguments instead of
    ``startColumn`` and ``endColumn`` (these are rarely created manually, so
    it's unlikely that this change will affect anyone).

  * ``BinTableHDU.tcreate()`` is now a classmethod, and returns a new
    ``BinTableHDU`` instance.

  * Use ``ExtensionHDU`` and ``NonstandardExtHDU`` for making new extension HDU
    classes.  They are now public interfaces, wheres previously they were
    private and prefixed with underscores.

  * Possibly others--please report if you find any changes that cause
    difficulties.

- Calls to deprecated functions will display a Deprecation warning.  However,
  in Python 2.7 and up Deprecation warnings are ignored by default, so run
  Python with the ``-Wd`` option to see if you're using any deprecated
  functions.  If we get close to actually removing any functions, we might
  make the Deprecation warnings display by default.

- Added basic Python 3 support

- Added support for multi-dimensional columns in tables as specified by the
  TDIMn keywords (#47)

- Fixed a major memory leak that occurred when creating new tables with the
  ``new_table()`` function (#49)
  be padded with zero-bytes) vs ASCII tables (where strings are padded with
  spaces) (#15)

- Fixed a bug in which the case of Random Access Group parameters names was not
  preserved when writing (#41)

- Added support for binary table fields with zero width (#42)

- Added support for wider integer types in ASCII tables; although this is non-
  standard, some GEIS images require it (#45)

- Fixed a bug that caused the index_of() method of HDULists to crash when the
  HDUList object is created from scratch (#48)

- Fixed the behavior of string padding in binary tables (where strings should
  be padded with nulls instead of spaces)

- Fixed a rare issue that caused excessive memory usage when computing
  checksums using a non-standard block size (see r818)

- Add support for forced uint data in image sections (#53)

- Fixed an issue where variable-length array columns were not extended when
  creating a new table with more rows than the original (#54)

- Fixed tuple and list-based indexing of FITS_rec objects (#55)

- Fixed an issue where BZERO and BSCALE keywords were appended to headers in
  the wrong location (#56)

- ``FITS_record`` objects (table rows) have full slicing support, including
  stepping, etc. (#59)

- Fixed a bug where updating multiple files simultaneously (such as when
  running parallel processes) could lead to a race condition with mktemp()
  (#61)

- Fixed a bug where compressed image headers were not in the order expected by
  the funpack utility (#62)


2.4.0 (2011-01-10)
====================
The following enhancements were added:

- Checksum support now correctly conforms to the FITS standard.  pyfits
  supports reading and writing both the old checksums and new
  standard-compliant checksums.  The ``fitscheck`` command-line utility is
  provided to verify and update checksums.

- Added a new optional keyword argument ``do_not_scale_image_data``
  to the ``pyfits.open`` convenience function.  When this argument
  is provided as True, and an ImageHDU is read that contains scaled
  data, the data is not automatically scaled when it is read.  This
  option may be used when opening a fits file for update, when you only
  want to update some header data.  Without the use of this argument, if
  the header updates required the size of the fits file to change, then
  when writing the updated information, the data would be read, scaled,
  and written back out in its scaled format (usually with a different
  data type) instead of in its non-scaled format.

- Added a new optional keyword argument ``disable_image_compression`` to the
  ``pyfits.open`` function.  When ``True``, any compressed image HDU's will
  be read in like they are binary table HDU's.

- Added a ``verify`` keyword argument to the ``pyfits.append`` function.  When
  ``False``, ``append`` will assume the existing FITS file is already valid
  and simply append new content to the end of the file, resulting in a large
  speed up appending to large files.

- Added HDU methods ``update_ext_name`` and ``update_ext_version`` for
  updating the name and version of an HDU.

- Added HDU method ``filebytes`` to calculate the number of bytes that will be
  written to the file associated with the HDU.

- Enhanced the section class to allow reading non-contiguous image data.
  Previously, the section class could only be used to read contiguous data.
  (CNSHD781626)

- Added method ``HDUList.fileinfo()`` that returns a dictionary with
  information about the location of header and data in the file associated
  with the HDU.

The following bugs were fixed:

- Reading in some malformed FITS headers would cause a ``NameError``
  exception, rather than information about the cause of the error.

- pyfits can now handle non-compliant ``CONTINUE`` cards produced by Java
  FITS.

- ``BinTable`` columns with ``TSCALn`` are now byte-swapped correctly.

- Ensure that floating-point card values are no longer than 20 characters.

- Updated ``flush`` so that when the data has changed in an HDU for a file
  opened in update mode, the header will be updated to match the changed data
  before writing out the HDU.

- Allow ``HIERARCH`` cards to contain a keyword and value whose total
  character length is 69 characters.  Previous length was limited at 68
  characters.

- Calls to ``FITS_rec['columnName']`` now return an ``ndarray``. exactly the
  same as a call to ``FITS_rec.field('columnName')`` or
  ``FITS_rec.columnName``.  Previously, ``FITS_rec['columnName']`` returned a
  much less useful ``fits_record`` object. (CNSHD789053)

- Corrected the ``append`` convenience function to eliminate the reading of
  the HDU data from the file that is being appended to.  (CNSHD794738)

- Eliminated common symbols between the pyfitsComp module and the cfitsio and
  zlib libraries.  These can cause problems on systems that use both PyFITS
  and cfitsio or zlib. (CNSHD795046)


2.3.1 (2010-06-03)
====================

The following bugs were fixed:

- Replaced code in the Compressed Image HDU extension which was covered under
  a GNU General Public License with code that is covered under a BSD License.
  This change allows the distribution of pyfits under a BSD License.


2.3 (2010-05-11)
==================

The following enhancements were made:

- Completely eliminate support for numarray.

- Rework pyfits documentation to use Sphinx.

- Support python 2.6 and future division.

- Added a new method to get the file name associated with an HDUList object.
  The method HDUList.filename() returns the name of an associated file.  It
  returns None if no file is associated with the HDUList.

- Support the python 2.5 'with' statement when opening fits files.
  (CNSHD766308)  It is now possible to use the following construct:

    >>> from __future__ import with_statement import pyfits
    >>> with pyfits.open("input.fits") as hdul:
    ...    #process hdul
    >>>

- Extended the support for reading unsigned integer 16 values from an ImageHDU
  to include unsigned integer 32 and unsigned integer 64 values.  ImageHDU
  data is considered to be unsigned integer 16 when the data type is signed
  integer 16 and BZERO is equal to 2**15 (32784) and BSCALE is equal to 1.
  ImageHDU data is considered to be unsigned integer 32 when the data type is
  signed integer 32 and BZERO is equal to 2**31 and BSCALE is equal to 1.
  ImageHDU data is considered to be unsigned integer 64 when the data type is
  signed integer 64 and BZERO is equal to 2**63 and BSCALE is equal to 1.  An
  optional keyword argument (uint) was added to the open convenience function
  for this purpose.  Supplying a value of True for this argument will cause
  data of any of these types to be read in and scaled into the appropriate
  unsigned integer array (uint16, uint32, or uint64) instead of into the
  normal float 32 or float 64 array.  If an HDU associated with a file that
  was opened with the 'int' option and containing unsigned integer 16, 32, or
  64 data is written to a file, the data will be reverse scaled into a signed
  integer 16, 32, or 64 array and written out to the file along with the
  appropriate BSCALE/BZERO header cards.  Note that for backward
  compatibility, the 'uint16' keyword argument will still be accepted in the
  open function when handling unsigned integer 16 conversion.

- Provided the capability to access the data for a column of a fits table by
  indexing the table using the column name.  This is consistent with Record
  Arrays in numpy (array with fields).  (CNSHD763378)  The following example
  will illustrate this:

    >>> import pyfits
    >>> hdul = pyfits.open('input.fits')
    >>> table = hdul[1].data
    >>> table.names
    ['c1','c2','c3','c4']
    >>> print table.field('c2') # this is the data for column 2
    ['abc' 'xy']
    >>> print table['c2'] # this is also the data for column 2
    array(['abc', 'xy '], dtype='|S3')
    >>> print table[1] # this is the data for row 1
    (2, 'xy', 6.6999997138977054, True)

- Provided capabilities to create a BinaryTableHDU directly from a numpy
  Record Array (array with fields). The new capabilities include table
  creation, writing a numpy Record Array directly to a fits file using the
  pyfits.writeto and pyfits.append convenience functions.  Reading the data
  for a BinaryTableHDU from a fits file directly into a numpy Record Array
  using the pyfits.getdata convenience function.  (CNSHD749034)  Thanks to
  Erin Sheldon at Brookhaven National Laboratory for help with this.

  The following should illustrate these new capabilities:

    >>> import pyfits
    >>> import numpy
    >>> t=numpy.zeros(5,dtype=[('x','f4'),('y','2i4')]) \
    ... # Create a numpy Record Array with fields
    >>> hdu = pyfits.BinTableHDU(t) \
    ... # Create a Binary Table HDU directly from the Record Array
    >>> print hdu.data
    [(0.0, array([0, 0], dtype=int32))
     (0.0, array([0, 0], dtype=int32))
     (0.0, array([0, 0], dtype=int32))
     (0.0, array([0, 0], dtype=int32))
     (0.0, array([0, 0], dtype=int32))]
    >>> hdu.writeto('test1.fits',clobber=True) \
    ... # Write the HDU to a file
    >>> pyfits.info('test1.fits')
    Filename: test1.fits
    No.    Name         Type      Cards   Dimensions   Format
    0    PRIMARY     PrimaryHDU       4  ()            uint8
    1                BinTableHDU     12  5R x 2C       [E, 2J]
    >>> pyfits.writeto('test.fits', t, clobber=True) \
    ... # Write the Record Array directly to a file
    >>> pyfits.append('test.fits', t) \
    ... # Append another Record Array to the file
    >>> pyfits.info('test.fits')
    Filename: test.fits
    No.    Name         Type      Cards   Dimensions   Format
    0    PRIMARY     PrimaryHDU       4  ()            uint8
    1                BinTableHDU     12  5R x 2C       [E, 2J]
    2                BinTableHDU     12  5R x 2C       [E, 2J]
    >>> d=pyfits.getdata('test.fits',ext=1) \
    ... # Get the first extension from the file as a FITS_rec
    >>> print type(d)
    <class 'pyfits.core.FITS_rec'>
    >>> print d
    [(0.0, array([0, 0], dtype=int32))
     (0.0, array([0, 0], dtype=int32))
     (0.0, array([0, 0], dtype=int32))
     (0.0, array([0, 0], dtype=int32))
     (0.0, array([0, 0], dtype=int32))]
    >>> d=pyfits.getdata('test.fits',ext=1,view=numpy.ndarray) \
    ... # Get the first extension from the file as a numpy Record
          Array
    >>> print type(d)
    <type 'numpy.ndarray'>
    >>> print d
    [(0.0, [0, 0]) (0.0, [0, 0]) (0.0, [0, 0]) (0.0, [0, 0])
     (0.0, [0, 0])]
    >>> print d.dtype
    [('x', '>f4'), ('y', '>i4', 2)]
    >>> d=pyfits.getdata('test.fits',ext=1,upper=True,
    ...                  view=pyfits.FITS_rec) \
    ... # Force the Record Array field names to be in upper case
          regardless of how they are stored in the file
    >>> print d.dtype
    [('X', '>f4'), ('Y', '>i4', 2)]

- Provided support for writing fits data to file-like objects that do not
  support the random access methods seek() and tell().  Most pyfits functions
  or methods will treat these file-like objects as an empty file that cannot
  be read, only written.  It is also expected that the file-like object is in
  a writable condition (ie. opened) when passed into a pyfits function or
  method.  The following methods and functions will allow writing to a
  non-random access file-like object: HDUList.writeto(), HDUList.flush(),
  pyfits.writeto(), and pyfits.append().  The pyfits.open() convenience
  function may be used to create an HDUList object that is associated with the
  provided file-like object.  (CNSHD770036)

  An illustration of the new capabilities follows.  In this example fits data
  is written to standard output which is associated with a file opened in
  write-only mode:

    >>> import pyfits
    >>> import numpy as np
    >>> import sys
    >>>
    >>> hdu = pyfits.PrimaryHDU(np.arange(100,dtype=np.int32))
    >>> hdul = pyfits.HDUList()
    >>> hdul.append(hdu)
    >>> tmpfile = open('tmpfile.py','w')
    >>> sys.stdout = tmpfile
    >>> hdul.writeto(sys.stdout, clobber=True)
    >>> sys.stdout = sys.__stdout__
    >>> tmpfile.close()
    >>> pyfits.info('tmpfile.py')
    Filename: tmpfile.py
    No.    Name         Type      Cards   Dimensions   Format
    0    PRIMARY     PrimaryHDU       5  (100,)        int32
    >>>

- Provided support for slicing a FITS_record object.  The FITS_record object
  represents the data from a row of a table.  Pyfits now supports the slice
  syntax to retrieve values from the row.  The following illustrates this new
  syntax:

    >>> hdul = pyfits.open('table.fits')
    >>> row = hdul[1].data[0]
    >>> row
    ('clear', 'nicmos', 1, 30, 'clear', 'idno= 100')
    >>> a, b, c, d, e = row[0:5]
    >>> a
    'clear'
    >>> b
    'nicmos'
    >>> c
    1
    >>> d
    30
    >>> e
    'clear'
    >>>

- Allow the assignment of a row value for a pyfits table using a tuple or a
  list as input.  The following example illustrates this new feature:

    >>> c1=pyfits.Column(name='target',format='10A')
    >>> c2=pyfits.Column(name='counts',format='J',unit='DN')
    >>> c3=pyfits.Column(name='notes',format='A10')
    >>> c4=pyfits.Column(name='spectrum',format='5E')
    >>> c5=pyfits.Column(name='flag',format='L')
    >>> coldefs=pyfits.ColDefs([c1,c2,c3,c4,c5])
    >>>
    >>> tbhdu=pyfits.new_table(coldefs, nrows = 5)
    >>>
    >>> # Assigning data to a table's row using a tuple
    >>> tbhdu.data[2] = ('NGC1',312,'A Note',
    ... num.array([1.1,2.2,3.3,4.4,5.5],dtype=num.float32),
    ... True)
    >>>
    >>> # Assigning data to a tables row using a list
    >>> tbhdu.data[3] = ['JIM1','33','A Note',
    ... num.array([1.,2.,3.,4.,5.],dtype=num.float32),True]

- Allow the creation of a Variable Length Format (P format) column from a list
  of data.  The following example illustrates this new feature:

    >>> a = [num.array([7.2e-20,7.3e-20]),num.array([0.0]),
    ... num.array([0.0])]
    >>> acol = pyfits.Column(name='testa',format='PD()',array=a)
    >>> acol.array
    _VLF([[  7.20000000e-20   7.30000000e-20], [ 0.], [ 0.]],
    dtype=object)
    >>>

- Allow the assignment of multiple rows in a table using the slice syntax. The
  following example illustrates this new feature:

    >>> counts = num.array([312,334,308,317])
    >>> names = num.array(['NGC1','NGC2','NGC3','NCG4'])
    >>> c1=pyfits.Column(name='target',format='10A',array=names)
    >>> c2=pyfits.Column(name='counts',format='J',unit='DN',
    ... array=counts)
    >>> c3=pyfits.Column(name='notes',format='A10')
    >>> c4=pyfits.Column(name='spectrum',format='5E')
    >>> c5=pyfits.Column(name='flag',format='L',array=[1,0,1,1])
    >>> coldefs=pyfits.ColDefs([c1,c2,c3,c4,c5])
    >>>
    >>> tbhdu1=pyfits.new_table(coldefs)
    >>>
    >>> counts = num.array([112,134,108,117])
    >>> names = num.array(['NGC5','NGC6','NGC7','NCG8'])
    >>> c1=pyfits.Column(name='target',format='10A',array=names)
    >>> c2=pyfits.Column(name='counts',format='J',unit='DN',
    ... array=counts)
    >>> c3=pyfits.Column(name='notes',format='A10')
    >>> c4=pyfits.Column(name='spectrum',format='5E')
    >>> c5=pyfits.Column(name='flag',format='L',array=[0,1,0,0])
    >>> coldefs=pyfits.ColDefs([c1,c2,c3,c4,c5])
    >>>
    >>> tbhdu=pyfits.new_table(coldefs)
    >>> tbhdu.data[0][3] = num.array([1.,2.,3.,4.,5.],
    ... dtype=num.float32)
    >>>
    >>> tbhdu2=pyfits.new_table(tbhdu1.data, nrows=9)
    >>>
    >>> # Assign the 4 rows from the second table to rows 5 thru
    ...   8 of the new table.  Note that the last row of the new
    ...   table will still be initialized to the default values.
    >>> tbhdu2.data[4:] = tbhdu.data
    >>>
    >>> print tbhdu2.data
    [ ('NGC1', 312, '0.0', array([ 0.,  0.,  0.,  0.,  0.],
    dtype=float32), True)
      ('NGC2', 334, '0.0', array([ 0.,  0.,  0.,  0.,  0.],
    dtype=float32), False)
      ('NGC3', 308, '0.0', array([ 0.,  0.,  0.,  0.,  0.],
    dtype=float32), True)
      ('NCG4', 317, '0.0', array([ 0.,  0.,  0.,  0.,  0.],
    dtype=float32), True)
      ('NGC5', 112, '0.0', array([ 1.,  2.,  3.,  4.,  5.],
    dtype=float32), False)
      ('NGC6', 134, '0.0', array([ 0.,  0.,  0.,  0.,  0.],
    dtype=float32), True)
      ('NGC7', 108, '0.0', array([ 0.,  0.,  0.,  0.,  0.],
    dtype=float32), False)
      ('NCG8', 117, '0.0', array([ 0.,  0.,  0.,  0.,  0.],
    dtype=float32), False)
      ('0.0', 0, '0.0', array([ 0.,  0.,  0.,  0.,  0.],
    dtype=float32), False)]
    >>>

The following bugs were fixed:

- Corrected bugs in HDUList.append and HDUList.insert to correctly handle the
  situation where you want to insert or append a Primary HDU as something
  other than the first HDU in an HDUList and the situation where you want to
  insert or append an Extension HDU as the first HDU in an HDUList.

- Corrected a bug involving scaled images (both compressed and not compressed)
  that include a BLANK, or ZBLANK card in the header.  When the image values
  match the BLANK or ZBLANK value, the value should be replaced with NaN after
  scaling.  Instead, pyfits was scaling the BLANK or ZBLANK value and
  returning it. (CNSHD766129)

- Corrected a byteswapping bug that occurs when writing certain column data.
  (CNSHD763307)

- Corrected a bug that occurs when creating a column from a chararray when one
  or more elements are shorter than the specified format length.  The bug
  wrote nulls instead of spaces to the file. (CNSHD695419)

- Corrected a bug in the HDU verification software to ensure that the header
  contains no NAXISn cards where n > NAXIS.

- Corrected a bug involving reading and writing compressed image data.  When
  written, the header keyword card ZTENSION will always have the value 'IMAGE'
  and when read, if the ZTENSION value is not 'IMAGE' the user will receive a
  warning, but the data will still be treated as image data.

- Corrected a bug that restricted the ability to create a custom HDU class and
  use it with pyfits.  The bug fix will allow something like this:

    >>> import pyfits
    >>> class MyPrimaryHDU(pyfits.PrimaryHDU):
    ...     def __init__(self, data=None, header=None):
    ...         pyfits.PrimaryHDU.__init__(self, data, header)
    ...     def _summary(self):
    ...         """
    ...         Reimplement a method of the class.
    ...         """
    ...         s = pyfits.PrimaryHDU._summary(self)
    ...         # change the behavior to suit me.
    ...         s1 = 'MyPRIMARY ' + s[11:]
    ...         return s1
    ...
    >>> hdul=pyfits.open("pix.fits",
    ... classExtensions={pyfits.PrimaryHDU: MyPrimaryHDU})
    >>> hdul.info()
    Filename: pix.fits
    No.    Name         Type      Cards   Dimensions   Format
    0    MyPRIMARY  MyPrimaryHDU     59  (512, 512)    int16
    >>>

- Modified ColDefs.add_col so that instead of returning a new ColDefs object
  with the column added to the end, it simply appends the new column to the
  current ColDefs object in place.  (CNSHD768778)

- Corrected a bug in ColDefs.del_col which raised a KeyError exception when
  deleting a column from a ColDefs object.

- Modified the open convenience function so that when a file is opened in
  readonly mode and the file contains no HDU's an IOError is raised.

- Modified _TableBaseHDU to ensure that all locations where data is referenced
  in the object actually reference the same ndarray, instead of copies of the
  array.

- Corrected a bug in the Column class that failed to initialize data when the
  data is a boolean array.  (CNSHD779136)

- Corrected a bug that caused an exception to be raised when creating a
  variable length format column from character data (PA format).

- Modified installation code so that when installing on Windows, when a C++
  compiler compatible with the Python binary is not found, the installation
  completes with a warning that all optional extension modules failed to
  build.  Previously, an Error was issued and the installation stopped.


2.2.2 (2009-10-12)
====================

Updates described in this release are only supported in the NUMPY version of
pyfits.

The following bugs were fixed:

- Corrected a bug that caused an exception to be raised when creating a
  CompImageHDU using an initial header that does not match the image data in
  terms of the number of axis.


2.2.1 (2009-10-06)
====================

Updates described in this release are only supported in the NUMPY version of
pyfits.

The following bugs were fixed:

- Corrected a bug that prevented the opening of a fits file where a header
  contained a CHECKSUM card but no DATASUM card.

- Corrected a bug that caused NULLs to be written instead of blanks when an
  ASCII table was created using a numpy chararray in which the original data
  contained trailing blanks.  (CNSHD695419)


2.2 (2009-09-23)
==================

Updates described in this release are only supported in the NUMPY version of
pyfits.

The following enhancements were made:

- Provide support for the FITS Checksum Keyword Convention.  (CNSHD754301)

- Adding the checksum=True keyword argument to the open convenience function
  will cause checksums to be verified on file open:

    >>> hdul=pyfits.open('in.fits', checksum=True)

- On output, CHECKSUM and DATASUM cards may be output to all HDU's in a fits
  file by using the keyword argument checksum=True in calls to the writeto
  convenience function, the HDUList.writeto method, the writeto methods of all
  of the HDU classes, and the append convenience function:

    >>> hdul.writeto('out.fits', checksum=True)

- Implemented a new insert method to the HDUList class that allows for the
  insertion of a HDU into a HDUList at a given index:

    >>> hdul.insert(2,hdu)

- Provided the capability to handle Unicode input for file names.

- Provided support for integer division required by Python 3.0.

The following bugs were fixed:

- Corrected a bug that caused an index out of bounds exception to be raised
  when iterating over the rows of a binary table HDU using the syntax  "for
  row in tbhdu.data:   ".  (CNSHD748609)

- Corrected a bug that prevented the use of the writeto convenience function
  for writing table data to a file.  (CNSHD749024)

- Modified the code to raise an IOError exception with the comment "Header
  missing END card." when pyfits can't find a valid END card for a header when
  opening a file.

  - This change addressed a problem with a non-standard fits file that
    contained several new-line characters at the end of each header and at the
    end of the file.  However, since some people want to be able to open these
    non-standard files anyway, an option was added to the open convenience
    function to allow these files to be opened without exception:

      >>> pyfits.open('infile.fits',ignore_missing_end=True)

- Corrected a bug that prevented the use of StringIO objects as fits files
  when reading and writing table data.  Previously, only image data was
  supported.  (CNSHD753698)

- Corrected a bug that caused a bus error to be generated when compressing
  image data using GZIP_1 under the Solaris operating system.

- Corrected bugs that prevented pyfits from properly reading Random Groups
  HDU's using numpy.  (CNSHD756570)

- Corrected a bug that can occur when writing a fits file.  (CNSHD757508)

  - If no default SIGINT signal handler has not been assigned, before the
    write, a TypeError exception is raised in the _File.flush() method when
    attempting to return the signal handler to its previous state.  Notably
    this occurred when using mod_python.  The code was changed to use SIG_DFL
    when no old handler was defined.

- Corrected a bug in CompImageHDU that prevented rescaling the image data
  using hdu.scale(option='old').


2.1.1 (2009-04-22)
===================

Updates described in this release are only supported in the NUMPY version of
pyfits.

The following bugs were fixed:

- Corrected a bug that caused an exception to be raised when closing a file
  opened for append, where an HDU was appended to the file, after data was
  accessed from the file.  This exception was only raised when running on a
  Windows platform.

- Updated the installation scripts, compression source code, and benchmark
  test scripts to properly install, build, and execute on a Windows platform.


2.1 (2009-04-14)
==================

Updates described in this release are only supported in the NUMPY version of
pyfits.

The following enhancements were made:

- Added new tdump and tcreate capabilities to pyfits.

  - The new tdump convenience function allows the contents of a binary table
    HDU to be dumped to a set of three files in ASCII format.  One file will
    contain column definitions, the second will contain header parameters, and
    the third will contain header data.

  - The new tcreate convenience function allows the creation of a binary table
    HDU from the three files dumped by the tdump convenience function.

  - The primary use for the tdump/tcreate methods are to allow editing in a
    standard text editor of the binary table data and parameters.

- Added support for case sensitive values of the EXTNAME card in an extension
  header.  (CNSHD745784)

  - By default, pyfits converts the value of EXTNAME cards to upper case when
    reading from a file.  A new convenience function
    (setExtensionNameCaseSensitive) was implemented to allow a user to
    circumvent this behavior so that the EXTNAME value remains in the same
    case as it is in the file.

  - With the following function call, pyfits will maintain the case of all
    characters in the EXTNAME card values of all extension HDU's during the
    entire python session, or until another call to the function is made:

      >>> import pyfits
      >>> pyfits.setExtensionNameCaseSensitive()

  - The following function call will return pyfits to its default (all upper
    case) behavior:

      >>> pyfits.setExtensionNameCaseSensitive(False)


- Added support for reading and writing FITS files in which the value of the
  first card in the header is 'SIMPLE=F'.  In this case, the pyfits open
  function returns an HDUList object that contains a single HDU of the new
  type _NonstandardHDU.  The header for this HDU is like a normal header (with
  the exception that the first card contains SIMPLE=F instead of SIMPLE=T).
  Like normal HDU's the reading of the data is delayed until actually
  requested.  The data is read from the file into a string starting from the
  first byte after the header END card and continuing till the end of the
  file.  When written, the header is written, followed by the data string.  No
  attempt is made to pad the data string so that it fills into a standard 2880
  byte FITS block.  (CNSHD744730)

- Added support for FITS files containing  extensions with unknown XTENSION
  card values.  (CNSHD744730)  Standard FITS files support extension HDU's of
  types TABLE, IMAGE, BINTABLE, and A3DTABLE.  Accessing a nonstandard
  extension from a FITS file will now create a _NonstandardExtHDU object.
  Accessing the data of this object will cause the data to be read from the
  file into a string.  If the HDU is written back to a file the string data is
  written after the Header and padded to fill a standard 2880 byte FITS block.

The following bugs were fixed:

- Extensive changes were made to the tiled image compression code to support
  the latest enhancements made in CFITSIO version 3.13 to support this
  convention.

- Eliminated a memory leak in the tiled image compression code.

- Corrected a bug in the FITS_record.__setitem__ method which raised a
  NameError exception when attempting to set a value in a FITS_record object.
  (CNSHD745844)

- Corrected a bug that caused a TypeError exception to be raised when reading
  fits files containing large table HDU's (>2Gig).  (CNSHD745522)

- Corrected a bug that caused a TypeError exception to be raised for all calls
  to the warnings module when running under Python 2.6.  The formatwarning
  method in the warnings module was changed in Python 2.6 to include a new
  argument.  (CNSHD746592)

- Corrected the behavior of the membership (in) operator in the Header class
  to check against header card keywords instead of card values.  (CNSHD744730)

- Corrected the behavior of iteration on a Header object.  The new behavior
  iterates over the unique card keywords instead of the card values.


2.0.1 (2009-02-03)
====================

Updates described in this release are only supported in the NUMPY version of
pyfits.

The following bugs were fixed:

- Eliminated a memory leak when reading Table HDU's from a fits file.
  (CNSHD741877)


2.0 (2009-01-30)
==================

Updates described in this release are only supported in the NUMPY version of
pyfits.

The following enhancements were made:

- Provide initial support for an image compression convention known as the
  "Tiled Image Compression Convention" `[1]`_.

  - The principle used in this convention is to first divide the n-dimensional
    image into a rectangular grid of subimages or "tiles".  Each tile is then
    compressed as a continuous block of data, and the resulting compressed
    byte stream is stored in a row of a variable length column in a FITS
    binary table.  Several commonly used algorithms for compressing image
    tiles are supported.  These include, GZIP, RICE, H-Compress and IRAF pixel
    list (PLIO).

  - Support for compressed image data is provided using the optional
    "pyfitsComp" module contained in a C shared library (pyfitsCompmodule.so).

  - The header of a compressed image HDU appears to the user like any image
    header.  The actual header stored in the FITS file is that of a binary
    table HDU with a set of special keywords, defined by the convention, to
    describe the structure of the compressed image.  The conversion between
    binary table HDU header and image HDU header is all performed behind the
    scenes.  Since the HDU is actually a binary table, it may not appear as a
    primary HDU in a FITS file.

  - The data of a compressed image HDU appears to the user as standard
    uncompressed image data.  The actual data is stored in the fits file as
    Binary Table data containing at least one column (COMPRESSED_DATA).  Each
    row of this variable-length column contains the byte stream that was
    generated as a result of compressing the corresponding image tile.
    Several optional columns may also appear.  These include,
    UNCOMPRESSED_DATA to hold the uncompressed pixel values for tiles that
    cannot be compressed, ZSCALE and ZZERO to hold the linear scale factor and
    zero point offset which may be needed to transform the raw uncompressed
    values back to the original image pixel values, and ZBLANK to hold the
    integer value used to represent undefined pixels (if any) in the image.

  - To create a compressed image HDU from scratch, simply construct a
    CompImageHDU object from an uncompressed image data array and its
    associated image header.  From there, the HDU can be treated just like any
    image HDU:

      >>> hdu=pyfits.CompImageHDU(imageData,imageHeader)
      >>> hdu.writeto('compressed_image.fits')

  - The signature for the CompImageHDU initializer method describes the
    possible options for constructing a CompImageHDU object::

      def __init__(self, data=None, header=None, name=None,
                   compressionType='RICE_1',
                   tileSize=None,
                   hcompScale=0.,
                   hcompSmooth=0,
                   quantizeLevel=16.):
          """
              data:            data of the image
              header:          header to be associated with the
                               image
              name:            the EXTNAME value; if this value
                               is None, then the name from the
                               input image header will be used;
                               if there is no name in the input
                               image header then the default name
                               'COMPRESSED_IMAGE' is used
              compressionType: compression algorithm 'RICE_1',
                               'PLIO_1', 'GZIP_1', 'HCOMPRESS_1'
              tileSize:        compression tile sizes default
                               treats each row of image as a tile
              hcompScale:      HCOMPRESS scale parameter
              hcompSmooth:     HCOMPRESS smooth parameter
              quantizeLevel:   floating point quantization level;
          """

- Added two new convenience functions.  The setval function allows the setting
  of the value of a single header card in a fits file.  The delval function
  allows the deletion of a single header card in a fits file.

- A modification was made to allow the reading of data from a fits file
  containing a Table HDU that has duplicate field names.  It is normally a
  requirement that the field names in a Table HDU be unique.  Prior to this
  change a ValueError was raised, when the data was accessed, to indicate that
  the HDU contained duplicate field names.  Now, a warning is issued and the
  field names are made unique in the internal record array.  This will not
  change the TTYPEn header card values.  You will be able to get the data from
  all fields using the field name, including the first field containing the
  name that is duplicated.  To access the data of the other fields with the
  duplicated names you will need to use the field number instead of the field
  name.  (CNSHD737193)

- An enhancement was made to allow the reading of unsigned integer 16 values
  from an ImageHDU when the data is signed integer 16 and BZERO is equal to
  32784 and BSCALE is equal to 1 (the standard way for scaling unsigned
  integer 16 data).  A new optional keyword argument (uint16) was added to the
  open convenience function.  Supplying a value of True for this argument will
  cause data of this type to be read in and scaled into an unsigned integer 16
  array, instead of a float 32 array.  If a HDU associated with a file that
  was opened with the uint16 option and containing unsigned integer 16 data is
  written to a file, the data will be reverse scaled into an integer 16 array
  and written out to the file and the BSCALE/BZERO header cards will be
  written with the values 1 and 32768 respectively.  (CHSHD736064) Reference
  the following example:

    >>> import pyfits
    >>> hdul=pyfits.open('o4sp040b0_raw.fits',uint16=1)
    >>> hdul[1].data
    array([[1507, 1509, 1505, ..., 1498, 1500, 1487],
           [1508, 1507, 1509, ..., 1498, 1505, 1490],
           [1505, 1507, 1505, ..., 1499, 1504, 1491],
           ...,
           [1505, 1506, 1507, ..., 1497, 1502, 1487],
           [1507, 1507, 1504, ..., 1495, 1499, 1486],
           [1515, 1507, 1504, ..., 1492, 1498, 1487]], dtype=uint16)
    >>> hdul.writeto('tmp.fits')
    >>> hdul1=pyfits.open('tmp.fits',uint16=1)
    >>> hdul1[1].data
    array([[1507, 1509, 1505, ..., 1498, 1500, 1487],
           [1508, 1507, 1509, ..., 1498, 1505, 1490],
           [1505, 1507, 1505, ..., 1499, 1504, 1491],
           ...,
           [1505, 1506, 1507, ..., 1497, 1502, 1487],
           [1507, 1507, 1504, ..., 1495, 1499, 1486],
           [1515, 1507, 1504, ..., 1492, 1498, 1487]], dtype=uint16)
    >>> hdul1=pyfits.open('tmp.fits')
    >>> hdul1[1].data
    array([[ 1507.,  1509.,  1505., ...,  1498.,  1500.,  1487.],
           [ 1508.,  1507.,  1509., ...,  1498.,  1505.,  1490.],
           [ 1505.,  1507.,  1505., ...,  1499.,  1504.,  1491.],
           ...,
           [ 1505.,  1506.,  1507., ...,  1497.,  1502.,  1487.],
           [ 1507.,  1507.,  1504., ...,  1495.,  1499.,  1486.],
           [ 1515.,  1507.,  1504., ...,  1492.,  1498.,  1487.]], dtype=float32)

- Enhanced the message generated when a ValueError exception is raised when
  attempting to access a header card with an unparsable value.  The message
  now includes the Card name.

The following bugs were fixed:

- Corrected a bug that occurs when appending a binary table HDU to a fits
  file.  Data was not being byteswapped on little endian machines.
  (CNSHD737243)

- Corrected a bug that occurs when trying to write an ImageHDU that is missing
  the required PCOUNT card in the header.  An UnboundLocalError exception
  complaining that the local variable 'insert_pos' was referenced before
  assignment was being raised in the method _ValidHDU.req_cards.  The code was
  modified so that it would properly issue a more meaningful ValueError
  exception with a description of what required card is missing in the header.

- Eliminated a redundant warning message about the PCOUNT card when validating
  an ImageHDU header with a PCOUNT card that is missing or has a value other
  than 0.

.. _[1]: https://fits.gsfc.nasa.gov/registry/tilecompression.html


1.4.1 (2008-11-04)
====================

Updates described in this release are only supported in the NUMPY version of
pyfits.

The following enhancements were made:

- Enhanced the way import errors are reported to provide more information.

The following bugs were fixed:

- Corrected a bug that occurs when a card value is a string and contains a
  colon but is not a record-valued keyword card.

- Corrected a bug where pyfits fails to properly handle a record-valued
  keyword card with values using exponential notation and trailing blanks.


1.4 (2008-07-07)
==================

Updates described in this release are only supported in the NUMPY version of
pyfits.

The following enhancements were made:

- Added support for file objects and file like objects.

  - All convenience functions and class methods that take a file name will now
    also accept a file object or file like object.  File like objects
    supported are StringIO and GzipFile objects.  Other file like objects will
    work only if they implement all of the standard file object methods.

  - For the most part, file or file like objects may be either opened or
    closed at function call.  An opened object must be opened with the proper
    mode depending on the function or method called.  Whenever possible, if
    the object is opened before the method is called, it will remain open
    after the call.  This will not be possible when writing a HDUList that has
    been resized or when writing to a GzipFile object regardless of whether it
    is resized.  If the object is closed at the time of the function call,
    only the name from the object is used, not the object itself.  The pyfits
    code will extract the file name used by the object and use that to create
    an underlying file object on which the function will be performed.

- Added support for record-valued keyword cards as introduced in the "FITS WCS
  proposal for representing a more general distortion model".

  - Record-valued keyword cards are string-valued cards where the string is
    interpreted as a definition giving a record field name, and its floating
    point value.  In a FITS header they have the following syntax::

      keyword= 'field-specifier: float'

    where keyword is a standard eight-character FITS keyword name, float is
    the standard FITS ASCII representation of a floating point number, and
    these are separated by a colon followed by a single blank.

    The grammar for field-specifier is::

      field-specifier:
          field
          field-specifier.field

      field:
          identifier
          identifier.index

    where identifier is a sequence of letters (upper or lower case),
    underscores, and digits of which the first character must not be a digit,
    and index is a sequence of digits.  No blank characters may occur in the
    field-specifier.  The index is provided primarily for defining array
    elements though it need not be used for that purpose.

    Multiple record-valued keywords of the same name but differing values may
    be present in a FITS header.  The field-specifier may be viewed as part of
    the keyword name.

    Some examples follow::

      DP1     = 'NAXIS: 2'
      DP1     = 'AXIS.1: 1'
      DP1     = 'AXIS.2: 2'
      DP1     = 'NAUX: 2'
      DP1     = 'AUX.1.COEFF.0: 0'
      DP1     = 'AUX.1.POWER.0: 1'
      DP1     = 'AUX.1.COEFF.1: 0.00048828125'
      DP1     = 'AUX.1.POWER.1: 1'

  - As with standard header cards, the value of a record-valued keyword card
    can be accessed using either the index of the card in a HDU's header or
    via the keyword name.  When accessing using the keyword name, the user may
    specify just the card keyword or the card keyword followed by a period
    followed by the field-specifier.  Note that while the card keyword is case
    insensitive, the field-specifier is not.  Thus, hdu['abc.def'],
    hdu['ABC.def'], or hdu['aBc.def'] are all equivalent but hdu['ABC.DEF'] is
    not.

  - When accessed using the card index of the HDU's header the value returned
    will be the entire string value of the card.  For example:

      >>> print hdr[10]
      NAXIS: 2
      >>> print hdr[11]
      AXIS.1: 1

  - When accessed using the keyword name exclusive of the field-specifier, the
    entire string value of the header card with the lowest index having that
    keyword name will be returned.  For example:

      >>> print hdr['DP1']
      NAXIS: 2

  - When accessing using the keyword name and the field-specifier, the value
    returned will be the floating point value associated with the
    record-valued keyword card.  For example:

      >>> print hdr['DP1.NAXIS']
      2.0

  - Any attempt to access a non-existent record-valued keyword card value will
    cause an exception to be raised (IndexError exception for index access or
    KeyError for keyword name access).

  - Updating the value of a record-valued keyword card can also be
    accomplished using either index or keyword name.  For example:

      >>> print hdr['DP1.NAXIS']
      2.0
      >>> hdr['DP1.NAXIS'] = 3.0
      >>> print hdr['DP1.NAXIS']
      3.0

  - Adding a new record-valued keyword card to an existing header is
    accomplished using the Header.update() method just like any other card.
    For example:

      >>> hdr.update('DP1', 'AXIS.3: 1', 'a comment', after='DP1.AXIS.2')

  - Deleting a record-valued keyword card from an existing header is
    accomplished using the standard list deletion syntax just like any other
    card.  For example:

      >>> del hdr['DP1.AXIS.1']

  - In addition to accessing record-valued keyword cards individually using a
    card index or keyword name, cards can be accessed in groups using a set of
    special pattern matching keys.  This access is made available via the
    standard list indexing operator providing a keyword name string that
    contains one or more of the special pattern matching keys.  Instead of
    returning a value, a CardList object will be returned containing shared
    instances of the Cards in the header that match the given keyword
    specification.

  - There are three special pattern matching keys.  The first key '*' will
    match any string of zero or more characters within the current level of
    the field-specifier.  The second key '?' will match a single character.
    The third key '...' must appear at the end of the keyword name string and
    will match all keywords that match the preceding pattern down all levels
    of the field-specifier.  All combinations of ?, \*, and ... are permitted
    (though ... is only permitted at the end).  Some examples follow:

      >>> cl=hdr['DP1.AXIS.*']
      >>> print cl
      DP1     = 'AXIS.1: 1'
      DP1     = 'AXIS.2: 2'
      >>> cl=hdr['DP1.*']
      >>> print cl
      DP1     = 'NAXIS: 2'
      DP1     = 'NAUX: 2'
      >>> cl=hdr['DP1.AUX...']
      >>> print cl
      DP1     = 'AUX.1.COEFF.0: 0'
      DP1     = 'AUX.1.POWER.0: 1'
      DP1     = 'AUX.1.COEFF.1: 0.00048828125'
      DP1     = 'AUX.1.POWER.1: 1'
      >>> cl=hdr['DP?.NAXIS']
      >>> print cl
      DP1     = 'NAXIS: 2'
      DP2     = 'NAXIS: 2'
      DP3     = 'NAXIS: 2'
      >>> cl=hdr['DP1.A*S.*']
      >>> print cl
      DP1     = 'AXIS.1: 1'
      DP1     = 'AXIS.2: 2'

  - The use of the special pattern matching keys for adding or updating header
    cards in an existing header is not allowed.  However, the deletion of
    cards from the header using the special keys is allowed.  For example:

      >>> del hdr['DP3.A*...']

- As noted above, accessing pyfits Header object using the special pattern
  matching keys will return a CardList object.  This CardList object can
  itself be searched in order to further refine the list of Cards.  For
  example:

      >>> cl=hdr['DP1...']
      >>> print cl
      DP1     = 'NAXIS: 2'
      DP1     = 'AXIS.1: 1'
      DP1     = 'AXIS.2: 2'
      DP1     = 'NAUX: 2'
      DP1     = 'AUX.1.COEFF.1: 0.000488'
      DP1     = 'AUX.2.COEFF.2: 0.00097656'
      >>> cl1=cl['*.*AUX...']
      >>> print cl1
      DP1     = 'NAUX: 2'
      DP1     = 'AUX.1.COEFF.1: 0.000488'
      DP1     = 'AUX.2.COEFF.2: 0.00097656'

  - The CardList keys() method will allow the retrieval of all of the key
    values in the CardList.  For example:

      >>> cl=hdr['DP1.AXIS.*']
      >>> print cl
      DP1     = 'AXIS.1: 1'
      DP1     = 'AXIS.2: 2'
      >>> cl.keys()
      ['DP1.AXIS.1', 'DP1.AXIS.2']

  - The CardList values() method will allow the retrieval of all of the values
    in the CardList.  For example:

      >>> cl=hdr['DP1.AXIS.*']
      >>> print cl
      DP1     = 'AXIS.1: 1'
      DP1     = 'AXIS.2: 2'
      >>> cl.values()
      [1.0, 2.0]

  - Individual cards can be retrieved from the list using standard list
    indexing.  For example:

      >>> cl=hdr['DP1.AXIS.*']
      >>> c=cl[0]
      >>> print c
      DP1     = 'AXIS.1: 1'
      >>> c=cl['DP1.AXIS.2']
      >>> print c
      DP1     = 'AXIS.2: 2'

  - Individual card values can be retrieved from the list using the value
    attribute of the card.  For example:

      >>> cl=hdr['DP1.AXIS.*']
      >>> cl[0].value
      1.0

  - The cards in the CardList are shared instances of the cards in the source
    header.  Therefore, modifying a card in the CardList also modifies it in
    the source header.  However, making an addition or a deletion to the
    CardList will not affect the source header.  For example:

      >>> hdr['DP1.AXIS.1']
      1.0
      >>> cl=hdr['DP1.AXIS.*']
      >>> cl[0].value = 4.0
      >>> hdr['DP1.AXIS.1']
      4.0
      >>> del cl[0]
      >>> print cl['DP1.AXIS.1']
      Traceback (most recent call last):
      ...
      KeyError: "Keyword 'DP1.AXIS.1' not found."
      >>> hdr['DP1.AXIS.1']
      4.0

  - A FITS header consists of card images.  In pyfits each card image is
    manifested by a Card object.  A pyfits Header object contains a list of
    Card objects in the form of a CardList object.  A record-valued keyword
    card image is represented in pyfits by a RecordValuedKeywordCard object.
    This object inherits from a Card object and has all of the methods and
    attributes of a Card object.

  - A new RecordValuedKeywordCard object is created with the
    RecordValuedKeywordCard constructor: RecordValuedKeywordCard(key, value,
    comment).  The key and value arguments may be specified in two ways.  The
    key value may be given as the 8 character keyword only, in which case the
    value must be a character string containing the field-specifier, a colon
    followed by a space, followed by the actual value.  The second option is
    to provide the key as a string containing the keyword and field-specifier,
    in which case the value must be the actual floating point value.  For
    example:

      >>> c1 = pyfits.RecordValuedKeywordCard('DP1', 'NAXIS: 2', 'Number of variables')
      >>> c2 = pyfits.RecordValuedKeywordCard('DP1.AXIS.1', 1.0, 'Axis number')

  - RecordValuedKeywordCards have attributes .key, .field_specifier, .value,
    and .comment.  Both .value and .comment can be changed but not .key or
    .field_specifier.  The constructor will extract the field-specifier from
    the input key or value, whichever is appropriate.  The .key attribute is
    the 8 character keyword.

  - Just like standard Cards, a RecordValuedKeywordCard may be constructed
    from a string using the fromstring() method or verified using the verify()
    method.  For example:

      >>> c1 = pyfits.RecordValuedKeywordCard().fromstring(
               "DP1     = 'NAXIS: 2' / Number of independent variables")
      >>> c2 = pyfits.RecordValuedKeywordCard().fromstring(
               "DP1     = 'AXIS.1: X' / Axis number")
      >>> print c1; print c2
      DP1     = 'NAXIS: 2' / Number of independent variables
      DP1     = 'AXIS.1: X' / Axis number
      >>> c2.verify()
      Output verification result:
      Card image is not FITS standard (unparsable value string).

  - A standard card that meets the criteria of a RecordValuedKeywordCard may
    be turned into a RecordValuedKeywordCard using the class method coerce.
    If the card object does not meet the required criteria then the original
    card object is just returned.

      >>> c1 = pyfits.Card('DP1','AUX: 1','comment')
      >>> c2 = pyfits.RecordValuedKeywordCard.coerce(c1)
      >>> print type(c2)
      <'pyfits.NP_pyfits.RecordValuedKeywordCard'>

  - Two other card creation methods are also available as
    RecordVauedKeywordCard class methods.  These are createCard() which will
    create the appropriate card object (Card or RecordValuedKeywordCard) given
    input key, value, and comment, and createCardFromString which will create
    the appropriate card object given an input string.  These two methods are
    also available as convenience functions:

      >>> c1 = pyfits.RecordValuedKeywordCard.createCard('DP1','AUX: 1','comment')

    or

      >>> c1 = pyfits.createCard('DP1','AUX: 1','comment')
      >>> print type(c1)
      <'pyfits.NP_pyfits.RecordValuedKeywordCard'>

      >>> c1 = pyfits.RecordValuedKeywordCard.createCard('DP1','AUX 1','comment')

    or

      >>> c1 = pyfits.createCard('DP1','AUX 1','comment')
      >>> print type(c1)
      <'pyfits.NP_pyfits.Card'>

      >>> c1 = pyfits.RecordValuedKeywordCard.createCardFromString \
               ("DP1 = 'AUX: 1.0' / comment")

    or

      >>> c1 = pyfits.createCardFromString("DP1     = 'AUX: 1.0' / comment")
      >>> print type(c1)
      <'pyfits.NP_pyfits.RecordValuedKeywordCard'>

The following bugs were fixed:

- Corrected a bug that occurs when writing a HDU out to a file.  During the
  write, any Keyboard Interrupts are trapped so that the write completes
  before the interrupt is handled.  Unfortunately, the Keyboard Interrupt was
  not properly reinstated after the write completed.  This was fixed.
  (CNSHD711138)

- Corrected a bug when using ipython, where temporary files created with the
  tempFile.NamedTemporaryFile method are not automatically removed.  This can
  happen for instance when opening a Gzipped fits file or when open a fits
  file over the internet.  The files will now be removed.  (CNSHD718307)

- Corrected a bug in the append convenience function's call to the writeto
  convenience function.  The classExtensions argument must be passed as a
  keyword argument.

- Corrected a bug that occurs when retrieving variable length character arrays
  from binary table HDUs (PA() format) and using slicing to obtain rows of
  data containing variable length arrays.  The code issued a TypeError
  exception.  The data can now be accessed with no exceptions. (CNSHD718749)

- Corrected a bug that occurs when retrieving data from a fits file opened in
  memory map mode when the file contains multiple image extensions or ASCII
  table or binary table HDUs.  The code issued a TypeError exception.  The
  data can now be accessed with no exceptions.  (CNSHD707426)

- Corrected a bug that occurs when attempting to get a subset of data from a
  Binary Table HDU and then use the data to create a new Binary Table HDU
  object.  A TypeError exception was raised.  The data can now be subsetted
  and used to create a new HDU.  (CNSHD723761)

- Corrected a bug that occurs when attempting to scale an Image HDU back to
  its original data type using the _ImageBaseHDU.scale method.  The code was
  not resetting the BITPIX header card back to the original data type.  This
  has been corrected.

- Changed the code to issue a KeyError exception instead of a NameError
  exception when accessing a non-existent field in a table.


1.3 (2008-02-22)
==================

Updates described in this release are only supported in the NUMPY version of
pyfits.

The following enhancements were made:

- Provided support for a new extension to pyfits called *stpyfits*.

  - The *stpyfits* module is a wrapper around pyfits.  It provides all of the
    features and functions of pyfits along with some STScI specific features.
    Currently, the only new feature supported by stpyfits is the ability to
    read and write fits files that contain image data quality extensions with
    constant data value arrays.  See stpyfits `[2]`_ for more details on
    stpyfits.

- Added a new feature to allow trailing HDUs to be deleted from a fits file
  without actually reading the data from the file.

  - This supports a JWST requirement to delete a trailing HDU from a file
    whose primary Image HDU is too large to be read on a 32 bit machine.

- Updated pyfits to use the warnings module to issue warnings.  All warnings
  will still be issued to stdout, exactly as they were before, however, you
  may now suppress warnings with the -Wignore command line option.  For
  example, to run a script that will ignore warnings use the following command
  line syntax:

    python -Wignore yourscript.py

- Updated the open convenience function to allow the input of an already
  opened file object in place of a file name when opening a fits file.

- Updated the writeto convenience function to allow it to accept the
  output_verify option.

  - In this way, the user can use the argument output_verify='fix' to allow
    pyfits to correct any errors it encounters in the provided header before
    writing the data to the file.

- Updated the verification code to provide additional detail with a
  VerifyError exception.

- Added the capability to create a binary table HDU directly from a
  numpy.ndarray.  This may be done using either the new_table convenience
  function or the BinTableHDU constructor.


The following performance improvements were made:

- Modified the import logic to dramatically decrease the time it takes to
  import pyfits.

- Modified the code to provide performance improvements when copying and
  examining header cards.

The following bugs were fixed:

- Corrected a bug that occurs when reading the data from a fits file that
  includes BZERO/BSCALE scaling.  When the data is read in from the file,
  pyfits automatically scales the data using the BZERO/BSCALE values in the
  header.  In the previous release, pyfits created a 32 bit floating point
  array to hold the scaled data.  This could cause a problem when the value of
  BZERO is so large that the scaled value will not fit into the float 32.  For
  this release, when the input data is 32 bit integer, a 64 bit floating point
  array is used for the scaled data.

- Corrected a bug that caused an exception to be raised when attempting to
  scale image data using the ImageHDU.scale method.

- Corrected a bug in the new_table convenience function that occurred when a
  binary table was created using a ColDefs object as input and supplying an
  nrows argument for a number of rows that is greater than the number of rows
  present in the input ColDefs object.  The previous version of pyfits failed
  to allocate the necessary memory for the additional rows.

- Corrected a bug in the new_table convenience function that caused an
  exception to be thrown when creating an ASCII table.

- Corrected a bug in the new_table convenience function that will allow the
  input of a ColDefs object that was read from a file as a binary table with a
  data value equal to None.

- Corrected a bug in the construction of ASCII tables from Column objects that
  are created with noncontinuous start columns.

- Corrected bugs in a number of areas that would sometimes cause a failure to
  improperly raise an exception when an error occurred.

- Corrected a bug where attempting to open a non-existent fits file on a
  windows platform using a drive letter in the file specification caused a
  misleading IOError exception to be raised.

.. _[2]: https://stscitools.readthedocs.io/en/latest/stpyfits.html


1.1 (2007-06-15)
==================

- Modified to use either NUMPY or NUMARRAY.

- New file writing modes have been provided to allow streaming data to
  extensions without requiring the whole output extension image in memory. See
  documentation on StreamingHDU.

- Improvements to minimize byteswapping and memory usage by byteswapping in
  place.

- Now supports ':' characters in filenames.

- Handles keyboard interrupts during long operations.

- Preserves the byte order of the input image arrays.


1.0.1 (2006-03-24)
====================

The changes to PyFITS were primarily to improve the docstrings and to
reclassify some public functions and variables as private. Readgeis and
fitsdiff which were distributed with PyFITS in previous releases were moved to
pytools. This release of PyFITS is v1.0.1. The next release of PyFITS will
support both numarray and numpy (and will be available separately from
stsci_python, as are all the python packages contained within stsci_python).
An alpha release for PyFITS numpy support will be made around the time of this
stsci_python release.

- Updated docstrings for public functions.

- Made some previously public functions private.


1.0 (2005-11-01)
==================

Major Changes since v0.9.6:

- Added support for the HIERARCH convention

- Added support for iteration and slicing for HDU lists

- PyFITS now uses the standard setup.py installation script

- Add utility functions at the module level, they include:

  - getheader
  - getdata
  - getval
  - writeto
  - append
  - update
  - info

Minor changes since v0.9.6:

- Fix a bug to make single-column ASCII table work.

- Fix a bug so a new table constructed from an existing table with X-formatted
  columns will work.

- Fix a problem in verifying HDUList right after the open statement.

- Verify that elements in an HDUList, besides the first one, are ExtensionHDU.

- Add output verification in methods flush() and close().

- Modify the design of the open() function to remove the output_verify
  argument.

- Remove the groups argument in GroupsHDU's constructor.

- Redesign the column definition class to make its column components more
  accessible.  Also to make it conducive for higher level functionalities,
  e.g. combining two column definitions.

- Replace the Boolean class with the Python Boolean type.  The old TRUE/FALSE
  will still work.

- Convert classes to the new style.

- Better format when printing card or card list.

- Add the optional argument clobber to all writeto() functions and methods.

- If adding a blank card, will not use existing blank card's space.

PyFITS Version 1.0 REQUIRES Python 2.3 or later.


0.9.6 (2004-11-11)
====================

Major changes since v0.9.3:

- Support for variable length array tables.

- Support for writing ASCII table extensions.

- Support for random groups, both reading and writing.

Some minor changes:

- Support for numbers with leading zeros in an ASCII table extension.

- Changed scaled columns' data type from Float32 to Float64 to preserve
  precision.

- Made Column constructor more flexible in accepting format specification.


0.9.3 (2004-07-02)
====================

Changes since v0.9.0:

- Lazy instantiation of full Headers/Cards for all HDU's when the file is
  opened.  At the open, only extracts vital info (e.g. NAXIS's) from the
  header parts.  This change will speed up the performance if the user only
  needs to access one extension in a multi-extension FITS file.

- Support the X format (bit flags) columns, both reading and writing, in a
  binary table.  At the user interface, they are converted to Boolean arrays
  for easy manipulation.  For example, if the column's TFORM is "11X",
  internally the data is stored in 2 bytes, but the user will see, at each row
  of this column, a Boolean array of 11 elements.

- Fix a bug such that when a table extension has no data, it will not try to
  scale the data when updating/writing the HDU list.


0.9 (2004-04-27)
==================

Changes since v0.8.0:

- Rewriting of the Card class to separate the parsing and verification of
  header cards

- Restructure the keyword indexing scheme which speed up certain applications
  (update large number of new keywords and reading a header with larger
  numbers of cards) by a factor of 30 or more

- Change the default to be lenient FITS standard checking on input and strict
  FITS standard checking on output

- Support CONTINUE cards, both reading and writing

- Verification can now be performed at any of the HDUList, HDU, and Card
  levels

- Support (contiguous) subsection (attribute .section) of images to reduce
  memory usage for large images


0.8.0 (2003-08-19)
====================

**NOTE:** This version will only work with numarray Version 0.6.  In addition,
earlier versions of PyFITS will not work with numarray 0.6.  Therefore, both
must be updated simultaneously.

Changes since 0.7.6:

- Compatible with numarray 0.6/records 2.0

- For binary tables, now it is possible to update the original array if a
  scaled field is updated.

- Support of complex columns

- Modify the __getitem__ method in FITS_rec.  In order to make sure the scaled
  quantities are also viewing the same data as the original FITS_rec, all
  fields need to be "touched" when __getitem__ is called.

- Add a new attribute mmobject for HDUList, and close the memmap object when
  close HDUList object.  Earlier version does not close memmap object and can
  cause memory lockup.

- Enable 'update' as a legitimate memmap mode.

- Do not print message when closing an HDUList object which is not created
  from reading a FITS file.  Such message is confusing.

- remove the internal attribute "closed" and related method (__getattr__ in
  HDUList).  It is redundant.


0.7.6 (2002-11-22)

**NOTE:** This version will only work with numarray Version 0.4.

Changes since 0.7.5:

- Change x*=n to numarray.multiply(x, n, x) where n is a floating number, in
  order to make pyfits to work under Python 2.2. (2 occurrences)

- Modify the "update" method in the Header class to use the "fixed-format"
  card even if the card already exists.  This is to avoid the misalignment as
  shown below:

  After running drizzle on ACS images it creates a CD matrix whose elements
  have very many digits, *e.g.*:

    CD1_1   =  1.1187596304411E-05 / partial of first axis coordinate w.r.t. x
    CD1_2   = -8.502767249350019E-06 / partial of first axis coordinate w.r.t. y

  with pyfits, an "update" on these header items and write in new values which
  has fewer digits, *e.g.*:

    CD1_1   =        1.0963011E-05 / partial of first axis coordinate w.r.t. x
    CD1_2   =          -8.527229E-06 / partial of first axis coordinate w.r.t. y

- Change some internal variables to make their appearance more consistent:

    old name                new name

    __octalRegex            _octalRegex
    __readblock()           _readblock()
    __formatter()           _formatter().
    __value_RE              _value_RE
    __numr                  _numr
    __comment_RE            _comment_RE
    __keywd_RE              _keywd_RE
    __number_RE             _number_RE.
    tmpName()               _tmpName()
    dimShape                _dimShape
    ErrList                 _ErrList

- Move up the module description.  Move the copyright statement to the bottom
  and assign to the variable __credits__.

- change the following line:

    self.__dict__ = input.__dict__

  to

    self.__setstate__(input.__getstate__())

  in order for pyfits to run under numarray 0.4.

- edit _readblock to add the (optional) firstblock argument and raise IOError
  if the first 8 characters in the first block is not 'SIMPLE  ' or
  'XTENSION'.  Edit the function open to check for IOError to skip the last
  null filled block(s).  Edit readHDU to add the firstblock argument.


0.7.5 (2002-08-16)
====================

Changes since v0.7.3:

- Memory mapping now works for readonly mode, both for images and binary
  tables.

  Usage:  pyfits.open('filename', memmap=1)

- Edit the field method in FITS_rec class to make the column scaling for
  numbers use less temporary memory.  (does not work under 2.2, due to Python
  "bug" of array \*=)

- Delete bscale/bzero in the ImageBaseHDU constructor.

- Update bitpix in BaseImageHDU.__getattr__  after deleting bscale/bzero. (bug
  fix)

- In BaseImageHDU.__getattr__  point self.data to raw_data if float and if not
  memmap.  (bug fix).

- Change the function get_tbdata() to private: _get_tbdata().


0.7.3 (2002-07-12)
====================

Changes since v0.7.2:

- It will scale all integer image data to Float32, if BSCALE/BZERO != 1/0.  It
  will also expunge the BSCALE/BZERO keywords.

- Add the scale() method for ImageBaseHDU, so data can be scaled just before
  being written to the file.  It has the following arguments:

  type: destination data type (string), e.g. Int32, Float32, UInt8, etc.

  option: scaling scheme. if 'old', use the old BSCALE/BZERO values.  if
  'minmax', use the data range to fit into the full range of specified integer
  type.  Float destination data type will not be scaled for this option.

  bscale/bzero: user specifiable BSCALE/BZERO values.  They overwrite the
  "option".

- Deal with data area resizing in 'update' mode.

- Make the data scaling (both input and output) faster and use less memory.

- Bug fix to make column name change takes effect for field.

- Bug fix to avoid exception if the key is not present in the header already.
  This affects (fixes) add_history(), add_comment(), and add_blank().

- Bug fix in __getattr__() in Card class.  The change made in 0.7.2 to rstrip
  the comment must be string type to avoid exception.

0.7.2.1 (2002-06-25)
======================

A couple of bugs were addressed in this version.

- Fix a bug in _add_commentary(). Due to a change in index_of() during version
  0.6.5.5, _add_commentary needs to be modified to avoid exception if the key
  is not present in the header already. This affects (fixes) add_history(),
  add_comment(), and add_blank().

- Fix a bug in __getattr__() in Card class. The change made in 0.7.2 to rstrip
  the comment must be string type to avoid exception.


0.7.2 (2002-06-19)
====================

The two major improvements from Version 0.6.2 are:

- support reading tables  with "scaled" columns (e.g.  tscal/tzero, Boolean,
  and ASCII tables)

- a prototype output verification.

This version of PyFITS requires numarray version 0.3.4.

Other changes include:

- Implement the new HDU hierarchy proposed earlier this year.  This in turn
  reduces some of the redundant methods common to several HDU classes.

- Add 3 new methods to the Header class: add_history, add_comment, and
  add_blank.

- The table attributes _columns are now .columns and the attributes in ColDefs
  are now all without the underscores.  So, a user can get a list of column
  names by: hdu.columns.names.

- The "fill" argument in the new_table method now has a new meaning:<br> If
  set to true (=1), it will fill the entire new table with zeros/blanks.
  Otherwise (=0), just the extra rows/cells are filled with zeros/blanks.
  Fill values other than zero/blank are now not possible.

- Add the argument output_verify to the open method and writeto method.  Not
  in the flush or close methods yet, due to possible complication.

- A new copy method for tables, the copy is totally independent from the table
  it copies from.

- The tostring() call in writeHDUdata takes up extra space to store the string
  object.  Use tofile() instead, to save space.

- Make changes from _byteswap to _byteorder, following corresponding changes
  in numarray and recarray.

- Insert(update) EXTEND in PrimaryHDU only when header is None.

- Strip the trailing blanks for the comment value of a card.

- Add seek(0) right after the __buildin__.open(0), because for the 'ab+' mode,
  the pointer is at the end after open in Linux, but it is at the beginning in
  Solaris.

- Add checking of data against header, update header keywords (NAXIS's,
  BITPIX) when they don't agree with the data.

- change version to __version__.

There are also many other minor internal bug fixes and
technical changes.


0.6.2 (2002-02-12)
====================

This version requires numarray version 0.2.

Things not yet supported but are part of future development:

- Verification and/or correction of FITS objects being written to disk so that
  they are legal FITS. This is being added now and should be available in
  about a month.  Currently, one may construct FITS headers that are
  inconsistent with the data and write such FITS objects to disk. Future
  versions will provide options to either a) correct discrepancies and warn,
  b) correct discrepancies silently, c) throw a Python exception, or d) write
  illegal FITS (for test purposes!).

- Support for ascii tables or random groups format. Support for ASCII tables
  will be done soon (~1 month). When random group support is added is
  uncertain.

- Support for memory mapping FITS data (to reduce memory demands). We expect
  to provide this capability in about 3 months.

- Support for columns in binary tables having scaled values (e.g. BSCALE or
  BZERO) or boolean values. Currently booleans are stored as Int8 arrays and
  users must explicitly convert them into a boolean array. Likewise, scaled
  columns must be copied with scaling and offset by testing for those
  attributes explicitly. Future versions will produce such copies
  automatically.

- Support for tables with TNULL values. This awaits an enhancement to numarray
  to support mask arrays (planned).  (At least a couple of months off).

.. _PyFITS: https://github.com/spacetelescope/pyfits
.. _io-fits-faq:

astropy.io.fits FAQ
*******************

.. contents::

General Questions
=================

What is PyFITS and how does it relate to ``astropy``?
-----------------------------------------------------

PyFITS_ is a library written in, and for use with the Python_ programming
language for reading, writing, and manipulating FITS_ formatted files. It
includes a high-level interface to FITS headers with the ability for high- and
low-level manipulation of headers, and it supports reading image and table
data as Numpy_ arrays. It also supports more obscure and nonstandard formats
found in some FITS files.

The `astropy.io.fits` module is identical to PyFITS but with the names changed.
When the development of ``astropy`` began, it was clear that one of the core
requirements would be a FITS reader. Rather than starting from scratch,
PyFITS  being the most flexible FITS reader available for Python  was ported
into ``astropy``. There are plans to gradually phase out PyFITS as a stand-alone
module and deprecate it in favor of `astropy.io.fits`. See more about this in
the next question.

Although PyFITS is written mostly in Python, it includes an optional module
written in C that is required to read/write compressed image data. However,
the rest of PyFITS functions without this extension module.

.. _PyFITS: https://github.com/spacetelescope/pyfits
.. _FITS: https://fits.gsfc.nasa.gov/


What is the development status of PyFITS?
-----------------------------------------

PyFITS was written and maintained by the Science Software Branch at the `Space
Telescope Science Institute`_, and is licensed by AURA_ under a `3-clause BSD
license`_.

It is now exclusively developed as a component of ``astropy``
(`astropy.io.fits`) rather than as a stand-alone module. There are a few
reasons for this: The first is simply to reduce development effort; the
overhead of maintaining both PyFITS *and* `astropy.io.fits` in separate code
bases is nontrivial. The second is that there are many features of ``astropy``
(units, tables, etc.) from which the `astropy.io.fits` module can benefit
greatly. Since PyFITS is already integrated into ``astropy``, it makes more
sense to continue development there rather than make ``astropy`` a dependency
of PyFITS.

PyFITS' past primary developer and active maintainer was Erik Bray. There
is a `GitHub project`_ for PyFITS, but PyFITS is not actively developed anymore
so patches and issue reports should be posted on the Astropy issue tracker.

The current (and last) stable release is 3.4.0.

.. _Space Telescope Science Institute: https://www.stsci.edu/
.. _AURA: https://www.aura-astronomy.org/
.. _3-clause BSD license: https://en.wikipedia.org/wiki/BSD_licenses#3-clause_license_.28.22New_BSD_License.22_or_.22Modified_BSD_License.22.29
.. _GitHub project: https://github.com/spacetelescope/PyFITS


Usage Questions
===============

Something did not work as I expected. Did I do something wrong?
---------------------------------------------------------------

Possibly. But if you followed the documentation and things still did not work
as expected, it is entirely possible that there is a mistake in the
documentation, a bug in the code, or both. So feel free to report it as a bug.
There are also many, many corner cases in FITS files, with new ones discovered
almost every week. `astropy.io.fits` is always improving, but does not support
all cases perfectly. There are some features of the FITS format (scaled data,
for example) that are difficult to support correctly and can sometimes cause
unexpected behavior.

For the most common cases, however, such as reading and updating FITS headers,
images, and tables, `astropy.io.fits` is very stable and well-tested. Before
every ``astropy`` release it is ensured that all of its tests pass on a variety
of platforms, and those tests cover the majority of use cases (until new corner
cases are discovered).


``astropy`` crashed and output a long string of code. What do I do?
-------------------------------------------------------------------

This listing of code is what is known as a `stack trace`_ (or in Python
parlance a "traceback"). When an unhandled exception occurs in the code
causing the program to end, this is a way of displaying where the exception
occurred and the path through the code that led to it.

As ``astropy`` is meant to be used as a piece in other software projects, some
exceptions raised by ``astropy`` are by design. For example, one of the most
common exceptions is a `KeyError` when an attempt is made to read
the value of a nonexistent keyword in a header::

    >>> from astropy.io import fits
    >>> h = fits.Header()
    >>> h['NAXIS']
    Traceback (most recent call last):
        ...
    KeyError: "Keyword 'NAXIS' not found."

This indicates that something was looking for a keyword called "NAXIS" that
does not exist. If an error like this occurs in some other software that uses
``astropy``, it may indicate a bug in that software, in that it expected to
find a keyword that did not exist in a file.

Most "expected" exceptions will output a message at the end of the traceback
giving some idea of why the exception occurred and what to do about it. The
more vague and mysterious the error message in an exception appears, the more
likely that it was caused by a bug in ``astropy``. So if you are getting an
exception and you really do not know why or what to do about it, feel free to
report it as a bug.

.. _stack trace: https://en.wikipedia.org/wiki/Stack_trace


Why does opening a file work in CFITSIO, ds9, etc., but not in ``astropy``?
---------------------------------------------------------------------------

As mentioned elsewhere in this FAQ, there are many unusual corner cases when
dealing with FITS files. It is possible that a file should work, but is not
supported due to a bug. Sometimes it is even possible for a file to work in an
older version of ``astropy``, but not a newer version due to a regression
that has not been tested for yet.

Another problem with the FITS format is that, as old as it is, there are many
conventions that appear in files from certain sources that do not meet the FITS
standard. And yet they are so commonplace that it is necessary to support
them in any FITS readers. CONTINUE cards are one such example. There are
nonstandard conventions supported by ``astropy`` that are not supported by
CFITSIO and possibly vice versa. You may have hit one of those cases.

If ``astropy`` is having trouble opening a file, a good way to rule out whether
not the problem is with ``astropy`` is to run the file through the `fitsverify`_
program. For smaller files you can even use the `online FITS verifier`_.
These use CFITSIO under the hood, and should give a good indication of whether
or not there is something erroneous about the file. If the file is
malformatted, fitsverify will output errors and warnings.

If fitsverify confirms no problems with a file, and ``astropy`` is still having
trouble opening it (especially if it produces a traceback), then it is possible
there is a bug in ``astropy``.

.. _fitsverify: https://heasarc.gsfc.nasa.gov/docs/software/ftools/fitsverify/
.. _online FITS verifier: https://fits.gsfc.nasa.gov/fits_verify.html


How do I turn off the warning messages ``astropy`` outputs to my console?
-------------------------------------------------------------------------

``astropy`` uses Python's built-in `warnings`_ subsystem for informing about
exceptional conditions in the code that are recoverable, but that the user may
want to be informed of. One of the most common warnings in `astropy.io.fits`
occurs when updating a header value in such a way that the comment must be
truncated to preserve space::

    Card is too long, comment is truncated.

Any console output generated by ``astropy`` can be assumed to be from the
warnings subsystem. See Astropy's documentation on the :ref:`python-warnings`
for more information on how to control and quiet warnings.

.. _warnings: https://docs.python.org/3/library/warnings.html


What convention does ``astropy`` use for indexing, such as of image coordinates?
--------------------------------------------------------------------------------

All arrays and sequences in ``astropy`` use a zero-based indexing scheme. For
example, the first keyword in a header is ``header[0]``, not ``header[1]``.
This is in accordance with Python itself, as well as C, on which Python is
based.

This may come as a surprise to veteran FITS users coming from IRAF, where
1-based indexing is typically used, due to its origins in Fortran.

Likewise, the top-left pixel in an N x N array is ``data[0,0]``. The indices
for 2-dimensional arrays are row-major order, in that the first index is the
row number, and the second index is the column number. Or put in terms of
axes, the first axis is the y-axis, and the second axis is the x-axis. This is
the opposite of column-major order, which is used by Fortran and hence FITS.
For example, the second index refers to the axis specified by NAXIS1 in the
FITS header.

In general, for N-dimensional arrays, row-major orders means that the
right-most axis is the one that varies the fastest while moving over the
array data linearly. For example, the 3-dimensional array::

    [[[1, 2],
      [3, 4]],
     [[5, 6],
      [7, 8]]]

is represented linearly in row-major order as::

    [1, 2, 3, 4, 5, 6, 7, 8]

Since 2 immediately follows 1, you can see that the right-most (or inner-most)
axis is the one that varies the fastest.

The discrepancy in axis-ordering may take some getting used to, but it is a
necessary evil. Since most other Python and C software assumes row-major
ordering, trying to enforce column-major ordering in arrays returned by
``astropy`` is likely to cause more difficulties than it is worth.


How do I open a very large image that will not fit in memory?
-------------------------------------------------------------

`astropy.io.fits.open` has an option to access the data portion of an
HDU by memory mapping using `mmap`_. In ``astropy`` this is used by default.

What this means is that accessing the data as in the example above only reads
portions of the data into memory on demand. For example, if we request just a
slice of the image, such as ``hdul[0].data[100:200]``, then only rows 100-200
will be read into memory. This happens transparently, as though the entire
image were already in memory. This works the same way for tables. For most
cases this is your best bet for working with large files.

To ensure use of memory mapping, add the ``memmap=True`` argument to
:func:`fits.open <astropy.io.fits.open>`. Likewise, using ``memmap=False`` will
force data to be read entirely into memory.

The default can also be controlled through a configuration option called
``USE_MEMMAP``. Setting this to ``0`` will disable mmap by default.

Unfortunately, memory mapping does not currently work as well with scaled
image data, where BSCALE and BZERO factors need to be applied to the data to
yield physical values. Currently this requires enough memory to hold the
entire array, though this is an area that will see improvement in the future.

An alternative, which currently only works for image data (that is, non-tables)
is the sections interface. It is largely replaced by the better support for
mmap, but may still be useful on systems with more limited virtual memory
space, such as on 32-bit systems. Support for scaled image data is flaky with
sections too, though that will be fixed. See the documentation on :ref:`image
sections <data-sections>` for more details on using this interface.

.. _mmap: https://en.wikipedia.org/wiki/Mmap


How can I create a very large FITS file from scratch?
-----------------------------------------------------

See :ref:`sphx_glr_generated_examples_io_skip_create-large-fits.py`.

For creating very large tables, this method may also be used, though it can be
difficult to determine ahead of time how many rows a table will need. In
general, use of the `astropy.io.fits` module is currently discouraged for the
creation and manipulation of large tables. The FITS format itself is not
designed for efficient on-disk or in-memory manipulation of table structures.
For large, heavy-duty table data it might be better too look into using `HDF5`_
through the `PyTables`_ library. The :ref:`Astropy Table <astropy-table>`
interface can provide an abstraction layer between different on-disk table
formats as well (for example, for converting a table between FITS and HDF5).

PyTables makes use of NumPy under the hood, and can be used to write binary
table data to disk in the same format required by FITS. It is then possible
to serialize your table to the FITS format for distribution. At some point
this FAQ might provide an example of how to do this.

.. _HDF5: https://www.hdfgroup.org/HDF5/
.. _PyTables: http://www.pytables.org/


How do I create a multi-extension FITS file from scratch?
---------------------------------------------------------

See :ref:`sphx_glr_generated_examples_io_create-mef.py`.


.. _fits-scaled-data-faq:

Why is an image containing integer data being converted unexpectedly to floats?
-------------------------------------------------------------------------------

If the header for your image contains nontrivial values for the optional
BSCALE and/or BZERO keywords (that is, BSCALE != 1 and/or BZERO != 0), then
the raw data in the file must be rescaled to its physical values according to
the formula::

    physical_value = BZERO + BSCALE * array_value

As BZERO and BSCALE are floating point values, the resulting value must be a
float as well. If the original values were 16-bit integers, the resulting
values are single-precision (32-bit) floats. If the original values were
32-bit integers, the resulting values are double-precision (64-bit floats).

This automatic scaling can easily catch you off guard if you are not expecting
it, because it does not happen until the data portion of the HDU is accessed
(to allow for things like updating the header without rescaling the data). For
example::

    >>> fits_scaledimage_filename = fits.util.get_testdata_filepath('scale.fits')

    >>> hdul = fits.open(fits_scaledimage_filename)
    >>> image = hdul[0]
    >>> image.header['BITPIX']
    16
    >>> image.header['BSCALE']
    0.045777764213996
    >>> data = image.data  # Read the data into memory
    >>> data.dtype.name    # Got float32 despite BITPIX = 16 (16-bit int)
    'float32'
    >>> image.header['BITPIX']  # The BITPIX will automatically update too
    -32
    >>> 'BSCALE' in image.header  # And the BSCALE keyword removed
    False

The reason for this is that once a user accesses the data they may also
manipulate it and perform calculations on it. If the data were forced to
remain as integers, a great deal of precision is lost. So it is best to err
on the side of not losing data, at the cost of causing some confusion at
first.

If the data must be returned to integers before saving, use the
`~astropy.io.fits.ImageHDU.scale` method::

    >>> image.scale('int32')
    >>> image.header['BITPIX']
    32
    >>> hdul.close()

Alternatively, if a file is opened with ``mode='update'`` along with the
``scale_back=True`` argument, the original BSCALE and BZERO scaling will
be automatically reapplied to the data before saving. Usually this is
not desirable, especially when converting from floating point values back to
unsigned integer values. But this may be useful in cases where the raw
data needs to be modified corresponding to changes in the physical values.

To prevent rescaling from occurring at all (which is good for updating headers
 even if you do not intend for the code to access the data, it is good to err
on the side of caution here), use the ``do_not_scale_image_data`` argument when
opening the file::

    >>> hdul = fits.open(fits_scaledimage_filename, do_not_scale_image_data=True)
    >>> image = hdul[0]
    >>> image.data.dtype.name
    'int16'
    >>> hdul.close()


Why am I losing precision when I assign floating point values in the header?
----------------------------------------------------------------------------

The FITS standard allows two formats for storing floating point numbers in a
header value. The "fixed" format requires the ASCII representation of the
number to be in bytes 11 through 30 of the header card, and to be
right-justified. This leaves a standard number of characters for any comment
string.

The fixed format is not wide enough to represent the full range of values that
can be stored in a 64-bit float with full precision. So FITS also supports a
"free" format in which the ASCII representation can be stored anywhere, using
the full 70 bytes of the card (after the keyword).

Currently ``astropy`` only supports writing fixed format (it can read both
formats), so all floating point values assigned to a header are stored in the
fixed format. There are plans to add support for more flexible formatting.

In the meantime, it is possible to add or update cards by manually formatting
the card image from a string, as it should appear in the FITS file::

    >>> c = fits.Card.fromstring('FOO     = 1234567890.123456789')
    >>> h = fits.Header()
    >>> h.append(c)
    >>> h
    FOO     = 1234567890.123456789

As long as you do not assign new values to 'FOO' via ``h['FOO'] = 123``, will
maintain the header value exactly as you formatted it (as long as it is valid
according to the FITS standard).


Why is reading rows out of a FITS table so slow?
------------------------------------------------

Underlying every table data array returned by `astropy.io.fits` is a ``numpy``
`~numpy.recarray` which is a ``numpy`` array type specifically for representing
structured array data (i.e., a table). As with normal image arrays, ``astropy``
accesses the underlying binary data from the FITS file via mmap (see the
question "`What performance differences are there between astropy.io.fits and
fitsio?`_" for a deeper explanation of this). The underlying mmap is then
exposed as a `~numpy.recarray` and in general this is a very efficient way to
read the data.

However, for many (if not most) FITS tables it is not all that simple. For
many columns there are conversions that have to take place between the actual
data that is "on disk" (in the FITS file) and the data values that are returned
to the user. For example, FITS binary tables represent boolean values
differently from how ``numpy`` expects them to be represented, "Logical" columns
need to be converted on the fly to a format ``numpy`` (and hence the user) can
understand. This issue also applies to data that is linearly scaled via the
``TSCALn`` and ``TZEROn`` header keywords.

Supporting all of these "FITS-isms" introduces a lot of overhead that might
not be necessary for all tables, but are still common nonetheless. That is
not to say it cannot be faster even while supporting the peculiarities of
FITS  CFITSIO, for example, supports all of the same features but is orders of
magnitude faster. ``astropy`` could do much better here too, and there are many
known issues causing slowdown. There are plenty of opportunities for speedups,
and patches are welcome. In the meantime, for high-performance applications
with FITS tables some users might find the ``fitsio`` library more to their
liking.


I am opening many FITS files in a loop and getting OSError: Too many open files
-------------------------------------------------------------------------------

Say you have some code like:

.. code:: python

    from astropy.io import fits

    for filename in filenames:
        with fits.open(filename) as hdul:
            for hdu in hdul:
                hdu_data = hdul.data
                # Do some stuff with the data


The details may differ, but the qualitative point is that the data to many
HDUs and/or FITS files are being accessed in a loop. This may result in
an exception like::

    Traceback (most recent call last):
      File "<stdin>", line 2, in <module>
    OSError: [Errno 24] Too many open files: 'my_data.fits'

As explained in the :ref:`note on working with large files <fits-large-files>`,
because ``astropy`` uses mmap by default to read the data in a FITS file, even
if you correctly close a file with :meth:`HDUList.close
<astropy.io.fits.HDUList.close>` a handle is kept open to that file so
that the memory-mapped data array can still continue to be read transparently.

The way ``numpy`` supports mmap is such that the file mapping is not closed
until the overlying `~numpy.ndarray` object has no references to it and is freed
memory. However, when looping over a large number of files (or even just HDUs)
rapidly, this may not happen immediately. Or in some cases if the HDU object
persists, the data array attached to it may persist too. The recommended
workaround is to *manually* delete the ``.data`` attribute on the HDU object so
that the `~numpy.ndarray` reference is freed and the mmap can be closed:

.. code:: python

    from astropy.io import fits

    for filename in filenames:
        with fits.open(filename) as hdul:
            for hdu in hdul:
                hdu_data = hdul.data
                # Do some stuff with the data
                # ...
                # Don't need the data anymore; delete all references to it
                # so that it can be garbage collected
                del hdu_data
                del hdu.data


In some extreme cases files are opened and closed fast enough that Python's
garbage collector does not free them (and hence free the file handles) often
enough. To mitigate this, your code can manually force a garbage collection
by calling :func:`gc.collect` at the end of the loop.

In a future release it will be more convenient to automatically perform this
sort of cleanup when closing FITS files, where needed.

Using header['NAXIS2'] += 1 does not add another row to my Table
----------------------------------------------------------------

``NAXIS`` and similar keywords are FITS *structural* keywords and should not be
modified by the user. They are automatically updated by :mod:`astropy.io.fits`
when checking the validity of the data and headers. See :ref:`structural_keywords`
for more information.

To add rows to a table, you can modify the actual data.

Comparison with Other FITS Readers
==================================

What is the difference between astropy.io.fits and fitsio?
----------------------------------------------------------

The `astropy.io.fits` module (originally PyFITS) is a "pure Python" FITS
reader in that all of the code for parsing the FITS file format is in Python,
though ``numpy`` is used to provide access to the FITS data via the
`~numpy.ndarray` interface. `astropy.io.fits` currently also accesses the
`CFITSIO <https://heasarc.gsfc.nasa.gov/fitsio/fitsio.html>`_ to support the
FITS Tile Compression convention, but this feature is optional. It does not
use CFITSIO outside of reading compressed images.

`fitsio <https://github.com/esheldon/fitsio>`_, on the other hand, is a Python
wrapper for the CFITSIO library. All of the heavy lifting of reading the FITS
format is handled by CFITSIO, while ``fitsio`` provides a better way to use
object-oriented API, including providing a ``numpy`` interface to FITS files
read from CFITSIO. Much of it is written in C (to provide the interface between
Python and CFITSIO), and the rest is in Python. The Python end mostly
provides the documentation and user-level API.

Because ``fitsio`` wraps CFITSIO it inherits most of its strengths and
weaknesses, though it has an added strength of providing a more convenient
API than if one were to use CFITSIO directly.


Why did Astropy adopt PyFITS as its FITS reader instead of fitsio?
------------------------------------------------------------------

When the Astropy Project was first started it was clear from the start that
one of its core components should be a submodule for reading and writing FITS
files, as many other components would be likely to depend on this
functionality. At the time, the ``fitsio`` package was in its infancy (it
goes back to roughly 2011) while PyFITS had already been established (going
back to before the year 2000). It was already a mature package with support
for the vast majority of FITS files found in the wild, including outdated
formats such as "Random Groups" FITS files still used extensively in the
radio astronomy community.

Although many aspects of PyFITS' interface have evolved over the years, much
of it has also remained the same, and is already familiar to astronomers
working with FITS files in Python. Most of if not all existing training
materials were also based around PyFITS. PyFITS was developed at STScI, which
also put forward significant resources to develop Astropy, with an eye toward
integrating Astropy into STScI's own software stacks. As most of the Python
software at STScI uses PyFITS, it was the only practical choice for making that
transition.

Finally, although CFITSIO (and by extension ``fitsio``) can read any FITS files
that conform to the FITS standard, it does not support all of the nonstandard
conventions that have been added to FITS files in the wild. While it does have
some support for some of these conventions (such as CONTINUE cards and, to a
limited extent, HIERARCH cards), it is not easy to add support for other
conventions to a large and complex C codebase.

PyFITS' object-oriented design makes supporting nonstandard conventions
somewhat easier in most cases, and as such PyFITS can be more flexible in the
types of FITS files it can read and return *useful* data from. This includes
better support for files that fail to meet the FITS standard, but still contain
useful data that should be readable enough to correct any violations of the
FITS standard. For example, a common error in non-English speaking regions is
to insert non-ASCII characters into FITS headers. This is not a valid FITS
file, but should still be readable in some sense. Supporting structural errors
such as this is more difficult in CFITSIO which assumes a more rigid structure.


What performance differences are there between astropy.io.fits and fitsio?
--------------------------------------------------------------------------

There are two main performance areas to look at: reading/parsing FITS headers
and reading FITS data (image-like arrays as well as tables).

In the area of headers, ``fitsio`` is significantly faster in most cases. This
is due in large part to the (almost) pure C implementation (due to the use of
CFITSIO), but also due to fact that it is more rigid and does not support as
many local conventions and other special cases as `astropy.io.fits` tries to
support in its pure Python implementation.

That said, the difference is small and only likely to be a bottleneck either
when opening files containing thousands of HDUs, or reading the headers out
of thousands of FITS files in succession (in either case the difference is
not even an order of magnitude).

Where data is concerned the situation is a little more complicated, and
requires some understanding of how `astropy.io.fits` is implemented versus
CFITSIO and ``fitsio``. First, it is important to understand how they differ in
terms of memory management.

`astropy.io.fits` uses mmap, by default, to provide access to the raw
binary data in FITS files. Mmap is a system call (or in most cases these days
a wrapper in your libc for a lower-level system call) which allows user-space
applications to essentially do the same thing your OS is doing when it uses a
pagefile (swap space) for virtual memory: it allows data in a file on disk to
be paged into physical memory one page (or in practice usually several pages)
at a time on an as-needed basis. These cached pages of the file are also
accessible from all processes on the system, so multiple processes can read
from the same file with little additional overhead. In the case of reading
over all of the data in the file, the performance difference between using mmap
versus reading the entire data into physical memory at once can vary widely
between systems, hardware, and depending on what else is happening on the
system at the moment, but mmap is almost always going to be better.

In principle, it requires more overhead since accessing each page will result in
a page fault and the system requires more requests to the disk. But in
practice, the OS will optimize this pretty aggressively, especially for the most
common case of sequential access  also in reality, reading the entire thing
into memory is still going to result in a whole lot of page faults too. For
random access, having all of the data in physical memory is always going to be
best, though with mmap it is usually going to be pretty good too. (Most users
do not normally access all of the data in a file in a totally random order 
usually a few sections of it will be accessed most frequently, so the OS will
keep those pages in physical memory as best it can.) For the most general case
of reading FITS files (or most large data on disk) this is therefore the best
choice, especially for casual users, and is hence enabled by default.

CFITSIO/``fitsio``, on the other hand, does not assume the existence of
technologies like mmap and page caching. Thus it implements its own LRU cache
of I/O buffers that store sections of FITS files read from disk in memory in
FITS' famous 2880 byte chunk size. The I/O buffers are used heavily in
particular for keeping the headers in memory. Though for large data reads (for
example, reading an entire image from a file), it *does* bypass the cache and
instead does a read directly from disk into a user-provided memory buffer.

However, even when CFITSIO reads direct from the file, this is still largely
less efficient than using mmap. Normally when your OS reads a file from disk,
it caches as much of that read as it can in physical memory (in its page cache)
so that subsequent access to those same pages does not require a subsequent
expensive disk read. This happens when using mmap too, since the data has to
be copied from disk into RAM at some point. The difference is that when using
mmap to access the data, the program is able to read that data *directly* out
of the OS's page cache (as long as it is only being read). On the other hand,
when reading data from a file into a local buffer such as with fread(), the
data is first read into the page cache (if not already present) and then copied
from the page cache into the local buffer. So every read performs at least one
additional memory copy per page read (requiring twice as much physical memory,
and possibly lots of paging if the file is large and pages need to dropped from
the cache).

The user API for CFITSIO usually works by having the user allocate a memory
buffer large enough to hold the image/table they want to read (or at least the
section they are interested in). There are some helper functions for
determining the appropriate amount of space to allocate. Then you pass in
a pointer to your buffer and CFITSIO handles all of the reading (usually using
the process described above), and copies the results into your user buffer. For
large reads, it reads directly from the file into your buffer, though if the
data needs to be scaled it makes a stop in CFITSIO's own buffer first, then
writes the rescaled values out to the user buffer (if rescaling has been
requested). Regardless, this means that if your program wishes to hold an
entire image in memory at once it will use as much RAM as the size of the
data. For most applications it is better (and sufficient) to work on
smaller sections of the data, but this requires extra complexity. Using mmap
on the other hand makes managing this complexity more efficient.

An informal test demonstrates this difference. This test was performed on four
simple FITS images (one of which is a cube) of dimensions 256x256, 1024x1024,
4096x4096, and 256x1024x1024. Each image was generated before the test and
filled with randomized 64-bit floating point values. A similar test was
performed using both `astropy.io.fits` and ``fitsio``. A handle to the FITS
file is opened using each library's basic semantics, and then the entire data
array of the files is copied into a temporary array in memory (for example, if
we were blitting the image to a video buffer). For ``astropy`` the test is
written:

.. code:: python

    def read_test_astropy(filename):
        with fits.open(filename, memmap=True) as hdul:
            data = hdul[0].data
            c = data.copy()

The test was timed in IPython on a Linux system with kernel version 2.6.32, a
6-core Intel Xeon X5650 CPU clocked at 2.67 GHz per core, and 11.6 GB of RAM
using:

.. code:: python

    for filename in filenames:
        print(filename)
        %timeit read_test_astropy(filename)

where ``filenames`` is just a list of the aforementioned generated sample
files. The results were::

    256x256.fits
    1000 loops, best of 3: 1.28 ms per loop
    1024x1024.fits
    100 loops, best of 3: 4.24 ms per loop
    4096x4096.fits
    10 loops, best of 3: 60.6 ms per loop
    256x1024x1024.fits
    1 loops, best of 3: 1.15 s per loop

For ``fitsio`` the test was:

.. code:: python

    def read_test_fitsio(filename):
        with fitsio.FITS(filename) as f:
            data = f[0].read()
            c = data.copy()

This was also run in a loop over all of the sample files, producing the
results::

    256x256.fits
    1000 loops, best of 3: 476 s per loop
    1024x1024.fits
    100 loops, best of 3: 12.2 ms per loop
    4096x4096.fits
    10 loops, best of 3: 136 ms per loop
    256x1024x1024.fits
    1 loops, best of 3: 3.65 s per loop

It should be made clear that the sample files were rewritten with new random
data between the ``astropy`` test and the fitsio test, so they were not reading
the same data from the OS's page cache. Fitsio was much faster on the small
(256x256) image because in that case the time is dominated by parsing the
headers. As already explained, this is much faster in CFITSIO. However, as
the data size goes up and the header parsing no longer dominates the time,
`astropy.io.fits` using mmap is roughly twice as fast. This discrepancy is
almost entirely due to it requiring roughly half as many in-memory copies
to read the data, as explained earlier. That said, more extensive benchmarking
could be very interesting.

This is also not to say that `astropy.io.fits` does better in all cases. There
are some cases where it is currently blown away by fitsio. See the subsequent
question.


Why is fitsio so much faster than ``astropy`` at reading tables?
----------------------------------------------------------------

In many cases it is not: there is either no difference, or it may be a little
faster in ``astropy`` depending on what you are trying to do with the table and
what types of columns or how many columns the table has. There are some
cases, however, where ``fitsio`` can be radically faster, mostly for reasons
explained above in "`Why is reading rows out of a FITS table so slow?`_"

In principle a table is no different from, say, an array of pixels. But
instead of pixels each element of the array is some kind of record structure
(for example, two floats, a boolean, and a 20-character string field). Just as
a 64-bit float is an 8 byte record in an array, a row in such a table can be
thought of as a 37 byte (in the case of the previous example) record in a 1D
array of rows. So in principle everything that was explained in the answer to
the question "`What performance differences are there between astropy.io.fits
and fitsio?`_" applies just as well to tables as it does to any other array.

However, FITS tables have many additional complexities that sometimes preclude
streaming the data directly from disk, and instead require transformation from
the on-disk FITS format to a format more immediately useful to the user. A
common example is how FITS represents boolean values in binary tables.
Another significantly more complicated example, is variable length arrays.

As explained in "`Why is reading rows out of a FITS table so slow?`_",
`astropy.io.fits` does not currently handle some of these cases as
efficiently as it could, in particular in cases where a user only wishes to
read a few rows out of a table. Fitsio, on the other hand, has a better
interface for copying one row at a time out of a table and performing the
necessary transformations on that row *only*, rather than on the entire column
or columns that the row is taken from. As such, for many cases ``fitsio`` gets
much better performance and should be preferred for many performance-critical
table operations.

Fitsio also exposes a microlanguage (implemented in CFITSIO) for making
efficient SQL-like queries of tables (single tables only though  no joins or
anything like that). This format, described in the `CFITSIO documentation
<https://heasarc.gsfc.nasa.gov/docs/software/fitsio/c/c_user/node97.html>`_ can
in some cases perform more efficient selections of rows than might be possible
with ``numpy`` alone, which requires creating an intermediate mask array in
order to perform row selection.
.. currentmodule:: astropy.io.fits
.. doctest-skip-all

.. _header-transition-guide:

*********************************
Header Interface Transition Guide
*********************************

.. note::

    This guide was originally included with the release of PyFITS 3.1, and
    still references PyFITS in many places, though the examples have been
    updated for ``astropy.io.fits``. It is still useful here for informational
    purposes, though Astropy has always used the PyFITS 3.1 Header interface.

PyFITS v3.1 included an almost complete rewrite of the :class:`Header`
interface. Although the new interface is largely compatible with the old
interface (whether due to similarities in the design, or backwards-compatibility
support), there are enough differences that a full explanation of the new
interface is merited.

Background
==========

Prior to 3.1, PyFITS users interacted with FITS headers by way of three
different classes: :class:`Card`, ``CardList``, and :class:`Header`.

The Card class represents a single header card with a keyword, value, and
comment. It also contains all of the machinery for parsing FITS header cards,
given the 80-character string, or "card image" read from the header.

The CardList class is actually a subclass of Python's `list` built-in. It was
meant to represent the actual list of cards that make up a header. That is, it
represents an ordered list of cards in the physical order that they appear in
the header. It supports the usual list methods for inserting and appending new
cards into the list. It also supports `dict`-like keyword access, where
``cardlist['KEYWORD']`` would return the first card in the list with the given
keyword.

A lot of the functionality for manipulating headers was actually buried in the
CardList class. The Header class was more of a wrapper around CardList that
added a little bit of abstraction. It also implemented a partial dict-like
interface, though for Headers a keyword lookup returned the header value
associated with that keyword, not the Card object, and almost every
method on the Header class was just performing some operations on the
underlying CardList.

The problem was that there were certain things a user could *only* do by
directly accessing the CardList, such as look up the comments on a card or
access cards that have duplicate keywords, such as HISTORY. Another long-
standing misfeature was that slicing a Header object actually returned a
CardList object, rather than a new Header. For all but the simplest use cases,
working with CardList objects was largely unavoidable.

But it was realized that CardList is really an implementation detail
not representing any element of a FITS file distinct from the header itself.
Users familiar with the FITS format know what a header is, but it is not clear
how a "card list" is distinct from that, or why operations go through the
Header object, while some have to be performed through the CardList.

So the primary goal of this redesign was to eliminate the ``CardList`` class
altogether, and make it possible for users to perform all header manipulations
directly through :class:`Header` objects. It also tried to present headers as
similarly as possible to a more familiar data structure  an ordered mapping
(or :class:`~collections.OrderedDict` in Python) for ease of use by new users
less familiar with the FITS format, though there are still many added
complexities for dealing with the idiosyncrasies of the FITS format.


Deprecation Warnings
====================

A few older methods on the :class:`Header` class have been marked as deprecated,
either because they have been renamed to a more `PEP 8`_-compliant name, or
because have become redundant due to new features. To check if your code is
using any deprecated methods or features, run your code with ``python -Wd``.
This will output any deprecation warnings to the console.

Two of the most common deprecation warnings related to Headers are:

- ``Header.has_key``: this has been deprecated since PyFITS 3.0,
  just as Python's `dict.has_key` is deprecated. To check a key's presence
  in a mapping object like `dict` or :class:`Header`, use the ``key in d``
  syntax. This has long been the preference in Python.

- ``Header.ascardlist`` and ``Header.ascard``: these were used to
  access the ``CardList`` object underlying a header. They should still
  work, and return a skeleton CardList implementation that should support most
  of the old CardList functionality. But try removing as much of this as
  possible. If direct access to the :class:`Card` objects making up a header
  is necessary, use :attr:`Header.cards`, which returns an iterator over the
  cards. More on that below.

.. _PEP 8: https://www.python.org/dev/peps/pep-0008/

New Header Design
=================

The new :class:`Header` class is designed to work as a drop-in replacement for
a `dict` via `duck typing`_. That is, although it is not a subclass of `dict`,
it implements all of the same methods and interfaces. In particular, it is
similar to an :class:`~collections.OrderedDict` in that the order of insertions
is preserved. However, Header also supports many additional features and
behaviors specific to the FITS format. It should also be noted that while the
old Header implementation also had a dict-like interface, it did not implement
the *entire* dict interface as the new Header does.

Although the new Header is used like a dict/mapping in most cases, it also
supports a `list` interface. The list-like interface is a bit idiosyncratic in
that in some contexts the Header acts like a list of values, in others like a
list of keywords, and in a few contexts like a list of :class:`Card` objects.
This may be the most difficult aspect of the new design, but there is a logic
to it.

As with the old Header implementation, integer index access is supported:
``header[0]`` returns the value of the first keyword. However, the
:meth:`Header.index` method treats the header as though it is a list of
keywords and returns the index of a given keyword. For example::

    >>> header.index('BITPIX')
    2

:meth:`Header.count` is similar to `list.count` and also takes a keyword as
its argument::

    >>> header.count('HISTORY')
    20

A good rule of thumb is that any item access using square brackets ``[]``
returns *value* in the header, whether using keyword or index lookup. Methods
like :meth:`~Header.index` and :meth:`~Header.count` that deal with the order
and quantity of items in the Header generally work on keywords. Finally,
methods like :meth:`~Header.insert` and :meth:`~Header.append` that add new
items to the header work on cards.

Aside from the list-like methods, the new Header class works very similarly to
the old implementation for most basic use cases and should not present too many
surprises. There are differences, however:

- As before, the Header() initializer can take a list of :class:`Card` objects
  with which to fill the header. However, now any iterable may be used. It is
  also important to note that *any* Header method that accepts :class:`Card`
  objects can also accept 2-tuples or 3-tuples in place of Cards. That is,
  either a ``(keyword, value, comment)`` tuple or a ``(keyword, value)`` tuple
  (comment is assumed blank) may be used anywhere in place of a Card object.
  This is even preferred, as it involves less typing. For example::

      >>> from astropy.io import fits
      >>> header = fits.Header([('A', 1), ('B', 2), ('C', 3, 'A comment')])
      >>> header
      A       =                    1
      B       =                    2
      C       =                    3 / A comment

- As demonstrated in the previous example, the ``repr()`` for a Header (that is,
  the text that is displayed when entering a Header object in the Python
  console as an expression), shows the header as it would appear in a FITS file.
  This inserts newlines after each card so that it is readable regardless of
  terminal width. It is *not* necessary to use ``print header`` to view this.
  Entering ``header`` displays the header contents as it would appear in the
  file (sans the END card).

- ``len(header)`` is now supported (previously it was necessary to do
  ``len(header.ascard)``). This returns the total number of cards in the
  header, including blank cards, but excluding the END card.

- FITS supports having duplicate keywords, although they are generally in error
  except for commentary keywords like COMMENT and HISTORY. PyFITS now supports
  reading, updating, and deleting duplicate keywords; instead of using the
  keyword by itself, use a ``(keyword, index)`` tuple. For example,
  ``('HISTORY', 0)`` represents the first HISTORY card, ``('HISTORY', 1)``
  represents the second HISTORY card, and so on. In fact, when a keyword is
  used by itself, it is shorthand for ``(keyword, 0)``. It is now possible to
  delete an accidental duplicate like so::

      >>> del header[('NAXIS', 1)]

  This will remove an accidental duplicate NAXIS card from the header.

- Even if there are duplicate keywords, keyword lookups like
  ``header['NAXIS']`` will always return the value associated with the first
  copy of that keyword, with one exception: commentary keywords like COMMENT
  and HISTORY are expected to have duplicates. So ``header['HISTORY']``, for
  example, returns the whole sequence of HISTORY values in the correct order.
  This list of values can be sliced arbitrarily. For example, to view the last
  three history entries in a header::

      >>> hdulist[0].header['HISTORY'][-3:]
        reference table oref$laf13367o_pct.fits
        reference table oref$laf13369o_apt.fits
      Heliocentric correction = 16.225 km/s

- Subscript assignment can now be used to add new keywords to the header. Just
  as with a normal `dict`, ``header['NAXIS'] = 1`` will either update the NAXIS
  keyword if it already exists, or add a new NAXIS keyword with a value of
  ``1`` if it does not exist. In the old interface this would return a
  `KeyError` if NAXIS did not exist, and the only way to add a new
  keyword was through the update() method.

  By default, new keywords added in this manner are added to the end of the
  header, with a few FITS-specific exceptions:

  * If the header contains extra blank cards at the end, new keywords are added
    before the blanks.

  * If the header ends with a list of commentary cards  for example, a sequence
    of HISTORY cards  those are kept at the end, and new keywords are inserted
    before the commentary cards.

  * If the keyword is a commentary keyword like COMMENT or HISTORY (or an empty
    string for blank keywords), a *new* commentary keyword is always added and
    appended to the last commentary keyword of the same type. For example,
    HISTORY keywords are always placed after the last history keyword::

        >>> header = fits.Header()
        >>> header['COMMENT'] = 'Comment 1'
        >>> header['HISTORY'] = 'History 1'
        >>> header['COMMENT'] = 'Comment 2'
        >>> header['HISTORY'] = 'History 2'
        >>> header
        COMMENT Comment 1
        COMMENT Comment 2
        HISTORY History 1
        HISTORY History 2

  These behaviors represent a sensible default behavior for keyword assignment,
  and the same behavior as :meth:`~Header.update` in the old Header
  implementation. The default behaviors may still be bypassed through the use
  of other assignment methods like the :meth:`Header.set` and
  :meth:`Header.append` methods described later.

- It is now also possible to assign a value and a comment to a keyword
  simultaneously using a tuple::

      >>> header['NAXIS'] = (2, 'Number of axis')

  This will update the value and comment of an existing keyword, or add a new
  keyword with the given value and comment.

- There is a new :attr:`Header.comments` attribute which lists all of the
  comments associated with keywords in the header (not to be confused with
  COMMENT cards). This allows viewing and updating the comments on specific
  cards::

      >>> header.comments['NAXIS']
      Number of axis
      >>> header.comments['NAXIS'] = 'Number of axes'
      >>> header.comments['NAXIS']
      Number of axes

- When deleting a keyword from a header, do not assume that the keyword already
  exists. In the old Header implementation, this action would silently do
  nothing. For backwards-compatibility, it is still okay to delete a
  nonexistent keyword, but a warning will be raised. In the future this
  *will* be changed so that trying to delete a nonexistent keyword raises a
  `KeyError`. This is for consistency with the behavior of Python dicts. So
  unless you know for certain that a keyword exists before deleting it, it is
  best to do something like::

      >>> try:
      ...     del header['BITPIX']
      ... except KeyError:
      ...     pass

  Or if you prefer to look before you leap::

      >>> if 'BITPIX' in header:
      ...     del header['BITPIX']

- ``del header`` now supports slices. For example, to delete the last three
  keywords from a header::

      >>> del header[-3:]

- Two headers can now be compared for equality  previously no two Header
  objects were the same. Now they compare as equal if they contain the exact
  same content. That is, this requires strict equality.

- Two headers can now be added with the '+' operator, which returns a copy of
  the left header extended by the right header with :meth:`~Header.extend`.
  Assignment addition is also possible.

- The Header.update() method used commonly with the old Header API has been
  renamed to :meth:`Header.set`. The primary reason for this change is very
  simple: Header implements the `dict` interface, which already has a method
  called update(), but that behaves differently from the old Header.update().

  The details of the new update() can be read in the API docs, but it is very
  similar to `dict.update`. It also supports backwards compatibility with the
  old update() by analysis of the arguments passed to it, so existing code will
  not break immediately. However, this *will* cause a deprecation warning to
  be output if they are enabled. It is best, for starters, to replace all
  update() calls with set(). Recall, also, that direct assignment is now
  possible for adding new keywords to a header. So by and large the only
  reason to prefer using :meth:`Header.set` is its capability of inserting or
  moving a keyword to a specific location using the ``before`` or ``after``
  arguments.

- Slicing a Header with a slice index returns a new Header containing only
  those cards contained in the slice. As mentioned earlier, it used to be that
  slicing a Header returned a card list  something of a misfeature. In
  general, objects that support slicing ought to return an object of the same
  type when you slice them.

  Likewise, wildcard keywords used to return a CardList object  now they
  return a new Header similarly to a slice. For example::

      >>> header['NAXIS*']

  returns a new header containing only the NAXIS and NAXISn cards from the
  original header.

.. _duck typing: https://en.wikipedia.org/wiki/Duck_typing


Transition Tips
===============

The above may seem like a lot, but the majority of existing code using PyFITS
to manipulate headers should not need to be updated, at least not immediately.
The most common operations still work the same.

As mentioned above, it would be helpful to run your code with ``python -Wd`` to
enable deprecation warnings  that should be a good idea of where to look to
update your code.

If your code needs to be able to support older versions of PyFITS
simultaneously with PyFITS 3.1, things are slightly trickier, but not by
much  the deprecated interfaces will not be removed for several more versions
because of this.

- The first change worth making, which is supported by any PyFITS version in
  the last several years, is to remove any use of ``Header.has_key`` and
  replace it with ``keyword in header`` syntax. It is worth making this change
  for any dict as well, since `dict.has_key` is deprecated. Running the
  following regular expression over your code may help with most (but not all)
  cases::

      s/([^ ]+)\.has_key\(([^)]+)\)/\2 in \1/

- If possible, replace any calls to Header.update() with Header.set() (though
  do not bother with this if you need to support older PyFITS versions). Also,
  if you have any calls to Header.update() that can be replaced with simple
  subscript assignments (e.g., ``header['NAXIS'] = (2, 'Number of axes')``) do
  that too, if possible.

- Find any code that uses ``header.ascard`` or ``header.ascardlist()``. First
  ascertain whether that code really needs to work directly on Card objects.
  If that is definitely the case, go ahead and replace those with
  ``header.cards``  that should work without too much fuss. If you do need to
  support older versions, you may keep using ``header.ascard`` for now.

- In the off chance that you have any code that slices a header, it is best to
  take the result of that and create a new Header object from it. For
  example::

      >>> new_header = fits.Header(old_header[2:])

  This avoids the problem that in PyFITS <= 3.0 slicing a Header returns a
  CardList by using the result to initialize a new Header object. This will
  work in both cases (in PyFITS 3.1, initializing a Header with an existing
  Header just copies it,  la `list`).

- As mentioned earlier, locate any code that deletes keywords with ``del`` and
  make sure they either look before they leap (``if keyword in header:``) or
  ask forgiveness (``try/except KeyError:``).

Other Gotchas
-------------

- As mentioned above, it is not necessary to enter ``print header`` to display
  a header in an interactive Python prompt. Entering ``>>> header``
  by itself is sufficient. Using ``print`` usually will *not* display the
  header readably, because it does not include line breaks between the header
  cards. The reason is that Python has two types of string representations.
  One is returned when a user calls ``str(header)``, which happens automatically
  when you ``print`` a variable. In the case of the Header class this actually
  returns the string value of the header as it is written literally in the
  FITS file, which includes no line breaks.

  The other type of string representation happens when one calls
  ``repr(header)``. The `repr` of an object is meant to be a useful
  string "representation" of the object; in this case the contents of the
  header but with line breaks between the cards and with the END card and
  trailing padding stripped off. This happens automatically when
  a user enters a variable at the Python prompt by itself without a ``print``
  call.

- The current version of the FITS Standard (3.0) states in section 4.2.1
  that trailing spaces in string values in headers are not significant and
  should be ignored. PyFITS < 3.1 *did* treat trailing spaces as significant.
  For example, if a header contained:

      KEYWORD1= 'Value    '

  then ``header['KEYWORD1']`` would return the string ``'Value    '`` exactly,
  with the trailing spaces intact. The new Header interface fixes this by
  automatically stripping trailing spaces, so that ``header['KEYWORD1']`` would
  return just ``'Value'``.

  There is, however, one convention used by the IRAF CCD mosaic task for
  representing its TNX World Coordinate System and ZPX World Coordinate System
  nonstandard WCS that uses a series of keywords in the form ``WATj_nnn``,
  which store a text description of coefficients for a nonlinear distortion
  projection. It uses its own microformat for listing the coefficients as a
  string, but the string is long, and thus broken up into several of these
  ``WATj_nnn`` keywords. Correct recombination of these keywords requires
  treating all whitespace literally. This convention either overlooked or
  predated the prescribed treatment of whitespace in the FITS standard.

  To get around this issue, a global variable ``fits.STRIP_HEADER_WHITESPACE``
  was introduced. Temporarily setting
  ``fits.STRIP_HEADER_WHITESPACE.set(False)`` before reading keywords affected
  by this issue will return their values with all trailing whitespace intact.

  A future version of PyFITS may be able to detect use of conventions like this
  contextually and behave according to the convention, but in most cases the
  default behavior of PyFITS is to behave according to the FITS Standard.
.. _masking_and_missing_values:

Masking and Missing Values
**************************

The `astropy.table` package provides support for masking and missing values in
a table by using the ``numpy.ma`` `masked array
<https://numpy.org/doc/stable/reference/maskedarray.html>`_ package to define
masked columns and by supporting :ref:`mixin_columns` that provide masking.
This allows handling tables with missing or invalid entries in much the same
manner as for standard (unmasked) tables. It is useful to be familiar with the
`masked array documentation
<https://numpy.org/doc/stable/reference/maskedarray.generic.html>`_
when using masked tables within `astropy.table`.

In a nutshell, the concept is to define a boolean mask that mirrors
the structure of a column data array. Wherever a mask value is
`True`, the corresponding entry is considered to be missing or invalid.
Operations involving column or row access and slicing are unchanged.
The key difference is that arithmetic or reduction operations involving
columns or column slices follow the rules for `operations
on masked arrays
<https://numpy.org/doc/stable/reference/maskedarray.generic.html#operations-on-masked-arrays>`_.

.. Note::

   Reduction operations like :func:`numpy.sum` or :func:`numpy.mean` follow the
   convention of ignoring masked (invalid) values. This differs from
   the behavior of the floating point ``NaN``, for which the sum of an
   array including one or more ``NaN's`` will result in ``NaN``.

   For more information see NumPy Enhancement Proposals `24
   <https://numpy.org/neps/nep-0024-missing-data-2.html>`_, `25
   <https://numpy.org/neps/nep-0025-missing-data-3.html>`_, and `26
   <https://numpy.org/neps/nep-0026-missing-data-summary.html>`_.

Table Creation
==============

A masked table can be created in several ways:

**Create a table with one or more columns as a MaskedColumn object**

  >>> from astropy.table import Table, Column, MaskedColumn
  >>> a = MaskedColumn([1, 2], name='a', mask=[False, True], dtype='i4')
  >>> b = Column([3, 4], name='b', dtype='i8')
  >>> Table([a, b])
  <Table length=2>
    a     b
  int32 int64
  ----- -----
      1     3
     --     4

The |MaskedColumn| is the masked analog of the |Column| class and provides the
interface for creating and manipulating a column of masked data. The
|MaskedColumn| class inherits from :class:`numpy.ma.MaskedArray`, in contrast
to |Column| which inherits from |ndarray|. This distinction is the main reason
there are different classes for these two cases.

Notice that masked entries in the table output are shown as ``--``.

**Create a table with one or more columns as a NumPy MaskedArray**

  >>> import numpy as np
  >>> a = np.ma.array([1, 2])
  >>> b = [3, 4]
  >>> t = Table([a, b], names=('a', 'b'))

**Create a table from list data containing numpy.ma.masked**

You can use the `numpy.ma.masked` constant to indicate masked or invalid data::

  >>> a = [1.0, np.ma.masked]
  >>> b = [np.ma.masked, 'val']
  >>> Table([a, b], names=('a', 'b'))
  <Table length=2>
    a     b
  float64 str3
  ------- ----
      1.0   --
      --  val

Initializing from lists with embedded `numpy.ma.masked` elements is
considerably slower than using :func:`numpy.ma.array` or |MaskedColumn|
directly, so if performance is a concern you should use the latter methods if
possible.

**Add a MaskedColumn object to an existing table**

  >>> t = Table([[1, 2]], names=['a'])
  >>> b = MaskedColumn([3, 4], mask=[True, False])
  >>> t['b'] = b

**Add a new row to an existing table and specify a mask argument**

  >>> a = Column([1, 2], name='a')
  >>> b = Column([3, 4], name='b')
  >>> t = Table([a, b])
  >>> t.add_row([3, 6], mask=[True, False])

**Create a new table object and specify masked=True**

If ``masked=True`` is provided when creating the table then every column will
be created as a |MaskedColumn|, and new columns will always be added as a
|MaskedColumn|.

  >>> Table([(1, 2), (3, 4)], names=('a', 'b'), masked=True, dtype=('i4', 'i8'))
  <Table masked=True length=2>
    a     b
  int32 int64
  ----- -----
      1     3
      2     4

**Convert an existing table to a masked table**

  >>> t = Table([[1, 2], ['x', 'y']])  # standard (unmasked) table
  >>> t = Table(t, masked=True, copy=False)  # convert to masked table

This operation will convert every |Column| to |MaskedColumn| and ensure that any
subsequently added columns are masked.

Table Access
============

Nearly all of the standard methods for accessing and modifying data
columns, rows, and individual elements also apply to masked tables.

There is a difference however regarding the |Row| objects that are obtained by
indexing a single row of a table. For standard tables, two such rows can be
compared for equality, but for masked tables this comparison will produce an
exception::

  >>> t[0] == t[1]
  Traceback (most recent call last):
  ...
  ValueError: Unable to compare rows for masked table due to numpy.ma bug

Masking and Filling
===================

Both the |Table| and |MaskedColumn| classes provide attributes and methods to
support manipulating tables with missing or invalid data.

Mask
----

.. EXAMPLE START: Manipulating Tables with Missing Data using Masks

The mask for a column can be viewed and modified via the ``mask`` attribute::

  >>> t = Table([(1, 2), (3, 4)], names=('a', 'b'), masked=True)
  >>> t['a'].mask = [False, True]  # Modify column mask (boolean array)
  >>> t['b'].mask = [True, False]  # Modify column mask (boolean array)
  >>> print(t)
   a   b
  --- ---
    1  --
   --   4

Masked entries are shown as ``--`` when the table is printed. You can
view the mask directly, either at the column or table level::

  >>> t['a'].mask
  array([False,  True]...)

  >>> t.mask
  <Table length=2>
    a     b
   bool  bool
  ----- -----
  False  True
   True False

To get the indices of masked elements, use an expression like::

  >>> t['a'].mask.nonzero()[0]  # doctest: +SKIP
  array([1])

.. EXAMPLE END

Filling
-------

.. EXAMPLE START: Manipulating Tables with Missing Data by Filling Masked Values

The entries which are masked (i.e., missing or invalid) can be replaced with
specified fill values. Filling a |MaskedColumn| produces a |Column|. Each
column in a masked table has a ``fill_value`` attribute that specifies the
default fill value for that column. To perform the actual replacement operation
the :meth:`~astropy.table.Table.filled` method is called. This takes an
optional argument which can override the default column ``fill_value``
attribute.
::

  >>> t['a'].fill_value = -99
  >>> t['b'].fill_value = 33

  >>> print(t.filled())
   a   b
  --- ---
    1  33
  -99   4

  >>> print(t['a'].filled())
   a
  ---
    1
  -99

  >>> print(t['a'].filled(999))
   a
  ---
    1
  999

  >>> print(t.filled(1000))
   a    b
  ---- ----
     1 1000
  1000    4

.. EXAMPLE END
.. |add_index| replace:: :func:`~astropy.table.Table.add_index`
.. |index_mode| replace:: :func:`~astropy.table.Table.index_mode`

.. _table-indexing:

Table Indexing
**************

Once a |Table| has been created, it is possible to create indices on one or
more columns of the table. An index internally sorts the rows of a table based
on the index column(s), allowing for element retrieval by column value and
improved performance for certain table operations.

Creating an Index
=================

.. EXAMPLE START: Creating Indexes on Table Columns

To create an index on a table, use the |add_index| method::

   >>> from astropy.table import Table
   >>> t = Table([(2, 3, 2, 1), (8, 7, 6, 5)], names=('a', 'b'))
   >>> t.add_index('a')

The optional argument ``unique`` may be specified to create an index with
uniquely valued elements.

To create a composite index on multiple columns, pass a list of columns
instead::

   >>> t.add_index(['a', 'b'])

In particular, the first index created using the
|add_index| method is considered the default index or the "primary key." To
retrieve an index from a table, use the `~astropy.table.Table.indices`
property::

   >>> t.indices['a']
   <SlicedIndex original=True index=<Index columns=('a',) data=<SortedArray length=4>
    a  rows
   --- ----
     1    3
     2    0
     2    2
     3    1>>
   >>> t.indices['a', 'b']
   <SlicedIndex original=True index=<Index columns=('a', 'b') data=<SortedArray length=4>
    a   b  rows
   --- --- ----
     1   5    3
     2   6    2
     2   8    0
     3   7    1>>

.. EXAMPLE END

Row Retrieval using Indices
===========================

.. EXAMPLE START: Retrieving Table Rows using Indices

Row retrieval can be accomplished using two table properties:
`~astropy.table.Table.loc` and `~astropy.table.Table.iloc`. The
`~astropy.table.Table.loc` property can be indexed either by column value,
range of column values (*including* the bounds), or a :class:`list` or
|ndarray| of column values::

   >>> t = Table([(1, 2, 3, 4), (10, 1, 9, 9)], names=('a', 'b'), dtype=['i8', 'i8'])
   >>> t.add_index('a')
   >>> t.loc[2]
   <Row index=1>
     a     b
   int64 int64
   ----- -----
       2     1
   >>> t.loc[[1, 4]]
   <Table length=2>
     a     b
   int64 int64
   ----- -----
       1    10
       4     9
   >>> t.loc[1:3]
   <Table length=3>
     a     b
   int64 int64
   ----- -----
       1    10
       2     1
       3     9
   >>> t.loc[:]
   <Table length=4>
     a     b
   int64 int64
   ----- -----
       1    10
       2     1
       3     9
       4     9

Note that by default, `~astropy.table.Table.loc` uses the primary index, which
here is column ``'a'``. To use a different index, pass the indexed column name
before the retrieval data::

   >>> t.add_index('b')
   >>> t.loc['b', 8:10]
   <Table length=3>
     a     b
   int64 int64
   ----- -----
       3     9
       4     9
       1    10

The property `~astropy.table.Table.iloc` works similarly, except that the
retrieval information must be either an integer or a :class:`slice`, and
relates to the sorted order of the index rather than column values. For
example::

   >>> t.iloc[0] # smallest row by value 'a'
   <Row index=0>
     a     b
   int64 int64
   ----- -----
       1    10
   >>> t.iloc['b', 1:] # all but smallest value of 'b'
   <Table length=3>
     a     b
   int64 int64
   ----- -----
       3     9
       4     9
       1    10

.. EXAMPLE END

Effects on Performance
======================

Table operations change somewhat when indices are present, and there are a
number of factors to consider when deciding whether the use of indices will
improve performance. In general, indexing offers the following advantages:

* Table grouping and sorting based on indexed column(s) both become faster.
* Retrieving values by index is faster than custom searching.

There are certain caveats, however:

* Creating an index requires time and memory.
* Table modifications become slower due to automatic index updates.
* Slicing a table becomes slower due to index relabeling.

See `here
<https://nbviewer.jupyter.org/github/mdmueller/astropy-notebooks/blob/master/table/indexing-profiling.ipynb>`_
for an IPython notebook profiling various aspects of table indexing.

Index Modes
===========

The |index_mode| method allows for some flexibility in the behavior of table
indexing by allowing the user to enter a specific indexing mode via a context
manager. There are currently three indexing modes: ``'freeze'``,
``'copy_on_getitem'``, and ``'discard_on_copy'``.

.. EXAMPLE START: Table Indexing with the "freeze" Index Mode

The ``'freeze'`` mode prevents automatic index updates whenever a column of the
index is modified, and all indices refresh themselves after the context ends::

  >>> with t.index_mode('freeze'):
  ...    t['a'][0] = 0
  ...    print(t.indices['a']) # unmodified
  <SlicedIndex original=True index=<Index columns=('a',) data=<SortedArray length=4>
   a  rows
  --- ----
    1    0
    2    1
    3    2
    4    3>>
  >>> print(t.indices['a']) # modified
  <SlicedIndex original=True index=<Index columns=('a',) data=<SortedArray length=4>
   a  rows
  --- ----
    0    0
    2    1
    3    2
    4    3>>

.. EXAMPLE END

.. EXAMPLE START: Table Indexing with the "copy_on_getitem" Index Mode

The ``'copy_on_getitem'`` mode forces columns to copy and relabel their indices
upon slicing. In the absence of this mode, table slices will preserve
indices while column slices will not::

  >>> ca = t['a'][[1, 3]]
  >>> ca.info.indices
  []
  >>> with t.index_mode('copy_on_getitem'):
  ...     ca = t['a'][[1, 3]]
  ...     print(ca.info.indices)
  [<SlicedIndex original=True index=<Index columns=('a',) data=<SortedArray length=2>
   a  rows
  --- ----
    2    0
    4    1>>]

.. EXAMPLE END

.. EXAMPLE START: Table Indexing with the "discard_on_copy" Index Mode

The ``'discard_on_copy'`` mode prevents indices from being copied whenever a
column or table is copied::

  >>> t2 = Table(t)
  >>> t2.indices['a']
  <SlicedIndex original=True index=<Index columns=('a',) data=<SortedArray length=4>
   a  rows
  --- ----
    0    0
    2    1
    3    2
    4    3>>
  >>> with t.index_mode('discard_on_copy'):
  ...    t2 = Table(t)
  ...    print(t2.indices)
  []

.. EXAMPLE END

Updating Rows using Indices
===========================

.. EXAMPLE START: Updating Table Rows using Indices

Row updates can be accomplished by assigning the table property
`~astropy.table.Table.loc` a complete row or a list of rows::

   >>> t = Table([('w', 'x', 'y', 'z'), (10, 1, 9, 9)], names=('a', 'b'), dtype=['str', 'i8'])
   >>> t.add_index('a')
   >>> t.loc['x']
   <Row index=1>
    a     b
   str1 int64
   ---- -----
      x     1
   >>> t.loc['x'] = ['a', 12]
   >>> t
   <Table length=4>
    a     b
   str1 int64
   ---- -----
      w    10
      a    12
      y     9
      z     9
   >>> t.loc[['w', 'y']]
   <Table length=2>
    a     b
   str1 int64
   ---- -----
      w    10
      y     9
   >>> t.loc[['w', 'z']] = [['b', 23], ['c', 56]]
   >>> t
   <Table length=4>
    a     b
   str1 int64
   ---- -----
      b    23
      a    12
      y     9
      c    56

.. EXAMPLE END

Retrieving the Location of Rows using Indices
=============================================

.. EXAMPLE START: Retrieving the Location of Table Rows using Indices

Retrieval of the location of rows can be accomplished using a table property:
`~astropy.table.Table.loc_indices`. The `~astropy.table.Table.loc_indices`
property can be indexed either by column value, range of column values
(*including* the bounds), or a :class:`list` or |ndarray| of column values::

   >>> t = Table([('w', 'x', 'y', 'z'), (10, 1, 9, 9)], names=('a', 'b'), dtype=['str', 'i8'])
   >>> t.add_index('a')
   >>> t.loc_indices['x']
   1

.. EXAMPLE END

Engines
=======

When creating an index via |add_index|, the keyword argument ``engine`` may be
specified to use a particular indexing engine. The available engines are:

* `~astropy.table.SortedArray`, a sorted array engine using an underlying
  sorted |Table|.
* `~astropy.table.SCEngine`, a sorted list engine using the `Sorted Containers
  <https://pypi.org/project/sortedcontainers/>`_ package.
* `~astropy.table.BST`, a Python-based binary search tree engine (not recommended).

The SCEngine depends on the ``sortedcontainers`` dependency. The most important takeaway is that
`~astropy.table.SortedArray` (the default engine) is usually best, although
`~astropy.table.SCEngine` may be more appropriate for an index created on an
empty column since adding new values is quicker.

The `~astropy.table.BST` engine demonstrates a simple pure Python implementation
of a search tree engine, but the performance is poor for larger tables. This
is available in the code largely as an implementation reference.
.. note that if this is changed from the default approach of using an *include*
   (in index.rst) to a separate performance page, the header needs to be changed
   from === to ***, the filename extension needs to be changed from .inc.rst to
   .rst, and a link needs to be added in the subpackage toctree

.. doctest-skip-all

.. _astropy-table-performance:

Performance Tips
================

Constructing |Table| objects row by row using
:meth:`~astropy.table.Table.add_row` can be very slow::

    >>> from astropy.table import Table
    >>> t = Table(names=['a', 'b'])
    >>> for i in range(100):
    ...     t.add_row((1, 2))

If you do need to loop in your code to create the rows, a much faster approach
is to construct a list of rows and then create the |Table| object at the very
end::

  >>> rows = []
  >>> for i in range(100):
  ...     rows.append((1, 2))
  >>> t = Table(rows=rows, names=['a', 'b'])

Writing a |Table| with |MaskedColumn| to ``.ecsv`` using
:meth:`~astropy.table.Table.write` can be very slow::

    >>> from astropy.table import Table
    >>> import numpy as np
    >>> x = np.arange(10000, dtype=float)
    >>> tm = Table([x], masked=True)
    >>> tm.write('tm.ecsv', overwrite=True)

If you want to write ``.ecsv`` using :meth:`~astropy.table.Table.write`,
then use ``serialize_method='data_mask'``.
This uses the non-masked version of data and it is faster::

    >>> tm.write('tm.ecsv', overwrite=True, serialize_method='data_mask')

Read FITS with memmap=True
--------------------------

By default :meth:`~astropy.table.Table.read` will read the whole table into
memory, which can take a lot of memory and can take a lot of time, depending on
the table size and file format. In some cases, it is possible to only read a
subset of the table by choosing the option ``memmap=True``.

For FITS binary tables, the data is stored row by row, and it is possible to
read only a subset of rows, but reading a full column loads the whole table data
into memory::

    >>> import numpy as np
    >>> from astropy.table import Table
    >>> tbl = Table({'a': np.arange(1e7),
    ...              'b': np.arange(1e7, dtype=float),
    ...              'c': np.arange(1e7, dtype=float)})
    >>> tbl.write('test.fits', overwrite=True)
    >>> table = Table.read('test.fits', memmap=True)  # Very fast, doesn't actually load data
    >>> table2 = tbl[:100]  # Fast, will read only first 100 rows
    >>> print(table2)  # Accessing column data triggers the read
     a    b    c
    ---- ---- ----
    0.0  0.0  0.0
    1.0  1.0  1.0
    2.0  2.0  2.0
    ...  ...  ...
    98.0 98.0 98.0
    99.0 99.0 99.0
    Length = 100 rows
    >>> col = table['my_column']  # Will load all table into memory

:meth:`~astropy.table.Table.read` does not support ``memmap=True``
for the HDF5 and ASCII file formats.
.. _modify_table:

Modifying a Table
*****************

The data values within a |Table| object can be modified in much the same manner
as for ``numpy`` `structured arrays
<https://numpy.org/doc/stable/user/basics.rec.html>`_ by accessing columns or
rows of data and assigning values appropriately. A key enhancement provided by
the |Table| class is the ability to modify the structure of the table: you can
add or remove columns, and add new rows of data.

Quick Overview
==============

The code below shows the basics of modifying a table and its data.

Examples
--------

.. EXAMPLE START: Making a Table and Modifying Data

**Make a table**
::

  >>> from astropy.table import Table
  >>> import numpy as np
  >>> arr = np.arange(15).reshape(5, 3)
  >>> t = Table(arr, names=('a', 'b', 'c'), meta={'keywords': {'key1': 'val1'}})

**Modify data values**
::

  >>> t['a'][:] = [1, -2, 3, -4, 5]  # Set all values of column 'a'
  >>> t['a'][2] = 30                 # Set row 2 of column 'a'
  >>> t[1] = (8, 9, 10)              # Set all values of row 1
  >>> t[1]['b'] = -9                 # Set column 'b' of row 1
  >>> t[0:3]['c'] = 100              # Set column 'c' of rows 0, 1, 2

Note that ``table[row][column]`` assignments will not work with ``numpy``
"fancy" ``row`` indexing (in that case ``table[row]`` would be a *copy* instead
of a *view*). "Fancy" ``numpy`` indices include a :class:`list`, |ndarray|, or
:class:`tuple` of |ndarray| (e.g., the return from :func:`numpy.where`)::

  >>> t[[1, 2]]['a'] = [3., 5.]             # doesn't change table t
  >>> t[np.array([1, 2])]['a'] = [3., 5.]   # doesn't change table t
  >>> t[np.where(t['a'] > 3)]['a'] = 3.     # doesn't change table t

Instead use ``table[column][row]`` order::

  >>> t['a'][[1, 2]] = [3., 5.]
  >>> t['a'][np.array([1, 2])] = [3., 5.]
  >>> t['a'][np.where(t['a'] > 3)] = 3.

You can also modify data columns with ``unit`` set in a way that follows
the conventions of `~astropy.units.Quantity` by using the
:attr:`~astropy.table.Column.quantity` property::

  >>> from astropy import units as u
  >>> tu = Table([[1, 2.5]], names=('a',))
  >>> tu['a'].unit = u.m
  >>> tu['a'].quantity[:] = [1, 2] * u.km
  >>> tu['a']
  <Column name='a' dtype='float64' unit='m' length=2>
  1000.0
  2000.0

.. note::

  The best way to combine the functionality of the |Table| and |Quantity|
  classes is to use a |QTable|. See :ref:`quantity_and_qtable` for more
  information.

.. EXAMPLE END

**Add a column or columns**

.. EXAMPLE START: Adding Columns to Tables

A single column can be added to a table using syntax like adding a key-value
pair to a :class:`dict`. The value on the right hand side can be a
:class:`list` or |ndarray| of the correct size, or a scalar value that will be
`broadcast <https://numpy.org/doc/stable/user/basics.broadcasting.html>`_::

  >>> t['d1'] = np.arange(5)
  >>> t['d2'] = [1, 2, 3, 4, 5]
  >>> t['d3'] = 6  # all 5 rows set to 6

For more explicit control, the :meth:`~astropy.table.Table.add_column` and
:meth:`~astropy.table.Table.add_columns` methods can be used to add one or
multiple columns to a table. In both cases the new column(s) can be specified as
a :class:`list`, |ndarray|, |Column|, |MaskedColumn|, or a scalar::

  >>> from astropy.table import Column
  >>> t.add_column(np.arange(5), name='aa', index=0)  # Insert before first table column
  >>> t.add_column(1.0, name='bb')  # Add column of all 1.0 to end of table
  >>> c = Column(np.arange(5), name='e')
  >>> t.add_column(c, index=0)  # Add Column using the existing column name 'e'
  >>> t.add_columns([[1, 2, 3, 4, 5], ['v', 'w', 'x', 'y', 'z']], names=['h', 'i'])

Finally, columns can also be added from |Quantity| objects, which automatically
sets the ``unit`` attribute on the column (but you might find it more
convenient to add a |Quantity| to a |QTable| instead, see
:ref:`quantity_and_qtable` for details)::

  >>> from astropy import units as u
  >>> t['d'] = np.arange(1., 6.) * u.m
  >>> t['d']
  <Column name='d' dtype='float64' unit='m' length=5>
  1.0
  2.0
  3.0
  4.0
  5.0

.. EXAMPLE END

**Remove columns**

.. EXAMPLE START: Removing Columns from Tables

To remove a column from a table::

  >>> t.remove_column('d1')
  >>> t.remove_columns(['aa', 'd2', 'e'])
  >>> del t['d3']
  >>> del t['h', 'i']
  >>> t.keep_columns(['a', 'b'])

.. EXAMPLE END

**Replace a column**

.. EXAMPLE START: Replacing Columns in Tables

You can entirely replace an existing column with a new column by setting the
column to any object that could be used to initialize a table column (e.g.,  a
:class:`list` or |ndarray|). For example, you could change the data type of the
``a`` column from ``int`` to ``float`` using::

  >>> t['a'] = t['a'].astype(float)

If the right-hand side value is not column-like, then an in-place update using
`broadcasting <https://numpy.org/doc/stable/user/basics.broadcasting.html>`_
will be done, for example::

  >>> t['a'] = 1  # Internally does t['a'][:] = 1

.. EXAMPLE END

**Perform a dictionary-style update**

It is possible to perform a dictionary-style update, which adds new columns to
the table and replaces existing ones::

  >>> t1 = Table({'name': ['foo', 'bar'], 'val': [0., 0.]}, meta={'n': 2})
  >>> t2 = Table({'val': [1., 2.], 'val2': [10., 10.]}, meta={'id': 0})
  >>> t1.update(t2)
  >>> t1
  <Table length=2>
  name   val     val2
  str3 float64 float64
  ---- ------- -------
   foo     1.0    10.0
   bar     2.0    10.0

:meth:`~astropy.table.Table.update` also takes care of silently :ref:`merging_metadata`::

  >>> t1.meta
  {'n': 2, 'id': 0}

The input of :meth:`~astropy.table.Table.update` does not have to be a |Table|,
it can be anything that can be used for :ref:`construct_table` with a
compatible number of rows.

**Rename columns**

.. EXAMPLE START: Renaming Columns in Tables

To rename a column::

  >>> t.rename_column('a', 'a_new')
  >>> t['b'].name = 'b_new'

.. EXAMPLE END

**Add a row of data**

.. EXAMPLE START: Adding a Row of Data to a Table

To add a row::

  >>> t.add_row([-8, -9])

.. EXAMPLE END

**Remove rows**

.. EXAMPLE START: Removing Rows of Data from Tables

To remove a row::

  >>> t.remove_row(0)
  >>> t.remove_rows(slice(4, 5))
  >>> t.remove_rows([1, 2])

.. EXAMPLE END

**Sort by one or more columns**

.. EXAMPLE START: Sorting Columns in Tables

To sort columns::

  >>> t.sort('b_new')
  >>> t.sort(['a_new', 'b_new'])

.. EXAMPLE END

**Reverse table rows**

.. EXAMPLE START: Reversing Table Rows

To reverse the order of table rows::

  >>> t.reverse()

.. EXAMPLE END

**Modify metadata**

.. EXAMPLE START: Modifying Metadata in Tables

To modify metadata::

  >>> t.meta['key'] = 'value'

.. EXAMPLE END

**Select or reorder columns**

.. EXAMPLE START: Selecting or Reordering Columns in Tables

A new table with a subset or reordered list of columns can be
created as shown in the following example::

  >>> t = Table(arr, names=('a', 'b', 'c'))
  >>> t_acb = t['a', 'c', 'b']

Another way to do the same thing is to provide a list or tuple
as the item, as shown below::

  >>> new_order = ['a', 'c', 'b']  # List or tuple
  >>> t_acb = t[new_order]

.. EXAMPLE END

Caveats
=======

Modifying the table data and properties is fairly clear-cut, but one thing
to keep in mind is that adding a row *may* require a new copy in memory of the
table data. This depends on the detailed layout of Python objects in memory
and cannot be reliably controlled. In some cases it may be possible to build a
table row by row in less than O(N**2) time but you cannot count on it.

Another subtlety to keep in mind is that in some cases the return value of an
operation results in a new table in memory while in other cases it results in a
view of the existing table data. As an example, imagine trying to set two table
elements using column selection with ``t['a', 'c']`` in combination with row
index selection::

  >>> t = Table([[1, 2], [3, 4], [5, 6]], names=('a', 'b', 'c'))
  >>> t['a', 'c'][1] = (100, 100)
  >>> print(t)
   a   b   c
  --- --- ---
    1   3   5
    2   4   6

This might be surprising because the data values did not change and there
was no error. In fact, what happened is that ``t['a', 'c']`` created a
new temporary table in memory as a *copy* of the original and then updated the
first row of the copy. The original ``t`` table was unaffected and the new
temporary table disappeared once the statement was complete. The takeaway
is to pay attention to how certain operations are performed one step at
a time.

.. _table-replace-1_3:

In-Place Versus Replace Column Update
=====================================

Consider this code snippet::

  >>> t = Table([[1, 2, 3]], names=['a'])
  >>> t['a'] = [10.5, 20.5, 30.5]

There are a couple of ways this could be handled. It could update the existing
array values in-place (truncating to integer), or it could replace the entire
column with a new column based on the supplied data values.

The answer for ``astropy`` is that the operation shown above does a *complete
replacement* of the column object. In this case it makes a new column object
with float values by internally calling ``t.replace_column('a', [10.5, 20.5,
30.5])``. In general this behavior is more consistent with Python and `pandas
<https://pandas.pydata.org>`_ behavior.

**Forcing in-place update**

It is possible to force an in-place update of a column as follows::

  t[colname][:] = value

**Finding the source of problems**

In order to find potential problems related to replacing columns, there is the
option `astropy.table.conf.replace_warnings
<astropy.table.Conf.replace_warnings>` in the :ref:`astropy_config`. This
controls a set of warnings that are emitted under certain circumstances when a
table column is replaced. This option must be set to a list that includes zero
or more of the following string values:

``always`` :
  Print a warning every time a column gets replaced via the
  ``__setitem__()`` syntax (i.e., ``t['a'] = new_col``).

``slice`` :
  Print a warning when a column that appears to be a :class:`slice` of
  a parent column is replaced.

``refcount`` :
  Print a warning when the Python reference count for the
  column changes. This indicates that a stale object exists that might
  be used elsewhere in the code and give unexpected results.

``attributes`` :
  Print a warning if any of the standard column attributes changed.

The default value for the ``table.conf.replace_warnings`` option is
``[]`` (no warnings).
.. doctest-skip-all

.. _pandas:

Interfacing with the Pandas Package
***********************************

The `pandas <https://pandas.pydata.org/>`__ package is a package for high
performance data analysis of table-like structures that is complementary to the
:class:`~astropy.table.Table` class in ``astropy``.

In order to exchange data between the :class:`~astropy.table.Table` class and
the :class:`pandas.DataFrame` class (the main data structure in ``pandas``),
the |Table| class includes two methods, :meth:`~astropy.table.Table.to_pandas`
and :meth:`~astropy.table.Table.from_pandas`.

Example
-------

.. EXAMPLE START: Interfacing Tables with the Pandas Package

To demonstrate, we can create a minimal table::

    >>> from astropy.table import Table
    >>> t = Table()
    >>> t['a'] = [1, 2, 3, 4]
    >>> t['b'] = ['a', 'b', 'c', 'd']

Which we can then convert to a :class:`~pandas.DataFrame`::

    >>> df = t.to_pandas()
    >>> df
       a  b
    0  1  a
    1  2  b
    2  3  c
    3  4  d
    >>> type(df)
    <class 'pandas.core.frame.DataFrame'>

It is also possible to create a table from a :class:`~pandas.DataFrame`::

    >>> t2 = Table.from_pandas(df)
    >>> t2
    <Table length=4>
      a      b
    int64 string8
    ----- -------
        1       a
        2       b
        3       c
        4       d

.. EXAMPLE END

The conversions to and from ``pandas`` are subject to the following caveats:

* The :class:`~pandas.DataFrame` structure does not support multidimensional
  columns, so |Table| objects with multidimensional columns cannot be converted
  to :class:`~pandas.DataFrame`.

* Masked tables can be converted, but in columns of ``float`` or string values
  the resulting :class:`~pandas.DataFrame` uses `numpy.nan` to indicate missing
  values. For ``float`` columns, the conversion therefore does not necessarily
  round-trip if converting back to an ``astropy`` table, because the
  distinction between `numpy.nan` and masked values is lost. This is not a
  problem for integer columns.

* Tables with :ref:`mixin_columns` can not be converted.
.. |join| replace:: :func:`~astropy.table.join`

.. _table_operations:

Table Operations
****************

In this section we describe high-level operations that can be used to generate
a new table from one or more input tables. This includes:

=======================

.. list-table::
   :header-rows: 1
   :widths: 28 52 20

   * - Documentation
     - Description
     - Function
   * - `Grouped operations`_
     - Group tables and columns by keys
     - :func:`~astropy.table.Table.group_by`
   * - `Binning`_
     - Binning tables
     - :func:`~astropy.table.Table.group_by`
   * - `Stack vertically`_
     - Concatenate input tables along rows
     - :func:`~astropy.table.vstack`
   * - `Stack horizontally`_
     - Concatenate input tables along columns
     - :func:`~astropy.table.hstack`
   * - `Join`_
     - Database-style join of two tables
     - |join|
   * - `Unique rows`_
     - Unique table rows by keys
     - :func:`~astropy.table.unique`
   * - `Set difference`_
     - Set difference of two tables
     - :func:`~astropy.table.setdiff`
   * - `Table diff`_
     - Generic difference of two simple tables
     - :func:`~astropy.utils.diff.report_diff_values`


.. _grouped-operations:

Grouped Operations
------------------

.. EXAMPLE START: Grouped Operations in Tables

Sometimes in a table or table column there are natural groups within the dataset
for which it makes sense to compute some derived values. A minimal example is a
list of objects with photometry from various observing runs::

  >>> from astropy.table import Table
  >>> obs = Table.read("""name    obs_date    mag_b  mag_v
  ...                     M31     2012-01-02  17.0   17.5
  ...                     M31     2012-01-02  17.1   17.4
  ...                     M101    2012-01-02  15.1   13.5
  ...                     M82     2012-02-14  16.2   14.5
  ...                     M31     2012-02-14  16.9   17.3
  ...                     M82     2012-02-14  15.2   15.5
  ...                     M101    2012-02-14  15.0   13.6
  ...                     M82     2012-03-26  15.7   16.5
  ...                     M101    2012-03-26  15.1   13.5
  ...                     M101    2012-03-26  14.8   14.3
  ...                     """, format='ascii')
  >>> # Make sure magnitudes are printed with one digit after the decimal point
  >>> obs['mag_b'].info.format = '{:.1f}'
  >>> obs['mag_v'].info.format = '{:.1f}'

.. EXAMPLE END

Table Groups
^^^^^^^^^^^^

Now suppose we want the mean magnitudes for each object. We first group the data
by the ``name`` column with the :func:`~astropy.table.Table.group_by` method.
This returns a new table sorted by ``name`` which has a ``groups`` property
specifying the unique values of ``name`` and the corresponding table rows::

  >>> obs_by_name = obs.group_by('name')
  >>> print(obs_by_name)  # doctest: +SKIP
  name  obs_date  mag_b mag_v
  ---- ---------- ----- -----
  M101 2012-01-02  15.1  13.5  << First group (index=0, key='M101')
  M101 2012-02-14  15.0  13.6
  M101 2012-03-26  15.1  13.5
  M101 2012-03-26  14.8  14.3
   M31 2012-01-02  17.0  17.5  << Second group (index=4, key='M31')
   M31 2012-01-02  17.1  17.4
   M31 2012-02-14  16.9  17.3
   M82 2012-02-14  16.2  14.5  << Third group (index=7, key='M83')
   M82 2012-02-14  15.2  15.5
   M82 2012-03-26  15.7  16.5
                               << End of groups (index=10)
  >>> print(obs_by_name.groups.keys)
  name
  ----
  M101
   M31
   M82
  >>> print(obs_by_name.groups.indices)
  [ 0  4  7 10]

The ``groups`` property is the portal to all grouped operations with tables and
columns. It defines how the table is grouped via an array of the unique row key
values and the indices of the group boundaries for those key values. The groups
here correspond to the row slices ``0:4``, ``4:7``, and ``7:10`` in the
``obs_by_name`` table.

The initial argument (``keys``) for the :func:`~astropy.table.Table.group_by`
function can take a number of input data types:

- Single string value with a table column name (as shown above)
- List of string values with table column names
- Another |Table| or |Column| with same length as table
- ``numpy`` structured array with same length as table
- ``numpy`` homogeneous array with same length as table

In all cases the corresponding row elements are considered as a :class:`tuple`
of values which form a key value that is used to sort the original table and
generate the required groups.

As an example, to get the average magnitudes for each object on each observing
night, we would first group the table on both ``name`` and ``obs_date`` as
follows::

  >>> print(obs.group_by(['name', 'obs_date']).groups.keys)
  name  obs_date
  ---- ----------
  M101 2012-01-02
  M101 2012-02-14
  M101 2012-03-26
   M31 2012-01-02
   M31 2012-02-14
   M82 2012-02-14
   M82 2012-03-26


Manipulating Groups
^^^^^^^^^^^^^^^^^^^

.. EXAMPLE START: Manipulating Groups in Tables

Once you have applied grouping to a table then you can access the individual
groups or subsets of groups. In all cases this returns a new grouped table.
For instance, to get the subtable which corresponds to the second group
(index=1) do::

  >>> print(obs_by_name.groups[1])
  name  obs_date  mag_b mag_v
  ---- ---------- ----- -----
   M31 2012-01-02  17.0  17.5
   M31 2012-01-02  17.1  17.4
   M31 2012-02-14  16.9  17.3

To get the first and second groups together use a :class:`slice`::

  >>> groups01 = obs_by_name.groups[0:2]
  >>> print(groups01)
  name  obs_date  mag_b mag_v
  ---- ---------- ----- -----
  M101 2012-01-02  15.1  13.5
  M101 2012-02-14  15.0  13.6
  M101 2012-03-26  15.1  13.5
  M101 2012-03-26  14.8  14.3
   M31 2012-01-02  17.0  17.5
   M31 2012-01-02  17.1  17.4
   M31 2012-02-14  16.9  17.3
  >>> print(groups01.groups.keys)
  name
  ----
  M101
   M31

You can also supply a ``numpy`` array of indices or a boolean mask to select
particular groups, for example::

  >>> mask = obs_by_name.groups.keys['name'] == 'M101'
  >>> print(obs_by_name.groups[mask])
  name  obs_date  mag_b mag_v
  ---- ---------- ----- -----
  M101 2012-01-02  15.1  13.5
  M101 2012-02-14  15.0  13.6
  M101 2012-03-26  15.1  13.5
  M101 2012-03-26  14.8  14.3

You can iterate over the group subtables and corresponding keys with::

  >>> for key, group in zip(obs_by_name.groups.keys, obs_by_name.groups):
  ...     print(f'****** {key["name"]} *******')
  ...     print(group)
  ...     print('')
  ...
  ****** M101 *******
  name  obs_date  mag_b mag_v
  ---- ---------- ----- -----
  M101 2012-01-02  15.1  13.5
  M101 2012-02-14  15.0  13.6
  M101 2012-03-26  15.1  13.5
  M101 2012-03-26  14.8  14.3
  ****** M31 *******
  name  obs_date  mag_b mag_v
  ---- ---------- ----- -----
   M31 2012-01-02  17.0  17.5
   M31 2012-01-02  17.1  17.4
   M31 2012-02-14  16.9  17.3
  ****** M82 *******
  name  obs_date  mag_b mag_v
  ---- ---------- ----- -----
   M82 2012-02-14  16.2  14.5
   M82 2012-02-14  15.2  15.5
   M82 2012-03-26  15.7  16.5

.. EXAMPLE END

Column Groups
^^^^^^^^^^^^^

Like |Table| objects, |Column| objects can also be grouped for subsequent
manipulation with grouped operations. This can apply both to columns within a
|Table| or bare |Column| objects.

As for |Table|, the grouping is generated with the
:func:`~astropy.table.Table.group_by` method. The difference here is that
there is no option of providing one or more column names since that
does not make sense for a |Column|.

Examples
~~~~~~~~

.. EXAMPLE START: Grouping Column Objects in Tables

To generate grouping in columns::

  >>> from astropy.table import Column
  >>> import numpy as np
  >>> c = Column([1, 2, 3, 4, 5, 6], name='a')
  >>> key_vals = np.array(['foo', 'bar', 'foo', 'foo', 'qux', 'qux'])
  >>> cg = c.group_by(key_vals)

  >>> for key, group in zip(cg.groups.keys, cg.groups):
  ...     print(f'****** {key} *******')
  ...     print(group)
  ...     print('')
  ...
  ****** bar *******
   a
  ---
    2
  ****** foo *******
   a
  ---
    1
    3
    4
  ****** qux *******
   a
  ---
    5
    6

.. EXAMPLE END

Aggregation
^^^^^^^^^^^

Aggregation is the process of applying a specified reduction function to the
values within each group for each non-key column. This function must accept a
|ndarray| as the first argument and return a single scalar value. Common
function examples are :func:`numpy.sum`, :func:`numpy.mean`, and
:func:`numpy.std`.

For the example grouped table ``obs_by_name`` from above, we compute the group
means with the :meth:`~astropy.table.groups.TableGroups.aggregate` method::

  >>> obs_mean = obs_by_name.groups.aggregate(np.mean)  # doctest: +SHOW_WARNINGS
  AstropyUserWarning: Cannot aggregate column 'obs_date' with type '<U10'
  >>> print(obs_mean)
  name mag_b mag_v
  ---- ----- -----
  M101  15.0  13.7
   M31  17.0  17.4
   M82  15.7  15.5

It seems the magnitude values were successfully averaged, but what about the
:class:`~astropy.utils.exceptions.AstropyUserWarning`? Since the ``obs_date``
column is a string-type array, the :func:`numpy.mean` function failed and
raised an exception.  Any time this happens
:meth:`~astropy.table.groups.TableGroups.aggregate` will issue a warning and
then drop that column from the output result. Note that the ``name`` column is
one of the ``keys`` used to determine the grouping so it is automatically
ignored from aggregation.

.. EXAMPLE START: Performing Aggregation on Grouped Tables

From a grouped table it is possible to select one or more columns on which
to perform the aggregation::

  >>> print(obs_by_name['mag_b'].groups.aggregate(np.mean))
  mag_b
  -----
   15.0
   17.0
   15.7

The order of the columns can be specified too::

  >>> print(obs_by_name['name', 'mag_v', 'mag_b'].groups.aggregate(np.mean))
  name mag_v mag_b
  ---- ----- -----
  M101  13.7  15.0
   M31  17.4  17.0
   M82  15.5  15.7


A single column of data can be aggregated as well::

  >>> c = Column([1, 2, 3, 4, 5, 6], name='a')
  >>> key_vals = np.array(['foo', 'bar', 'foo', 'foo', 'qux', 'qux'])
  >>> cg = c.group_by(key_vals)
  >>> cg_sums = cg.groups.aggregate(np.sum)
  >>> for key, cg_sum in zip(cg.groups.keys, cg_sums):
  ...     print(f'Sum for {key} = {cg_sum}')
  ...
  Sum for bar = 2
  Sum for foo = 8
  Sum for qux = 11

.. EXAMPLE END

If the specified function has a :meth:`numpy.ufunc.reduceat` method, this will
be called instead. This can improve the performance by a factor of 10 to 100
(or more) for large unmasked tables or columns with many relatively small
groups.  It also allows for the use of certain ``numpy`` functions which
normally take more than one input array but also work as reduction functions,
like `numpy.add`.  The ``numpy`` functions which should take advantage of using
:meth:`numpy.ufunc.reduceat` include:

- `numpy.add`
- `numpy.arctan2`
- `numpy.bitwise_and`
- `numpy.bitwise_or`
- `numpy.bitwise_xor`
- `numpy.copysign`
- `numpy.divide`
- `numpy.equal`
- `numpy.floor_divide`
- `numpy.fmax`
- `numpy.fmin`
- `numpy.fmod`
- `numpy.greater_equal`
- `numpy.greater`
- `numpy.hypot`
- `numpy.left_shift`
- `numpy.less_equal`
- `numpy.less`
- `numpy.logaddexp2`
- `numpy.logaddexp`
- `numpy.logical_and`
- `numpy.logical_or`
- `numpy.logical_xor`
- `numpy.maximum`
- `numpy.minimum`
- `numpy.mod`
- `numpy.multiply`
- `numpy.not_equal`
- `numpy.power`
- `numpy.remainder`
- `numpy.right_shift`
- `numpy.subtract`
- `numpy.true_divide`

In special cases, :func:`numpy.sum` and :func:`numpy.mean` are substituted with
their respective ``reduceat`` methods.

Filtering
^^^^^^^^^

Table groups can be filtered by means of the
:meth:`~astropy.table.groups.TableGroups.filter` method. This is done by
supplying a function which is called for each group. The function
which is passed to this method must accept two arguments:

- ``table`` : |Table| object
- ``key_colnames`` : list of columns in ``table`` used as keys for grouping

It must then return either `True` or `False`.

Example
~~~~~~~

.. EXAMPLE START: Filtering Table Groups

The following will select all table groups with only positive values in the non-
key columns::

  >>> def all_positive(table, key_colnames):
  ...     colnames = [name for name in table.colnames if name not in key_colnames]
  ...     for colname in colnames:
  ...         if np.any(table[colname] <= 0):
  ...             return False
  ...     return True

An example of using this function is::

  >>> t = Table.read(""" a   b    c
  ...                   -2  7.0   2
  ...                   -2  5.0   1
  ...                    1  3.0  -5
  ...                    1 -2.0  -6
  ...                    1  1.0   7
  ...                    0  4.0   4
  ...                    3  3.0   5
  ...                    3 -2.0   6
  ...                    3  1.0   7""", format='ascii')
  >>> tg = t.group_by('a')
  >>> t_positive = tg.groups.filter(all_positive)
  >>> for group in t_positive.groups:
  ...     print(group)
  ...     print('')
  ...
   a   b   c
  --- --- ---
   -2 7.0   2
   -2 5.0   1
  <BLANKLINE>
   a   b   c
  --- --- ---
    0 4.0   4

As can be seen only the groups with ``a == -2`` and ``a == 0`` have all
positive values in the non-key columns, so those are the ones that are selected.

Likewise a grouped column can be filtered with the
:meth:`~astropy.table.groups.ColumnGroups.filter`, method but in this case the
filtering function takes only a single argument which is the column group. It
still must return either `True` or `False`. For example::

  def all_positive(column):
      return np.all(column > 0)

.. EXAMPLE END

.. _table_binning:

Binning
-------

A common tool in analysis is to bin a table based on some reference value.
Examples:

- Photometry of a binary star in several bands taken over a
  span of time which should be binned by orbital phase.
- Reducing the sampling density for a table by combining
  100 rows at a time.
- Unevenly sampled historical data which should binned to
  four points per year.

All of these examples of binning a table can be accomplished using
`grouped operations`_. The examples in that section are focused on the
case of discrete key values such as the name of a source. In this
section we show a concise yet powerful way of applying grouped operations to
accomplish binning on key values such as time, phase, or row number.

The common theme in all of these cases is to convert the key value array into
a new float- or int-valued array whose values are identical for rows in the same
output bin.

Example
^^^^^^^

.. EXAMPLE START: Binning a Table using Grouped Operations

As an example, we generate a fake light curve::

  >>> year = np.linspace(2000.0, 2010.0, 200)  # 200 observations over 10 years
  >>> period = 1.811
  >>> y0 = 2005.2
  >>> mag = 14.0 + 1.2 * np.sin(2 * np.pi * (year - y0) / period)
  >>> phase = ((year - y0) / period) % 1.0
  >>> dat = Table([year, phase, mag], names=['year', 'phase', 'mag'])

Now we make an array that will be used for binning the data by 0.25 year
intervals::

  >>> year_bin = np.trunc(year / 0.25)

This has the property that all samples in each 0.25 year bin have the same
value of ``year_bin``. Think of ``year_bin`` as the bin number for ``year``.
Then do the binning by grouping and immediately aggregating with
:func:`numpy.mean`.

  >>> dat_grouped = dat.group_by(year_bin)
  >>> dat_binned = dat_grouped.groups.aggregate(np.mean)

We can plot the results with ``plt.plot(dat_binned['year'], dat_binned['mag'],
'.')``. Alternately, we could bin into 10 phase bins::

  >>> phase_bin = np.trunc(phase / 0.1)
  >>> dat_grouped = dat.group_by(phase_bin)
  >>> dat_binned = dat_grouped.groups.aggregate(np.mean)

This time, try plotting with ``plt.plot(dat_binned['phase'],
dat_binned['mag'])``.

.. EXAMPLE END

.. _stack-vertically:

Stack Vertically
----------------

The |Table| class supports stacking tables vertically with the
:func:`~astropy.table.vstack` function. This process is also commonly known as
concatenating or appending tables in the row direction. It corresponds roughly
to the :func:`numpy.vstack` function.

Examples
^^^^^^^^

.. EXAMPLE START: Stacking (or Concatenating) Tables Vertically

Suppose we have two tables of observations with several column names in
common::

  >>> from astropy.table import Table, vstack
  >>> obs1 = Table.read("""name    obs_date    mag_b  logLx
  ...                      M31     2012-01-02  17.0   42.5
  ...                      M82     2012-10-29  16.2   43.5
  ...                      M101    2012-10-31  15.1   44.5""", format='ascii')

  >>> obs2 = Table.read("""name    obs_date    logLx
  ...                      NGC3516 2011-11-11  42.1
  ...                      M31     1999-01-05  43.1
  ...                      M82     2012-10-30  45.0""", format='ascii')

Now we can stack these two tables::

  >>> print(vstack([obs1, obs2]))
    name   obs_date  mag_b logLx
  ------- ---------- ----- -----
      M31 2012-01-02  17.0  42.5
      M82 2012-10-29  16.2  43.5
     M101 2012-10-31  15.1  44.5
  NGC3516 2011-11-11    --  42.1
      M31 1999-01-05    --  43.1
      M82 2012-10-30    --  45.0

Notice that the ``obs2`` table is missing the ``mag_b`` column, so in the
stacked output table those values are marked as missing. This is the default
behavior and corresponds to ``join_type='outer'``. There are two other allowed
values for the ``join_type`` argument, ``'inner'`` and ``'exact'``::

  >>> print(vstack([obs1, obs2], join_type='inner'))
    name   obs_date  logLx
  ------- ---------- -----
      M31 2012-01-02  42.5
      M82 2012-10-29  43.5
     M101 2012-10-31  44.5
  NGC3516 2011-11-11  42.1
      M31 1999-01-05  43.1
      M82 2012-10-30  45.0

  >>> print(vstack([obs1, obs2], join_type='exact'))  # doctest: +IGNORE_EXCEPTION_DETAIL
  Traceback (most recent call last):
    ...
  TableMergeError: Inconsistent columns in input arrays (use 'inner'
  or 'outer' join_type to allow non-matching columns)

In the case of ``join_type='inner'``, only the common columns (the intersection)
are present in the output table. When ``join_type='exact'`` is specified, then
:func:`~astropy.table.vstack` requires that all of the input tables have
exactly the same column names.

More than two tables can be stacked by supplying a longer list of tables::

  >>> obs3 = Table.read("""name    obs_date    mag_b  logLx
  ...                      M45     2012-02-03  15.0   40.5""", format='ascii')
  >>> print(vstack([obs1, obs2, obs3]))
    name   obs_date  mag_b logLx
  ------- ---------- ----- -----
      M31 2012-01-02  17.0  42.5
      M82 2012-10-29  16.2  43.5
     M101 2012-10-31  15.1  44.5
  NGC3516 2011-11-11    --  42.1
      M31 1999-01-05    --  43.1
      M82 2012-10-30    --  45.0
      M45 2012-02-03  15.0  40.5

See also the sections on `Merging metadata`_ and `Merging column attributes`_
for details on how these characteristics of the input tables are merged in the
single output table. Note also that you can use a single table |Row| instead of
a full table as one of the inputs.

.. EXAMPLE END

.. _stack-horizontally:

Stack Horizontally
------------------

The |Table| class supports stacking tables horizontally (in the column-wise
direction) with the :func:`~astropy.table.hstack` function. It corresponds
roughly to the :func:`numpy.hstack` function.

Examples
^^^^^^^^

.. EXAMPLE START: Stacking (or Concatenating) Tables Horizontally

Suppose we have the following two tables::

  >>> from astropy.table import Table, hstack
  >>> t1 = Table.read("""a   b    c
  ...                    1   foo  1.4
  ...                    2   bar  2.1
  ...                    3   baz  2.8""", format='ascii')
  >>> t2 = Table.read("""d     e
  ...                    ham   eggs
  ...                    spam  toast""", format='ascii')

Now we can stack these two tables horizontally::

  >>> print(hstack([t1, t2]))
   a   b   c   d     e
  --- --- --- ---- -----
    1 foo 1.4  ham  eggs
    2 bar 2.1 spam toast
    3 baz 2.8   --    --

As with :func:`~astropy.table.vstack`, there is an optional ``join_type``
argument that can take values ``'inner'``, ``'exact'``, and ``'outer'``. The
default is ``'outer'``, which effectively takes the union of available rows and
masks out any missing values. This is illustrated in the example above. The
other options give the intersection of rows, where ``'exact'`` requires that
all tables have exactly the same number of rows::

  >>> print(hstack([t1, t2], join_type='inner'))
   a   b   c   d     e
  --- --- --- ---- -----
    1 foo 1.4  ham  eggs
    2 bar 2.1 spam toast

  >>> print(hstack([t1, t2], join_type='exact'))  # doctest: +IGNORE_EXCEPTION_DETAIL
  Traceback (most recent call last):
    ...
  TableMergeError: Inconsistent number of rows in input arrays (use 'inner' or
  'outer' join_type to allow non-matching rows)

More than two tables can be stacked by supplying a longer list of tables. The
example below also illustrates the behavior when there is a conflict in the
input column names (see the section on `Column renaming`_ for details)::

  >>> t3 = Table.read("""a    b
  ...                    M45  2012-02-03""", format='ascii')
  >>> print(hstack([t1, t2, t3]))
  a_1 b_1  c   d     e   a_3    b_3
  --- --- --- ---- ----- --- ----------
    1 foo 1.4  ham  eggs M45 2012-02-03
    2 bar 2.1 spam toast  --         --
    3 baz 2.8   --    --  --         --

The metadata from the input tables is merged by the process described in the
`Merging metadata`_ section. Note also that you can use a single table |Row|
instead of a full table as one of the inputs.

.. EXAMPLE END

.. _stack-depthwise:

Stack Depth-Wise
----------------

The |Table| class supports stacking columns within tables depth-wise using the
:func:`~astropy.table.dstack` function. It corresponds roughly to running the
:func:`numpy.dstack` function on the individual columns matched by name.

Examples
^^^^^^^^

.. EXAMPLE START: Stacking (or Concatenating) Tables Depth-Wise

Suppose we have tables of data for sources giving information on the enclosed
source counts for different PSF fractions::

  >>> from astropy.table import Table, dstack
  >>> src1 = Table.read("""psf_frac  counts
  ...                      0.10        45
  ...                      0.50        90
  ...                      0.90       120
  ...                      """, format='ascii')

  >>> src2 = Table.read("""psf_frac  counts
  ...                      0.10       200
  ...                      0.50       300
  ...                      0.90       350
  ...                      """, format='ascii')

Now we can stack these two tables depth-wise to get a single table with the
characteristics of both sources::

  >>> srcs = dstack([src1, src2])
  >>> print(srcs)
  psf_frac [2] counts [2]
  ------------ ----------
    0.1 .. 0.1  45 .. 200
    0.5 .. 0.5  90 .. 300
    0.9 .. 0.9 120 .. 350

In this case the counts for the first source are accessible as
``srcs['counts'][:, 0]``, and likewise the second source counts are
``srcs['counts'][:, 1]``.

For this function the length of all input tables must be the same. This
function can accept ``join_type`` and ``metadata_conflicts`` just like the
:func:`~astropy.table.vstack` function. The ``join_type`` argument controls how
to handle mismatches in the columns of the input table.

See also the sections on `Merging metadata`_ and `Merging column attributes`_
for details on how these characteristics of the input tables are merged in the
single output table. Note also that you can use a single table |Row| instead of
a full table as one of the inputs.

.. EXAMPLE END

.. _table-join:

Join
----

The |Table| class supports the `database join
<https://en.wikipedia.org/wiki/Join_(SQL)>`_ operation. This provides a flexible
and powerful way to combine tables based on the values in one or more key
columns.

Examples
^^^^^^^^

.. EXAMPLE START: Combining Tables using the Database Join Operation

Suppose we have two tables of observations, the first with B and V magnitudes
and the second with X-ray luminosities of an overlapping (but not identical)
sample::

  >>> from astropy.table import Table, join
  >>> optical = Table.read("""name    obs_date    mag_b  mag_v
  ...                         M31     2012-01-02  17.0   16.0
  ...                         M82     2012-10-29  16.2   15.2
  ...                         M101    2012-10-31  15.1   15.5""", format='ascii')
  >>> xray = Table.read("""   name    obs_date    logLx
  ...                         NGC3516 2011-11-11  42.1
  ...                         M31     1999-01-05  43.1
  ...                         M82     2012-10-29  45.0""", format='ascii')

The |join| method allows you to merge these two tables into a single table based
on matching values in the "key columns". By default, the key columns are the set
of columns that are common to both tables. In this case the key columns are
``name`` and ``obs_date``. We can find all of the observations of the same
object on the same date as follows::

  >>> opt_xray = join(optical, xray)
  >>> print(opt_xray)
  name  obs_date  mag_b mag_v logLx
  ---- ---------- ----- ----- -----
   M82 2012-10-29  16.2  15.2  45.0

We can perform the match by ``name`` only by providing the ``keys`` argument,
which can be either a single column name or a list of column names::

  >>> print(join(optical, xray, keys='name'))
  name obs_date_1 mag_b mag_v obs_date_2 logLx
  ---- ---------- ----- ----- ---------- -----
   M31 2012-01-02  17.0  16.0 1999-01-05  43.1
   M82 2012-10-29  16.2  15.2 2012-10-29  45.0

This output table has all of the observations that have both optical and X-ray
data for an object (M31 and M82). Notice that since the ``obs_date`` column
occurs in both tables, it has been split into two columns, ``obs_date_1`` and
``obs_date_2``. The values are taken from the "left" (``optical``) and "right"
(``xray``) tables, respectively.

.. EXAMPLE END

Different Join Options
^^^^^^^^^^^^^^^^^^^^^^

The table joins so far are known as "inner" joins and represent the strict
intersection of the two tables on the key columns.

.. EXAMPLE START: Table Join Options

If you want to make a new table which has *every* row from the left table and
includes matching values from the right table when available, this is known as a
left join::

  >>> print(join(optical, xray, join_type='left'))
  name  obs_date  mag_b mag_v logLx
  ---- ---------- ----- ----- -----
  M101 2012-10-31  15.1  15.5    --
   M31 2012-01-02  17.0  16.0    --
   M82 2012-10-29  16.2  15.2  45.0

Two of the observations do not have X-ray data, as indicated by the ``--`` in
the table. You might be surprised that there is no X-ray data for M31 in the
output. Remember that the default matching key includes both ``name`` and
``obs_date``. Specifying the key as only the ``name`` column gives::

  >>> print(join(optical, xray, join_type='left', keys='name'))
  name obs_date_1 mag_b mag_v obs_date_2 logLx
  ---- ---------- ----- ----- ---------- -----
  M101 2012-10-31  15.1  15.5         --    --
   M31 2012-01-02  17.0  16.0 1999-01-05  43.1
   M82 2012-10-29  16.2  15.2 2012-10-29  45.0

Likewise you can construct a new table with every row of the right table and
matching left values (when available) using ``join_type='right'``.

To make a table with the union of rows from both tables do an "outer" join::

  >>> print(join(optical, xray, join_type='outer'))
    name   obs_date  mag_b mag_v logLx
  ------- ---------- ----- ----- -----
     M101 2012-10-31  15.1  15.5    --
      M31 1999-01-05    --    --  43.1
      M31 2012-01-02  17.0  16.0    --
      M82 2012-10-29  16.2  15.2  45.0
  NGC3516 2011-11-11    --    --  42.1

In all the above cases the output join table will be sorted by the key
column(s) and in general will not preserve the row order of the input tables.

Finally, you can do a "Cartesian" join, which is the Cartesian product of all
available rows. In this case there are no key columns (and supplying the
``keys`` argument is an error)::

  >>> print(join(optical, xray, join_type='cartesian'))
  name_1 obs_date_1 mag_b mag_v  name_2 obs_date_2 logLx
  ------ ---------- ----- ----- ------- ---------- -----
     M31 2012-01-02  17.0  16.0 NGC3516 2011-11-11  42.1
     M31 2012-01-02  17.0  16.0     M31 1999-01-05  43.1
     M31 2012-01-02  17.0  16.0     M82 2012-10-29  45.0
     M82 2012-10-29  16.2  15.2 NGC3516 2011-11-11  42.1
     M82 2012-10-29  16.2  15.2     M31 1999-01-05  43.1
     M82 2012-10-29  16.2  15.2     M82 2012-10-29  45.0
    M101 2012-10-31  15.1  15.5 NGC3516 2011-11-11  42.1
    M101 2012-10-31  15.1  15.5     M31 1999-01-05  43.1
    M101 2012-10-31  15.1  15.5     M82 2012-10-29  45.0

.. EXAMPLE END

Non-Identical Key Column Names
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. EXAMPLE START: Joining Tables with Unique Key Column Names

To use the |join| function with non-identical key column names, use the
``keys_left`` and ``keys_right`` arguments. In the following example one table
has a ``'name'`` column while the other has an ``'obj_id'`` column::

  >>> optical = Table.read("""name    obs_date    mag_b  mag_v
  ...                         M31     2012-01-02  17.0   16.0
  ...                         M82     2012-10-29  16.2   15.2
  ...                         M101    2012-10-31  15.1   15.5""", format='ascii')
  >>> xray_1 = Table.read("""obj_id    obs_date    logLx
  ...                        NGC3516 2011-11-11  42.1
  ...                        M31     1999-01-05  43.1
  ...                        M82     2012-10-29  45.0""", format='ascii')

In order to perform a match based on the names of the objects, do the
following::

  >>> print(join(optical, xray_1, keys_left='name', keys_right='obj_id'))
  name obs_date_1 mag_b mag_v obj_id obs_date_2 logLx
  ---- ---------- ----- ----- ------ ---------- -----
   M31 2012-01-02  17.0  16.0    M31 1999-01-05  43.1
   M82 2012-10-29  16.2  15.2    M82 2012-10-29  45.0

The ``keys_left`` and ``keys_right`` arguments can also take a list of column
names or even a list of column-like objects. The latter case allows specifying
the matching key column values independent of the tables being joined.

.. EXAMPLE END

Identical Key Values
^^^^^^^^^^^^^^^^^^^^

.. EXAMPLE START: Joining Tables with Identical Key Values

The |Table| join operation works even if there are multiple rows with identical
key values. For example, the following tables have multiple rows for the column
``'key'``::

  >>> from astropy.table import Table, join
  >>> left = Table([[0, 1, 1, 2], ['L1', 'L2', 'L3', 'L4']], names=('key', 'L'))
  >>> right = Table([[1, 1, 2, 4], ['R1', 'R2', 'R3', 'R4']], names=('key', 'R'))
  >>> print(left)
  key  L
  --- ---
    0  L1
    1  L2
    1  L3
    2  L4
  >>> print(right)
  key  R
  --- ---
    1  R1
    1  R2
    2  R3
    4  R4

Doing an outer join on these tables shows that what is really happening is a
`Cartesian product <https://en.wikipedia.org/wiki/Cartesian_product>`_. For
each matching key, every combination of the left and right tables is
represented. When there is no match in either the left or right table, the
corresponding column values are designated as missing::

  >>> print(join(left, right, join_type='outer'))
  key  L   R
  --- --- ---
    0  L1  --
    1  L2  R1
    1  L2  R2
    1  L3  R1
    1  L3  R2
    2  L4  R3
    4  --  R4

An inner join is the same but only returns rows where there is a key match in
both the left and right tables::

  >>> print(join(left, right, join_type='inner'))
  key  L   R
  --- --- ---
    1  L2  R1
    1  L2  R2
    1  L3  R1
    1  L3  R2
    2  L4  R3

Conflicts in the input table names are handled by the process described in the
section on `Column renaming`_. See also the sections on `Merging metadata`_ and
`Merging column attributes`_ for details on how these characteristics of the
input tables are merged in the single output table.

.. EXAMPLE END

Merging Details
---------------

When combining two or more tables there is the need to merge certain
characteristics in the inputs and potentially resolve conflicts. This
section describes the process.

Column Renaming
^^^^^^^^^^^^^^^

In cases where the input tables have conflicting column names, there
is a mechanism to generate unique output column names. There are two
keyword arguments that control the renaming behavior:

``table_names``
    List of strings that provide names for the tables being joined.
    By default this is ``['1', '2', ...]``, where the numbers correspond to
    the input tables.

``uniq_col_name``
    String format specifier with a default value of ``'{col_name}_{table_name}'``.

This is best understood by example using the ``optical`` and ``xray`` tables
in the |join| example defined previously::

  >>> print(join(optical, xray, keys='name',
  ...            table_names=['OPTICAL', 'XRAY'],
  ...            uniq_col_name='{table_name}_{col_name}'))
  name OPTICAL_obs_date mag_b mag_v XRAY_obs_date logLx
  ---- ---------------- ----- ----- ------------- -----
   M31       2012-01-02  17.0  16.0    1999-01-05  43.1
   M82       2012-10-29  16.2  15.2    2012-10-29  45.0

.. _merging_metadata:

Merging Metadata
^^^^^^^^^^^^^^^^

|Table| objects can have associated metadata:

- ``Table.meta``: table-level metadata as an ordered dictionary
- ``Column.meta``: per-column metadata as an ordered dictionary

The table operations described here handle the task of merging the metadata in
the input tables into a single output structure. Because the metadata can be
arbitrarily complex there is no unique way to do the merge. The current
implementation uses a recursive algorithm with four rules:

- :class:`dict` elements are merged by keys.
- Conflicting :class:`list` or :class:`tuple` elements are concatenated.
- Conflicting :class:`dict` elements are merged by recursively calling the
  merge function.
- Conflicting elements that are not :class:`list`, :class:`tuple`, or
  :class:`dict` will follow the following rules:

    - If both metadata values are identical, the output is set to this value.
    - If one of the conflicting metadata values is `None`, the other value is
      picked.
    - If both metadata values are different and neither is `None`, the one for
      the last table in the list is picked.

By default, a warning is emitted in the last case (both metadata values are not
`None`). The warning can be silenced or made into an exception using the
``metadata_conflicts`` argument to :func:`~astropy.table.hstack`,
:func:`~astropy.table.vstack`, or
:func:`~astropy.table.join`. The ``metadata_conflicts`` option can be set to:

- ``'silent'``  no warning is emitted, the value for the last table is silently
  picked.
- ``'warn'``  a warning is emitted, the value for the last table is picked.
- ``'error'``  an exception is raised.

The default strategies for merging metadata can be augmented or customized by
defining subclasses of the `~astropy.utils.metadata.MergeStrategy` base class.
In most cases you will also use
:func:`~astropy.utils.metadata.enable_merge_strategies` for enabling the custom
strategies. The linked documentation strings provide details.

Merging Column Attributes
^^^^^^^^^^^^^^^^^^^^^^^^^

In addition to the table and column ``meta`` attributes, the column attributes
``unit``, ``format``, and ``description`` are merged by going through the input
tables in order and taking the last value which is defined (i.e., is not
`None`).

Example
~~~~~~~

.. EXAMPLE START: Merging Column Attributes in a Table

To merge column attributes ``unit``, ``format``, or ``description``::

  >>> from astropy.table import Column, Table, vstack
  >>> col1 = Column([1], name='a')
  >>> col2 = Column([2], name='a', unit='cm')
  >>> col3 = Column([3], name='a', unit='m')
  >>> t1 = Table([col1])
  >>> t2 = Table([col2])
  >>> t3 = Table([col3])
  >>> out = vstack([t1, t2, t3])  # doctest: +SHOW_WARNINGS
  MergeConflictWarning: In merged column 'a' the 'unit' attribute does
  not match (cm != m).  Using m for merged output
  >>> out['a'].unit
  Unit("m")

The rules for merging are the same as for `Merging metadata`_, and the
``metadata_conflicts`` option also controls the merging of column attributes.

.. EXAMPLE END

.. _astropy-table-join-functions:

Joining Coordinates and Custom Join Functions
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Source catalogs that have |SkyCoord| coordinate columns can be joined using
cross-matching of the coordinates with a specified distance threshold. This is
a special case of a more general problem of "fuzzy" matching of key column
values, where instead of an exact match we require only an approximate match.
This is supported using the ``join_funcs`` argument.

.. warning::

   The coordinate and distance table joins discussed in this section are most
   applicable in the case where the relevant entries in at least one of the
   tables are all separated from one another by more than twice the join
   distance. If this is not satisfied then the join results may be unexpected.

   This is a consequence of the algorithm which effectively finds clusters of
   nearby points (an "equivalence class") and assigns a unique cluster
   identifier to each entry in both tables. This assumes the join matching
   function is a transitive relation where ``join_func(A, B)`` and
   ``join_func(B, C)`` implies ``join_func(A, C)``. With multiple matches on
   both left and right sides it is possible for the cluster of points having a
   single cluster identifier to expand in size beyond the distance threshold.

   Users should be especially aware of this issue if additional join keys
   are provided beyond the ``join_funcs``. The code does not do a "pre-join"
   on the other keys, so the possibility of having overlaps within the distance
   in both tables is higher.

Example
~~~~~~~

.. EXAMPLE START: Joining a Table on Coordinates

To join two tables on a |SkyCoord| key column we use the ``join_funcs`` keyword
to supply a :class:`dict` of functions that specify how to match a particular
key column by name. In the example below we are joining on the ``sc`` column,
so we provide the following argument::

  join_funcs={'sc': join_skycoord(0.2 * u.deg)}

This tells |join| to match the ``sc`` key column using the join function
:func:`~astropy.table.join_skycoord` with a matching distance threshold of 0.2
deg. Under the hood this calls
:meth:`~astropy.coordinates.SkyCoord.search_around_sky` or
:meth:`~astropy.coordinates.SkyCoord.search_around_3d` to do the
cross-matching. The default is to use
:meth:`~astropy.coordinates.SkyCoord.search_around_sky` (angle) matching, but
:meth:`~astropy.coordinates.SkyCoord.search_around_3d` (length or
dimensionless) is also available. This is specified using the ``distance_func``
argument of :func:`~astropy.table.join_skycoord`, which can also be a function
that matches the input and output API of
:meth:`~astropy.coordinates.SkyCoord.search_around_sky`.

Now we show the whole process:

..  doctest-requires:: scipy

  >>> from astropy.coordinates import SkyCoord
  >>> import astropy.units as u
  >>> from astropy.table import Table, join, join_skycoord

..  doctest-requires:: scipy

  >>> sc1 = SkyCoord([0, 1, 1.1, 2], [0, 0, 0, 0], unit='deg')
  >>> sc2 = SkyCoord([1.05, 0.5, 2.1], [0, 0, 0], unit='deg')

..  doctest-requires:: scipy

  >>> t1 = Table([sc1, [0, 1, 2, 3]], names=['sc', 'idx'])
  >>> t2 = Table([sc2, [0, 1, 2]], names=['sc', 'idx'])

..  doctest-requires:: scipy

  >>> t12 = join(t1, t2, keys='sc', join_funcs={'sc': join_skycoord(0.2 * u.deg)})
  >>> print(t12)
  sc_id   sc_1  idx_1   sc_2   idx_2
        deg,deg       deg,deg
  ----- ------- ----- -------- -----
      1 1.0,0.0     1 1.05,0.0     0
      1 1.1,0.0     2 1.05,0.0     0
      2 2.0,0.0     3  2.1,0.0     2

The joined table has matched the sources within 0.2 deg and created a new
column ``sc_id`` with a unique identifier for each source.

.. EXAMPLE END

You might be wondering what is happening in the join function defined above,
especially if you are interested in defining your own such function. This could
be done in order to allow fuzzy word matching of tables, for example joining
tables of people by name where the names do not always match exactly.

The first thing to note here is that the :func:`~astropy.table.join_skycoord`
function actually returns a function itself. This allows specifying a variable
match distance via a function enclosure. The requirement of the join function
is that it accepts two arguments corresponding to the two key columns, and
returns a tuple of ``(ids1, ids2)``. These identifiers correspond to the
identification of each column entry with a unique matched source.

..  doctest-requires:: scipy

    >>> join_func = join_skycoord(0.2 * u.deg)
    >>> join_func(sc1, sc2)  # Associate each coordinate with unique source ID
    (array([3, 1, 1, 2]), array([1, 4, 2]))

If you would like to write your own fuzzy matching function, we suggest starting
from the source code for :func:`~astropy.table.join_skycoord` or
:func:`~astropy.table.join_distance`.

Join on Distance
~~~~~~~~~~~~~~~~

The example above focused on joining on a |SkyCoord|, but you can also join on
a generic distance between column values using the
:func:`~astropy.table.join_distance` join function. This can apply to 1D or 2D
(vector) columns. This will look very similar to the coordinates example, but
here there is a bit more flexibility. The matching is done using
:class:`scipy.spatial.cKDTree` and
:meth:`scipy.spatial.cKDTree.query_ball_tree`, and the behavior of these can be
controlled via the ``kdtree_args`` and ``query_args`` arguments, respectively.

.. _unique-rows:

Unique Rows
-----------

Sometimes it makes sense to use only rows with unique key columns or even
fully unique rows from a table. This can be done using the above described
:meth:`~astropy.table.Table.group_by` method and ``groups`` attribute, or with
the :func:`~astropy.table.unique` convenience function. The
:func:`~astropy.table.unique` function returns a sorted table containing the
first row for each unique ``keys`` column value. If no ``keys`` is provided, it
returns a sorted table containing all of the fully unique rows.

Example
^^^^^^^

.. EXAMPLE START: Grouping Unique Rows in Tables

An example of a situation where you might want to use rows with unique key
columns is a list of objects with photometry from various observing
runs. Using ``'name'`` as the only ``keys``, it returns with the first
occurrence of each of the three targets::

  >>> from astropy import table
  >>> obs = table.Table.read("""name    obs_date    mag_b  mag_v
  ...                           M31     2012-01-02  17.0   17.5
  ...                           M82     2012-02-14  16.2   14.5
  ...                           M101    2012-01-02  15.1   13.5
  ...                           M31     2012-01-02  17.1   17.4
  ...                           M101    2012-01-02  15.1   13.5
  ...                           M82     2012-02-14  16.2   14.5
  ...                           M31     2012-02-14  16.9   17.3
  ...                           M82     2012-02-14  15.2   15.5
  ...                           M101    2012-02-14  15.0   13.6
  ...                           M82     2012-03-26  15.7   16.5
  ...                           M101    2012-03-26  15.1   13.5
  ...                           M101    2012-03-26  14.8   14.3
  ...                           """, format='ascii')
  >>> unique_by_name = table.unique(obs, keys='name')
  >>> print(unique_by_name)
  name  obs_date  mag_b mag_v
  ---- ---------- ----- -----
  M101 2012-01-02  15.1  13.5
   M31 2012-01-02  17.0  17.5
   M82 2012-02-14  16.2  14.5

Using multiple columns as ``keys``::

  >>> unique_by_name_date = table.unique(obs, keys=['name', 'obs_date'])
  >>> print(unique_by_name_date)
  name  obs_date  mag_b mag_v
  ---- ---------- ----- -----
  M101 2012-01-02  15.1  13.5
  M101 2012-02-14  15.0  13.6
  M101 2012-03-26  15.1  13.5
   M31 2012-01-02  17.0  17.5
   M31 2012-02-14  16.9  17.3
   M82 2012-02-14  16.2  14.5
   M82 2012-03-26  15.7  16.5

.. EXAMPLE END

.. _set-difference:

Set Difference
--------------

A set difference will tell you the elements that are contained in the first set
but not in the other. This concept can be applied to rows of a table by using
the :func:`~astropy.table.setdiff` function. You provide the function with two
input tables and it will return all rows in the first table which do not occur
in the second table.

The optional ``keys`` parameter specifies the names of columns that are used to
match table rows. This can be a subset of the full list of columns, but both
the first and second tables must contain all columns specified by ``keys``.
If not provided, then ``keys`` defaults to all column names in the first table.

If no different rows are found, the :func:`~astropy.table.setdiff` function
will return an empty table.

Example
^^^^^^^

.. EXAMPLE START: Using Set Difference in Tables

The example below illustrates finding the set difference of two observation
lists using a common subset of the columns in two tables.::

  >>> from astropy.table import Table, setdiff
  >>> cat_1 = Table.read("""name    obs_date    mag_b  mag_v
  ...                       M31     2012-01-02  17.0   16.0
  ...                       M82     2012-10-29  16.2   15.2
  ...                       M101    2012-10-31  15.1   15.5""", format='ascii')
  >>> cat_2 = Table.read("""   name    obs_date    logLx
  ...                          NGC3516 2011-11-11  42.1
  ...                          M31     2012-01-02  43.1
  ...                          M82     2012-10-29  45.0""", format='ascii')
  >>> sdiff = setdiff(cat_1, cat_2, keys=['name', 'obs_date'])
  >>> print(sdiff)
  name  obs_date  mag_b mag_v
  ---- ---------- ----- -----
  M101 2012-10-31  15.1  15.5

In this example there is a column in the first table that is not
present in the second table, so the ``keys`` parameter must be used to specify
the desired column names.

.. EXAMPLE END

.. _table-diff:

Table Diff
----------

To compare two tables, you can use
:func:`~astropy.utils.diff.report_diff_values`, which would produce a report
identical to :ref:`FITS diff <io-fits-differs>`.

Example
^^^^^^^

.. EXAMPLE START: Using Table Diff to Compare Tables

The example below illustrates finding the difference between two tables::

  >>> from astropy.table import Table
  >>> from astropy.utils.diff import report_diff_values
  >>> import sys
  >>> cat_1 = Table.read("""name    obs_date    mag_b  mag_v
  ...                       M31     2012-01-02  17.0   16.0
  ...                       M82     2012-10-29  16.2   15.2
  ...                       M101    2012-10-31  15.1   15.5""", format='ascii')
  >>> cat_2 = Table.read("""name    obs_date    mag_b  mag_v
  ...                       M31     2012-01-02  17.0   16.5
  ...                       M82     2012-10-29  16.2   15.2
  ...                       M101    2012-10-30  15.1   15.5
  ...                       NEW     2018-05-08   nan    9.0""", format='ascii')
  >>> identical = report_diff_values(cat_1, cat_2, fileobj=sys.stdout)
       name  obs_date  mag_b mag_v
       ---- ---------- ----- -----
    a>  M31 2012-01-02  17.0  16.0
     ?                           ^
    b>  M31 2012-01-02  17.0  16.5
     ?                           ^
        M82 2012-10-29  16.2  15.2
    a> M101 2012-10-31  15.1  15.5
     ?               ^
    b> M101 2012-10-30  15.1  15.5
     ?               ^
    b>  NEW 2018-05-08   nan   9.0
  >>> identical
  False

.. EXAMPLE END
.. |join| replace:: :func:`~astropy.table.join`

.. _mixin_columns:

Mixin Columns
*************

``astropy`` tables support the concept of "mixin columns", which
allows integration of appropriate non-|Column| based class objects within a
|Table| object. These mixin column objects are not converted in any way but are
used natively.

The available built-in mixin column classes are:

- |Quantity| and subclasses
- |SkyCoord| and coordinate frame classes
- |Time| and :class:`~astropy.time.TimeDelta`
- :class:`~astropy.coordinates.EarthLocation`
- `~astropy.table.NdarrayMixin`

Basic Example
=============

.. EXAMPLE START: Using Mixin Columns in Tables

As an example we can create a table and add a time column::

  >>> from astropy.table import Table
  >>> from astropy.time import Time
  >>> t = Table()
  >>> t['index'] = [1, 2]
  >>> t['time'] = Time(['2001-01-02T12:34:56', '2001-02-03T00:01:02'])
  >>> print(t)
  index           time
  ----- -----------------------
      1 2001-01-02T12:34:56.000
      2 2001-02-03T00:01:02.000

The important point here is that the ``time`` column is a bona fide |Time|
object::

  >>> t['time']
  <Time object: scale='utc' format='isot' value=['2001-01-02T12:34:56.000' '2001-02-03T00:01:02.000']>
  >>> t['time'].mjd  # doctest: +FLOAT_CMP
  array([51911.52425926, 51943.00071759])

.. EXAMPLE END

.. _quantity_and_qtable:

Quantity and QTable
===================

The ability to natively handle |Quantity| objects within a table makes it more
convenient to manipulate tabular data with units in a natural and robust way.
However, this feature introduces an ambiguity because data with a unit
(e.g., from a FITS binary table) can be represented as either a |Column| with a
``unit`` attribute or as a |Quantity| object. In order to cleanly resolve this
ambiguity, ``astropy`` defines a minor variant of the |Table| class called
|QTable|. The |QTable| class is exactly the same as |Table| except that
|Quantity| is the default for any data column with a defined unit.

If you take advantage of the |Quantity| infrastructure in your analysis, then
|QTable| is the preferred way to create tables with units. If instead you use
table column units more as a descriptive label, then the plain |Table| class is
probably the best class to use.

Example
-------

.. EXAMPLE START: Using Quantity Columns and QTables

To illustrate these concepts we first create a standard |Table| where we supply
as input a |Time| object and a |Quantity| object with units of ``m / s``. In
this case the quantity is converted to a |Column| (which has a ``unit``
attribute but does not have all of the features of a |Quantity|)::

  >>> import astropy.units as u
  >>> t = Table()
  >>> t['index'] = [1, 2]
  >>> t['time'] = Time(['2001-01-02T12:34:56', '2001-02-03T00:01:02'])
  >>> t['velocity'] = [3, 4] * u.m / u.s

  >>> print(t)
  index           time          velocity
                                 m / s
  ----- ----------------------- --------
      1 2001-01-02T12:34:56.000      3.0
      2 2001-02-03T00:01:02.000      4.0

  >>> type(t['velocity'])
  <class 'astropy.table.column.Column'>

  >>> t['velocity'].unit
  Unit("m / s")

  >>> (t['velocity'] ** 2).unit  # WRONG because Column is not smart about unit
  Unit("m / s")

So instead let's do the same thing using a |QTable|::

  >>> from astropy.table import QTable

  >>> qt = QTable()
  >>> qt['index'] = [1, 2]
  >>> qt['time'] = Time(['2001-01-02T12:34:56', '2001-02-03T00:01:02'])
  >>> qt['velocity'] = [3, 4] * u.m / u.s

The ``velocity`` column is now a |Quantity| and behaves accordingly::

  >>> type(qt['velocity'])
  <class 'astropy.units.quantity.Quantity'>

  >>> qt['velocity'].unit
  Unit("m / s")

  >>> (qt['velocity'] ** 2).unit  # GOOD!
  Unit("m2 / s2")

You can conveniently convert |Table| to |QTable| and vice-versa::

  >>> qt2 = QTable(t)
  >>> type(qt2['velocity'])
  <class 'astropy.units.quantity.Quantity'>

  >>> t2 = Table(qt2)
  >>> type(t2['velocity'])
  <class 'astropy.table.column.Column'>

.. Note::

   To summarize: the **only** difference between `~astropy.table.QTable` and
   `~astropy.table.Table` is the behavior when adding a column that has a
   specified unit. With `~astropy.table.QTable` such a column is always
   converted to a `~astropy.units.Quantity` object before being added to the
   table. Likewise if a unit is specified for an existing unit-less
   `~astropy.table.Column` in a `~astropy.table.QTable`, then the column is
   converted to `~astropy.units.Quantity`.

   The converse is that if you add a `~astropy.units.Quantity` column to an
   ordinary `~astropy.table.Table` then it gets converted to an ordinary
   `~astropy.table.Column` with the corresponding ``unit`` attribute.

.. attention::

   When a column of ``int`` ``dtype`` is converted to `~astropy.units.Quantity`,
   its ``dtype`` is converted to ``float``.

   For example, for a quality flag column of ``int``, if it is
   assigned with the :ref:`dimensionless unit <doc_dimensionless_unit>`, it will still
   be converted to ``float``. Therefore such columns typically should not be
   assigned with any unit.

.. EXAMPLE END

.. _mixin_attributes:

Mixin Attributes
================

The usual column attributes ``name``, ``dtype``, ``unit``, ``format``, and
``description`` are available in any mixin column via the ``info`` property::

  >>> qt['velocity'].info.name
  'velocity'

This ``info`` property is a key bit of glue that allows a non-|Column| object
to behave much like a |Column|.

The same ``info`` property is also available in standard
`~astropy.table.Column` objects. These ``info`` attributes like
``t['a'].info.name`` refer to the direct `~astropy.table.Column`
attribute (e.g., ``t['a'].name``) and can be used interchangeably.
Likewise in a `~astropy.units.Quantity` object, ``info.dtype``
attribute refers to the native ``dtype`` attribute of the object.

.. Note::

   When writing generalized code that handles column objects which
   might be mixin columns, you must *always* use the ``info``
   property to access column attributes.

.. _details_and_caveats:

Details and Caveats
===================

Most common table operations behave as expected when mixin columns are part of
the table. However, there are limitations in the current implementation.

**Adding or inserting a row**

Adding or inserting a row works as expected only for mixin classes that are
mutable (data can be changed internally) and that have an ``insert()`` method.
Adding rows to a |Table| with |Quantity|, |Time| or |SkyCoord| columns does
work.

**Masking**

Masking of mixin columns is enabled by the |Masked| class. See
:ref:`utils-masked` for details.

**High-level table operations**

Some :ref:`grouped-operations` can be used with a |QTable| with |Quantity|
columns, but performing aggregation on such a |QTable| fails::

  >>> import numpy as np
  >>> t = QTable()
  >>> t['name'] = ['foo', 'foo', 'bar']
  >>> t['a'] = np.arange(3)*u.m
  >>> t_grouped = t.group_by('name')
  >>> print(t_grouped)
  name  a
        m
  ---- ---
   bar 2.0
   foo 0.0
   foo 1.0
  >>> t_grouped.groups.aggregate(np.mean)
  Traceback (most recent call last):
  ...
  AttributeError: 'Quantity' object has no 'groups' member

**ASCII table writing**

Tables with mixin columns can be written out to file using the
`astropy.io.ascii` module, but the fast C-based writers are not available.
Instead, the pure-Python writers will be used. For writing tables with mixin
columns it is recommended to use the :ref:`ecsv_format`. This will fully
serialize the table data and metadata, allowing full "round-trip" of the table
when it is read back.

**Binary table writing**

Tables with mixin columns can be written to binary files using FITS, HDF5 and
Parquet formats. These can be read back to recover exactly the original |Table|
including mixin columns and metadata. See :ref:`table_io` for details.

.. _mixin_protocol:

Mixin Protocol
==============

A key idea behind mixin columns is that any class which satisfies a specified
protocol can be used. That means many user-defined class objects which handle
array-like data can be used natively within a |Table|. The protocol is
relatively concise and requires that a class behave like a minimal ``numpy``
array with the following properties:

- Contains array-like data.
- Implements ``__getitem__()`` to support getting data as a
  single item, slicing, or index array access.
- Has a ``shape`` attribute.
- Has a ``__len__()`` method for length.
- Has an ``info`` class descriptor which is a subclass of the
  :class:`astropy.utils.data_info.MixinInfo` class.

The `Example: ArrayWrapper`_ section shows a minimal working example of a class
which can be used as a mixin column. A :class:`pandas.Series` object can
function as a mixin column as well.

Other interesting possibilities for mixin columns include:

- Columns which are dynamically computed as a function of other columns (AKA
  spreadsheet).
- Columns which are themselves a |Table| (i.e., nested tables). A `proof of
  concept <https://github.com/astropy/astropy/pull/3963>`_ is available.

new_like() method
-----------------

In order to support high-level operations like :func:`~astropy.table.join` and
:func:`~astropy.table.vstack`, a mixin class must provide a ``new_like()``
method in the ``info`` class descriptor. A key part of the functionality is to
ensure that the input column metadata are merged appropriately and that the
columns have consistent properties such as the shape.

A mixin class that provides ``new_like()`` must also implement
``__setitem__()`` to support setting via a single item, slicing, or index
array.

The ``new_like()`` method has the following signature::

    def new_like(self, cols, length, metadata_conflicts='warn', name=None):
        """
        Return a new instance of this class which is consistent with the
        input ``cols`` and has ``length`` rows.

        This is intended for creating an empty column object whose elements can
        be set in-place for table operations like join or vstack.

        Parameters
        ----------
        cols : list
            List of input columns
        length : int
            Length of the output column object
        metadata_conflicts : {'warn', 'error', 'silent'}
            How to handle metadata conflicts
        name : str
            Output column name

        Returns
        -------
        col : object
            New instance of this class consistent with ``cols``
        """

Examples of this are found in the `~astropy.table.column.ColumnInfo` and
`~astropy.units.quantity.QuantityInfo` classes.


.. _arraywrapper_example:

Example: ArrayWrapper
=====================

The code listing below shows an example of a data container class which acts as
a mixin column class. This class is a wrapper around a |ndarray|. It is used in
the ``astropy`` mixin test suite and is fully compliant as a mixin column.

::

  from astropy.utils.data_info import ParentDtypeInfo

  class ArrayWrapper(object):
      """
      Minimal mixin using a simple wrapper around a numpy array
      """
      info = ParentDtypeInfo()

      def __init__(self, data):
          self.data = np.array(data)
          if 'info' in getattr(data, '__dict__', ()):
              self.info = data.info

      def __getitem__(self, item):
          if isinstance(item, (int, np.integer)):
              out = self.data[item]
          else:
              out = self.__class__(self.data[item])
              if 'info' in self.__dict__:
                  out.info = self.info
          return out

      def __setitem__(self, item, value):
          self.data[item] = value

      def __len__(self):
          return len(self.data)

      @property
      def dtype(self):
          return self.data.dtype

      @property
      def shape(self):
          return self.data.shape

      def __repr__(self):
          return f"<{self.__class__.__name__} name='{self.info.name}' data={self.data}>"

.. _table_mixin_registry:

Registering array-like objects as mixin columns
===============================================

In some cases, you may want to directly add an array-like
object as a table column while maintaining the original object properties
(instead of the default conversion of the object to a `~astropy.table.Column`).
This is done by registering the object class as a mixin column and
defining a handler which allows `~astropy.table.Table` to treat that object
class as a mixin similar to the built-in mixin columns such as `~astropy.time.Time`
or `~astropy.units.quantity.Quantity`.

This can be done for data classes that are defined in third-party packages and which
you have no control over. As an example, we define a class
that is not numpy-like and stores the data in a private attribute::

    >>> class ExampleDataClass:
    ...     def __init__(self):
    ...         self._data = np.array([0, 1, 3, 4], dtype=float)

By default, this cannot be used as a table column::

    >>> t = Table()
    >>> t['data'] = ExampleDataClass()
    Traceback (most recent call last):
    ...
    TypeError: Empty table cannot have column set to scalar value

However, you can create a function (or 'handler') which takes
an instance of the data class you want to have automatically
handled and returns a mixin column::

    >>> from astropy.table.table_helpers import ArrayWrapper
    >>> def handle_example_data_class(obj):
    ...     return ArrayWrapper(obj._data)

You can then register this by providing the fully qualified name
of the class and the handler function::

    >>> from astropy.table.mixins.registry import register_mixin_handler
    >>> register_mixin_handler('__main__.ExampleDataClass', handle_example_data_class)
    >>> t['data'] = ExampleDataClass()
    >>> t
    <Table length=4>
      data
    float64
    -------
        0.0
        1.0
        3.0
        4.0

Because we defined the data class as part of the example
above, the fully qualified name starts with ``__main__``,
but for a class in a third-party package, this might look
like ``package.Class`` for example.

.. _table_implementation_details:

Table Implementation Details
*****************************

This page provides a brief overview of the |Table| class implementation, in
particular highlighting the internal data storage architecture. This is aimed
at developers and/or users who are interested in optimal use of the |Table|
class.

The image below illustrates the basic architecture of the |Table| class.
The fundamental data container is an ordered dictionary of individual column
objects maintained as the ``columns`` attribute. It is via this container
that columns are managed and accessed.

.. image:: table_architecture.png
   :width: 45%

Each |Column| (or |MaskedColumn|) object is an |ndarray| (or
:class:`numpy.ma.MaskedArray`) subclass and is the sole owner of its data.
Maintaining the table as separate columns simplifies table management
considerably. It also makes operations like adding or removing columns much
faster in comparison to implementations using a ``numpy`` structured array
container.

As shown below, a |Row| object corresponds to a single row in the table. The
|Row| object does not create a view of the full row at any point. Instead it
manages access (e.g., ``row['a']``) dynamically by referencing the appropriate
elements of the parent table.

.. image:: table_row.png
   :width: 83%

In some cases it is desirable to have a static copy of the full row. This is
available via the `~astropy.table.Row.as_void()` method, which creates and
returns a :class:`numpy.void` or ``numpy.ma.mvoid`` object with a copy of the
original data.
.. _access_table:

Accessing a Table
*****************

Accessing table properties and data is generally consistent with the basic
interface for ``numpy`` `structured arrays
<https://numpy.org/doc/stable/user/basics.rec.html>`_.

Basics
======

For a quick overview, the code below shows the basics of accessing table data.
Where relevant, there is a comment about what sort of object is returned.
Except where noted, table access returns objects that can be modified in order
to update the original table data or properties. See also the section on
:ref:`copy_versus_reference` to learn more about this topic.

**Make a table**
::

  from astropy.table import Table
  import numpy as np

  arr = np.arange(15).reshape(5, 3)
  t = Table(arr, names=('a', 'b', 'c'), meta={'keywords': {'key1': 'val1'}})

**Table properties**
::

  t.columns   # Dict of table columns (access by column name, index, or slice)
  t.colnames  # List of column names
  t.meta      # Dict of meta-data
  len(t)      # Number of table rows

**Access table data**
::

  t['a']       # Column 'a'
  t['a'][1]    # Row 1 of column 'a'
  t[1]         # Row 1
  t[1]['a']    # Column 'a' of row 1
  t[2:5]       # Table object with rows 2:5
  t[[1, 3, 4]]  # Table object with rows 1, 3, 4 (copy)
  t[np.array([1, 3, 4])]  # Table object with rows 1, 3, 4 (copy)
  t[[]]        # Same table definition but with no rows of data
  t['a', 'c']  # Table with cols 'a', 'c' (copy)
  dat = np.array(t)  # Copy table data to numpy structured array object
  t['a'].quantity  # an astropy.units.Quantity for Column 'a'
  t['a'].to('km')  # an astropy.units.Quantity for Column 'a' in units of kilometers
  t.columns[1]  # Column 1 (which is the 'b' column)
  t.columns[0:2]  # New table with columns 0 and 1

.. Note::
   Although they appear nearly equivalent, there is a factor of two performance
   difference between ``t[1]['a']`` (slower, because an intermediate |Row|
   object gets created) versus ``t['a'][1]`` (faster). Always use the latter
   when possible.

**Print table or column**
::

  print(t)     # Print formatted version of table to the screen
  t.pprint()   # Same as above
  t.pprint(show_unit=True)  # Show column unit
  t.pprint(show_name=False)  # Do not show column names
  t.pprint_all() # Print full table no matter how long / wide it is (same as t.pprint(max_lines=-1, max_width=-1))

  t.more()  # Interactively scroll through table like Unix "more"

  print(t['a'])    # Formatted column values
  t['a'].pprint()  # Same as above, with same options as Table.pprint()
  t['a'].more()    # Interactively scroll through column
  t['a', 'c'].pprint()  # Print columns 'a' and 'c' of table

  lines = t.pformat()  # Formatted table as a list of lines (same options as pprint)
  lines = t['a'].pformat()  # Formatted column values as a list


Details
=======

For all of the following examples it is assumed that the table has been created
as follows::

  >>> from astropy.table import Table, Column
  >>> import numpy as np
  >>> import astropy.units as u

  >>> arr = np.arange(15, dtype=np.int32).reshape(5, 3)
  >>> t = Table(arr, names=('a', 'b', 'c'), meta={'keywords': {'key1': 'val1'}})
  >>> t['a'].format = "{:.3f}"  # print with 3 digits after decimal point
  >>> t['a'].unit = 'm sec^-1'
  >>> t['a'].description = 'unladen swallow velocity'
  >>> print(t)
       a      b   c
    m sec^-1
    -------- --- ---
       0.000   1   2
       3.000   4   5
       6.000   7   8
       9.000  10  11
      12.000  13  14

.. Note::

   In the example above the ``format``, ``unit``, and ``description``
   attributes of the |Column| were set directly. For :ref:`mixin_columns` like
   |Quantity| you must set via the ``info`` attribute, for example,
   ``t['a'].info.format = "{:.3f}"``. You can use the ``info`` attribute with
   |Column| objects as well, so the general solution that works with any table
   column is to set via the ``info`` attribute. See :ref:`mixin_attributes` for
   more information.

.. _table-summary-information:

Summary Information
-------------------

You can get summary information about the table as follows::

  >>> t.info
  <Table length=5>
  name dtype   unit   format       description
  ---- ----- -------- ------ ------------------------
     a int32 m sec^-1 {:.3f} unladen swallow velocity
     b int32
     c int32

If called as a function then you can supply an ``option`` that specifies
the type of information to return. The built-in ``option`` choices are
``'attributes'`` (column attributes, which is the default) or ``'stats'``
(basic column statistics). The ``option`` argument can also be a list
of available options::

  >>> t.info('stats')  # doctest: +FLOAT_CMP
  <Table length=5>
  name mean   std   min max
  ---- ---- ------- --- ---
     a    6 4.24264   0  12
     b    7 4.24264   1  13
     c    8 4.24264   2  14

  >>> t.info(['attributes', 'stats'])  # doctest: +FLOAT_CMP
  <Table length=5>
  name dtype   unit   format       description        mean   std   min max
  ---- ----- -------- ------ ------------------------ ---- ------- --- ---
     a int32 m sec^-1 {:.3f} unladen swallow velocity    6 4.24264   0  12
     b int32                                             7 4.24264   1  13
     c int32                                             8 4.24264   2  14

Columns also have an ``info`` property that has the same behavior and
arguments, but provides information about a single column::

  >>> t['a'].info
  name = a
  dtype = int32
  unit = m sec^-1
  format = {:.3f}
  description = unladen swallow velocity
  class = Column
  n_bad = 0
  length = 5

  >>> t['a'].info('stats')  # doctest: +FLOAT_CMP
  name = a
  mean = 6
  std = 4.24264
  min = 0
  max = 12
  n_bad = 0
  length = 5


Accessing Properties
--------------------

The code below shows accessing the table columns as a |TableColumns| object,
getting the column names, table metadata, and number of table rows. The table
metadata is an `~collections.OrderedDict` by default.
::

  >>> t.columns
  <TableColumns names=('a','b','c')>

  >>> t.colnames
  ['a', 'b', 'c']

  >>> t.meta  # Dict of meta-data
  {'keywords': {'key1': 'val1'}}

  >>> len(t)
  5


Accessing Data
--------------

As expected you can access a table column by name and get an element from that
column with a numerical index::

  >>> t['a']  # Column 'a'
  <Column name='a' dtype='int32' unit='m sec^-1' format='{:.3f}' description='unladen swallow velocity' length=5>
   0.000
   3.000
   6.000
   9.000
  12.000


  >>> t['a'][1]  # Row 1 of column 'a'
  3

When a table column is printed, it is formatted according to the ``format``
attribute (see :ref:`table_format_string`). Note the difference between the
column representation above and how it appears via ``print()`` or ``str()``::

  >>> print(t['a'])
     a
  m sec^-1
  --------
     0.000
     3.000
     6.000
     9.000
    12.000

Likewise a table row and a column from that row can be selected::

  >>> t[1]  # Row object corresponding to row 1
  <Row index=1>
     a       b     c
  m sec^-1
   int32   int32 int32
  -------- ----- -----
     3.000     4     5

  >>> t[1]['a']  # Column 'a' of row 1
  3

A |Row| object has the same columns and metadata as its parent table::

  >>> t[1].columns
  <TableColumns names=('a','b','c')>

  >>> t[1].meta
  {'keywords': {'key1': 'val1'}}

Slicing a table returns a new table object with references to the original
data within the slice region (See :ref:`copy_versus_reference`). The table
metadata and column definitions are copied.
::

  >>> t[2:5]  # Table object with rows 2:5 (reference)
  <Table length=3>
     a       b     c
  m sec^-1
   int32   int32 int32
  -------- ----- -----
     6.000     7     8
     9.000    10    11
    12.000    13    14

It is possible to select table rows with an array of indexes or by specifying
multiple column names. This returns a copy of the original table for the
selected rows or columns.  ::

  >>> print(t[[1, 3, 4]])  # Table object with rows 1, 3, 4 (copy)
       a      b   c
    m sec^-1
    -------- --- ---
       3.000   4   5
       9.000  10  11
      12.000  13  14


  >>> print(t[np.array([1, 3, 4])])  # Table object with rows 1, 3, 4 (copy)
       a      b   c
    m sec^-1
    -------- --- ---
       3.000   4   5
       9.000  10  11
      12.000  13  14


  >>> print(t['a', 'c'])  # or t[['a', 'c']] or t[('a', 'c')]
  ...                     # Table with cols 'a', 'c' (copy)
       a      c
    m sec^-1
    -------- ---
       0.000   2
       3.000   5
       6.000   8
       9.000  11
      12.000  14

We can select rows from a table using conditionals to create boolean masks. A
table indexed with a boolean array will only return rows where the mask array
element is `True`. Different conditionals can be combined using the bitwise
operators.  ::

  >>> mask = (t['a'] > 4) & (t['b'] > 8)  # Table rows where column a > 4
  >>> print(t[mask])                      # and b > 8
  ...
       a      b   c
    m sec^-1
    -------- --- ---
       9.000  10  11
      12.000  13  14

Finally, you can access the underlying table data as a native ``numpy``
structured array by creating a copy or reference with :func:`numpy.array`::

  >>> data = np.array(t)  # copy of data in t as a structured array
  >>> data = np.array(t, copy=False)  # reference to data in t


Table Equality
--------------

We can check table data equality using two different methods:

- The ``==`` comparison operator. This returns a `True` or `False` for
  each row if the *entire row* matches. This is the same as the behavior of
  ``numpy`` structured arrays.
- Table :meth:`~astropy.table.Table.values_equal` to compare table values
  element-wise. This returns a boolean `True` or `False` for each table
  *element*, so you get a `~astropy.table.Table` of values.

Examples
^^^^^^^^

.. EXAMPLE START: Checking Table Equality

To check table equality::

  >>> t1 = Table(rows=[[1, 2, 3],
  ...                  [4, 5, 6],
  ...                  [7, 7, 9]], names=['a', 'b', 'c'])
  >>> t2 = Table(rows=[[1, 2, -1],
  ...                  [4, -1, 6],
  ...                  [7, 7, 9]], names=['a', 'b', 'c'])

  >>> t1 == t2
  array([False, False,  True])

  >>> t1.values_equal(t2)  # Compare to another table
  <Table length=3>
   a     b     c
  bool  bool  bool
  ---- ----- -----
  True  True False
  True False  True
  True  True  True

  >>> t1.values_equal([2, 4, 7])  # Compare to an array column-wise
  <Table length=3>
    a     b     c
   bool  bool  bool
  ----- ----- -----
  False  True False
   True False False
   True  True False

  >>> t1.values_equal(7)  # Compare to a scalar column-wise
  <Table length=3>
    a     b     c
   bool  bool  bool
  ----- ----- -----
  False False False
  False False False
   True  True False

.. EXAMPLE END

Formatted Printing
------------------

The values in a table or column can be printed or retrieved as a formatted
table using one of several methods:

- `print()` function.
- `Table.more() <astropy.table.Table.more>` or `Column.more()
  <astropy.table.Column.more>` methods to interactively scroll through
  table values.
- `Table.pprint() <astropy.table.Table.pprint>` or `Column.pprint()
  <astropy.table.Column.pprint>` methods to print a formatted version of
  the table to the screen.
- `Table.pformat() <astropy.table.Table.pformat>` or `Column.pformat()
  <astropy.table.Column.pformat>` methods to return the formatted table
  or column as a list of fixed-width strings. This could be used as a quick way
  to save a table.

These methods use :ref:`table_format_string`
if available and strive to make the output readable.
By default, table and column printing will
not print the table larger than the available interactive screen size. If the
screen size cannot be determined (in a non-interactive environment or on
Windows) then a default size of 25 rows by 80 columns is used. If a table is
too large, then rows and/or columns are cut from the middle so it fits.

Example
^^^^^^^

.. EXAMPLE START: Printing Formatted Tables

To print a formatted table::

  >>> arr = np.arange(3000).reshape(100, 30)  # 100 rows x 30 columns array
  >>> t = Table(arr)
  >>> print(t)
  col0 col1 col2 col3 col4 col5 col6 ... col23 col24 col25 col26 col27 col28 col29
  ---- ---- ---- ---- ---- ---- ---- ... ----- ----- ----- ----- ----- ----- -----
     0    1    2    3    4    5    6 ...    23    24    25    26    27    28    29
    30   31   32   33   34   35   36 ...    53    54    55    56    57    58    59
    60   61   62   63   64   65   66 ...    83    84    85    86    87    88    89
    90   91   92   93   94   95   96 ...   113   114   115   116   117   118   119
   120  121  122  123  124  125  126 ...   143   144   145   146   147   148   149
   150  151  152  153  154  155  156 ...   173   174   175   176   177   178   179
   180  181  182  183  184  185  186 ...   203   204   205   206   207   208   209
   210  211  212  213  214  215  216 ...   233   234   235   236   237   238   239
   240  241  242  243  244  245  246 ...   263   264   265   266   267   268   269
   270  271  272  273  274  275  276 ...   293   294   295   296   297   298   299
   ...  ...  ...  ...  ...  ...  ... ...   ...   ...   ...   ...   ...   ...   ...
  2670 2671 2672 2673 2674 2675 2676 ...  2693  2694  2695  2696  2697  2698  2699
  2700 2701 2702 2703 2704 2705 2706 ...  2723  2724  2725  2726  2727  2728  2729
  2730 2731 2732 2733 2734 2735 2736 ...  2753  2754  2755  2756  2757  2758  2759
  2760 2761 2762 2763 2764 2765 2766 ...  2783  2784  2785  2786  2787  2788  2789
  2790 2791 2792 2793 2794 2795 2796 ...  2813  2814  2815  2816  2817  2818  2819
  2820 2821 2822 2823 2824 2825 2826 ...  2843  2844  2845  2846  2847  2848  2849
  2850 2851 2852 2853 2854 2855 2856 ...  2873  2874  2875  2876  2877  2878  2879
  2880 2881 2882 2883 2884 2885 2886 ...  2903  2904  2905  2906  2907  2908  2909
  2910 2911 2912 2913 2914 2915 2916 ...  2933  2934  2935  2936  2937  2938  2939
  2940 2941 2942 2943 2944 2945 2946 ...  2963  2964  2965  2966  2967  2968  2969
  2970 2971 2972 2973 2974 2975 2976 ...  2993  2994  2995  2996  2997  2998  2999
  Length = 100 rows

.. EXAMPLE END

more() method
^^^^^^^^^^^^^

In order to browse all rows of a table or column use the `Table.more()
<astropy.table.Table.more>` or `Column.more() <astropy.table.Column.more>`
methods. These let you interactively scroll through the rows much like the Unix
``more`` command. Once part of the table or column is displayed the supported
navigation keys are:

|  **f, space** : forward one page
|  **b** : back one page
|  **r** : refresh same page
|  **n** : next row
|  **p** : previous row
|  **<** : go to beginning
|  **>** : go to end
|  **q** : quit browsing
|  **h** : print this help

pprint() method
^^^^^^^^^^^^^^^

In order to fully control the print output use the `Table.pprint()
<astropy.table.Table.pprint>` or `Column.pprint()
<astropy.table.Column.pprint>` methods. These have keyword arguments
``max_lines``, ``max_width``, ``show_name``, and ``show_unit``, with meanings
as shown below::

  >>> arr = np.arange(3000, dtype=float).reshape(100, 30)
  >>> t = Table(arr)
  >>> t['col0'].format = '%e'
  >>> t['col0'].unit = 'km**2'
  >>> t['col29'].unit = 'kg sec m**-2'

  >>> t.pprint(max_lines=8, max_width=40)
      col0     ...    col29
      km2      ... kg sec m**-2
  ------------ ... ------------
  0.000000e+00 ...         29.0
           ... ...          ...
  2.940000e+03 ...       2969.0
  2.970000e+03 ...       2999.0
  Length = 100 rows

  >>> t.pprint(max_lines=8, max_width=40, show_unit=False)
      col0     ... col29
  ------------ ... ------
  0.000000e+00 ...   29.0
           ... ...    ...
  2.940000e+03 ... 2969.0
  2.970000e+03 ... 2999.0
  Length = 100 rows

  >>> t.pprint(max_lines=8, max_width=40, show_name=False)
      km2      ... kg sec m**-2
  ------------ ... ------------
  0.000000e+00 ...         29.0
  3.000000e+01 ...         59.0
           ... ...          ...
  2.940000e+03 ...       2969.0
  2.970000e+03 ...       2999.0
  Length = 100 rows

In order to force printing all values regardless of the output length or width
use :meth:`~astropy.table.Table.pprint_all`, which is equivalent to setting
``max_lines`` and ``max_width`` to ``-1`` in :meth:`~astropy.table.Table.pprint`.
:meth:`~astropy.table.Table.pprint_all` takes the same arguments as :meth:`~astropy.table.Table.pprint`.
For the wide table in this example you see six lines of wrapped output like the
following::

  >>> t.pprint_all(max_lines=8)  # doctest: +SKIP
      col0         col1     col2   col3   col4   col5   col6   col7   col8   col9  col10  col11  col12  col13  col14  col15  col16  col17  col18  col19  col20  col21  col22  col23  col24  col25  col26  col27  col28     col29
      km2                                                                                                                                                                                                               kg sec m**-2
  ------------ ----------- ------ ------ ------ ------ ------ ------ ------ ------ ------ ------ ------ ------ ------ ------ ------ ------ ------ ------ ------ ------ ------ ------ ------ ------ ------ ------ ------ ------------
  0.000000e+00    1.000000    2.0    3.0    4.0    5.0    6.0    7.0    8.0    9.0   10.0   11.0   12.0   13.0   14.0   15.0   16.0   17.0   18.0   19.0   20.0   21.0   22.0   23.0   24.0   25.0   26.0   27.0   28.0         29.0
           ...         ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...          ...
  2.940000e+03 2941.000000 2942.0 2943.0 2944.0 2945.0 2946.0 2947.0 2948.0 2949.0 2950.0 2951.0 2952.0 2953.0 2954.0 2955.0 2956.0 2957.0 2958.0 2959.0 2960.0 2961.0 2962.0 2963.0 2964.0 2965.0 2966.0 2967.0 2968.0       2969.0
  2.970000e+03 2971.000000 2972.0 2973.0 2974.0 2975.0 2976.0 2977.0 2978.0 2979.0 2980.0 2981.0 2982.0 2983.0 2984.0 2985.0 2986.0 2987.0 2988.0 2989.0 2990.0 2991.0 2992.0 2993.0 2994.0 2995.0 2996.0 2997.0 2998.0       2999.0
  Length = 100 rows

For columns, the syntax and behavior of :func:`~astropy.table.Column.pprint` is
the same except that there is no ``max_width`` keyword argument::

  >>> t['col3'].pprint(max_lines=8)
   col3
  ------
     3.0
    33.0
     ...
  2943.0
  2973.0
  Length = 100 rows

Column alignment
^^^^^^^^^^^^^^^^

Individual columns have the ability to be aligned in a number of different
ways for an enhanced viewing experience::

  >>> t1 = Table()
  >>> t1['long column name 1'] = [1, 2, 3]
  >>> t1['long column name 2'] = [4, 5, 6]
  >>> t1['long column name 3'] = [7, 8, 9]
  >>> t1['long column name 4'] = [700000, 800000, 900000]
  >>> t1['long column name 2'].info.format = '<'
  >>> t1['long column name 3'].info.format = '0='
  >>> t1['long column name 4'].info.format = '^'
  >>> t1.pprint()
   long column name 1 long column name 2 long column name 3 long column name 4
  ------------------ ------------------ ------------------ ------------------
                   1 4                  000000000000000007       700000
                   2 5                  000000000000000008       800000
                   3 6                  000000000000000009       900000

Conveniently, alignment can be handled another way  by passing a list to the
keyword argument ``align``::

  >>> t1 = Table()
  >>> t1['column1'] = [1, 2, 3]
  >>> t1['column2'] = [2, 4, 6]
  >>> t1.pprint(align=['<', '0='])
  column1 column2
  ------- -------
  1       0000002
  2       0000004
  3       0000006

It is also possible to set the alignment of all columns with a single
string value::

  >>> t1.pprint(align='^')
  column1 column2
  ------- -------
     1       2
     2       4
     3       6

The fill character for justification can be set as a prefix to the
alignment character (see `Format Specification Mini-Language
<https://docs.python.org/3/library/string.html#format-specification-mini-language>`_
for additional explanation). This can be done both in the ``align`` argument
and in the column ``format`` attribute. Note the interesting interaction below::

  >>> t1 = Table([[1.0, 2.0], [1, 2]], names=['column1', 'column2'])

  >>> t1['column1'].format = '#^.2f'
  >>> t1.pprint()
  column1 column2
  ------- -------
  ##1.00#       1
  ##2.00#       2

Now if we set a global align, it seems like our original column format
got lost::

  >>> t1.pprint(align='!<')
  column1 column2
  ------- -------
  1.00!!! 1!!!!!!
  2.00!!! 2!!!!!!

The way to avoid this is to explicitly specify the alignment strings
for every column and use `None` where the column format should be
used::

  >>> t1.pprint(align=[None, '!<'])
  column1 column2
  ------- -------
  ##1.00# 1!!!!!!
  ##2.00# 2!!!!!!

pformat() method
^^^^^^^^^^^^^^^^

In order to get the formatted output for manipulation or writing to a file use
the `Table.pformat() <astropy.table.Table.pformat>` or `Column.pformat()
<astropy.table.Column.pformat>` methods. These behave just as for
:meth:`~astropy.table.Table.pprint` but return a list corresponding to each
formatted line in the :meth:`~astropy.table.Table.pprint` output. The
:meth:`~astropy.table.Table.pformat_all` method can be used to return a list
for all lines in the |Table|.

  >>> lines = t['col3'].pformat(max_lines=8)

Hiding columns
^^^^^^^^^^^^^^

The |Table| class has functionality to selectively show or hide certain columns
within the table when using any of the print methods. This can be useful for
columns that are very wide or else "uninteresting" for various reasons. The
specification of which columns are outputted is associated with the table itself
so that it persists through slicing, copying, and serialization (e.g. saving to
:ref:`ecsv_format`). One use case is for specialized table subclasses that
contain auxiliary columns that are not typically useful to the user.

The specification of which columns to include when printing is handled through
two complementary |Table| attributes:

- `~astropy.table.Table.pprint_include_names`: column names to include, where
  the default value of `None` implies including all columns.
- `~astropy.table.Table.pprint_exclude_names`: column names to exclude, where
  the default value of `None` implies excluding no columns.

Typically you should use just one of the two attributes at a time. However,
both can be set at once and the set of columns that actually gets printed
is conceptually expressed in this pseudo-code::

  include_names = (set(table.pprint_include_names() or table.colnames)
                   - set(table.pprint_exclude_names() or ())

Examples
""""""""
Let's start with defining a simple table with one row and six columns::

  >>> from astropy.table.table_helpers import simple_table
  >>> t = simple_table(size=1, cols=6)
  >>> print(t)
  a   b   c   d   e   f
  --- --- --- --- --- ---
  1 1.0   c   4 4.0   f

Now you can get the value of the ``pprint_include_names`` attribute by calling
it as a function, and then include some names for printing::

  >>> print(t.pprint_include_names())
  None
  >>> t.pprint_include_names = ('a', 'c', 'e')
  >>> print(t.pprint_include_names())
  ('a', 'c', 'e')
  >>> print(t)
   a   c   e
  --- --- ---
    1   c 4.0

Now you can instead exclude some columns from printing. Note that for both
include and exclude, you can add column names that do not exist in the table.
This allows pre-defining the attributes before the table has been fully
constructed.
::

  >>> t.pprint_include_names = None  # Revert to printing all columns
  >>> t.pprint_exclude_names = ('a', 'c', 'e', 'does-not-exist')
  >>> print(t)
   b   d   f
  --- --- ---
  1.0   4   f

Next you can ``add`` or ``remove`` names from the attribute::

  >>> t = simple_table(size=1, cols=6)  # Start with a fresh table
  >>> t.pprint_exclude_names.add('b')  # Single name
  >>> t.pprint_exclude_names.add(['d', 'f'])  # List or tuple of names
  >>> t.pprint_exclude_names.remove('f')  # Single name or list/tuple of names
  >>> t.pprint_exclude_names()
  ('b', 'd')

Finally, you can temporarily set the attributes within a `context manager
<https://docs.python.org/3/reference/datamodel.html#context-managers>`_. For
example::

  >>> t = simple_table(size=1, cols=6)
  >>> t.pprint_include_names = ('a', 'b')
  >>> print(t)
   a   b
  --- ---
    1 1.0

  >>> # Show all (for pprint_include_names the value of None => all columns)
  >>> with t.pprint_include_names.set(None):
  ...     print(t)
   a   b   c   d   e   f
  --- --- --- --- --- ---
    1 1.0   c   4 4.0   f

The specification of names for these attributes can include Unix-style globs
like ``*`` and ``?``. See `fnmatch` for details (and in particular how to
escape those characters if needed). For example::

  >>> t = Table()
  >>> t.pprint_exclude_names = ['boring*']
  >>> t['a'] = [1]
  >>> t['b'] = ['b']
  >>> t['boring_ra'] = [122.0]
  >>> t['boring_dec'] = [89.9]
  >>> print(t)
   a   b
  --- ---
    1   b

Multidimensional columns
^^^^^^^^^^^^^^^^^^^^^^^^

If a column has more than one dimension then each element of the column is
itself an array. In the example below there are three rows, each of which is a
``2 x 2`` array. The formatted output for such a column shows only the first
and last value of each row element and indicates the array dimensions in the
column name header::

  >>> t = Table()
  >>> arr = [ np.array([[ 1,  2],
  ...                   [10, 20]]),
  ...         np.array([[ 3,  4],
  ...                   [30, 40]]),
  ...         np.array([[ 5,  6],
  ...                   [50, 60]]) ]
  >>> t['a'] = arr
  >>> t['a'].shape
  (3, 2, 2)
  >>> t.pprint()
  a [2,2]
  -------
  1 .. 20
  3 .. 40
  5 .. 60

In order to see all of the data values for a multidimensional column use the
column representation. This uses the standard ``numpy`` mechanism for printing
any array::

  >>> t['a'].data
  array([[[ 1,  2],
          [10, 20]],
         [[ 3,  4],
          [30, 40]],
         [[ 5,  6],
          [50, 60]]])

.. _columns_with_units:

Columns with Units
^^^^^^^^^^^^^^^^^^

.. note::

  |Table| and |QTable| instances handle entries with units differently. The
  following describes |Table|. :ref:`quantity_and_qtable` explains how a
  |QTable| differs from a |Table|.

A |Column| object with units within a standard |Table| has certain
quantity-related conveniences available. To begin with, it can be converted
explicitly to a |Quantity| object via the
:attr:`~astropy.table.Column.quantity` property and the
:meth:`~astropy.table.Column.to` method::

  >>> data = [[1., 2., 3.], [40000., 50000., 60000.]]
  >>> t = Table(data, names=('a', 'b'))
  >>> t['a'].unit = u.m
  >>> t['b'].unit = 'km/s'
  >>> t['a'].quantity  # doctest: +FLOAT_CMP
  <Quantity [1., 2., 3.] m>
  >>> t['b'].to(u.kpc/u.Myr)  # doctest: +FLOAT_CMP
  <Quantity [40.9084866 , 51.13560825, 61.3627299 ] kpc / Myr>

Note that the :attr:`~astropy.table.Column.quantity` property is actually
a *view* of the data in the column, not a copy. Hence, you can set the
values of a column in a way that respects units by making in-place
changes to the :attr:`~astropy.table.Column.quantity` property::

  >>> t['b']
  <Column name='b' dtype='float64' unit='km / s' length=3>
  40000.0
  50000.0
  60000.0

  >>> t['b'].quantity[0] = 45000000*u.m/u.s
  >>> t['b']
  <Column name='b' dtype='float64' unit='km / s' length=3>
  45000.0
  50000.0
  60000.0

Even without explicit conversion, columns with units can be treated like a
|Quantity| in *some* arithmetic expressions (see the warning below for caveats
to this)::

  >>> t['a'] + .005*u.km  # doctest: +FLOAT_CMP
  <Quantity [6., 7., 8.] m>
  >>> from astropy.constants import c
  >>> (t['b'] / c).decompose()  # doctest: +FLOAT_CMP
  <Quantity [0.15010384, 0.16678205, 0.20013846]>

.. warning::

  |Table| columns do *not* always behave the same as |Quantity|. |Table|
  columns act more like regular ``numpy`` arrays unless either explicitly
  converted to a |Quantity| or combined with a |Quantity| using an arithmetic
  operator. For example, the following does not work in the way you would
  expect::

    >>> data = [[30, 90]]
    >>> t = Table(data, names=('angle',))
    >>> t['angle'].unit = 'deg'
    >>> np.sin(t['angle'])  # doctest: +FLOAT_CMP
    <Column name='angle' dtype='float64' unit='deg' length=2>
    -0.988031624093
     0.893996663601

  This is wrong both in that it says the result is in degrees, *and*
  `~numpy.sin` treated the values as radians rather than degrees. If at all in
  doubt that you will get the right result, the safest choice is to either use
  |QTable| or to explicitly convert to |Quantity|::

    >>> np.sin(t['angle'].quantity)  # doctest: +FLOAT_CMP
    <Quantity [0.5, 1. ]>

.. _bytestring-columns-python-3:

Bytestring Columns
^^^^^^^^^^^^^^^^^^

Using bytestring columns (``numpy`` ``'S'`` dtype) is possible
with ``astropy`` tables since they can be compared with the natural
Python string (``str``) type. See `The bytes/str dichotomy in Python 3
<https://eli.thegreenplace.net/2012/01/30/the-bytesstr-dichotomy-in-python-3>`_
for a very brief overview of the difference.

The standard method of representing strings in ``numpy`` is via the
unicode ``'U'`` dtype. The problem is that this requires 4 bytes per
character, and if you have a very large number of strings this could
fill memory and impact performance. A very common use case is that these
strings are actually ASCII and can be represented with 1 byte per character.
In ``astropy`` it is possible to work directly and conveniently with
bytestring data in |Table| and |Column| operations.

Note that the bytestring issue is a particular problem when dealing with HDF5
files, where character data are read as bytestrings (``'S'`` dtype) when using
the :ref:`table_io`. Since HDF5 files are frequently used to store very large
datasets, the memory bloat associated with conversion to ``'U'`` dtype is
unacceptable.


Examples
""""""""

.. EXAMPLE START: Bytestring Data in Astropy Tables

The examples below illustrate dealing with bytestring data in ``astropy``::

    >>> t = Table([['abc', 'def']], names=['a'], dtype=['S'])

    >>> t['a'] == 'abc'  # Gives expected answer
    array([ True, False])

    >>> t['a'] == b'abc'  # Still gives expected answer
    array([ True, False])

    >>> t['a'][0] == 'abc'  # Expected answer
    True

    >>> t['a'][0] == b'abc'  # Cannot compare to bytestring
    False

    >>> t['a'][0] = 'b'
    >>> t
    <Table length=2>
      a
    bytes3
    ------
        b
       def

    >>> t['a'] == 'b'
    array([ True, False])

.. doctest-skip::

    >>> # Round trip unicode strings through HDF5
    >>> t.write('test.hdf5', format='hdf5', path='data', overwrite=True)
    >>> t2 = Table.read('test.hdf5', format='hdf5', path='data')
    >>> t2
    <Table length=2>
     col0
    bytes3
    ------
        b
       def

.. EXAMPLE END
.. _astropy-table:

*****************************
Data Tables (`astropy.table`)
*****************************

Introduction
============

`astropy.table` provides functionality for storing and manipulating
heterogeneous tables of data in a way that is familiar to ``numpy`` users. A few
notable capabilities of this package are:

* Initialize a table from a wide variety of input data structures and types.
* Modify a table by adding or removing columns, changing column names,
  or adding new rows of data.
* Handle tables containing missing values.
* Include table and column metadata as flexible data structures.
* Specify a description, units, and output formatting for columns.
* Interactively scroll through long tables similar to using ``more``.
* Create a new table by selecting rows or columns from a table.
* Perform :ref:`table_operations` like database joins, concatenation, and binning.
* Maintain a table index for fast retrieval of table items or ranges.
* Manipulate multidimensional columns.
* Handle non-native (mixin) column types within table.
* Methods for :ref:`read_write_tables` to files.
* Hooks for :ref:`subclassing_table` and its component classes.

Getting Started
===============

The basic workflow for creating a table, accessing table elements,
and modifying the table is shown below. These examples demonstrate a concise
case, while the full `astropy.table` documentation is available from the
:ref:`using_astropy_table` section.

First create a simple table with columns of data named ``a``, ``b``, ``c``, and
``d``. These columns have integer, float, string, and |Quantity| values
respectively::

  >>> from astropy.table import QTable
  >>> import astropy.units as u
  >>> import numpy as np

  >>> a = np.array([1, 4, 5], dtype=np.int32)
  >>> b = [2.0, 5.0, 8.5]
  >>> c = ['x', 'y', 'z']
  >>> d = [10, 20, 30] * u.m / u.s

  >>> t = QTable([a, b, c, d],
  ...            names=('a', 'b', 'c', 'd'),
  ...            meta={'name': 'first table'})

Comments:

- Column ``a`` is a |ndarray| with a specified ``dtype`` of ``int32``. If the
  data type is not provided, the default type for integers is ``int64`` on Mac
  and Linux and ``int32`` on Windows.
- Column ``b`` is a list of ``float`` values, represented as ``float64``.
- Column ``c`` is a list of ``str`` values, represented as unicode.
  See :ref:`bytestring-columns-python-3` for more information.
- Column ``d`` is a |Quantity| array. Since we used |QTable|, this stores a
  native |Quantity| within the table and brings the full power of
  :ref:`astropy-units` to this column in the table.

.. Note::

   If the table data have no units or you prefer to not use |Quantity|, then you
   can use the |Table| class to create tables. The **only** difference between
   |QTable| and |Table| is the behavior when adding a column that has units.
   See :ref:`quantity_and_qtable` and :ref:`columns_with_units` for details on
   the differences and use cases.

There are many other ways of :ref:`construct_table`, including from a list of
rows (either tuples or dicts), from a ``numpy`` structured or 2D array, by
adding columns or rows incrementally, or even converting from a |SkyCoord| or a
:class:`pandas.DataFrame`.

There are a few ways of :ref:`access_table`. You can get detailed information
about the table values and column definitions as follows::

  >>> t
  <QTable length=3>
    a      b     c      d
                      m / s
  int32 float64 str1 float64
  ----- ------- ---- -------
      1     2.0    x    10.0
      4     5.0    y    20.0
      5     8.5    z    30.0

You can get summary information about the table as follows::

  >>> t.info
  <QTable length=3>
  name  dtype   unit  class
  ---- ------- ----- --------
     a   int32         Column
     b float64         Column
     c    str1         Column
     d float64 m / s Quantity

From within a `Jupyter notebook <https://jupyter.org/>`_, the table is
displayed as a formatted HTML table (details of how it appears can be changed
by altering the `astropy.table.conf.default_notebook_table_class
<astropy.table.Conf.default_notebook_table_class>` item in the
:ref:`astropy_config`:

.. image:: table_repr_html.png
   :width: 450px

Or you can get a fancier notebook interface with in-browser search, and sort
using :meth:`~astropy.table.Table.show_in_notebook`:

.. image:: table_show_in_nb.png
   :width: 450px

If you print the table (either from the notebook or in a text console session)
then a formatted version appears::

  >>> print(t)
   a   b   c    d
              m / s
  --- --- --- -----
    1 2.0   x  10.0
    4 5.0   y  20.0
    5 8.5   z  30.0


If you do not like the format of a particular column, you can change it through
:ref:`the 'info' property <mixin_attributes>`::

  >>> t['b'].info.format = '7.3f'
  >>> print(t)
   a     b     c    d
                  m / s
  --- ------- --- -----
    1   2.000   x  10.0
    4   5.000   y  20.0
    5   8.500   z  30.0

For a long table you can scroll up and down through the table one page at
time::

  >>> t.more()  # doctest: +SKIP

You can also display it as an HTML-formatted table in the browser::

  >>> t.show_in_browser()  # doctest: +SKIP

Or as an interactive (searchable and sortable) javascript table::

  >>> t.show_in_browser(jsviewer=True)  # doctest: +SKIP

Now examine some high-level information about the table::

  >>> t.colnames
  ['a', 'b', 'c', 'd']
  >>> len(t)
  3
  >>> t.meta
  {'name': 'first table'}

Access the data by column or row using familiar ``numpy`` structured array
syntax::

  >>> t['a']       # Column 'a'
  <Column name='a' dtype='int32' length=3>
  1
  4
  5

  >>> t['a'][1]    # Row 1 of column 'a'
  4

  >>> t[1]         # Row 1 of the table
  <Row index=1>
    a      b     c      d
                      m / s
  int32 float64 str1 float64
  ----- ------- ---- -------
      4   5.000    y    20.0


  >>> t[1]['a']    # Column 'a' of row 1
  4

You can retrieve a subset of a table by rows (using a :class:`slice`) or by
columns (using column names), where the subset is returned as a new table::

  >>> print(t[0:2])      # Table object with rows 0 and 1
   a     b     c    d
                  m / s
  --- ------- --- -----
    1   2.000   x  10.0
    4   5.000   y  20.0


  >>> print(t['a', 'c'])  # Table with cols 'a' and 'c'
   a   c
  --- ---
    1   x
    4   y
    5   z

:ref:`modify_table` in place is flexible and works as you would expect::

  >>> t['a'][:] = [-1, -2, -3]    # Set all column values in place
  >>> t['a'][2] = 30              # Set row 2 of column 'a'
  >>> t[1] = (8, 9.0, "W", 4 * u.m / u.s) # Set all values of row 1
  >>> t[1]['b'] = -9              # Set column 'b' of row 1
  >>> t[0:2]['b'] = 100.0         # Set column 'b' of rows 0 and 1
  >>> print(t)
   a     b     c    d
                  m / s
  --- ------- --- -----
   -1 100.000   x  10.0
    8 100.000   W   4.0
   30   8.500   z  30.0

Replace, add, remove, and rename columns with the following::

  >>> t['b'] = ['a', 'new', 'dtype']   # Replace column 'b' (different from in-place)
  >>> t['e'] = [1, 2, 3]               # Add column 'e'
  >>> del t['c']                       # Delete column 'c'
  >>> t.rename_column('a', 'A')        # Rename column 'a' to 'A'
  >>> t.colnames
  ['A', 'b', 'd', 'e']

Adding a new row of data to the table is as follows. Note that the unit
value is given in ``cm / s`` but will be added to the table as ``0.1 m / s`` in
accord with the existing unit.

  >>> t.add_row([-8, 'string', 10 * u.cm / u.s, 10])
  >>> t['d']
  <Quantity [10. ,  4. , 30. ,  0.1] m / s>

Tables can be used for data with missing values::

  >>> from astropy.table import MaskedColumn
  >>> a_masked = MaskedColumn(a, mask=[True, True, False])
  >>> t = QTable([a_masked, b, c], names=('a', 'b', 'c'),
  ...            dtype=('i4', 'f8', 'U1'))
  >>> t
  <QTable length=3>
    a      b     c
  int32 float64 str1
  ----- ------- ----
     --     2.0    x
     --     5.0    y
      5     8.5    z

In addition to |Quantity|, you can include certain object types like
`~astropy.time.Time`, `~astropy.coordinates.SkyCoord`, and
`~astropy.table.NdarrayMixin` in your table. These "mixin" columns behave like
a hybrid of a regular `~astropy.table.Column` and the native object type (see
:ref:`mixin_columns`). For example::

  >>> from astropy.time import Time
  >>> from astropy.coordinates import SkyCoord
  >>> tm = Time(['2000:002', '2002:345'])
  >>> sc = SkyCoord([10, 20], [-45, +40], unit='deg')
  >>> t = QTable([tm, sc], names=['time', 'skycoord'])
  >>> t
  <QTable length=2>
           time          skycoord
                         deg,deg
           Time          SkyCoord
  --------------------- ----------
  2000:002:00:00:00.000 10.0,-45.0
  2002:345:00:00:00.000  20.0,40.0

Now let us compute the interval since the launch of the `Chandra X-ray Observatory
<https://en.wikipedia.org/wiki/Chandra_X-ray_Observatory>`_ aboard `STS-93
<https://en.wikipedia.org/wiki/STS-93>`_ and store this in our table as a
|Quantity| in days::

  >>> dt = t['time'] - Time('1999-07-23 04:30:59.984')
  >>> t['dt_cxo'] = dt.to(u.d)
  >>> t['dt_cxo'].info.format = '.3f'
  >>> print(t)
           time          skycoord   dt_cxo
                         deg,deg      d
  --------------------- ---------- --------
  2000:002:00:00:00.000 10.0,-45.0  162.812
  2002:345:00:00:00.000  20.0,40.0 1236.812

.. _using_astropy_table:

Using ``table``
===============

The details of using `astropy.table` are provided in the following sections:

Construct Table
---------------

.. toctree::
   :maxdepth: 2

   construct_table.rst

Access Table
------------

.. toctree::
   :maxdepth: 2

   access_table.rst

Modify Table
------------

.. toctree::
   :maxdepth: 2

   modify_table.rst

Table Operations
----------------

.. toctree::
   :maxdepth: 2

   operations.rst

Indexing
--------

.. toctree::
   :maxdepth: 2

   indexing.rst

Masking
-------

.. toctree::
   :maxdepth: 2

   masking.rst

I/O with Tables
---------------

.. toctree::
   :maxdepth: 2

   io.rst
   pandas.rst

Mixin Columns
-------------

.. toctree::
   :maxdepth: 2

   mixin_columns.rst

Implementation
--------------

.. toctree::
   :maxdepth: 2

   implementation_details.rst

.. note that if this section gets too long, it should be moved to a separate
   doc page - see the top of performance.inc.rst for the instructions on how to do
   that
.. include:: performance.inc.rst

Reference/API
=============

.. automodapi:: astropy.table
.. doctest-skip-all

.. _read_write_tables:

Reading and Writing Table Objects
*********************************

``astropy`` provides a unified interface for reading and writing data in
different formats. For many common cases this will streamline the process of
file I/O and reduce the need to learn the separate details of all of the I/O
packages within ``astropy``. For details and examples of using this interface
see the :ref:`table_io` section.

Getting Started
===============

The :class:`~astropy.table.Table` class includes two methods,
:meth:`~astropy.table.Table.read` and :meth:`~astropy.table.Table.write`, that
make it possible to read from and write to files. A number of formats are
automatically supported (see :ref:`built_in_readers_writers`) and new file
formats and extensions can be registered with the :class:`~astropy.table.Table`
class (see :ref:`io_registry`).

.. EXAMPLE START: Reading and Writing Table Objects

To use this interface, first import the :class:`~astropy.table.Table` class,
then call the :class:`~astropy.table.Table` :meth:`~astropy.table.Table.read`
method with the name of the file and the file format, for instance
``'ascii.daophot'``::

    >>> from astropy.table import Table
    >>> t = Table.read('photometry.dat', format='ascii.daophot')

It is possible to load tables directly from the Internet using URLs. For
example, download tables from `VizieR catalogs <https://vizier.u-strasbg.fr/>`_
in CDS format (``'ascii.cds'``)::

    >>> t = Table.read("ftp://cdsarc.u-strasbg.fr/pub/cats/VII/253/snrs.dat",
    ...         readme="ftp://cdsarc.u-strasbg.fr/pub/cats/VII/253/ReadMe",
    ...         format="ascii.cds")

.. EXAMPLE END

For certain file formats, the format can be automatically detected, for
example from the filename extension::

    >>> t = Table.read('table.tex')

Similarly, for writing, the format can be explicitly specified::

    >>> t.write(filename, format='latex')

As for the :meth:`~astropy.table.Table.read` method, the format may
be automatically identified in some cases.

Any additional arguments specified will depend on the format. For examples of
this see the section :ref:`built_in_readers_writers`. This section also
provides the full list of choices for the ``format`` argument.

Supported Formats
=================

The :ref:`table_io` has built-in support for the following data file formats:

* :ref:`table_io_ascii`
* :ref:`table_io_hdf5`
* :ref:`table_io_fits`
* :ref:`table_io_votable`
* :ref:`table_io_parquet`
.. _construct_table:

Constructing a Table
********************

There is great deal of flexibility in the way that a table can be initially
constructed. Details on the inputs to the |Table| and |QTable|
constructors are in the `Initialization Details`_ section. However, the
best way to understand how to make a table is by example.

Examples
========

Setup
-----

For the following examples you need to import the |QTable|, |Table|, and
|Column| classes along with the :ref:`astropy-units` package and the ``numpy``
package::

  >>> from astropy.table import QTable, Table, Column
  >>> from astropy import units as u
  >>> import numpy as np

Creating from Scratch
---------------------

.. EXAMPLE START: Creating an Astropy Table from Scratch

A |Table| can be created without any initial input data or even without any
initial columns. This is useful for building tables dynamically if the initial
size, columns, or data are not known.

.. Note::
   Adding rows requires making a new copy of the entire
   table each time, so in the case of large tables this may be slow.
   On the other hand, adding columns is fast.

::

  >>> t = Table()
  >>> t['a'] = [1, 4]
  >>> t['b'] = [2.0, 5.0]
  >>> t['c'] = ['x', 'y']

  >>> t = Table(names=('a', 'b', 'c'), dtype=('f4', 'i4', 'S2'))
  >>> t.add_row((1, 2.0, 'x'))
  >>> t.add_row((4, 5.0, 'y'))

  >>> t = Table(dtype=[('a', 'f4'), ('b', 'i4'), ('c', 'S2')])

If your data columns have physical units associated with them then we
recommend using the |QTable| class. This will allow the column to be
stored in the table as a native |Quantity| and bring the full power of
:ref:`astropy-units` to the table. See :ref:`quantity_and_qtable` for details.
::

  >>> t = QTable()
  >>> t['a'] = [1, 4]
  >>> t['b'] = [2.0, 5.0] * u.cm / u.s
  >>> t['c'] = ['x', 'y']
  >>> type(t['b'])
  <class 'astropy.units.quantity.Quantity'>

.. EXAMPLE END

List of Columns
---------------

.. EXAMPLE START: Creating an Astropy Table from a List of Columns

A typical case is where you have a number of data columns with the same length
defined in different variables. These might be Python lists or ``numpy`` arrays
or a mix of the two. These can be used to create a |Table| by putting the column
data variables into a Python list. In this case the column names are not
defined by the input data, so they must either be set using the ``names``
keyword or they will be automatically generated as ``col<N>``.

::

  >>> a = np.array([1, 4], dtype=np.int32)
  >>> b = [2.0, 5.0]
  >>> c = ['x', 'y']
  >>> t = Table([a, b, c], names=('a', 'b', 'c'))
  >>> t
  <Table length=2>
    a      b     c
  int32 float64 str1
  ----- ------- ----
      1     2.0    x
      4     5.0    y

.. EXAMPLE END

**Make a new table using columns from the first table**

Once you have a |Table|, then you can make a new table by selecting columns
and putting them into a Python list (e.g., ``[ t['c'], t['a'] ]``)::

  >>> Table([t['c'], t['a']])
  <Table length=2>
   c     a
  str1 int32
  ---- -----
     x     1
     y     4

**Make a new table using expressions involving columns**

The |Column| object is derived from the standard |ndarray| and can be used
directly in arithmetic expressions. This allows for a compact way of making a
new table with modified column values::

  >>> Table([t['a']**2, t['b'] + 10])
  <Table length=2>
    a      b
  int32 float64
  ----- -------
      1    12.0
     16    15.0


**Different types of column data**

The list input method for |Table| is very flexible since you can use a mix
of different data types to initialize a table::

  >>> a = (1., 4.)
  >>> b = np.array([[2, 3], [5, 6]], dtype=np.int64)  # vector column
  >>> c = Column(['x', 'y'], name='axis')
  >>> arr = (a, b, c)
  >>> Table(arr)
  <Table length=2>
    col0  col1 [2] axis
  float64  int64   str1
  ------- -------- ----
      1.0   2 .. 3    x
      4.0   5 .. 6    y

Notice that in the third column the existing column name ``'axis'`` is used.

Dict of Columns
---------------

.. EXAMPLE START: Creating an Astropy Table from a Dictionary of Columns

A :class:`dict` of column data can be used to initialize a |Table|::

  >>> arr = {'a': np.array([1, 4], dtype=np.int32),
  ...        'b': [2.0, 5.0],
  ...        'c': ['x', 'y']}
  >>>
  >>> Table(arr)
  <Table length=2>
    a      b     c
  int32 float64 str1
  ----- ------- ----
      1     2.0    x
      4     5.0    y

.. EXAMPLE END

**Specify the column order and optionally the data types**
::

  >>> Table(arr, names=('a', 'c', 'b'), dtype=('f8', 'U2', 'i4'))
  <Table length=2>
     a     c     b
  float64 str2 int32
  ------- ---- -----
      1.0    x     2
      4.0    y     5

**Different types of column data**

The input column data can be any data type that can initialize a |Column|
object::

  >>> arr = {'a': (1., 4.),
  ...        'b': np.array([[2, 3], [5, 6]], dtype=np.int64),
  ...        'c': Column(['x', 'y'], name='axis')}
  >>> Table(arr, names=('a', 'b', 'c'))
  <Table length=2>
    a   b [2]   c
  float64 int64  str1
  ------- ------ ----
      1.0 2 .. 3    x
      4.0 5 .. 6    y

Notice that the key ``'c'`` takes precedence over the existing column name
``'axis'`` in the third column. Also see that the ``'b'`` column is a vector
column where each row element is itself a two-element array.

**Renaming columns is not possible**
::

  >>> Table(arr, names=('a_new', 'b_new', 'c_new'))
  Traceback (most recent call last):
    ...
  KeyError: 'a_new'

Row Data
--------

Row-oriented data can be used to create a table using the ``rows``
keyword argument.

**List or tuple of data records**

If you have row-oriented input data such as a list of records, you
need to use the ``rows`` keyword to create a table::

  >>> data_rows = [(1, 2.0, 'x'),
  ...              (4, 5.0, 'y'),
  ...              (5, 8.2, 'z')]
  >>> t = Table(rows=data_rows, names=('a', 'b', 'c'))
  >>> print(t)
   a   b   c
  --- --- ---
    1 2.0   x
    4 5.0   y
    5 8.2   z

**List of dict objects**

You can also initialize a table with row values. This is constructed as a
list of :class:`dict` objects. The keys determine the column names::

  >>> data = [{'a': 5, 'b': 10},
  ...         {'a': 15, 'b': 20}]
  >>> t = Table(rows=data)
  >>> print(t)
   a   b
  --- ---
    5  10
   15  20

If there are missing keys in one or more rows then the corresponding values
will be marked as missing (masked)::

  >>> t = Table(rows=[{'a': 5, 'b': 10}, {'a': 15, 'c': 50}])
  >>> print(t)
   a   b   c
  --- --- ---
    5  10  --
   15  --  50

You can specify the column order with the ``names`` argument::

  >>> data = [{'a': 5, 'b': 10},
  ...         {'a': 15, 'b': 20}]
  >>> t = Table(rows=data, names=('b', 'a'))
  >>> print(t)
   b   a
  --- ---
   10   5
   20  15

If ``names`` are not provided then column ordering will be determined by the
first :class:`dict` if it contains values for all the columns, or by sorting
the column names alphabetically if it doesn't::

  >>> data = [{'b': 10, 'c': 7, 'a': 5},
  ...         {'a': 15, 'c': 35, 'b': 20}]
  >>> t = Table(rows=data)
  >>> print(t)
   b   c   a
  --- --- ---
   10   7   5
   20  35  15
  >>> data = [{'b': 10, 'c': 7, },
  ...         {'a': 15, 'c': 35, 'b': 20}]
  >>> t = Table(rows=data)
  >>> print(t)
   a   b   c
  --- --- ---
   --  10   7
   15  20  35

**Single row**

You can also make a new table from a single row of an existing table::

  >>> a = [1, 4]
  >>> b = [2.0, 5.0]
  >>> t = Table([a, b], names=('a', 'b'))
  >>> t2 = Table(rows=t[1])

Remember that a |Row| has effectively a zero length compared to the
newly created |Table| which has a length of one. This is similar to
the difference between a scalar ``1`` (length 0) and an array such as
``np.array([1])`` with length 1.

.. Note::

   In the case of input data as a list of dicts or a single |Table| row, you
   can supply the data as the ``data`` argument since these forms
   are always unambiguous. For example, ``Table([{'a': 1}, {'a': 2}])`` is
   accepted. However, a list of records must always be provided using the
   ``rows`` keyword, otherwise it will be interpreted as a list of columns.

NumPy Structured Array
----------------------

The `structured array <https://numpy.org/doc/stable/user/basics.rec.html>`_ is
the standard mechanism in ``numpy`` for storing heterogeneous table data. Most
scientific I/O packages that read table files (e.g., `astropy.io.fits`,
`astropy.io.votable`, and `asciitable
<https://cxc.harvard.edu/contrib/asciitable/>`_) will return the table in an
object that is based on the structured array. A structured array can be
created using::

  >>> arr = np.array([(1, 2.0, 'x'),
  ...                 (4, 5.0, 'y')],
  ...                dtype=[('a', 'i4'), ('b', 'f8'), ('c', 'U2')])

From ``arr`` it is possible to create the corresponding |Table| object::

  >>> Table(arr)
  <Table length=2>
    a      b     c
  int32 float64 str2
  ----- ------- ----
      1     2.0    x
      4     5.0    y

Note that in the above example and most of the following examples we are
creating a table and immediately asking the interactive Python interpreter to
print the table to see what we made. In real code you might do something like::

  >>> table = Table(arr)
  >>> print(table)
   a   b   c
  --- --- ---
    1 2.0   x
    4 5.0   y

**New column names**

The column names can be changed from the original values by providing the
``names`` argument::

  >>> Table(arr, names=('a_new', 'b_new', 'c_new'))
  <Table length=2>
  a_new  b_new  c_new
  int32 float64  str2
  ----- ------- -----
      1     2.0     x
      4     5.0     y

**New data types**

The data type for each column can likewise be changed with ``dtype``::

  >>> Table(arr, dtype=('f4', 'i4', 'U4'))
  <Table length=2>
     a      b    c
  float32 int32 str4
  ------- ----- ----
      1.0     2    x
      4.0     5    y

  >>> Table(arr, names=('a_new', 'b_new', 'c_new'), dtype=('f4', 'i4', 'U4'))
  <Table length=2>
   a_new  b_new c_new
  float32 int32  str4
  ------- ----- -----
      1.0     2     x
      4.0     5     y

NumPy Homogeneous Array
-----------------------

A ``numpy`` 1D array is treated as a single row table where each element of the
array corresponds to a column::

  >>> Table(np.array([1, 2, 3]), names=['a', 'b', 'c'], dtype=('i8', 'i8', 'i8'))
  <Table length=1>
    a     b     c
  int64 int64 int64
  ----- ----- -----
      1     2     3

A ``numpy`` 2D array (where all elements have the same type) can also be
converted into a |Table|. In this case the column names are not specified by
the data and must either be provided by the user or will be automatically
generated as ``col<N>`` where ``<N>`` is the column number.

**Basic example with automatic column names**
::

  >>> arr = np.array([[1, 2, 3],
  ...                 [4, 5, 6]], dtype=np.int32)
  >>> Table(arr)
  <Table length=2>
   col0  col1  col2
  int32 int32 int32
  ----- ----- -----
      1     2     3
      4     5     6

**Column names and types specified**
::

  >>> Table(arr, names=('a_new', 'b_new', 'c_new'), dtype=('f4', 'i4', 'U4'))
  <Table length=2>
   a_new  b_new c_new
  float32 int32  str4
  ------- ----- -----
      1.0     2     3
      4.0     5     6

**Referencing the original data**

It is possible to reference the original data as long as the data types are not
changed::

  >>> t = Table(arr, copy=False)

See the `Copy versus Reference`_ section for more information.

**Python arrays versus NumPy arrays as input**

There is a slightly subtle issue that is important to understand about the way
that |Table| objects are created. Any data input that looks like a Python
:class:`list` (including a :class:`tuple`) is considered to be a list of
columns. In contrast, a homogeneous |ndarray| input is interpreted as a list of
rows::

  >>> arr = [[1, 2, 3],
  ...        [4, 5, 6]]
  >>> np_arr = np.array(arr)

  >>> print(Table(arr))    # Two columns, three rows
  col0 col1
  ---- ----
     1    4
     2    5
     3    6

  >>> print(Table(np_arr))  # Three columns, two rows
  col0 col1 col2
  ---- ---- ----
     1    2    3
     4    5    6

This dichotomy is needed to support flexible list input while retaining the
natural interpretation of 2D ``numpy`` arrays where the first index corresponds
to data "rows" and the second index corresponds to data "columns."

From an Existing Table
----------------------

.. EXAMPLE START: Creating an Astropy Table from an Existing Table

A new table can be created by selecting a subset of columns in an existing
table::

  >>> t = Table(names=('a', 'b', 'c'))
  >>> t['c', 'b', 'a']  # Makes a copy of the data
  <Table length=0>
     c       b       a
  float64 float64 float64
  ------- ------- -------

An alternate way is to use the ``columns`` attribute (explained in the
`TableColumns`_ section) to initialize a new table. This lets you choose
columns by their numerical index or name and supports slicing syntax::

  >>> Table(t.columns[0:2])
  <Table length=0>
     a       b
  float64 float64
  ------- -------

  >>> Table([t.columns[0], t.columns['c']])
  <Table length=0>
     a       c
  float64 float64
  ------- -------

To create a copy of an existing table that is empty (has no rows)::

 >>> t = Table([[1.0, 2.3], [2.1, 3]], names=['x', 'y'])
 >>> t
 <Table length=2>
    x       y
 float64 float64
 ------- -------
     1.0     2.1
     2.3     3.0

 >>> tcopy = t[:0].copy()
 >>> tcopy
 <Table length=0>
    x       y
 float64 float64
 ------- -------

.. EXAMPLE END

Empty Array of a Known Size
---------------------------

.. EXAMPLE START: Creating an Astropy Table from an Empty Array

If you do know the size that your table will be, but do not know the values in
advance, you can create a zeroed |ndarray| and build the |Table| from it::

  >>> N = 3
  >>> dtype = [('a', 'i4'), ('b', 'f8'), ('c', 'bool')]
  >>> t = Table(data=np.zeros(N, dtype=dtype))
  >>> t
  <Table length=3>
    a      b      c
  int32 float64  bool
  ----- ------- -----
      0     0.0 False
      0     0.0 False
      0     0.0 False

For example, you can then fill in this table row by row with values extracted
from another table, or generated on the fly::

  >>> for i in range(len(t)):
  ...     t[i] = (i, 2.5*i, i % 2)
  >>> t
  <Table length=3>
    a      b      c
  int32 float64  bool
  ----- ------- -----
      0     0.0 False
      1     2.5  True
      2     5.0 False

.. EXAMPLE END

SkyCoord
--------

A |SkyCoord| object can be converted to a |QTable| using its
:meth:`~astropy.coordinates.SkyCoord.to_table` method. For details and examples
see :ref:`skycoord-table-conversion`.

Pandas DataFrame
----------------

The section on :ref:`pandas` gives details on how to initialize a |Table| using
a :class:`pandas.DataFrame` via the :func:`~astropy.table.Table.from_pandas`
class method. This provides a convenient way to take advantage of the many I/O
and table manipulation methods in `pandas <https://pandas.pydata.org/>`_.

Comment Lines
-------------

.. EXAMPLE START: Adding Comment Lines in an ASCII File

Comment lines in an ASCII file can be added via the ``'comments'`` key in the
table's metadata. The following will insert two comment lines in the output
ASCII file unless ``comment=False`` is explicitly set in ``write()``::

  >>> import sys
  >>> from astropy.table import Table
  >>> t = Table(names=('a', 'b', 'c'), dtype=('f4', 'i4', 'S2'))
  >>> t.add_row((1, 2.0, 'x'))
  >>> t.meta['comments'] = ['Here is my explanatory text. This is awesome.',
  ...                       'Second comment line.']
  >>> t.write(sys.stdout, format='ascii')
  # Here is my explanatory text. This is awesome.
  # Second comment line.
  a b c
  1.0 2 x

.. EXAMPLE END

Initialization Details
======================

A table object is created by initializing a |Table| class
object with the following arguments, all of which are optional:

``data`` : |ndarray|, :class:`dict`, :class:`list`, |Table|, or table-like object, optional
    Data to initialize table.
``masked`` : :class:`bool`, optional
    Specify whether the table is masked.
``names`` : :class:`list`, optional
    Specify column names.
``dtype`` : :class:`list`, optional
    Specify column data types.
``meta`` : :class:`dict`, optional
    Metadata associated with the table.
``copy`` : :class:`bool`, optional
    Copy the input data. If the input is a |Table| the ``meta`` is always
    copied regardless of the ``copy`` parameter.
    Default is `True`.
``rows`` : |ndarray|, :class:`list` of lists, optional
    Row-oriented data for table instead of ``data`` argument.
``copy_indices`` : :class:`bool`, optional
    Copy any indices in the input data. Default is `True`.
``units`` : :class:`list`, :class:`dict`, optional
    List or dict of units to apply to columns.
``descriptions`` : :class:`list`, :class:`dict`, optional
    List or dict of descriptions to apply to columns.
``**kwargs`` : :class:`dict`, optional
    Additional keyword args when converting table-like object.

The following subsections provide further detail on the values and options for
each of the keyword arguments that can be used to create a new |Table| object.

data
----

The |Table| object can be initialized with several different forms
for the ``data`` argument.

**NumPy ndarray (structured array)**
    The base column names are the field names of the ``data`` structured
    array. The ``names`` list (optional) can be used to select
    particular fields and/or reorder the base names. The ``dtype`` list
    (optional) must match the length of ``names`` and is used to
    override the existing ``data`` types.

**NumPy ndarray (homogeneous)**
    If the ``data`` is a one-dimensional |ndarray| then it is treated as a
    single row table where each element of the array corresponds to a column.

    If the ``data`` is an at least two-dimensional |ndarray|, then the first
    (left-most) index corresponds to row number (table length) and the
    second index corresponds to column number (table width). Higher
    dimensions get absorbed in the shape of each table cell.

    If provided, the ``names`` list must match the "width" of the ``data``
    argument. The default for ``names`` is to auto-generate column names
    in the form ``col<N>``. If provided, the ``dtype`` list overrides the
    base column types and must match the length of ``names``.

**dict-like**
    The keys of the ``data`` object define the base column names. The
    corresponding values can be |Column| objects, ``numpy`` arrays, or list-
    like objects. The ``names`` list (optional) can be used to select
    particular fields and/or reorder the base names. The ``dtype`` list
    (optional) must match the length of ``names`` and is used to override
    the existing or default data types.

**list-like**
    Each item in the ``data`` list provides a column of data values and
    can be a |Column| object, |ndarray|, or list-like object. The
    ``names`` list defines the name of each column. The names will be
    auto-generated if not provided (either with the ``names`` argument or
    by |Column| objects). If provided, the ``names`` argument must match the
    number of items in the ``data`` list. The optional ``dtype`` list
    will override the existing or default data types and must match
    ``names`` in length.

**list-of-dicts**
    Similar to Python's built-in :class:`csv.DictReader`, each item in the
    ``data`` list provides a row of data values and must be a :class:`dict`.
    The key values in each :class:`dict` define the column names. The ``names``
    argument may be supplied to specify column ordering. If ``names`` are not
    provided then column ordering will be determined by the first :class:`dict`
    if it contains values for all the columns, or by sorting the column names
    alphabetically if it does not. The ``dtype`` list may be specified, and
    must correspond to the order of output columns.

**Table-like object**
    If another table-like object has a ``__astropy_table__()`` method then
    that object can be used to directly create a |Table|. See the
    `table-like objects`_ section for details.

**None**
    Initialize a zero-length table. If ``names`` and optionally ``dtype``
    are provided, then the corresponding columns are created.

names
-----

The ``names`` argument provides a way to specify the table column names or
override the existing ones. By default, the column names are either taken from
existing names (for |ndarray| or |Table| input) or auto-generated as
``col<N>``. If ``names`` is provided, then it must be a list with the same
length as the number of columns. Any list elements with value `None` fall back
to the default name.

In the case where ``data`` is provided as a :class:`dict` of columns, the
``names`` argument can be supplied to specify the order of columns. The
``names`` list must then contain each of the keys in the ``data``
:class:`dict`.

dtype
-----

The ``dtype`` argument provides a way to specify the table column data types or
override the existing types. By default, the types are either taken from
existing types (for |ndarray| or |Table| input) or auto-generated by the
:func:`numpy.array` routine. If ``dtype`` is provided then it must be a list
with the same length as the number of columns. The values must be valid
:class:`numpy.dtype` initializers or `None`. Any list elements with value
`None` fall back to the default type.

meta
----

The ``meta`` argument is an object that contains metadata associated with the
table. It is recommended that this object be a :class:`dict` or
:class:`~collections.OrderedDict`, but the only firm requirement is that it can
be copied with the standard library :func:`copy.deepcopy` routine. By
default, ``meta`` is an empty :class:`~collections.OrderedDict`.

copy
----

In the case where ``data`` is either an |ndarray| object, a :class:`dict`, or
an existing |Table|, it is possible to use a reference to the existing data by
setting ``copy=False``. This has the advantage of reducing memory use and being
faster. However, you should take care because any modifications to the new
|Table| data will also be seen in the original input data. See the `Copy versus
Reference`_ section for more information.

rows
----

This argument allows for providing data as a sequence of rows, in contrast
to the ``data`` keyword, which generally assumes data are a sequence of columns.
The `Row data`_ section provides details.

copy_indices
------------

If you are initializing a |Table| from another |Table| that makes use of
:ref:`table-indexing`, then this option allows copying that table *without*
copying the indices by setting ``copy_indices=False``. By default, the indices
are copied.

units
-----

This allows for setting the unit for one or more columns at the time of
creating the table. The input can be either a list of unit values corresponding
to each of the columns in the table (using `None` or ``''`` for no unit), or a
:class:`dict` that provides the unit for specified column names. For example::

  >>> dat = [[1, 2], ['hello', 'world']]
  >>> qt = QTable(dat, names=['a', 'b'], units=(u.m, None))
  >>> qt = QTable(dat, names=['a', 'b'], units={'a': u.m})

See :ref:`quantity_and_qtable` for why we used a |QTable| here instead of a
|Table|.

descriptions
------------

This allows for setting the description for one or more columns at the time of
creating the table. The input can be either a list of description values
corresponding to each of the columns in the table (using `None` for no
description), or a :class:`dict` that provides the description for specified
column names. This works in the same way as the ``units`` example above.

.. _copy_versus_reference:

Copy versus Reference
=====================

Normally when a new |Table| object is created, the input data are *copied*.
This ensures that if the new table elements are modified then the original data
will not be affected. However, when creating a table from an existing |Table|,
a |ndarray| object (structured or homogeneous) or a :class:`dict`, it is
possible to disable copying so that a memory reference to the original data is
used instead. This has the advantage of being faster and using less memory.
However, caution must be exercised because the new table data and original data
will be linked, as shown below::

  >>> arr = np.array([(1, 2.0, 'x'),
  ...                 (4, 5.0, 'y')],
  ...                dtype=[('a', 'i8'), ('b', 'f8'), ('c', 'S2')])
  >>> print(arr['a'])  # column "a" of the input array
  [1 4]
  >>> t = Table(arr, copy=False)
  >>> t['a'][1] = 99
  >>> print(arr['a'])  # arr['a'] got changed when we modified t['a']
  [ 1 99]

Note that when referencing the data it is not possible to change the data types
since that operation requires making a copy of the data. In this case an error
occurs::

  >>> t = Table(arr, copy=False, dtype=('f4', 'i4', 'S4'))
  Traceback (most recent call last):
    ...
  ValueError: Cannot specify dtype when copy=False

Another caveat to using referenced data is that if you add a new row to the
table, the reference to the original data array is lost and the table will now
instead hold a copy of the original values (in addition to the new row).

Column and TableColumns Classes
===============================

There are two classes, |Column| and |TableColumns|, that are useful when
constructing new tables.

Column
------

A |Column| object can be created as follows, where in all cases the column
``name`` should be provided as a keyword argument and you can optionally provide
these values:

``data`` : :class:`list`, |ndarray| or `None`
    Column data values.
``dtype`` : :class:`numpy.dtype` compatible value
    Data type for column.
``description`` : :class:`str`
    Full description of column.
``unit`` : :class:`str`
    Physical unit.
``format`` : :class:`str` or function
    `Format specifier`_ for outputting column values.
``meta`` : :class:`dict`
    Metadata associated with the column.

Initialization Options
^^^^^^^^^^^^^^^^^^^^^^

The column data values, shape, and data type are specified in one of two ways:

**Provide data but not length or shape**

  Examples::

    col = Column([1, 2], name='a')  # shape=(2,)
    col = Column([[1, 2], [3, 4]], name='a')  # shape=(2, 2)
    col = Column([1, 2], name='a', dtype=float)
    col = Column(np.array([1, 2]), name='a')
    col = Column(['hello', 'world'], name='a')

  The ``dtype`` argument can be any value which is an acceptable fixed-size
  data type initializer for a :class:`numpy.dtype`. See the reference for
  `data type objects
  <https://numpy.org/doc/stable/reference/arrays.dtypes.html>`_. Examples
  include:

  - Python non-string type (:class:`float`, :class:`int`, :class:`bool`).
  - ``numpy`` non-string type (e.g., ``np.float32``, ``np.int64``).
  - ``numpy.dtype`` array-protocol type strings (e.g., ``'i4'``, ``'f8'``, ``'U15'``).

  If no ``dtype`` value is provided, then the type is inferred using
  :func:`numpy.array`. When ``data`` is provided then the ``shape``
  and ``length`` arguments are ignored.

**Provide length and optionally shape, but not data**

  Examples::

    col = Column(name='a', length=5)
    col = Column(name='a', dtype=int, length=10, shape=(3,4))

  The default ``dtype`` is ``np.float64``. The ``shape`` argument is the array
  shape of a single cell in the column. The default ``shape`` is ``()`` which means
  a single value in each element.

.. note::

   After setting the type for a column, that type cannot be changed.
   If data values of a different type are assigned to the column then they
   will be cast to the existing column type.

.. _table_format_string:

Format Specifier
^^^^^^^^^^^^^^^^

The format specifier controls the output of column values when a table or column
is printed or written to an ASCII table. In the simplest case, it is a string
that can be passed to Python's built-in :func:`format` function. For more
complicated formatting, one can also give "old style" or "new style"
format strings, or even a function:

**Plain format specification**

This type of string specifies directly how the value should be formatted
using a `format specification mini-language
<https://docs.python.org/3/library/string.html#formatspec>`_ that is
quite similar to C.

   ``".4f"`` will give four digits after the decimal in float format, or

   ``"6d"`` will give integers in six-character fields.

**Old style format string**

This corresponds to syntax like ``"%.4f" % value`` as documented in
`printf-style String Formatting
<https://docs.python.org/3/library/stdtypes.html#printf-style-string-formatting>`_.

   ``"%.4f"`` to print four digits after the decimal in float format, or

   ``"%6d"`` to print an integer in a six-character wide field.

**New style format string**

This corresponds to syntax like ``"{:.4f}".format(value)`` as documented in
`format string syntax
<https://docs.python.org/3/library/string.html#format-string-syntax>`_.

   ``"{:.4f}"`` to print four digits after the decimal in float format, or

   ``"{:6d}"`` to print an integer in a six-character wide field.

Note that in either format string case any Python string that formats exactly
one value is valid, so ``{:.4f} angstroms`` or ``Value: %12.2f`` would both
work.

**Function**

.. EXAMPLE START: Initialization Options for Column Objects

The greatest flexibility can be achieved by setting a formatting function. This
function must accept a single argument (the value) and return a string. In the
following example this is used to make a LaTeX ready output::

    >>> t = Table([[1,2],[1.234e9,2.34e-12]], names = ('a','b'))
    >>> def latex_exp(value):
    ...     val = f'{value:8.2}'
    ...     mant, exp = val.split('e')
    ...     # remove leading zeros
    ...     exp = exp[0] + exp[1:].lstrip('0')
    ...     return f'$ {mant} \\times 10^{{ {exp} }}$'
    >>> t['b'].format = latex_exp
    >>> t['a'].format = '.4f'
    >>> import sys
    >>> t.write(sys.stdout, format='latex')
    \begin{table}
    \begin{tabular}{cc}
    a & b \\
    1.0000 & $  1.2 \times 10^{ +9 }$ \\
    2.0000 & $  2.3 \times 10^{ -12 }$ \\
    \end{tabular}
    \end{table}

.. EXAMPLE END

TableColumns
------------

Each |Table| object has an attribute ``columns`` which is an ordered dictionary
that stores all of the |Column| objects in the table (see also the `Column`_
section). Technically, the ``columns`` attribute is a |TableColumns| object,
which is an enhanced ordered dictionary that provides easier ways to select
multiple columns. There are a few key points to remember:

- A |Table| can be initialized from a |TableColumns| object (``copy`` is always
  `True`).
- Selecting multiple columns from a |TableColumns| object returns another
  |TableColumns| object.
- Selecting one column from a |TableColumns| object returns a |Column|.

There are a few different ways to select columns from a |TableColumns| object:

**Select columns by name**
::

  >>> t = Table(names=('a', 'b', 'c', 'd'))

  >>> t.columns['d', 'c', 'b']
  <TableColumns names=('d','c','b')>

**Select columns by index slicing**
::

  >>> t.columns[0:2]  # Select first two columns
  <TableColumns names=('a','b')>

  >>> t.columns[::-1]  # Reverse column order
  <TableColumns names=('d','c','b','a')>

**Select single columns by index or name**
::

  >>> t.columns[1]  # Choose a column by index
  <Column name='b' dtype='float64' length=0>

  >>> t.columns['b']  # Choose a column by name
  <Column name='b' dtype='float64' length=0>

.. _subclassing_table:

Subclassing Table
=================

For some applications it can be useful to subclass the |Table| class in order
to introduce specialized behavior. Here we address two particular use cases
for subclassing: adding custom table attributes and changing the behavior of
internal class objects.

.. _table-custom-attributes:

Adding Custom Table Attributes
------------------------------

One simple customization that can be useful is adding new attributes to
the table object.  There is nothing preventing setting an attribute on an
existing table object, for example ``t.foo = 'hello'``.  However, this attribute
would be ephemeral because it will be lost if the table is sliced, copied, or
pickled. Instead, you can add persistent attributes as shown in this example::

  from astropy.table import Table, TableAttribute

  class MyTable(Table):
      foo = TableAttribute()
      bar = TableAttribute(default=[])
      baz = TableAttribute(default=1)

  t = MyTable([[1, 2]], foo='foo')
  t.bar.append(2.0)
  t.baz = 'baz'

Some key points:

- A custom attribute can be set when the table is created or using
  the usual syntax for setting an object attribute.
- A custom attribute always has a default value, either explicitly set
  in the class definition or `None`.
- The attribute values are stored in the table ``meta`` dictionary. This is
  the mechanism by which they are persistent through copy, slice, and
  serialization such as pickling or writing to an :ref:`ecsv_format` file.

Changing Behavior of Internal Class Objects
-------------------------------------------

It is also possible to change the behavior of the internal class objects which
are contained or created by a |Table|. This includes rows, columns, formatting,
and the columns container. In order to do this the subclass needs to declare
what class to use (if it is different from the built-in version). This is done
by specifying one or more of the class attributes ``Row``, ``Column``,
``MaskedColumn``, ``TableColumns``, or ``TableFormatter``.

The following trivial example overrides all of these with do-nothing
subclasses, but in practice you would override only the necessary
subcomponents::

  >>> from astropy.table import Table, Row, Column, MaskedColumn, TableColumns, TableFormatter

  >>> class MyRow(Row): pass
  >>> class MyColumn(Column): pass
  >>> class MyMaskedColumn(MaskedColumn): pass
  >>> class MyTableColumns(TableColumns): pass
  >>> class MyTableFormatter(TableFormatter): pass

  >>> class MyTable(Table):
  ...     """
  ...     Custom subclass of astropy.table.Table
  ...     """
  ...     Row = MyRow  # Use MyRow to create a row object
  ...     Column = MyColumn  # Column
  ...     MaskedColumn = MyMaskedColumn  # Masked Column
  ...     TableColumns = MyTableColumns  # Ordered dict holding Column objects
  ...     TableFormatter = MyTableFormatter  # Controls table output


Example
^^^^^^^

.. EXAMPLE START: Subclassing the Table Class

As a more practical example, suppose you have a table of data with a certain
set of fixed columns, but you also want to carry an arbitrary dictionary of
parameters for each row and then access those values using the same item access
syntax as if they were columns. It is assumed here that the extra parameters
are contained in a ``numpy`` object-dtype column named ``params``::

  >>> from astropy.table import Table, Row
  >>> class ParamsRow(Row):
  ...    """
  ...    Row class that allows access to an arbitrary dict of parameters
  ...    stored as a dict object in the ``params`` column.
  ...    """
  ...    def __getitem__(self, item):
  ...        if item not in self.colnames:
  ...            return super().__getitem__('params')[item]
  ...        else:
  ...            return super().__getitem__(item)
  ...
  ...    def keys(self):
  ...        out = [name for name in self.colnames if name != 'params']
  ...        params = [key.lower() for key in sorted(self['params'])]
  ...        return out + params
  ...
  ...    def values(self):
  ...        return [self[key] for key in self.keys()]

Now we put this into action with a trivial |Table| subclass::

  >>> class ParamsTable(Table):
  ...     Row = ParamsRow

First make a table and add a couple of rows::

  >>> t = ParamsTable(names=['a', 'b', 'params'], dtype=['i', 'f', 'O'])
  >>> t.add_row((1, 2.0, {'x': 1.5, 'y': 2.5}))
  >>> t.add_row((2, 3.0, {'z': 'hello', 'id': 123123}))
  >>> print(t)
   a   b             params
  --- --- ----------------------------
    1 2.0         {'x': 1.5, 'y': 2.5}
    2 3.0 {'z': 'hello', 'id': 123123}

Now see what we have from our specialized ``ParamsRow`` object::

  >>> t[0]['y']
  2.5
  >>> t[1]['id']
  123123
  >>> t[1].keys()
  ['a', 'b', 'id', 'z']
  >>> t[1].values()
  [2, 3.0, 123123, 'hello']

To make this example really useful, you might want to override
``Table.__getitem__()`` in order to allow table-level access to the parameter
fields. This might look something like::

  class ParamsTable(table.Table):
      Row = ParamsRow

      def __getitem__(self, item):
          if isinstance(item, str):
              if item in self.colnames:
                  return self.columns[item]
              else:
                  # If item is not a column name then create a new MaskedArray
                  # corresponding to self['params'][item] for each row.  This
                  # might not exist in some rows so mark as masked (missing) in
                  # those cases.
                  mask = np.zeros(len(self), dtype=np.bool_)
                  item = item.upper()
                  values = [params.get(item) for params in self['params']]
                  for ii, value in enumerate(values):
                      if value is None:
                          mask[ii] = True
                          values[ii] = ''
                  return self.MaskedColumn(name=item, data=values, mask=mask)

          # ... and then the rest of the original __getitem__ ...

.. EXAMPLE END

Columns and Quantities
======================

.. EXAMPLE START: Handling Astropy Column and Quantity Objects within Tables

``astropy`` `~astropy.units.Quantity` objects can be handled within tables in
two complementary ways. The first method stores the `~astropy.units.Quantity`
object natively within the table via the "mixin" column protocol. See the
sections on :ref:`mixin_columns` and :ref:`quantity_and_qtable` for details,
but in brief, the key difference is using the `~astropy.table.QTable` class to
indicate that a `~astropy.units.Quantity` should be stored natively within the
table::

  >>> from astropy.table import QTable
  >>> from astropy import units as u
  >>> t = QTable()
  >>> t['velocity'] = [3, 4] * u.m / u.s
  >>> type(t['velocity'])
  <class 'astropy.units.quantity.Quantity'>

For new code that is quantity-aware we recommend using `~astropy.table.QTable`,
but this may not be possible in all situations (particularly when interfacing
with legacy code that does not handle quantities) and there are
:ref:`details_and_caveats` that apply. In this case, use the
`~astropy.table.Table` class, which will convert a `~astropy.units.Quantity` to
a `~astropy.table.Column` object with a ``unit`` attribute::

  >>> from astropy.table import Table
  >>> t = Table()
  >>> t['velocity'] = [3, 4] * u.m / u.s
  >>> type(t['velocity'])
  <class 'astropy.table.column.Column'>
  >>> t['velocity'].unit
  Unit("m / s")

To learn more about using standard `~astropy.table.Column` objects with defined
units, see the :ref:`columns_with_units` section.

.. EXAMPLE END

.. _Table-like Objects:

Table-like Objects
==================

In order to improve interoperability between different table classes, an
``astropy`` |Table| object can be created directly from any other table-like
object that provides an ``__astropy_table__()`` method. In this case the
``__astropy_table__()`` method will be called as follows::

  >>> data = SomeOtherTableClass({'a': [1, 2], 'b': [3, 4]})  # doctest: +SKIP
  >>> t = QTable(data, copy=False, strict_copy=True)  # doctest: +SKIP

Internally the following call will be made to ask the ``data`` object
to return a representation of itself as an ``astropy`` |Table|, respecting
the ``copy`` preference of the original call to ``QTable()``::

  data.__astropy_table__(cls, copy, **kwargs)

Here ``cls`` is the |Table| class or subclass that is being instantiated
(|QTable| in this example), ``copy`` indicates whether a copy of the values in
``data`` should be provided, and ``**kwargs`` are any extra keyword arguments
which are not valid |Table| ``_init_()`` keyword arguments. In the example
above, ``strict_copy=True`` would end up in ``**kwargs`` and get passed to
``__astropy_table__()``.

If ``copy`` is `True` then the ``__astropy_table__()`` method must ensure that
a copy of the original data is returned. If ``copy`` is `False` then a
reference to the table data should be returned if possible. If it is not
possible (e.g., the original data are in a Python list or must be otherwise
transformed in memory) then ``__astropy_table__()`` method is free to either
return a copy or else raise an exception. This choice depends on the preference
of the implementation. The implementation might choose to allow an additional
keyword argument (e.g., ``strict_copy`` which gets passed via ``**kwargs``) to
control the behavior in this case.

As a concise example, imagine a dict-based table class. (Note that |Table|
already can be initialized from a dict-like object, so this is a bit contrived
but does illustrate the principles involved.) Please pay attention to the
method signature::

  def __astropy_table__(self, cls, copy, **kwargs):

Your class implementation of this must use the ``**kwargs`` technique for
catching keyword arguments at the end. This is to ensure future compatibility
in case additional keywords are added to the internal ``table =
data.__astropy_table__(cls, copy)`` call. Including ``**kwargs`` will prevent
breakage in this case. ::

  class DictTable(dict):
      """
      Trivial "table" class that just uses a dict to hold columns.
      This does not actually implement anything useful that makes
      this a table.

      The non-standard ``strict_copy=False`` keyword arg here will be passed
      via the **kwargs of Table __init__().
      """

      def __astropy_table__(self, cls, copy, strict_copy=False, **kwargs):
          """
          Return an astropy Table of type ``cls``.

          Parameters
          ----------
          cls : type
               Astropy ``Table`` class or subclass.
          copy : bool
               Copy input data (True) or return a reference (False).
          strict_copy : bool, optional
               Raise an exception if copy is False but reference is not
               possible.
          **kwargs : dict, optional
               Additional keyword args (ignored currently).
          """
          if kwargs:
              warnings.warn(f'unexpected keyword args {kwargs}')

          cols = list(self.values())
          names = list(self.keys())

          # If returning a reference to existing data (copy=False) and
          # strict_copy=True, make sure that each column is a numpy ndarray.
          # If a column is a Python list or tuple then it must be copied for
          # representation in an astropy Table.

          if not copy and strict_copy:
              for name, col in zip(names, cols):
                  if not isinstance(col, np.ndarray):
                      raise ValueError(f'cannot have copy=False because column {name} is '
                                       'not an ndarray')

          return cls(cols, names=names, copy=copy)
.. _utils-data:

***************************************************
Downloadable Data Management (`astropy.utils.data`)
***************************************************

Introduction
============

A number of Astropy's tools work with data sets that are either awkwardly
large (e.g., `~astropy.coordinates.solar_system_ephemeris`) or
regularly updated (e.g., `~astropy.utils.iers.IERS_B`) or both
(e.g., `~astropy.utils.iers.IERS_A`). This kind of
data - authoritative data made available on the Web, and possibly updated
from time to time - is reasonably common in astronomy. The Astropy Project therefore
provides some tools for working with such data.

The primary tool for this is the ``astropy`` *cache*. This is a repository of
downloaded data, indexed by the URL where it was obtained. The tool
`~astropy.utils.data.download_file` and various other things built upon it can
use this cache to request the contents of a URL, and (if they choose to use the
cache) the data will only be downloaded if it is not already present in the
cache. The tools can be instructed to obtain a new copy of data
that is in the cache but has been updated online.

The ``astropy`` cache is stored in a centralized place (on Linux machines by
default it is ``$HOME/.astropy/cache``; see :ref:`astropy_config` for
more details).  You can check its location on your machine::

   >>> import astropy.config.paths
   >>> astropy.config.paths.get_cache_dir()  # doctest: +SKIP
   '/home/burnell/.astropy/cache'

This centralization means that the cache is persistent and shared between all
``astropy`` runs in any virtualenv by one user on one machine (possibly more if
your home directory is shared between multiple machines).  This can
dramatically accelerate ``astropy`` operations and reduce the load on servers,
like those of the IERS, that were not designed for heavy Web traffic. If you
find the cache has corrupted or outdated data in it, you can remove an entry or
clear the whole thing with `~astropy.utils.data.clear_download_cache`.

The files in the cache directory are named according to a cryptographic hash of
their URL (currently MD5, so in principle malevolent entities can cause
collisions, though the security risks this poses are marginal at most). The
modification times on these files normally indicate when they were last
downloaded from the Internet.

Usage Within Astropy
====================

For the most part, you can ignore the caching mechanism and rely on
``astropy`` to have the correct data when you need it. For example, precise
time conversions and sky locations need measured tables of the Earth's
rotation from the IERS. The table `~astropy.utils.iers.IERS_Auto` provides
the infrastructure for many of these calculations. It makes available
Earth rotation parameters, and if you request them for a time more recent
than its tables cover, it will download updated tables from the IERS. So
for example asking what time it is in UT1 (a timescale that reflects the
irregularity of the Earth's rotation) probably triggers a download of the
IERS data::

   >>> from astropy.time import Time
   >>> Time.now().ut1  # doctest: +SKIP
   Downloading https://maia.usno.navy.mil/ser7/finals2000A.all
   |============================================| 3.2M/3.2M (100.00%)         1s
   <Time object: scale='ut1' format='datetime' value=2019-09-22 08:39:03.812731>

But running it a second time does not require any new download::

   >>> Time.now().ut1  # doctest: +SKIP
   <Time object: scale='ut1' format='datetime' value=2019-09-22 08:41:21.588836>

Some data is also made available from the `Astropy data server`_ either
for use within ``astropy`` or for your convenience. These are available more
conveniently with the ``get_pkg_data_*`` functions::

   >>> from astropy.utils.data import get_pkg_data_contents
   >>> print(get_pkg_data_contents("coordinates/sites-un-ascii"))  # doctest: +SKIP
   # these are all mappings from the name in sites.json (which is ASCII-only) to the "true" unicode names
   TUBITAK->TBTAK

.. note::

    Sometimes when downloading files from internet resources secured with
    TLS/SSL, you may get an exception regarding a certificate verification
    error.  Typically this indicates that Python could not find an
    up-to-date collection of `root certificates`_ on your system.  This is
    especially common on Windows.  This problem can usually be resolved
    by installing the `certifi`_ package, which Astropy will use if
    available to verify remote connections.  In rare cases, certificate
    verification may still fail if the remote server is misconfigured (e.g.,
    with expired certificates).  In this case, you may pass the
    ``allow_insecure=True`` argument to
    :func:`~astropy.utils.data.download_file` to allow the download with a
    warning instead (not recommended unless you understand the `potential
    risks <https://en.wikipedia.org/wiki/Man-in-the-middle_attack>`_).


Usage From Outside Astropy
==========================

Users of ``astropy`` can also make use of ``astropy``'s caching and downloading
mechanism. In its simplest form, this amounts to using
`~astropy.utils.data.download_file` with the ``cache=True`` argument to obtain
their data, from the cache if the data is there::

   >>> from astropy.utils.iers import IERS_B_URL, IERS_B
   >>> from astropy.utils.data import download_file
   >>> IERS_B.open(download_file(IERS_B_URL, cache=True))["year","month","day"][-3:]  # doctest: +SKIP
    <IERS_B length=3>
    year month  day
   int64 int64 int64
   ----- ----- -----
    2019     8     4
    2019     8     5
    2019     8     6

If users want to update the cache to a newer version of the
data (note that here the data was already up to date; users
will have to decide for themselves when to obtain new versions),
they can use the ``cache='update'`` argument::

   >>> IERS_B.open(download_file(IERS_B_URL,
   ...                           cache='update')
   ... )["year","month","day"][-3:]  # doctest: +SKIP
   Downloading http://hpiers.obspm.fr/iers/eop/eopc04/eopc04_IAU2000.62-now
   |=========================================| 3.2M/3.2M (100.00%)         0s
   <IERS_B length=3>
    year month  day
   int64 int64 int64
   ----- ----- -----
    2019     8    18
    2019     8    19
    2019     8    20

If they are concerned that the primary source of the data may be
overloaded or unavailable, they can use the ``sources`` argument
to provide a list of sources to attempt downloading from, in order.
This need not include the original source. Regardless, the data
will be stored in the cache under the original URL requested::

   >>> f = download_file("ftp://ssd.jpl.nasa.gov/pub/eph/planets/bsp/de405.bsp",
   ...     cache=True,
   ...     sources=['https://data.nanograv.org/static/data/ephem/de405.bsp',
   ...              'ftp://ssd.jpl.nasa.gov/pub/eph/planets/bsp/de405.bsp'])  # doctest: +SKIP
   Downloading ftp://ssd.jpl.nasa.gov/pub/eph/planets/bsp/de405.bsp from https://data.nanograv.org/static/data/ephem/de405.bsp
   |========================================|  65M/ 65M (100.00%)        19s

.. _Astropy data server: https://www.astropy.org/astropy-data/
.. _root certificates: https://en.wikipedia.org/wiki/Root_certificate
.. _certifi: https://pypi.org/project/certifi/

Cache Management
================

Because the cache is persistent, it is possible for it to become inconveniently
large, or become filled with irrelevant data. While it is simply a directory on
disk, each file is supposed to represent the contents of a URL, and many URLs
do not make acceptable on-disk filenames (for example, containing troublesome
characters like ":" and "~"). There is reason to worry that multiple
``astropy`` processes accessing the cache simultaneously might lead to cache
corruption. The data is therefore stored in a subdirectory named after the hash
of the URL, and write access is handled in a way that is resistant to
concurrency problems. So access to the cache is more convenient with a few
helpers provided by `~astropy.utils.data`.

If your cache starts behaving oddly you can use
`~astropy.utils.data.check_download_cache` to examine your cache contents and
raise an exception if it finds any anomalies.  If a single file is undesired or
damaged, it can be removed by calling
`~astropy.utils.data.clear_download_cache` with an argument that is the URL it
was obtained from, the filename of the downloaded file, or the hash of its
contents. Should the cache ever become badly corrupted,
`~astropy.utils.data.clear_download_cache` with no arguments will simply delete
the whole directory, freeing the space and removing any inconsistent data. Of
course, if you remove data using either of these tools, any processes currently
using that data may be disrupted (or, under Windows, deleting the cache may not
be possible until those processes terminate). So use
`~astropy.utils.data.clear_download_cache` with care.

To check the total space occupied by the cache, use
`~astropy.utils.data.cache_total_size`. The contents of the cache can be
listed with `~astropy.utils.data.get_cached_urls`, and the presence of a
particular URL in the cache can be tested with
`~astropy.utils.data.is_url_in_cache`. More general manipulations can be
carried out using `~astropy.utils.data.cache_contents`, which returns a
`~dict` mapping URLs to on-disk filenames of their contents.

If you want to transfer the cache to another computer, or preserve its contents
for later use, you can use the functions `~astropy.utils.data.export_download_cache` to
produce a ZIP file listing some or all of the cache contents, and
`~astropy.utils.data.import_download_cache` to load the ``astropy`` cache from such a
ZIP file.

The Astropy cache has changed format - once in the Python 2 to Python
3 transition, and again before Astropy version 4.0.2 to resolve some
concurrency problems that arose on some compute clusters. Each version of the
cache is in its own subdirectory, so the old versions do not interfere with the
new versions and vice versa, but their contents are not used by this version
and are not cleared by `~astropy.utils.data.clear_download_cache`. To remove
these old cache directories, you can run::

   >>> from shutil import rmtree
   >>> from os.path import join
   >>> from astropy.config.paths import get_cache_dir
   >>> rmtree(join(get_cache_dir(), 'download', 'py2'), ignore_errors=True)  # doctest: +SKIP
   >>> rmtree(join(get_cache_dir(), 'download', 'py3'), ignore_errors=True)  # doctest: +SKIP

Using Astropy With Limited or No Internet Access
================================================

You might want to use ``astropy`` on a telescope control machine behind a strict
firewall. Or you might be running continuous integration (CI) on your ``astropy``
server and want to avoid hammering astronomy servers on every pull request for
every architecture. Or you might not have access to US government or military
web servers. Whichever is the case, you may need to avoid ``astropy`` needing data
from the Internet. There is no simple and complete solution to this problem at
the moment, but there are tools that can help.

Exactly which external data your project depends on will depend on what parts
of ``astropy`` you use and how. The most general solution is to use a computer that
can access the Internet to run a version of your calculation that pulls in all of
the data files you will require, including sufficiently up-to-date versions of
files like the IERS data that update regularly. Then once the cache on this
connected machine is loaded with everything necessary, transport the cache
contents to your target machine by whatever means you have available, whether
by copying via an intermediate machine, portable disk drive, or some other
tool. The cache directory itself is somewhat portable between machines of the
same UNIX flavour; this may be sufficient if you can persuade your CI system to
cache the directory between runs. For greater portability, though, you can
simply use `~astropy.utils.data.export_download_cache` and
`~astropy.utils.data.import_download_cache`, which are portable and will allow
adding files to an existing cache directory.

If your application needs IERS data specifically, you can download the
appropriate IERS table, covering the appropriate time span, by any means you
find convenient. You can then load this file into your application and use the
resulting table rather than `~astropy.utils.iers.IERS_Auto`. In fact, the IERS
B table is small enough that a version (not necessarily recent) is bundled with
``astropy`` as ``astropy.utils.iers.IERS_B_FILE``. Using a specific non-automatic
table also has the advantage of giving you control over exactly which version
of the IERS data your application is using. See also :ref:`iers-working-offline`.

If your issue is with certain specific servers, even if they are the ones
``astropy`` normally uses, if you can anticipate exactly which files will be needed
(or just pick up after ``astropy`` fails to obtain them) and make those files
available somewhere else, you can request they be downloaded to the cache
using `~astropy.utils.data.download_file` with the ``sources`` argument set
to locations you know do work. You can also set ``sources`` to an empty list
to ensure that `~astropy.utils.data.download_file` does not attempt to use
the Internet at all.

If you have a particular URL that is giving you trouble, you can download it
using some other tool (e.g., ``wget``), possibly on another machine, and
then use `~astropy.utils.data.import_file_to_cache`.

Astropy Data and Clusters
=========================

Astronomical calculations often require the use of a large number of different
processes on different machines with a shared home filesystem. This can pose
certain complexities. In particular, if the many different processes attempt to
download a file simultaneously this can overload a server or trigger security
systems. The parallel access to the home directory can also trigger concurrency
problems in the Astropy data cache, though we have tried to minimize these. We
therefore recommend the following guidelines:

 * Write a simple script that sets ``astropy.utils.iers.conf.auto_download = True``
   and then accesses all cached resources your code will need, including source name
   lookups and IERS tables. Run it on the head node from time to time (frequently
   enough to beat the timeout ``astropy.utils.iers.conf.auto_max_age``, which
   defaults to 30 days) to ensure all data is up to date.

 * Make an Astropy config file (see :ref:`astropy_config`) that sets
   ``astropy.utils.iers.conf.auto_download = False`` so that the worker jobs will
   not suddenly notice an out-of-date table all at once and frantically attempt
   to download it.

 * Optionally, in this file, set ``astropy.utils.data.conf.allow_internet = False`` to
   prevent any attempt to download any file from the worker nodes; if you do this,
   you will need to override this setting in your script that does the actual
   downloading.

Now your worker nodes should not need to obtain anything from the Internet and
all should run smoothly.
.. _utils-iers:

***************************************
IERS data access (`astropy.utils.iers`)
***************************************

Introduction
============

The `~astropy.utils.iers` package provides access to the tables provided by
the International Earth Rotation and Reference Systems (IERS) service, in
particular files allowing interpolation of published UT1-UTC and polar motion
values for given times.  The UT1-UTC values are used in `astropy.time` to
provide UT1 values, and the polar motions are used in `astropy.coordinates` to
determine Earth orientation for celestial-to-terrestrial coordinate
transformations.

.. note:: The package also provides machinery to track leap seconds.  Since it
          generally should not be necessary to deal with those by hand, this
          is not discussed below.  For details, see the documentation of
          `~astropy.utils.iers.LeapSeconds`.

Getting started
===============

Starting with astropy 1.2, the latest IERS values (which include approximately
one year of predictive values) are automatically downloaded from the IERS
service when required.  This happens when a time or coordinate transformation
needs a value which is not already available via the download cache.  In most
cases there is no need for invoking the `~astropy.utils.iers` classes oneself,
but it is useful to understand the situations when a download will occur
and how this can be controlled.

Basic usage
-----------

By default, the IERS data are managed via instances of the
:class:`~astropy.utils.iers.IERS_Auto` class.  These instances are created
internally within the relevant time and coordinate objects during
transformations.  If the astropy data cache does not have the required IERS
data file then astropy will request the file from the IERS service.  This will
occur the first time such a transform is done for a new setup or on a new
machine.  Here is an example that shows the typical download progress bar::

  >>> from astropy.time import Time
  >>> t = Time('2016:001')
  >>> t.ut1  # doctest: +SKIP
  Downloading https://maia.usno.navy.mil/ser7/finals2000A.all
  |==================================================================| 3.0M/3.0M (100.00%)         6s
  <Time object: scale='ut1' format='yday' value=2016:001:00:00:00.082>

Note that you can forcibly clear the download cache as follows::

  >>> from astropy.utils.data import clear_download_cache
  >>> clear_download_cache()  # doctest: +SKIP

The default IERS data used automatically is updated by the service every 7 days
and includes transforms dating back to 1973-01-01.

.. note:: The :class:`~astropy.utils.iers.IERS_Auto` class contains machinery
    to ensure that the IERS table is kept up to date by auto-downloading the
    latest version as needed.  This means that the IERS table is assured of
    having the state-of-the-art definitive and predictive values for Earth
    rotation.  As a user it is **your responsibility** to understand the
    accuracy of IERS predictions if your science depends on that.  If you
    request ``UT1-UTC`` or polar motions for times beyond the range of IERS
    table data then the nearest available values will be provided.


Configuration parameters
------------------------

There are a number of IERS configuration parameters in `astropy.utils.iers.Conf`
that control the behavior of the automatic IERS downloading. Three of the most
important to consider are the following:

  auto_download:
    Enable auto-downloading of the latest IERS data.  If set to ``False`` then
    the local IERS-B file will be used by default (even if the full IERS file
    with predictions was already downloaded and cached).  This parameter also
    controls whether internet resources will be queried to update the leap
    second table if the installed version is out of date.

  auto_max_age:
    Maximum age of predictive data before auto-downloading (days).  See
    next section for details. (default=30)

  remote_timeout:
    Remote timeout downloading IERS file data (seconds)


Auto refresh behavior
---------------------

The first time that one attempts a time or coordinate transformation that
requires IERS data, the latest version of the IERS table (from 1973 through
one year into the future) will be downloaded and stored in the astropy cache.

Transformations will then use the cached data file if possible.  However, the
``IERS_Auto`` table is automatically updated in place from the network if the
following two conditions a met when the table is queried for ``UT1-UTC`` or
polar motion values:

- Any of the requested IERS values are *predictive*, meaning that they have
  been extrapolated into the future with a model that is fit to measured data.
  The IERS table contains approximately one year of predictive data from the
  time it is created.
- The first predictive values in the table are at least ``conf.auto_max_age
  days`` old relative to the current actual time (i.e. ``Time.now()``).  This
  means that the IERS table is out of date and a newer version can be found on
  the IERS service.

The IERS Service provides the default online table
(set by ``astropy.utils.iers.IERS_A_URL``) and updates the content
once each 7 days.  The default value of ``auto_max_age`` is 30 days to avoid
unnecessary network access, but one can reduce this to as low as 10 days.

.. _iers-working-offline:

Working offline
---------------

If you are working without an internet connection and doing transformations
that require IERS data, there are a couple of options.

**Disable auto downloading**

Here you can do::

  >>> from astropy.utils import iers
  >>> iers.conf.auto_download = False  # doctest: +SKIP

In this case any transforms will use the bundled IERS-B data which covers
the time range from 1962 to just before the astropy release date.  Any
transforms outside of this range will not be allowed.

**Set the auto-download max age parameter**

*Only do this if you understand what you are doing, THIS CAN GIVE INACCURATE
ANSWERS!* Assuming you have previously been connected to the internet and have
downloaded and cached the IERS auto values previously, then do the following::

  >>> iers.conf.auto_max_age = None  # doctest: +SKIP

This disables the check of whether the IERS values are sufficiently recent, and
all the transformations (even those outside the time range of available IERS
data) will succeed with at most warnings.

Direct table access
-------------------

In most cases the automatic interface will suffice, but you may need to
directly load and manipulate IERS tables.  IERS-B values are provided as part
of astropy and can be used to calculate time offsets and polar motion
directly, or set up for internal use in further time and coordinate
transformations.  For example::

  >>> from astropy.utils import iers
  >>> t = Time('2010:001')
  >>> iers_b = iers.IERS_B.open()
  >>> iers_b.ut1_utc(t)  # doctest: +FLOAT_CMP
  <Quantity 0.114033 s>
  >>> iers.earth_orientation_table.set(iers_b)
  <ScienceState earth_orientation_table: <IERS_B length=...>...>
  >>> t.ut1.iso
  '2010-01-01 00:00:00.114'

Instead of local copies of IERS files, one can also download them, using
``iers.IERS_A_URL`` (or ``iers.IERS_A_URL_MIRROR``) and ``iers.IERS_B_URL``,
and then use those for future time and coordinate transformations (in this
example, just for a single calculation, by using
`~astropy.utils.iers.earth_orientation_table` as a context manager)::

  >>> iers_a = iers.IERS_A.open(iers.IERS_A_URL)  # doctest: +SKIP
  >>> with iers.earth_orientation_table.set(iers_a):  # doctest: +SKIP
  ...     print(t.ut1.iso)
  2010-01-01 00:00:00.114

To reset to the default, pass in `None` (which is equivalent to passing in
``iers.IERS_Auto.open()``)::

  >>> iers.earth_orientation_table.set(None)  # doctest: +REMOTE_DATA
  <ScienceState earth_orientation_table: <IERS...>...>

To see the internal IERS data that gets used in astropy you can do the
following::

  >>> dat = iers.earth_orientation_table.get()  # doctest: +REMOTE_DATA
  >>> type(dat)  # doctest: +REMOTE_DATA
  <class 'astropy.utils.iers.iers.IERS...'>
  >>> dat  # doctest: +SKIP
  <IERS_Auto length=16196>
   year month  day    MJD   PolPMFlag_A ... UT1Flag    PM_x     PM_y   PolPMFlag
                       d                ...           arcsec   arcsec
  int64 int64 int64 float64     str1    ... unicode1 float64  float64   unicode1
  ----- ----- ----- ------- ----------- ... -------- -------- -------- ---------
     73     1     2 41684.0           I ...        B    0.143    0.137         B
     73     1     3 41685.0           I ...        B    0.141    0.134         B
     73     1     4 41686.0           I ...        B    0.139    0.131         B
     73     1     5 41687.0           I ...        B    0.137    0.128         B
    ...   ...   ...     ...         ... ...      ...      ...      ...       ...
     17     5     2 57875.0           P ...        P 0.007211  0.44884         P
     17     5     3 57876.0           P ...        P 0.008757 0.450321         P
     17     5     4 57877.0           P ...        P 0.010328 0.451777         P
     17     5     5 57878.0           P ...        P 0.011924 0.453209         P
     17     5     6 57879.0           P ...        P 0.013544 0.454617         P

The explanation for most of the columns can be found in the file named
``iers.IERS_A_README``.  The important columns of this table are MJD, UT1_UTC,
UT1Flag, PM_x, PM_y, PolPMFlag::

  >>> dat['MJD', 'UT1_UTC', 'UT1Flag', 'PM_x', 'PM_y', 'PolPMFlag']  # doctest: +SKIP
  <IERS_Auto length=16196>
    MJD    UT1_UTC   UT1Flag    PM_x     PM_y   PolPMFlag
     d        s                arcsec   arcsec
  float64  float64   unicode1 float64  float64   unicode1
  ------- ---------- -------- -------- -------- ---------
  41684.0     0.8075        B    0.143    0.137         B
  41685.0     0.8044        B    0.141    0.134         B
  41686.0     0.8012        B    0.139    0.131         B
  41687.0     0.7981        B    0.137    0.128         B
      ...        ...      ...      ...      ...       ...
  57875.0 -0.6545408        P 0.007211  0.44884         P
  57876.0 -0.6559528        P 0.008757 0.450321         P
  57877.0 -0.6573705        P 0.010328 0.451777         P
  57878.0 -0.6587712        P 0.011924 0.453209         P
  57879.0  -0.660187        P 0.013544 0.454617         P
.. _utils:

************************************************
Astropy Core Package Utilities (`astropy.utils`)
************************************************

Introduction
============

The `astropy.utils` package contains general-purpose utility functions and
classes.  Examples include data structures, tools for downloading and caching
from URLs, and version intercompatibility functions.

This functionality is not astronomy-specific, but is intended primarily for
use by Astropy developers. It is all safe for users to use, but the functions
and classes are typically more complicated or specific to a particular need of
Astropy.

Because of the mostly standalone and grab-bag nature of these utilities, they
are generally best understood through their docstrings, and hence this
documentation generally does not have detailed sections like the other packages.
The exceptions are below:

.. toctree::
   :maxdepth: 1

   iers
   data
   masked/index

.. note:: The ``astropy.utils.compat`` subpackage is not included in this
    documentation. It contains utility modules for compatibility with
    older/newer versions of python and numpy, as well as including some
    bugfixes for the stdlib that are important for ``astropy``. It is recommended
    that developers at least glance over the source code for this subpackage,
    but most of it cannot be reliably included here because of the large
    amount of version-specific code it contains. Its content is solely for
    internal use of ``astropy`` and subject to changes without deprecations.
    Do not use it in external packages or code.

Reference/API
=============
.. module:: astropy.utils

.. automodapi:: astropy.utils.codegen
    :no-inheritance-diagram:

.. automodapi:: astropy.utils.collections
    :no-inheritance-diagram:

.. automodapi:: astropy.utils.console
    :no-inheritance-diagram:

.. automodapi:: astropy.utils.data_info
    :no-inheritance-diagram:

.. automodapi:: astropy.utils.decorators
    :no-inheritance-diagram:

.. automodapi:: astropy.utils.diff
    :no-inheritance-diagram:

.. automodapi:: astropy.utils.exceptions
    :no-inheritance-diagram:

.. automodapi:: astropy.utils.iers
    :no-inheritance-diagram:

.. automodapi:: astropy.utils.introspection
    :no-inheritance-diagram:

.. automodapi:: astropy.utils.metadata
    :no-inheritance-diagram:

.. automodapi:: astropy.utils.misc
    :no-inheritance-diagram:

.. automodapi:: astropy.utils.parsing
    :no-inheritance-diagram:

.. automodapi:: astropy.utils.state
    :no-inheritance-diagram:

.. automodapi:: astropy.utils.shapes
    :no-inheritance-diagram:


File Downloads
--------------

.. automodapi:: astropy.utils.data
    :no-inheritance-diagram:

XML
---
The ``astropy.utils.xml.*`` modules provide various
`XML <http://www.w3.org/XML/>`_ processing tools.

.. automodapi:: astropy.utils.xml.check
    :no-inheritance-diagram:
    :headings: ^"

.. automodapi:: astropy.utils.xml.iterparser
    :no-inheritance-diagram:
    :headings: ^"

.. automodapi:: astropy.utils.xml.unescaper
    :no-inheritance-diagram:
    :headings: ^"

.. automodapi:: astropy.utils.xml.validate
    :no-inheritance-diagram:
    :headings: ^"

.. automodapi:: astropy.utils.xml.writer
    :no-inheritance-diagram:
    :headings: ^"
.. _utils-masked:

**************************************
Masked Values (`astropy.utils.masked`)
**************************************

Often, data sets are incomplete or corrupted and it would be handy to be able
to mask certain values.  Astropy provides a |Masked| class to help represent
such data sets.

.. warning:: |Masked| is experimental! While we hope basic usage will remain
   similar, we are not yet sure whether it will not be necessary to change it
   to make things work throughout Astropy. This also means that comments and
   suggestions for improvements are especially welcome!

.. note:: |Masked| is similar to Numpy's :class:`~numpy.ma.MaskedArray`,
   but it supports subclasses much better and also has some important
   :ref:`differences in behaviour <utils-masked-vs-numpy-maskedarray>`.
   As a result, the behaviour of functions inside `numpy.ma` is poorly
   defined, and one should instead use regular ``numpy`` functions, which
   are overridden to work properly with masks (with non-obvious
   choices documented in `astropy.utils.masked.function_helpers`; please
   report numpy functions that do not work properly with |Masked| values!).

Usage
=====

Astropy |Masked| instances behave like `~numpy.ndarray` or subclasses such as
|Quantity| but with a mask associated, which is propagated in operations such
as addition, etc.::

  >>> import numpy as np
  >>> from astropy import units as u
  >>> from astropy.utils.masked import Masked
  >>> ma = Masked([1., 2., 3.], mask=[False, False, True])
  >>> ma
  MaskedNDArray([1., 2., ])
  >>> mq = ma * u.m
  >>> mq + 25 * u.cm
  <MaskedQuantity [1.25, 2.25,  ] m>

You can get the values without the mask using
`~astropy.utils.masked.Masked.unmasked`, or, if you need to control what
should be substituted for any masked values, with
:meth:`~astropy.utils.masked.Masked.filled`::

  >>> mq.unmasked
  <Quantity [1., 2., 3.] m>
  >>> mq.filled(fill_value=-75*u.cm)
  <Quantity [ 1.  ,  2.  , -0.75] m>

For reductions such as sums, the mask propagates as if the sum was
done directly::

  >>> ma = Masked([[0., 1.], [2., 3.]], mask=[[False, True], [False, False]])
  >>> ma.sum(axis=-1)
  MaskedNDArray([, 5.])
  >>> ma.sum()
  MaskedNDArray()

You might wonder why masked elements are propagated, instead of just being
skipped (as is done in `~numpy.ma.MaskedArray`; see :ref:`below
<utils-masked-vs-numpy-maskedarray>`).  The rationale is that this leaves a
sum which is generally not useful unless one knows the number of masked
elements.  In contrast, for sample properties such as the mean, for which the
number of elements are counted, it seems natural to simply omit the masked
elements from the calculation::

  >> ma.mean(-1)
  MaskedNDArray([0.0, 2.5])


.. _utils-masked-vs-numpy-maskedarray:

Differences from `numpy.ma.MaskedArray`
=======================================

|Masked| differs from `~numpy.ma.MaskedArray` in a number of ways.  In usage,
a major difference is that most operations act on the masked values, i.e., no
effort is made to preserve values.  For instance, compare::

  >>> np_ma = np.ma.MaskedArray([1., 2., 3.], mask=[False, True, False])
  >>> (np_ma + 1).data
  array([2., 2., 4.])
  >>> (Masked(np_ma) + 1).unmasked
  array([2., 3., 4.])

The main reason for this decision is that for some masked subclasses, like
masked |Quantity|, keeping the original value makes no sense (e.g., consider
dividing a length by a time: if the unit of a masked quantity is changing, why
should its value not change?).  But it also helps to keep the implementation
considerably simpler, as the |Masked| class now primarily has to deal with
propagating the mask rather than deciding what to do with values.

A second difference is that for reductions, the mask propagates as it would
have if the operations were done on the individual elements::

  >>> np_ma.prod()
  3.0
  >>> np_ma[0] * np_ma[1] * np_ma[2]
  masked
  >>> Masked(np_ma).prod()
  MaskedNDArray()

The rationale for this becomes clear again by thinking about subclasses like a
masked |Quantity|.  For instance, consider an array ``s`` of lengths with
shape ``(N, 3)``, in which the last axis represents width, height, and depth.
With this, you could compute corresponding volumes by taking the product of
the values in the last axis, ``s.prod(axis=-1)``. But if masked elements were
skipped, the physical dimension of entries in the result would depend how many
elements were masked, which is something |Quantity| could not represent (and
would be rather surprising!).  As noted above, however, masked elements are
skipped for operations for which this is well defined, such as for getting the
mean and other sample properties such as the variance and standard deviation.

A third difference is more conceptual.  For `~numpy.ma.MaskedArray`, the
instance that is created is a masked version of the unmasked instance, i.e.,
`~numpy.ma.MaskedArray` remembers that is has wrapped a subclass like
|Quantity|, but does not share any of its methods.  Hence, even though the
resulting class looks reasonable at first glance, it does not work as expected::

  >>> q = [1., 2.] * u.m
  >>> np_mq = np.ma.MaskedArray(q, mask=[False, True])
  >>> np_mq
  masked_Quantity(data=[1.0, --],
                  mask=[False,  True],
            fill_value=1e+20)
  >>> np_mq.unit
  Traceback (most recent call last):
  ...
  AttributeError: 'MaskedArray' object has no attribute 'unit'
  >>> np_mq / u.s
  <Quantity [1., 2.] 1 / s>

In contrast, |Masked| is always wrapped around the data properly, i.e., a
``MaskedQuantity`` is a quantity which has masked values, but with a unit that
is never masked.  Indeed, one can see this from the class hierarchy::

  >>> mq.__class__.__mro__
  (<class 'astropy.utils.masked.core.MaskedQuantity'>,
   <class 'astropy.units.quantity.Quantity'>,
   <class 'astropy.utils.masked.core.MaskedNDArray'>,
   <class 'astropy.utils.masked.core.Masked'>,
   <class 'astropy.utils.shapes.NDArrayShapeMethods'>,
   <class 'numpy.ndarray'>,
   <class 'object'>)

This choice has made the implementation much simpler: |Masked| only has to
worry about how to deal with masked values, while |Quantity| can worry just
about unit propagation, etc.  Indeed, an experiment showed that applying
|Masked| to `~astropy.table.Column` (which is a subclass of `~numpy.ndarray`),
the result is a new ``MaskedColumn`` that "just works", with no need for the
overrides and special-casing that were needed to make `~numpy.ma.MaskedArray`
work with `~astropy.table.Column`.  (Because the behaviour does change
somewhat, however, we chose not to replace the existing implementation.)

In some respects, rather than think of |Masked| as similar to
`~numpy.ma.MaskedArray`, it may be more useful to think of |Masked| as similar
to marking bad elements in arrays with NaN (not-a-number).  Like those NaN,
the mask just propagates, except that for some operations like taking the mean
the equivalence of `~numpy.nanmean` is used.

Reference/API
=============

.. automodapi:: astropy.utils.masked

.. automodapi:: astropy.utils.masked.function_helpers
:orphan:

`What's New in Astropy 0.2? <https://docs.astropy.org/en/v0.2/whatsnew/0.2.html>`__
:orphan:

`What's New in Astropy 3.2? <https://docs.astropy.org/en/v3.2/whatsnew/3.2.html>`__
:orphan:

`What's New in Astropy 1.2? <https://docs.astropy.org/en/v1.2/whatsnew/1.2.html>`__
:orphan:

`What's New in Astropy 1.3? <https://docs.astropy.org/en/v1.3/whatsnew/1.3.html>`__
:orphan:

`What's New in Astropy 1.0? <https://docs.astropy.org/en/v1.0/whatsnew/1.0.html>`__
:orphan:

`What's New in Astropy 0.1? <https://docs.astropy.org/en/v0.2/whatsnew/0.1.html>`__
:orphan:

`What's New in Astropy 0.4? <https://docs.astropy.org/en/v0.4/whatsnew/0.4.html>`__
:orphan:

`What's New in Astropy 4.1?
<https://docs.astropy.org/en/v4.1/whatsnew/4.1.html>`__
:orphan:

`What's New in Astropy 4.3?
<https://docs.astropy.org/en/v4.3post1/whatsnew/4.3.html>`__
:orphan:

`What's New in Astropy 3.1? <https://docs.astropy.org/en/v3.1.2/whatsnew/3.1.html>`__
:orphan:

`What's New in Astropy 0.3? <https://docs.astropy.org/en/v0.3/whatsnew/0.3.html>`__
:orphan:

`What's New in Astropy 2.0? <https://docs.astropy.org/en/v2.0/whatsnew/2.0.html>`__
.. _whatsnew-5.1:

**************************
What's New in Astropy 5.1?
**************************

Overview
========

Astropy 5.1 is a major release that adds significant new functionality since
the 5.0 LTS release.

In particular, this release includes:

* :ref:`whatsnew-5.1-cosmology`
* :ref:`whatsnew-doppler-redshift-eq`

.. _whatsnew-5.1-cosmology:

Updates to ``Cosmology``
========================

:class:`~astropy.cosmology.Cosmology` is now an abstract base class,
and subclasses must override the abstract property ``is_flat``.
For :class:`~astropy.cosmology.FLRW`, ``is_flat`` checks that ``Ok0=0`` and
``Otot0=1``.

Astropy v5.0 introduced Cosmological equivalency -- with method
:meth:`~astropy.cosmology.Cosmology.is_equivalent` -- where two cosmologies may
be equivalent even if not of the same class. For example, an instance of
:class:`~astropy.cosmology.LambdaCDM` might have :math:`\Omega_0=1` and
:math:`\Omega_k=0` and therefore be flat, like ``FlatLambdaCDM``.
Now the keyword argument ``format`` is added to extend the notion of
equivalence to any Python object that can be converted to a Cosmology.

    >>> from astropy.cosmology import Planck18
    >>> tbl = Planck18.to_format("astropy.table")
    >>> Planck18.is_equivalent(tbl, format=True)
    True

The list of valid formats, e.g. the Table in this example, may be
checked with ``Cosmology.from_format.list_formats()``


.. _whatsnew-doppler-redshift-eq:

``doppler_redshift`` equivalency
================================

New :func:`astropy.units.equivalencies.doppler_redshift` is added to
provide conversion between Doppler redshift and radial velocity.

Full change log
===============

To see a detailed list of all changes in version v5.1, including changes in
API, please see the :ref:`changelog`.

Renamed/removed functionality
=============================
*********************
Major Release History
*********************

Examples in these documents are frozen in time to respect the status of the
API at the time of the release they are describing. Please refer to the
main, up-to-date documentation if you run into any issues with the
functionality highlighted in these pages.


.. toctree::
   :maxdepth: 1

   5.1

* `What's New in Astropy 5.0? <https://docs.astropy.org/en/v5.0/whatsnew/5.0.html>`__
* `What's New in Astropy 4.3? <https://docs.astropy.org/en/v4.3post1/whatsnew/4.3.html>`__
* `What's New in Astropy 4.2? <https://docs.astropy.org/en/v4.2/whatsnew/4.2.html>`__
* `What's New in Astropy 4.1? <https://docs.astropy.org/en/v4.1/whatsnew/4.1.html>`__
* `What's New in Astropy 4.0? <https://docs.astropy.org/en/v4.0/whatsnew/4.0.html>`__
* `What's New in Astropy 3.2? <https://docs.astropy.org/en/v3.2/whatsnew/3.2.html>`__
* `What's New in Astropy 3.1? <https://docs.astropy.org/en/v3.1.2/whatsnew/3.1.html>`__
* `What's New in Astropy 3.0? <https://docs.astropy.org/en/v3.0/whatsnew/3.0.html>`__
* `What's New in Astropy 2.0? <https://docs.astropy.org/en/v2.0/whatsnew/2.0.html>`__
* `What's New in Astropy 1.3? <https://docs.astropy.org/en/v1.3/whatsnew/1.3.html>`__
* `What's New in Astropy 1.2? <https://docs.astropy.org/en/v1.2/whatsnew/1.2.html>`__
* `What's New in Astropy 1.1? <https://docs.astropy.org/en/v1.1/whatsnew/1.1.html>`__
* `What's New in Astropy 1.0? <https://docs.astropy.org/en/v1.0/whatsnew/1.0.html>`__
* `What's New in Astropy 0.4? <https://docs.astropy.org/en/v0.4/whatsnew/0.4.html>`__
* `What's New in Astropy 0.3? <https://docs.astropy.org/en/v0.3/whatsnew/0.3.html>`__
* `What's New in Astropy 0.2? <https://docs.astropy.org/en/v0.2/whatsnew/0.2.html>`__
* `What's New in Astropy 0.1? <https://docs.astropy.org/en/v0.2/whatsnew/0.1.html>`__
:orphan:

  `What's New in Astropy 5.0?
  <https://docs.astropy.org/en/v5.0/whatsnew/5.0.html>`__
:orphan:

`What's New in Astropy 4.0?
<https://docs.astropy.org/en/v4.0.x/whatsnew/4.0.html>`__
:orphan:

`What's New in Astropy 3.0? <https://docs.astropy.org/en/v3.0/whatsnew/3.0.html>`__
:orphan:

`What's New in Astropy 4.2?
<https://docs.astropy.org/en/v4.2/whatsnew/4.2.html>`__
:orphan:

`What's New in Astropy 1.1? <https://docs.astropy.org/en/v1.1/whatsnew/1.1.html>`__
.. _astropy-coordinates-definitions:

Important Definitions
*********************

For reference, below, we define some key terms as they are used in
`~astropy.coordinates`, due to some ambiguities that exist in the
colloquial use of these terms. Chief among these terms is the concept
of a "coordinate system." To some members of the community, "coordinate
system" means the *representation* of a point in space (e.g., "Cartesian
coordinate system" is different from "Spherical polar coordinate
system"). Another use of "coordinate system" is to mean a unique
reference frame with a particular set of reference points (e.g., "the
ICRS coordinate system" or the "J2000 coordinate system"). This second
meaning is further complicated by the fact that such systems use quite
different ways of defining a frame.

Because of the likelihood of confusion between these meanings of
"coordinate system," `~astropy.coordinates` avoids this term wherever
possible, and instead adopts the following terms (loosely inspired by
the IAU2000 resolutions on celestial coordinate systems):

* A "Coordinate Representation" is a particular way of describing a unique
  point in a vector space. (Here, this means three-dimensional space, but future
  extensions might have different dimensionality, particularly if relativistic
  effects are desired.) Examples include Cartesian coordinates, cylindrical
  polar, or latitude/longitude spherical polar coordinates. Note that this term
  applies to the positions, *not* their velocities or other derivatives (which
  are represented as "differential" classes).

* A "Reference System" is a scheme for orienting points in a space and
  describing how they transform to other systems. Examples include the ICRS,
  equatorial coordinates with mean equinox, or the WGS84 geoid for
  latitude/longitude on the Earth.

* A "Coordinate Frame," "Reference Frame," or just "Frame" is a specific
  realization of a reference system (e.g., the ICRF, or J2000 equatorial
  coordinates). For some systems, there may be only one meaningful frame, while
  others may have many different frames (differentiated by something like a
  different equinox, or a different set of reference points).

* A "Coordinate" is a combination of all of the above that specifies a unique
  point.
.. _astropy-coordinates-satellites:

Working with Earth Satellites Using Astropy Coordinates
*******************************************************

Satellite data is normally provided in the Two-Line Element (TLE) format
(see `here <https://www.celestrak.com/NORAD/documentation/tle-fmt.php>`_
for a definition). These datasets are designed to be used in combination
with a theory for orbital propagation model to predict the positions
of satellites.

The history of such models is discussed in detail in
`Vallado et al (2006) <https://celestrak.com/publications/AIAA/2006-6753/AIAA-2006-6753-Rev2.pdf>`_
who also provide a reference implementation of the SGP4 orbital propagation
code, designed to be compatible with the TLE sets provided by the United
States Department of Defense, which are available from a source like
`Celestrak <http://celestrak.com/>`_.

The output coordinate frame of the SGP4 model is the True Equator, Mean Equinox
frame (TEME), which is one of the frames built-in to `astropy.coordinates`.
TEME is an Earth-centered inertial frame (i.e., it does not rotate with respect
to the stars). Several definitions exist; ``astropy`` uses the implementation described
in `Vallado et al (2006) <https://celestrak.com/publications/AIAA/2006-6753/AIAA-2006-6753-Rev2.pdf>`_.

Finding TEME Coordinates from TLE Data
======================================

There is currently no support in `astropy.coordinates` for computing satellite orbits
from TLE orbital element sets. Full support for handling TLE files is available in
the `Skyfield <https://rhodesmill.org/skyfield/>`_ library, but some advice for dealing
with satellite data in ``astropy`` is below.

.. EXAMPLE START Using sgp4 to get a TEME coordinate

You will need some external library to compute the position and velocity of the satellite from the
TLE orbital elements. The `SGP4 <https://pypi.org/project/sgp4/>`_ library can do this. An example
of using this library to find the  `~astropy.coordinates.TEME` coordinates of a satellite is:

.. doctest-requires:: sgp4

    >>> from sgp4.api import Satrec
    >>> from sgp4.api import SGP4_ERRORS
    >>> s = '1 25544U 98067A   19343.69339541  .00001764  00000-0  38792-4 0  9991'
    >>> t = '2 25544  51.6439 211.2001 0007417  17.6667  85.6398 15.50103472202482'
    >>> satellite = Satrec.twoline2rv(s, t)

The ``satellite`` object has a method, ``satellite.sgp4``, that will try to compute the TEME position
and velocity at a given time:

.. doctest-requires:: sgp4

    >>> from astropy.time import Time
    >>> t = Time(2458827.362605, format='jd')
    >>> error_code, teme_p, teme_v = satellite.sgp4(t.jd1, t.jd2)  # in km and km/s
    >>> if error_code != 0:
    ...     raise RuntimeError(SGP4_ERRORS[error_code])

Now that we have the position and velocity in kilometers and kilometers per second, we can create a
position in the `~astropy.coordinates.TEME` reference frame:

.. doctest-requires:: sgp4

    >>> from astropy.coordinates import TEME, CartesianDifferential, CartesianRepresentation
    >>> from astropy import units as u
    >>> teme_p = CartesianRepresentation(teme_p*u.km)
    >>> teme_v = CartesianDifferential(teme_v*u.km/u.s)
    >>> teme = TEME(teme_p.with_differentials(teme_v), obstime=t)

.. EXAMPLE END

Note how we are careful to set the observed time of the `~astropy.coordinates.TEME` frame to
the time at which we calculated satellite position.

Transforming TEME to Other Coordinate Systems
=============================================

Once you have satellite positions in `~astropy.coordinates.TEME` coordinates they can be transformed
into any `astropy.coordinates` frame.

For example, to find the overhead latitude, longitude, and height of the satellite:

.. EXAMPLE START Transforming TEME

.. doctest-requires:: sgp4

    >>> from astropy.coordinates import ITRS
    >>> itrs = teme.transform_to(ITRS(obstime=t))  # doctest: +IGNORE_WARNINGS
    >>> location = itrs.earth_location
    >>> location.geodetic  # doctest: +FLOAT_CMP
    GeodeticLocation(lon=<Longitude 160.34199789 deg>, lat=<Latitude -24.6609379 deg>, height=<Quantity 420.17927591 km>)

.. testsetup::

    >>> from astropy.coordinates import EarthLocation
    >>> siding_spring = EarthLocation(-4680888.60272112, 2805218.44653429, -3292788.0804506, unit='m')

Or, if you want to find the altitude and azimuth of the satellite from a particular location:

.. doctest-requires:: sgp4

    >>> from astropy.coordinates import EarthLocation, AltAz
    >>> siding_spring = EarthLocation.of_site('aao')  # doctest: +SKIP
    >>> aa = teme.transform_to(AltAz(obstime=t, location=siding_spring))  # doctest: +IGNORE_WARNINGS
    >>> aa.alt  # doctest: +FLOAT_CMP
    <Latitude 10.95229446 deg>
    >>> aa.az  # doctest: +FLOAT_CMP
    <Longitude 59.30081255 deg>

.. EXAMPLE END
.. _working_with_angles:

Working with Angles
*******************

The angular components of the various coordinate objects are represented
by objects of the |Angle| class. While most likely to be encountered in
the context of coordinate objects, |Angle| objects can also be used on
their own wherever a representation of an angle is needed.

.. _angle-creation:

Creation
========

The creation of an |Angle| object is quite flexible and supports a wide
variety of input object types and formats. The type of the input angle(s)
can be array, scalar, tuple, string, `~astropy.units.Quantity` or another
|Angle|. This is best illustrated with a number of examples of valid ways
to create an |Angle|.

Examples
--------

..
  EXAMPLE START
  Different Ways to Create an Angle Object

There are a number of ways to create an |Angle|::

    >>> import numpy as np
    >>> from astropy import units as u
    >>> from astropy.coordinates import Angle

    >>> Angle('10.2345d')              # String with 'd' abbreviation for degrees  # doctest: +FLOAT_CMP
    <Angle 10.2345 deg>
    >>> Angle(['10.2345d', '-20d'])    # Array of strings  # doctest: +FLOAT_CMP
    <Angle [ 10.2345, -20.    ] deg>
    >>> Angle('1:2:30.43 degrees')     # Sexagesimal degrees  # doctest: +FLOAT_CMP
    <Angle 1.04178611 deg>
    >>> Angle('1 2 0 hours')           # Sexagesimal hours  # doctest: +FLOAT_CMP
    <Angle 1.03333333 hourangle>
    >>> Angle(np.arange(1., 8.), unit=u.deg)  # Numpy array from 1..7 in degrees  # doctest: +FLOAT_CMP
    <Angle [1., 2., 3., 4., 5., 6., 7.] deg>
    >>> Angle('123')               # Unicode degree, arcmin and arcsec symbols  # doctest: +FLOAT_CMP
    <Angle 1.03416667 deg>
    >>> Angle('123N')               # Unicode degree, arcmin, arcsec symbols and direction  # doctest: +FLOAT_CMP
    <Angle 1.03416667 deg>
    >>> Angle('1d2m3.4s')              # Degree, arcmin, arcsec.  # doctest: +FLOAT_CMP
    <Angle 1.03427778 deg>
    >>> Angle('1d2m3.4sS')              # Degree, arcmin, arcsec, direction.  # doctest: +FLOAT_CMP
    <Angle -1.03427778 deg>
    >>> Angle('-1h2m3s')               # Hour, minute, second  # doctest: +FLOAT_CMP
    <Angle -1.03416667 hourangle>
    >>> Angle('-1h2m3sW')               # Hour, minute, second, direction  # doctest: +FLOAT_CMP
    <Angle 1.03416667 hourangle>
    >>> Angle((-1, 2, 3), unit=u.deg)  # (degree, arcmin, arcsec)  # doctest: +FLOAT_CMP
    <Angle -1.03416667 deg>
    >>> Angle(10.2345 * u.deg)         # From a Quantity object in degrees  # doctest: +FLOAT_CMP
    <Angle 10.2345 deg>
    >>> Angle(Angle(10.2345 * u.deg))  # From another Angle object  # doctest: +FLOAT_CMP
    <Angle 10.2345 deg>

..
  EXAMPLE END

Representation
==============

The |Angle| object also supports a variety of ways of representing the value
of the angle, both as a floating point number and as a string.

Examples
--------

..
  EXAMPLE START
  Representation of Angle Object Values

There are many ways to represent the value of an |Angle|::

    >>> a = Angle(1, u.radian)
    >>> a  # doctest: +FLOAT_CMP
    <Angle 1. rad>
    >>> a.radian
    1.0
    >>> a.degree  # doctest: +FLOAT_CMP
    57.29577951308232
    >>> a.hour  # doctest: +FLOAT_CMP
    3.8197186342054885
    >>> a.hms  # doctest: +FLOAT_CMP
    hms_tuple(h=3.0, m=49.0, s=10.987083139758766)
    >>> a.dms  # doctest: +FLOAT_CMP
    dms_tuple(d=57.0, m=17.0, s=44.806247096362313)
    >>> a.signed_dms  # doctest: +FLOAT_CMP
    signed_dms_tuple(sign=1.0, d=57.0, m=17.0, s=44.806247096362313)
    >>> (-a).dms  # doctest: +FLOAT_CMP
    dms_tuple(d=-57.0, m=-17.0, s=-44.806247096362313)
    >>> (-a).signed_dms  # doctest: +FLOAT_CMP
    signed_dms_tuple(sign=-1.0, d=57.0, m=17.0, s=44.806247096362313)
    >>> a.arcminute  # doctest: +FLOAT_CMP
    3437.7467707849396
    >>> a.to_string()
    '1rad'
    >>> a.to_string(unit=u.degree)
    '57d17m44.8062471s'
    >>> a.to_string(unit=u.degree, sep=':')
    '57:17:44.8062471'
    >>> a.to_string(unit=u.degree, sep=('deg', 'm', 's'))
    '57deg17m44.8062471s'
    >>> a.to_string(unit=u.hour)
    '3h49m10.98708314s'
    >>> a.to_string(unit=u.hour, decimal=True)
    '3.81972'

..
  EXAMPLE END

Usage
=====

Angles will also behave correctly for appropriate arithmetic operations.

Example
-------

..
  EXAMPLE START
  Arithmetic Operations Using Angle Objects

To use |Angle| objects in arithmetic operations::

    >>> a = Angle(1.0, u.radian)
    >>> a + 0.5 * u.radian + 2 * a  # doctest: +FLOAT_CMP
    <Angle 3.5 rad>
    >>> np.sin(a / 2)  # doctest: +FLOAT_CMP
    <Quantity 0.47942554>
    >>> a == a  # doctest: +SKIP
    array(True, dtype=bool)
    >>> a == (a + a)    # doctest: +SKIP
    array(False, dtype=bool)

..
  EXAMPLE END

|Angle| objects can also be used for creating coordinate objects.

Example
-------

..
  EXAMPLE START
  Creating Coordinate Objects with Angle Objects

To create a coordinate object using an |Angle|::

    >>> from astropy.coordinates import ICRS
    >>> ICRS(Angle(1, u.deg), Angle(0.5, u.deg))  # doctest: +FLOAT_CMP
    <ICRS Coordinate: (ra, dec) in deg
        (1., 0.5)>

..
  EXAMPLE END

Wrapping and Bounds
===================

There are two utility methods for working with angles that should have bounds.
The :meth:`~astropy.coordinates.Angle.wrap_at` method allows taking an angle or
angles and wrapping to be within a single 360 degree slice. The
:meth:`~astropy.coordinates.Angle.is_within_bounds` method returns a
boolean indicating whether an angle or angles is within the specified bounds.


Longitude and Latitude Objects
==============================

|Longitude| and |Latitude| are two specialized subclasses of the |Angle|
class that are used for all of the spherical coordinate classes.
|Longitude| is used to represent values like right ascension, Galactic
longitude, and azimuth (for Equatorial, Galactic, and Alt-Az coordinates,
respectively). |Latitude| is used for declination, Galactic latitude, and
elevation.

Longitude
---------

A |Longitude| object is distinguished from a pure |Angle| by virtue of a
``wrap_angle`` property. The ``wrap_angle`` specifies that all angle values
represented by the object will be in the range::

  wrap_angle - 360 * u.deg <= angle(s) < wrap_angle

The default ``wrap_angle`` is 360 deg. Setting ``'wrap_angle=180 * u.deg'``
would instead result in values between -180 and +180 deg. Setting the
``wrap_angle`` attribute of an existing ``Longitude`` object will result in
re-wrapping the angle values in-place. For example::

    >>> from astropy.coordinates import Longitude
    >>> a = Longitude([-20, 150, 350, 360] * u.deg)
    >>> a.degree  # doctest: +FLOAT_CMP
    array([340., 150., 350.,   0.])
    >>> a.wrap_angle = 180 * u.deg
    >>> a.degree  # doctest: +FLOAT_CMP
    array([-20., 150., -10.,   0.])

Latitude
--------

A Latitude object is distinguished from a pure |Angle| by virtue
of being bounded so that::

  -90.0 * u.deg <= angle(s) <= +90.0 * u.deg

Any attempt to set a value outside of that range will result in a
`ValueError`.


Generating Angle Values
=======================

Astropy provides utility functions for generating angular or spherical
positions, either with random sampling or with a grid of values. These functions
all return `~astropy.coordinates.BaseRepresentation` subclass instances, which
can be passed directly into coordinate frame classes or |SkyCoord| to create
random or gridded coordinate objects.


With Random Sampling
--------------------

These functions both use standard, random `spherical point picking
<https://mathworld.wolfram.com/SpherePointPicking.html>`_ to generate angular
positions that are uniformly distributed on the surface of the unit sphere. To
retrieve angular values only, use
`~astropy.coordinates.uniform_spherical_random_surface`. For
example, to generate 4 random angular positions::

    >>> from astropy.coordinates import uniform_spherical_random_surface
    >>> pts = uniform_spherical_random_surface(size=4)
    >>> pts  # doctest: +SKIP
    <UnitSphericalRepresentation (lon, lat) in rad
        [(0.52561028, 0.38712031), (0.29900285, 0.52776066),
         (0.98199282, 0.34247723), (2.15260367, 1.01499232)]>

To generate three-dimensional positions uniformly within a spherical volume set
by a maximum radius, instead use the
`~astropy.coordinates.uniform_spherical_random_volume`
function. For example, to generate 4 random 3D positions::

    >>> from astropy.coordinates import uniform_spherical_random_volume
    >>> pts_3d = uniform_spherical_random_volume(size=4)
    >>> pts_3d  # doctest: +SKIP
    <SphericalRepresentation (lon, lat, distance) in (rad, rad, )
        [(4.98504602, -0.74247419, 0.39752416),
         (5.53281607,  0.89425191, 0.7391255 ),
         (0.88100456,  0.21080555, 0.5531785 ),
         (6.00879324,  0.61547168, 0.61746148)]>

By default, the distance values returned are uniformly distributed within the
unit sphere (i.e., the distance values are dimensionless). To instead generate
random points within a sphere of a given dimensional radius, for example, 1
parsec, pass in a |Quantity| object with the ``max_radius`` argument::

    >>> import astropy.units as u
    >>> pts_3d = uniform_spherical_random_volume(size=4, max_radius=2*u.pc)
    >>> pts_3d  # doctest: +SKIP
    <SphericalRepresentation (lon, lat, distance) in (rad, rad, pc)
        [(3.36590297, -0.23085809, 1.47210093),
         (6.14591179,  0.06840621, 0.9325143 ),
         (2.19194797,  0.55099774, 1.19294064),
         (5.25689272, -1.17703409, 1.63773358)]>


On a Grid
---------

No grid or lattice of points on the sphere can produce equal spacing between all
grid points, but many approximate algorithms exist for generating angular grids
with nearly even spacing (for example, `see this page
<https://bendwavy.org/pack/pack.htm>`_).

One simple and popular method in this context is the `golden spiral method
<https://stackoverflow.com/a/44164075>`_, which is available in
`astropy.coordinates` through the utility function
`~astropy.coordinates.golden_spiral_grid`. This function accepts
a single argument, ``size``, which specifies the number of points to generate in
the grid::

    >>> from astropy.coordinates import golden_spiral_grid
    >>> golden_pts = golden_spiral_grid(size=32)
    >>> golden_pts  # doctest: +FLOAT_CMP
    <UnitSphericalRepresentation (lon, lat) in rad
        [(1.94161104,  1.32014066), (5.82483312,  1.1343273 ),
         (3.42486989,  1.004232  ), (1.02490666,  0.89666582),
         (4.90812873,  0.80200278), (2.5081655 ,  0.71583806),
         (0.10820227,  0.63571129), (3.99142435,  0.56007531),
         (1.59146112,  0.48787515), (5.4746832 ,  0.41834639),
         (3.07471997,  0.35090734), (0.67475674,  0.28509644),
         (4.55797882,  0.22053326), (2.15801559,  0.15689287),
         (6.04123767,  0.09388788), (3.64127444,  0.03125509),
         (1.24131121, -0.03125509), (5.12453328, -0.09388788),
         (2.72457005, -0.15689287), (0.32460682, -0.22053326),
         (4.2078289 , -0.28509644), (1.80786567, -0.35090734),
         (5.69108775, -0.41834639), (3.29112452, -0.48787515),
         (0.89116129, -0.56007531), (4.77438337, -0.63571129),
         (2.37442014, -0.71583806), (6.25764222, -0.80200278),
         (3.85767899, -0.89666582), (1.45771576, -1.004232  ),
         (5.34093783, -1.1343273 ), (2.9409746 , -1.32014066)]>




Comparing Spherical Point Generation Methods
--------------------------------------------

.. plot::
    :align: center
    :context: close-figs

    import matplotlib.pyplot as plt
    from astropy.coordinates import uniform_spherical_random_surface, golden_spiral_grid

    fig, axes = plt.subplots(1, 2, figsize=(10, 6),
                             subplot_kw=dict(projection='3d'),
                             constrained_layout=True)

    for func, ax in zip([uniform_spherical_random_surface,
                         golden_spiral_grid], axes):
        pts = func(size=128)

        xyz = pts.to_cartesian().xyz
        ax.plot(*xyz, ls='none')

        ax.set(xlim=(-1, 1),
            ylim=(-1, 1),
            zlim=(-1, 1),
            xlabel='$x$',
            ylabel='$y$',
            zlabel='$z$')
        ax.set_title(func.__name__, fontsize=14)

    fig.suptitle('128 points', fontsize=18)
.. note that if this is changed from the default approach of using an *include*
   (in index.rst) to a separate performance page, the header needs to be changed
   from === to ***, the filename extension needs to be changed from .inc.rst to
   .rst, and a link needs to be added in the subpackage toctree

.. _astropy-coordinates-performance:

Performance Tips
================

If you are using |SkyCoord| for many different coordinates, you will see much
better performance if you create a single |SkyCoord| with arrays of coordinates
as opposed to creating individual |SkyCoord| objects for each individual
coordinate::

    >>> coord = SkyCoord(ra_array, dec_array, unit='deg')  # doctest: +SKIP

In addition, looping over a |SkyCoord| object can be slow. If you need to
transform the coordinates to a different frame, it is much faster to transform a
single |SkyCoord| with arrays of values as opposed to looping over the
|SkyCoord| and transforming them individually.

Finally, for more advanced users, note that you can use broadcasting to
transform |SkyCoord| objects into frames with vector properties.

Example
-------

..
  EXAMPLE START
  Performance Tips for Transforming SkyCoord Objects

To use broadcasting to transform |SkyCoord| objects into frames with vector
properties::

    >>> from astropy.coordinates import SkyCoord, EarthLocation
    >>> from astropy import coordinates as coord
    >>> from astropy.coordinates.angle_utilities import golden_spiral_grid
    >>> from astropy.time import Time
    >>> from astropy import units as u
    >>> import numpy as np

    >>> # 1000 locations in a grid on the sky
    >>> coos = SkyCoord(golden_spiral_grid(size=1000))

    >>> # 300 times over the space of 10 hours
    >>> times = Time.now() + np.linspace(-5, 5, 300)*u.hour

    >>> # note the use of broadcasting so that 300 times are broadcast against 1000 positions
    >>> lapalma = EarthLocation.from_geocentric(5327448.9957829, -1718665.73869569, 3051566.90295403, unit='m')
    >>> aa_frame = coord.AltAz(obstime=times[:, np.newaxis], location=lapalma)

    >>> # calculate alt-az of each object at each time.
    >>> aa_coos = coos.transform_to(aa_frame)  # doctest: +REMOTE_DATA +IGNORE_WARNINGS

..
  EXAMPLE END


Improving Performance for Arrays of ``obstime``
-----------------------------------------------

The most expensive operations when transforming between observer-dependent coordinate
frames (e.g. ``AltAz``) and sky-fixed frames (e.g. ``ICRS``) are the calculation
of the orientation and position of Earth.

If |SkyCoord| instances are transformed for a large  number of closely spaced ``obstime``,
these calculations can be sped up by factors up to 100, whilst still keeping micro-arcsecond precision,
by utilizing interpolation instead of calculating Earth orientation parameters for each individual point.

..
  EXAMPLE START
  Improving performance for obstime arrays

To use interpolation for the astrometric values in coordinate transformation, use::

   >>> from astropy.coordinates import SkyCoord, EarthLocation, AltAz
   >>> from astropy.coordinates.erfa_astrom import erfa_astrom, ErfaAstromInterpolator
   >>> from astropy.time import Time
   >>> from time import perf_counter
   >>> import numpy as np
   >>> import astropy.units as u


   >>> # array with 10000 obstimes
   >>> obstime = Time('2010-01-01T20:00') + np.linspace(0, 6, 10000) * u.hour
   >>> location = location = EarthLocation(lon=-17.89 * u.deg, lat=28.76 * u.deg, height=2200 * u.m)
   >>> frame = AltAz(obstime=obstime, location=location)
   >>> crab = SkyCoord(ra='05h34m31.94s', dec='22d00m52.2s')

   >>> # transform with default transformation and print duration
   >>> t0 = perf_counter()
   >>> crab_altaz = crab.transform_to(frame)  # doctest:+IGNORE_WARNINGS +REMOTE_DATA
   >>> print(f'Transformation took {perf_counter() - t0:.2f} s')  # doctest:+IGNORE_OUTPUT
   Transformation took 1.77 s

   >>> # transform with interpolating astrometric values
   >>> t0 = perf_counter()
   >>> with erfa_astrom.set(ErfaAstromInterpolator(300 * u.s)): # doctest:+REMOTE_DATA
   ...     crab_altaz_interpolated = crab.transform_to(frame)  # doctest:+IGNORE_WARNINGS +REMOTE_DATA
   >>> print(f'Transformation took {perf_counter() - t0:.2f} s')  # doctest:+IGNORE_OUTPUT
   Transformation took 0.03 s

   >>> err = crab_altaz.separation(crab_altaz_interpolated)  # doctest:+IGNORE_WARNINGS +REMOTE_DATA
   >>> print(f'Mean error of interpolation: {err.to(u.microarcsecond).mean():.4f}')  # doctest:+ELLIPSIS +REMOTE_DATA
   Mean error of interpolation: 0.0... uarcsec

   >>> # To set erfa_astrom for a whole session, use it without context manager:
   >>> erfa_astrom.set(ErfaAstromInterpolator(300 * u.s))  # doctest:+SKIP

..
  EXAMPLE END


Here, we look into choosing an appropriate ``time_resolution``.
We will transform a single sky coordinate for lots of observation times from
``ICRS`` to ``AltAz`` and evaluate precision and runtime for different values
for ``time_resolution`` compared to the non-interpolating, default approach.

.. plot::
   :include-source:
   :context: reset

    from time import perf_counter

    import numpy as np
    import matplotlib.pyplot as plt

    from astropy.coordinates.erfa_astrom import erfa_astrom, ErfaAstromInterpolator
    from astropy.coordinates import SkyCoord, EarthLocation, AltAz
    from astropy.time import Time
    import astropy.units as u

    np.random.seed(1337)

    # 100_000 times randomly distributed over 12 hours
    t = Time('2020-01-01T20:00:00') + np.random.uniform(0, 1, 10_000) * u.hour

    location = location = EarthLocation(
        lon=-17.89 * u.deg, lat=28.76 * u.deg, height=2200 * u.m
    )

    # A celestial object in ICRS
    crab = SkyCoord.from_name("Crab Nebula")

    # target horizontal coordinate frame
    altaz = AltAz(obstime=t, location=location)


    # the reference transform using no interpolation
    t0 = perf_counter()
    no_interp = crab.transform_to(altaz)
    reference = perf_counter() - t0
    print(f'No Interpolation took {reference:.4f} s')


    # now the interpolating approach for different time resolutions
    resolutions = 10.0**np.arange(-1, 5) * u.s
    times = []
    seps = []

    for resolution in resolutions:
        with erfa_astrom.set(ErfaAstromInterpolator(resolution)):
            t0 = perf_counter()
            interp = crab.transform_to(altaz)
            duration = perf_counter() - t0

        print(
            f'Interpolation with {resolution.value: 9.1f} {str(resolution.unit)}'
            f' resolution took {duration:.4f} s'
            f' ({reference / duration:5.1f}x faster) '
        )
        seps.append(no_interp.separation(interp))
        times.append(duration)

    seps = u.Quantity(seps)

    fig = plt.figure()

    ax1, ax2 = fig.subplots(2, 1, gridspec_kw={'height_ratios': [2, 1]}, sharex=True)

    ax1.plot(
        resolutions.to_value(u.s),
        seps.mean(axis=1).to_value(u.microarcsecond),
        'o', label='mean',
    )

    for p in [25, 50, 75, 95]:
        ax1.plot(
            resolutions.to_value(u.s),
            np.percentile(seps.to_value(u.microarcsecond), p, axis=1),
            'o', label=f'{p}%', color='C1', alpha=p / 100,
        )

    ax1.set_title('Transformation of SkyCoord with 100.000 obstimes over 12 hours')

    ax1.legend()
    ax1.set_xscale('log')
    ax1.set_yscale('log')
    ax1.set_ylabel('Angular distance to no interpolation / as')

    ax2.plot(resolutions.to_value(u.s), reference / np.array(times), 's')
    ax2.set_yscale('log')
    ax2.set_ylabel('Speedup')
    ax2.set_xlabel('time resolution / s')

    ax2.yaxis.grid()
    fig.tight_layout()
.. _astropy-coordinates-velocities:

Working with Velocities in Astropy Coordinates
**********************************************

Using Velocities with ``SkyCoord``
==================================

The best way to start getting a coordinate object with velocities is to use the
|SkyCoord| interface.

Examples
--------

..
  EXAMPLE START
  Using SkyCoord to Get Coordinate Objects with Velocities

A |SkyCoord| to represent a star with a measured radial velocity but unknown
proper motion and distance could be created as::

    >>> from astropy.coordinates import SkyCoord
    >>> import astropy.units as u
    >>> sc = SkyCoord(1*u.deg, 2*u.deg, radial_velocity=20*u.km/u.s)
    >>> sc  # doctest: +FLOAT_CMP
    <SkyCoord (ICRS): (ra, dec) in deg
        (1., 2.)
     (radial_velocity) in km / s
        (20.,)>
    >>> sc.radial_velocity  # doctest: +FLOAT_CMP
    <Quantity 20.0 km / s>

|SkyCoord| objects created in this manner follow all of the same transformation
rules and will correctly update their velocities when transformed to other
frames. For example, to determine proper motions in Galactic coordinates for
a star with proper motions measured in ICRS::

    >>> sc = SkyCoord(1*u.deg, 2*u.deg, pm_ra_cosdec=.2*u.mas/u.yr, pm_dec=.1*u.mas/u.yr)
    >>> sc.galactic  # doctest: +FLOAT_CMP
    <SkyCoord (Galactic): (l, b) in deg
      ( 99.63785528, -58.70969293)
    (pm_l_cosb, pm_b) in mas / yr
      ( 0.22240398,  0.02316181)>

..
  EXAMPLE END

For more details on valid operations and limitations of velocity support in
`astropy.coordinates` (particularly the :ref:`current accuracy limitations
<astropy-coordinate-finite-difference-velocities>`), see the more detailed
discussions below of velocity support in the lower-level frame objects. All
these same rules apply for |SkyCoord| objects, as they are built directly on top
of the frame classes' velocity functionality detailed here.

.. _astropy-coordinate-custom-frame-with-velocities:

Creating Frame Objects with Velocity Data
=========================================

The coordinate frame classes support storing and transforming velocity data
(alongside the positional coordinate data). Similar to the positional data that
use the ``Representation`` classes to abstract away the particular
representation and allow re-representing from (e.g., Cartesian to Spherical),
the velocity data makes use of ``Differential`` classes to do the
same. (For more information about the differential classes, see
:ref:`astropy-coordinates-differentials`.) Also like the positional data, the
names of the differential (velocity) components depend on the particular
coordinate frame.

Most frames expect velocity data in the form of two proper motion components
and/or a radial velocity because the default differential for most frames is the
`~astropy.coordinates.SphericalCosLatDifferential` class. When supported, the
proper motion components all begin with ``pm_`` and, by default, the
longitudinal component is expected to already include the ``cos(latitude)``
term. For example, the proper motion components for the ``ICRS`` frame are
(``pm_ra_cosdec``, ``pm_dec``).

Examples
--------

..
  EXAMPLE START
  Creating Frame Objects with Proper Motions

To create frame objects with velocity data in the form of proper motion
components::

    >>> from astropy.coordinates import ICRS
    >>> ICRS(ra=8.67*u.degree, dec=53.09*u.degree,
    ...      pm_ra_cosdec=4.8*u.mas/u.yr, pm_dec=-15.16*u.mas/u.yr)  # doctest: +FLOAT_CMP
    <ICRS Coordinate: (ra, dec) in deg
        (8.67, 53.09)
     (pm_ra_cosdec, pm_dec) in mas / yr
        (4.8, -15.16)>
    >>> ICRS(ra=8.67*u.degree, dec=53.09*u.degree,
    ...      pm_ra_cosdec=4.8*u.mas/u.yr, pm_dec=-15.16*u.mas/u.yr,
    ...      radial_velocity=23.42*u.km/u.s)  # doctest: +FLOAT_CMP
    <ICRS Coordinate: (ra, dec) in deg
        (8.67, 53.09)
     (pm_ra_cosdec, pm_dec, radial_velocity) in (mas / yr, mas / yr, km / s)
        (4.8, -15.16, 23.42)>

For proper motion components in the ``Galactic`` frame, the names track the
longitude and latitude names::

    >>> from astropy.coordinates import Galactic
    >>> Galactic(l=11.23*u.degree, b=58.13*u.degree,
    ...          pm_l_cosb=21.34*u.mas/u.yr, pm_b=-55.89*u.mas/u.yr)  # doctest: +FLOAT_CMP
    <Galactic Coordinate: (l, b) in deg
        (11.23, 58.13)
     (pm_l_cosb, pm_b) in mas / yr
        (21.34, -55.89)>

Like the positional data, velocity data must be passed in as
`~astropy.units.Quantity` objects.

..
  EXAMPLE END

..
  EXAMPLE START
  Changing the Differential Class when Creating Frame Objects

The expected differential class can be changed to control the argument names
that the frame expects. By default the proper motion components are expected to
contain the ``cos(latitude)``, but this can be changed by specifying the
`~astropy.coordinates.SphericalDifferential` class (instead of the default
`~astropy.coordinates.SphericalCosLatDifferential`)::

    >>> from astropy.coordinates import SphericalDifferential
    >>> Galactic(l=11.23*u.degree, b=58.13*u.degree,
    ...          pm_l=21.34*u.mas/u.yr, pm_b=-55.89*u.mas/u.yr,
    ...          differential_type=SphericalDifferential)  # doctest: +FLOAT_CMP
    <Galactic Coordinate: (l, b) in deg
        (11.23, 58.13)
     (pm_l, pm_b) in mas / yr
        (21.34, -55.89)>

This works in parallel to specifying the expected representation class, as long
as the differential class is compatible with the representation. For example, to
specify all coordinate and velocity components in Cartesian::

    >>> from astropy.coordinates import (CartesianRepresentation,
    ...                                  CartesianDifferential)
    >>> Galactic(u=103*u.pc, v=-11*u.pc, w=93.*u.pc,
    ...          U=31*u.km/u.s, V=-10*u.km/u.s, W=75*u.km/u.s,
    ...          representation_type=CartesianRepresentation,
    ...          differential_type=CartesianDifferential)  # doctest: +FLOAT_CMP
    <Galactic Coordinate: (u, v, w) in pc
        (103., -11., 93.)
     (U, V, W) in km / s
        (31., -10., 75.)>

Note that the ``Galactic`` frame has special, standard names for Cartesian
position and velocity components. For other frames, these are just ``x,y,z`` and
``v_x,v_y,v_z``::

    >>> ICRS(x=103*u.pc, y=-11*u.pc, z=93.*u.pc,
    ...      v_x=31*u.km/u.s, v_y=-10*u.km/u.s, v_z=75*u.km/u.s,
    ...      representation_type=CartesianRepresentation,
    ...      differential_type=CartesianDifferential)  # doctest: +FLOAT_CMP
    <ICRS Coordinate: (x, y, z) in pc
        (103., -11., 93.)
     (v_x, v_y, v_z) in km / s
        (31., -10., 75.)>

..
  EXAMPLE END

..
  EXAMPLE START
  Shorthands for Convenient Access to Velocity Data in Frame Objects

For any frame with velocity data with any representation, there are also
shorthands that provide easier access to the underlying velocity data in
commonly needed formats. With any frame object with 3D velocity data, the 3D
Cartesian velocity can be accessed with::

    >>> icrs = ICRS(ra=8.67*u.degree, dec=53.09*u.degree,
    ...             distance=171*u.pc,
    ...             pm_ra_cosdec=4.8*u.mas/u.yr, pm_dec=-15.16*u.mas/u.yr,
    ...             radial_velocity=23.42*u.km/u.s)
    >>> icrs.velocity # doctest: +FLOAT_CMP
    <CartesianDifferential (d_x, d_y, d_z) in km / s
        ( 23.03160789,  7.44794505,  11.34587732)>

There are also shorthands for retrieving a single `~astropy.units.Quantity`
object that contains the two-dimensional proper motion data, and for retrieving
the radial (line-of-sight) velocity::

    >>> icrs.proper_motion # doctest: +FLOAT_CMP
    <Quantity [  4.8 ,-15.16] mas / yr>
    >>> icrs.radial_velocity # doctest: +FLOAT_CMP
    <Quantity 23.42 km / s>

..
  EXAMPLE END

Adding Velocities to Existing Frame Objects
===========================================

Another use case similar to the above comes up when you have an existing frame
object (or |SkyCoord|) and want an object with the same position but with
velocities added. The most conceptually direct way to do this is to
use the differential objects along with
`~astropy.coordinates.BaseCoordinateFrame.realize_frame`.

Examples
--------

..
  EXAMPLE START
  Adding Velocities to Existing Frame Objects

The following snippet accomplishes a well-defined case where the desired
velocities are known in the Cartesian representation. To add the velocities to
the existing frame using
`~astropy.coordinates.BaseCoordinateFrame.realize_frame`::

    >>> icrs = ICRS(1*u.deg, 2*u.deg, distance=3*u.kpc)
    >>> icrs # doctest: +FLOAT_CMP
    <ICRS Coordinate: (ra, dec, distance) in (deg, deg, kpc)
        (1., 2., 3.)>
    >>> vel_to_add = CartesianDifferential(4*u.km/u.s, 5*u.km/u.s, 6*u.km/u.s)
    >>> newdata = icrs.data.to_cartesian().with_differentials(vel_to_add)
    >>> icrs.realize_frame(newdata) # doctest: +FLOAT_CMP
    <ICRS Coordinate: (ra, dec, distance) in (deg, deg, kpc)
        (1., 2., 3.)
     (pm_ra_cosdec, pm_dec, radial_velocity) in (mas / yr, mas / yr, km / s)
        (0.34662023, 0.41161335, 4.29356031)>

A similar mechanism can also be used to add velocities even if full 3D coordinates
are not available (e.g., for a radial velocity observation of an object where
the distance is unknown). However, it requires a slightly different way of
specifying the differentials because of the lack of explicit unit information::

    >>> from astropy.coordinates import RadialDifferential
    >>> icrs_no_distance = ICRS(1*u.deg, 2*u.deg)
    >>> icrs_no_distance
    <ICRS Coordinate: (ra, dec) in deg
        (1., 2.)>
    >>> rv_to_add = RadialDifferential(500*u.km/u.s)
    >>> data_with_rv = icrs_no_distance.data.with_differentials({'s':rv_to_add})
    >>> icrs_no_distance.realize_frame(data_with_rv) # doctest: +FLOAT_CMP
    <ICRS Coordinate: (ra, dec) in deg
        (1., 2.)
     (radial_velocity) in km / s
        (500.,)>

Which we can see yields an object identical to what you get when you specify a
radial velocity when you create the object::

    >>> ICRS(1*u.deg, 2*u.deg, radial_velocity=500*u.km/u.s) # doctest: +FLOAT_CMP
    <ICRS Coordinate: (ra, dec) in deg
        (1., 2.)
     (radial_velocity) in km / s
        (500.,)>

..
  EXAMPLE END

.. _astropy-coordinate-transform-with-velocities:

Transforming Frames with Velocities
===================================

Transforming coordinate frame instances that contain velocity data to a
different frame (which may involve both position and velocity transformations)
is done exactly the same way as transforming position-only frame instances.

Example
-------

..
  EXAMPLE START
  Transforming Coordinate Frames with Velocities

To transform a coordinate frame that contains velocity data::

    >>> from astropy.coordinates import Galactic
    >>> icrs = ICRS(ra=8.67*u.degree, dec=53.09*u.degree,
    ...             pm_ra_cosdec=4.8*u.mas/u.yr, pm_dec=-15.16*u.mas/u.yr)  # doctest: +FLOAT_CMP
    >>> icrs.transform_to(Galactic()) # doctest: +FLOAT_CMP
    <Galactic Coordinate: (l, b) in deg
        (120.38084191, -9.69872044)
     (pm_l_cosb, pm_b) in mas / yr
        (3.78957965, -15.44359693)>

..
  EXAMPLE END

However, the details of how the velocity components are transformed depends on
the particular set of transforms required to get from the starting frame to the
desired frame (i.e., the path taken through the frame transform graph). If all
frames in the chain of transformations are transformed to each other via
`~astropy.coordinates.BaseAffineTransform` subclasses (i.e., are matrix
transformations or affine transformations), then the transformations can be
applied explicitly to the velocity data. If this is not the case, the velocity
transformation is computed numerically by finite-differencing the positional
transformation. See the subsections below for more details about these two
methods.

Affine Transformations
----------------------

Frame transformations that involve a rotation and/or an origin shift and/or
a velocity offset are implemented as affine transformations using the
`~astropy.coordinates.BaseAffineTransform` subclasses:
`~astropy.coordinates.StaticMatrixTransform`,
`~astropy.coordinates.DynamicMatrixTransform`, and
`~astropy.coordinates.AffineTransform`.

Matrix-only transformations (e.g., rotations such as
`~astropy.coordinates.ICRS` to `~astropy.coordinates.Galactic`) can be performed
on proper-motion-only data or full-space, 3D velocities.

Examples
^^^^^^^^

..
  EXAMPLE START
  Affine Frame Transformations

To perform a matrix-only transformation::

    >>> icrs = ICRS(ra=8.67*u.degree, dec=53.09*u.degree,
    ...             pm_ra_cosdec=4.8*u.mas/u.yr, pm_dec=-15.16*u.mas/u.yr,
    ...             radial_velocity=23.42*u.km/u.s)
    >>> icrs.transform_to(Galactic())  # doctest: +FLOAT_CMP
    <Galactic Coordinate: (l, b) in deg
        (120.38084191, -9.69872044)
     (pm_l_cosb, pm_b, radial_velocity) in (mas / yr, mas / yr, km / s)
        (3.78957965, -15.44359693, 23.42)>

The same rotation matrix is applied to both the position vector and the velocity
vector. Any transformation that involves a velocity offset requires all 3D
velocity components (which typically require specifying a distance as well),
for example, `~astropy.coordinates.ICRS` to `~astropy.coordinates.LSR`::

    >>> from astropy.coordinates import LSR
    >>> icrs = ICRS(ra=8.67*u.degree, dec=53.09*u.degree,
    ...             distance=117*u.pc,
    ...             pm_ra_cosdec=4.8*u.mas/u.yr, pm_dec=-15.16*u.mas/u.yr,
    ...             radial_velocity=23.42*u.km/u.s)
    >>> icrs.transform_to(LSR())  # doctest: +FLOAT_CMP
    <LSR Coordinate (v_bary=(11.1, 12.24, 7.25) km / s): (ra, dec, distance) in (deg, deg, pc)
        (8.67, 53.09, 117.)
     (pm_ra_cosdec, pm_dec, radial_velocity) in (mas / yr, mas / yr, km / s)
        (-24.51315607, -2.67935501, 27.07339176)>

..
  EXAMPLE END

.. _astropy-coordinate-finite-difference-velocities:

Finite Difference Transformations
---------------------------------

Some frame transformations cannot be expressed as affine transformations.
For example, transformations from the `~astropy.coordinates.AltAz` frame can
include an atmospheric dispersion correction, which is inherently nonlinear.
Additionally, some frames are more conveniently implemented as functions, even
if they can be cast as affine transformations. For these frames, a finite
difference approach to transforming velocities is available. Note that this
approach is implemented such that user-defined frames can use it in
the same manner (i.e., by defining a transformation of the
`~astropy.coordinates.FunctionTransformWithFiniteDifference` type).

This finite difference approach actually combines two separate (but important)
elements of the transformation:

  * Transformation of the *direction* of the velocity vector that already exists
    in the starting frame. That is, a frame transformation sometimes involves
    reorienting the coordinate frame (e.g., rotation), and the velocity vector
    in the new frame must account for this. The finite difference approach
    models this by moving the position of the starting frame along the velocity
    vector, and computing this offset in the target frame.
  * The "induced" velocity due to motion of the frame *itself*. For example,
    shifting from a frame centered at the solar system barycenter to one
    centered on the Earth includes a velocity component due entirely to the
    Earth's motion around the barycenter. This is accounted for by computing
    the location of the starting frame in the target frame at slightly different
    times, and computing the difference between those. Note that this step
    depends on assuming that a particular frame attribute represents a "time"
    of relevance for the induced velocity. By convention this is typically the
    ``obstime`` frame attribute, although it is an option that can be set when
    defining a finite difference transformation function.

Example
^^^^^^^

..
  EXAMPLE START
  Transforming Velocity Data Between Frames Using a Finite Difference Scheme

It is important to recognize that the finite difference transformations
have inherent limits set by the finite difference algorithm and machine
precision. To illustrate this problem, consider the AltAz to GCRS  (i.e.,
geocentric) transformation. Let us try to compute the radial velocity in the
GCRS frame for something observed from the Earth at a distance of 100 AU with a
radial velocity of 10 km/s:

.. plot::
    :context: reset
    :include-source:

    import numpy as np
    from matplotlib import pyplot as plt

    from astropy import units as u
    from astropy.time import Time
    from astropy.coordinates import EarthLocation, AltAz, GCRS

    time = Time('J2010') + np.linspace(-1,1,1000)*u.min
    location = EarthLocation(lon=0*u.deg, lat=45*u.deg)
    aa = AltAz(alt=[45]*1000*u.deg, az=90*u.deg, distance=100*u.au,
               radial_velocity=[10]*1000*u.km/u.s,
               location=location, obstime=time)
    gcrs = aa.transform_to(GCRS(obstime=time))
    plt.plot_date(time.plot_date, gcrs.radial_velocity.to(u.km/u.s))
    plt.ylabel('RV [km/s]')

This seems plausible: the radial velocity should indeed be very close to 10 km/s
because the frame does not involve a velocity shift.

Now let us consider 100 *kiloparsecs* as the distance. In this case we expect
the same: the radial velocity should be essentially the same in both frames:

.. plot::
    :context:
    :include-source:

    time = Time('J2010') + np.linspace(-1,1,1000)*u.min
    location = EarthLocation(lon=0*u.deg, lat=45*u.deg)
    aa = AltAz(alt=[45]*1000*u.deg, az=90*u.deg, distance=100*u.kpc,
               radial_velocity=[10]*1000*u.km/u.s,
               location=location, obstime=time)
    gcrs = aa.transform_to(GCRS(obstime=time))
    plt.plot_date(time.plot_date, gcrs.radial_velocity.to(u.km/u.s))
    plt.ylabel('RV [km/s]')

But this result is nonsense, with values from -1000 to 1000 km/s instead of the
~10 km/s we expected. The root of the problem here is that the machine
precision is not sufficient to compute differences on the order of kilometers
over distances on the order of kiloparsecs. Hence, the straightforward finite
difference method will not work for this use case with the default values.

.. testsetup::

    >>> import numpy as np
    >>> from astropy.coordinates import EarthLocation, AltAz, GCRS
    >>> from astropy.time import Time
    >>> time = Time('J2010') + np.linspace(-1,1,1000) * u.min
    >>> location = EarthLocation(lon=0*u.deg, lat=45*u.deg)
    >>> aa = AltAz(alt=[45]*1000*u.deg, az=90*u.deg, distance=100*u.kpc,
    ...            radial_velocity=[10]*1000*u.km/u.s,
    ...            location=location, obstime=time)

It is possible to override the timestep over which the finite difference occurs.
For example::

    >>> from astropy.coordinates import frame_transform_graph, AltAz, CIRS
    >>> trans = frame_transform_graph.get_transform(AltAz, CIRS).transforms[0]
    >>> trans.finite_difference_dt = 1 * u.year
    >>> gcrs = aa.transform_to(GCRS(obstime=time))  # doctest: +REMOTE_DATA
    >>> trans.finite_difference_dt = 1 * u.second  # return to default

In the above example, there is exactly one transformation step from
`~astropy.coordinates.AltAz` to `~astropy.coordinates.GCRS`.  In general, there
may be more than one step between two frames, or the single step may perform
other transformations internally.  One can use the context manager
:func:`~astropy.coordinates.TransformGraph.impose_finite_difference_dt` for the
transformation graph to override ``finite_difference_dt`` for *all*
finite-difference transformations on the graph::

    >>> from astropy.coordinates import frame_transform_graph
    >>> with frame_transform_graph.impose_finite_difference_dt(1 * u.year):
    ...     gcrs = aa.transform_to(GCRS(obstime=time))  # doctest: +REMOTE_DATA

But beware that this will *not* help in cases like the above, where the relevant
timescales for the velocities are seconds. (The velocity of the Earth relative
to a particular direction changes dramatically over the course of one year.)

..
  EXAMPLE END

Future versions of Astropy will improve on this algorithm to make the results
more numerically stable and practical for use in these (not unusual) use cases.

.. _astropy-coordinates-rv-corrs:

Radial Velocity Corrections
===========================

Separately from the above, Astropy supports computing barycentric or
heliocentric radial velocity corrections. While in the future this may
be a high-level convenience function using the framework described above, the
current implementation is independent to ensure sufficient accuracy (see
:ref:`astropy-coordinates-rv-corrs` and the
`~astropy.coordinates.SkyCoord.radial_velocity_correction` API docs for
details).

Example
-------

..
  EXAMPLE START
  Computing Barycentric or Heliocentric Radial Velocity Corrections

This example demonstrates how to compute this correction if observing some
object at a known RA and Dec from the Keck observatory at a particular time. If
a precision of around 3 m/s is sufficient, the computed correction can then be
added to any observed radial velocity to determine the final heliocentric
radial velocity::

    >>> from astropy.time import Time
    >>> from astropy.coordinates import SkyCoord, EarthLocation
    >>> # keck = EarthLocation.of_site('Keck')  # the easiest way... but requires internet
    >>> keck = EarthLocation.from_geodetic(lat=19.8283*u.deg, lon=-155.4783*u.deg, height=4160*u.m)
    >>> sc = SkyCoord(ra=4.88375*u.deg, dec=35.0436389*u.deg)
    >>> barycorr = sc.radial_velocity_correction(obstime=Time('2016-6-4'), location=keck)  # doctest: +REMOTE_DATA
    >>> barycorr.to(u.km/u.s)  # doctest: +REMOTE_DATA +FLOAT_CMP
    <Quantity 20.077135 km / s>
    >>> heliocorr = sc.radial_velocity_correction('heliocentric', obstime=Time('2016-6-4'), location=keck)  # doctest: +REMOTE_DATA
    >>> heliocorr.to(u.km/u.s)  # doctest: +REMOTE_DATA +FLOAT_CMP
    <Quantity 20.070039 km / s>

Note that there are a few different ways to specify the options for the
correction (e.g., the location, observation time, etc.). See the
`~astropy.coordinates.SkyCoord.radial_velocity_correction` docs for more
information.

..
  EXAMPLE END

Precision of `~astropy.coordinates.SkyCoord.radial_velocity_correction`
------------------------------------------------------------------------

The correction computed by `~astropy.coordinates.SkyCoord.radial_velocity_correction`
uses the optical approximation :math:`v = zc` (see :ref:`astropy-units-doppler-equivalencies`
for details). The correction can be added to any observed radial velocity
to provide a correction that is accurate to a level of approximately 3 m/s.
If you need more precise corrections, there are a number of subtleties of
which you must be aware.

The first is that you should always use a barycentric correction, as the
barycenter is a fixed point where gravity is constant. Since the heliocenter
does not satisfy these conditions, corrections to the heliocenter are only
suitable for low precision work. As a result, and to increase speed, the
heliocentric correction in
`~astropy.coordinates.SkyCoord.radial_velocity_correction` does not include
effects such as the gravitational redshift due to the potential at the Earth's
surface. For these reasons, the barycentric correction in
`~astropy.coordinates.SkyCoord.radial_velocity_correction` should always
be used for high precision work.

Other considerations necessary for radial velocity corrections at the cm/s
level are outlined in `Wright & Eastman (2014) <https://ui.adsabs.harvard.edu/abs/2014PASP..126..838W>`_.
Most important is that the barycentric correction is, strictly speaking,
*multiplicative*, so that you should apply it as:

.. math::

    v_t = v_m + v_b + \frac{v_b v_m}{c},

Where :math:`v_t` is the true radial velocity, :math:`v_m` is the measured
radial velocity and :math:`v_b` is the barycentric correction returned by
`~astropy.coordinates.SkyCoord.radial_velocity_correction`. Failure to apply
the barycentric correction in this way leads to errors of order 3 m/s.

The barycentric correction in `~astropy.coordinates.SkyCoord.radial_velocity_correction` is consistent
with the `IDL implementation <http://astroutils.astronomy.ohio-state.edu/exofast/barycorr.html>`_ of
the Wright & Eastmann (2014) paper to a level of 10 mm/s for a source at
infinite distance. We do not include the Shapiro delay nor the light
travel time correction from equation 28 of that paper. The neglected terms
are not important unless you require accuracies of better than 1 cm/s.
If you do require that precision, see `Wright & Eastmann (2014) <https://ui.adsabs.harvard.edu/abs/2014PASP..126..838W>`_.
.. _astropy-coordinates-apply-space-motion:

Accounting for Space Motion
***************************

The |SkyCoord| object supports updating the position of a source given its space
motion and a time at which to evaluate the new position (or a difference
between the coordinate's current time and a new one). This is
done using the :meth:`~astropy.coordinates.SkyCoord.apply_space_motion` method.

Example
-------

..
  EXAMPLE START
  Accounting for Space Motion with SkyCoord Objects

First we will create a |SkyCoord| object with a specified ``obstime``::

    >>> import astropy.units as u
    >>> from astropy.time import Time
    >>> from astropy.coordinates import SkyCoord
    >>> c = SkyCoord(l=10*u.degree, b=45*u.degree, distance=100*u.pc,
    ...              pm_l_cosb=34*u.mas/u.yr, pm_b=-117*u.mas/u.yr,
    ...              frame='galactic',
    ...              obstime=Time('1988-12-18 05:11:23.5'))

We can now find the position at some other time, taking the space motion into
account. We can either specify the time difference between the observation time
and the desired time::

    >>> c.apply_space_motion(dt=10. * u.year) # doctest: +FLOAT_CMP
    <SkyCoord (Galactic): (l, b, distance) in (deg, deg, pc)
        ( 10.00013356,  44.999675,  99.99999994)
     (pm_l_cosb, pm_b, radial_velocity) in (mas / yr, mas / yr, km / s)
        ( 33.99980714, -117.00005604,  0.00034117)>
    >>> c.apply_space_motion(dt=-10. * u.year) # doctest: +FLOAT_CMP
    <SkyCoord (Galactic): (l, b, distance) in (deg, deg, pc)
        ( 9.99986643,  45.000325,  100.00000006)
     (pm_l_cosb, pm_b, radial_velocity) in (mas / yr, mas / yr, km / s)
        ( 34.00019286, -116.99994395, -0.00034117)>

Or, we can specify the new time to evaluate the position at::

    >>> c.apply_space_motion(new_obstime=Time('2017-12-18 01:12:07.3')) # doctest: +FLOAT_CMP
    <SkyCoord (Galactic): (l, b, distance) in (deg, deg, pc)
        ( 10.00038732,  44.99905754,  99.99999985)
     (pm_l_cosb, pm_b, radial_velocity) in (mas / yr, mas / yr, km / s)
        ( 33.99944073, -117.00016248,  0.00098937)>

..
  EXAMPLE END

If the |SkyCoord| object has no specified radial velocity (RV), the RV is
assumed to be 0. The new position of the source is determined assuming the
source moves in a straight line with constant velocity in an inertial frame.
There are no plans to support more complex evolution (e.g., non-inertial
frames or more complex evolution), as that is out of scope for the ``astropy``
core package (although it may well be in-scope for a variety of affiliated
packages).

Example: Use velocity to compute sky position at different epochs
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

..
  EXAMPLE START
  Using Velocity to Compute Sky Position at Different Epochs

In this example, we will use *Gaia* `TGAS
<https://www.cosmos.esa.int/web/gaia/dr1>`_ astrometry for a nearby star to
compute the sky position of the source on the date that the 2MASS survey
observed that region of the sky. The TGAS astrometry is provided on the
reference epoch J2015.0, whereas the 2MASS survey occurred in the late 1990's.
For the star of interest, the proper motion is large enough that there are
appreciable differences in the sky position between the two surveys.

After computing the previous position of the source, we will then cross-match
the source with the 2MASS catalog to compute *Gaia*-2MASS colors for this object
source.

.. note::

    This example requires accessing data from the *Gaia* TGAS and 2MASS
    catalogs. For convenience and speed below, we have created dictionary
    objects that contain the data. We retrieved the data using the Astropy
    affiliated package `astroquery <https://astroquery.readthedocs.io/>`_ using
    the following queries::

        import astropy.coordinates as coord
        import astropy.units as u
        from astroquery.gaia import Gaia
        from astroquery.vizier import Vizier

        job = Gaia.launch_job("SELECT TOP 1 * FROM gaiadr1.tgas_source \
            WHERE parallax_error < 0.3  AND parallax > 5 AND pmra > 100 \
            ORDER BY random_index")
        result_tgas = job.get_results()[0]

        c_tgas = coord.SkyCoord(ra=result_tgas['ra'] * u.deg,
                                dec=result_tgas['dec'] * u.deg)
        v = Vizier(columns=["**"], catalog="II/246/out")
        result_2mass = v.query_region(c, radius=1*u.arcmin)['II/246/out']

The TGAS data from relevant columns for this source (see queries in Note
above)::

    >>> result_tgas = dict(ra=66.44280212823296,
    ...                    dec=-69.99366255906372,
    ...                    parallax=22.764078749733947,
    ...                    pmra=144.91354358297048,
    ...                    pmdec=5.445648092997134,
    ...                    ref_epoch=2015.0,
    ...                    phot_g_mean_mag=7.657174523348196)

The 2MASS data for all sources within 1 arcminute around the above position
(see queries in Note above)::

    >>> result_2mass = dict(RAJ2000=[66.421970000000002, 66.433521999999996,
    ...                              66.420564999999996, 66.485068999999996,
    ...                              66.467928999999998, 66.440815000000001,
    ...                              66.440454000000003],
    ...                     DEJ2000=[-70.003722999999994, -69.990768000000003,
    ...                              -69.992255999999998, -69.994881000000007,
    ...                              -69.994926000000007, -69.993613999999994,
    ...                              -69.990836999999999],
    ...                     Jmag=[16.35, 13.663, 16.171, 16.184, 16.292,
    ...                           6.6420002, 12.275],
    ...                     Hmag=[15.879, 13.955, 15.154, 15.856, 15.642,
    ...                           6.3660002, 12.185],
    ...                     Kmag=[15.581, 14.238, 14.622, 15.398, 15.123,
    ...                           6.2839999, 12.106],
    ...                     Date=['1998-10-24', '1998-10-24', '1998-10-24',
    ...                           '1998-10-24', '1998-10-24', '1998-10-24',
    ...                           '1998-10-24'])

We will first create a |SkyCoord| object from the information provided in the
TGAS catalog. Note that we set the ``obstime`` of the object to the reference
epoch provided by the TGAS catalog (J2015.0 in Barycentric Coordinate Time)::

    >>> import astropy.units as u
    >>> from astropy.coordinates import SkyCoord, Distance
    >>> from astropy.time import Time
    >>> c = SkyCoord(ra=result_tgas['ra'] * u.deg,
    ...              dec=result_tgas['dec'] * u.deg,
    ...              distance=Distance(parallax=result_tgas['parallax'] * u.mas),
    ...              pm_ra_cosdec=result_tgas['pmra'] * u.mas/u.yr,
    ...              pm_dec=result_tgas['pmdec'] * u.mas/u.yr,
    ...              obstime=Time(result_tgas['ref_epoch'], format='jyear',
    ...                           scale='tcb'))

We next create a |SkyCoord| object with the sky positions from the 2MASS
catalog, and an `~astropy.time.Time` object for the date of the 2MASS
observations provided in the 2MASS catalog (for the data in this region the
observation date is the same, so we take only the 0th value)::

    >>> catalog_2mass = SkyCoord(ra=result_2mass['RAJ2000'] * u.deg,
    ...                          dec=result_2mass['DEJ2000'] * u.deg)
    >>> epoch_2mass = Time(result_2mass['Date'][0])

We can now use the :meth:`~astropy.coordinates.SkyCoord.apply_space_motion`
method to compute the position of the TGAS source at another epoch. This uses
the proper motion and parallax information to evolve the position of the source
assuming straight-line motion::

    >>> c_2mass_epoch = c.apply_space_motion(epoch_2mass)

Now that we have the coordinates of the TGAS source at the 2MASS epoch, we can
do the cross-match (see also :ref:`astropy-coordinates-separations-matching`)::

    >>> idx, sep, _ = c_2mass_epoch.match_to_catalog_sky(catalog_2mass) # doctest: +SKIP
    >>> sep[0].to_string() # doctest: +FLOAT_CMP +SKIP
    '0d00m00.2818s'
    >>> idx # doctest: +SKIP
    array(5)

The closest source it found is 0.2818 arcseconds away and corresponds to
row index 5 in the 2MASS catalog. We can then, for example, compute *Gaia*-2MASS
colors::

    >>> G = result_tgas['phot_g_mean_mag']
    >>> J = result_2mass['Jmag'][idx] # doctest: +SKIP
    >>> K = result_2mass['Kmag'][idx] # doctest: +SKIP
    >>> G - J, G - K # doctest: +SKIP
    (1.0151743233481962, 1.3731746233481958)

..
  EXAMPLE END
.. We call EarthLocation.of_site here first to force the downloading
.. of sites.json so that future doctest output isn't cluttered with
.. "Downloading ... [done]". This can be removed once we have a better
.. way of ignoring output lines based on pattern-matching, e.g.:
.. https://github.com/astropy/pytest-doctestplus/issues/11

.. testsetup::
    >>> from astropy.coordinates import EarthLocation
    >>> EarthLocation.of_site('greenwich') # doctest: +IGNORE_OUTPUT +IGNORE_WARNINGS

Using and Designing Coordinate Frames
*************************************

In `astropy.coordinates`, as outlined in the
:ref:`astropy-coordinates-overview`, subclasses of |BaseFrame| ("frame
classes") define particular coordinate frames. They can (but do not
*have* to) contain representation objects storing the actual coordinate
data. The actual coordinate transformations are defined as functions
that transform representations between frame classes. This approach
serves to separate high-level user functionality (see :doc:`skycoord`)
and details of how the coordinates are actually stored (see
:doc:`representations`) from the definition of frames and how they are
transformed.

Using Frame Objects
===================

Frames without Data
-------------------

Frame objects have two distinct (but related) uses. The first is
storing the information needed to uniquely define a frame (e.g.,
equinox, observation time). This information is stored on the frame
objects as (read-only) Python attributes, which are set when the object
is first created::

    >>> from astropy.coordinates import ICRS, FK5
    >>> FK5(equinox='J1975')
    <FK5 Frame (equinox=J1975.000)>
    >>> ICRS()  # has no attributes
    <ICRS Frame>
    >>> FK5()  # uses default equinox
    <FK5 Frame (equinox=J2000.000)>

The specific names of attributes available for a particular frame (and
their default values) are available as the class method
``get_frame_attr_names``::

    >>> FK5.get_frame_attr_names()
    {'equinox': <Time object: scale='tt' format='jyear_str' value=J2000.000>}

You can access any of the attributes on a frame by using standard Python
attribute access. Note that for cases like ``equinox``, which are time
inputs, if you pass in any unambiguous time string, it will be converted
into an `~astropy.time.Time` object (see
:ref:`astropy-time-inferring-input`)::

    >>> f = FK5(equinox='J1975')
    >>> f.equinox
    <Time object: scale='tt' format='jyear_str' value=J1975.000>
    >>> f = FK5(equinox='2011-05-15T12:13:14')
    >>> f.equinox
    <Time object: scale='utc' format='isot' value=2011-05-15T12:13:14.000>


Frames with Data
----------------

The second use for frame objects is to store actual realized coordinate
data for frames like those described above. In this use, it is similar
to the |SkyCoord| class, and in fact, the |SkyCoord| class internally
uses the frame classes as its implementation. However, the frame
classes have fewer "convenience" features, thereby streamlining the
implementation of frame classes. As such, they are created
similarly to |SkyCoord| objects. One suggested way is to use
with keywords appropriate for the frame (e.g., ``ra`` and ``dec`` for
equatorial systems)::

    >>> from astropy import units as u
    >>> ICRS(ra=1.1*u.deg, dec=2.2*u.deg)  # doctest: +FLOAT_CMP
    <ICRS Coordinate: (ra, dec) in deg
        (1.1, 2.2)>
    >>> FK5(ra=1.1*u.deg, dec=2.2*u.deg, equinox='J1975')  # doctest: +FLOAT_CMP
    <FK5 Coordinate (equinox=J1975.000): (ra, dec) in deg
        (1.1, 2.2)>

These same attributes can be used to access the data in the frames as
|Angle| objects (or |Angle| subclasses)::

    >>> coo = ICRS(ra=1.1*u.deg, dec=2.2*u.deg)
    >>> coo.ra  # doctest: +FLOAT_CMP
    <Longitude 1.1 deg>
    >>> coo.ra.value  # doctest: +FLOAT_CMP
    1.1
    >>> coo.ra.to(u.hourangle)  # doctest: +FLOAT_CMP
    <Longitude 0.07333333 hourangle>

You can use the ``representation_type`` attribute in conjunction
with the ``representation_component_names`` attribute to figure out what
keywords are accepted by a particular class object. The former will be the
representation class in which the system is expressed (e.g., spherical for
equatorial frames), and the latter will be a dictionary mapping names for that
frame to the attribute name on the representation class::

    >>> import astropy.units as u
    >>> icrs = ICRS(1*u.deg, 2*u.deg)
    >>> icrs.representation_type
    <class 'astropy.coordinates.representation.SphericalRepresentation'>
    >>> icrs.representation_component_names
    {'ra': 'lon', 'dec': 'lat', 'distance': 'distance'}

You can get the data in a different representation if needed::

    >>> icrs.represent_as('cartesian')  # doctest: +FLOAT_CMP
    <CartesianRepresentation (x, y, z) [dimensionless]
         (0.99923861, 0.01744177, 0.0348995)>

The representation of the coordinate object can also be changed directly, as
shown below. This does *nothing* to the object internal data which stores the
coordinate values, but it changes the external view of that data in two ways:
(1) the object prints itself in accord with the new representation, and (2) the
available attributes change to match those of the new representation (e.g., from
``ra, dec, distance`` to ``x, y, z``). Setting the ``representation_type``
thus changes a *property* of the object (how it appears) without changing the
intrinsic object itself which represents a point in 3D space.::

    >>> from astropy.coordinates import CartesianRepresentation
    >>> icrs.representation_type = CartesianRepresentation
    >>> icrs  # doctest: +FLOAT_CMP
    <ICRS Coordinate: (x, y, z) [dimensionless]
        (0.99923861, 0.01744177, 0.0348995)>
    >>> icrs.x  # doctest: +FLOAT_CMP
    <Quantity 0.99923861>

The representation can also be set at the time of creating a coordinate
and affects the set of keywords used to supply the coordinate data. For
example, to create a coordinate with Cartesian data do::

    >>> ICRS(x=1*u.kpc, y=2*u.kpc, z=3*u.kpc, representation_type='cartesian')  #  doctest: +FLOAT_CMP
    <ICRS Coordinate: (x, y, z) in kpc
        (1., 2., 3.)>

For more information about the use of representations in coordinates see the
:ref:`astropy-skycoord-representations` section, and for details about the
representations themselves see :ref:`astropy-coordinates-representations`.

There are two other ways to create frame classes with coordinates. A
representation class can be passed in directly at creation, along with
any frame attributes required::

    >>> from astropy.coordinates import SphericalRepresentation
    >>> rep = SphericalRepresentation(lon=1.1*u.deg, lat=2.2*u.deg, distance=3.3*u.kpc)
    >>> FK5(rep, equinox='J1975')  # doctest: +FLOAT_CMP
    <FK5 Coordinate (equinox=J1975.000): (ra, dec, distance) in (deg, deg, kpc)
        (1.1, 2.2, 3.3)>

A final way is to create a frame object from an already existing frame
(either one with or without data), using the ``realize_frame`` method. This
will yield a frame with the same attributes, but new data::

    >>> f1 = FK5(equinox='J1975')
    >>> f1
    <FK5 Frame (equinox=J1975.000)>
    >>> rep = SphericalRepresentation(lon=1.1*u.deg, lat=2.2*u.deg, distance=3.3*u.kpc)
    >>> f1.realize_frame(rep)  # doctest: +FLOAT_CMP
    <FK5 Coordinate (equinox=J1975.000): (ra, dec, distance) in (deg, deg, kpc)
        (1.1, 2.2, 3.3)>

You can check if a frame object has data using the ``has_data`` attribute, and
if it is present, it can be accessed from the ``data`` attribute::

    >>> ICRS().has_data
    False
    >>> cooi = ICRS(ra=1.1*u.deg, dec=2.2*u.deg)
    >>> cooi.has_data
    True
    >>> cooi.data  # doctest: +FLOAT_CMP
    <UnitSphericalRepresentation (lon, lat) in deg
        (1.1, 2.2)>

All of the above methods can also accept array data (in the form of
class:`~astropy.units.Quantity`, or other Python sequences) to create arrays of
coordinates::

    >>> ICRS(ra=[1.5, 2.5]*u.deg, dec=[3.5, 4.5]*u.deg)  # doctest: +FLOAT_CMP
    <ICRS Coordinate: (ra, dec) in deg
        [(1.5, 3.5), (2.5, 4.5)]>

If you pass in mixed arrays and scalars, the arrays will be broadcast
over the scalars appropriately::

    >>> ICRS(ra=[1.5, 2.5]*u.deg, dec=[3.5, 4.5]*u.deg, distance=5*u.kpc)  # doctest: +FLOAT_CMP
    <ICRS Coordinate: (ra, dec, distance) in (deg, deg, kpc)
        [(1.5, 3.5, 5.), (2.5, 4.5, 5.)]>

Similar broadcasting happens if you transform to another frame. For example::

    >>> import numpy as np
    >>> from astropy.coordinates import EarthLocation, AltAz
    >>> coo = ICRS(ra=180.*u.deg, dec=51.477811*u.deg)
    >>> lf = AltAz(location=EarthLocation.of_site('greenwich'),
    ...            obstime=['2012-03-21T00:00:00', '2012-06-21T00:00:00'])
    >>> lcoo = coo.transform_to(lf)  # this can load finals2000A.all # doctest: +REMOTE_DATA +IGNORE_OUTPUT
    >>> lcoo  # doctest: +REMOTE_DATA +FLOAT_CMP
    <AltAz Coordinate (obstime=['2012-03-21T00:00:00.000' '2012-06-21T00:00:00.000'], location=(3980608.9024681724, -102.47522910648239, 4966861.273100675) m, pressure=0.0 hPa, temperature=0.0 deg_C, relative_humidity=0.0, obswl=1.0 micron): (az, alt) in deg
        [( 94.71264993, 89.21424259), (307.69488825, 37.98077772)]>

Above, the shapes  ``()`` for ``coo`` and ``(2,)`` for ``lf``  were
broadcast against each other. If you wish to determine the positions for a
set of coordinates, you will need to make sure that the shapes allow this::

    >>> coo2 = ICRS(ra=[180., 225., 270.]*u.deg, dec=[51.5, 0., 51.5]*u.deg)
    >>> coo2.transform_to(lf)
    Traceback (most recent call last):
    ...
    ValueError: operands could not be broadcast together...
    >>> coo2.shape
    (3,)
    >>> lf.shape
    (2,)
    >>> lf2 = lf[:, np.newaxis]
    >>> lf2.shape
    (2, 1)
    >>> coo2.transform_to(lf2)  # doctest:  +REMOTE_DATA +FLOAT_CMP
    <AltAz Coordinate (obstime=[['2012-03-21T00:00:00.000' '2012-03-21T00:00:00.000'
      '2012-03-21T00:00:00.000']
     ['2012-06-21T00:00:00.000' '2012-06-21T00:00:00.000'
      '2012-06-21T00:00:00.000']], location=(3980608.90246817, -102.47522911, 4966861.27310068) m, pressure=0.0 hPa, temperature=0.0 deg_C, relative_humidity=0.0, obswl=1.0 micron): (az, alt) in deg
        [[( 93.09845183, 89.21613128), (126.85789664, 25.4660055 ),
          ( 51.37993234, 37.18532527)],
         [(307.71713698, 37.99437658), (231.3740787 , 26.36768329),
          ( 85.42187236, 89.69297998)]]>

.. Note::
   Frames without data have a ``shape`` that is determined by their frame
   attributes. For frames with data, the ``shape`` always is that of the data;
   any non-scalar attributes are broadcast to have matching shapes
   (as can be seen for ``obstime`` in the last line above).

Coordinate values in a array-valued frame object can be modified in-place
(added in astropy 4.1). This requires that the new values be set from an
another frame object that is equivalent in all ways except for the actual
coordinate data values. In this way, no frame transformations are required and
the item setting operation is extremely robust.

To modify an array of coordinates use the same syntax for a numpy array::

  >>> coo1 = ICRS([1, 2] * u.deg, [3, 4] * u.deg)
  >>> coo2 = ICRS(10 * u.deg, 20 * u.deg)
  >>> coo1[0] = coo2
  >>> coo1
  <ICRS Coordinate: (ra, dec) in deg
      [(10., 20.), ( 2.,  4.)]>

This method is relatively slow because it requires setting from an
existing frame object and it performs extensive validation to ensure
that the operation is valid. For some applications it may be necessary to
take a different lower-level approach which is described in the section
:ref:`astropy-coordinates-fast-in-place`.

.. warning::

  You may be tempted to try an apparently obvious way of modifying a frame
  object in place by updating the component attributes directly, for example
  ``coo1.ra[1] = 40 * u.deg``. However, while this will *appear* to give a correct
  result it does not actually modify the underlying representation data. This
  is related to the current implementation of performance-based caching.
  The current cache implementation is similarly unable to handle in-place changes
  to the representation (``.data``) or frame attributes such as ``.obstime``.

Transforming between Frames
===========================

To transform a frame object with data into another frame, use the
``transform_to`` method of an object, and provide it the frame you wish to
transform to.  This frame should be a frame object (with or without coordinate
data).  If you wish to use all default frame attributes, you can instantiate
the frame class with no arguments (i.e., empty parentheses)::

    >>> cooi = ICRS(1.5*u.deg, 2.5*u.deg)
    >>> cooi.transform_to(FK5())  # doctest: +FLOAT_CMP
    <FK5 Coordinate (equinox=J2000.000): (ra, dec) in deg
        (1.50000661, 2.50000238)>
    >>> cooi.transform_to(FK5(equinox='J1975'))  # doctest: +FLOAT_CMP
    <FK5 Coordinate (equinox=J1975.000): (ra, dec) in deg
        (1.17960348, 2.36085321)>

The :ref:`astropy-coordinates-api` includes a list of all of the frames built
into `astropy.coordinates`, as well as the defined transformations between
them. Any transformation that has a valid path, even if it passes through
other frames, can be transformed too. To programmatically check for or
manipulate transformations, see the `~astropy.coordinates.TransformGraph`
documentation.


.. _astropy-coordinates-design:

Defining a New Frame
====================

Implementing a new frame class that connects to the ``astropy.coordinates``
infrastructure can be done by subclassing
`~astropy.coordinates.BaseCoordinateFrame`. Some guidance and examples are given
below, but detailed instructions for creating new frames are given in the
docstring of `~astropy.coordinates.BaseCoordinateFrame`.

All frame classes must specify a default representation for the coordinate
positions by, at minimum, defining a ``default_representation`` class attribute
(see :ref:`astropy-coordinates-representations` for more information about the
supported ``Representation`` objects).

Examples
--------

..
  EXAMPLE START
  Defining a New Frame Class that Connects to astropy.coordinates

To create a new frame that, by default, expects to receive its coordinate data
in spherical coordinates, we would create a subclass as follows::

    >>> from astropy.coordinates import BaseCoordinateFrame
    >>> import astropy.coordinates.representation as r
    >>> class MyFrame1(BaseCoordinateFrame):
    ...     # Specify how coordinate values are represented when outputted
    ...     default_representation = r.SphericalRepresentation

Already, this is a valid frame class::

    >>> fr = MyFrame1(1*u.deg, 2*u.deg)
    >>> fr # doctest: +FLOAT_CMP
    <MyFrame1 Coordinate: (lon, lat) in deg
        (1., 2.)>
    >>> fr.lon # doctest: +FLOAT_CMP
    <Longitude 1. deg>

However, as we have defined it above, (1) the coordinate component names will be
the same as used in the specified ``default_representation`` (in this case,
``lon``, ``lat``, and ``distance`` for longitude, latitude, and distance,
respectively), (2) this frame does not have any additional attributes or
metadata, (3) this frame does not support transformations to any other
coordinate frame, and (4) this frame does not support velocity data. We can
address each of these points by seeing some other ways of customizing frame
subclasses.

..
  EXAMPLE END

Customizing Frame Component Names
---------------------------------

First, as mentioned in the point (1) :ref:`above <astropy-coordinates-design>`,
some frame classes have special names for their components. For example, the
`~astropy.coordinates.ICRS` frame and other equatorial frame classes often use
"Right Ascension" or "RA" in place of longitude, and "Declination" or "Dec." in
place of latitude. These component name overrides, which change the frame
component name defaults taken from the ``Representation`` classes, are defined
by specifying a set of `~astropy.coordinates.RepresentationMapping` instances
(one per component) as a part of defining an additional class attribute on a
frame class: ``frame_specific_representation_info``. This attribute must be a
dictionary, and the keys should be either ``Representation`` or ``Differential``
classes (see below for a discussion about customizing behavior for velocity
components, which is done with the ``Differential`` classes). Using our example
frame implemented above, we can customize it to use the names "R" and "D"
instead of "lon" and "lat"::

    >>> from astropy.coordinates import RepresentationMapping
    >>> class MyFrame2(BaseCoordinateFrame):
    ...     # Specify how coordinate values are represented when outputted
    ...     default_representation = r.SphericalRepresentation
    ...
    ...     # Override component names (e.g., "ra" instead of "lon")
    ...     frame_specific_representation_info = {
    ...         r.SphericalRepresentation: [RepresentationMapping('lon', 'R'),
    ...                                     RepresentationMapping('lat', 'D')]
    ...     }

With this frame, we can now use the names ``R`` and ``D`` to access the frame
data::

    >>> fr = MyFrame2(3*u.deg, 4*u.deg)
    >>> fr # doctest: +FLOAT_CMP
    <MyFrame2 Coordinate: (R, D) in deg
        (3., 4.)>
    >>> fr.R # doctest: +FLOAT_CMP
    <Longitude 3. deg>

We can specify name mappings for any ``Representation`` class in
``astropy.coordinates`` to change the default component names. For example, the
`~astropy.coordinates.Galactic` frame uses the standard longitude and latitude
names "l" and "b" when used with a
`~astropy.coordinates.SphericalRepresentation`, but uses the component names
"x", "y", and "z" when the representation is changed to a
`~astropy.coordinates.CartesianRepresentation`. With our example above, we could
add an additional set of mappings to override the Cartesian component names to
be "a", "b", and "c" instead of the default "x", "y", and "z"::

    >>> class MyFrame3(BaseCoordinateFrame):
    ...     # Specify how coordinate values are represented when outputted
    ...     default_representation = r.SphericalRepresentation
    ...
    ...     # Override component names (e.g., "ra" instead of "lon")
    ...     frame_specific_representation_info = {
    ...         r.SphericalRepresentation: [RepresentationMapping('lon', 'R'),
    ...                                     RepresentationMapping('lat', 'D')],
    ...         r.CartesianRepresentation: [RepresentationMapping('x', 'a'),
    ...                                     RepresentationMapping('y', 'b'),
    ...                                     RepresentationMapping('z', 'c')]
    ...     }

For any `~astropy.coordinates.RepresentationMapping`, you can also specify a
default unit for the component by setting the ``defaultunit`` keyword argument.


Defining Frame Attributes
-------------------------

Second, as indicated by the point (2) in the :ref:`introduction above
<astropy-coordinates-design>`, it is often useful for coordinate frames to allow
specifying frame "attributes" that may specify additional data or parameters
needed in order to fully specify transformations between a given frame and some
other frame. For example, the `~astropy.coordinates.FK5` frame allows specifying
an ``equinox`` that helps define the transformation between
`~astropy.coordinates.FK5` and the `~astropy.coordinates.ICRS` frame. Frame
attributes are defined by creating class attributes that are instances of
`~astropy.coordinates.Attribute` or its subclasses (e.g.,
`~astropy.coordinates.TimeAttribute`, `~astropy.coordinates.QuantityAttribute`,
etc.). If attributes are defined using these classes, there is often no need to
define an ``__init__`` function, as the initializer in
`~astropy.coordinates.BaseCoordinateFrame` will probably behave in the way you
want. Let us now modify the above toy frame class implementation to add two
frame attributes::

    >>> from astropy.coordinates import TimeAttribute, QuantityAttribute
    >>> class MyFrame4(BaseCoordinateFrame):
    ...     # Specify how coordinate values are represented when outputted
    ...     default_representation = r.SphericalRepresentation
    ...
    ...     # Override component names (e.g., "ra" instead of "lon")
    ...     frame_specific_representation_info = {
    ...         r.SphericalRepresentation: [RepresentationMapping('lon', 'R'),
    ...                                     RepresentationMapping('lat', 'D')],
    ...         r.CartesianRepresentation: [RepresentationMapping('x', 'a'),
    ...                                     RepresentationMapping('y', 'b'),
    ...                                     RepresentationMapping('z', 'c')]
    ...     }
    ...
    ...     # Specify frame attributes required to fully specify the frame
    ...     time = TimeAttribute(default='B1950')
    ...     orientation = QuantityAttribute(default=42*u.deg)

Without specifying an initializer, defining these attributes tells the
`~astropy.coordinates.BaseCoordinateFrame` what to expect in terms of additional
arguments passed in to our subclass initializer. For example, when defining a
frame instance with our subclass, we can now optionally specify values for these
attributes::

    >>> fr = MyFrame4(R=1*u.deg, D=2*u.deg, orientation=21*u.deg)
    >>> fr # doctest: +FLOAT_CMP
    <MyFrame4 Coordinate (time=B1950.000, orientation=21.0 deg): (R, D) in deg
        (1., 2.)>

Note that we specified both frame attributes with default values, so they are
optional arguments to the frame initializer. Note also that the frame attributes
now appear in the ``repr`` of the frame instance above. As a bonus, for most of
the ``Attribute`` subclasses, even without defining an initializer, attributes
specified as arguments will be validated. For example, arguments passed in to
`~astropy.coordinates.QuantityAttribute` attributes will be checked that they
have valid and compatible units with the expected attribute units. Using our
frame example above, which expects an ``orientation`` with angular units,
passing in a time results in an error::

    >>> MyFrame4(R=1*u.deg, D=2*u.deg, orientation=55*u.microyear) # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
    ...
    UnitConversionError: 'uyr' (time) and 'deg' (angle) are not convertible

When defining frame attributes, you do not always have to specify a default
value as long as the ``Attribute`` subclass is able to validate the input. For
example, with the above frame, if the ``orientation`` does not require a default
value but we still want to enforce it to have angular units, we could instead
define it as::

    orientation = QuantityAttribute(unit=u.deg)

In the above case, if ``orientation`` is not specified when a new frame instance
is created, its value will be `None`: Note that it is up to the frame
classes and transformation function implementations to define how to handle a
`None` value. In most cases `None` should signify a special case like "use a
different frame attribute for this value" or similar.

Customizing Display of Attributes
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

While the default `repr` for coordinate frames is suitable for most cases, you
may want to customize how frame attributes are displayed in certain cases. To
do this you can define a method named ``_astropy_repr_in_frame``. This method
should be defined on the object that is set to the frame attribute itself,
**not** the `~astropy.coordinates.Attribute` descriptor.

Example
^^^^^^^

..
  EXAMPLE START
  Customizing Display of Attributes in Coordinate Frames

As an example of method ``_astropy_repr_in_frame``, say you have an
object ``Spam`` which you have as an attribute of your frame::

  >>> class Spam:
  ...     def _astropy_repr_in_frame(self):
  ...         return "<A can of Spam>"

If your frame has this class as an attribute::

  >>> from astropy.coordinates import Attribute
  >>> class Egg(BaseCoordinateFrame):
  ...     can = Attribute(default=Spam())

When it is displayed by the frame it will use the result of
``_astropy_repr_in_frame``::

  >>> Egg()
  <Egg Frame (can=<A can of Spam>)>

..
  EXAMPLE END

Defining Transformations between Frames
---------------------------------------

As indicated by the point (3) in the :ref:`introduction above
<astropy-coordinates-design>`, a frame class on its own is likely not very
useful until transformations are defined between it and other coordinate frame
classes. The key concept for defining transformations in ``astropy.coordinates``
is the "frame transform graph" (in the "graph theory" sense, not "plot"), which
stores all of the transformations between the built-in frames, as well as tools
for finding the shortest paths through this graph to transform from any frame to
any other by composing the transformations. The power behind this concept is
available to user-created frames as well, meaning that once you define even one
transform from your frame to any frame in the graph, coordinates defined in your
frame can be transformed to *any* other frame in the graph. The "frame transform
graph" is available in code as ``astropy.coordinates.frame_transform_graph``,
which is an instance of the `~astropy.coordinates.TransformGraph` class.

The transformations themselves are represented as
`~astropy.coordinates.CoordinateTransform` objects or their subclasses. The
useful subclasses/types of transformations are:

* `~astropy.coordinates.FunctionTransform`

    A transform that is defined as a function that takes a frame object
    of one frame class and returns an object of another class.

* `~astropy.coordinates.AffineTransform`

    A transformation that includes a linear matrix operation and a translation
    (vector offset). These transformations are defined by a 3x3 matrix and a
    3-vector for the offset (supplied as a Cartesian representation). The
    transformation is applied to the Cartesian representation of one frame and
    transforms into the Cartesian representation of the target frame.

* `~astropy.coordinates.StaticMatrixTransform`
* `~astropy.coordinates.DynamicMatrixTransform`

    The matrix transforms are `~astropy.coordinates.AffineTransform`
    transformations without a translation (i.e., only a rotation). The static
    version is for the case where the matrix is independent of the frame
    attributes (e.g., the ICRS->FK5 transformation, because ICRS has no frame
    attributes). The dynamic case is for transformations where the
    transformation matrix depends on the frame attributes of either the
    to or from frame.

Generally, it is not necessary to use these classes directly. Instead,
use methods on the ``frame_transform_graph`` that can be used as function
decorators. Define functions that either do the actual
transformation (for `~astropy.coordinates.FunctionTransform`), or that compute
the necessary transformation matrices to transform. Then decorate the functions
to register these transformations with the frame transform graph::

    from astropy.coordinates import frame_transform_graph

    @frame_transform_graph.transform(DynamicMatrixTransform, ICRS, FK5)
    def icrs_to_fk5(icrscoord, fk5frame):
        ...

    @frame_transform_graph.transform(DynamicMatrixTransform, FK5, ICRS)
    def fk5_to_icrs(fk5coord, icrsframe):
        ...

If the transformation to your coordinate frame of interest is not
representable by a matrix operation, you can also specify a function to do
the actual transformation, and pass the
`~astropy.coordinates.FunctionTransform` class to the transform graph
decorator instead::

    @frame_transform_graph.transform(FunctionTransform, FK4NoETerms, FK4)
    def fk4_no_e_to_fk4(fk4noecoord, fk4frame):
        ...

Furthermore, the ``frame_transform_graph`` does some caching and
optimization to speed up transformations after the first attempt to go
from one frame to another, and shortcuts steps where relevant (for
example, combining multiple static matrix transforms into a single
matrix). Hence, in general, it is better to define whatever are the
most natural transformations for a user-defined frame, rather than
worrying about optimizing or caching a transformation to speed up the
process.

For a demonstration of how to define transformation functions that also work for
transforming velocity components, see
:ref:`astropy-coordinate-transform-with-velocities`.


Supporting Velocity Data in Frames
----------------------------------

As alluded to by point (4) in the :ref:`introduction above
<astropy-coordinates-design>`, the examples we have seen above mostly deal with
customizing frame behavior for positional information. (For some context about
how velocities are handled in ``astropy.coordinates``, it may be useful to read
the overview: :ref:`astropy-coordinate-custom-frame-with-velocities`.)

When defining a frame class, it is also possible to set a
``default_differential`` (analogous to ``default_representation``), and to
customize how velocity data components are named. Expanding on our custom frame
example above, we can use `~astropy.coordinates.RepresentationMapping` to
override ``Differential`` component names. The default ``Differential``
components are typically named after the corresponding ``Representation``
component, preceded by ``d_``. So, for example, the longitude ``Differential``
component is, by default, ``d_lon``. However, there are some defaults to be
aware of. Here, if we set the default ``Differential`` class to also be
Spherical, it will implement a set of default "nicer" names for the velocity
components, mapping ``pm_R`` to ``d_lon``, ``pm_D`` to ``d_lat``, and
``radial_velocity`` to ``d_distance`` (taking the previously overridden
longitude and latitude component names)::

    >>> class MyFrame4WithVelocity(BaseCoordinateFrame):
    ...     # Specify how coordinate values are represented when outputted
    ...     default_representation = r.SphericalRepresentation
    ...     default_differential = r.SphericalDifferential
    ...
    ...     # Override component names (e.g., "ra" instead of "lon")
    ...     frame_specific_representation_info = {
    ...         r.SphericalRepresentation: [RepresentationMapping('lon', 'R'),
    ...                                     RepresentationMapping('lat', 'D')],
    ...         r.CartesianRepresentation: [RepresentationMapping('x', 'a'),
    ...                                     RepresentationMapping('y', 'b'),
    ...                                     RepresentationMapping('z', 'c')]
    ...     }
    >>> fr = MyFrame4WithVelocity(R=1*u.deg, D=2*u.deg,
    ...                           pm_R=3*u.mas/u.yr, pm_D=4*u.mas/u.yr)
    >>> fr # doctest: +FLOAT_CMP
    <MyFrame4WithVelocity Coordinate: (R, D) in deg
        (1., 2.)
    (pm_R, pm_D) in mas / yr
        (3., 4.)>

If you want to override the default "nicer" names, you can specify a new key in
the ``frame_specific_representation_info`` for any of the ``Differential``
classes, for example::

    >>> class MyFrame4WithVelocity2(BaseCoordinateFrame):
    ...     # Specify how coordinate values are represented when outputted
    ...     default_representation = r.SphericalRepresentation
    ...     default_differential = r.SphericalDifferential
    ...
    ...     # Override component names (e.g., "ra" instead of "lon")
    ...     frame_specific_representation_info = {
    ...         r.SphericalRepresentation: [RepresentationMapping('lon', 'R'),
    ...                                     RepresentationMapping('lat', 'D')],
    ...         r.CartesianRepresentation: [RepresentationMapping('x', 'a'),
    ...                                     RepresentationMapping('y', 'b'),
    ...                                     RepresentationMapping('z', 'c')],
    ...         r.SphericalDifferential: [RepresentationMapping('d_lon', 'pm1'),
    ...                                   RepresentationMapping('d_lat', 'pm2'),
    ...                                   RepresentationMapping('d_distance', 'rv')]
    ...     }
    >>> fr = MyFrame4WithVelocity2(R=1*u.deg, D=2*u.deg,
    ...                           pm1=3*u.mas/u.yr, pm2=4*u.mas/u.yr)
    >>> fr # doctest: +FLOAT_CMP
    <MyFrame4WithVelocity2 Coordinate: (R, D) in deg
        (1., 2.)
    (pm1, pm2) in mas / yr
        (3., 4.)>


Final Notes
-----------

You can also define arbitrary methods for any added functionality you
want your frame to have that is unique to that frame. These methods will
be available in any |SkyCoord| that is created using your user-defined
frame.

For examples of defining frame classes, the first place to look is
at the source code for the frames that are included in ``astropy``
(available at ``astropy.coordinates.builtin_frames``). These are not
special-cased, but rather use all of the same API and features available to
user-created frames.

.. topic:: Examples:

    See also :ref:`sphx_glr_generated_examples_coordinates_plot_sgr-coordinate-frame.py`
    for a more annotated example of defining a new coordinate frame.
.. _coordinates-galactocentric:

**************************************************
Description of the Galactocentric Coordinate Frame
**************************************************

While many other frames implemented in `astropy.coordinates` are standardized in
some way (e.g., defined by the IAU), there is no standard Milky Way
reference frame with the center of the Milky Way as its origin. (This is
distinct from `~astropy.coordinates.Galactic` coordinates, which point
toward the Galactic Center but have their origin in the Solar System).
The `~astropy.coordinates.Galactocentric` frame
class is meant to be flexible enough to support all common definitions of such a
transformation, but with reasonable default parameter values, such as the solar
velocity relative to the Galactic center, the solar height above the Galactic
midplane, etc. Below, :ref:`we describe our generalized definition of the
transformation <astropy-coordinates-galactocentric-transformation>` from the
ICRS to/from Galactocentric coordinates, and :ref:`describe how to customize the
default Galactocentric parameters
<astropy-coordinates-galactocentric-defaults>` that are used when the
`~astropy.coordinates.Galactocentric` frame is initialized without explicitly
passing in parameter values.


.. _astropy-coordinates-galactocentric-transformation:

Definition of the Transformation
================================

This document describes the mathematics behind the transformation from
`~astropy.coordinates.ICRS` to `~astropy.coordinates.Galactocentric`
coordinates. This is described in detail here on account of the mathematical
subtleties and the fact that there is no official standard/definition for this
frame. For examples of how to use this transformation in code, see the
the *Examples* section of the `~astropy.coordinates.Galactocentric` class
documentation.

We assume that we start with a 3D position in the ICRS reference frame:
a Right Ascension, Declination, and heliocentric distance,
:math:`(\alpha, \delta, d)`. We can convert this to a Cartesian position using
the standard transformation from Cartesian to spherical coordinates:

.. math::

   \begin{aligned}
       x_{\rm icrs} &= d\cos{\alpha}\cos{\delta}\\
       y_{\rm icrs} &= d\sin{\alpha}\cos{\delta}\\
       z_{\rm icrs} &= d\sin{\delta}\\
       \boldsymbol{r}_{\rm icrs} &= \begin{pmatrix}
         x_{\rm icrs}\\
         y_{\rm icrs}\\
         z_{\rm icrs}
       \end{pmatrix}\end{aligned}

The first transformation rotates the :math:`x_{\rm icrs}` axis so that the new
:math:`x'` axis points towards the Galactic Center (GC), specified by the ICRS
position :math:`(\alpha_{\rm GC}, \delta_{\rm GC})` (in the
`~astropy.coordinates.Galactocentric` frame, this is controlled by the frame
attribute ``galcen_coord``):

.. math::

   \begin{aligned}
       \boldsymbol{R}_1 &= \begin{bmatrix}
         \cos\delta_{\rm GC}& 0 & \sin\delta_{\rm GC}\\
         0 & 1 & 0 \\
         -\sin\delta_{\rm GC}& 0 & \cos\delta_{\rm GC}\end{bmatrix}\\
       \boldsymbol{R}_2 &=
       \begin{bmatrix}
         \cos\alpha_{\rm GC}& \sin\alpha_{\rm GC}& 0\\
         -\sin\alpha_{\rm GC}& \cos\alpha_{\rm GC}& 0\\
         0 & 0 & 1
       \end{bmatrix}.\end{aligned}

The transformation thus far has aligned the :math:`x'` axis with the
vector pointing from the Sun to the GC, but the :math:`y'` and
:math:`z'` axes point in arbitrary directions. We adopt the
orientation of the Galactic plane as the normal to the north pole of
Galactic coordinates defined by the IAU
(`Blaauw et. al. 1960 <https://ui.adsabs.harvard.edu/abs/1960MNRAS.121..164B>`_).
This extra roll angle, :math:`\eta`, was measured by transforming a grid
of points along :math:`l=0` to this interim frame and minimizing the square
of their :math:`y'` positions. We find:

.. math::

   \begin{aligned}
       \eta &= 58.5986320306^\circ\\
       \boldsymbol{R}_3 &=
       \begin{bmatrix}
         1 & 0 & 0\\
         0 & \cos\eta & \sin\eta\\
         0 & -\sin\eta & \cos\eta
       \end{bmatrix}\end{aligned}

The full rotation matrix thus far is:

.. math::

   \begin{gathered}
       \boldsymbol{R} = \boldsymbol{R}_3 \boldsymbol{R}_1 \boldsymbol{R}_2 = \\
       \begin{bmatrix}
         \cos\alpha_{\rm GC}\cos\delta_{\rm GC}& \cos\delta_{\rm GC}\sin\alpha_{\rm GC}& -\sin\delta_{\rm GC}\\
         \cos\alpha_{\rm GC}\sin\delta_{\rm GC}\sin\eta - \sin\alpha_{\rm GC}\cos\eta & \sin\alpha_{\rm GC}\sin\delta_{\rm GC}\sin\eta + \cos\alpha_{\rm GC}\cos\eta & \cos\delta_{\rm GC}\sin\eta\\
         \cos\alpha_{\rm GC}\sin\delta_{\rm GC}\cos\eta + \sin\alpha_{\rm GC}\sin\eta & \sin\alpha_{\rm GC}\sin\delta_{\rm GC}\cos\eta - \cos\alpha_{\rm GC}\sin\eta & \cos\delta_{\rm GC}\cos\eta
       \end{bmatrix}\end{gathered}

With the rotated position vector
:math:`\boldsymbol{R}\boldsymbol{r}_{\rm icrs}`, we can now subtract the
distance to the GC, :math:`d_{\rm GC}`, which is purely along the
:math:`x'` axis:

.. math::

   \begin{aligned}
       \boldsymbol{r}' &= \boldsymbol{R}\boldsymbol{r}_{\rm icrs} - d_{\rm GC}\hat{\boldsymbol{x}}_{\rm GC}.\end{aligned}

where :math:`\hat{\boldsymbol{x}}_{\rm GC} = (1,0,0)^{\mathsf{T}}`.

The final transformation accounts for the (specified) height of the Sun above
the Galactic midplane by rotating about the final :math:`y''` axis by
the angle :math:`\theta= \sin^{-1}(z_\odot / d_{\rm GC})`:

.. math::

   \begin{aligned}
       \boldsymbol{H} &=
       \begin{bmatrix}
         \cos\theta & 0 & \sin\theta\\
         0 & 1 & 0\\
         -\sin\theta & 0 & \cos\theta
       \end{bmatrix}\end{aligned}

where :math:`z_\odot` is the measured height of the Sun above the
midplane.

The full transformation is then:

.. math:: \boldsymbol{r}_{\rm GC} = \boldsymbol{H} \left( \boldsymbol{R}\boldsymbol{r}_{\rm icrs} - d_{\rm GC}\hat{\boldsymbol{x}}_{\rm GC}\right).

.. topic:: Examples:

    For an example of how to use the `~astropy.coordinates.Galactocentric`
    frame, see
    :ref:`sphx_glr_generated_examples_coordinates_plot_galactocentric-frame.py`.


.. _astropy-coordinates-galactocentric-defaults:

Controlling the Default Frame Parameters
========================================

All of the frame-defining parameters of the
`~astropy.coordinates.Galactocentric` frame are customizable and can be set by
passing arguments in to the `~astropy.coordinates.Galactocentric` initializer.
However, it is often convenient to use the frame without having to pass in every
parameter. Hence, the class comes with reasonable default values for these
parameters, but more precise measurements of the solar position or motion in the
Galaxy are constantly being made. The default values of the
`~astropy.coordinates.Galactocentric` frame attributes will therefore be updated
as necessary with subsequent releases of ``astropy``. We therefore provide a
mechanism to globally or locally control the default parameter values used in
this frame through the `~astropy.coordinates.galactocentric_frame_defaults`
`~astropy.utils.state.ScienceState` class.

The `~astropy.coordinates.galactocentric_frame_defaults` class controls the
default parameter settings in `~astropy.coordinates.Galactocentric` by mapping a
set of string names to particular choices of the parameter values. For an
up-to-date list of valid names, see the docstring of
`~astropy.coordinates.galactocentric_frame_defaults`, but these names are things
like ``'pre-v4.0'``, which sets the default parameter values to their original
definition (i.e. pre-astropy-v4.0) values, and ``'v4.0'``, which sets the
default parameter values to a more modern set of measurements as updated in
Astropy version 4.0. Also, custom sets of measurements can be registered to
`~astropy.coordinates.galactocentric_frame_defaults` and used like the
built-in options.

`~astropy.coordinates.galactocentric_frame_defaults` also tracks the
references (i.e. scientific papers that define the parameter values) for all
parameter values, as well as any further specified metadata information.

As with other `~astropy.utils.state.ScienceState` subclasses, the
`~astropy.coordinates.galactocentric_frame_defaults` class can be used to
globally set the frame defaults at runtime.

Examples
--------

..
  EXAMPLE START
  Setting Galactocentric Coordinate Frame Defaults at Runtime

The default parameter values can be seen by initializing the
`~astropy.coordinates.Galactocentric` frame with no arguments:

::

    >>> from astropy.coordinates import Galactocentric
    >>> Galactocentric()
    <Galactocentric Frame (galcen_coord=<ICRS Coordinate: (ra, dec) in deg
        (266.4051, -28.936175)>, galcen_distance=8.122 kpc, galcen_v_sun=(12.9, 245.6, 7.78) km / s, z_sun=20.8 pc, roll=0.0 deg)>

These default values can be modified using this class::

    >>> from astropy.coordinates import galactocentric_frame_defaults
    >>> _ = galactocentric_frame_defaults.set('v4.0') # doctest: +SKIP
    >>> Galactocentric() # doctest: +SKIP
    <Galactocentric Frame (galcen_coord=<ICRS Coordinate: (ra, dec) in deg
        (266.4051, -28.936175)>, galcen_distance=8.122 kpc, galcen_v_sun=(12.9, 245.6, 7.78) km / s, z_sun=20.8 pc, roll=0.0 deg)>
    >>> _ = galactocentric_frame_defaults.set('pre-v4.0') # doctest: +SKIP
    >>> Galactocentric() # doctest: +SKIP
    <Galactocentric Frame (galcen_coord=<ICRS Coordinate: (ra, dec) in deg
        (266.4051, -28.936175)>, galcen_distance=8.3 kpc, galcen_v_sun=(11.1, 232.24, 7.25) km / s, z_sun=27.0 pc, roll=0.0 deg)>

The default parameters can also be updated by using this class as a context
manager to change the default parameter values locally to a piece of your code::

    >>> with galactocentric_frame_defaults.set('pre-v4.0'):
    ...     print(Galactocentric()) # doctest: +FLOAT_CMP
    <Galactocentric Frame (galcen_coord=<ICRS Coordinate: (ra, dec) in deg
        (266.4051, -28.936175)>, galcen_distance=8.3 kpc, galcen_v_sun=(11.1, 232.24, 7.25) km / s, z_sun=27.0 pc, roll=0.0 deg)>

Again, changing the default parameter values will not affect frame
attributes that are explicitly specified::

    >>> import astropy.units as u
    >>> with galactocentric_frame_defaults.set('pre-v4.0'):
    ...     print(Galactocentric(galcen_distance=8.0*u.kpc)) # doctest: +FLOAT_CMP
    <Galactocentric Frame (galcen_coord=<ICRS Coordinate: (ra, dec) in deg
        (266.4051, -28.936175)>, galcen_distance=8.0 kpc, galcen_v_sun=(11.1, 232.24, 7.25) km / s, z_sun=27.0 pc, roll=0.0 deg)>


Additional parameter sets may be registered, for instance to use the Dehnen & Binney (1998) measurements of the solar motion. We can also add metadata, such as the 1-sigma errors::

    >>> state = galactocentric_frame_defaults.get_from_registry("v4.0")
    >>> state["parameters"]["galcen_v_sun"] = (10.00, 225.25, 7.17) * (u.km / u.s)
    >>> state["references"]["galcen_v_sun"] = "http://www.adsabs.harvard.edu/full/1998MNRAS.298..387D"
    >>> state["error"] = {"galcen_v_sun": (0.36, 0.62, 0.38) * (u.km / u.s)}
    >>> galactocentric_frame_defaults.register(name="DB1998", **state)

Just as in the previous examples, the new parameter set can be get / set::

    >>> state = galactocentric_frame_defaults.get_from_registry("DB1998")
    >>> print(state["error"]["galcen_v_sun"])  # doctest: +FLOAT_CMP
    [0.36 0.62 0.38] km / s

..
  EXAMPLE END

Unless set with the `~astropy.coordinates.galactocentric_frame_defaults` class,
the default parameter values for the `~astropy.coordinates.Galactocentric`
frame are set to ``'latest'``, meaning that the default parameter values may
change if you update Astropy. If you use the
`~astropy.coordinates.Galactocentric` frame without specifying all parameter
values explicitly, we therefore suggest manually setting the frame default set
manually in any science code that depends sensitively on the choice of, e.g.,
solar motion or the other frame parameters.  For example, in such code, we
recommend adding something like this to your import block (here using
``'v4.0'`` as an example)::

    >>> import astropy.coordinates as coord
    >>> coord.galactocentric_frame_defaults.set('v4.0') # doctest: +SKIP
.. _astropy-coordinates-separations-matching:

Separations, Offsets, Catalog Matching, and Related Functionality
*****************************************************************

`astropy.coordinates` contains commonly-used tools for comparing or
matching coordinate objects. Of particular importance are those for
determining separations between coordinates and those for matching a
coordinate (or coordinates) to a catalog. These are mainly implemented
as methods on the coordinate objects.

In the examples below, we will assume that the following imports have already
been executed::

    >>> import astropy.units as u
    >>> from astropy.coordinates import SkyCoord

Separations
===========

The on-sky separation can be computed with the
:meth:`astropy.coordinates.BaseCoordinateFrame.separation` or
:meth:`astropy.coordinates.SkyCoord.separation` methods,
which computes the great-circle distance (*not* the small-angle approximation)::

    >>> c1 = SkyCoord('5h23m34.5s', '-69d45m22s', frame='icrs')
    >>> c2 = SkyCoord('0h52m44.8s', '-72d49m43s', frame='fk5')
    >>> sep = c1.separation(c2)
    >>> sep  # doctest: +FLOAT_CMP
    <Angle 20.74611448 deg>

The returned object is an `~astropy.coordinates.Angle` instance, so it
is possible to access the angle in any of several equivalent angular
units::

    >>> sep.radian  # doctest: +FLOAT_CMP
    0.36208800460262563
    >>> sep.hour  # doctest: +FLOAT_CMP
    1.3830742984029318
    >>> sep.arcminute  # doctest: +FLOAT_CMP
    1244.7668685626384
    >>> sep.arcsecond  # doctest: +FLOAT_CMP
    74686.0121137583

Also note that the two input coordinates were not in the same frame 
one is automatically converted to match the other, ensuring that even
though they are in different frames, the separation is determined
consistently.

In addition to the on-sky separation described above,
:meth:`astropy.coordinates.BaseCoordinateFrame.separation_3d` or
:meth:`astropy.coordinates.SkyCoord.separation_3d` methods will
determine the 3D distance between two coordinates that have ``distance``
defined::

    >>> c1 = SkyCoord('5h23m34.5s', '-69d45m22s', distance=70*u.kpc, frame='icrs')
    >>> c2 = SkyCoord('0h52m44.8s', '-72d49m43s', distance=80*u.kpc, frame='icrs')
    >>> sep = c1.separation_3d(c2)
    >>> sep  # doctest: +FLOAT_CMP
    <Distance 28.74398816 kpc>


Offsets
=======

Closely related to angular separations are offsets between coordinates. The key
distinction for offsets is generally the concept of a "from" and "to" coordinate
rather than the single scalar angular offset of a separation.
`~astropy.coordinates` contains conveniences to compute some of the common
offsets encountered in astronomy.

The first piece of such functionality is the
:meth:`~astropy.coordinates.SkyCoord.position_angle` method. This method
computes the position angle between one
|SkyCoord| instance and another (passed as the argument) following the
astronomy convention (positive angles East of North)::

    >>> c1 = SkyCoord(1*u.deg, 1*u.deg, frame='icrs')
    >>> c2 = SkyCoord(2*u.deg, 2*u.deg, frame='icrs')
    >>> c1.position_angle(c2).to(u.deg)  # doctest: +FLOAT_CMP
    <Angle 44.97818294 deg>

The combination of :meth:`~astropy.coordinates.SkyCoord.separation` and
:meth:`~astropy.coordinates.SkyCoord.position_angle` thus give a set of
directional offsets. To do the inverse operation  determining the new
"destination" coordinate given a separation and position angle  the
:meth:`~astropy.coordinates.SkyCoord.directional_offset_by` method is provided::

    >>> c1 = SkyCoord(1*u.deg, 1*u.deg, frame='icrs')
    >>> position_angle = 45 * u.deg
    >>> separation = 1.414 * u.deg
    >>> c1.directional_offset_by(position_angle, separation)  # doctest: +FLOAT_CMP
    <SkyCoord (ICRS): (ra, dec) in deg
        (2.0004075, 1.99964588)>

This technique is also useful for computing the midpoint (or indeed any point)
between two coordinates in a way that accounts for spherical geometry
(i.e., instead of averaging the RAs/Decs separately)::

    >>> coord1 = SkyCoord(0*u.deg, 0*u.deg, frame='icrs')
    >>> coord2 = SkyCoord(1*u.deg, 1*u.deg, frame='icrs')
    >>> pa = coord1.position_angle(coord2)
    >>> sep = coord1.separation(coord2)
    >>> coord1.directional_offset_by(pa, sep/2)  # doctest: +FLOAT_CMP
    <SkyCoord (ICRS): (ra, dec) in deg
        (0.49996192, 0.50001904)>

There is also a :meth:`~astropy.coordinates.SkyCoord.spherical_offsets_to`
method for computing angular offsets (e.g., small shifts like you might give a
telescope operator to move from a bright star to a fainter target)::

    >>> bright_star = SkyCoord('8h50m59.75s', '+11d39m22.15s', frame='icrs')
    >>> faint_galaxy = SkyCoord('8h50m47.92s', '+11d39m32.74s', frame='icrs')
    >>> dra, ddec = bright_star.spherical_offsets_to(faint_galaxy)
    >>> dra.to(u.arcsec)  # doctest: +FLOAT_CMP
    <Angle -173.78873354 arcsec>
    >>> ddec.to(u.arcsec)  # doctest: +FLOAT_CMP
    <Angle 10.60510342 arcsec>

The conceptual inverse of
:meth:`~astropy.coordinates.SkyCoord.spherical_offsets_to` is also available as
a method on any |SkyCoord| object:
:meth:`~astropy.coordinates.SkyCoord.spherical_offsets_by`, which accepts two
angular offsets (in longitude and latitude) and returns the coordinates at the
offset location::

    >>> target_star = SkyCoord(86.75309*u.deg, -31.5633*u.deg, frame='icrs')
    >>> target_star.spherical_offsets_by(1.3*u.arcmin, -0.7*u.arcmin)  # doctest: +FLOAT_CMP
    <SkyCoord (ICRS): (ra, dec) in deg
        (86.77852168, -31.57496415)>

.. _astropy-skyoffset-frames:

"Sky Offset" Frames
-------------------

To extend the concept of spherical offsets, `~astropy.coordinates` has
a frame class :class:`~astropy.coordinates.builtin_frames.skyoffset.SkyOffsetFrame`
which creates distinct frames that are centered on a specific point.
These are known as "sky offset frames," as they are a convenient way to create
a frame centered on an arbitrary position on the sky suitable for computing
positional offsets (e.g., for astrometry)::

    >>> from astropy.coordinates import SkyOffsetFrame, ICRS
    >>> center = ICRS(10*u.deg, 45*u.deg)
    >>> center.transform_to(SkyOffsetFrame(origin=center)) # doctest: +FLOAT_CMP
    <SkyOffsetICRS Coordinate (rotation=0.0 deg, origin=<ICRS Coordinate: (ra, dec) in deg
        (10., 45.)>): (lon, lat) in deg
        (0., 0.)>
    >>> target = ICRS(11*u.deg, 46*u.deg)
    >>> target.transform_to(SkyOffsetFrame(origin=center))  # doctest: +FLOAT_CMP
    <SkyOffsetICRS Coordinate (rotation=0.0 deg, origin=<ICRS Coordinate: (ra, dec) in deg
        (10., 45.)>): (lon, lat) in deg
        (0.69474685, 1.00428706)>


Alternatively, the convenience method
:meth:`~astropy.coordinates.SkyCoord.skyoffset_frame` lets you create a sky
offset frame from an existing |SkyCoord|::

    >>> center = SkyCoord(10*u.deg, 45*u.deg)
    >>> aframe = center.skyoffset_frame()
    >>> target.transform_to(aframe)  # doctest: +FLOAT_CMP
    <SkyOffsetICRS Coordinate (rotation=0.0 deg, origin=<ICRS Coordinate: (ra, dec) in deg
        (10., 45.)>): (lon, lat) in deg
        (0.69474685, 1.00428706)>
    >>> other = SkyCoord(9*u.deg, 44*u.deg, frame='fk5')
    >>> other.transform_to(aframe)  # doctest: +FLOAT_CMP
    <SkyCoord (SkyOffsetICRS: rotation=0.0 deg, origin=<ICRS Coordinate: (ra, dec) in deg
        (10., 45.)>): (lon, lat) in deg
        (-0.71943945, -0.99556216)>

.. note ::

    While sky offset frames *appear* to be all the same class, this not the
    case: the sky offset frame for each different type of frame for ``origin`` is
    actually a distinct class. E.g., ``SkyOffsetFrame(origin=ICRS(...))``
    yields an object of class ``SkyOffsetICRS``, *not* ``SkyOffsetFrame``.
    While this is not important for most uses of this class, it is important for
    things like type-checking, because something like
    ``SkyOffsetFrame(origin=ICRS(...)).__class__ is SkyOffsetFrame`` will
    *not* be ``True``, as it would be for most classes.

This same frame is also useful as a tool for defining frames that are relative
to a specific, known object useful for hierarchical physical systems like galaxy
groups. For example, objects around M31 are sometimes shown in a coordinate
frame aligned with standard ICRA RA/Dec, but on M31::

    >>> m31 = SkyCoord(10.6847083*u.deg, 41.26875*u.deg, frame='icrs')
    >>> ngc147 = SkyCoord(8.3005*u.deg, 48.5087389*u.deg, frame='icrs')
    >>> ngc147_inm31 = ngc147.transform_to(m31.skyoffset_frame())
    >>> xi, eta = ngc147_inm31.lon, ngc147_inm31.lat
    >>> xi  # doctest: +FLOAT_CMP
    <Longitude -1.59206948 deg>
    >>> eta  # doctest: +FLOAT_CMP
    <Latitude 7.26183757 deg>

.. note::

    Currently, distance information in the ``origin`` of a
    :class:`~astropy.coordinates.builtin_frames.skyoffset.SkyOffsetFrame` is not
    used to compute any part of the transform. The ``origin`` is only used for
    on-sky rotation. This may change in the future, however.


.. _astropy-coordinates-matching:

Matching Catalogs
=================

`~astropy.coordinates` leverages the coordinate framework to make it
possible to find the closest coordinates in a catalog to a desired set
of other coordinates. For example, assuming ``ra1``/``dec1`` and
``ra2``/``dec2`` are NumPy arrays loaded from some file:

.. testsetup::
    >>> ra1 = [5.3517]
    >>> dec1 = [-5.2328]
    >>> distance1 = 1344
    >>> ra2 = [6.459]
    >>> dec2 = [-16.4258]
    >>> distance2 = 8.611

.. doctest-requires:: scipy

    >>> c = SkyCoord(ra=ra1*u.degree, dec=dec1*u.degree)
    >>> catalog = SkyCoord(ra=ra2*u.degree, dec=dec2*u.degree)
    >>> idx, d2d, d3d = c.match_to_catalog_sky(catalog)

The distances returned ``d3d`` are 3-dimensional distances.
Unless both source (``c``) and catalog (``catalog``) coordinates have
associated distances, this quantity assumes that all sources are at a distance
of 1 (dimensionless).

You can also find the nearest 3D matches, different from the on-sky
separation shown above only when the coordinates were initialized with
a ``distance``:

.. doctest-requires:: scipy

    >>> c = SkyCoord(ra=ra1*u.degree, dec=dec1*u.degree, distance=distance1*u.kpc)
    >>> catalog = SkyCoord(ra=ra2*u.degree, dec=dec2*u.degree, distance=distance2*u.kpc)
    >>> idx, d2d, d3d = c.match_to_catalog_3d(catalog)

Now ``idx`` are indices into ``catalog`` that are the closest objects to each
of the coordinates in ``c``, ``d2d`` are the on-sky distances between them, and
``d3d`` are the 3-dimensional distances. Because coordinate objects support
indexing, ``idx`` enables easy access to the matched set of coordinates in
the catalog:

.. doctest-requires:: scipy

    >>> matches = catalog[idx]
    >>> (matches.separation_3d(c) == d3d).all()
    True
    >>> dra, ddec = c.spherical_offsets_to(matches)

This functionality can also be accessed from the
:func:`~astropy.coordinates.match_coordinates_sky` and
:func:`~astropy.coordinates.match_coordinates_3d` functions. These
will work on either |SkyCoord| objects *or* the lower-level frame classes:

.. doctest-requires:: scipy

    >>> from astropy.coordinates import match_coordinates_sky
    >>> idx, d2d, d3d = match_coordinates_sky(c, catalog)
    >>> idx, d2d, d3d = match_coordinates_sky(c.frame, catalog.frame)

It is possible to impose a separation constraint (e.g., the maximum separation to be
considered a match) by creating a boolean mask with ``d2d`` or ``d3d``. For example:

.. doctest-requires:: scipy

    >>> max_sep = 1.0 * u.arcsec
    >>> idx, d2d, d3d = c.match_to_catalog_3d(catalog)
    >>> sep_constraint = d2d < max_sep
    >>> c_matches = c[sep_constraint]
    >>> catalog_matches = catalog[idx[sep_constraint]]

Now, ``c_matches`` and ``catalog_matches`` are the matched sources in ``c``
and ``catalog``, respectively, which are separated by less than 1 arcsecond.

.. _astropy-searching-coordinates:

Searching around Coordinates
============================

Closely related functionality can be used to search for *all* coordinates within
a certain distance (either 3D distance or on-sky) of another set of coordinates.
The ``search_around_*`` methods (and functions) provide this functionality,
with an interface very similar to ``match_coordinates_*``:

..  doctest-requires:: scipy

    >>> import numpy as np
    >>> idxc, idxcatalog, d2d, d3d = catalog.search_around_sky(c, 1*u.deg)
    >>> np.all(d2d < 1*u.deg)
    True

.. doctest-requires:: scipy

    >>> idxc, idxcatalog, d2d, d3d = catalog.search_around_3d(c, 1*u.kpc)
    >>> np.all(d3d < 1*u.kpc)
    True

The key difference for these methods is that there can be multiple (or no)
matches in ``catalog`` around any locations in ``c``. Hence, indices into both
``c`` and ``catalog`` are returned instead of just indices into ``catalog``.
These can then be indexed back into the two |SkyCoord| objects, or, for that
matter, any array with the same order:

..  doctest-requires:: scipy

    >>> np.all(c[idxc].separation(catalog[idxcatalog]) == d2d)
    True
    >>> np.all(c[idxc].separation_3d(catalog[idxcatalog]) == d3d)
    True
    >>> print(catalog_objectnames[idxcatalog]) #doctest: +SKIP
    ['NGC 1234' 'NGC 4567' ...]

Note, though, that this dual-indexing means that ``search_around_*`` does not
work well if one of the coordinates is a scalar, because the returned index
would not make sense for a scalar::

    >>> scalarc = SkyCoord(ra=1*u.deg, dec=2*u.deg, distance=distance1*u.kpc)
    >>> idxscalarc, idxcatalog, d2d, d3d = catalog.search_around_sky(scalarc, 1*u.deg) # doctest: +SKIP
    ValueError: One of the inputs to search_around_sky is a scalar.

As a result (and because the ``search_around_*`` algorithm is inefficient in
the scalar case), the best approach for this scenario is to instead
use the ``separation*`` methods:

..  doctest-requires:: scipy

    >>> d2d = scalarc.separation(catalog)
    >>> catalogmsk = d2d < 1*u.deg
    >>> d3d = scalarc.separation_3d(catalog)
    >>> catalog3dmsk = d3d < 1*u.kpc

The resulting ``catalogmsk`` or ``catalog3dmsk`` variables are boolean arrays
rather than arrays of indices, but in practice they usually can be used in
the same way as ``idxcatalog`` from the above examples. If you definitely do
need indices instead of boolean masks, you can do:

..  doctest-requires:: scipy

    >>> idxcatalog = np.where(catalogmsk)[0]
    >>> idxcatalog3d = np.where(catalog3dmsk)[0]
.. _astropy-coordinates-transforming:

Transforming between Systems
****************************

`astropy.coordinates` supports a rich system for transforming
coordinates from one frame to another. While common astronomy frames
are built into Astropy, the transformation infrastructure is dynamic.
This means it allows users to define new coordinate frames and their
transformations. The topic of writing your own coordinate frame or
transforms is detailed in :ref:`astropy-coordinates-design`, and this
section is focused on how to *use* transformations.

The full list of built-in coordinate frames, the included transformations,
and the frame names are shown as a (clickable) graph in the
`~astropy.coordinates` API documentation.

Examples
--------

..
  EXAMPLE START
  Transforming Coordinates to Another Frame

The recommended method of transformation is shown below::

    >>> import astropy.units as u
    >>> from astropy.coordinates import SkyCoord
    >>> gc = SkyCoord(l=0*u.degree, b=45*u.degree, frame='galactic')
    >>> gc.fk5  # doctest: +FLOAT_CMP
    <SkyCoord (FK5: equinox=J2000.000): (ra, dec) in deg
        ( 229.27251463, -1.12844288)>

While this appears to be ordinary attribute-style access, it is actually
syntactic sugar for the more general
:meth:`~astropy.coordinates.SkyCoord.transform_to` method, which can
accept either a frame name, class, or instance::

    >>> from astropy.coordinates import FK5
    >>> gc.transform_to('fk5')  # doctest: +FLOAT_CMP
    <SkyCoord (FK5: equinox=J2000.000): (ra, dec) in deg
        ( 229.27251463, -1.12844288)>
    >>> gc.transform_to(FK5)  # doctest: +FLOAT_CMP
    <SkyCoord (FK5: equinox=J2000.000): (ra, dec) in deg
        ( 229.27251463, -1.12844288)>
    >>> gc.transform_to(FK5(equinox='J1980.0'))  # doctest: +FLOAT_CMP
    <SkyCoord (FK5: equinox=J1980.000): (ra, dec) in deg
        ( 229.0146935, -1.05560349)>

..
  EXAMPLE END

..
  EXAMPLE START
  Using SkyCoord Objects as the Frame in Transformations

As a convenience, it is also possible to use a |SkyCoord| object as the frame in
:meth:`~astropy.coordinates.SkyCoord.transform_to`. This allows for putting one
coordinate object into the frame of another::

    >>> sc = SkyCoord(ra=1.0, dec=2.0, unit='deg', frame=FK5, equinox='J1980.0')
    >>> gc.transform_to(sc)  # doctest: +FLOAT_CMP
    <SkyCoord (FK5: equinox=J1980.000): (ra, dec) in deg
        ( 229.0146935, -1.05560349)>

..
  EXAMPLE END

..
  EXAMPLE START
  Self Transformations of Coordinate Frames

Some coordinate frames (including `~astropy.coordinates.FK5`,
`~astropy.coordinates.FK4`, and `~astropy.coordinates.FK4NoETerms`) support
"self transformations," meaning the *type* of frame does not change, but the
frame attributes do. An example is precessing a coordinate from one equinox
to another in an equatorial frame. This is done by passing ``transform_to`` a
frame class with the relevant attributes, as shown below. Note that these
frames use a default equinox if you do not specify one::

    >>> fk5c = SkyCoord('02h31m49.09s', '+89d15m50.8s', frame=FK5)
    >>> fk5c.equinox
    <Time object: scale='tt' format='jyear_str' value=J2000.000>
    >>> fk5c  # doctest: +FLOAT_CMP
    <SkyCoord (FK5: equinox=J2000.000): (ra, dec) in deg
        ( 37.95454167,  89.26411111)>
    >>> fk5_2005 = FK5(equinox='J2005')  # String initializes an astropy.time.Time object
    >>> fk5c.transform_to(fk5_2005)  # doctest: +FLOAT_CMP
    <SkyCoord (FK5: equinox=J2005.000): (ra, dec) in deg
        ( 39.39317639,  89.28584422)>

You can also specify the equinox when you create a coordinate using a
`~astropy.time.Time` object::

    >>> from astropy.time import Time
    >>> fk5c = SkyCoord('02h31m49.09s', '+89d15m50.8s',
    ...                 frame=FK5(equinox=Time('J1970')))
    >>> fk5_2000 = FK5(equinox=Time(2000, format='jyear'))
    >>> fk5c.transform_to(fk5_2000)  # doctest: +FLOAT_CMP
    <SkyCoord (FK5: equinox=2000.0): (ra, dec) in deg
        ( 48.023171,  89.38672485)>

The same lower-level frame classes also have a
:meth:`~astropy.coordinates.BaseCoordinateFrame.transform_to` method
that works the same as above, but they do not support attribute-style
access. They are also subtly different in that they only use frame
attributes present in the initial or final frame, while |SkyCoord|
objects use any frame attributes they have for all transformation
steps. So |SkyCoord| can always transform from one frame to another and
back again without change, while low-level classes may lose information
and hence often do not round-trip.

..
  EXAMPLE END

.. _astropy-coordinates-transforming-ephemerides:

Transformations and Solar System Ephemerides
============================================

Some transformations (e.g., the transformation between
`~astropy.coordinates.ICRS` and `~astropy.coordinates.GCRS`) require the use of
a Solar System ephemeris to calculate the position and velocity of the Earth
and Sun. By default, transformations are calculated using built-in
`ERFA <https://github.com/liberfa/erfa>`_ routines, but they can also use more
precise ones using the JPL ephemerides (which are derived from dynamical
models).

Example
-------

..
  EXAMPLE START
  Calculating Transformations Using Solar System Ephemeris

To use the JPL ephemerides, use the
`~astropy.coordinates.solar_system_ephemeris` context manager, as shown below:

.. doctest-requires:: jplephem

    >>> from astropy.coordinates import solar_system_ephemeris
    >>> from astropy.coordinates import GCRS
    >>> with solar_system_ephemeris.set('jpl'): # doctest: +REMOTE_DATA +IGNORE_OUTPUT
    ...     fk5c.transform_to(GCRS(obstime=Time("J2000"))) # doctest: +REMOTE_DATA +IGNORE_OUTPUT

For locations at large distances from the Solar system, using the JPL
ephemerides will make a negligible difference on the order of micro-arcseconds.
For nearby objects, such as the Moon, the difference can be of the
order of milli-arcseconds. For more details about what ephemerides
are available, including the requirements for using JPL ephemerides, see
:ref:`astropy-coordinates-solarsystem`.

..
  EXAMPLE END
.. _astropy-coordinates-remote:

Usage Tips/Suggestions for Methods That Access Remote Resources
***************************************************************

There are currently two methods that rely on getting remote data to work.

The first is the :class:`~astropy.coordinates.SkyCoord` :meth:`~astropy.coordinates.SkyCoord.from_name` method, which uses
`Sesame <http://cds.u-strasbg.fr/cgi-bin/Sesame>`_ to retrieve coordinates
for a particular named object::

    >>> from astropy.coordinates import SkyCoord
    >>> SkyCoord.from_name("PSR J1012+5307")  # doctest: +REMOTE_DATA +FLOAT_CMP
    <SkyCoord (ICRS): (ra, dec) in deg
        ( 153.1393271,  53.117343)>

.. testsetup::

    >>> from astropy.coordinates import EarthLocation
    >>> apo = EarthLocation(-1463969.3018517173, -5166673.342234327, 3434985.7120456537, unit='m')

The second is the :class:`~astropy.coordinates.EarthLocation` :meth:`~astropy.coordinates.EarthLocation.of_site` method, which
provides a similar quick way to get an
:class:`~astropy.coordinates.EarthLocation` from an observatory name::

    >>> from astropy.coordinates import EarthLocation
    >>> apo = EarthLocation.of_site('Apache Point Observatory')  # doctest: +SKIP
    >>> apo  # doctest: +FLOAT_CMP
    <EarthLocation (-1463969.3018517173, -5166673.342234327, 3434985.7120456537) m>

The full list of available observatory names can be obtained with
 :meth:`astropy.coordinates.EarthLocation.get_site_names`.

.. testsetup::

    >>> loc = EarthLocation(-1994502.60430614, -5037538.54232911, 3358104.99690298, unit='m')

While these methods are convenient, there are several considerations to take
into account:

* Since these methods access online data, the data may evolve over time (for
  example, the accuracy of coordinates might improve, and new observatories
  may be added). Therefore, this means that a script using these and currently
  running may give a different answer in five years. Therefore, users concerned
  with reproducibility should not use these methods in their final scripts,
  but can instead use them to get the values required, and then hard-code them
  into the scripts. For example, we can check the coordinates of the Kitt
  Peak Observatories using::

    >>> loc = EarthLocation.of_site('Kitt Peak')  # doctest: +SKIP

  Note that this command requires an internet connection.

  We can then view the actual Cartesian coordinates for the observatory:

    >>> loc  # doctest: +FLOAT_CMP
    <EarthLocation (-1994502.6043061386, -5037538.54232911, 3358104.9969029757) m>

  This can then be converted into code::

    >>> loc = EarthLocation(-1994502.6043061386, -5037538.54232911, 3358104.9969029757, unit='m')

  This latter line can then be included in a script and will ensure that the
  results stay the same over time.

* The online data may not be accurate enough for your purposes. If maximum
  accuracy is paramount, we recommend that you determine the celestial or
  Earth coordinates yourself and hard-code these, rather than using the
  convenience methods.

* These methods will not function if an internet connection is not available.
  Therefore, if you need to work on a script while offline, follow the
  instructions in the first bullet point above to hard-code the coordinates
  before going offline.
.. _astropy-coordinates-fast-in-place:

Fast In-Place Modification of Coordinates
*****************************************

For some applications the recommended method of
:ref:`astropy-coordinates-modifying-in-place` may not be fast enough due to the
extensive validation performed in that process to ensure correctness.  Likewise,
you may find that creating another coordinate frame with different data using
`~astropy.coordinates.BaseCoordinateFrame.realize_frame` does not meet your
performance requirements.

For these high-performance situations, you can directly modify in-place the
representation data in the frame object as shown in this example::

    >>> import astropy.units as u
    >>> from astropy.coordinates import SkyCoord
    >>> sc = SkyCoord([1,2],[3,4], unit='deg')
    >>> sc.data.lon[()] = [10, 20] * u.deg
    >>> sc.data.lat[1] = 40 * u.deg

    >>> sc.cache.clear()  # IMPORTANT TO DO THIS!

    >>> sc  # doctest: +FLOAT_CMP
    <SkyCoord (ICRS): (ra, dec) in deg
        [(10., 3.), (20., 40.)]>

Notice that the ``.data`` representation object uses different names for the
components than in the coordinate object.  If you wish to inspect the
mapping between frame attributes (e.g., ``.ra``) and representation attributes
(e.g., ``.lon``) you can look at the following dictionary::

    >>> sc.representation_component_names
    {'ra': 'lon', 'dec': 'lat', 'distance': 'distance'}

.. warning::

   You *must* include the step to clear the cache as shown. Failing to do so
   will cause the object to be inconsistent and likely result in incorrect
   results. `~astropy.coordinates.SkyCoord`
   and `~astropy.coordinates.BaseCoordinateFrame` cache various kinds of
   information for performance reasons, so you need clear the cache so that
   the new representation values are used when required.

You should note that the only way to modify the data in a frame is by using
the ``.data`` attribute directly and not the aliases for components on the
frame.  For example the following will *appear* to give a correct
result but it does not actually modify the underlying representation data::

    >>> sc.ra[1] = 20 * u.deg  # THIS IS WRONG

This problem is related to the current implementation of performance-based
caching and cannot be easily resolved.
.. _astropy-coordinates-high-level:

Using the SkyCoord High-Level Class
***********************************

The |SkyCoord| class provides a simple and flexible user interface for
celestial coordinate representation, manipulation, and transformation between
coordinate frames. This is a high-level class that serves as a wrapper
around the low-level coordinate frame classes like `~astropy.coordinates.ICRS`
and `~astropy.coordinates.FK5` which do most of the heavy lifting.

The key distinctions between |SkyCoord| and the low-level classes
(:doc:`frames`) are as follows:

- The |SkyCoord| object can maintain the union of frame attributes for all
  built-in and user-defined coordinate frames in the
  ``astropy.coordinates.frame_transform_graph``. Individual frame classes hold
  only the required attributes (e.g., equinox, observation time, or observer
  location) for that frame. This means that a transformation from
  `~astropy.coordinates.FK4` (with equinox and observation time) to
  `~astropy.coordinates.ICRS` (with neither) and back to
  `~astropy.coordinates.FK4` via the low-level classes would not remember the
  original equinox and observation time. Since the |SkyCoord| object stores
  all attributes, such a round-trip transformation will return to the same
  coordinate object.

- The |SkyCoord| class is more flexible with inputs to accommodate a wide
  variety of user preferences and available data formats, whereas the frame
  classes expect to receive quantity-like objects with angular units.

- The |SkyCoord| class has a number of convenience methods that are useful
  in typical analysis.

- At present, |SkyCoord| objects can use only coordinate frames that have
  transformations defined in the ``astropy.coordinates.frame_transform_graph``
  transform graph object.

Creating SkyCoord Objects
=========================

The |SkyCoord| class accepts a wide variety of inputs for initialization.
At a minimum, these must provide one or more celestial coordinate values
with unambiguous units. Typically you must also specify the coordinate
frame, though this is not required.

Common patterns are shown below. In this description the values in upper
case like ``COORD`` or ``FRAME`` represent inputs which are described in detail
in the `Initialization Syntax`_ section. Elements in square brackets like
``[unit=UNIT]`` are optional.
::

  SkyCoord(COORD, [FRAME], keyword_args ...)
  SkyCoord(LON, LAT, [frame=FRAME], [unit=UNIT], keyword_args ...)
  SkyCoord([FRAME], <lon_attr>=LON, <lat_attr>=LAT, keyword_args ...)

The examples below illustrate common ways of initializing a |SkyCoord| object.
These all reflect initializing using spherical coordinates, which is the
default for all built-in frames. In order to understand working with coordinates
using a different representation, such as Cartesian or cylindrical, see the
section on `Representations`_. First, some imports::

  >>> from astropy.coordinates import SkyCoord  # High-level coordinates
  >>> from astropy.coordinates import ICRS, Galactic, FK4, FK5  # Low-level frames
  >>> from astropy.coordinates import Angle, Latitude, Longitude  # Angles
  >>> import astropy.units as u
  >>> import numpy as np

Examples
--------

..
  EXAMPLE START
  Initializing SkyCoord Objects Using Spherical Coordinates

The coordinate values and frame specification can be provided using
positional and keyword arguments. First we show positional arguments for
RA and Dec::

  >>> SkyCoord(10, 20, unit='deg')  # Defaults to ICRS  # doctest: +FLOAT_CMP
  <SkyCoord (ICRS): (ra, dec) in deg
      (10., 20.)>

  >>> SkyCoord([1, 2, 3], [-30, 45, 8], frame='icrs', unit='deg')  # doctest: +FLOAT_CMP
  <SkyCoord (ICRS): (ra, dec) in deg
      [(1., -30.), (2., 45.), (3.,   8.)]>

Notice that the first example above does not explicitly give a frame. In
this case, the default is taken to be the ICRS system (approximately
correct for "J2000" equatorial coordinates). It is always better to
explicitly specify the frame when it is known to be ICRS, however, as
anyone reading the code will be better able to understand the intent.

String inputs in common formats are acceptable, and the frame can be supplied
as either a class type like `~astropy.coordinates.FK4`, an instance of a
frame class, a `~astropy.coordinates.SkyCoord` instance (from which the frame
will be extracted), or the lowercase version of a frame name as a string, for
example, ``"fk4"``::

  >>> coords = ["1:12:43.2 +1:12:43", "1 12 43.2 +1 12 43"]
  >>> sc = SkyCoord(coords, frame=FK4, unit=(u.hourangle, u.deg), obstime="J1992.21")
  >>> sc = SkyCoord(coords, frame=FK4(obstime="J1992.21"), unit=(u.hourangle, u.deg))
  >>> sc = SkyCoord(coords, frame='fk4', unit='hourangle,deg', obstime="J1992.21")

  >>> sc = SkyCoord("1h12m43.2s", "+1d12m43s", frame=Galactic)  # Units from strings
  >>> sc = SkyCoord("1h12m43.2s +1d12m43s", frame=Galactic)  # Units from string
  >>> sc = SkyCoord(l="1h12m43.2s", b="+1d12m43s", frame='galactic')
  >>> sc = SkyCoord("1h12.72m +1d12.71m", frame='galactic')

Note that frame instances with data and `~astropy.coordinates.SkyCoord`
instances can only be passed as frames using the ``frame=`` keyword argument
and not as positional arguments.

For representations that have ``ra`` and ``dec`` attributes you can supply a
coordinate string in a number of other common formats. Examples include::

  >>> sc = SkyCoord("15h17+89d15")
  >>> sc = SkyCoord("275d11m15.6954s+17d59m59.876s")
  >>> sc = SkyCoord("8 00 -5 00.6", unit=(u.hour, u.deg))
  >>> sc = SkyCoord("J080000.00-050036.00", unit=(u.hour, u.deg))
  >>> sc = SkyCoord("J1874221.31+122328.03", unit=u.deg)

Astropy `~astropy.units.Quantity`-type objects are acceptable and encouraged
as a form of input::

  >>> ra = Longitude([1, 2, 3], unit=u.deg)  # Could also use Angle
  >>> dec = np.array([4.5, 5.2, 6.3]) * u.deg  # Astropy Quantity
  >>> sc = SkyCoord(ra, dec, frame='icrs')
  >>> sc = SkyCoord(ra=ra, dec=dec, frame=ICRS, obstime='2001-01-02T12:34:56')

Finally, it is possible to initialize from a low-level coordinate frame object.

  >>> c = FK4(1 * u.deg, 2 * u.deg)
  >>> sc = SkyCoord(c, obstime='J2010.11', equinox='B1965')  # Override defaults

A key subtlety highlighted here is that when low-level objects are created they
have certain default attribute values. For instance, the
`~astropy.coordinates.FK4` frame uses ``equinox='B1950.0`` and
``obstime=equinox`` as defaults. If this object is used to initialize a
|SkyCoord| it is possible to override the low-level object attributes that were
not explicitly set. If the coordinate above were created with
``c = FK4(1 * u.deg, 2 * u.deg, equinox='B1960')`` then creating a |SkyCoord|
with a different ``equinox`` would raise an exception.

..
  EXAMPLE END

Initialization Syntax
---------------------

For spherical representations, which are the most common and are the default
input format for all built-in frames, the syntax for |SkyCoord| is given
below::

  SkyCoord(COORD, [FRAME | frame=FRAME], [unit=UNIT], keyword_args ...)
  SkyCoord(LON, LAT, [DISTANCE], [FRAME | frame=FRAME], [unit=UNIT], keyword_args ...)
  SkyCoord([FRAME | frame=FRAME], <lon_name>=LON, <lat_name>=LAT, [unit=UNIT],
           keyword_args ...)

In the above description, elements in all capital letters (e.g., ``FRAME``)
describe a user input of that element type. Elements in square brackets are
optional. For nonspherical inputs, see the `Representations`_ section.


**LON**, **LAT**

Longitude and latitude value can be specified as separate positional arguments.
The following options are available for longitude and latitude:

- Single angle value:

  - |Quantity| object
  - Plain numeric value with ``unit`` keyword specifying the unit
  - Angle string which is formatted for :ref:`angle-creation` of
    |Longitude| or |Latitude| objects

- List or |Quantity| array, or NumPy array of angle values
- |Angle|, |Longitude|, or |Latitude| object, which can be scalar or
  array-valued

.. note::

    While |SkyCoord| is flexible with respect to specifying longitude and
    latitude component inputs, the frame classes expect to receive
    |Quantity|-like objects with angular units (i.e., |Angle| or |Quantity|).
    For example, when specifying components, the frame classes (e.g., ``ICRS``)
    must be created as

        >>> ICRS(0 * u.deg, 0 * u.deg) # doctest: +FLOAT_CMP
        <ICRS Coordinate: (ra, dec) in deg
            (0., 0.)>

    and other methods of flexible initialization (that work with |SkyCoord|)
    will not work

        >>> ICRS(0, 0, unit=u.deg) # doctest: +SKIP
        UnitTypeError: Longitude instances require units equivalent to 'rad', but no unit was given.

**DISTANCE**

The distance to the object from the frame center can be optionally specified:

- Single distance value:

  - |Quantity| or `~astropy.coordinates.Distance` object
  - Plain numeric value for a dimensionless distance
  - Plain numeric value with ``unit`` keyword specifying the unit

- List, or |Quantity|, or `~astropy.coordinates.Distance` array, or NumPy array
  of angle values

**COORD**

This input form uses a single object to supply coordinate data. For the case
of spherical coordinate frames, the coordinate can include one or more
longitude and latitude pairs in one of the following ways:

- Single coordinate string with a LON and LAT value separated by a space. The
  respective values can be any string which is formatted for
  :ref:`angle-creation` of |Longitude| or |Latitude| objects, respectively.
- List or NumPy array of such coordinate strings.
- List of (LON, LAT) tuples, where each LON and LAT are scalars (not arrays).
- ``N x 2`` NumPy or |Quantity| array of values where the first column is
  longitude and the second column is latitude, for example,
  ``[[270, -30], [355, +85]] * u.deg``.
- List of (LON, LAT, DISTANCE) tuples.
- ``N x 3`` NumPy or |Quantity| array of values where columns are
  longitude, latitude, and distance, respectively.

The input can also be more generalized objects that are not necessarily
represented in the standard spherical coordinates:

- Coordinate frame object (e.g., ``FK4(1*u.deg, 2*u.deg, obstime='J2012.2')``).
- |SkyCoord| object (which just makes a copy of the object).
- `~astropy.coordinates.BaseRepresentation` subclass object like
  `~astropy.coordinates.SphericalRepresentation`,
  `~astropy.coordinates.CylindricalRepresentation`, or
  `~astropy.coordinates.CartesianRepresentation`.

**FRAME**

This can be a `~astropy.coordinates.BaseCoordinateFrame` frame class, an
instance of such a class, or the corresponding string alias. The frame
classes that are built in to Astropy are `~astropy.coordinates.ICRS`,
`~astropy.coordinates.FK5`, `~astropy.coordinates.FK4`,
`~astropy.coordinates.FK4NoETerms`, `~astropy.coordinates.Galactic`, and
`~astropy.coordinates.AltAz`. The string aliases are lowercase versions of the
class name.

If the frame is not supplied then you will see a special ``ICRS``
identifier. This indicates that the frame is unspecified and operations
that require comparing coordinates (even within that object) are not allowed.

**unit=UNIT**

The unit specifier can be one of the following:

- `~astropy.units.Unit` object, which is an angular unit that is equivalent to
  ``Unit('radian')``.
- Single string with a valid angular unit name.
- 2-tuple of `~astropy.units.Unit` objects or string unit names specifying the
  LON and LAT unit, respectively (e.g., ``('hourangle', 'degree')``).
- Single string with two unit names separated by a comma (e.g.,
  ``'hourangle,degree'``).

If only a single unit is provided then it applies to both LON and LAT.

**Other keyword arguments**

In lieu of positional arguments to specify the longitude and latitude, the
frame-specific names can be used as keyword arguments:

*ra*, *dec*: **LON**, **LAT** values, optional
    RA and Dec for frames where these are representation, including [FIXME]
    `~astropy.coordinates.ICRS`, `~astropy.coordinates.FK5`,
    `~astropy.coordinates.FK4`, and `~astropy.coordinates.FK4NoETerms`.

*l*, *b*:  **LON**, **LAT** values, optional
    Galactic ``l`` and ``b`` for the `~astropy.coordinates.Galactic` frame.

The following keywords can be specified for any frame:

*distance*: distance quantity-like, optional
    Distance from reference from center to source

*obstime*: time-like, optional
    Time of observation

*equinox*: time-like, optional
    Coordinate frame equinox

If custom user-defined frames are included in the transform graph and they
have additional frame attributes, then those attributes can also be
set via corresponding keyword arguments in the |SkyCoord| initialization.

.. _astropy-coordinates-array-operations:

Array Operations
================

It is possible to store arrays of coordinates in a |SkyCoord| object, and
manipulations done in this way will be orders of magnitude faster than
looping over a list of individual |SkyCoord| objects.

Examples
--------

..
  EXAMPLE START
  Storing Arrays of Coordinates in a SkyCoord Object

To store arrays of coordinates in a |SkyCoord| object::

  >>> ra = np.linspace(0, 36000, 1001) * u.deg
  >>> dec = np.linspace(-90, 90, 1001) * u.deg

  >>> sc_list = [SkyCoord(r, d, frame='icrs') for r, d in zip(ra, dec)]  # doctest: +SKIP
  >>> timeit sc_gal_list = [c.galactic for c in sc_list]  # doctest: +SKIP
  1 loops, best of 3: 20.4 s per loop

  >>> sc = SkyCoord(ra, dec, frame='icrs')
  >>> timeit sc_gal = sc.galactic  # doctest: +SKIP
  100 loops, best of 3: 21.8 ms per loop

..
  EXAMPLE END

..
  EXAMPLE START
  Array Operations Using SkyCoord

In addition to vectorized transformations, you can do the usual array slicing,
dicing, and selection using the same methods and attributes that you use for
`~numpy.ndarray` instances. Corresponding functions, as well as others that
affect the shape, such as `~numpy.atleast_1d` and `~numpy.rollaxis`, work as
expected. (The relevant functions have to be explicitly enabled in ``astropy``
source code; let us know if a ``numpy`` function is not supported that you
think should work.)::

  >>> north_mask = sc.dec > 0
  >>> sc_north = sc[north_mask]
  >>> len(sc_north)
  500
  >>> sc[2:4]  # doctest: +FLOAT_CMP
  <SkyCoord (ICRS): (ra, dec) in deg
      [( 72., -89.64), (108., -89.46)]>
  >>> sc[500]
  <SkyCoord (ICRS): (ra, dec) in deg
      (0., 0.)>
  >>> sc[0:-1:100].reshape(2, 5)
  <SkyCoord (ICRS): (ra, dec) in deg
      [[(0., -90.), (0., -72.), (0., -54.), (0., -36.), (0., -18.)],
       [(0.,   0.), (0.,  18.), (0.,  36.), (0.,  54.), (0.,  72.)]]>
  >>> np.roll(sc[::100], 1)
  <SkyCoord (ICRS): (ra, dec) in deg
      [(0.,  90.), (0., -90.), (0., -72.), (0., -54.), (0., -36.),
       (0., -18.), (0.,   0.), (0.,  18.), (0.,  36.), (0.,  54.),
       (0.,  72.)]>

Note that similarly to the `~numpy.ndarray` methods, all but ``flatten`` try to
use new views of the data, with the data copied only if that is impossible
(as discussed, for example, in the documentation for NumPy
:func:`~numpy.reshape`).

..
  EXAMPLE END

.. _astropy-coordinates-modifying-in-place:

Modifying Coordinate Objects In-place
-------------------------------------

Coordinate values in a array-valued |SkyCoord| object can be modified in-place
(added in astropy 4.1). This requires that the new values be set from an
another |SkyCoord| object that is equivalent in all ways except for the actual
coordinate data values. In this way, no frame transformations are required and
the item setting operation is extremely robust.

Specifically, the right hand ``value`` must be strictly consistent with the
object being modified:

- Identical class
- Equivalent frames (`~astropy.coordinates.BaseCoordinateFrame.is_equivalent_frame`)
- Identical representation_types
- Identical representation differentials keys
- Identical frame attributes
- Identical "extra" frame attributes (e.g., ``obstime`` for an ICRS coord)

..
  EXAMPLE START
  Modifying an Array of Coordinates in a SkyCoord Object

To modify an array of coordinates in a |SkyCoord| object use the same
syntax for a numpy array::

  >>> sc1 = SkyCoord([1, 2] * u.deg, [3, 4] * u.deg)
  >>> sc2 = SkyCoord(10 * u.deg, 20 * u.deg)
  >>> sc1[0] = sc2
  >>> sc1
  <SkyCoord (ICRS): (ra, dec) in deg
      [(10., 20.), ( 2.,  4.)]>

..
  EXAMPLE END

..
  EXAMPLE START
  Inserting Coordinates into a SkyCoord Object

You can insert a scalar or array-valued |SkyCoord| object into another
compatible |SkyCoord| object::

  >>> sc1 = SkyCoord([1, 2] * u.deg, [3, 4] * u.deg)
  >>> sc2 = SkyCoord(10 * u.deg, 20 * u.deg)
  >>> sc1.insert(1, sc2)
  <SkyCoord (ICRS): (ra, dec) in deg
      [( 1.,  3.), (10., 20.), ( 2.,  4.)]>

..
  EXAMPLE END

With the ability to modify a |SkyCoord| object in-place, all of the
:ref:`table_operations` such as joining, stacking, and inserting are
functional with |SkyCoord| mixin columns (so long as no masking is required).

These methods are relatively slow because they require setting from an
existing |SkyCoord| object and they perform extensive validation to ensure
that the operation is valid. For some applications it may be necessary to
take a different lower-level approach which is described in the section
:ref:`astropy-coordinates-fast-in-place`.

.. warning::

  You may be tempted to try an apparently obvious way of modifying a coordinate
  object in place by updating the component attributes directly, for example
  ``sc1.ra[1] = 40 * u.deg``. However, while this will *appear* to give a correct
  result it does not actually modify the underlying representation data. This
  is related to the current implementation of performance-based caching.
  The current cache implementation is similarly unable to handle in-place changes
  to the representation (``.data``) or frame attributes such as ``.obstime``.


Attributes
==========

The |SkyCoord| object has a number of useful attributes which come in handy.
By digging through these we will learn a little bit about |SkyCoord| and how it
works.

To begin, one of the most important tools for
learning about attributes and methods of objects is "TAB-discovery." From
within IPython you can type an object name, the period, and then the <TAB> key
to see what is available. This can often be faster than reading the
documentation::

  >>> sc = SkyCoord(1, 2, frame='icrs', unit='deg', obstime='2013-01-02 14:25:36')
  >>> sc.<TAB>  # doctest: +SKIP
  sc.T                                   sc.match_to_catalog_3d
  sc.altaz                               sc.match_to_catalog_sky
  sc.barycentrictrueecliptic             sc.name
  sc.cartesian                           sc.ndim
  sc.cirs                                sc.obsgeoloc
  sc.copy                                sc.obsgeovel
  sc.data                                sc.obstime
  sc.dec                                 sc.obswl
  sc.default_representation              sc.position_angle
  sc.diagonal                            sc.precessedgeocentric
  sc.distance                            sc.pressure
  sc.equinox                             sc.ra
  sc.fk4                                 sc.ravel
  sc.fk4noeterms                         sc.realize_frame
  sc.fk5                                 sc.relative_humidity
  sc.flatten                             sc.represent_as
  sc.frame                               sc.representation_component_names
  sc.frame_attributes                    sc.representation_component_units
  sc.frame_specific_representation_info  sc.representation_info
  sc.from_name                           sc.reshape
  sc.from_pixel                          sc.roll
  sc.galactic                            sc.search_around_3d
  sc.galactocentric                      sc.search_around_sky
  sc.galcen_distance                     sc.separation
  sc.gcrs                                sc.separation_3d
  sc.geocentrictrueecliptic              sc.shape
  sc.get_constellation                   sc.size
  sc.get_frame_attr_names                sc.skyoffset_frame
  sc.guess_from_table                    sc.spherical
  sc.has_data                            sc.spherical_offsets_to
  sc.hcrs                                sc.squeeze
  sc.heliocentrictrueecliptic            sc.supergalactic
  sc.icrs                                sc.swapaxes
  sc.info                                sc.take
  sc.is_equivalent_frame                 sc.temperature
  sc.is_frame_attr_default               sc.to_pixel
  sc.is_transformable_to                 sc.to_string
  sc.isscalar                            sc.transform_to
  sc.itrs                                sc.transpose
  sc.location                            sc.z_sun

Here we see many attributes and methods. The most recognizable may be the
longitude and latitude attributes which are named ``ra`` and ``dec`` for the
``ICRS`` frame::

  >>> sc.ra  # doctest: +FLOAT_CMP
  <Longitude 1. deg>
  >>> sc.dec  # doctest: +FLOAT_CMP
  <Latitude 2. deg>

Next, notice that all of the built-in frame names ``icrs``, ``galactic``,
``fk5``, ``fk4``, and ``fk4noeterms`` are there. Through the magic of Python
properties, accessing these attributes calls the object
`~astropy.coordinates.SkyCoord.transform_to` method appropriately and returns a
new |SkyCoord| object in the requested frame::

  >>> sc_gal = sc.galactic
  >>> sc_gal  # doctest: +FLOAT_CMP
  <SkyCoord (Galactic): (l, b) in deg
      (99.63785528, -58.70969293)>

Other attributes you may recognize are ``distance``, ``equinox``,
``obstime``, and ``shape``.

Digging Deeper
--------------
*[Casual users can skip this section]*

After transforming to Galactic, the longitude and latitude values are now
labeled ``l`` and ``b``, following the normal convention for Galactic
coordinates. How does the object know what to call its values? The answer
lies in some less obvious attributes::

  >>> sc_gal.representation_component_names
  {'l': 'lon', 'b': 'lat', 'distance': 'distance'}

  >>> sc_gal.representation_component_units
  {'l': Unit("deg"), 'b': Unit("deg")}

  >>> sc_gal.representation_type
  <class 'astropy.coordinates.representation.SphericalRepresentation'>

Together these tell the object that ``l`` and ``b`` are the longitude and
latitude, and that they should both be displayed in units of degrees as
a spherical-type coordinate (and not, for example, a Cartesian coordinate).
Furthermore, the frame's ``representation_component_names`` attribute defines
the coordinate keyword arguments that |SkyCoord| will accept.

Another important attribute is ``frame_attr_names``, which defines the
additional attributes that are required to fully define the frame::

  >>> sc_fk4 = SkyCoord(1, 2, frame='fk4', unit='deg')
  >>> sc_fk4.get_frame_attr_names()
  {'equinox': <Time object: scale='tt' format='byear_str' value=B1950.000>, 'obstime': None}

The key values correspond to the defaults if no explicit value is provided by
the user. This example shows that the `~astropy.coordinates.FK4` frame has two
attributes, ``equinox`` and ``obstime``, that are required to fully define the
frame.

Some trickery is happening here because many of these attributes are
actually owned by the underlying coordinate ``frame`` object which does much of
the real work. This is the middle layer in the three-tiered system of objects:
representation (spherical, Cartesian, etc.), frame (a.k.a. low-level frame
class), and |SkyCoord| (a.k.a. high-level class; see
:ref:`astropy-coordinates-overview` and
:ref:`astropy-coordinates-definitions`)::

  >>> sc.frame  # doctest: +FLOAT_CMP
  <ICRS Coordinate: (ra, dec) in deg
      (1., 2.)>

  >>> sc.has_data is sc.frame.has_data
  True

  >>> sc.frame.<TAB>  # doctest: +SKIP
  sc.frame.T                                   sc.frame.ra
  sc.frame.cartesian                           sc.frame.ravel
  sc.frame.copy                                sc.frame.realize_frame
  sc.frame.data                                sc.frame.represent_as
  sc.frame.dec                                 sc.frame.representation
  sc.frame.default_representation              sc.frame.representation_component_names
  sc.frame.diagonal                            sc.frame.representation_component_units
  sc.frame.distance                            sc.frame.representation_info
  sc.frame.flatten                             sc.frame.reshape
  sc.frame.frame_attributes                    sc.frame.separation
  sc.frame.frame_specific_representation_info  sc.frame.separation_3d
  sc.frame.get_frame_attr_names                sc.frame.shape
  sc.frame.has_data                            sc.frame.size
  sc.frame.is_equivalent_frame                 sc.frame.spherical
  sc.frame.is_frame_attr_default               sc.frame.squeeze
  sc.frame.is_transformable_to                 sc.frame.swapaxes
  sc.frame.isscalar                            sc.frame.take
  sc.frame.name                                sc.frame.transform_to
  sc.frame.ndim                                sc.frame.transpose

  >>> sc.frame.name
  'icrs'

The |SkyCoord| object exposes the ``frame`` object attributes as its own. Though
it might seem a tad confusing at first, this is a good thing because it makes
|SkyCoord| objects and `~astropy.coordinates.BaseCoordinateFrame` objects
behave very similarly and most routines can accept either one as input without
much bother (duck typing!).

The lowest layer in the stack is the abstract
`~astropy.coordinates.UnitSphericalRepresentation` object:

  >>> sc_gal.frame.data  # doctest: +FLOAT_CMP
  <UnitSphericalRepresentation (lon, lat) in rad
      (1.73900863, -1.02467744)>

Transformations
===============

The topic of transformations is covered in detail in the section on
:ref:`astropy-coordinates-transforming`.

For completeness, here we will give some examples. Once you have defined
your coordinates and the reference frame, you can transform from that frame to
another frame. You can do this in a few different ways: if you only want the
default version of that frame, you can use attribute-style access (as mentioned
previously). For more control, you can use the
`~astropy.coordinates.SkyCoord.transform_to` method, which accepts a frame
name, frame class, frame instance, or |SkyCoord|.

Examples
--------

..
  EXAMPLE START
  Transforming Between Frames

To transform from one frame to another::

  >>> from astropy.coordinates import FK5
  >>> sc = SkyCoord(1, 2, frame='icrs', unit='deg')
  >>> sc.galactic  # doctest: +FLOAT_CMP
  <SkyCoord (Galactic): (l, b) in deg
      (99.63785528, -58.70969293)>

  >>> sc.transform_to('fk5')  # Same as sc.fk5 and sc.transform_to(FK5)  # doctest: +FLOAT_CMP
  <SkyCoord (FK5: equinox=J2000.000): (ra, dec) in deg
          (1.00000656, 2.00000243)>

  >>> sc.transform_to(FK5(equinox='J1975'))  # Transform to FK5 with a different equinox  # doctest: +FLOAT_CMP
  <SkyCoord (FK5: equinox=J1975.000): (ra, dec) in deg
          (0.67967282, 1.86083014)>

Transforming to a |SkyCoord| instance is a convenient way of ensuring that two
coordinates are in the exact same reference frame::

  >>> sc2 = SkyCoord(3, 4, frame='fk4', unit='deg', obstime='J1978.123', equinox='B1960.0')
  >>> sc.transform_to(sc2)  # doctest: +FLOAT_CMP
  <SkyCoord (FK4: equinox=B1960.000, obstime=J1978.123): (ra, dec) in deg
      (0.48726331, 1.77731617)>

..
  EXAMPLE END

.. _astropy-skycoord-representations:

Representations
===============

So far we have been using a spherical coordinate representation in all of the
examples, and this is the default for the built-in frames. Frequently it is
convenient to initialize or work with a coordinate using a different
representation such as Cartesian or cylindrical. In this section, we discuss
how to initialize an object using a different representation and how to
change the representation of an object. For more information about
representation objects themselves, see :ref:`astropy-coordinates-representations`.

Initialization
--------------

Most of what you need to know can be inferred from the examples below and
by extrapolating the previous documentation for spherical representations.
Initialization requires setting the ``representation_type`` keyword and
supplying the corresponding components for that representation.

Examples
^^^^^^^^

..
  EXAMPLE START
  Initialization of a SkyCoord Object Using Different Representations

To initialize an object using a representation type other than spherical::

    >>> c = SkyCoord(x=1, y=2, z=3, unit='kpc', representation_type='cartesian')
    >>> c  # doctest: +FLOAT_CMP
    <SkyCoord (ICRS): (x, y, z) in kpc
        (1., 2., 3.)>
    >>> c.x, c.y, c.z  # doctest: +FLOAT_CMP
    (<Quantity 1. kpc>, <Quantity 2. kpc>, <Quantity 3. kpc>)

Other variations include::

    >>> SkyCoord(1, 2*u.deg, 3, representation_type='cylindrical')  # doctest: +FLOAT_CMP
    <SkyCoord (ICRS): (rho, phi, z) in (, deg, )
        (1., 2., 3.)>

    >>> SkyCoord(rho=1*u.km, phi=2*u.deg, z=3*u.m, representation_type='cylindrical')  # doctest: +FLOAT_CMP
    <SkyCoord (ICRS): (rho, phi, z) in (km, deg, m)
        (1., 2., 3.)>

    >>> SkyCoord(rho=1, phi=2, z=3, unit=(u.km, u.deg, u.m), representation_type='cylindrical')  # doctest: +FLOAT_CMP
    <SkyCoord (ICRS): (rho, phi, z) in (km, deg, m)
        (1., 2., 3.)>

    >>> SkyCoord(1, 2, 3, unit=(None, u.deg, None), representation_type='cylindrical')  # doctest: +FLOAT_CMP
    <SkyCoord (ICRS): (rho, phi, z) in (, deg, )
        (1., 2., 3.)>

In general terms, the allowed syntax is as follows::

  SkyCoord(COORD, [FRAME | frame=FRAME], [unit=UNIT], [representation_type=REPRESENTATION],
           keyword_args ...)
  SkyCoord(COMP1, COMP2, [COMP3], [FRAME | frame=FRAME], [unit=UNIT],
           [representation_type=REPRESENTATION], keyword_args ...)
  SkyCoord([FRAME | frame=FRAME], <comp1_name>=COMP1, <comp2_name>=COMP2,
           <comp3_name>=COMP3, [representation_type=REPRESENTATION], [unit=UNIT],
           keyword_args ...)

In this case, the ``keyword_args`` now includes the element
``representation_type=REPRESENTATION``. In the above description, elements in
all capital letters (e.g., ``FRAME``) describe a user input of that element
type. Elements in square brackets are optional.

..
  EXAMPLE END

**COMP1**, **COMP2**, **COMP3**

Component values can be specified as separate positional arguments or as
keyword arguments. In this formalism the exact type of allowed input depends
on the details of the representation. In general, the following input forms
are supported:

- Single value:

  - Component class object
  - Plain numeric value with ``unit`` keyword specifying the unit

- List or component class array, or NumPy array of values

Each representation component has a specified class (the "component class")
which is used to convert generic input data into a predefined object
class with a certain unit. These component classes are expected to be
subclasses of the `~astropy.units.Quantity` class.

**COORD**

This input form uses a single object to supply coordinate data. The coordinate
can specify one or more coordinate positions as follows:

- List of ``(COMP1, .., COMP<M>)`` tuples, where each component is a scalar (not
  array) and there are ``M`` components in the representation. Typically
  there are three components, but some
  (e.g., `~astropy.coordinates.UnitSphericalRepresentation`)
  can have fewer.
- ``N x M`` NumPy or |Quantity| array of values, where ``N`` is the number
  of coordinates and ``M`` is the number of components.

**REPRESENTATION**

The representation can be supplied either as a
`~astropy.coordinates.representation.BaseRepresentation` class (e.g.,
`~astropy.coordinates.CartesianRepresentation`) or as a string name
that is simply the class name in lowercase without the
``'representation'`` suffix (e.g., ``'cartesian'``).

The rest of the inputs for creating a |SkyCoord| object in the general case are
the same as for spherical.

Details
-------

The available set of representations is dynamic and may change depending on what
representation classes have been defined. The built-in representations are:

=====================  =======================================================
  Name                   Class
=====================  =======================================================
``spherical``          `~astropy.coordinates.SphericalRepresentation`
``unitspherical``      `~astropy.coordinates.UnitSphericalRepresentation`
``physicsspherical``   `~astropy.coordinates.PhysicsSphericalRepresentation`
``cartesian``          `~astropy.coordinates.CartesianRepresentation`
``cylindrical``        `~astropy.coordinates.CylindricalRepresentation`
=====================  =======================================================

Each frame knows about all of the available representations, but different
frames may use different names for the same components. A common example
is that the `~astropy.coordinates.Galactic` frame uses ``l`` and ``b``
instead of ``ra`` and ``dec`` for the ``lon`` and ``lat`` components of
the `~astropy.coordinates.SphericalRepresentation`.

For a particular frame, in order to see the full list of representations
and how it names all of the components, first make an instance of that frame
without any data, and then print the ``representation_info`` property::

    >>> ICRS().representation_info  # doctest: +SKIP
    {astropy.coordinates.representation.CartesianRepresentation:
      {'names': ('x', 'y', 'z'),
       'units': (None, None, None)},
     astropy.coordinates.representation.SphericalRepresentation:
      {'names': ('ra', 'dec', 'distance'),
       'units': (Unit("deg"), Unit("deg"), None)},
     astropy.coordinates.representation.UnitSphericalRepresentation:
      {'names': ('ra', 'dec'),
       'units': (Unit("deg"), Unit("deg"))},
     astropy.coordinates.representation.PhysicsSphericalRepresentation:
      {'names': ('phi', 'theta', 'r'),
       'units': (Unit("deg"), Unit("deg"), None)},
     astropy.coordinates.representation.CylindricalRepresentation:
      {'names': ('rho', 'phi', 'z'),
       'units': (None, Unit("deg"), None)}
    }

This is a bit messy but it shows that for each representation there is a
``dict`` with two keys:

- ``names``: defines how each component is named in that frame.
- ``units``: defines the units of each component when output, where ``None``
  means to not force a particular unit.

For a particular coordinate instance you can use the ``representation_type``
attribute in conjunction with the ``representation_component_names`` attribute
to figure out what keywords are accepted by a particular class object. The
former will be the representation class the system is expressed in (e.g.,
spherical for equatorial frames), and the latter will be a dictionary mapping
names for that frame to the component name on the representation class::

    >>> import astropy.units as u
    >>> icrs = ICRS(1*u.deg, 2*u.deg)
    >>> icrs.representation_type
    <class 'astropy.coordinates.representation.SphericalRepresentation'>
    >>> icrs.representation_component_names
    {'ra': 'lon', 'dec': 'lat', 'distance': 'distance'}

Changing Representation
-----------------------

The representation of the coordinate object can be changed, as shown
below. This actually does *nothing* to the object internal data which
stores the coordinate values, but it changes the external view of that
data in two ways:

- The object prints itself in accord with the new representation.
- The available attributes change to match those of the new representation
  (e.g., from ``ra, dec, distance`` to ``x, y, z``).

Setting the ``representation_type`` thus changes a *property* of the
object (how it appears) without changing the intrinsic object itself
which represents a point in 3D space.

Examples
^^^^^^^^

..
  EXAMPLE START
  Changing the Representation of a Coordinate Object

To change the representation of a coordinate object by setting the
``representation_type`` ::

    >>> c = SkyCoord(x=1, y=2, z=3, unit='kpc', representation_type='cartesian')
    >>> c  # doctest: +FLOAT_CMP
    <SkyCoord (ICRS): (x, y, z) in kpc
        (1., 2., 3.)>

    >>> c.representation_type = 'cylindrical'
    >>> c  # doctest: +FLOAT_CMP
    <SkyCoord (ICRS): (rho, phi, z) in (kpc, deg, kpc)
        (2.23606798, 63.43494882, 3.)>
    >>> c.phi.to(u.deg)  # doctest: +FLOAT_CMP
    <Angle 63.43494882 deg>
    >>> c.x
    Traceback (most recent call last):
    ...
    AttributeError: 'SkyCoord' object has no attribute 'x'

    >>> c.representation_type = 'spherical'
    >>> c  # doctest: +FLOAT_CMP
    <SkyCoord (ICRS): (ra, dec, distance) in (deg, deg, kpc)
        (63.43494882, 53.3007748, 3.74165739)>

    >>> c.representation_type = 'unitspherical'
    >>> c  # doctest: +FLOAT_CMP
    <SkyCoord (ICRS): (ra, dec) in deg
        (63.43494882, 53.3007748)>

You can also use any representation class to set the representation::

    >>> from astropy.coordinates import CartesianRepresentation
    >>> c.representation_type = CartesianRepresentation

Note that if all you want is a particular representation without changing the
state of the |SkyCoord| object, you should instead use the
``astropy.coordinates.SkyCoord.represent_as()`` method::

    >>> c.representation_type = 'spherical'
    >>> cart = c.represent_as(CartesianRepresentation)
    >>> cart  # doctest: +FLOAT_CMP
    <CartesianRepresentation (x, y, z) in kpc
        (1., 2., 3.)>
    >>> c.representation_type
    <class 'astropy.coordinates.representation.SphericalRepresentation'>

..
  EXAMPLE END

Example 1: Plotting random data in Aitoff projection
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

..
  EXAMPLE START
  Plotting Random Data in Aitoff Projection

This is an example of how to make a plot in the Aitoff projection using data
in a |SkyCoord| object. Here, a randomly generated data set will be used.

First we need to import the required packages. We use
`matplotlib <https://matplotlib.org/>`_ here for
plotting and `numpy <https://numpy.org/>`_  to get the value of pi and to
generate our random data.

    >>> from astropy import units as u
    >>> from astropy.coordinates import SkyCoord
    >>> import numpy as np

We now generate random data for visualization. For RA this is done in the range
of 0 and 360 degrees (``ra_random``), for DEC between -90 and +90 degrees
(``dec_random``). Finally, we multiply these values by degrees to get a
`~astropy.units.Quantity` with units of degrees.

    >>> ra_random = np.random.rand(100)*360.0 * u.degree
    >>> dec_random = (np.random.rand(100)*180.0-90.0) * u.degree

As the next step, those coordinates are transformed into an
`astropy.coordinates` |SkyCoord| object.

    >>> c = SkyCoord(ra=ra_random, dec=dec_random, frame='icrs')

Because matplotlib needs the coordinates in radians and between :math:`-\pi`
and :math:`\pi`, not 0 and :math:`2\pi`, we have to convert them.
For this purpose the `astropy.coordinates.Angle` object provides a special
method, which we use here to wrap at 180:

    >>> ra_rad = c.ra.wrap_at(180 * u.deg).radian
    >>> dec_rad = c.dec.radian

As a last step, we set up the plotting environment with matplotlib using the
Aitoff projection with a specific title, a grid, filled circles as markers with
a marker size of 2, and an alpha value of 0.3. We use a figure with an x-y ratio
that is well suited for such a projection and we move the title upwards from
its usual position to avoid overlap with the axis labels.

.. doctest-skip::

    >>> import matplotlib.pyplot as plt
    >>> plt.figure(figsize=(8,4.2))
    >>> plt.subplot(111, projection="aitoff")
    >>> plt.title("Aitoff projection of our random data")
    >>> plt.grid(True)
    >>> plt.plot(ra_rad, dec_rad, 'o', markersize=2, alpha=0.3)
    >>> plt.subplots_adjust(top=0.95,bottom=0.0)
    >>> plt.show()


.. plot::

    # This is an example how to make a plot in the Aitoff projection using data
    # in a SkyCoord object. Here a randomly generated data set will be used. The
    # final script can be found below.

    # First we need to import the required packages. We use
    # `matplotlib <https://matplotlib.org/>`_ here for
    # plotting and `numpy <https://numpy.org/>`_  to get the value of pi and to
    # generate our random data.
    from astropy import units as u
    from astropy.coordinates import SkyCoord
    import matplotlib.pyplot as plt
    import numpy as np

    # We now generate random data for visualization. For RA this is done in the range
    # of 0 and 360 degrees (``ra_random``), for DEC between -90 and +90 degrees
    # (``dec_random``). Finally, we multiply these values by degrees to get a
    # `~astropy.units.Quantity` with units of degrees.
    ra_random = np.random.rand(100)*360.0 * u.degree
    dec_random = (np.random.rand(100)*180.0-90.0) * u.degree

    # As the next step, those coordinates are transformed into an astropy.coordinates
    # astropy.coordinates.SkyCoord object.
    c = SkyCoord(ra=ra_random, dec=dec_random, frame='icrs')

    # Because matplotlib needs the coordinates in radians and between :math:`-\pi`
    # and :math:`\pi`, not 0 and :math:`2\pi`, we have to convert them.
    # For this purpose the `astropy.coordinates.Angle` object provides a special method,
    # which we use here to wrap at 180:
    ra_rad = c.ra.wrap_at(180 * u.deg).radian
    dec_rad = c.dec.radian

    # As a last step we set up the plotting environment with matplotlib using the
    # Aitoff projection with a specific title, a grid, filled circles as markers with
    # a marker size of 2, and an alpha value of 0.3.
    plt.figure(figsize=(8,4.2))
    plt.subplot(111, projection="aitoff")
    plt.title("Aitoff projection of our random data", y=1.08)
    plt.grid(True)
    plt.plot(ra_rad, dec_rad, 'o', markersize=2, alpha=0.3)
    plt.subplots_adjust(top=0.95, bottom=0.0)
    plt.show()

..
  EXAMPLE END

Example 2: Plotting star positions in bulge and disk
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

..
  EXAMPLE START
  Plotting Star Positions in Bulge and Disk

This is a more realistic example of how to make a plot in the Aitoff projection
using data in a |SkyCoord| object. Here, a randomly generated data set
(multivariate normal distribution) for both stars in the bulge and in the disk
of a galaxy will be used. Both types will be plotted with different number
counts.

As in the last example, we first import the required packages.

    >>> from astropy import units as u
    >>> from astropy.coordinates import SkyCoord
    >>> import numpy as np

We now generate random data for visualization using
``numpy.random.Generator.multivariate_normal``.

    >>> disk = np.random.multivariate_normal(mean=[0,0,0], cov=np.diag([1,1,0.5]), size=5000)
    >>> bulge = np.random.multivariate_normal(mean=[0,0,0], cov=np.diag([1,1,1]), size=500)
    >>> galaxy = np.concatenate([disk, bulge])

As the next step, those coordinates are transformed into an
`astropy.coordinates` |SkyCoord| object.

    >>> c_gal = SkyCoord(galaxy, representation_type='cartesian', frame='galactic')
    >>> c_gal_icrs = c_gal.icrs

Again, as in the last example, we need to convert the coordinates in radians
and make sure they are between :math:`-\pi` and :math:`\pi`:

    >>> ra_rad = c_gal_icrs.ra.wrap_at(180 * u.deg).radian
    >>> dec_rad = c_gal_icrs.dec.radian

We use the same plotting setup as in the last example:

.. doctest-skip::

    >>> import matplotlib.pyplot as plt
    >>> plt.figure(figsize=(8,4.2))
    >>> plt.subplot(111, projection="aitoff")
    >>> plt.title("Aitoff projection of our random data")
    >>> plt.grid(True)
    >>> plt.plot(ra_rad, dec_rad, 'o', markersize=2, alpha=0.3)
    >>> plt.subplots_adjust(top=0.95,bottom=0.0)
    >>> plt.show()


.. plot::

    # This is more realistic example how to make a plot in the Aitoff projection
    # using data in a SkyCoord object.
    # Here a randomly generated data set (multivariate normal distribution)
    # for both stars in the bulge and in the disk of a galaxy
    # will be used. Both types will be plotted with different number counts. The
    # final script can be found below.

    # As in the last example, we first import the required packages.
    from astropy import units as u
    from astropy.coordinates import SkyCoord
    import matplotlib.pyplot as plt
    import numpy as np

    # We now generate random data for visualization with
    # np.random.multivariate_normal.
    disk = np.random.multivariate_normal(mean=[0,0,0], cov=np.diag([1,1,0.5]), size=5000)
    bulge = np.random.multivariate_normal(mean=[0,0,0], cov=np.diag([1,1,1]), size=500)
    galaxy = np.concatenate([disk, bulge])

    # As the next step, those coordinates are transformed into an astropy.coordinates
    # astropy.coordinates.SkyCoord object.
    c_gal = SkyCoord(galaxy, representation_type='cartesian', frame='galactic')
    c_gal_icrs = c_gal.icrs

    # Again, as in the last example, we need to convert the coordinates in radians
    # and make sure they are between :math:`-\pi` and :math:`\pi`:
    ra_rad = c_gal_icrs.ra.wrap_at(180 * u.deg).radian
    dec_rad = c_gal_icrs.dec.radian

    # We use the same plotting setup as in the last example:
    plt.figure(figsize=(8,4.2))
    plt.subplot(111, projection="aitoff")
    plt.title("Aitoff projection of our random data", y=1.08)
    plt.grid(True)
    plt.plot(ra_rad, dec_rad, 'o', markersize=2, alpha=0.3)
    plt.subplots_adjust(top=0.95,bottom=0.0)
    plt.show()

..
  EXAMPLE END

.. _coordinates-skycoord-comparing:

Comparing SkyCoord Objects
==========================

There are two primary ways to compare |SkyCoord| objects to each other. First is
checking if the coordinates are within a specified distance of each other. This
is what most users should do in their science or processing analysis work
because it allows for a tolerance due to floating point representation issues.
The second is checking for exact equivalence of two objects down to the bit,
which is most useful for developers writing tests.

The example below illustrates the floating point issue using the exact
equality comparison, where we do a roundtrip transformation
FK4 => ICRS => FK4 and then compare::

  >>> sc1 = SkyCoord(1*u.deg, 2*u.deg, frame='fk4')
  >>> sc1.icrs.fk4 == sc1
  False

Matching Within Tolerance
-------------------------

To test if coordinates are within a certain angular distance of one other, use the
`~astropy.coordinates.SkyCoord.separation` method::

  >>> sc1.icrs.fk4.separation(sc1).to(u.arcsec)  # doctest: +SKIP
  <Angle 7.98873629e-13 arcsec>
  >>> sc1.icrs.fk4.separation(sc1) < 1e-9 * u.arcsec
  True

Exact Equality
--------------

Astropy also provides an exact equality operator for coordinates.
For example, when comparing, e.g., two |SkyCoord| objects::

    >>> left_skycoord == right_skycoord  # doctest: +SKIP

the right object must be strictly consistent with the left object for
comparison:

- Identical class
- Equivalent frames (`~astropy.coordinates.BaseCoordinateFrame.is_equivalent_frame`)
- Identical representation_types
- Identical representation differentials keys
- Identical frame attributes
- Identical "extra" frame attributes (e.g., ``obstime`` for an ICRS coord)

In the first example we show simple comparisons using array-valued coordinates::

  >>> sc1 = SkyCoord([1, 2]*u.deg, [3, 4]*u.deg)
  >>> sc2 = SkyCoord([1, 20]*u.deg, [3, 4]*u.deg)

  >>> sc1 == sc2  # Array-valued comparison
  array([ True, False])
  >>> sc2 == sc2[1]  # Broadcasting comparison with a scalar
  array([False,  True])
  >>> sc2[0] == sc2[1]  # Scalar to scalar comparison
  False
  >>> sc1 != sc2  # Not equal
  array([False,  True])

In addition to numerically comparing the representation component data (which
may include velocities), the equality comparison includes strict tests that all
of the frame attributes like ``equinox`` or ``obstime`` are exactly equal.  Any
mismatch in attributes will result in an exception being raised.  For example::

  >>> sc1 = SkyCoord([1, 2]*u.deg, [3, 4]*u.deg)
  >>> sc2 = SkyCoord([1, 20]*u.deg, [3, 4]*u.deg, obstime='2020-01-01')
  >>> sc1 == sc2  # doctest: +SKIP
  ...
  ValueError: cannot compare: extra frame attribute 'obstime' is not equivalent
   (perhaps compare the frames directly to avoid this exception)

In this example the ``obstime`` attribute is a so-called "extra" frame attribute
that does not apply directly to the ICRS coordinate frame. So we could compare
with the following, this time using the ``!=`` operator for variety::

  >>> sc1.frame != sc2.frame
  array([False, True])

One slightly special case is comparing two frames that both have no data, where
the return value is the same as ``frame1.is_equivalent_frame(frame2)``. For
example::

  >>> from astropy.coordinates import FK4
  >>> FK4() == FK4(obstime='2020-01-01')
  False

.. _skycoord-table-conversion:

Converting a SkyCoord to a Table
================================

A |SkyCoord| object can be converted to a |QTable| using its
:meth:`~astropy.coordinates.SkyCoord.to_table` method. The attributes of the
|SkyCoord| are converted to columns of the table or added to its metadata
depending on whether or not they have the same length as the |SkyCoord|. This
means that attributes such as ``obstime`` can become columns or metadata::

  >>> from astropy.coordinates import SkyCoord
  >>> from astropy.time import Time
  >>> sc = SkyCoord(ra=[15, 30], dec=[-70, -50], unit=u.deg,
  ...               obstime=Time([2000, 2010], format='jyear'))
  >>> t = sc.to_table()
  >>> t
  <QTable length=2>
     ra     dec   obstime
    deg     deg
  float64 float64   Time
  ------- ------- -------
     15.0   -70.0  2000.0
     30.0   -50.0  2010.0
  >>> t.meta
  {'representation_type': 'spherical', 'frame': 'icrs'}

  >>> sc = SkyCoord(l=[0, 20], b=[20, 0], unit=u.deg, frame='galactic',
  ...               obstime=Time(2000, format='jyear'))
  >>> t = sc.to_table()
  >>> t
  <QTable length=2>
     l       b
    deg     deg
  float64 float64
  ------- -------
      0.0    20.0
     20.0     0.0
  >>> t.meta
  {'obstime': <Time object: scale='tt' format='jyear' value=2000.0>,
   'representation_type': 'spherical', 'frame': 'galactic'}

Convenience Methods
===================

A number of convenience methods are available, and you are encouraged to read
the available docstrings below:

- `~astropy.coordinates.SkyCoord.match_to_catalog_sky`,
- `~astropy.coordinates.SkyCoord.match_to_catalog_3d`,
- `~astropy.coordinates.SkyCoord.position_angle`,
- `~astropy.coordinates.SkyCoord.separation`,
- `~astropy.coordinates.SkyCoord.separation_3d`
- `~astropy.coordinates.SkyCoord.apply_space_motion`

Additional information and examples can be found in the section on
:ref:`astropy-coordinates-separations-matching` and
:ref:`astropy-coordinates-apply-space-motion`.
.. We call EarthLocation.of_site here first to force the downloading
.. of sites.json so that future doctest output isn't cluttered with
.. "Downloading ... [done]". This can be removed once we have a better
.. way of ignoring output lines based on pattern-matching, e.g.:
.. https://github.com/astropy/pytest-doctestplus/issues/11

.. testsetup::

    >>> from astropy.coordinates import EarthLocation
    >>> EarthLocation.of_site('greenwich') # doctest: +IGNORE_OUTPUT +IGNORE_WARNINGS

.. _astropy-coordinates:

*******************************************************
Astronomical Coordinate Systems (`astropy.coordinates`)
*******************************************************

Introduction
============

The `~astropy.coordinates` package provides classes for representing a variety
of celestial/spatial coordinates and their velocity components, as well as tools
for converting between common coordinate systems in a uniform way.

Getting Started
===============

The best way to start using `~astropy.coordinates` is to use the |SkyCoord|
class. |SkyCoord| objects are instantiated by passing in positions (and
optional velocities) with specified units and a coordinate frame. Sky positions
are commonly passed in as `~astropy.units.Quantity` objects and the frame is
specified with the string name.

Example
-------

..
  EXAMPLE START
  Using the SkyCoord Class

To create a |SkyCoord| object to represent an ICRS (Right ascension [RA],
Declination [Dec]) sky position::

    >>> from astropy import units as u
    >>> from astropy.coordinates import SkyCoord
    >>> c = SkyCoord(ra=10.625*u.degree, dec=41.2*u.degree, frame='icrs')

The initializer for |SkyCoord| is very flexible and supports inputs provided in
a number of convenient formats. The following ways of initializing a coordinate
are all equivalent to the above::

    >>> c = SkyCoord(10.625, 41.2, frame='icrs', unit='deg')
    >>> c = SkyCoord('00h42m30s', '+41d12m00s', frame='icrs')
    >>> c = SkyCoord('00h42.5m', '+41d12m')
    >>> c = SkyCoord('00 42 30 +41 12 00', unit=(u.hourangle, u.deg))
    >>> c = SkyCoord('00:42.5 +41:12', unit=(u.hourangle, u.deg))
    >>> c  # doctest: +FLOAT_CMP
    <SkyCoord (ICRS): (ra, dec) in deg
        (10.625, 41.2)>

The examples above illustrate a few rules to follow when creating a
coordinate object:

- Coordinate values can be provided either as unnamed positional arguments or
  via keyword arguments like ``ra`` and ``dec``, or  ``l`` and ``b`` (depending
  on the frame).
- The coordinate ``frame`` keyword is optional because it defaults to
  `~astropy.coordinates.ICRS`.
- Angle units must be specified for all components, either by passing in a
  `~astropy.units.Quantity` object (e.g., ``10.5*u.degree``), by including them
  in the value (e.g., ``'+41d12m00s'``), or via the ``unit`` keyword.

..
  EXAMPLE END

|SkyCoord| and all other `~astropy.coordinates` objects also support
array coordinates. These work in the same way as single-value coordinates, but
they store multiple coordinates in a single object. When you are going
to apply the same operation to many different coordinates (say, from a
catalog), this is a better choice than a list of |SkyCoord| objects,
because it will be *much* faster than applying the operation to each
|SkyCoord| in a ``for`` loop. Like the underlying `~numpy.ndarray` instances
that contain the data, |SkyCoord| objects can be sliced, reshaped, etc.,
and can be used with functions like `numpy.moveaxis`, etc., that affect the
shape::

    >>> import numpy as np
    >>> c = SkyCoord(ra=[10, 11, 12, 13]*u.degree, dec=[41, -5, 42, 0]*u.degree)
    >>> c
    <SkyCoord (ICRS): (ra, dec) in deg
        [(10., 41.), (11., -5.), (12., 42.), (13.,  0.)]>
    >>> c[1]
    <SkyCoord (ICRS): (ra, dec) in deg
        (11., -5.)>
    >>> c.reshape(2, 2)
    <SkyCoord (ICRS): (ra, dec) in deg
        [[(10., 41.), (11., -5.)],
         [(12., 42.), (13.,  0.)]]>
    >>> np.roll(c, 1)
    <SkyCoord (ICRS): (ra, dec) in deg
        [(13.,  0.), (10., 41.), (11., -5.), (12., 42.)]>


Coordinate Access
-----------------

Once you have a coordinate object you can access the components of that
coordinate (e.g., RA, Dec) to get string representations of the full
coordinate.

The component values are accessed using (typically lowercase) named attributes
that depend on the coordinate frame (e.g., ICRS, Galactic, etc.). For the
default, ICRS, the coordinate component names are ``ra`` and ``dec``::

    >>> c = SkyCoord(ra=10.68458*u.degree, dec=41.26917*u.degree)
    >>> c.ra  # doctest: +FLOAT_CMP
    <Longitude 10.68458 deg>
    >>> c.ra.hour  # doctest: +FLOAT_CMP
    0.7123053333333335
    >>> c.ra.hms  # doctest: +FLOAT_CMP
    hms_tuple(h=0.0, m=42.0, s=44.299200000000525)
    >>> c.dec  # doctest: +FLOAT_CMP
    <Latitude 41.26917 deg>
    >>> c.dec.degree  # doctest: +FLOAT_CMP
    41.26917
    >>> c.dec.radian  # doctest: +FLOAT_CMP
    0.7202828960652683

Coordinates can be converted to strings using the
:meth:`~astropy.coordinates.SkyCoord.to_string` method::

    >>> c = SkyCoord(ra=10.68458*u.degree, dec=41.26917*u.degree)
    >>> c.to_string('decimal')
    '10.6846 41.2692'
    >>> c.to_string('dms')
    '10d41m04.488s 41d16m09.012s'
    >>> c.to_string('hmsdms')
    '00h42m44.2992s +41d16m09.012s'

For additional information see the section on :ref:`working_with_angles`.

Transformation
--------------

One convenient way to transform to a new coordinate frame is by accessing
the appropriately named attribute.

Example
^^^^^^^

..
  EXAMPLE START
  Transforming to a New Coordinate Frame

To get the coordinate in the `~astropy.coordinates.Galactic` frame use::

    >>> c_icrs = SkyCoord(ra=10.68458*u.degree, dec=41.26917*u.degree, frame='icrs')
    >>> c_icrs.galactic  # doctest: +FLOAT_CMP
    <SkyCoord (Galactic): (l, b) in deg
        (121.17424181, -21.57288557)>

For more control, you can use the `~astropy.coordinates.SkyCoord.transform_to`
method, which accepts a frame name, frame class, or frame instance::

    >>> c_fk5 = c_icrs.transform_to('fk5')  # c_icrs.fk5 does the same thing
    >>> c_fk5  # doctest: +FLOAT_CMP
    <SkyCoord (FK5: equinox=J2000.000): (ra, dec) in deg
        (10.68459154, 41.26917146)>

    >>> from astropy.coordinates import FK5
    >>> c_fk5.transform_to(FK5(equinox='J1975'))  # precess to a different equinox  # doctest: +FLOAT_CMP
    <SkyCoord (FK5: equinox=J1975.000): (ra, dec) in deg
        (10.34209135, 41.13232112)>

..
  EXAMPLE END

This form of `~astropy.coordinates.SkyCoord.transform_to` also makes it
possible to convert from celestial coordinates to
`~astropy.coordinates.AltAz` coordinates, allowing the use of |SkyCoord|
as a tool for planning observations. For a more complete example of
this, see :ref:`sphx_glr_generated_examples_coordinates_plot_obs-planning.py`.

Some coordinate frames such as `~astropy.coordinates.AltAz` require Earth
rotation information (UT1-UTC offset and/or polar motion) when transforming
to/from other frames. These Earth rotation values are automatically downloaded
from the International Earth Rotation and Reference Systems (IERS) service when
required. See :ref:`utils-iers` for details of this process.

Representation
--------------

So far we have been using a spherical coordinate representation in all of our
examples, and this is the default for the built-in frames. Frequently it is
convenient to initialize or work with a coordinate using a different
representation such as Cartesian or Cylindrical. This can be done by setting
the ``representation_type`` for either |SkyCoord| objects or low-level frame
coordinate objects.

Example
^^^^^^^

..
  EXAMPLE START
  Working with Nonspherical Coordinate Representations

To initialize or work with a coordinate using a different representation such
as Cartesian or Cylindrical::

    >>> c = SkyCoord(x=1, y=2, z=3, unit='kpc', representation_type='cartesian')
    >>> c  # doctest: +FLOAT_CMP
    <SkyCoord (ICRS): (x, y, z) in kpc
        (1., 2., 3.)>
    >>> c.x, c.y, c.z  # doctest: +FLOAT_CMP
    (<Quantity 1. kpc>, <Quantity 2. kpc>, <Quantity 3. kpc>)

    >>> c.representation_type = 'cylindrical'
    >>> c  # doctest: +FLOAT_CMP
    <SkyCoord (ICRS): (rho, phi, z) in (kpc, deg, kpc)
        (2.23606798, 63.43494882, 3.)>

For all of the details see :ref:`astropy-skycoord-representations`.

..
  EXAMPLE END

Distance
--------

|SkyCoord| and the individual frame classes also support specifying a distance
from the frame origin. The origin depends on the particular coordinate frame;
this can be, for example, centered on the earth, centered on the solar system
barycenter, etc.

Examples
^^^^^^^^

..
  EXAMPLE START
  Specifying a Distance with SkyCoord

Two angles and a distance specify a unique point in 3D space, which also allows
converting the coordinates to a Cartesian representation::

    >>> c = SkyCoord(ra=10.68458*u.degree, dec=41.26917*u.degree, distance=770*u.kpc)
    >>> c.cartesian.x  # doctest: +FLOAT_CMP
    <Quantity 568.71286542 kpc>
    >>> c.cartesian.y  # doctest: +FLOAT_CMP
    <Quantity 107.3008974 kpc>
    >>> c.cartesian.z  # doctest: +FLOAT_CMP
    <Quantity 507.88994292 kpc>

With distances assigned, |SkyCoord| convenience methods are more powerful, as
they can make use of the 3D information. For example, to compute the physical,
3D separation between two points in space::

    >>> c1 = SkyCoord(ra=10*u.degree, dec=9*u.degree, distance=10*u.pc, frame='icrs')
    >>> c2 = SkyCoord(ra=11*u.degree, dec=10*u.degree, distance=11.5*u.pc, frame='icrs')
    >>> c1.separation_3d(c2)  # doctest: +FLOAT_CMP
    <Distance 1.52286024 pc>

..
  EXAMPLE END

Convenience Methods
-------------------

|SkyCoord| defines a number of convenience methods that support, for example,
computing on-sky (i.e., angular) and 3D separations between two coordinates.

Examples
^^^^^^^^

..
  EXAMPLE START
  SkyCoord Convenience Methods

To compute on-sky and 3D separations between two coordinates::

    >>> c1 = SkyCoord(ra=10*u.degree, dec=9*u.degree, frame='icrs')
    >>> c2 = SkyCoord(ra=11*u.degree, dec=10*u.degree, frame='fk5')
    >>> c1.separation(c2)  # Differing frames handled correctly  # doctest: +FLOAT_CMP
    <Angle 1.40453359 deg>

Or cross-matching catalog coordinates (detailed in
:ref:`astropy-coordinates-matching`)::

    >>> target_c = SkyCoord(ra=10*u.degree, dec=9*u.degree, frame='icrs')
    >>> # read in coordinates from a catalog...
    >>> catalog_c = ... # doctest: +SKIP
    >>> idx, sep, _ = target_c.match_to_catalog_sky(catalog_c) # doctest: +SKIP

..
  EXAMPLE END

The `astropy.coordinates` sub-package also provides a quick way to get
coordinates for named objects, assuming you have an active internet
connection. The `~astropy.coordinates.SkyCoord.from_name` method of |SkyCoord|
uses `Sesame <http://cds.u-strasbg.fr/cgi-bin/Sesame>`_ to retrieve coordinates
for a particular named object.

..
  EXAMPLE START
  Retrieving Coordinates for a Named Object with SkyCoord

To retrieve coordinates for a particular named object::

    >>> SkyCoord.from_name("PSR J1012+5307")  # doctest: +REMOTE_DATA +FLOAT_CMP
    <SkyCoord (ICRS): (ra, dec) in deg
        (153.1393271, 53.117343)>

In some cases, the coordinates are embedded in the catalog name of the object.
For such object names, `~astropy.coordinates.SkyCoord.from_name` is able
to parse the coordinates from the name if given the ``parse=True`` option.
For slow connections, this may be much faster than a sesame query for the same
object name. It's worth noting, however, that the coordinates extracted in this
way may differ from the database coordinates by a few deci-arcseconds, so only
use this option if you do not need sub-arcsecond accuracy for your coordinates::

    >>> SkyCoord.from_name("CRTS SSS100805 J194428-420209", parse=True)  # doctest: +FLOAT_CMP
    <SkyCoord (ICRS): (ra, dec) in deg
        (296.11666667, -42.03583333)>

..
  EXAMPLE END

.. testsetup::

    >>> from astropy.coordinates import EarthLocation, SkyCoord
    >>> apo = EarthLocation(-1463969.30185172, -5166673.34223433, 3434985.71204565, unit='m')
    >>> keck = EarthLocation(-5464487.81759887, -2492806.59108569, 2151240.19451846, unit='m')
    >>> target = SkyCoord(10.68470833, 41.26875, unit='deg')  # M31

For sites (primarily observatories) on the Earth, `astropy.coordinates` provides
a quick way to get an `~astropy.coordinates.EarthLocation` - the
`~astropy.coordinates.EarthLocation.of_site` method::

    >>> from astropy.coordinates import EarthLocation
    >>> apo = EarthLocation.of_site('Apache Point Observatory')  # doctest: +SKIP
    >>> apo  # doctest: +FLOAT_CMP
    <EarthLocation (-1463969.30185172, -5166673.34223433, 3434985.71204565) m>

To see the list of site names available, use
:func:`astropy.coordinates.EarthLocation.get_site_names`.

For arbitrary Earth addresses (e.g., not observatory sites), use the
`~astropy.coordinates.EarthLocation.of_address` classmethod. Any address passed
to this function uses Google maps to retrieve the latitude and longitude and can
also (optionally) query Google maps to get the height of the location. As with
Google maps, this works with fully specified addresses, location names, city
names, etc.:

.. doctest-skip::

    >>> EarthLocation.of_address('1002 Holy Grail Court, St. Louis, MO')
    <EarthLocation (-26726.98216371, -4997009.8604809, 3950271.16507911) m>
    >>> EarthLocation.of_address('1002 Holy Grail Court, St. Louis, MO',
    ...                          get_height=True)
    <EarthLocation (-26727.6272786, -4997130.47437768, 3950367.15622108) m>
    >>> EarthLocation.of_address('Danbury, CT')
    <EarthLocation ( 1364606.64511651, -4593292.9428273,  4195415.93695139) m>

.. note::
    `~astropy.coordinates.SkyCoord.from_name`,
    `~astropy.coordinates.EarthLocation.of_site`, and
    `~astropy.coordinates.EarthLocation.of_address` are for convenience, and
    hence are by design relatively low precision. If you need more precise coordinates for an
    object you should find the appropriate reference and input the coordinates
    manually, or use more specialized functionality like that in the `astroquery
    <http://www.astropy.org/astroquery/>`_ or `astroplan
    <https://astroplan.readthedocs.io/>`_ affiliated packages.

    Also note that these methods retrieve data from the internet to
    determine the celestial or Earth coordinates. The online data may be
    updated, so if you need to guarantee that your scripts are reproducible
    in the long term, see the :doc:`remote_methods` section.

This functionality can be combined to do more complicated tasks like computing
barycentric corrections to radial velocity observations (also a supported
high-level |SkyCoord| method - see :ref:`astropy-coordinates-rv-corrs`)::

    >>> from astropy.time import Time
    >>> obstime = Time('2017-2-14')
    >>> target = SkyCoord.from_name('M31')  # doctest: +SKIP
    >>> keck = EarthLocation.of_site('Keck')  # doctest: +SKIP
    >>> target.radial_velocity_correction(obstime=obstime, location=keck).to('km/s')  # doctest: +FLOAT_CMP  +REMOTE_DATA
    <Quantity -22.359784554780255 km / s>

While ``astropy.coordinates`` does not natively support converting an Earth
location to a timezone, the longitude and latitude can be retrieved from any
`~astropy.coordinates.EarthLocation` object, which could then be passed to any
third-party package that supports timezone solving, such as `timezonefinder
<https://timezonefinder.readthedocs.io/>`_. For example, ``timezonefinder`` can
be used to retrieve the timezone name for an address with:

.. doctest-skip::

    >>> loc = EarthLocation.of_address('Tucson, AZ')
    >>> from timezonefinder import TimezoneFinder
    >>> tz_name = TimezoneFinder().timezone_at(lng=loc.lon.degree,
    ...                                        lat=loc.lat.degree)
    >>> tz_name
    'America/Phoenix'

The resulting timezone name could then be used with any packages that support
time zone definitions, such as the (Python 3.9 default package) `zoneinfo
<https://docs.python.org/3/library/zoneinfo.html>`_:

.. doctest-skip::

    >>> from zoneinfo import ZoneInfo  # requires Python 3.9 or greater
    >>> tz = ZoneInfo(tz_name)
    >>> dt = datetime.datetime(2021, 4, 12, 20, 0, 0, tzinfo=tz)

(Please note that the above code is not tested regularly with the ``astropy`` test
suite, so please raise an issue if this no longer works.)

Velocities (Proper Motions and Radial Velocities)
-------------------------------------------------

In addition to positional coordinates, `~astropy.coordinates` supports storing
and transforming velocities. These are available both via the lower-level
:doc:`coordinate frame classes <frames>`, and via |SkyCoord| objects::

    >>> sc = SkyCoord(1*u.deg, 2*u.deg, radial_velocity=20*u.km/u.s)
    >>> sc  # doctest: +FLOAT_CMP
    <SkyCoord (ICRS): (ra, dec) in deg
        (1., 2.)
     (radial_velocity) in km / s
        (20.,)>

For more details on velocity support (and limitations), see the
:doc:`velocities` page.

.. _astropy-coordinates-overview:

Overview of `astropy.coordinates` Concepts
==========================================

.. note ::
    More detailed information and justification of the design is available in
    `APE (Astropy Proposal for Enhancement) 5
    <https://github.com/astropy/astropy-APEs/blob/main/APE5.rst>`_.

Here we provide an overview of the package and associated framework.
This background information is not necessary for using `~astropy.coordinates`,
particularly if you use the |SkyCoord| high-level class, but it is helpful for
more advanced usage, particularly creating your own frame, transformations, or
representations. Another useful piece of background information are some
:ref:`astropy-coordinates-definitions` as they are used in
`~astropy.coordinates`.

`~astropy.coordinates` is built on a three-tiered system of objects:
representations, frames, and a high-level class. Representations
classes are a particular way of storing a three-dimensional data point
(or points), such as Cartesian coordinates or spherical polar
coordinates. Frames are particular reference frames like FK5 or ICRS,
which may store their data in different representations, but have well-
defined transformations between each other. These transformations are
all stored in the ``astropy.coordinates.frame_transform_graph``, and new
transformations can be created by users. Finally, the high-level class
(|SkyCoord|) uses the frame classes, but provides a more accessible
interface to these objects as well as various convenience methods and
more string-parsing capabilities.

Separating these concepts makes it easier to extend the functionality of
`~astropy.coordinates`. It allows representations, frames, and
transformations to be defined or extended separately, while still
preserving the high-level capabilities and ease-of-use of the |SkyCoord|
class.

.. topic:: Examples:

    See :ref:`sphx_glr_generated_examples_coordinates_plot_obs-planning.py` for
    an example of using the `~astropy.coordinates` functionality to prepare for
    an observing run.

Using `astropy.coordinates`
===========================

More detailed information on using the package is provided on separate pages,
listed below.

.. toctree::
   :maxdepth: 1

   angles
   skycoord
   transforming
   solarsystem
   satellites
   formatting
   matchsep
   representations
   frames
   velocities
   apply_space_motion
   spectralcoord
   galactocentric
   remote_methods
   common_errors
   definitions
   inplace


In addition, another resource for the capabilities of this package is the
``astropy.coordinates.tests.test_api_ape5`` testing file. It showcases most of
the major capabilities of the package, and hence is a useful supplement to
this document. You can see it by either downloading a copy of the Astropy
source code, or typing the following in an IPython session::

    In [1]: from astropy.coordinates.tests import test_api_ape5
    In [2]: test_api_ape5??


.. note that if this section gets too long, it should be moved to a separate
   doc page - see the top of performance.inc.rst for the instructions on how to
   do that
.. include:: performance.inc.rst

.. _astropy-coordinates-seealso:

See Also
========

Some references that are particularly useful in understanding subtleties of the
coordinate systems implemented here include:

* `USNO Circular 179 <https://arxiv.org/abs/astro-ph/0602086>`_
    A useful guide to the IAU 2000/2003 work surrounding ICRS/IERS/CIRS and
    related problems in precision coordinate system work.
* `Standards Of Fundamental Astronomy <http://www.iausofa.org/>`_
    The definitive implementation of IAU-defined algorithms. The "SOFA Tools
    for Earth Attitude" document is particularly valuable for understanding
    the latest IAU standards in detail.
* `IERS Conventions (2010) <https://www.iers.org/IERS/EN/Publications/TechnicalNotes/tn36.html>`_
    An exhaustive reference covering the ITRS, the IAU2000 celestial coordinates
    framework, and other related details of modern coordinate conventions.
* Meeus, J. "Astronomical Algorithms"
    A valuable text describing details of a wide range of coordinate-related
    problems and concepts.
* `Revisiting Spacetrack Report #3 <https://celestrak.com/publications/AIAA/2006-6753/AIAA-2006-6753-Rev2.pdf>`_
    A discussion of the simplified general perturbation (SGP) for satellite orbits, with a description of
    the True Equator Mean Equinox (TEME) coordinate frame.


Built-in Frame Classes
======================

.. automodapi:: astropy.coordinates.builtin_frames
    :skip: make_transform_graph_docs
    :no-inheritance-diagram:


.. _astropy-coordinates-api:

Reference/API
=============

.. automodapi:: astropy.coordinates
.. _astropy-coordinates-solarsystem:

Solar System Ephemerides
************************

`astropy.coordinates` can calculate the |SkyCoord| of some of the major solar
system objects. By default, it uses approximate orbital elements calculated
using PyERFA_ routines, but it can
also use more precise ones using the JPL ephemerides (which are derived from
dynamical models). The default JPL ephemerides (DE430) provide predictions
valid roughly for the years between 1550 and 2650. The file is 115 MB and will
need to be downloaded the first time you use this functionality, but will be
cached after that.

.. note::
   Using JPL ephemerides requires that the `jplephem
   <https://pypi.org/project/jplephem/>`_ package be installed. This is
   most conveniently achieved via ``pip install jplephem``, although whatever
   package management system you use might have it as well.

Three functions are provided; :meth:`~astropy.coordinates.get_body`,
:meth:`~astropy.coordinates.get_moon` and
:meth:`~astropy.coordinates.get_body_barycentric`. The first two functions
return |SkyCoord| objects in the `~astropy.coordinates.GCRS` frame, while the
latter returns a `~astropy.coordinates.CartesianRepresentation` of the
barycentric position of a body (i.e., in the `~astropy.coordinates.ICRS` frame).

Examples
--------

..
  EXAMPLE START
  Using the Solar System Ephemerides

Here is an example of using these functions with built-in ephemerides (i.e.,
without the need to download a large ephemerides file)::

  >>> from astropy.time import Time
  >>> from astropy.coordinates import solar_system_ephemeris, EarthLocation
  >>> from astropy.coordinates import get_body_barycentric, get_body, get_moon
  >>> t = Time("2014-09-22 23:22")
  >>> loc = EarthLocation.of_site('greenwich') # doctest: +REMOTE_DATA
  >>> with solar_system_ephemeris.set('builtin'):
  ...     jup = get_body('jupiter', t, loc) # doctest: +REMOTE_DATA +IGNORE_OUTPUT
  >>> jup  # doctest: +FLOAT_CMP +REMOTE_DATA
  <SkyCoord (GCRS: obstime=2014-09-22 23:22:00.000, obsgeoloc=(3949481.69182405, -550931.91022387, 4961151.73597633) m, obsgeovel=(40.159527, 287.47873161, -0.04597922) m / s): (ra, dec, distance) in (deg, deg, AU)
      (136.91116253, 17.02935396, 5.94386022)>

Above, we used ``solar_system_ephemeris`` as a context, which sets the default
ephemeris while in the ``with`` clause, and resets it at the end.

To get more precise positions than is possible with the built-in ephemeris
(see :ref:`astropy-coordinates-solarsystem-erfa-precision`), you
could use the ``de430`` ephemeris mentioned above, or, if you only care about
times between 1950 and 2050, opt for the ``de432s`` ephemeris, which is stored
in a smaller, ~10 MB, file (which will be downloaded and cached when the
ephemeris is set):

.. doctest-requires:: jplephem

  >>> solar_system_ephemeris.set('de432s') # doctest: +REMOTE_DATA, +IGNORE_OUTPUT
  <ScienceState solar_system_ephemeris: 'de432s'>
  >>> get_body('jupiter', t, loc) # doctest: +REMOTE_DATA, +FLOAT_CMP
  <SkyCoord (GCRS: obstime=2014-09-22 23:22:00.000, obsgeoloc=(3949481.69182405, -550931.91022387, 4961151.73597633) m, obsgeovel=(40.159527, 287.47873161, -0.04597922) m / s): (ra, dec, distance) in (deg, deg, km)
      (136.90234846, 17.03160654, 8.89196021e+08)>
  >>> get_moon(t, loc) # doctest: +REMOTE_DATA, +FLOAT_CMP
  <SkyCoord (GCRS: obstime=2014-09-22 23:22:00.000, obsgeoloc=(3949481.69182405, -550931.91022387, 4961151.73597633) m, obsgeovel=(40.159527, 287.47873161, -0.04597922) m / s): (ra, dec, distance) in (deg, deg, km)
      (165.51854528, 2.32861794, 407229.55638763)>
  >>> get_body_barycentric('moon', t) # doctest: +REMOTE_DATA, +FLOAT_CMP
  <CartesianRepresentation (x, y, z) in km
      (1.50107535e+08, -866789.11996916, -418963.55218495)>

For one-off calculations with a given ephemeris, you can also pass it directly
to the various functions:

.. doctest-requires:: jplephem

  >>> get_body_barycentric('moon', t, ephemeris='de432s')
  ... # doctest: +REMOTE_DATA, +FLOAT_CMP
  <CartesianRepresentation (x, y, z) in km
      (1.50107535e+08, -866789.11996916, -418963.55218495)>
  >>> get_body_barycentric('moon', t, ephemeris='builtin')
  ... # doctest: +FLOAT_CMP
  <CartesianRepresentation (x, y, z) in AU
      (1.00340683, -0.00579417, -0.00280064)>

..
  EXAMPLE END

For a list of the bodies for which positions can be calculated, do:

.. note that we skip the next test if jplephem is not installed because if
.. jplephem was not installed, we didn't change the science state higher up

.. doctest-requires:: jplephem

  >>> solar_system_ephemeris.bodies # doctest: +REMOTE_DATA
  ('sun',
   'mercury',
   'venus',
   'earth-moon-barycenter',
   'earth',
   'moon',
   'mars',
   'jupiter',
   'saturn',
   'uranus',
   'neptune',
   'pluto')
  >>> solar_system_ephemeris.set('builtin')
  <ScienceState solar_system_ephemeris: 'builtin'>
  >>> solar_system_ephemeris.bodies
  ('earth',
   'sun',
   'moon',
   'mercury',
   'venus',
   'earth-moon-barycenter',
   'mars',
   'jupiter',
   'saturn',
   'uranus',
   'neptune')

.. note ::
    While the sun is included in the these ephemerides, it is important to
    recognize that `~astropy.coordinates.get_sun` always uses the built-in,
    polynomial model (as this requires no special download). So it is not safe
    to assume that ``get_body(time, 'sun')`` and ``get_sun(time)`` will give
    the same result.

.. _astropy-coordinates-solarsystem-erfa-precision:

Precision of the Built-In Ephemeris
===================================

The algorithm for calculating positions and velocities for planets other than
Earth used by ERFA_ is due to J.L. Simon, P. Bretagnon, J. Chapront,
M. Chapront-Touze, G. Francou and J. Laskar (Bureau des Longitudes, Paris,
France).  From comparisons with JPL ephemeris DE102, they quote the maximum
errors over the interval 1800-2050 below. For more details, see the PyERFA_ routine, `erfa.plan94`.
For the Earth, the rms errors in position and velocity are about 4.6 km and
1.4 mm/s, respectively (see `erfa.epv00`).

.. list-table::

  * - Planet
    - L (arcsec)
    - B (arcsec)
    - R (km)
  * - Mercury
    - 4
    - 1
    - 300
  * - Venus
    - 5
    - 1
    - 800
  * - EMB
    - 6
    - 1
    - 1000
  * - Mars
    - 17
    - 1
    - 7700
  * - Jupiter
    - 71
    - 5
    - 76000
  * - Saturn
    - 81
    - 13
    - 267000
  * - Uranus
    - 86
    - 7
    - 712000
  * - Neptune
    - 11
    - 1
    - 253000
.. _astropy-coordinates-representations:

Using and Designing Coordinate Representations
**********************************************

Points in a 3D vector space can be represented in different ways, such as
Cartesian, spherical polar, cylindrical, and so on. These underlie the way
coordinate data in `astropy.coordinates` is represented, as described in the
:ref:`astropy-coordinates-overview`. Below, we describe how you can use them on
their own as a way to convert between different representations, including
ones not built-in, and to do simple vector arithmetic.

The built-in representation classes are:

* `~astropy.coordinates.CartesianRepresentation`: Cartesian
  coordinates ``x``, ``y``, and ``z``.
* `~astropy.coordinates.SphericalRepresentation`: spherical
  polar coordinates represented by a longitude (``lon``), a latitude
  (``lat``), and a distance (``distance``). The latitude is a value ranging
  from -90 to 90 degrees.
* `~astropy.coordinates.UnitSphericalRepresentation`:
  spherical polar coordinates on a unit sphere, represented by a longitude
  (``lon``) and latitude (``lat``).
* `~astropy.coordinates.PhysicsSphericalRepresentation`:
  spherical polar coordinates, represented by an inclination (``theta``) and
  azimuthal angle (``phi``), and radius ``r``. The inclination goes from 0 to
  180 degrees, and is related to the latitude in the
  `~astropy.coordinates.SphericalRepresentation` by
  ``theta = 90 deg - lat``.
* `~astropy.coordinates.CylindricalRepresentation`:
  cylindrical polar coordinates, represented by a cylindrical radius
  (``rho``), azimuthal angle (``phi``), and height (``z``).

.. Note::
   For information about using and changing the representation of
   `~astropy.coordinates.SkyCoord` objects, see the
   :ref:`astropy-skycoord-representations` section.

Instantiating and Converting
============================

Representation classes are instantiated with `~astropy.units.Quantity`
objects::

    >>> from astropy import units as u
    >>> from astropy.coordinates.representation import CartesianRepresentation
    >>> car = CartesianRepresentation(3 * u.kpc, 5 * u.kpc, 4 * u.kpc)
    >>> car  # doctest: +FLOAT_CMP
    <CartesianRepresentation (x, y, z) in kpc
        (3., 5., 4.)>

Array `~astropy.units.Quantity` objects can also be passed to
representations. They will have the expected shape, which can be changed using
methods with the same names as those for `~numpy.ndarray`, such as ``reshape``,
``ravel``, etc.::

  >>> x = u.Quantity([[1., 0., 0.], [3., 5., 3.]], u.m)
  >>> y = u.Quantity([[0., 2., 0.], [4., 0., -4.]], u.m)
  >>> z = u.Quantity([[0., 0., 3.], [0., 12., -12.]], u.m)
  >>> car_array = CartesianRepresentation(x, y, z)
  >>> car_array  # doctest: +FLOAT_CMP
  <CartesianRepresentation (x, y, z) in m
      [[(1.,  0.,   0.), (0.,  2.,   0.), (0.,  0.,   3.)],
       [(3.,  4.,   0.), (5.,  0.,  12.), (3., -4., -12.)]]>
  >>> car_array.shape
  (2, 3)
  >>> car_array.ravel()  # doctest: +FLOAT_CMP
  <CartesianRepresentation (x, y, z) in m
      [(1.,  0.,   0.), (0.,  2.,   0.), (0.,  0.,   3.), (3.,  4.,   0.),
       (5.,  0.,  12.), (3., -4., -12.)]>

Representations can be converted to other representations using the
``represent_as`` method::

    >>> from astropy.coordinates.representation import SphericalRepresentation, CylindricalRepresentation
    >>> sph = car.represent_as(SphericalRepresentation)
    >>> sph  # doctest: +FLOAT_CMP
    <SphericalRepresentation (lon, lat, distance) in (rad, rad, kpc)
        (1.03037683, 0.60126422, 7.07106781)>
    >>> cyl = car.represent_as(CylindricalRepresentation)
    >>> cyl  # doctest: +FLOAT_CMP
    <CylindricalRepresentation (rho, phi, z) in (kpc, rad, kpc)
        (5.83095189, 1.03037683, 4.)>

All representations can be converted to each other without loss of
information, with the exception of
`~astropy.coordinates.UnitSphericalRepresentation`. This class
is used to store the longitude and latitude of points but does not contain
any distance to the points, and assumes that they are located on a unit and
dimensionless sphere::

    >>> from astropy.coordinates.representation import UnitSphericalRepresentation
    >>> sph_unit = car.represent_as(UnitSphericalRepresentation)
    >>> sph_unit  # doctest: +FLOAT_CMP
    <UnitSphericalRepresentation (lon, lat) in rad
        (1.03037683, 0.60126422)>

Converting back to Cartesian, the absolute scaling information has been
removed, and the points are still located on a unit sphere::

    >>> sph_unit = car.represent_as(UnitSphericalRepresentation)
    >>> sph_unit.represent_as(CartesianRepresentation)  # doctest: +FLOAT_CMP
    <CartesianRepresentation (x, y, z) [dimensionless]
        (0.42426407, 0.70710678, 0.56568542)>


Array Values and NumPy Array Method Analogs
===========================================

Array `~astropy.units.Quantity` objects can also be passed to representations,
and such representations can be sliced, reshaped, etc., using the same methods
as are available to `~numpy.ndarray`. Corresponding functions, as well as
others that affect the shape, such as `~numpy.atleast_1d` and
`~numpy.rollaxis`, work as expected.

Example
-------

..
  EXAMPLE START
  Array Values and NumPy Array Method Analogs

To pass array `~astropy.units.Quantity` objects to representations::

  >>> import numpy as np
  >>> x = np.linspace(0., 5., 6)
  >>> y = np.linspace(10., 15., 6)
  >>> z = np.linspace(20., 25., 6)
  >>> car_array = CartesianRepresentation(x * u.m, y * u.m, z * u.m)
  >>> car_array
  <CartesianRepresentation (x, y, z) in m
      [(0., 10., 20.), (1., 11., 21.), (2., 12., 22.),
       (3., 13., 23.), (4., 14., 24.), (5., 15., 25.)]>

To manipulate using methods and ``numpy`` functions::

  >>> car_array.reshape(3, 2)
  <CartesianRepresentation (x, y, z) in m
      [[(0., 10., 20.), (1., 11., 21.)],
       [(2., 12., 22.), (3., 13., 23.)],
       [(4., 14., 24.), (5., 15., 25.)]]>
  >>> car_array[2]
  <CartesianRepresentation (x, y, z) in m
      (2., 12., 22.)>
  >>> car_array[2] = car_array[1]
  >>> car_array[:3]
  <CartesianRepresentation (x, y, z) in m
      [(0., 10., 20.), (1., 11., 21.), (1., 11., 21.)]>
  >>> np.roll(car_array, 1)
  <CartesianRepresentation (x, y, z) in m
      [(5., 15., 25.), (0., 10., 20.), (1., 11., 21.), (1., 11., 21.),
       (3., 13., 23.), (4., 14., 24.)]>

And to set elements using other representation classes (as long
as they are compatible in their units and number of dimensions)::

  >>> car_array[2] = SphericalRepresentation(0*u.deg, 0*u.deg, 99*u.m)
  >>> car_array[:3]  # doctest: +FLOAT_CMP
  <CartesianRepresentation (x, y, z) in m
      [(0., 10., 20.), (1., 11., 21.), (99., 0., 0.)]>
  >>> car_array[0] = UnitSphericalRepresentation(0*u.deg, 0*u.deg)
  Traceback (most recent call last):
  ...
  ValueError: value must be representable as CartesianRepresentation without loss of information.

..
  EXAMPLE END

.. _astropy-coordinates-representations-arithmetic:

Vector Arithmetic
=================

Representations support basic vector arithmetic such as taking the norm,
multiplying with and dividing by quantities, and taking dot and cross products,
as well as adding, subtracting, summing and taking averages of representations,
and multiplying with matrices.

.. Note:: All arithmetic except the matrix multiplication works with
   non-Cartesian representations as well. For taking the norm, multiplication,
   and division, this uses just the non-angular components, while for the other
   operations the representation is converted to Cartesian internally before
   the operation is done, and the result is converted back to the original
   representation. Hence, for optimal speed it may be best to work using
   Cartesian representations.

Examples
--------

..
  EXAMPLE START
  Vector Arithmetic Operations with Representation Objects

To see how vector arithmetic operations work with representation objects,
consider the following examples::

  >>> car_array = CartesianRepresentation([[1., 0., 0.], [3., 5.,  3.]] * u.m,
  ...                                     [[0., 2., 0.], [4., 0., -4.]] * u.m,
  ...                                     [[0., 0., 3.], [0.,12.,-12.]] * u.m)
  >>> car_array  # doctest: +FLOAT_CMP
  <CartesianRepresentation (x, y, z) in m
      [[(1.,  0.,  0.), (0.,  2.,   0.), (0.,  0.,   3.)],
       [(3.,  4.,  0.), (5.,  0.,  12.), (3., -4., -12.)]]>
  >>> car_array.norm()  # doctest: +FLOAT_CMP
  <Quantity [[ 1.,  2.,  3.],
             [ 5., 13., 13.]] m>
  >>> car_array / car_array.norm()  # doctest: +FLOAT_CMP
  <CartesianRepresentation (x, y, z) [dimensionless]
      [[(1.        ,  0.        ,  0.        ),
        (0.        ,  1.        ,  0.        ),
        (0.        ,  0.        ,  1.        )],
       [(0.6       ,  0.8       ,  0.        ),
        (0.38461538,  0.        ,  0.92307692),
        (0.23076923, -0.30769231, -0.92307692)]]>
  >>> (car_array[1] - car_array[0]) / (10. * u.s)  # doctest: +FLOAT_CMP
  <CartesianRepresentation (x, y, z) in m / s
      [(0.2,  0.4,  0. ), (0.5, -0.2,  1.2), (0.3, -0.4, -1.5)]>
  >>> car_array.sum()  # doctest: +FLOAT_CMP
  <CartesianRepresentation (x, y, z) in m
      (12.,  2.,  3.)>
  >>> car_array.mean(axis=0)  # doctest: +FLOAT_CMP
  <CartesianRepresentation (x, y, z) in m
      [(2. ,  2.,  0. ), (2.5,  1.,  6. ), (1.5, -2., -4.5)]>

  >>> unit_x = UnitSphericalRepresentation(0.*u.deg, 0.*u.deg)
  >>> unit_y = UnitSphericalRepresentation(90.*u.deg, 0.*u.deg)
  >>> unit_z = UnitSphericalRepresentation(0.*u.deg, 90.*u.deg)
  >>> car_array.dot(unit_x)  # doctest: +FLOAT_CMP
  <Quantity [[1., 0., 0.],
             [3., 5., 3.]] m>
  >>> car_array.dot(unit_y)  # doctest: +FLOAT_CMP
  <Quantity [[ 6.12323400e-17,  2.00000000e+00,  0.00000000e+00],
             [ 4.00000000e+00,  3.06161700e-16, -4.00000000e+00]] m>
  >>> car_array.dot(unit_z)  # doctest: +FLOAT_CMP
  <Quantity [[ 6.12323400e-17,  0.00000000e+00,  3.00000000e+00],
             [ 1.83697020e-16,  1.20000000e+01, -1.20000000e+01]] m>
  >>> car_array.cross(unit_x)  # doctest: +FLOAT_CMP
  <CartesianRepresentation (x, y, z) in m
      [[(0.,  0.,  0.), (0.,   0., -2.), (0.,   3.,  0.)],
       [(0.,  0., -4.), (0.,  12.,  0.), (0., -12.,  4.)]]>

  >>> from astropy.coordinates.matrix_utilities import rotation_matrix
  >>> rotation = rotation_matrix(90 * u.deg, axis='z')
  >>> rotation  # doctest: +FLOAT_CMP
  array([[ 6.12323400e-17,  1.00000000e+00,  0.00000000e+00],
         [-1.00000000e+00,  6.12323400e-17,  0.00000000e+00],
         [ 0.00000000e+00,  0.00000000e+00,  1.00000000e+00]])
  >>> car_array.transform(rotation)  # doctest: +FLOAT_CMP
  <CartesianRepresentation (x, y, z) in m
      [[( 6.12323400e-17, -1.00000000e+00,   0.),
        ( 2.00000000e+00,  1.22464680e-16,   0.),
        ( 0.00000000e+00,  0.00000000e+00,   3.)],
       [( 4.00000000e+00, -3.00000000e+00,   0.),
        ( 3.06161700e-16, -5.00000000e+00,  12.),
        (-4.00000000e+00, -3.00000000e+00, -12.)]]>

..
  EXAMPLE END

.. _astropy-coordinates-differentials:

Differentials and Derivatives of Representations
================================================

In addition to positions in 3D space, coordinates also deal with proper motions
and radial velocities, which require a way to represent differentials of
coordinates (i.e., finite realizations) of derivatives. To support this, the
representations all have corresponding ``Differential`` classes, which can hold
offsets or derivatives in terms of the components of the representation class.
Adding such an offset to a representation means the offset is taken in the
direction of the corresponding coordinate. (Although for any representation
other than Cartesian, this is only defined relative to a specific location, as
the unit vectors are not invariant.)

Examples
--------

..
  EXAMPLE START
  Differentials and Derivatives of Representations

To see how the ``Differential`` classes of representations works, consider the
following::

  >>> from astropy.coordinates import SphericalRepresentation, SphericalDifferential
  >>> sph_coo = SphericalRepresentation(lon=0.*u.deg, lat=0.*u.deg,
  ...                                   distance=1.*u.kpc)
  >>> sph_derivative = SphericalDifferential(d_lon=1.*u.arcsec/u.yr,
  ...                                        d_lat=0.*u.arcsec/u.yr,
  ...                                        d_distance=0.*u.km/u.s)
  >>> sph_derivative.to_cartesian(base=sph_coo)  # doctest: +FLOAT_CMP
  <CartesianRepresentation (x, y, z) in arcsec kpc / (rad yr)
      (0., 1., 0.)>

Note how the conversion to Cartesian can only be done using a ``base``, since
otherwise the code cannot know what direction an increase in longitude
corresponds to. For ``lon=0``, this is in the ``y`` direction. Now, to get
the coordinates at two later times::

  >>> sph_coo + sph_derivative * [1., 3600*180/np.pi] * u.yr  # doctest: +FLOAT_CMP
  <SphericalRepresentation (lon, lat, distance) in (rad, rad, kpc)
      [(4.84813681e-06, 0., 1.        ), (7.85398163e-01, 0., 1.41421356)]>

The above shows how addition is not to longitude itself, but in the direction
of increasing longitude: for the large shift, by the equivalent of one radian,
the distance has increased as well (after all, a source will likely not move
along a curve on the sky!). This also means that the order of operations is
important::

  >>> big_offset = SphericalDifferential(1.*u.radian, 0.*u.radian, 0.*u.kpc)
  >>> sph_coo + big_offset + big_offset  # doctest: +FLOAT_CMP
  <SphericalRepresentation (lon, lat, distance) in (rad, rad, kpc)
      (1.57079633, 0., 2.)>
  >>> sph_coo + (big_offset + big_offset)  # doctest: +FLOAT_CMP
  <SphericalRepresentation (lon, lat, distance) in (rad, rad, kpc)
      (1.10714872, 0., 2.23606798)>

..
  EXAMPLE END

..
  EXAMPLE START
  Working with Proper Motions and Radial Velocities in Differential Objects

Often, you may have just a proper motion or a radial velocity, but not both::

  >>> from astropy.coordinates import UnitSphericalDifferential, RadialDifferential
  >>> radvel = RadialDifferential(1000*u.km/u.s)
  >>> sph_coo + radvel * 1. * u.Myr  # doctest: +FLOAT_CMP
  <SphericalRepresentation (lon, lat, distance) in (rad, rad, kpc)
      (0., 0., 2.02271217)>
  >>> pm = UnitSphericalDifferential(1.*u.mas/u.yr, 0.*u.mas/u.yr)
  >>> sph_coo + pm * 1. * u.Myr  # doctest: +FLOAT_CMP
  <SphericalRepresentation (lon, lat, distance) in (rad, rad, kpc)
      (0.0048481, 0., 1.00001175)>
  >>> pm + radvel  # doctest: +FLOAT_CMP
  <SphericalDifferential (d_lon, d_lat, d_distance) in (mas / yr, mas / yr, km / s)
      (1., 0., 1000.)>
  >>> sph_coo + (pm + radvel) * 1. * u.Myr  # doctest: +FLOAT_CMP
  <SphericalRepresentation (lon, lat, distance) in (rad, rad, kpc)
      (0.00239684, 0., 2.02271798)>

Note in the above that the proper motion is defined strictly as a change in
longitude (i.e., it does not include a ``cos(latitude)`` term). There are
special classes where this term is included::

  >>> from astropy.coordinates import UnitSphericalCosLatDifferential
  >>> sph_lat60 = SphericalRepresentation(lon=0.*u.deg, lat=60.*u.deg,
  ...                                     distance=1.*u.kpc)
  >>> pm = UnitSphericalDifferential(1.*u.mas/u.yr, 0.*u.mas/u.yr)
  >>> pm  # doctest: +FLOAT_CMP
  <UnitSphericalDifferential (d_lon, d_lat) in mas / yr
      (1., 0.)>
  >>> pm_coslat = UnitSphericalCosLatDifferential(1.*u.mas/u.yr, 0.*u.mas/u.yr)
  >>> pm_coslat  # doctest: +FLOAT_CMP
  <UnitSphericalCosLatDifferential (d_lon_coslat, d_lat) in mas / yr
      (1., 0.)>
  >>> sph_lat60 + pm * 1. * u.Myr  # doctest: +FLOAT_CMP
  <SphericalRepresentation (lon, lat, distance) in (rad, rad, kpc)
      (0.0048481, 1.04719246, 1.00000294)>
  >>> sph_lat60 + pm_coslat * 1. * u.Myr  # doctest: +FLOAT_CMP
  <SphericalRepresentation (lon, lat, distance) in (rad, rad, kpc)
      (0.00969597, 1.0471772, 1.00001175)>

Close inspections shows that indeed the changes are as expected. The systems
with and without ``cos(latitude)`` can be converted to each other, provided you
supply the ``base`` (representation)::

  >>> usph_lat60 = sph_lat60.represent_as(UnitSphericalRepresentation)
  >>> pm_coslat2 = pm.represent_as(UnitSphericalCosLatDifferential,
  ...                              base=usph_lat60)
  >>> pm_coslat2  # doctest: +FLOAT_CMP
  <UnitSphericalCosLatDifferential (d_lon_coslat, d_lat) in mas / yr
      (0.5, 0.)>
  >>> sph_lat60 + pm_coslat2 * 1. * u.Myr  # doctest: +FLOAT_CMP
  <SphericalRepresentation (lon, lat, distance) in (rad, rad, kpc)
      (0.0048481, 1.04719246, 1.00000294)>

.. Note:: At present, the differential classes are generally meant to work with
   first derivatives, but they do not check the units of the inputs to enforce
   this. Passing in second derivatives (e.g., acceleration values with
   acceleration units) will succeed, but any transformations that occur through
   re-representation of the differential will not necessarily be correct.

..
  EXAMPLE END

Attaching ``Differential`` Objects to ``Representation`` Objects
================================================================

``Differential`` objects can be attached to ``Representation`` objects as a way
to encapsulate related information into a single object. ``Differential``
objects can be passed in to the initializer of any of the built-in
``Representation`` classes.

Example
-------

..
  EXAMPLE START
  Attaching Differential Objects to Representation Objects

To store a single velocity differential with a position::

  >>> from astropy.coordinates import representation as r
  >>> dif = r.SphericalDifferential(d_lon=1 * u.mas/u.yr,
  ...                               d_lat=2 * u.mas/u.yr,
  ...                               d_distance=3 * u.km/u.s)
  >>> rep = r.SphericalRepresentation(lon=0.*u.deg, lat=0.*u.deg,
  ...                                 distance=1.*u.kpc,
  ...                                 differentials=dif)
  >>> rep  # doctest: +FLOAT_CMP
  <SphericalRepresentation (lon, lat, distance) in (deg, deg, kpc)
      (0., 0., 1.)
   (has differentials w.r.t.: 's')>
  >>> rep.differentials  # doctest: +FLOAT_CMP
  {'s': <SphericalDifferential (d_lon, d_lat, d_distance) in (mas / yr, mas / yr, km / s)
       (1., 2., 3.)>}

..
  EXAMPLE END

The ``Differential`` objects are stored as a Python dictionary on the
``Representation`` object with keys equal to the (string) unit with which the
differential derivatives are taken (converted to SI).

..
  EXAMPLE START
  Differential and Representation Object Storage

In this case the key is ``'s'`` (second) because the ``Differential`` units are
velocities, a time derivative. Passing a single differential to the
``Representation`` initializer will automatically generate the necessary key
and store it in the differentials dictionary, but a dictionary is required to
specify multiple differentials::

  >>> dif2 = r.SphericalDifferential(d_lon=4 * u.mas/u.yr**2,
  ...                                d_lat=5 * u.mas/u.yr**2,
  ...                                d_distance=6 * u.km/u.s**2)
  >>> rep = r.SphericalRepresentation(lon=0.*u.deg, lat=0.*u.deg,
  ...                                 distance=1.*u.kpc,
  ...                                 differentials={'s': dif, 's2': dif2})
  >>> rep.differentials['s']  # doctest: +FLOAT_CMP
  <SphericalDifferential (d_lon, d_lat, d_distance) in (mas / yr, mas / yr, km / s)
      (1., 2., 3.)>
  >>> rep.differentials['s2']  # doctest: +FLOAT_CMP
  <SphericalDifferential (d_lon, d_lat, d_distance) in (mas / yr2, mas / yr2, km / s2)
      (4., 5., 6.)>

..
  EXAMPLE END

..
  EXAMPLE START
  Attaching Differential Objects to a Representation after Creation

``Differential`` objects can also be attached to a ``Representation`` after
creation::

  >>> rep = r.CartesianRepresentation(x=1 * u.kpc, y=2 * u.kpc, z=3 * u.kpc)
  >>> dif = r.CartesianDifferential(*[1, 2, 3] * u.km/u.s)
  >>> rep = rep.with_differentials(dif)
  >>> rep  # doctest: +FLOAT_CMP
  <CartesianRepresentation (x, y, z) in kpc
      (1., 2., 3.)
   (has differentials w.r.t.: 's')>

This works for array data as well, as long as the shape of the
``Differential`` data is the same as that of the ``Representation``::

  >>> xyz = np.arange(12).reshape(3, 4) * u.au
  >>> d_xyz = np.arange(12).reshape(3, 4) * u.km/u.s
  >>> rep = r.CartesianRepresentation(*xyz)
  >>> dif = r.CartesianDifferential(*d_xyz)
  >>> rep = rep.with_differentials(dif)
  >>> rep  # doctest: +FLOAT_CMP
  <CartesianRepresentation (x, y, z) in AU
      [(0., 4.,  8.), (1., 5.,  9.), (2., 6., 10.), (3., 7., 11.)]
   (has differentials w.r.t.: 's')>

..
  EXAMPLE END

..
  EXAMPLE START
  Converting Positional Data to a New Representation

As with a ``Representation`` instance without a differential, to convert the
positional data to a new representation, use the ``.represent_as()``::

  >>> rep.represent_as(r.SphericalRepresentation)  # doctest: +FLOAT_CMP
  <SphericalRepresentation (lon, lat, distance) in (rad, rad, AU)
      [(1.57079633, 1.10714872,  8.94427191),
       (1.37340077, 1.05532979, 10.34408043),
       (1.24904577, 1.00685369, 11.83215957),
       (1.16590454, 0.96522779, 13.37908816)]>

However, by passing just the desired representation class, only the
``Representation`` has changed, and the differentials are dropped. To
re-represent both the ``Representation`` and any ``Differential`` objects, you
must specify target classes for the ``Differential`` as well::

  >>> rep2 = rep.represent_as(r.SphericalRepresentation, r.SphericalDifferential)
  >>> rep2  # doctest: +FLOAT_CMP
  <SphericalRepresentation (lon, lat, distance) in (rad, rad, AU)
    [(1.57079633, 1.10714872,  8.94427191),
     (1.37340077, 1.05532979, 10.34408043),
     (1.24904577, 1.00685369, 11.83215957),
     (1.16590454, 0.96522779, 13.37908816)]
   (has differentials w.r.t.: 's')>
  >>> rep2.differentials['s']  # doctest: +FLOAT_CMP
  <SphericalDifferential (d_lon, d_lat, d_distance) in (km rad / (AU s), km rad / (AU s), km / s)
      [( 6.12323400e-17, 1.11022302e-16,  8.94427191),
       (-2.77555756e-17, 5.55111512e-17, 10.34408043),
       ( 0.00000000e+00, 0.00000000e+00, 11.83215957),
       ( 5.55111512e-17, 0.00000000e+00, 13.37908816)]>

..
  EXAMPLE END

..
  EXAMPLE START
  Shape-Changing Operations with Differential Objects

Shape-changing operations (e.g., reshapes) are propagated to all
``Differential`` objects because they are guaranteed to have the same shape as
their host ``Representation`` object::

  >>> rep.shape
  (4,)
  >>> rep.differentials['s'].shape
  (4,)
  >>> new_rep = rep.reshape(2, 2)
  >>> new_rep.shape
  (2, 2)
  >>> new_rep.differentials['s'].shape
  (2, 2)

This also works for slicing::

  >>> new_rep = rep[:2]
  >>> new_rep.shape
  (2,)
  >>> new_rep.differentials['s'].shape
  (2,)

Operations on representations that return `~astropy.units.Quantity` objects (as
opposed to other ``Representation`` instances) still work, but only operate on
the positional information, for example::

  >>> rep.norm()  # doctest: +FLOAT_CMP
  <Quantity [ 8.94427191, 10.34408043, 11.83215957, 13.37908816] AU>

Operations that involve combining or scaling representations or pairs of
representation objects that contain differentials will currently fail, but
support for some operations may be added in future versions::

  >>> rep + rep
  Traceback (most recent call last):
  ...
  TypeError: Operation 'add' is not supported when differentials are attached to a CartesianRepresentation.

If you have a ``Representation`` with attached ``Differential`` objects, you
can retrieve a copy of the ``Representation`` without the ``Differential``
object and use this ``Differential``-free object for any arithmetic operation::

  >>> 15 * rep.without_differentials()  # doctest: +FLOAT_CMP
  <CartesianRepresentation (x, y, z) in AU
      [( 0.,  60., 120.), (15.,  75., 135.), (30.,  90., 150.),
       (45., 105., 165.)]>

..
  EXAMPLE END

.. _astropy-coordinates-create-repr:

Creating Your Own Representations
=================================

To create your own representation class, your class must inherit from the
`~astropy.coordinates.BaseRepresentation` class. This base has an ``__init__``
method that will put all arguments components through their initializers,
verify they can be broadcast against each other, and store the components on
``self`` as the name prefixed with '_'. Furthermore, through its metaclass it
provides default properties for the components so that they can be accessed
using ``<instance>.<component>``. For the machinery to work, the following
must be defined:

* ``attr_classes`` class attribute (``OrderedDict``):

  Defines through its keys the names of the components (as well as the default
  order), and through its values defines the class of which they should be
  instances (which should be `~astropy.units.Quantity` or a subclass, or
  anything that can initialize it).

* ``from_cartesian`` class method:

  Takes a `~astropy.coordinates.CartesianRepresentation` object and
  returns an instance of your class.

* ``to_cartesian`` method:

  Returns a `~astropy.coordinates.CartesianRepresentation` object.

* ``__init__`` method (optional):

  If you want more than the basic initialization and checks provided by the
  base representation class, or just an explicit signature, you can define your
  own ``__init__``. In general, it is recommended to stay close to the
  signature assumed by the base representation, ``__init__(self, comp1, comp2,
  comp3, copy=True)``, and use ``super`` to call the base representation
  initializer.

Once you do this, you will then automatically be able to call ``represent_as``
to convert other representations to/from your representation class. Your
representation will also be available for use in |SkyCoord| and all frame
classes.

A representation class may also have a ``_unit_representation`` attribute
(although it is not required). This attribute points to the appropriate
"unit" representation (i.e., a representation that is dimensionless). This is
probably only meaningful for subclasses of
`~astropy.coordinates.SphericalRepresentation`, where it is assumed that it
will be a subclass of `~astropy.coordinates.UnitSphericalRepresentation`.

Finally, if you wish to also use offsets in your coordinate system, two further
methods should be defined (please see
`~astropy.coordinates.SphericalRepresentation` for an example):

* ``unit_vectors`` method:

  Returns a ``dict`` with a
  `~astropy.coordinates.CartesianRepresentation` of unit vectors in the
  direction of each component.

* ``scale_factors`` method:

  Returns a ``dict`` with a `~astropy.units.Quantity` for each component with
  the appropriate physical scale factor for a unit change in that direction.

And furthermore you should define a ``Differential`` class based on
`~astropy.coordinates.BaseDifferential`. This class only needs to define:

* ``base_representation`` attribute:

  A link back to the representation for which this differential holds.


In pseudo-code, this means that a class will look like::

    class MyRepresentation(BaseRepresentation):

        attr_classes = OrderedDict([('comp1', ComponentClass1),
                                     ('comp2', ComponentClass2),
                                     ('comp3', ComponentClass3)])

	# __init__ is optional
        def __init__(self, comp1, comp2, comp3, copy=True):
            super().__init__(comp1, comp2, comp3, copy=copy)
            ...

        @classmethod
        def from_cartesian(self, cartesian):
            ...
            return MyRepresentation(...)

        def to_cartesian(self):
            ...
            return CartesianRepresentation(...)

	# if differential motion is needed
	def unit_vectors(self):
	    ...
	    return {'comp1': CartesianRepresentation(...),
	            'comp2': CartesianRepresentation(...),
		    'comp3': CartesianRepresentation(...)}

        def scale_factors(self):
	    ...
	    return {'comp1': ...,
	            'comp2': ...,
		    'comp3': ...}

    class MyDifferential(BaseDifferential):
        base_representation = MyRepresentation
Formatting Coordinate Strings
*****************************

.. todo: @taldcroft should change this to start with a discussion of SkyCoord's capabilities

Getting a string representation of a coordinate is most powerfully
approached by treating the components (e.g., RA and Dec) separately.

Examples
--------

..
  EXAMPLE START
  Getting and Formatting String Representations of Coordinates

To get the string representation of a coordinate::

  >>> from astropy.coordinates import ICRS
  >>> from astropy import units as u
  >>> coo = ICRS(187.70592*u.degree, 12.39112*u.degree)
  >>> str(coo.ra) + ' ' + str(coo.dec)
  '187d42m21.312s 12d23m28.032s'

To get better control over the formatting, you can use the angles'
:meth:`~astropy.coordinates.Angle.to_string` method (see :doc:`angles` for
more). For example::

  >>> rahmsstr = coo.ra.to_string(u.hour)
  >>> str(rahmsstr)
  '12h30m49.4208s'
  >>> decdmsstr = coo.dec.to_string(u.degree, alwayssign=True)
  >>> str(decdmsstr)
  '+12d23m28.032s'
  >>> rahmsstr + ' ' + decdmsstr
  u'12h30m49.4208s +12d23m28.032s'

You can also use Python's `format` string method to create more complex
string expressions, such as IAU-style coordinates or even full sentences::

  >>> (f'SDSS J{coo.ra.to_string(unit=u.hourangle, sep="", precision=2, pad=True)}'
  ...  f'{coo.dec.to_string(sep="", precision=2, alwayssign=True, pad=True)}')
  'SDSS J123049.42+122328.03'
  >>> f'The galaxy M87, at an RA of {coo.ra.hour:.2f} hours and Dec of {coo.dec.deg:.1f} degrees, has an impressive jet.'
  'The galaxy M87, at an RA of 12.51 hours and Dec of 12.4 degrees, has an impressive jet.'

..
  EXAMPLE END
.. _astropy-coordinates-common-errors:

Common mistakes
***************

The following are some common sources of difficulty when using `~astropy.coordinates`.

Object Separation
-----------------

When calculating the separation between objects, it is important to bear in mind that
:meth:`astropy.coordinates.SkyCoord.separation` gives a different answer depending
upon the order in which is used. For example::

    >>> import numpy as np
    >>> from astropy import units as u
    >>> from astropy.coordinates import SkyCoord, GCRS
    >>> from astropy.time import Time
    >>> t = Time("2010-05-22T00:00")
    >>> moon = SkyCoord(104.29*u.deg, 23.51*u.deg, 359367.3*u.km, frame=GCRS(obstime=t))
    >>> star = SkyCoord(101.4*u.deg, 23.02*u.deg, frame='icrs')
    >>> star.separation(moon) # doctest: +FLOAT_CMP
    <Angle 139.84211884 deg>
    >>> moon.separation(star) # doctest: +FLOAT_CMP
    <Angle 2.70390995 deg>

Why do these give such different answers?

The reason is that :meth:`astropy.coordinates.SkyCoord.separation` gives the separation as measured
in the frame of the |SkyCoord| object. So ``star.separation(moon)`` gives the angular separation
in the ICRS frame. This is the separation as it would appear from the Solar System Barycenter. For a
geocentric observer, ``moon.separation(star)`` gives the correct answer, since ``moon`` is in a
geocentric frame.

AltAz calculations for Earth-based objects
------------------------------------------

One might expect that the following code snippet would produce an altitude of exactly 90 degrees::

    >>> from astropy.coordinates import EarthLocation, AltAz
    >>> from astropy.time import Time
    >>> from astropy import units as u

    >>> t = Time('J2010')
    >>> obj = EarthLocation(-1*u.deg, 52*u.deg, height=10.*u.km)
    >>> home = EarthLocation(-1*u.deg, 52*u.deg, height=0.*u.km)
    >>> altaz_frame = AltAz(obstime=t, location=home)
    >>> obj.get_itrs(t).transform_to(altaz_frame).alt # doctest: +FLOAT_CMP
    <Latitude 86.32878441 deg>

Why is the result over three degrees away from the zenith? It is only possible to understand by taking a very careful
look at what ``obj.get_itrs(t)`` returns. This call provides the ITRS position of the source ``obj``. ITRS is
a geocentric coordinate system, and includes the aberration of light. So the code above provides the ITRS position
of a source that appears directly overhead *for an observer at the geocenter*.

Due to aberration, the actual position of this source will be displaced from its apparent position, by an angle of
approximately 20.5 arcseconds. Because this source is about one Earth radius away, it's actual position is therefore
around 600 metres away from where it appears to be. This 600 metre shift, for an object 10 km above the Earth's surface
is an angular difference of just over three degrees - which is why this object does not appear overhead for a topocentric
observer - one on the surface of the Earth.

The correct way to construct a |SkyCoord| object for a source that is directly overhead a topocentric observer is
as follows::

    >>> from astropy.coordinates import EarthLocation, AltAz, ITRS, CIRS
    >>> from astropy.time import Time
    >>> from astropy import units as u

    >>> t = Time('J2010')
    >>> obj = EarthLocation(-1*u.deg, 52*u.deg, height=10.*u.km)
    >>> home = EarthLocation(-1*u.deg, 52*u.deg, height=0.*u.km)

    >>> # Now we make a ITRS vector of a straight overhead object
    >>> itrs_vec = obj.get_itrs(t).cartesian - home.get_itrs(t).cartesian

    >>> # Now we create a topocentric coordinate with this data
    >>> # Any topocentric frame will work, we use CIRS
    >>> # Start by transforming the ITRS vector to CIRS
    >>> cirs_vec = ITRS(itrs_vec, obstime=t).transform_to(CIRS(obstime=t)).cartesian
    >>> # Finally, make CIRS frame object with the correct data
    >>> cirs_topo = CIRS(cirs_vec, obstime=t, location=home)

    >>> # convert to AltAz
    >>> aa = cirs_topo.transform_to(AltAz(obstime=t, location=home))
    >>> aa.alt # doctest: +FLOAT_CMP
    <Latitude 90. deg>
.. _astropy-spectralcoord:

Using the SpectralCoord Class
*****************************

.. warning::

    The |SpectralCoord| class is new in Astropy v4.1 and should be considered
    experimental at this time. Note that we do not fully support cases
    where the observer and target are moving relativistically relative to each
    other, so care should be taken in those cases. It is possible that there
    will be API changes in future versions of Astropy based on user feedback. If
    you have specific ideas for how it might be improved, please  let us know on
    the `astropy-dev mailing list`_ or at http://feedback.astropy.org.

The |SpectralCoord| class provides an interface for representing and
transforming spectral coordinates such as frequencies, wavelengths, and photon
energies, as well as equivalent Doppler velocities. While the plain |Quantity|
class can also represent these kinds of physical quantities, and allow
conversion via dedicated equivalencies (such as :ref:`u.spectral
<astropy-units-spectral-equivalency>` or the :ref:`u.doppler_*
<astropy-units-doppler-equivalencies>` equivalencies), |SpectralCoord| (which is
a sub-class of |Quantity|) aims to make this more straightforward, and can also
be made aware of the observer and target reference frames, allowing for example
transformation from telescope-centric (or topocentric) frames to e.g.
Barycentric or Local Standard of Rest (LSRK and LSRD) velocity frames.

Creating SpectralCoord Objects
==============================

Since the |SpectralCoord| class is a sub-class of |Quantity|, the simplest way
to initialize it is to provide a value (or values) and a unit, or an existing
|Quantity|::

    >>> from astropy import units as u
    >>> from astropy.coordinates import SpectralCoord
    >>> sc1 = SpectralCoord(34.2, unit='GHz')
    >>> sc1
    <SpectralCoord 34.2 GHz>
    >>> sc2 = SpectralCoord([654.2, 654.4, 654.6] * u.nm)
    >>> sc2
    <SpectralCoord [654.2, 654.4, 654.6] nm>

At this point, we are not making any assumptions about the observer frame, or
the target that is being observed. As we will see in subsequent sections, more
information can be provided when initializing |SpectralCoord| objects, but first
we take a look at simple unit conversions with these objects.

Unit conversion
===============

By default, unit conversions between spectral units will work without having to
specify the :ref:`u.spectral <astropy-units-spectral-equivalency>` equivalency::

    >>> sc2.to(u.micron)
    <SpectralCoord [0.6542, 0.6544, 0.6546] micron>
    >>> sc2.to(u.eV)
    <SpectralCoord [1.89520328, 1.89462406, 1.89404519] eV>
    >>> sc2.to(u.THz)
    <SpectralCoord [458.25811373, 458.11805929, 457.97809044] THz>

As is the case with |Quantity| and the :ref:`Doppler equivalencies
<astropy-units-doppler-equivalencies>`, it is also possible to convert these
absolute spectral coordinates into velocities, assuming a particular rest
frequency or wavelength (such as that of a spectral line). For example, to
convert the above values into velocities relative to the Halpha line at 656.65
nm, assuming the optical Doppler convention, you can do::

    >>> sc3 = sc2.to(u.km / u.s,
    ...              doppler_convention='optical',
    ...              doppler_rest=656.65 * u.nm)
    >>> sc3
    <SpectralCoord
       (doppler_rest=656.65 nm
        doppler_convention=optical)
      [-1118.5433977 , -1027.23373258,  -935.92406746] km / s>

The rest value for the Doppler conversion as well as the convention to use are
stored in the resulting ``sc3`` |SpectralCoord| object. You can then convert
back to frequency without having to specify them again::

    >>> sc3.to(u.THz)
    <SpectralCoord
       (doppler_rest=656.65 nm
        doppler_convention=optical)
      [458.25811373, 458.11805929, 457.97809044] THz>

or you can explicitly specify a different convention or rest value to use::

    >>> sc3.to(u.km / u.s, doppler_convention='relativistic')
    <SpectralCoord
       (doppler_rest=656.65 nm
        doppler_convention=relativistic)
      [-1120.63005892, -1028.99362163,  -937.38499411] km / s>

It is also possible to set ``doppler_convention`` and ``doppler_rest`` from the
start, even when creating a |SpectralCoord| in frequency, energy, or
wavelength::

    >>> sc4 = SpectralCoord(343 * u.GHz,
    ...                     doppler_convention='radio',
    ...                     doppler_rest=342.91 * u.GHz)
    >>> sc4.to(u.km / u.s)
    <SpectralCoord
       (doppler_rest=342.91 GHz
        doppler_convention=radio)
      -78.68338987 km / s>


Reference frame transformations
===============================

If you work with any kind of spectral data, you will often need to determine
and/or apply velocity corrections due to different frames of reference, or apply
or remove the effects of redshift. There are two main ways to do this using the
|SpectralCoord| class:

* You can specify or change the velocity offset or redshift
  between the observer and the target without having to specify the
  absolute observer and target, but rather specify a velocity difference.  For example, that you know that there
  is a velocity difference of 15km/s along the line of sight, or that you are
  observing a galaxy at z=3.2. This can be useful for quick analysis but
  will not determine any frame transformations (e.g. from topocentric to
  barycentric) for you.

* You can specify the absolute position of the observer and the target,
  as well as the date of observation, which means that |SpectralCoord| can
  then compute different frame transformations. If information about the
  observer and target are available, this is the recommended approach,
  although it requires you to specify more information when setting up the
  |SpectralCoord|

In the next two sections we will look at each of these in turn.

Specifying radial velocity or redshift manually
-----------------------------------------------

As an example, we will consider an example of a |SpectralCoord| which represents
frequencies which form the x-axis of a (small) spectrum. We happen to know that
the target that was observed appears to be at a redshift of z=0.5, and we will
assume that any frequency shifts due to the Earth's motion are unimportant. In
the reference frame of the telescope, the spectrometer provides 10 values
between 500 and 900nm::

    >>> import numpy as np
    >>> wavs = SpectralCoord(np.linspace(500, 900, 9) * u.nm, redshift=0.5)
    >>> wavs  # doctest: +FLOAT_CMP
    <SpectralCoord
       (observer to target:
          radial_velocity=115304.79153846153 km / s
          redshift=0.5)
      [500., 550., 600., 650., 700., 750., 800., 850., 900.] nm>

We have set redshift=0.5 here so that we can keep track of what frame of reference
our spectral values are in. The ``radial_velocity`` property gives the recession
velocity equivalent to that redshift, and it is indeed large enough that we don't need
to worry about the rotation of the Earth on itself around the Sun (which would be
at most a ~30km/s contribution).

.. note:: In the context of |SpectralCoord|, we use the full relativistic relation
          between redshift and velocity, i.e. :math:`1 + z = \sqrt{(1 + v/c)/(1 - v/c)}`

We now want to shift the wavelengths so that they would be in the rest frame of
the galaxy. We can do this using the
:meth:`~astropy.coordinates.SpectralCoord.to_rest` method::

    >>> wavs_rest = wavs.to_rest()
    >>> wavs_rest
    <SpectralCoord
       (observer to target:
          radial_velocity=0.0 km / s
          redshift=0.0)
      [333.33333333, 366.66666667, 400.        , 433.33333333, 466.66666667,
       500.        , 533.33333333, 566.66666667, 600.        ] nm>

The wavelengths have decreased by 1/3, which is what we expect for z=0.5. Note
that the ``redshift`` and ``radial_velocity`` properties are now zero, since we
are in the reference frame of the target. We can also use the
:meth:`~astropy.coordinates.SpectralCoord.with_radial_velocity_shift` method to more
generically apply redshift and velocity corrections. The simplest way to use
this method is to give a single value that will be applied to the target - if
this value does not have units, it is interpreted as a redshift::

    >>> wavs_orig = wavs_rest.with_radial_velocity_shift(0.5)
    >>> wavs_orig  # doctest: +FLOAT_CMP
    <SpectralCoord
       (observer to target:
          radial_velocity=115304.79153846153 km / s
          redshift=0.5)
      [500., 550., 600., 650., 700., 750., 800., 850., 900.] nm>

This returns an object equivalent to the one we started with, since we've
re-applied a redshift of 0.5. We could also provide a velocity as a |Quantity|::

    >>> wavs_rest.with_radial_velocity_shift(100000 * u.km / u.s)
    <SpectralCoord
       (observer to target:
          radial_velocity=100000.0 km / s
          redshift=0.41458078170200463)
      [471.52692723, 518.67961996, 565.83231268, 612.9850054 , 660.13769813,
       707.29039085, 754.44308357, 801.5957763 , 848.74846902] nm>

which shifts the values to a frame of reference at a redshift of approximately
0.33 (that is, if the spectrum did contain a contribution from an object at
z=0.33, these would be the rest wavelengths for that object.

Specifying an observer and a target explicitly
----------------------------------------------

.. testsetup::

    >>> from astropy.coordinates import EarthLocation
    >>> location = EarthLocation(2225015.30883296, -5440016.41799762, -2481631.27428014, unit='m')

To use the more advanced functionality in |SpectralCoord|, including the ability
to easily transform between different well-defined velocity frames, you will
need to give it information about the location (and optionally velocity) of
the observer and target. This is done by passing either coordinate frame objects
or |SkyCoord| objects. To take a concrete example, let's assume that we are now
observe the source T Tau using the ALMA telescope. To create an observer object
corresponding to this, we can make use of the |EarthLocation| class::

    >>> from astropy.coordinates import EarthLocation
    >>> location = EarthLocation.of_site('ALMA')  # doctest: +SKIP
    >>> location  # doctest: +FLOAT_CMP
    <EarthLocation (2225015.30883296, -5440016.41799762, -2481631.27428014) m>

The three values in meters are geocentric coordinates, i.e. the 3D coordinates
relative to the center of the Earth. See |EarthLocation| for more details about
the different ways of creating these kinds of objects.

Once you have done this, you will need to convert ``location`` to a coordinate
object using the :meth:`~astropy.coordinates.EarthLocation.get_itrs` method,
which takes the observation time (which is important to know for any kind of
velocity frame transformation)::

    >>> from astropy.time import Time
    >>> alma = location.get_itrs(obstime=Time('2019-04-24T02:32:10'))
    >>> alma  # doctest: +FLOAT_CMP
    <ITRS Coordinate (obstime=2019-04-24T02:32:10.000): (x, y, z) in m
        (2225015.30883296, -5440016.41799762, -2481631.27428014)>

ITRS here stands for International Terrestrial Reference System which is a 3D
coordinate frame centered on the Earth's center and rotating with the Earth, so
the observatory will be stationary in this frame of reference.

For the target, the simplest way is to use the |SkyCoord| class::

    >>> from astropy.coordinates import SkyCoord
    >>> ttau = SkyCoord('04h21m59.43s +19d32m06.4', frame='icrs',
    ...                 radial_velocity=23.9 * u.km / u.s,
    ...                 distance=144.321 * u.pc)

In this case we specified a radial velocity and a distance for the target (using
the `T Tauri SIMBAD entry
<http://simbad.u-strasbg.fr/simbad/sim-id?Ident=T+Tauri>`_, but it is also
possible to not specify these, which means the target is assumed to be
stationary in the frame in which it is observed, and are assumed to be at large
distance from the Sun (such that any parallax effects would be unimportant if
relevant). The radial velocity is assumed to be in the frame used to define the
target location, so it is relative to the ICRS origin (the Solar System
barycenter) in the above case.

We now define a set of frequencies corresponding to the channels in which fluxes
have been measured (for the purposes of the example here we will assume we have only
11 frequencies)::

    >>> sc_ttau = SpectralCoord(np.linspace(200, 300, 11) * u.GHz,
    ...                         observer=alma, target=ttau)  # doctest: +IGNORE_WARNINGS
    >>> sc_ttau  # doctest: +FLOAT_CMP +REMOTE_DATA
    <SpectralCoord
       (observer: <ITRS Coordinate (obstime=2019-04-24T02:32:10.000): (x, y, z) in m
                      (2225015.30883296, -5440016.41799762, -2481631.27428014)
                   (v_x, v_y, v_z) in km / s
                      (0., 0., 0.)>
        target: <ICRS Coordinate: (ra, dec, distance) in (deg, deg, pc)
                    (65.497625, 19.53511111, 144.321)
                 (radial_velocity) in km / s
                    (23.9,)>
        observer to target (computed from above):
          radial_velocity=41.03594953774002 km / s
          redshift=0.00013689056329480032)
      [200., 210., 220., 230., 240., 250., 260., 270., 280., 290., 300.] GHz>

We can already see above that |SpectralCoord| has computed the difference in
velocity between the observatory and T Tau, which includes the motion of the
observatory around the Earth, the motion of the Earth around the Solar System
barycenter, and the radial velocity of T Tau relative to the Solar System
barycenter. We can get this value directly with::

    >>> sc_ttau.radial_velocity  # doctest: +FLOAT_CMP +REMOTE_DATA
    <Quantity 41.03594948 km / s>

If you work with any kind of spectral data, you will often need to determine
and/or apply velocity corrections due to different frames of reference. For
example if you have observations of the same object on the sky taken at
different dates, it is common to transform these to a common velocity frame of
reference, so that your spectral coordinates are those that would have applied
if the observer had been stationary relative to e.g. the Solar System
Barycenter. You may also want to transform your spectral coordinates so that
they would be in a frame at rest relative to the local standard of rest (LSR),
the center of the Milky Way, the Local Group, or even the Cosmic Microwave
Background (CMB) dipole.

We can transform our frequencies for the observations of T Tau to different
velocity frames using the
:meth:`~astropy.coordinates.SpectralCoord.with_observer_stationary_relative_to`
method. This method can take the name of an existing coordinate/velocity frame,
a :class:`~astropy.coordinates.BaseCoordinateFrame` instance, or any arbitrary
3D position and velocity coordinate object defined either as a
:class:`~astropy.coordinates.BaseCoordinateFrame` or a |SkyCoord| object. Most
commonly-used frames are accessible using strings. For example to transform to a
velocity frame stationary with respect to the center of the Earth (so removing
the effect of the Earth's rotation), we can use the ``'gcrs'`` which stands for
*Geocentric Celestial Reference System* (GCRS)::

    >>> sc_ttau.with_observer_stationary_relative_to('gcrs')  # doctest: +SKIP
    <SpectralCoord
       (observer: <GCRS Coordinate (obstime=2019-04-24T02:32:10.000, obsgeoloc=(0., 0., 0.) m, obsgeovel=(0., 0., 0.) m / s): (x, y, z) in m
                      (-5878853.86171412, -192921.84773269, -2470794.19765021)
                   (v_x, v_y, v_z) in km / s
                      (4.33251262e-09, 8.96175625e-08, -1.49258412e-08)>
        target: <ICRS Coordinate: (ra, dec, distance) in (deg, deg, pc)
                    (65.497625, 19.53511111, 144.321)
                 (radial_velocity) in km / s
                    (23.9,)>
        observer to target (computed from above):
          radial_velocity=40.674086368345165 km / s
          redshift=0.00013568335316072044)
      [200.00024141, 210.00025348, 220.00026555, 230.00027762, 240.00028969,
       250.00030176, 260.00031383, 270.0003259 , 280.00033797, 290.00035004,
       300.00036211] GHz>

As you can see, the frequencies have changed slightly, which is because we have
removed the Doppler shift caused by the Earth's rotation (this can also be seen
in the ``radial_velocity`` property, which has changed by ~0.35 km/s. To use a
velocity reference frame relative to the Solar System barycenter, which is the
origin of the *International Celestial Reference System* (ICRS) system, we can use::

    >>> sc_ttau.with_observer_stationary_relative_to('icrs')  # doctest: +FLOAT_CMP +REMOTE_DATA
    <SpectralCoord
       (observer: <ICRS Coordinate: (x, y, z) in m
                      (-1.25867767e+11, -7.48979688e+10, -3.24757657e+10)
                   (v_x, v_y, v_z) in km / s
                      (0., 0., 0.)>
        target: <ICRS Coordinate: (ra, dec, distance) in (deg, deg, pc)
                    (65.497625, 19.53511111, 144.321)
                 (radial_velocity) in km / s
                    (23.9,)>
        observer to target (computed from above):
          radial_velocity=23.9 km / s
          redshift=7.97249967898761e-05)
      [200.0114322 , 210.01200381, 220.01257542, 230.01314703, 240.01371864,
       250.01429025, 260.01486186, 270.01543347, 280.01600508, 290.01657669,
       300.0171483 ] GHz>

Note that in this case the total radial velocity between the observer and the
target matches what we specified when we set up the target, since it was defined
relative to the ICRS origin (the Solar System barycenter). The observer location
is still as before, but the observer velocity is now ~10-20 km/s in x, y, and z,
which is because the observer is now stationary relative to the barycenter so has
a significant velocity relative to the surface of the Earth.

We can also transform the frequencies to the Kinematic Local Standard of Rest
(LSRK) frame of reference, which is a reference frame commonly used in some
branches of astronomy (such as radio astronomy)::

    >>> sc_ttau.with_observer_stationary_relative_to('lsrk')  # doctest: +FLOAT_CMP +REMOTE_DATA
    <SpectralCoord
       (observer: <LSRK Coordinate: (x, y, z) in m
                      (-1.25867767e+11, -7.48979688e+10, -3.24757657e+10)
                   (v_x, v_y, v_z) in km / s
                      (0., 0., 0.)>
        target: <ICRS Coordinate: (ra, dec, distance) in (deg, deg, pc)
                    (65.497625, 19.53511111, 144.321)
                 (radial_velocity) in km / s
                    (23.9,)>
        observer to target (computed from above):
          radial_velocity=12.50698856018455 km / s
          redshift=4.171969349386906e-05)
      [200.01903338, 210.01998505, 220.02093672, 230.02188839, 240.02284006,
       250.02379172, 260.02474339, 270.02569506, 280.02664673, 290.0275984 ,
       300.02855007] GHz>


See :ref:`spectralcoord-common-frames` for a list of common velocity frames
available as strings on the |SpectralCoord| class.

Since we can give any arbitrary |SkyCoord| to the
:meth:`~astropy.coordinates.SpectralCoord.with_observer_stationary_relative_to`
method, we can also specify the target itself, to find the frequencies in the
rest frame of the target::

    >>> sc_ttau_targetframe = sc_ttau.with_observer_stationary_relative_to(sc_ttau.target)  # doctest: +REMOTE_DATA
    >>> sc_ttau_targetframe  # doctest: +FLOAT_CMP +REMOTE_DATA
    <SpectralCoord
       (observer: <ICRS Coordinate: (x, y, z) in m
                      (-1.25867767e+11, -7.48979688e+10, -3.24757657e+10)
                   (v_x, v_y, v_z) in km / s
                      (9.34149908, 20.49579745, 7.99178839)>
        target: <ICRS Coordinate: (ra, dec, distance) in (deg, deg, pc)
                    (65.497625, 19.53511111, 144.321)
                 (radial_velocity) in km / s
                    (23.9,)>
        observer to target (computed from above):
          radial_velocity=0.0 km / s
          redshift=0.0)
      [200.02737811, 210.02874702, 220.03011592, 230.03148483, 240.03285374,
       250.03422264, 260.03559155, 270.03696045, 280.03832936, 290.03969826,
       300.04106717] GHz>

The ``radial_velocity``, which is the velocity offset between observer and
target, is now zero.

|SpectralCoord| is intended to be versatile and be useful for representing any spectral
values - not just the x-axis of a spectrum, but also for example the
frequencies of spectral features. For example, if we now consider that we found a
spectral feature that appears to have components at the following frequencies
in the frame of reference of the telescope::

    >>> sc_feat = SpectralCoord([115.26, 115.266, 115.267] * u.GHz,
    ...                         observer=alma, target=ttau)  # doctest: +IGNORE_WARNINGS

We can convert these to the rest frame of the target using::

    >>> sc_feat_rest = sc_feat.with_observer_stationary_relative_to(sc_feat.target)  # doctest: +REMOTE_DATA
    >>> sc_feat_rest  # doctest: +FLOAT_CMP +REMOTE_DATA
    <SpectralCoord
       (observer: <ICRS Coordinate: (x, y, z) in m
                      (-1.25867767e+11, -7.48979688e+10, -3.24757657e+10)
                   (v_x, v_y, v_z) in km / s
                      (9.34149908, 20.49579745, 7.99178839)>
        target: <ICRS Coordinate: (ra, dec, distance) in (deg, deg, pc)
                    (65.497625, 19.53511111, 144.321)
                 (radial_velocity) in km / s
                    (23.9,)>
        observer to target (computed from above):
          radial_velocity=0.0 km / s
          redshift=0.0)
      [115.27577801, 115.28177883, 115.28277896] GHz>

The frequencies are very close to the rest frequency of the 12CO J=1-0 molecular line transition,
which is 115.2712018 GHz. However, they are not exactly the same, so if the features we see are
indeed from 12CO, then they are Doppler shifted compared to what we consider the rest frame of
T Tau. We can convert these frequencies to velocities assuming the Doppler shift equation
(in this case with the radio convention)::

    >>> sc_feat_rest.to(u.km / u.s, doppler_convention='radio', doppler_rest=115.27120180 * u.GHz)  # doctest: +FLOAT_CMP +REMOTE_DATA
    <SpectralCoord
       (observer: <ICRS Coordinate: (x, y, z) in m
                      (-1.25867767e+11, -7.48979688e+10, -3.24757657e+10)
                   (v_x, v_y, v_z) in km / s
                      (9.34149908, 20.49579745, 7.99178839)>
        target: <ICRS Coordinate: (ra, dec, distance) in (deg, deg, pc)
                    (65.497625, 19.53511111, 144.321)
                 (radial_velocity) in km / s
                    (23.9,)>
        observer to target (computed from above):
          radial_velocity=0.0 km / s
          redshift=0.0
        doppler_rest=115.2712018 GHz
        doppler_convention=radio)
      [-11.90160353, -27.50828545, -30.1093991 ] km / s>

Note that these resulting velocities are different from the ``radial_velocity``
property (which is still zero here) - the latter is the difference in velocity
between observer and target, while the former are how much the spectral values
are Doppler shifted by relative to the rest frequency or wavelength.

So if the features are indeed from 12CO, they have velocities of approximately -11.9, -27.5 and
-30.1 km/s relative to the T tau rest frame.

.. _spectralcoord-common-frames:

Common velocity frames
======================

Any valid astropy coordinate frame can be passed to the
:meth:`~astropy.coordinates.SpectralCoord.with_observer_stationary_relative_to`
method, including string aliases such as ``icrs``. Below we list some of the
frames commonly used to define spectral coordinates in:

The velocity frames available as constants on the |SpectralCoord| class are:

========================== =================================================
Frame name                 Description
========================== =================================================
``'gcrs'``                 Geocentric frame (defined as stationary relative to the GCRS origin)
``'icrs'``                 Barycentric frame (defined as stationary relative to the ICRS origin)
``'hcrs'``                 Heliocentric frame (defined as stationary relative to the HCRS origin)
``'lsrk``                  Kinematic Local Standard of Rest (LSRK),
                           defined as having a velocity of 20 km/s towards
                           18h +30d (B1900) relative to the Solar System
                           Barycenter [1]_.
``'lsrd'``                 Dynamical Local Standard of Rest (LSRD),
                           defined as having a velocity of U=9 km/s,
                           V=12 km/s, and W=7 km/s in Galactic coordinates
                           (equivalent to 16.552945 km/s towards l=53.13
                           and b=25.02) [2]_.
``'lsr'``                  A more recent definition of the Local Standard
                           of rest, with U=11.1 km/s,
                           V=12.24 km/s, and W=7.25 km/s in Galactic coordinates [3]_.
========================== =================================================

Defining custom velocity frames
===============================

As mentioned in the earlier examples on this page, it is possible to pass any
arbitrary :class:`~astropy.coordinates.BaseCoordinateFrame` or |SkyCoord| object
to the :meth:`~astropy.coordinates.SpectralCoord.with_observer_stationary_relative_to` method,
and the observer will be updated to be stationary relative to those coordinates.
As an example, we can define an object that can be used to define a velocity
frame that moves with the local group of galaxies. There is not a unique definition
of this, but for the purposes of this example we use the IAU 1976-recommended
value which states that the Solar System barycenter is moving at 300 km/s towards
l=90 and b=0 in the velocity frame of the local group of galaxies [4]_. Given
this value, we can define the velocity frame using::

    >>> from astropy.coordinates import Galactic
    >>> localgroup_frame = Galactic(u=0 * u.km, v=0 * u.km, w=0 * u.km,
    ...                             U=0 * u.km / u.s, V=-300 * u.km / u.s, W=0 * u.km / u.s,
    ...                             representation_type='cartesian',
    ...                             differential_type='cartesian')

Note that here we specify the velocity as -300, because what we need here is the
velocity of the local group relative to the Solar System barycenter. With this
object, we can then transform a |SpectralCoord| so that the observer is stationary
in that frame of reference::

    >>> sc_ttau.with_observer_stationary_relative_to(localgroup_frame)  # doctest: +FLOAT_CMP +REMOTE_DATA
    <SpectralCoord
       (observer: <Galactic Coordinate: (u, v, w) in m
                      (8.8038652e+10, -5.31344273e+10, 1.09238291e+11)
                   (U, V, W) in km / s
                      (-1.42108547e-14, -300., 2.84217094e-14)>
        target: <ICRS Coordinate: (ra, dec, distance) in (deg, deg, pc)
                    (65.497625, 19.53511111, 144.321)
                 (radial_velocity) in km / s
                    (23.9,)>
        observer to target (computed from above):
          radial_velocity=42.33062895275233 km / s
          redshift=0.00014120974955456056)
      [199.99913628, 209.9990931 , 219.99904991, 229.99900673, 239.99896354,
       249.99892036, 259.99887717, 269.99883398, 279.9987908 , 289.99874761,
       299.99870443] GHz>

References
==========

.. [1] Meeks, M. L. 1976, *Methods of experimental physics. Vol._12.
       Astrophysics. Part C: Radio observations*, Section 6.1 by Gordon, M. A.
       `[ADS] <https://ui.adsabs.harvard.edu/abs/1976mep..book.....M>`__.
.. [2] Delhaye, J. 1965, *Galactic Structure*. Edited by Adriaan Blaauw and
       Maarten Schmidt. Published by the University of Chicago Press, p61
       `[ADS] <https://ui.adsabs.harvard.edu/abs/1965gast.book...61D>`__.
.. [3] Schnrich, R., Binney, J., & Dehnen, W. 2010, MNRAS, 403, 1829
       `[ADS] <https://ui.adsabs.harvard.edu/abs/2010MNRAS.403.1829S>`__.
.. [4] *Transactions of the IAU Vol. XVI B Proceedings of the 16th General
      Assembly, Reports of Meetings of Commissions: Comptes Rendus
      Des Sances Des Commissions, Commission 28*.
      `[DOI] <https://doi.org/10.1017/S0251107X00002406>`__

.. The following frames are defined in FITS WCS and may be added here in future:
..
.. ``GALACTOCENTRIC_KLB1986`` Galactocentric frame defined as having a velocity
..                            of 220 km/s towards l=90 and b=0 relative to
..                            the Solar System Barycenter [3]_.
.. ``LOCALGROUP_IAU1976``     Velocity frame representing the motion of the
..                            Local Group of galaxies, and defined as having a velocity
..                            of 300 km/s towards l=90 and b=0 relative to
..                            the Solar System Barycenter [4]_.
.. ``CMBDIPOL_WMAP1``         Velocity frame representing the motion of the
..                            cosmic microwave background (CMB) dipole based on the
..                            1-year WMAP data, and defined as a temperature
..                            difference of 3.346mK (corresponding to approximately
..                            368 km/s) in the direction of l=263.85, b=48.25 [5]_
.. .. [3] Kerr, F. J., & Lynden-Bell, D. 1986, MNRAS, 221, 1023
..       `[ADS] <https://ui.adsabs.harvard.edu/abs/1986MNRAS.221.1023K>`__.
.. .. [5] Bennett, C. L., Halpern, M., Hinshaw, G., et al. 2003, ApJS, 148, 1
..       `[ADS] <https://ui.adsabs.harvard.edu/abs/2003ApJS..148....1B>`__.
Fitting with constraints
========================

`~astropy.modeling.fitting` support constraints, however, different fitters support
different types of constraints. The `~astropy.modeling.fitting.Fitter.supported_constraints`
attribute shows the type of constraints supported by a specific fitter::

    >>> from astropy.modeling import fitting
    >>> fitting.LinearLSQFitter.supported_constraints
    ['fixed']
    >>> fitting.LevMarLSQFitter.supported_constraints
    ['fixed', 'tied', 'bounds']
    >>> fitting.SLSQPLSQFitter.supported_constraints
    ['bounds', 'eqcons', 'ineqcons', 'fixed', 'tied']

Fixed Parameter Constraint
--------------------------

All fitters support fixed (frozen) parameters through the ``fixed`` argument
to models or setting the `~astropy.modeling.Parameter.fixed`
attribute directly on a parameter.

For linear fitters, freezing a polynomial coefficient means that the
corresponding term will be subtracted from the data before fitting a
polynomial without that term to the result. For example, fixing ``c0`` in a
polynomial model will fit a polynomial with the zero-th order term missing
to the data minus that constant. The fixed coefficients and corresponding terms
are restored to the fit polynomial and this is the polynomial returned from the fitter::

    >>> import numpy as np
    >>> np.random.seed(seed=12345)
    >>> from astropy.modeling import models, fitting
    >>> x = np.arange(1, 10, .1)
    >>> p1 = models.Polynomial1D(2, c0=[1, 1], c1=[2, 2], c2=[3, 3],
    ...                          n_models=2)
    >>> p1  # doctest: +FLOAT_CMP
    <Polynomial1D(2, c0=[1., 1.], c1=[2., 2.], c2=[3., 3.], n_models=2)>
    >>> y = p1(x, model_set_axis=False)
    >>> n = (np.random.randn(y.size)).reshape(y.shape)
    >>> p1.c0.fixed = True
    >>> pfit = fitting.LinearLSQFitter()
    >>> new_model = pfit(p1, x, y + n)  # doctest: +IGNORE_WARNINGS
    >>> print(new_model)  # doctest: +SKIP
    Model: Polynomial1D
    Inputs: ('x',)
    Outputs: ('y',)
    Model set size: 2
    Degree: 2
    Parameters:
         c0         c1                 c2
        --- ------------------ ------------------
        1.0  2.072116176718454   2.99115839177437
        1.0 1.9818866652726403 3.0024208951927585

The syntax to fix the same parameter ``c0`` using an argument to the model
instead of ``p1.c0.fixed = True`` would be::

    >>> p1 = models.Polynomial1D(2, c0=[1, 1], c1=[2, 2], c2=[3, 3],
    ...                          n_models=2, fixed={'c0': True})


Bounded Constraints
-------------------

Bounded fitting is supported through the ``bounds`` arguments to models or by
setting `~astropy.modeling.Parameter.min` and `~astropy.modeling.Parameter.max`
attributes on a parameter.  Bounds for the
`~astropy.modeling.fitting.LevMarLSQFitter` are always exactly satisfied--if
the value of the parameter is outside the fitting interval, it will be reset to
the value at the bounds. The `~astropy.modeling.fitting.SLSQPLSQFitter` optimization
algorithm handles bounds internally.

.. _tied:

Tied Constraints
----------------

The `~astropy.modeling.Parameter.tied` constraint is often useful with :ref:`Compound models <compound-models-intro>`.
In this example we will read a spectrum from a file called ``spec.txt``
and fit Gaussians to the lines simultaneously while linking the flux of the OIII_1 and OIII_2 lines.

.. plot::
    :include-source:

    import numpy as np
    from astropy.io import ascii
    from astropy.utils.data import get_pkg_data_filename
    from astropy.modeling import models, fitting
    fname = get_pkg_data_filename('data/spec.txt', package='astropy.modeling.tests')
    spec = ascii.read(fname)
    wave = spec['lambda']
    flux = spec['flux']

    # Use the rest wavelengths of known lines as initial values for the fit.

    Hbeta = 4862.721
    OIII_1 = 4958.911
    OIII_2 = 5008.239

    # Create Gaussian1D models for each of the Hbeta and OIII lines.

    h_beta = models.Gaussian1D(amplitude=34, mean=Hbeta, stddev=5)
    o3_2 = models.Gaussian1D(amplitude=170, mean=OIII_2, stddev=5)
    o3_1 = models.Gaussian1D(amplitude=57, mean=OIII_1, stddev=5)


    # Tie the ratio of the intensity of the two OIII lines.

    def tie_ampl(model):
        return model.amplitude_2 / 3.1

    o3_1.amplitude.tied = tie_ampl


    # Also tie the wavelength of the Hbeta line to the OIII wavelength.

    def tie_wave(model):
        return model.mean_0 * OIII_1 / Hbeta

    o3_1.mean.tied = tie_wave

    # Create a Polynomial model to fit the continuum.

    mean_flux = flux.mean()
    cont = np.where(flux > mean_flux, mean_flux, flux)
    linfitter = fitting.LinearLSQFitter()
    poly_cont = linfitter(models.Polynomial1D(1), wave, cont)

    # Create a compound model for the three lines and the continuum.

    hbeta_combo = h_beta + o3_1 + o3_2 + poly_cont

    # Fit all lines simultaneously -
    # this will need one iteration more than the default of 100.

    fitter = fitting.LevMarLSQFitter()
    fitted_model = fitter(hbeta_combo, wave, flux, maxiter=111)
    fitted_lines = fitted_model(wave)

    from matplotlib import pyplot as plt
    fig = plt.figure(figsize=(9, 6))
    p = plt.plot(wave, flux, label="data")
    p = plt.plot(wave, fitted_lines, 'r', label="fit")
    p = plt.legend()
    p = plt.xlabel("Wavelength")
    p = plt.ylabel("Flux")
    t = plt.text(4800, 70, 'Hbeta', rotation=90)
    t = plt.text(4900, 100, 'OIII_1', rotation=90)
    t = plt.text(4950, 180, 'OIII_2', rotation=90)
    plt.show()
.. _predef_physicalmodels:

***************
Physical Models
***************

These are models that are physical motivated, generally as solutions to
physical problems.  This is in contrast to those that are mathematically motivated,
generally as solutions to mathematical problems.

.. _blackbody-planck-law:

BlackBody
=========

The :class:`~astropy.modeling.physical_models.BlackBody` model provides a model
for using `Planck's Law <https://en.wikipedia.org/wiki/Planck%27s_law>`_.
The blackbody function is

.. math::

   B_{\nu}(T) = A \frac{2 h \nu^{3} / c^{2}}{exp(h \nu / k T) - 1}

where :math:`\nu` is the frequency, :math:`T` is the temperature,
:math:`A` is the scaling factor,
:math:`h` is the Plank constant, :math:`c` is the speed of light, and
:math:`k` is the Boltzmann constant.

The two parameters of the model the scaling factor ``scale`` (A) and
the absolute temperature ``temperature`` (T).  If the ``scale`` factor does not
have units, then the result is in units of spectral radiance, specifically
ergs/(cm^2 Hz s sr).  If the ``scale`` factor is passed with spectral radiance units,
then the result is in those units (e.g., ergs/(cm^2 A s sr) or MJy/sr).
Setting the ``scale`` factor with units of ergs/(cm^2 A s sr) will give the
Planck function as :math:`B_\lambda`.
The temperature can be passed as a Quantity with any supported temperature unit.

An example plot for a blackbody with a temperature of 10000 K and a scale of 1 is
shown below.  A scale of 1 shows the Planck function with no scaling in the
default units returned by :class:`~astropy.modeling.physical_models.BlackBody`.

.. plot::
    :include-source:

    import numpy as np
    import matplotlib.pyplot as plt

    from astropy.modeling.models import BlackBody
    import astropy.units as u

    wavelengths = np.logspace(np.log10(1000), np.log10(3e4), num=1000) * u.AA

    # blackbody parameters
    temperature = 10000 * u.K

    # BlackBody provides the results in ergs/(cm^2 Hz s sr) when scale has no units
    bb = BlackBody(temperature=temperature, scale=10000.0)
    bb_result = bb(wavelengths)

    fig, ax = plt.subplots(ncols=1)
    ax.plot(wavelengths, bb_result, '-')

    ax.set_xscale('log')
    ax.set_xlabel(fr"$\lambda$ [{wavelengths.unit}]")
    ax.set_ylabel(fr"$F(\lambda)$ [{bb_result.unit}]")

    plt.tight_layout()
    plt.show()

The :meth:`~astropy.modeling.physical_models.BlackBody.bolometric_flux` member
function gives the bolometric flux using
:math:`\sigma T^4/\pi` where :math:`\sigma` is the Stefan-Boltzmann constant.

The :meth:`~astropy.modeling.physical_models.BlackBody.lambda_max` and
:meth:`~astropy.modeling.physical_models.BlackBody.nu_max` member functions
give the wavelength and frequency of the maximum for :math:`B_\lambda`
and :math:`B_\nu`, respectively, calculated using `Wien's Law
<https://en.wikipedia.org/wiki/Wien%27s_displacement_law>`_.

Drude1D
=======

The :class:`~astropy.modeling.physical_models.Drude1D` model provides a model
for the behavior of an electron in a material
(see `Drude Model <https://en.wikipedia.org/wiki/Drude_model>`_).
Like the :class:`~astropy.modeling.functional_models.Lorentz1D` model, the Drude model
has broader wings than the :class:`~astropy.modeling.functional_models.Gaussian1D`
model.  The Drude profile has been used to model dust features including the
2175 Angstrom extinction feature and the mid-infrared aromatic/PAH features.
The Drude function at :math:`x` is

.. math::

    D(x) = A \frac{(f/x_0)^2}{((x/x_0 - x_0/x)^2 + (f/x_0)^2}

where :math:`A` is the amplitude, :math:`f` is the full width at half maximum,
and :math:`x_0` is the central wavelength.  An example of a Drude1D model
with :math:`x_0 = 2175` Angstrom and :math:`f = 400` Angstrom is shown below.

.. plot::
    :include-source:

    import numpy as np
    import matplotlib.pyplot as plt

    from astropy.modeling.models import Drude1D
    import astropy.units as u

    wavelengths = np.linspace(1000, 4000, num=1000) * u.AA

    # Parameters and model
    mod = Drude1D(amplitude=1.0, x_0=2175. * u.AA, fwhm=400. * u.AA)
    mod_result = mod(wavelengths)

    fig, ax = plt.subplots(ncols=1)
    ax.plot(wavelengths, mod_result, '-')

    ax.set_xlabel(fr"$\lambda$ [{wavelengths.unit}]")
    ax.set_ylabel(r"$D(\lambda)$")

    plt.tight_layout()
    plt.show()

.. _NFW:

NFW
=========

The :class:`~astropy.modeling.physical_models.NFW` model computes a
1-dimensional NavarroFrenkWhite profile. The dark matter density in an
NFW profile is given by:


.. math::

   \rho(r)=\frac{\delta_c\rho_{c}}{r/r_s(1+r/r_s)^2}

where :math:`\rho_{c}` is the critical density of the Universe at the redshift
of the profile, :math:`\delta_c` is the over density, and :math:`r_s` is the
scale radius of the profile.


This model relies on three parameters:

  ``mass`` : the mass of the profile (in solar masses if no units are provided)

  ``concentration`` : the profile concentration

  ``redshift`` : the redshift of the profile

As well as two optional initialization variables:

  ``massfactor`` : tuple or string specifying the overdensity type and factor (default ("critical", 200))

  ``cosmo`` : the cosmology for density calculation (default default_cosmology)

.. note::
	Initialization of NFW profile object required before evaluation (in order to set mass
	overdensity and cosmology).


Sample plots of an NFW profile with the following parameters are displayed below:
  ``mass`` = :math:`2.0 x 10^{15} M_{sun}`

  ``concentration`` = 8.5

  ``redshift`` = 0.63

The first plot is of the NFW profile density as a function of radius.
The second plot displays the profile density and radius normalized by the NFW scale
density and scale radius, respectively. The scale density and scale radius are available
as attributes ``rho_s`` and ``r_s``, and the overdensity radius can be accessed via ``r_virial``.

.. plot::
    :include-source:

    import numpy as np
    import matplotlib.pyplot as plt
    from astropy.modeling.models import NFW
    import astropy.units as u
    from astropy import cosmology

    # NFW Parameters
    mass = u.Quantity(2.0E15, u.M_sun)
    concentration = 8.5
    redshift = 0.63
    cosmo = cosmology.Planck15
    massfactor = ("critical", 200)

    # Create NFW Object
    n = NFW(mass=mass, concentration=concentration, redshift=redshift, cosmo=cosmo,
	    massfactor=massfactor)

    # Radial distribution for plotting
    radii = range(1,2001,10) * u.kpc

    # Radial NFW density distribution
    n_result = n(radii)

    # Plot creation
    fig, ax = plt.subplots(2)
    fig.suptitle('1 Dimensional NFW Profile')

    # Density profile subplot
    ax[0].plot(radii, n_result, '-')
    ax[0].set_yscale('log')
    ax[0].set_xlabel(fr"$r$ [{radii.unit}]")
    ax[0].set_ylabel(fr"$\rho$ [{n_result.unit}]")

    # Create scaled density / scaled radius subplot
    # NFW Object
    n = NFW(mass=mass, concentration=concentration, redshift=redshift, cosmo=cosmo,
	    massfactor=massfactor)

    # Radial distribution for plotting
    radii = np.logspace(np.log10(1e-5), np.log10(2), num=1000) * u.Mpc
    n_result = n(radii)

    # Scaled density / scaled radius subplot
    ax[1].plot(radii / n.radius_s, n_result / n.density_s, '-')
    ax[1].set_xscale('log')
    ax[1].set_yscale('log')
    ax[1].set_xlabel(r"$r / r_s$")
    ax[1].set_ylabel(r"$\rho / \rho_s$")

    # Display plot
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.show()



The :meth:`~astropy.modeling.physical_models.NFW.circular_velocity` member provides the circular
velocity at each position ``r`` via the equation:


.. math::

   v_{circ}(r)^2=\frac{1}{x}\frac{\ln(1+cx)-(cx)/(1+cx)}{\ln(1+c)-c/(1+c)}

where x is the ratio ``r``:math:`/r_{vir}`. Circular velocities are provided in km/s.

A sample plot of circular velocities of an NFW profile with the following parameters is displayed
below:

  ``mass`` = :math:`2.0 x 10^{15} M_{sun}`

  ``concentration`` = 8.5

  ``redshift`` = 0.63

The maximum circular velocity and radius of maximum circular velocity are available as attributes
``v_max`` and ``r_max``.


.. plot::
    :include-source:

    import matplotlib.pyplot as plt
    from astropy.modeling.models import NFW
    import astropy.units as u
    from astropy import cosmology

    # NFW Parameters
    mass = u.Quantity(2.0E15, u.M_sun)
    concentration = 8.5
    redshift = 0.63
    cosmo = cosmology.Planck15
    massfactor = ("critical", 200)

    # Create NFW Object
    n = NFW(mass=mass, concentration=concentration, redshift=redshift, cosmo=cosmo,
            massfactor=massfactor)

    # Radial distribution for plotting
    radii = range(1,200001,10) * u.kpc

    # NFW circular velocity distribution
    n_result = n.circular_velocity(radii)

    # Plot creation
    fig,ax = plt.subplots()
    ax.set_title('NFW Profile Circular Velocity')
    ax.plot(radii, n_result, '-')
    ax.set_xscale('log')
    ax.set_xlabel(fr"$r$ [{radii.unit}]")
    ax.set_ylabel(r"$v_{circ}$" + f" [{n_result.unit}]")

    # Display plot
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.show()


.. _Cosmologies:

Cosmologies
===========

The instances of the |Cosmology| class (and subclasses) include
|Cosmology.to_format|, a method to convert a Cosmology to another python
object. Specifically, any redshift method can be converted to a
:class:`~astropy.modeling.FittableModel` instance using the argument
``format="astropy.model"``.
During the conversion, each |Cosmology| :class:`~astropy.cosmology.Parameter`
is converted to a :class:`astropy.modeling.Model`
:class:`~astropy.modeling.Parameter`, while the redshift-method becomes the
model's ``__call__`` / ``evaluate`` method.
This means cosmologies can now be fit with data!

.. code-block::

    >>> from astropy.cosmology import Planck18
    >>> model = Planck18.to_format(format="astropy.model", method="lookback_time")
    >>> model
    <FlatLambdaCDMCosmologyLookbackTimeModel(H0=67.66 km / (Mpc s), Om0=0.30966,
        Tcmb0=2.7255 K, Neff=3.046, m_nu=[0.  , 0.  , 0.06] eV, Ob0=0.04897,
        name='Planck18')>

When finished, e.g. fitting, a model can be turned back into a |Cosmology|
using |Cosmology.from_format|.

.. code-block::

    >>> from astropy.cosmology import Cosmology
    >>> cosmo = Cosmology.from_format(model, format="astropy.model")
    >>> cosmo == Planck18
    True
.. _new_fitter:

Defining New Fitter Classes
***************************

This section describes how to add a new nonlinear fitting algorithm to this
package or write a user-defined fitter.  In short, one needs to define an error
function and a ``__call__`` method and define the types of constraints which
work with this fitter (if any).

The details are described below using scipy's SLSQP algorithm as an example.
The base class for all fitters is `~astropy.modeling.fitting.Fitter`::

    class SLSQPFitter(Fitter):
        supported_constraints = ['bounds', 'eqcons', 'ineqcons', 'fixed',
                                 'tied']

        def __init__(self):
            # Most currently defined fitters take no arguments in their
            # __init__, but the option certainly exists for custom fitters
            super().__init__()

All fitters take a model (their ``__call__`` method modifies the model's
parameters) as their first argument.

Next, the error function takes a list of parameters returned by an iteration of
the fitting algorithm and input coordinates, evaluates the model with them and
returns some type of a measure for the fit.  In the example the sum of the
squared residuals is used as a measure of fitting.::

    def objective_function(self, fps, *args):
        model = args[0]
        meas = args[-1]
        model.fitparams(fps)
        res = self.model(*args[1:-1]) - meas
        return np.sum(res**2)

The ``__call__`` method performs the fitting. As a minimum it takes all
coordinates as separate arguments. Additional arguments are passed as
necessary::

    def __call__(self, model, x, y , maxiter=MAXITER, epsilon=EPS):
        if model.linear:
                raise ModelLinearityException(
                    'Model is linear in parameters; '
                    'non-linear fitting methods should not be used.')
        model_copy = model.copy()
        init_values, _ = _model_to_fit_params(model_copy)
        self.fitparams = optimize.fmin_slsqp(self.errorfunc, p0=init_values,
                                             args=(y, x),
                                             bounds=self.bounds,
                                             eqcons=self.eqcons,
                                             ineqcons=self.ineqcons)
        return model_copy

Defining a Plugin Fitter
========================

`astropy.modeling` includes a plugin mechanism which allows fitters
defined outside of astropy's core to be inserted into the
`astropy.modeling.fitting` namespace through the use of entry points.
Entry points are references to importable objects. A tutorial on defining
entry points can be found in `setuptools' documentation <https://setuptools.readthedocs.io/en/latest/setuptools.html#dynamic-discovery-of-services-and-plugins>`_.
Plugin fitters must to extend from the `~astropy.modeling.fitting.Fitter`
base class. For the fitter to be discovered and inserted into
`astropy.modeling.fitting` the entry points must be inserted into
the `astropy.modeling` entry point group

.. doctest-skip::

    setup(
          # ...
          entry_points = {'astropy.modeling': 'PluginFitterName = fitter_module:PlugFitterClass'}
    )

This would allow users to import the ``PlugFitterName`` through `astropy.modeling.fitting` by

.. doctest-skip::

    from astropy.modeling.fitting import PlugFitterName

One project which uses this functionality is `Saba <https://saba.readthedocs.io/>`_
and be can be used as a reference.

Using a Custom Statistic Function
=================================

This section describes how to write a new fitter with a user-defined statistic
function.  The example below shows a specialized class which fits a straight
line with uncertainties in both variables.

The following import statements are needed::

    import numpy as np
    from astropy.modeling.fitting import (_validate_model,
                                          _fitter_to_model_params,
                                          _model_to_fit_params, Fitter,
                                          _convert_input)
    from astropy.modeling.optimizers import Simplex

First one needs to define a statistic. This can be a function or a callable
class.::

    def chi_line(measured_vals, updated_model, x_sigma, y_sigma, x):
        """
        Chi^2 statistic for fitting a straight line with uncertainties in x and
        y.

        Parameters
        ----------
        measured_vals : array
        updated_model : `~astropy.modeling.ParametricModel`
            model with parameters set by the current iteration of the optimizer
        x_sigma : array
            uncertainties in x
        y_sigma : array
            uncertainties in y

        """
        model_vals = updated_model(x)
        if x_sigma is None and y_sigma is None:
            return np.sum((model_vals - measured_vals) ** 2)
        elif x_sigma is not None and y_sigma is not None:
            weights = 1 / (y_sigma ** 2 + updated_model.parameters[1] ** 2 *
                           x_sigma ** 2)
            return np.sum((weights * (model_vals - measured_vals)) ** 2)
        else:
            if x_sigma is not None:
                weights = 1 / x_sigma ** 2
            else:
                weights = 1 / y_sigma ** 2
            return np.sum((weights * (model_vals - measured_vals)) ** 2)

In general, to define a new fitter, all one needs to do is provide a statistic
function and an optimizer. In this example we will let the optimizer be an
optional argument to the fitter and will set the statistic to ``chi_line``
above::

    class LineFitter(Fitter):
        """
        Fit a straight line with uncertainties in both variables

        Parameters
        ----------
        optimizer : class or callable
            one of the classes in optimizers.py (default: Simplex)
        """

        def __init__(self, optimizer=Simplex):
            self.statistic = chi_line
            super().__init__(optimizer, statistic=self.statistic)

The last thing to define is the ``__call__`` method::

    def __call__(self, model, x, y, x_sigma=None, y_sigma=None, **kwargs):
        """
        Fit data to this model.

        Parameters
        ----------
        model : `~astropy.modeling.core.ParametricModel`
            model to fit to x, y
        x : array
            input coordinates
        y : array
            input coordinates
        x_sigma : array
            uncertainties in x
        y_sigma : array
            uncertainties in y
        kwargs : dict
            optional keyword arguments to be passed to the optimizer

        Returns
        ------
        model_copy : `~astropy.modeling.core.ParametricModel`
            a copy of the input model with parameters set by the fitter

        """
        model_copy = _validate_model(model,
                                     self._opt_method.supported_constraints)

        farg = _convert_input(x, y)
        farg = (model_copy, x_sigma, y_sigma) + farg
        p0, _ = _model_to_fit_params(model_copy)

        fitparams, self.fit_info = self._opt_method(
            self.objective_function, p0, farg, **kwargs)
        _fitter_to_model_params(model_copy, fitparams)

        return model_copy

.. _astropy-modeling-performance:

Performance Tips
****************

Initializing a compound model with many constituent models can be time consuming.
If your code uses the same compound model repeatedly consider initializing it
once and reusing the model.

Consider the :ref:`performance tips <astropy-units-performance>` that apply to
quantities when initializing and evaluating models with quantities.
.. _example-fitting-model-sets:

Fitting Model Sets
==================

Astropy model sets let you fit the same (linear) model to lots of independent
data sets. It solves the linear equations simultaneously, so can avoid looping.
But getting the data into the right shape can be a bit tricky.

The time savings could be worth the effort. In the example below, if we change
the width*height of the data cube to 500*500 it takes 140 ms on a 2015 MacBook Pro
to fit the models using model sets. Doing the same fit by looping over the 500*500 models
takes 1.5 minutes, more than 600 times slower.

In the example below, we create a 3D data cube where the first dimension is a ramp --
for example as from non-destructive readouts of an IR detector. So each pixel has a
depth along a time axis, and flux that results a total number of counts that is
increasing with time. We will be fitting a 1D polynomial vs. time to estimate the
flux in counts/second (the slope of the fit). We will use just a small image
of 3 rows by 4 columns, with a depth of 10 non-destructive reads.

First, import the necessary libraries:

    >>> import numpy as np
    >>> np.random.seed(seed=12345)
    >>> from astropy.modeling import models, fitting

    >>> depth, width, height = 10, 3, 4  # Time is along the depth axis
    >>> t = np.arange(depth, dtype=np.float64)*10.  # e.g. readouts every 10 seconds

The number of counts in neach pixel is flux*time with the addition of some Gaussian noise::

    >>> fluxes = np.arange(1. * width * height).reshape(width, height)
    >>> image = fluxes[np.newaxis, :, :] * t[:, np.newaxis, np.newaxis]
    >>> image += np.random.normal(0., image*0.05, size=image.shape)  # Add noise
    >>> image.shape
    (10, 3, 4)

Create the models and the fitter. We need N=width*height instances of the same linear,
parametric model (model sets currently only work with linear models and fitters)::

    >>> N = width * height
    >>> line = models.Polynomial1D(degree=1, n_models=N)
    >>> fit = fitting.LinearLSQFitter()
    >>> print(f"We created {len(line)} models")
    We created 12 models

We need to get the data to be fit into the right shape. It's not possible to just feed
the 3D data cube. In this case, the time axis can be one dimensional.
The fluxes have to be organized into an array that is of shape ``width*height,depth`` --  in
other words, we are reshaping to flatten last two axes and transposing to put them first::

    >>> pixels = image.reshape((depth, width*height))
    >>> y = pixels.T
    >>> print("x axis is one dimensional: ",t.shape)
    x axis is one dimensional:  (10,)
    >>> print("y axis is two dimensional, N by len(x): ", y.shape)
    y axis is two dimensional, N by len(x):  (12, 10)

Fit the model. It fits the N models simultaneously::

    >>> new_model = fit(line, x=t, y=y)
    >>> print(f"We fit {len(new_model)} models")
    We fit 12 models

Fill an array with values computed from the best fit and reshape it to match the original::

    >>> best_fit = new_model(t, model_set_axis=False).T.reshape((depth, height, width))
    >>> print("We reshaped the best fit to dimensions: ", best_fit.shape)
    We reshaped the best fit to dimensions:  (10, 4, 3)

Now inspect the model::

    >>> print(new_model) # doctest: +FLOAT_CMP
    Model: Polynomial1D
    Inputs: ('x',)
    Outputs: ('y',)
    Model set size: 12
    Degree: 1
    Parameters:
                 c0                 c1
        ------------------- ------------------
	                0.0                0.0
	-0.5206606340901005 1.0463998276552442
         0.6401930368329991 1.9818733492667582
         0.1134712985541639  3.049279878262541
        -3.3556420351251313  4.013810434122983
          6.782223372575449  4.755912707001437
          3.628220497058842  5.841397947835126
        -5.8828309622531565  7.016044775363114
        -11.676538736037775  8.072519832452022
          -6.17932185981594  9.103924115403503
        -4.7258541419613165 10.315295021908833
           4.95631951675311 10.911167956770575

    >>> print("The new_model has a param_sets attribute with shape: ",new_model.param_sets.shape)
    The new_model has a param_sets attribute with shape:  (2, 12)

    >>> print(f"And values that are the best-fit parameters for each pixel:\n{new_model.param_sets}") # doctest: +FLOAT_CMP
    And values that are the best-fit parameters for each pixel:
    [[  0.          -0.52066063   0.64019304   0.1134713   -3.35564204
        6.78222337   3.6282205   -5.88283096 -11.67653874  -6.17932186
       -4.72585414   4.95631952]
     [  0.           1.04639983   1.98187335   3.04927988   4.01381043
        4.75591271   5.84139795   7.01604478   8.07251983   9.10392412
       10.31529502  10.91116796]]

Plot the fit along a couple of pixels:

    >>> def plotramp(t, image, best_fit, row, col):
    ...     plt.plot(t, image[:, row, col], '.', label=f'data pixel {row},{col}')
    ...     plt.plot(t, best_fit[:, row, col], '-', label=f'fit to pixel {row},{col}')
    ...     plt.xlabel('Time')
    ...     plt.ylabel('Counts')
    ...     plt.legend(loc='upper left')
    >>> fig = plt.figure(figsize=(10, 5)) # doctest: +SKIP
    >>> plotramp(t, image, best_fit, 1, 1) # doctest: +SKIP
    >>> plotramp(t, image, best_fit, 2, 1) # doctest: +SKIP

The data and the best fit model are shown together on one plot.

.. plot::

    import numpy as np
    import matplotlib.pyplot as plt
    from scipy import stats
    from astropy.modeling import models, fitting

    # Set up the shape of the image and create the time axis
    depth,width,height=10,3,4 # Time is along the depth axis
    t = np.arange(depth, dtype=np.float64)*10.  # e.g. readouts every 10 seconds

    # Make up a flux in each pixel
    fluxes = np.arange(1.*width*height).reshape(height, width)
    # Create the ramps by integrating the fluxes along the time steps
    image = fluxes[np.newaxis, :, :] * t[:, np.newaxis, np.newaxis]
    # Add some Gaussian noise to each sample
    image += stats.norm.rvs(0., image*0.05, size=image.shape)  # Add noise

    # Create the models and the fitter
    N = width * height # This is how many instances we need
    line = models.Polynomial1D(degree=1, n_models=N)
    fit = fitting.LinearLSQFitter()

    # We need to get the data to be fit into the right shape
    # In this case, the time axis can be one dimensional.
    # The fluxes have to be organized into an array
    # that is of shape `(width*height, depth)`
    # i.e we are reshaping to flatten last two axes and
    # transposing to put them first.
    pixels = image.reshape((depth, width*height))
    y = pixels.T

    # Fit the model. It does the looping over the N models implicitly
    new_model = fit(line, x=t, y=y)

    # Fill an array with values computed from the best fit and reshape it to match the original
    best_fit = new_model(t, model_set_axis=False).T.reshape((depth, height, width))


    # Plot the fit along a couple of pixels
    def plotramp(t, image, best_fit, row, col):
        plt.plot(t, image[:, row, col], '.', label=f'data pixel {row},{col}')
        plt.plot(t, best_fit[:, row, col], '-', label=f'fit to pixel {row},{col}')
        plt.xlabel('Time')
        plt.ylabel('Counts')
        plt.legend(loc='upper left')


    plt.figure(figsize=(10, 5))
    plotramp(t, image, best_fit, 1, 1)
    plotramp(t, image, best_fit, 3, 2)
    plt.show()
.. include:: links.inc

.. _modeling-parameters:

**********
Parameters
**********

Basics
======

Most models in this package are "parametric" in the sense that each subclass
of `~astropy.modeling.Model` represents an entire family of models, each
member of which is distinguished by a fixed set of parameters that fit that
model to some dependent and independent variable(s) (also referred to
throughout the package as the outputs and inputs of the model).

Parameters are used in three different contexts within this package: Basic
evaluation of models, fitting models to data, and providing information about
individual models to users (including documentation).

Most subclasses of `~astropy.modeling.Model`--specifically those implementing a
specific physical or statistical model, have a fixed set of parameters that can
be specified for instances of that model.  There are a few classes of models
(in particular polynomials) in which the number of parameters depends on some
other property of the model (the degree in the case of polynomials).

Models maintain a list of parameter names,
`~astropy.modeling.Model.param_names`.  Single parameters are instances of
`~astropy.modeling.Parameter` which provides a proxy for the actual parameter
values.  Simple mathematical operations can be performed with them, but they
also contain additional attributes specific to model parameters, such as any
constraints on their values and documentation.

Parameter values may be scalars *or* array values.  Some parameters are
required by their very nature to be arrays (such as the transformation matrix
for an `~astropy.modeling.projections.AffineTransformation2D`).  In most other
cases, however, array-valued parameters have no meaning specific to the model,
and are simply combined with input arrays during model evaluation according to
the standard `Numpy broadcasting rules`_.

Parameter constraints
=====================

`astropy.modeling` supports several types of parameter constraints. They are implemented
as properties of `~astropy.modeling.Parameter`, the class which defines all fittable
parameters, and can be set on individual parameters or on model instances.

The `astropy.modeling.Parameter.fixed` constraint is boolean and indicates
whether a parameter is kept "fixed" or "frozen" during fitting. For example, fixing the
``stddev`` of a :class:`~astropy.modeling.functional_models.Gaussian1D` model
means it will be excluded from the list of fitted parameters::

    >>> from astropy.modeling.models import Gaussian1D
    >>> g = Gaussian1D(amplitude=10.2, mean=2.3, stddev=1.2)
    >>> g.stddev.fixed
    False
    >>> g.stddev.fixed = True
    >>> g.stddev.fixed
    True

`astropy.modeling.Parameter.bounds` is a tuple of numbers
setting minimum and maximum value for a parameter. ``(None, None)`` indicates
the parameter values are not bound. ``bounds`` can be set also using the
`~astropy.modeling.Parameter.min` and
`~astropy.modeling.Parameter.max` properties. Assigning ``None`` to
the corresponding property removes the bound on the parameter. For example, setting
bounds on the ``mean`` value of a :class:`~astropy.modeling.functional_models.Gaussian1D`
model can be done either by setting ``min`` and ``max``::

    >>> g.mean.bounds
    (None, None)
    >>> g.mean.min = 2.2
    >>> g.mean.bounds
    (2.2, None)
    >>> g.mean.max = 2.4
    >>> g.mean.bounds
    (2.2, 2.4)

or using the ``bounds`` property::

    >>> g.mean.bounds = (2.2, 2.4)

`astropy.modeling.Parameter.tied` is a user supplied callable
which takes a model instance and returns a value for the parameter.  It is most useful
with setting constraints on compounds models, for example a ratio between two parameters (:ref:`example<tied>`).

Constraints can also be set when the model is initialized. For example::

    >>> g = Gaussian1D(amplitude=10.2, mean=2.3, stddev=1.2,
    ...                fixed={'stddev': True},
    ... 	        bounds={'mean': (2.2, 2.4)})
    >>> g.stddev.fixed
    True
    >>> g.mean.bounds
    (2.2, 2.4)


Parameter examples
==================

- Model classes can be introspected directly to find out what parameters they
  accept::

      >>> from astropy.modeling import models
      >>> models.Gaussian1D.param_names
      ('amplitude', 'mean', 'stddev')

  The order of the items in the ``param_names`` list is relevant--this
  is the same order in which values for those parameters should be passed in
  when constructing an instance of that model::

      >>> g = models.Gaussian1D(1.0, 0.0, 0.1)
      >>> g  # doctest: +FLOAT_CMP
      <Gaussian1D(amplitude=1.0, mean=0.0, stddev=0.1)>

  However, parameters may also be given as keyword arguments (in any order)::

      >>> g = models.Gaussian1D(mean=0.0, amplitude=2.0, stddev=0.2)
      >>> g  # doctest: +FLOAT_CMP
      <Gaussian1D(amplitude=2.0, mean=0.0, stddev=0.2)>

  So all that really matters is knowing the names (and meanings) of the
  parameters that each model accepts.  More information about an individual
  model can also be obtained using the `help` built-in::

      >>> help(models.Gaussian1D)  # doctest: +SKIP

- Some types of models can have different numbers of parameters depending
  on other properties of the model.  In particular, the parameters of
  polynomial models are their coefficients, the number of which depends on the
  polynomial's degree::

      >>> p1 = models.Polynomial1D(degree=3, c0=1.0, c1=0.0, c2=2.0, c3=3.0)
      >>> p1.param_names
      ('c0', 'c1', 'c2', 'c3')
      >>> p1  # doctest: +FLOAT_CMP
      <Polynomial1D(3, c0=1., c1=0., c2=2., c3=3.)>

  For the basic `~astropy.modeling.polynomial.Polynomial1D` class the
  parameters are named ``c0`` through ``cN`` where ``N`` is the degree of the
  polynomial.  The above example represents the polynomial :math:`3x^3 + 2x^2 +
  1`.

- Some models also have default values for one or more of their parameters.
  For polynomial models, for example, the default value of all coefficients is
  zero--this allows a polynomial instance to be created without specifying any
  of the coefficients initially::

      >>> p2 = models.Polynomial1D(degree=4)
      >>> p2  # doctest: +FLOAT_CMP
      <Polynomial1D(4, c0=0., c1=0., c2=0., c3=0., c4=0.)>

- Parameters can then be set/updated by accessing attributes on the model of
  the same names as the parameters::

      >>> p2.c4 = 1
      >>> p2.c2 = 3.5
      >>> p2.c0 = 2.0
      >>> p2  # doctest: +FLOAT_CMP
      <Polynomial1D(4, c0=2., c1=0., c2=3.5, c3=0., c4=1.)>

  This example now represents the polynomial :math:`x^4 + 3.5x^2 + 2`.

- It is possible to set the coefficients of a polynomial by passing the
  parameters in a dictionary, since all parameters can be provided as keyword
  arguments::

      >>> ch2 = models.Chebyshev2D(x_degree=2, y_degree=3)
      >>> coeffs = dict((name, [idx, idx + 10])
      ...               for idx, name in enumerate(ch2.param_names))
      >>> ch2 = models.Chebyshev2D(x_degree=2, y_degree=3, n_models=2,
      ...                          **coeffs)
      >>> ch2.param_sets  # doctest: +FLOAT_CMP
      array([[ 0., 10.],
             [ 1., 11.],
             [ 2., 12.],
             [ 3., 13.],
             [ 4., 14.],
             [ 5., 15.],
             [ 6., 16.],
             [ 7., 17.],
             [ 8., 18.],
             [ 9., 19.],
             [10., 20.],
             [11., 21.]])

- Or directly, using keyword arguments::

      >>> ch2 = models.Chebyshev2D(x_degree=2, y_degree=3,
      ...                          c0_0=[0, 10], c0_1=[3, 13],
      ...                          c0_2=[6, 16], c0_3=[9, 19],
      ...                          c1_0=[1, 11], c1_1=[4, 14],
      ...                          c1_2=[7, 17], c1_3=[10, 20,],
      ...                          c2_0=[2, 12], c2_1=[5, 15],
      ...                          c2_2=[8, 18], c2_3=[11, 21])

- Individual parameters values may be arrays of different sizes and shapes::

      >>> p3 = models.Polynomial1D(degree=2, c0=1.0, c1=[2.0, 3.0],
      ...                          c2=[[4.0, 5.0], [6.0, 7.0], [8.0, 9.0]])
      >>> p3(2.0)  # doctest: +FLOAT_CMP
      array([[21., 27.],
             [29., 35.],
             [37., 43.]])

  This is equivalent to evaluating the Numpy expression::

      >>> import numpy as np
      >>> c2 = np.array([[4.0, 5.0],
      ...                [6.0, 7.0],
      ...                [8.0, 9.0]])
      >>> c1 = np.array([2.0, 3.0])
      >>> c2 * 2.0**2 + c1 * 2.0 + 1.0  # doctest: +FLOAT_CMP
      array([[21., 27.],
             [29., 35.],
             [37., 43.]])

  Note that in most cases, when using array-valued parameters, the parameters
  must obey the standard broadcasting rules for Numpy arrays with respect to
  each other::

      >>> models.Polynomial1D(degree=2, c0=1.0, c1=[2.0, 3.0],
      ...                     c2=[4.0, 5.0, 6.0])  # doctest: +IGNORE_EXCEPTION_DETAIL
      Traceback (most recent call last):
      ...
      InputParameterError: Parameter u'c1' of shape (2,) cannot be broadcast
      with parameter u'c2' of shape (3,).  All parameter arrays must have
      shapes that are mutually compatible according to the broadcasting rules.
Reference/API
=============

Capabilities
************

.. automodapi:: astropy.modeling
.. automodapi:: astropy.modeling.bounding_box
.. automodapi:: astropy.modeling.mappings
.. automodapi:: astropy.modeling.fitting
.. automodapi:: astropy.modeling.optimizers
.. automodapi:: astropy.modeling.statistic
.. automodapi:: astropy.modeling.separable

Pre-Defined Models
******************

.. automodapi:: astropy.modeling.functional_models
.. automodapi:: astropy.modeling.physical_models
.. automodapi:: astropy.modeling.powerlaws
.. automodapi:: astropy.modeling.polynomial
.. automodapi:: astropy.modeling.projections
.. automodapi:: astropy.modeling.rotations
.. automodapi:: astropy.modeling.spline
.. automodapi:: astropy.modeling.tabular
.. include:: links.inc

.. _polynomial_models:

*****************
Polynomial Models
*****************

.. _domain-window-note:

Notes regarding usage of domain and window
------------------------------------------

Most of the polynomial models have optional domain and window attributes.
It is important to understand how they currently are interpreted, which
can be confusing since the terminology often implies something different.

Both the domain and window attributes for a polynomial consist of a two
element list (this will change to tuples in a future release) that
indicate a range of values for input values. For 2-Dimensional polynomials
the attributes become x_domain, y_domain, x_window, and y_window.
Generally speaking, the main purpose of these attributes is to define
a linear transform between the supplied input variable and the resultant
input variable that is supplied to the polynomial. For example, if
domain = [-2, 2] and window = [-1, 1], input values will be divided by
two so that the domain maps to the window. Correspondingly the pair
domain = [0, 2], window = [-1, 1] implies that 1 will be subtracted from
the input variable before using it in the polynomial.

Neither domain or window are meant to imply that values that fall outside
of their corresponding ranges will result in an exception, or that
such values are necessarily invalid (the latter depends on the context
of how the polynomial is being used).

It is the case that the orthogonal polynomials are defined on a range of
[-1, 1], but nothing in the current machinery prevents them from being
evaluated outside that range.

Domain is used in fitting polynomials to bound the input variable to map
to the defined window so that they fall within the expected [-1, 1] range
for such polynomials. That is, the fitting routine will set the domain to
map to the window range for the range of input x values supplied (so that
domain may change if the minimum and maximum x values being fit change).

The meaning of these terms may conflict with expectations (e.g., domain
is often meant to mean the range of input values the function is valid
for). For fit results that is somewhat true, but otherwise, it is not.
The default values for ordinary polynomials is [-1, 1] for both domain
and window, which effectively signals no transformation of the input
variable.

The terminology was adopted from numpy polynomials, which have the same
confusion in meaning.


**************
1D Polynomials
**************

- :class:`~astropy.modeling.polynomial.Polynomial1D`

- :class:`~astropy.modeling.polynomial.Chebyshev1D`

- :class:`~astropy.modeling.polynomial.Legendre1D`

- :class:`~astropy.modeling.polynomial.Hermite1D`

**************
2D Polynomials
**************

- :class:`~astropy.modeling.polynomial.Polynomial2D`

- :class:`~astropy.modeling.polynomial.Chebyshev2D`

- :class:`~astropy.modeling.polynomial.Legendre2D`

- :class:`~astropy.modeling.polynomial.Hermite2D`

- :class:`~astropy.modeling.polynomial.SIP` model implements the
  Simple Imaging Polynomial (`SIP`_) convention
**********************
Fitting Models to Data
**********************

This module provides wrappers, called Fitters, around some Numpy and Scipy
fitting functions. All Fitters can be called as functions. They take an
instance of `~astropy.modeling.FittableModel` as input and modify its
``parameters`` attribute. The idea is to make this extensible and allow
users to easily add other fitters.

Linear fitting is done using Numpy's `numpy.linalg.lstsq` function.  There are
currently two non-linear fitters which use `scipy.optimize.leastsq` and
`scipy.optimize.fmin_slsqp`.

The rules for passing input to fitters are:

* Non-linear fitters currently work only with single models (not model sets).

* The linear fitter can fit a single input to multiple model sets creating
  multiple fitted models.  This may require specifying the ``model_set_axis``
  argument just as used when evaluating models; this may be required for the
  fitter to know how to broadcast the input data.

* The `~astropy.modeling.fitting.LinearLSQFitter` currently works only with
  simple (not compound) models.

* The current fitters work only with models that have a single output
  (including bivariate functions such as
  `~astropy.modeling.polynomial.Chebyshev2D` but not compound models that map
  ``x, y -> x', y'``).

.. _modeling-getting-started-1d-fitting:

Simple 1-D model fitting
------------------------

In this section, we look at a simple example of fitting a Gaussian to a
simulated dataset. We use the `~astropy.modeling.functional_models.Gaussian1D`
and `~astropy.modeling.functional_models.Trapezoid1D` models and the
`~astropy.modeling.fitting.LevMarLSQFitter` fitter to fit the data:

.. plot::
   :include-source:

    import numpy as np
    import matplotlib.pyplot as plt
    from astropy.modeling import models, fitting

    # Generate fake data
    np.random.seed(0)
    x = np.linspace(-5., 5., 200)
    y = 3 * np.exp(-0.5 * (x - 1.3)**2 / 0.8**2)
    y += np.random.normal(0., 0.2, x.shape)

    # Fit the data using a box model.
    # Bounds are not really needed but included here to demonstrate usage.
    t_init = models.Trapezoid1D(amplitude=1., x_0=0., width=1., slope=0.5,
                                bounds={"x_0": (-5., 5.)})
    fit_t = fitting.LevMarLSQFitter()
    t = fit_t(t_init, x, y)

    # Fit the data using a Gaussian
    g_init = models.Gaussian1D(amplitude=1., mean=0, stddev=1.)
    fit_g = fitting.LevMarLSQFitter()
    g = fit_g(g_init, x, y)

    # Plot the data with the best-fit model
    plt.figure(figsize=(8,5))
    plt.plot(x, y, 'ko')
    plt.plot(x, t(x), label='Trapezoid')
    plt.plot(x, g(x), label='Gaussian')
    plt.xlabel('Position')
    plt.ylabel('Flux')
    plt.legend(loc=2)

As shown above, once instantiated, the fitter class can be used as a function
that takes the initial model (``t_init`` or ``g_init``) and the data values
(``x`` and ``y``), and returns a fitted model (``t`` or ``g``).

.. _modeling-getting-started-2d-fitting:

Simple 2-D model fitting
------------------------

Similarly to the 1-D example, we can create a simulated 2-D data dataset, and
fit a polynomial model to it.  This could be used for example to fit the
background in an image.

.. plot::
   :include-source:

    import warnings
    import numpy as np
    import matplotlib.pyplot as plt
    from astropy.modeling import models, fitting
    from astropy.utils.exceptions import AstropyUserWarning

    # Generate fake data
    np.random.seed(0)
    y, x = np.mgrid[:128, :128]
    z = 2. * x ** 2 - 0.5 * x ** 2 + 1.5 * x * y - 1.
    z += np.random.normal(0., 0.1, z.shape) * 50000.

    # Fit the data using astropy.modeling
    p_init = models.Polynomial2D(degree=2)
    fit_p = fitting.LevMarLSQFitter()

    with warnings.catch_warnings():
        # Ignore model linearity warning from the fitter
        warnings.filterwarnings('ignore', message='Model is linear in parameters',
                                category=AstropyUserWarning)
        p = fit_p(p_init, x, y, z)

    # Plot the data with the best-fit model
    plt.figure(figsize=(8, 2.5))
    plt.subplot(1, 3, 1)
    plt.imshow(z, origin='lower', interpolation='nearest', vmin=-1e4, vmax=5e4)
    plt.title("Data")
    plt.subplot(1, 3, 2)
    plt.imshow(p(x, y), origin='lower', interpolation='nearest', vmin=-1e4,
               vmax=5e4)
    plt.title("Model")
    plt.subplot(1, 3, 3)
    plt.imshow(z - p(x, y), origin='lower', interpolation='nearest', vmin=-1e4,
               vmax=5e4)
    plt.title("Residual")
.. _example_fitting_line:

Fitting a Line
==============

Fitting a line to (x,y) data points is a common case in many areas.
Examples fits are given for fitting, fitting using the uncertainties
as weights, and fitting using iterative sigma clipping.

Simple Fit
----------

Here the (x,y) data points are fit with a line.  The (x,y) data
points are simulated and have a range of uncertainties to give
a realistic example.

.. plot::
    :include-source:

    import numpy as np
    import matplotlib.pyplot as plt
    from astropy.modeling import models, fitting

    # define a model for a line
    line_orig = models.Linear1D(slope=1.0, intercept=0.5)

    # generate x, y data non-uniformly spaced in x
    # add noise to y measurements
    npts = 30
    np.random.seed(10)
    x = np.random.uniform(0.0, 10.0, npts)
    y = line_orig(x)
    yunc = np.absolute(np.random.normal(0.5, 2.5, npts))
    y += np.random.normal(0.0, yunc, npts)

    # initialize a linear fitter
    fit = fitting.LinearLSQFitter()

    # initialize a linear model
    line_init = models.Linear1D()

    # fit the data with the fitter
    fitted_line = fit(line_init, x, y)

    # plot
    plt.figure()
    plt.plot(x, y, 'ko', label='Data')
    plt.plot(x, line_orig(x), 'b-', label='Simulation Model')
    plt.plot(x, fitted_line(x), 'k-', label='Fitted Model')
    plt.xlabel('x')
    plt.ylabel('y')
    plt.legend()

Fit using uncertainties
-----------------------

Fitting can be done using the uncertainties as weights.
To get the standard weighting of 1/unc^2 for the case of
Gaussian errors, the weights to pass to the fitting are 1/unc.

.. plot::
    :include-source:

    import numpy as np
    import matplotlib.pyplot as plt
    from astropy.modeling import models, fitting

    # define a model for a line
    line_orig = models.Linear1D(slope=1.0, intercept=0.5)

    # generate x, y data non-uniformly spaced in x
    # add noise to y measurements
    npts = 30
    np.random.seed(10)
    x = np.random.uniform(0.0, 10.0, npts)
    y = line_orig(x)
    yunc = np.absolute(np.random.normal(0.5, 2.5, npts))
    y += np.random.normal(0.0, yunc, npts)

    # initialize a linear fitter
    fit = fitting.LinearLSQFitter()

    # initialize a linear model
    line_init = models.Linear1D()

    # fit the data with the fitter
    fitted_line = fit(line_init, x, y, weights=1.0/yunc)

    # plot
    plt.figure()
    plt.errorbar(x, y, yerr=yunc, fmt='ko', label='Data')
    plt.plot(x, line_orig(x), 'b-', label='Simulation Model')
    plt.plot(x, fitted_line(x), 'k-', label='Fitted Model')
    plt.xlabel('x')
    plt.ylabel('y')
    plt.legend()

Iterative fitting using sigma clipping
--------------------------------------

When fitting, there may be data that are outliers from the fit
that can significantly bias the fitting.  These outliers can
be identified and removed from the fitting iteratively.
Note that the iterative sigma clipping assumes all the data
have the same uncertainties for the sigma clipping decision.

.. plot::
    :include-source:

    import numpy as np
    import matplotlib.pyplot as plt
    from astropy.stats import sigma_clip
    from astropy.modeling import models, fitting

    # define a model for a line
    line_orig = models.Linear1D(slope=1.0, intercept=0.5)

    # generate x, y data non-uniformly spaced in x
    # add noise to y measurements
    npts = 30
    np.random.seed(10)
    x = np.random.uniform(0.0, 10.0, npts)
    y = line_orig(x)
    yunc = np.absolute(np.random.normal(0.5, 2.5, npts))
    y += np.random.normal(0.0, yunc, npts)

    # make true outliers
    y[3] = line_orig(x[3]) + 6 * yunc[3]
    y[10] = line_orig(x[10]) - 4 * yunc[10]

    # initialize a linear fitter
    fit = fitting.LinearLSQFitter()

    # initialize the outlier removal fitter
    or_fit = fitting.FittingWithOutlierRemoval(fit, sigma_clip, niter=3, sigma=3.0)

    # initialize a linear model
    line_init = models.Linear1D()

    # fit the data with the fitter
    fitted_line, mask = or_fit(line_init, x, y, weights=1.0/yunc)
    filtered_data = np.ma.masked_array(y, mask=mask)

    # plot
    plt.figure()
    plt.errorbar(x, y, yerr=yunc, fmt="ko", fillstyle="none", label="Clipped Data")
    plt.plot(x, filtered_data, "ko", label="Fitted Data")
    plt.plot(x, line_orig(x), 'b-', label='Simulation Model')
    plt.plot(x, fitted_line(x), 'k-', label='Fitted Model')
    plt.xlabel('x')
    plt.ylabel('y')
    plt.legend()
.. include:: links.inc

.. _spline_models:

****************
1D Spline Models
****************

`~astropy.modeling.spline.Spline1D` models are models which can be used
to fit a piecewise polynomial to a set of data. This means that splines
are closely tied to the method used to fit the spline to the data. Currently,
we provide three methods for fitting splines to data:

- :class:`~astropy.modeling.spline.SplineInterpolateFitter`, which
  fits an interpolating spline to the data. This means that the spline
  will exactly fit all data points.

- :class:`~astropy.modeling.spline.SplineSmoothingFitter`, which fits
  a smoothing spline to the data. This means that the number of knots
  is chosen to satisfy the "smoothing condition":

  .. math:: \sum_{i} \left(w_i * (y_i - spl(x_i))\right)^{2} \leq s

- :class:`~astropy.modeling.spline.SplineExactKnotsFitter`, which fits
  a spline to the data using an exact set of knots. This means that the
  spline will use least-squares regression using the user supplied (interior)
  knots to find the best fit spline to the data.

.. plot::
    :include-source:

    import numpy as np
    import matplotlib.pyplot as plt
    from astropy.modeling.models import Spline1D
    from astropy.modeling.fitting import (SplineInterpolateFitter,
                                          SplineSmoothingFitter,
                                          SplineExactKnotsFitter)

    x = np.linspace(-3, 3, 50)
    y = np.exp(-x**2) + 0.1 * np.random.randn(50)
    xs = np.linspace(-3, 3, 1000)
    t = [-1, 0, 1]
    spl = Spline1D()

    fitter = SplineInterpolateFitter()
    spl1 = fitter(spl, x, y)

    fitter = SplineSmoothingFitter()
    spl2 = fitter(spl, x, y, s=0.5)

    fitter = SplineExactKnotsFitter()
    spl3 = fitter(spl, x, y, t=t)

    plt.plot(x, y, 'ro', label="Data")
    plt.plot(xs, spl1(xs), 'b-', label="Interpolating")
    plt.plot(xs, spl2(xs), 'g-', label="Smoothing")
    plt.plot(xs, spl3(xs), 'k-', label="Exact Knots")
    plt.legend()
    plt.show()

Note that by default, splines have `degree <astropy.modeling.spline.Spline1D.degree>` 3.
In the case of these splines, the ``degree - 1`` is the number of derivatives that
are matched by the spline across knot points. So for degree 3 splines, the value,
first, and second derivatives of the spline will match across each knot point.

.. warning::

    Splines only support integer degrees, such that ``1 <= degree <= 5``.
.. include:: links.inc

.. _models:

******
MODELS
******

.. _basics-models:

Basics
======

The `astropy.modeling` package defines a number of models that are collected
under a single namespace as ``astropy.modeling.models``.  Models behave like
parametrized functions::

    >>> import numpy as np
    >>> from astropy.modeling import models
    >>> g = models.Gaussian1D(amplitude=1.2, mean=0.9, stddev=0.5)
    >>> print(g)
    Model: Gaussian1D
    Inputs: ('x',)
    Outputs: ('y',)
    Model set size: 1
    Parameters:
        amplitude mean stddev
        --------- ---- ------
              1.2  0.9    0.5

Model parameters can be accessed as attributes::

    >>> g.amplitude
    Parameter('amplitude', value=1.2)
    >>> g.mean
    Parameter('mean', value=0.9)
    >>> g.stddev  # doctest: +FLOAT_CMP
    Parameter('stddev', value=0.5, bounds=(1.1754943508222875e-38, None))

and can also be updated via those attributes::

    >>> g.amplitude = 0.8
    >>> g.amplitude
    Parameter('amplitude', value=0.8)

Models can be evaluated by calling them as functions::

    >>> g(0.1)
    0.22242984036255528
    >>> g(np.linspace(0.5, 1.5, 7))  # doctest: +FLOAT_CMP
    array([0.58091923, 0.71746405, 0.7929204 , 0.78415894, 0.69394278,
           0.54952605, 0.3894018 ])

As the above example demonstrates, in general most models evaluate array-like
inputs according to the standard `Numpy broadcasting rules`_ for arrays.
Models can therefore already be useful to evaluate common functions,
independently of the fitting features of the package.

.. _modeling-instantiating:


Instantiating and Evaluating Models
-----------------------------------

In general, models are instantiated by supplying the parameter values that
define that instance of the model to the constructor, as demonstrated in
the section on :ref:`modeling-parameters`.

Additionally, a `~astropy.modeling.Model` instance may represent a single model
with one set of parameters, or a :ref:`Model set <modeling-model-sets>` consisting
of a set of parameters each representing a different parameterization of the same
parametric model. For example, you may instantiate a single Gaussian model
with one mean, standard deviation, and amplitude. Or you may create a set
of N Gaussians, each one of which would be evaluated on, for example, a
different plane in an image cube.

For example, a single Gaussian model may be instantiated with all scalar parameters::

    >>> from astropy.modeling.models import Gaussian1D
    >>> g = Gaussian1D(amplitude=1, mean=0, stddev=1)
    >>> g  # doctest: +FLOAT_CMP
    <Gaussian1D(amplitude=1., mean=0., stddev=1.)>

The newly created model instance ``g`` now works like a Gaussian function
with the specific parameters.  It takes a single input::

    >>> g.inputs
    ('x',)
    >>> g(x=0)
    1.0

The model can also be called without explicitly using keyword arguments::

    >>> g(0)
    1.0

Or a set of Gaussians may be instantiated by passing multiple parameter values::

    >>> from astropy.modeling.models import Gaussian1D
    >>> gset = Gaussian1D(amplitude=[1, 1.5, 2],
    ...                   mean=[0, 1, 2],
    ...                   stddev=[1., 1., 1.],
    ...                   n_models=3)
    >>> print(gset)  # doctest: +FLOAT_CMP
    Model: Gaussian1D
    Inputs: ('x',)
    Outputs: ('y',)
    Model set size: 3
    Parameters:
        amplitude mean stddev
        --------- ---- ------
              1.0  0.0    1.0
              1.5  1.0    1.0
              2.0  2.0    1.0

This model also works like a Gaussian function. The three models in
the model set can be evaluated on the same input::

    >>> gset(1.)
    array([0.60653066, 1.5       , 1.21306132])

or on ``N=3`` inputs::

    >>> gset([1, 2, 3])
    array([0.60653066, 0.90979599, 1.21306132])

For a comprehensive example of fitting a model set see :ref:`example-fitting-model-sets`.

Model inverses
--------------

All models have a `Model.inverse <astropy.modeling.Model.inverse>` property
which may, for some models, return a new model that is the analytic inverse of
the model it is attached to.  For example::

    >>> from astropy.modeling.models import Linear1D
    >>> linear = Linear1D(slope=0.8, intercept=1.0)
    >>> linear.inverse
    <Linear1D(slope=1.25, intercept=-1.25)>

The inverse of a model will always be a fully instantiated model in its own
right, and so can be evaluated directly like::

    >>> linear.inverse(2.0)
    1.25

It is also possible to assign a *custom* inverse to a model.  This may be
useful, for example, in cases where a model does not have an analytic inverse,
but may have an approximate inverse that was computed numerically and is
represented by another model. This works even if the target model has a
default analytic inverse--in this case the default is overridden with the
custom inverse::

    >>> from astropy.modeling.models import Polynomial1D
    >>> linear.inverse = Polynomial1D(degree=1, c0=-1.25, c1=1.25)
    >>> linear.inverse
    <Polynomial1D(1, c0=-1.25, c1=1.25)>

If a custom inverse has been assigned to a model, it can be deleted with
``del model.inverse``.  This resets the inverse to its default (if one exists).
If a default does not exist, accessing ``model.inverse`` raises a
`NotImplementedError`.  For example polynomial models do not have a default
inverse::

    >>> del linear.inverse
    >>> linear.inverse
    <Linear1D(slope=1.25, intercept=-1.25)>
    >>> p = Polynomial1D(degree=2, c0=1.0, c1=2.0, c2=3.0)
    >>> p.inverse
    Traceback (most recent call last):
      File "<stdin>", line 1, in <module>
      File "astropy\modeling\core.py", line 796, in inverse
        raise NotImplementedError("An analytical inverse transform has not "
    NotImplementedError: No analytical or user-supplied inverse transform
    has been implemented for this model.

One may certainly compute an inverse and assign it to a polynomial model
though.

.. note::

    When assigning a custom inverse to a model no validation is performed to
    ensure that it is actually an inverse or even approximate inverse.  So
    assign custom inverses at your own risk.

Bounding Boxes
--------------

.. _bounding-boxes:

Efficient Model Rendering with Bounding Boxes
+++++++++++++++++++++++++++++++++++++++++++++


All `Model <astropy.modeling.Model>` subclasses have a
`bounding_box <astropy.modeling.Model.bounding_box>` attribute that
can be used to set the limits over which the model is significant. This greatly
improves the efficiency of evaluation when the input range is much larger than
the characteristic width of the model itself. For example, to create a sky model
image from a large survey catalog, each source should only be evaluated over the
pixels to which it contributes a significant amount of flux. This task can
otherwise be computationally prohibitive on an average CPU.

The :func:`Model.render <astropy.modeling.Model.render>` method can be used to
evaluate a model on an output array, or input coordinate arrays, limiting the
evaluation to the `bounding_box <astropy.modeling.Model.bounding_box>` region if
it is set. This function will also produce postage stamp images of the model if
no other input array is passed. To instead extract postage stamps from the data
array itself, see :ref:`cutout_images`.

Using the standard Bounding Box
+++++++++++++++++++++++++++++++

For basic usage, see `Model.bounding_box <astropy.modeling.Model.bounding_box>`.
By default no `~astropy.modeling.Model.bounding_box` is set, except on model
subclasses where a ``bounding_box`` property or method is explicitly defined.
The default is then the minimum rectangular region symmetric about the position
that fully contains the model. If the model does not have a finite extent,
the containment criteria are noted in the documentation. For example, see
``Gaussian2D.bounding_box``.

.. warning::

    Accessing the `Model.bounding_box <astropy.modeling.Model.bounding_box>`
    property when it has not been set, or does not have a default will
    result in a ``NotImplementedError``. If this behavior is undesireable,
    then one can instead use the `Model.get_bounding_box <astropy.modeling.Model.get_bounding_box>`
    method instead. This method will return the bounding box if one exists
    (by setting or default) otherwise it will return ``None`` instead
    of raising an error.

A `Model.bounding_box <astropy.modeling.Model.bounding_box>` default can be
set by the user to any callable. This is particularly useful for models created
with `~astropy.modeling.custom_model` or as a `~astropy.modeling.core.CompoundModel`::

    >>> from astropy.modeling import custom_model
    >>> def ellipsoid(x, y, z, x0=0, y0=0, z0=0, a=2, b=3, c=4, amp=1):
    ...     rsq = ((x - x0) / a) ** 2 + ((y - y0) / b) ** 2 + ((z - z0) / c) ** 2
    ...     val = (rsq < 1) * amp
    ...     return val
    ...
    >>> class Ellipsoid3D(custom_model(ellipsoid)):
    ...     # A 3D ellipsoid model
    ...     def bounding_box(self):
    ...         return ((self.z0 - self.c, self.z0 + self.c),
    ...                 (self.y0 - self.b, self.y0 + self.b),
    ...                 (self.x0 - self.a, self.x0 + self.a))
    ...
    >>> model1 = Ellipsoid3D()
    >>> model1.bounding_box
    ModelBoundingBox(
        intervals={
            x0: Interval(lower=-2.0, upper=2.0)
            x1: Interval(lower=-3.0, upper=3.0)
            x2: Interval(lower=-4.0, upper=4.0)
        }
        model=Ellipsoid3D(inputs=('x0', 'x1', 'x2'))
        order='C'
    )

By default models are evaluated on any inputs. By passing a flag they can be evaluated
only on inputs within the bounding box. For inputs outside of the bounding_box a ``fill_value`` is
returned (``np.nan`` by default)::

    >>> model1(-5, 1, 1)
    0.0
    >>> model1(-5, 1, 1, with_bounding_box=True)
    nan
    >>> model1(-5, 1, 1, with_bounding_box=True, fill_value=-1)
    -1.0

`Model.bounding_box <astropy.modeling.Model.bounding_box>` can be set on any
model instance via the usage of the property setter. For example for a single
input model one needs to only set a tuple of the lower and upper bounds ::

    >>> from astropy.modeling.models import Polynomial1D
    >>> model2 = Polynomial1D(2)
    >>> model2.bounding_box = (-1, 1)
    >>> model2.bounding_box
    ModelBoundingBox(
        intervals={
            x: Interval(lower=-1, upper=1)
        }
        model=Polynomial1D(inputs=('x',))
        order='C'
    )
    >>> model2(-2)
    0.0
    >>> model2(-2, with_bounding_box=True)
    nan
    >>> model2(-2, with_bounding_box=True, fill_value=47)
    47.0

For multi-input models, `Model.bounding_box <astropy.modeling.Model.bounding_box>`
can be set on any model instance by specifying a tuple of lower/upper bound tuples ::

    >>> from astropy.modeling.models import Polynomial2D
    >>> model3 = Polynomial2D(2)
    >>> model3.bounding_box = ((-2, 2), (-1, 1))
    >>> model3.bounding_box
    ModelBoundingBox(
        intervals={
            x: Interval(lower=-1, upper=1)
            y: Interval(lower=-2, upper=2)
        }
        model=Polynomial2D(inputs=('x', 'y'))
        order='C'
    )
    >>> model3(-2, 0)
    0.0
    >>> model3(-2, 0, with_bounding_box=True)
    nan
    >>> model3(-2, 0, with_bounding_box=True, fill_value=7)
    7.0

Note that if one wants to directly recover the tuple used to formulate
a bounding box, then one can use the
`ModelBoundingBox.bounding_box() <astropy.modeling.bounding_box.ModelBoundingBox.bounding_box>`
method ::

    >>> model1.bounding_box.bounding_box()
    ((-4.0, 4.0), (-3.0, 3.0), (-2.0, 2.0))
    >>> model2.bounding_box.bounding_box()
    (-1, 1)
    >>> model3.bounding_box.bounding_box()
    ((-2, 2), (-1, 1))

.. warning::

    When setting multi-dimensional bounding boxes it is important to
    remember that by default the tuple of tuples is assumed to be ``'C'`` ordered,
    which means that the bound tuples will be ordered in the reverse order
    to their respective input order. That is if the inputs are in the order
    ``('x', 'y', 'z')`` then the bounds will need to be listed in ``('z', 'y', 'x')``
    order.

The if one does not want to work directly with the default ``'C'`` ordered
bounding boxes. It is possible to use the alternate ``'F'`` ordering, which
orders the bounding box tuple in the same order as the inputs. To do this
one can use the `bind_bounding_box <astropy.modeling.bind_bounding_box>`
function, and passing the ``order='F'`` keyword argument ::

    >>> from astropy.modeling import bind_bounding_box
    >>> model4 = Polynomial2D(3)
    >>> bind_bounding_box(model4, ((-1, 1), (-2, 2)), order='F')
    >>> model4.bounding_box
    ModelBoundingBox(
        intervals={
            x: Interval(lower=-1, upper=1)
            y: Interval(lower=-2, upper=2)
        }
        model=Polynomial2D(inputs=('x', 'y'))
        order='F'
    )
    >>> model4(-2, 0)
    0.0
    >>> model4(-2, 0, with_bounding_box=True)
    nan
    >>> model4(-2, 0, with_bounding_box=True, fill_value=12)
    12.0
    >>> model4.bounding_box.bounding_box()
    ((-1, 1), (-2, 2))
    >>> model4.bounding_box.bounding_box(order='C')
    ((-2, 2), (-1, 1))

.. warning::

    Currently when combining models the bounding boxes of components are
    combined only when joining models with the ``&`` operator.
    For the other operators bounding boxes for compound models must be assigned
    explicitly.  A future release will determine the appropriate bounding box
    for a compound model where possible.

Using the Compound Bounding Box
+++++++++++++++++++++++++++++++

Sometimes it is useful to have multiple bounding boxes for the same model,
which are selectable when the model is evaluated. In this case, one should
consider using a `CompoundBoundingBox <astropy.modeling.bounding_box.CompoundBoundingBox>`.

A common use case for this may be if the model has a single "discrete"
selector input (for example ``'slit_id'``), which among other things,
determines what bounding box should be applied to the other inputs. To
do this one needs to first define a dictionary of bounding box tuples,
with dictionary keys being the specific values of the selector input
corresponding to that specific bounding box ::

    >>> from astropy.modeling.models import Shift, Identity
    >>> model1 = Shift(1) & Shift(2) & Identity(1)
    >>> model1.inputs = ('x', 'y', 'slit_id')
    >>> bboxes = {
    ...     0: ((0, 1), (1, 2)),
    ...     1: ((2, 3), (3, 4))
    ... }

In order for the compound bounding box to function one must specify a list
of selector arguments, where the elements of this list are tuples of the input's
name and whether or not the bounding box should be applied to the selector argument
or not. In this case, it makes sense for the selector argument to be ignored ::

    >>> from astropy.modeling.core import bind_compound_bounding_box
    >>> selector_args = [('slit_id', True)]
    >>> bind_compound_bounding_box(model1, bboxes, selector_args, order='F')
    >>> model1.bounding_box
    CompoundBoundingBox(
        bounding_boxes={
            (0,) = ModelBoundingBox(
                    intervals={
                        x: Interval(lower=0, upper=1)
                        y: Interval(lower=1, upper=2)
                    }
                    ignored=['slit_id']
                    model=CompoundModel(inputs=('x', 'y', 'slit_id'))
                    order='F'
                )
            (1,) = ModelBoundingBox(
                    intervals={
                        x: Interval(lower=2, upper=3)
                        y: Interval(lower=3, upper=4)
                    }
                    ignored=['slit_id']
                    model=CompoundModel(inputs=('x', 'y', 'slit_id'))
                    order='F'
                )
        }
        selector_args = SelectorArguments(
                Argument(name='slit_id', ignore=True)
            )
    )
    >>> model1(0.5, 1.5, 0, with_bounding_box=True)
    (1.5, 3.5, 0.0)
    >>> model1(0.5, 1.5, 1, with_bounding_box=True)
    (nan, nan, nan)

Multiple selector arguments can also be used, in this case the keys of the
dictionary of bounding boxes need to be specified as tuples of values ::

    >>> model2 = Shift(1) & Shift(2) & Identity(2)
    >>> model2.inputs = ('x', 'y', 'slit_x', 'slit_y')
    >>> bboxes = {
    ...     (0, 0): ((0, 1), (1, 2)),
    ...     (0, 1): ((2, 3), (3, 4)),
    ...     (1, 0): ((4, 5), (5, 6)),
    ...     (1, 1): ((6, 7), (7, 8)),
    ... }
    >>> selector_args = [('slit_x', True), ('slit_y', True)]
    >>> bind_compound_bounding_box(model2, bboxes, selector_args, order='F')
    >>> model2.bounding_box
    CompoundBoundingBox(
        bounding_boxes={
            (0, 0) = ModelBoundingBox(
                    intervals={
                        x: Interval(lower=0, upper=1)
                        y: Interval(lower=1, upper=2)
                    }
                    ignored=['slit_x', 'slit_y']
                    model=CompoundModel(inputs=('x', 'y', 'slit_x', 'slit_y'))
                    order='F'
                )
            (0, 1) = ModelBoundingBox(
                    intervals={
                        x: Interval(lower=2, upper=3)
                        y: Interval(lower=3, upper=4)
                    }
                    ignored=['slit_x', 'slit_y']
                    model=CompoundModel(inputs=('x', 'y', 'slit_x', 'slit_y'))
                    order='F'
                )
            (1, 0) = ModelBoundingBox(
                    intervals={
                        x: Interval(lower=4, upper=5)
                        y: Interval(lower=5, upper=6)
                    }
                    ignored=['slit_x', 'slit_y']
                    model=CompoundModel(inputs=('x', 'y', 'slit_x', 'slit_y'))
                    order='F'
                )
            (1, 1) = ModelBoundingBox(
                    intervals={
                        x: Interval(lower=6, upper=7)
                        y: Interval(lower=7, upper=8)
                    }
                    ignored=['slit_x', 'slit_y']
                    model=CompoundModel(inputs=('x', 'y', 'slit_x', 'slit_y'))
                    order='F'
                )
        }
        selector_args = SelectorArguments(
                Argument(name='slit_x', ignore=True)
                Argument(name='slit_y', ignore=True)
            )
    )
    >>> model2(0.5, 1.5, 0, 0, with_bounding_box=True)
    (1.5, 3.5, 0.0, 0.0)
    >>> model2(0.5, 1.5, 1, 1, with_bounding_box=True)
    (nan, nan, nan, nan)

Note that one can also specify the ordering for all the bounding boxes
comprising the compound bounding using the ``order`` keyword argument.

Another use case for this maybe a if one wants to use multiple bounding
boxes for the same model, where the user chooses the bounding box when
evaluating the model. In this case, one must still choose a selector
argument as a fall back default for bounding box selection; however, this
argument should not be ignored by the bounding box::

    >>> from astropy.modeling.models import Polynomial2D
    >>> from astropy.modeling import bind_compound_bounding_box
    >>> model = Polynomial2D(3)
    >>> bboxes = {
    ...     0: ((0, 1), (1, 2)),
    ...     1: ((2, 3), (3, 4))
    ... }
    >>> selector_args = [('x', False)]
    >>> bind_compound_bounding_box(model, bboxes, selector_args, order='F')
    >>> model.bounding_box
        CompoundBoundingBox(
        bounding_boxes={
            (0,) = ModelBoundingBox(
                    intervals={
                        x: Interval(lower=0, upper=1)
                        y: Interval(lower=1, upper=2)
                    }
                    model=Polynomial2D(inputs=('x', 'y'))
                    order='F'
                )
            (1,) = ModelBoundingBox(
                    intervals={
                        x: Interval(lower=2, upper=3)
                        y: Interval(lower=3, upper=4)
                    }
                    model=Polynomial2D(inputs=('x', 'y'))
                    order='F'
                )
        }
        selector_args = SelectorArguments(
                Argument(name='x', ignore=False)
            )
    )

For the user to select the bounding box on evaluation, instead of
specifying, ``with_bounding_box=True`` as the keyword argument; the user
instead specifies ``with_bounding_box=<bounding_key>`` ::

    >>> model(0.5, 1.5, with_bounding_box=0)
    0.0
    >>> model(0.5, 1.5, with_bounding_box=1)
    nan


Ignoring Inputs in Bounding Boxes
+++++++++++++++++++++++++++++++++

Both `standard bounding box <astropy.modeling.bounding_box.ModelBoundingBox>`
and `CompoundBoundingBox <astropy.modeling.bounding_box.CompoundBoundingBox>`
support ignoring specific inputs from enforcement by the bounding box. Effectively,
for multi-dimensional models one can define bounding boxes so that bounds are
only applied to a subset of the model's inputs rather than the default of enforcing
a bound of some kind on every input. Note that use of this feature is equivalent
to defining the bounds for an input to be ``[-np.inf, np.inf]``.

.. warning::
   The ``ignored`` input feature is not available when constructing/adding bounding
   boxes to models using tuples and the property interface. That is one cannot
   ignore inputs when setting bounding boxes using ``model.bounding_box = (-1, 1)``.
   This feature is only available via the methods
   `bind_bounding_box <astropy.modeling.bind_bounding_box>` and
   `bind_compound_bounding_box <astropy.modeling.bind_compound_bounding_box>`.

Ignoring inputs for a bounding box can be achieved via passing a list of the input
name strings to be ignored to the ``ignored`` keyword argument in any of the main
bounding box interfaces. ::

    >>> from astropy.modeling.models import Polynomial1D
    >>> from astropy.modeling import bind_bounding_box
    >>> model1 = Polynomial2D(3)
    >>> bind_bounding_box(model1, {'x': (-1, 1)}, ignored=['y'])
    >>> model1.bounding_box
    ModelBoundingBox(
        intervals={
            x: Interval(lower=-1, upper=1)
        }
        ignored=['y']
        model=Polynomial2D(inputs=('x', 'y'))
        order='C'
    )
    >>> model1(-2, 0, with_bounding_box=True)
    nan
    >>> model1(0, 300, with_bounding_box=True)
    0.0

Similarly, the ignored inputs will be applied to all of the bounding boxes
contained within a compound bounding box. ::

    >>> from astropy.modeling import bind_compound_bounding_box
    >>> model2 = Polynomial2D(3)
    >>> bboxes = {
    ...     0: {'x': (0, 1)},
    ...     1: {'x': (1, 2)}
    ... }
    >>> selector_args = [('x', False)]
    >>> bind_compound_bounding_box(model2, bboxes, selector_args, ignored=['y'], order='F')
    >>> model2.bounding_box
        CompoundBoundingBox(
        bounding_boxes={
            (0,) = ModelBoundingBox(
                    intervals={
                        x: Interval(lower=0, upper=1)
                    }
                    ignored=['y']
                    model=Polynomial2D(inputs=('x', 'y'))
                    order='F'
                )
            (1,) = ModelBoundingBox(
                    intervals={
                        x: Interval(lower=1, upper=2)
                    }
                    ignored=['y']
                    model=Polynomial2D(inputs=('x', 'y'))
                    order='F'
                )
        }
        selector_args = SelectorArguments(
                Argument(name='x', ignore=False)
            )
    )
    >>> model2(0.5, 300, with_bounding_box=0)
    0.0
    >>> model2(0.5, 300, with_bounding_box=1)
    nan


Efficient evaluation with `Model.render() <astropy.modeling.Model.render>`
--------------------------------------------------------------------------

When a model is evaluated over a range much larger than the model itself, it
may be prudent to use the :func:`Model.render <astropy.modeling.Model.render>`
method if efficiency is a concern. The :func:`render <astropy.modeling.Model.render>`
method can be used to evaluate the model on an
array of the same dimensions.  ``model.render()`` can be called with no
arguments to return a "postage stamp" of the bounding box region.

In this example, we generate a 300x400 pixel image of 100 2D Gaussian sources.
For comparison, the models are evaluated both with and without using bounding
boxes. By using bounding boxes, the evaluation speed increases by approximately
a factor of 10 with negligible loss of information.

.. plot::
    :include-source:

    import numpy as np
    from time import time
    from astropy.modeling import models
    import matplotlib.pyplot as plt
    from matplotlib.patches import Rectangle

    imshape = (300, 400)
    y, x = np.indices(imshape)

    # Generate random source model list
    np.random.seed(0)
    nsrc = 100
    model_params = [
        dict(amplitude=np.random.uniform(.5, 1),
             x_mean=np.random.uniform(0, imshape[1] - 1),
             y_mean=np.random.uniform(0, imshape[0] - 1),
             x_stddev=np.random.uniform(2, 6),
             y_stddev=np.random.uniform(2, 6),
             theta=np.random.uniform(0, 2 * np.pi))
        for _ in range(nsrc)]

    model_list = [models.Gaussian2D(**kwargs) for kwargs in model_params]

    # Render models to image using bounding boxes
    bb_image = np.zeros(imshape)
    t_bb = time()
    for model in model_list:
        model.render(bb_image)
    t_bb = time() - t_bb

    # Render models to image using full evaluation
    full_image = np.zeros(imshape)
    t_full = time()
    for model in model_list:
        model.bounding_box = None
        model.render(full_image)
    t_full = time() - t_full

    flux = full_image.sum()
    diff = (full_image - bb_image)
    max_err = diff.max()

    # Plots
    plt.figure(figsize=(16, 7))
    plt.subplots_adjust(left=.05, right=.97, bottom=.03, top=.97, wspace=0.15)

    # Full model image
    plt.subplot(121)
    plt.imshow(full_image, origin='lower')
    plt.title(f'Full Models\nTiming: {t_full:.2f} seconds', fontsize=16)
    plt.xlabel('x')
    plt.ylabel('y')

    # Bounded model image with boxes overplotted
    ax = plt.subplot(122)
    plt.imshow(bb_image, origin='lower')
    for model in model_list:
        del model.bounding_box  # Reset bounding_box to its default
        dy, dx = np.diff(model.bounding_box).flatten()
        pos = (model.x_mean.value - dx / 2, model.y_mean.value - dy / 2)
        r = Rectangle(pos, dx, dy, edgecolor='w', facecolor='none', alpha=.25)
        ax.add_patch(r)
    plt.title(f'Bounded Models\nTiming: {t_bb:.2f} seconds', fontsize=16)
    plt.xlabel('x')
    plt.ylabel('y')

    # Difference image
    plt.figure(figsize=(16, 8))
    plt.subplot(111)
    plt.imshow(diff, vmin=-max_err, vmax=max_err)
    plt.colorbar(format='%.1e')
    plt.title(f'Difference Image\nTotal Flux Err = {((flux - np.sum(bb_image)) / flux):.0e}')
    plt.xlabel('x')
    plt.ylabel('y')
    plt.show()



.. _separability:

Model Separability
------------------

Simple models have a boolean `Model.separable <astropy.modeling.Model.separable>` property.
It indicates whether the outputs are independent and is essential for computing the
separability of compound models using the :func:`~astropy.modeling.is_separable` function.
Having a separable compound model means that it can be decomposed into independent models,
which in turn is useful in many applications.
For example, it may be easier to define inverses using the independent parts of a model
than the entire model.
In other cases, tools using `Generalized World Coordinate System (GWCS)`_,
can be more flexible and take advantage of separable spectral and spatial transforms.


.. _modeling-model-sets:

Model Sets
==========

In some cases it is useful to describe many models of the same type but with
different sets of parameter values.  This could be done simply by instantiating
as many instances of a `~astropy.modeling.Model` as are needed.  But that can
be inefficient for a large number of models.  To that end, all model classes in
`astropy.modeling` can also be used to represent a model **set** which is a
collection of models of the same type, but with different values for their
parameters.

To instantiate a model set, use argument ``n_models=N`` where ``N`` is the
number of models in the set when constructing the model.  The value of each
parameter must be a list or array of length ``N``, such that each item in
the array corresponds to one model in the set::

    >>> from astropy.modeling import models
    >>> g = models.Gaussian1D(amplitude=[1, 2], mean=[0, 0],
    ...                       stddev=[0.1, 0.2], n_models=2)
    >>> print(g)
    Model: Gaussian1D
    Inputs: ('x',)
    Outputs: ('y',)
    Model set size: 2
    Parameters:
        amplitude mean stddev
        --------- ---- ------
              1.0  0.0    0.1
              2.0  0.0    0.2

This is equivalent to two Gaussians with the parameters ``amplitude=1, mean=0,
stddev=0.1`` and ``amplitude=2, mean=0, stddev=0.2`` respectively.  When
printing the model the parameter values are displayed as a table, with each row
corresponding to a single model in the set.

The number of models in a model set can be determined using the `len` builtin::

    >>> len(g)
    2

Single models have a length of 1, and are not considered a model set as such.

When evaluating a model set, by default the input must be the same length as
the number of models, with one input per model::

    >>> g([0, 0.1])  # doctest: +FLOAT_CMP
    array([1.        , 1.76499381])

The result is an array with one result per model in the set.  It is also
possible to broadcast a single input value to all models in the set::

    >>> g(0)  # doctest: +FLOAT_CMP
    array([1., 2.])

Or when the input is an array::

    >>> x = np.array([[0, 0, 0], [0.1, 0.1, 0.1]])
    >>> print(x)
    [[0.  0.  0. ]
     [0.1 0.1 0.1]]
    >>> g(x)
    array([[1.        , 1.        , 1.        ],
           [1.76499381, 1.76499381, 1.76499381]])

Internally the shape of the inputs, outputs, and parameter values is controlled
by an attribute - ``model_set_axis``. In the above case ``model_set_axis=0``::

    >>> g.model_set_axis
    0

This indicates that elements along the 0-th axis will be passed as inputs to individual models.
Sometimes it may be useful to pass inputs along a different axis, for example the 1st axis::

    >>> x = np.array([[0, 0, 0], [0.1, 0.1, 0.1]]).T
    >>> print(x)
    [[0.  0.1]
     [0.  0.1]
     [0.  0.1]]

Because there are two models in this model set and we are passing three inputs
along the 0th axis, evaluation will fail::

    >>> g(x)
    Traceback (most recent call last):
    ...
    ValueError: Input argument 'x' does not have the correct dimensions in
    model_set_axis=0 for a model set with n_models=2.

There are two ways to get around this. ``model_set_axis`` can be passed in
when the model is evaluated::

    >>> g(x, model_set_axis=1)
    array([[1.        , 1.76499381],
           [1.        , 1.76499381],
           [1.        , 1.76499381]])

Or when the model is initialized::

    >>> g = models.Gaussian1D(amplitude=[[1, 2]], mean=[[0, 0]],
    ...                       stddev=[[0.1, 0.2]], n_models=2,
    ...                       model_set_axis=1)
    >>> g(x)
    array([[1.        , 1.76499381],
           [1.        , 1.76499381],
           [1.        , 1.76499381]])

Note that in the latter case, the shape of the individual parameters has changed to 2D
because now the parameters are defined along the 1st axis.

The value of ``model_set_axis`` is either an integer number, representing the axis along which
the different parameter sets and inputs are defined, or a boolean of value ``False``,
in which case it indicates all model sets should use the same inputs on evaluation.
For example, the above model has a value of 1 for ``model_set_axis``.
If ``model_set_axis=False`` is passed the two models will be evaluated on the same input::

    >>> g.model_set_axis
    1
    >>> result = g(x, model_set_axis=False)
    >>> result
    array([[[1.        , 0.60653066],
            [2.        , 1.76499381]],
    <BLANKLINE>
           [[1.        , 0.60653066],
            [2.        , 1.76499381]],
    <BLANKLINE>
           [[1.        , 0.60653066],
            [2.        , 1.76499381]]])
    >>> result[: , 0]
    array([[1.        , 0.60653066],
           [1.        , 0.60653066],
           [1.        , 0.60653066]])
    >>> result[: , 1]
    array([[2.        , 1.76499381],
           [2.        , 1.76499381],
           [2.        , 1.76499381]])

Currently model sets are most useful for fitting a set of **linear** models
(:ref:`example <example-fitting-model-sets>`)
allowing a large number of models of the same type to be fitted simultaneously
(and independently from each other) to some large set of inputs, such as
fitting a polynomial to the time response of each pixel in a data cube.
This can greatly speed up the fitting process. The speed-up is due to solving
the set of equations to find the exact solution. Nonlinear models, which require
an iterative algorithm, cannot be currently fit using model sets. Model sets of nonlinear
models can only be evaluated.

When fitting model sets it is important that data arrays are passed to the fitter
in the correct shape. The shape depends on the ``model_set_axis`` attribute of the
model to be fit. The rule is that the index of the dependent variable that corresponds
to a model set should be along the ``model_set_axis`` dimension. For example, for a
1D model set with 3 models with ``model_set_axis == 1`` the shape of ``y`` should be (x, 3)::

    >>> import numpy as np
    >>> from astropy.modeling.models import Polynomial1D
    >>> from astropy.modeling.fitting import LinearLSQFitter
    >>> fitter = LinearLSQFitter()
    >>> x = np.arange(4)
    >>> y = np.array([2*x+1, x+4, x]).T
    >>> print(y)
    [[1 4 0]
     [3 5 1]
     [5 6 2]
     [7 7 3]]
    >>> print(y.shape)
    (4, 3)
    >>> m = Polynomial1D(1, n_models=3, model_set_axis=1)
    >>> mfit = fitter(m, x, y)

For 2D models with 3 models and ``model_set_axis = 0`` the shape of ``z`` should be (3, x, y)::

    >>> import numpy as np
    >>> from astropy.modeling.models import Polynomial2D
    >>> from astropy.modeling.fitting import LinearLSQFitter
    >>> fitter = LinearLSQFitter()
    >>> x = np.arange(8).reshape(2, 4)
    >>> y = x
    >>> z = np.asarray([2 * x + 1, x + 4, x + 3])
    >>> print(z.shape)
    (3, 2, 4)
    >>> m = Polynomial2D(1, n_models=3, model_set_axis=0)
    >>> mfit = fitter(m, x, y, z)

.. _modeling-asdf:

Model Serialization (Writing a Model to a File)
===============================================

Models are serializable using the `ASDF`_
format. This can be useful in many contexts, one of which is the implementation of a
`Generalized World Coordinate System (GWCS)`_.

Serializing a model to disk is possible by assigning the object to ``AsdfFile.tree``:

.. doctest-requires:: asdf

    >>> from asdf import AsdfFile
    >>> from astropy.modeling import models
    >>> rotation = models.Rotation2D(angle=23.7)
    >>> f = AsdfFile()
    >>> f.tree['model'] = rotation
    >>> f.write_to('rotation.asdf')

To read the file and create the model:

.. doctest-requires:: asdf

    >>> import asdf
    >>> with asdf.open('rotation.asdf') as f:
    ...     model = f.tree['model']
    >>> print(model)
    Model: Rotation2D
    Inputs: ('x', 'y')
    Outputs: ('x', 'y')
    Model set size: 1
    Parameters:
        angle
        -----
         23.7

Compound models can also be serialized. Please note that some model attributes (e.g ``meta``,
``tied`` parameter constraints used in fitting), as well as model sets are not yet serializable.
For more information on serialization of models, see :ref:`asdf_dev`.
.. _predef_models2D:

*********
2D Models
*********

These models take as input x and y arrays.

Operations
==========

These models perform simple mathematical operations.

- :class:`~astropy.modeling.functional_models.Const2D` model returns the
  constant replicated by the number of input x and y values.

Shapes
======

These models provide shapes, often used to model general x, y, z data.

- :class:`~astropy.modeling.functional_models.Planar2D` model computes
  a tilted plan with specified x,y slopes and z intercept

Profiles
========

These models provide profiles, often used sources in images.
All models have parameters giving the x,y location of the center and
an amplitude.

- :class:`~astropy.modeling.functional_models.AiryDisk2D` model computes
  the Airy function for a radius

- :class:`~astropy.modeling.functional_models.Box2D` model computes a box
  with x,y dimensions

- :class:`~astropy.modeling.functional_models.Disk2D` model computes a
  disk a radius

- :class:`~astropy.modeling.functional_models.Ellipse2D` model computes
  an ellipse with major and minor axis and rotation angle

- :class:`~astropy.modeling.functional_models.Gaussian2D` model computes
  a Gaussian with x,y standard deviations and rotation angle

- :class:`~astropy.modeling.functional_models.Moffat2D` model computes
  a Moffat with x,y dimensions and alpha (power index) and gamma (core width)

- :class:`~astropy.modeling.functional_models.RickerWavelet2D` model computes
  a symmetric RickerWavelet function with the specified sigma

- :class:`~astropy.modeling.functional_models.Sersic2D` model computes
  a Sersic profile with an effective half-light radius, rotation, and
  Sersic index

- :class:`~astropy.modeling.functional_models.TrapezoidDisk2D` model
  computes a disk with a radius and slope

- :class:`~astropy.modeling.functional_models.Ring2D` model computes
  a ring with inner and outer radii

.. plot::

    import numpy as np
    import math
    import matplotlib.pyplot as plt
    from matplotlib.colors import LogNorm

    from astropy.modeling.models import (AiryDisk2D, Box2D, Disk2D, Ellipse2D,
                                         Gaussian2D, Moffat2D, RickerWavelet2D,
                                         Sersic2D, TrapezoidDisk2D, Ring2D)

    x = np.linspace(-4.0, 6.0, num=100)
    r = np.logspace(-1.0, 2.0, num=100)

    fig, sax = plt.subplots(nrows=4, ncols=3, figsize=(9, 12))
    ax = sax.flatten()

    # setup the x,y coordinates
    x_npts = 100
    y_npts = x_npts
    x0, x1 = -4, 6
    y0, y1 = -3, 7
    x = np.linspace(x0, x1, num=x_npts)
    y = np.linspace(y0, y1, num=y_npts)
    X, Y = np.meshgrid(x, y)

    # plot the different 2D profiles
    mods = [AiryDisk2D(amplitude=10.0, x_0=1.0, y_0=2.0, radius=1.0),
            Box2D(amplitude=10.0, x_0=1.0, y_0=2.0, x_width=1.0, y_width=2.0),
            Disk2D(amplitude=10.0, x_0=1.0, y_0=2.0, R_0=1.0),
            Ellipse2D(amplitude=10.0, x_0=1.0, y_0=2.0, a=1.0, b=2.0, theta=math.pi/4.),
            Gaussian2D(amplitude=10.0, x_mean=1.0, y_mean=2.0, x_stddev=1.0, y_stddev=2.0, theta=math.pi/4.),
            Moffat2D(amplitude=10.0, x_0=1.0, y_0=2.0, alpha=3, gamma=4),
            RickerWavelet2D(amplitude=10.0, x_0=1.0, y_0=2.0, sigma=1.0),
            Sersic2D(amplitude=10.0, x_0=1.0, y_0=2.0, r_eff=1.0, ellip=0.5, theta=math.pi/4.),
            TrapezoidDisk2D(amplitude=10.0, x_0=1.0, y_0=2.0, R_0=1.0, slope=5.0),
            Ring2D(amplitude=10.0, x_0=1.0, y_0=2.0, r_in=1.0, r_out=2.0)]

    for k, mod in enumerate(mods):
        cname = mod.__class__.__name__
        ax[k].set_title(cname)
        if cname == "AiryDisk2D":
            normfunc = LogNorm(vmin=0.001, vmax=10.)
        elif cname in ["Gaussian2D", "Sersic2D"]:
            normfunc = LogNorm(vmin=0.1, vmax=10.)
        else:
            normfunc = None
        ax[k].imshow(mod(X, Y), extent=[x0, x1, y0, y1], origin="lower", cmap=plt.cm.gray_r,
                     norm=normfunc)

    for k in range(len(mods)):
        ax[k].set_xlabel("x")
        ax[k].set_ylabel("y")

    # remove axis for any plots not used
    for k in range(len(mods), len(ax)):
        ax[k].axis("off")

    plt.tight_layout()
    plt.show()
.. _jointfitter:

JointFitter
===========

There are cases where one may wish to fit multiple datasets with models that
share parameters.  This is possible with the
`astropy.modeling.fitting.JointFitter`.  Basically, this fitter is
setup with a list of defined models, the parameters in common between the
different models, and the initial values for those parameters. Then the fitter
is called supplying as many x and y arrays, one for each model to be fit.  The
fit parameters are the result of the jointly fitting the models to the
combined datasets.

.. note::
   The JointFitter uses the scipy.optimize.leastsq.  In addition, it
   does not support fixed, bounded, or tied parameters at this time.

Example: Spectral Line
======================

This example is for two spectral segments with different spectral resolutions
that have the same spectral line in the wavelength region that is overlapping
between both segments.

We will need to define a Gaussian function that has mean wavelength, area, and
width parameters.  This is needed as the `astropy.modeling.functional_models.Gaussian1D`
function has mean wavelength, central intensity, and width parameters, but the
central intensity of a line will be different at different spectral resolutions,
but the area will be the same.

First, imports needed for this example

   >>> # imports
   >>> import math
   >>> import numpy as np
   >>> from astropy.modeling import fitting, Fittable1DModel
   >>> from astropy.modeling.parameters import Parameter
   >>> from astropy.modeling.functional_models import FLOAT_EPSILON

Now define AreaGaussian1D with area instead of intensity as a parameter.
This new is modified and trimmed version of the standard Gaussian1D model.

   >>> class AreaGaussian1D(Fittable1DModel):
   ...   """
   ...   One dimensional Gaussian model with area as a parameter.
   ...
   ...   Parameters
   ...   ----------
   ...   area : float or `~astropy.units.Quantity`.
   ...       Integrated area
   ...       Note: amplitude = area / (stddev * np.sqrt(2 * np.pi))
   ...   mean : float or `~astropy.units.Quantity`.
   ...       Mean of the Gaussian.
   ...   stddev : float or `~astropy.units.Quantity`.
   ...       Standard deviation of the Gaussian with FWHM = 2 * stddev * np.sqrt(2 * np.log(2)).
   ...   """
   ...   area = Parameter(default=1)
   ...   mean = Parameter(default=0)
   ...
   ...   # Ensure stddev makes sense if its bounds are not explicitly set.
   ...   # stddev must be non-zero and positive.
   ...   stddev = Parameter(default=1, bounds=(FLOAT_EPSILON, None))
   ...
   ...   @staticmethod
   ...   def evaluate(x, area, mean, stddev):
   ...       """
   ...       AreaGaussian1D model function.
   ...       """
   ...       return (area / (stddev * np.sqrt(2 * np.pi))) * np.exp(
   ...           -0.5 * (x - mean) ** 2 / stddev ** 2
   ...       )

Data to be fit is simulated.  The 1st spectral segment will have a spectral
resolution that is a factor of 2 higher than the second segment.  The first
segment will have wavelengths from 1 to 6 and the second from 4 to 10 giving
an overlapping wavelength region from 4 to 6.

   >>> # Generate fake data
   >>> mean = 5.1
   >>> sigma1 = 0.2
   >>> sigma2 = 0.4
   >>> noise = 0.10

   >>> # compute the central amplitudes so the lines in each segment have the
   >>> # same area
   >>> area = 1.5
   >>> amp1 = area / np.sqrt(2.0 * math.pi * sigma1 ** 2)
   >>> amp2 = area / np.sqrt(2.0 * math.pi * sigma2 ** 2)

   >>> # segment 1
   >>> np.random.seed(0)
   >>> x1 = np.linspace(1.0, 6.0, 200)
   >>> y1 = amp1 * np.exp(-0.5 * (x1 - mean) ** 2 / sigma1 ** 2)
   >>> y1 += np.random.normal(0.0, noise, x1.shape)

   >>> # segment 2
   >>> np.random.seed(0)
   >>> x2 = np.linspace(4.0, 10.0, 200)
   >>> y2 = amp2 * np.exp(-0.5 * (x2 - mean) ** 2 / sigma2 ** 2)
   >>> y2 += np.random.normal(0.0, noise, x2.shape)

Now define the models to be fit and fitter to use.  Then fit the two simulated
datasets.

   >>> # define the two models to be fit
   >>> gjf1 = AreaGaussian1D(area=1.0, mean=5.0, stddev=1.0)
   >>> gjf2 = AreaGaussian1D(area=1.0, mean=5.0, stddev=1.0)

.. doctest-requires:: scipy

   >>> # define the jointfitter specifying the parameters in common and their initial values
   >>> fit_joint = fitting.JointFitter(
   ...    [gjf1, gjf2], {gjf1: ["area", "mean"], gjf2: ["area", "mean"]}, [1.0, 5.0]
   ... )
   >>>
   >>> # perform the fit
   >>> g12 = fit_joint(x1, y1, x2, y2)


The resulting fit parameters show that the area and mean wavelength of the
two AreaGaussian1D models are exactly the same while the width (stddev) is
different reflecting the different spectral resolutions of the two segments.

AreaGaussian1 parameters

.. doctest-requires:: scipy

   >>> print(gjf1.param_names)
   ('area', 'mean', 'stddev')
   >>> print(gjf1.parameters)
   [1.48697226 5.09826068 0.19761087]

AreaGaussian2 parameters

.. doctest-requires:: scipy

   >>> print(gjf1.param_names)
   ('area', 'mean', 'stddev')
   >>> print(gjf2.parameters)
   [1.48697226 5.09826068 0.4015368 ]


The simulated data and best fit models can be plotted showing good agreement
between the two AreaGaussian1D models and the two spectral segments.

.. plot::

   # imports
   import numpy as np
   import math
   import matplotlib.pyplot as plt
   from astropy.modeling import fitting, Fittable1DModel
   from astropy.modeling.parameters import Parameter
   from astropy.modeling.functional_models import FLOAT_EPSILON


   class AreaGaussian1D(Fittable1DModel):
       """
       One dimensional Gaussian model with area as a parameter.

       Parameters
       ----------
       area : float or `~astropy.units.Quantity`.
           Integrated area
           Note: amplitude = area / (stddev * np.sqrt(2 * np.pi))
       mean : float or `~astropy.units.Quantity`.
           Mean of the Gaussian.
       stddev : float or `~astropy.units.Quantity`.
           Standard deviation of the Gaussian with FWHM = 2 * stddev * np.sqrt(2 * np.log(2)).
       """

       area = Parameter(default=1)
       mean = Parameter(default=0)

       # Ensure stddev makes sense if its bounds are not explicitly set.
       # stddev must be non-zero and positive.
       stddev = Parameter(default=1, bounds=(FLOAT_EPSILON, None))

       @staticmethod
       def evaluate(x, area, mean, stddev):
           """
           AreaGaussian1D model function.
           """
           return (area / (stddev * np.sqrt(2 * np.pi))) * np.exp(
               -0.5 * (x - mean) ** 2 / stddev ** 2
           )


   # Generate fake data
   mean = 5.1
   sigma1 = 0.2
   sigma2 = 0.4
   noise = 0.10

   # compute the central amplitudes so the lines in each segment have the
   # same area
   area = 1.5
   amp1 = area / np.sqrt(2.0 * math.pi * sigma1 ** 2)
   amp2 = area / np.sqrt(2.0 * math.pi * sigma2 ** 2)

   # segment 1
   np.random.seed(0)
   x1 = np.linspace(1.0, 6.0, 200)
   y1 = amp1 * np.exp(-0.5 * (x1 - mean) ** 2 / sigma1 ** 2)
   y1 += np.random.normal(0.0, noise, x1.shape)

   # segment 2
   np.random.seed(0)
   x2 = np.linspace(4.0, 10.0, 200)
   y2 = amp2 * np.exp(-0.5 * (x2 - mean) ** 2 / sigma2 ** 2)
   y2 += np.random.normal(0.0, noise, x2.shape)

   # define the two models to be fit
   gjf1 = AreaGaussian1D(area=1.0, mean=5.0, stddev=1.0)
   gjf2 = AreaGaussian1D(area=1.0, mean=5.0, stddev=1.0)

   # define the jointfitter specifying the parameters in common and their initial values
   fit_joint = fitting.JointFitter(
       [gjf1, gjf2], {gjf1: ["area", "mean"], gjf2: ["area", "mean"]}, [1.0, 5.0]
   )

   # perform the fit
   g12 = fit_joint(x1, y1, x2, y2)

   # Plot the data with the best-fit models
   plt.figure(figsize=(8, 5))
   plt.plot(x1, y1, "bo", alpha=0.25)
   plt.plot(x2, y2, "go", alpha=0.25)
   plt.plot(x1, gjf1(x1), "b--", label="AreaGaussian1")
   plt.plot(x2, gjf2(x2), "g--", label="AreaGaussian2")
   plt.xlabel("Wavelength")
   plt.ylabel("Flux")
   plt.legend(loc=2)
.. _modeling-units:

********************************
Support for units and quantities
********************************


.. note:: The functionality presented here was recently added. If you run into
          any issues, please don't hesitate to open an issue in the `issue
          tracker <https://github.com/astropy/astropy/issues>`_.

The `astropy.modeling` package includes partial support for the use of units and
quantities in model parameters, models, and during fitting. At this time, only
some of the built-in models (such as
:class:`~astropy.modeling.functional_models.Gaussian1D`) support units, but this
will be extended in future to all models where this is appropriate.

Setting parameters to quantities
================================

Models can take :class:`~astropy.units.Quantity` objects as parameters::

    >>> from astropy import units as u
    >>> from astropy.modeling.models import Gaussian1D
    >>> g1 = Gaussian1D(mean=3 * u.m, stddev=2 * u.cm, amplitude=3 * u.Jy)

Accessing the parameter then returns a Parameter object that contains the value
and the unit::

    >>> g1.mean
    Parameter('mean', value=3.0, unit=m)

It is then possible to access the individual properties of the parameter::

    >>> g1.mean.name
    'mean'
    >>> g1.mean.value
    3.0
    >>> g1.mean.unit
    Unit("m")

If a parameter has been initialized as a Quantity, it should always be set to a
quantity, but the units don't have to be compatible with the initial ones::

    >>> g1.mean = 3 * u.s
    >>> g1  # doctest: +FLOAT_CMP
    <Gaussian1D(amplitude=3. Jy, mean=3. s, stddev=2. cm)>

To change the value of a parameter and not the unit, simply set the value
property::

    >>> g1.mean.value = 2
    >>> g1  # doctest: +FLOAT_CMP
    <Gaussian1D(amplitude=3. Jy, mean=2. s, stddev=2. cm)>

Setting a parameter which was originally set to a quantity to a scalar doesn't
work because it's ambiguous whether the user means to change just the value and
preserve the unit, or get rid of the unit::

    >>> g1.mean = 2  # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
    ...
    UnitsError : The 'mean' parameter should be given as a Quantity because it
    was originally initialized as a Quantity

On the other hand, if a parameter previously defined without units is given a
Quantity with a unit, this works because it is unambiguous::

    >>> g2 = Gaussian1D(mean=3)
    >>> g2.mean = 3 * u.m

In other words, once units are attached to a parameter, they can't be removed
due to ambiguous meaning.

Evaluating models with quantities
=================================

Quantities can be passed to model during evaluation::

    >>> g3 = Gaussian1D(mean=3 * u.m, stddev=5 * u.cm)
    >>> g3(2.9 * u.m)  # doctest: +FLOAT_CMP
    <Quantity 0.1353352832366122>
    >>> g3(2.9 * u.s)  # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
    ...
    UnitsError : Units of input 'x', s (time), could not be converted to
    required input units of m (length)

In this case, since the mean and standard deviation have units, the value passed
during evaluation also needs units::

    >>> g3(3)  # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
    ...
    UnitsError : Units of input 'x', (dimensionless), could not be converted to
    required input units of m (length)

Equivalencies
-------------

Equivalencies require special care - a Gaussian defined in frequency space is
not a Gaussian in wavelength space for example. For this reason, we don't allow
equivalencies to be attached to the parameters themselves. Instead, we take the
approach of converting the input data to the parameter space, and any
equivalencies should be applied at evaluation time to the data (not the
parameters).

Let's consider a model that is Gaussian in wavelength space::

    >>> g4 = Gaussian1D(mean=3 * u.micron, stddev=1 * u.micron, amplitude=3 * u.Jy)

By default, passing a frequency will not work:

    >>> g4(1e2 * u.THz)  # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
    ...
    UnitsError : Units of input 'x', THz (frequency), could not be converted to
    required input units of micron (length)

But you can pass a dictionary of equivalencies to the equivalencies argument
(this needs to be a dictionary since some models can contain multiple inputs)::

    >>> g4(110 * u.THz, equivalencies={'x': u.spectral()})  # doctest: +FLOAT_CMP
    <Quantity 2.888986819525229 Jy>

The key of the dictionary should be the name of the inputs according to::

    >>> g4.inputs
    ('x',)

It is also possible to set default equivalencies for the input parameters using
the input_units_equivalencies property::

    >>> g4.input_units_equivalencies = {'x': u.spectral()}
    >>> g4(110 * u.THz)  # doctest: +FLOAT_CMP
    <Quantity 2.888986819525229 Jy>

Fitting models with units to data
=================================

Fitting models with units to data with units should be seamless provided that
the model supports fitting with units. To demonstrate this, we start off by
generating synthetic data:

.. plot::
   :context: reset
   :include-source:

    import numpy as np
    from astropy import units as u
    import matplotlib.pyplot as plt

    x = np.linspace(1, 5, 30) * u.micron
    y = np.exp(-0.5 * (x - 2.5 * u.micron)**2 / (200 * u.nm)**2) * u.mJy
    plt.plot(x, y, 'ko')
    plt.xlabel('Wavelength (microns)')
    plt.ylabel('Flux density (mJy)')

and we then define the initial guess for the fitting and we carry out the fit as
we would without any units:

.. plot::
   :context:
   :include-source:

    from astropy.modeling import models, fitting

    g5 = models.Gaussian1D(mean=3 * u.micron, stddev=1 * u.micron, amplitude=1 * u.Jy)

    fitter = fitting.LevMarLSQFitter()

    g5_fit = fitter(g5, x, y)

    plt.plot(x, y, 'ko')
    plt.plot(x, g5_fit(x), 'r-')
    plt.xlabel('Wavelength (microns)')
    plt.ylabel('Flux density (mJy)')

Fitting with equivalencies
--------------------------

Let's now consider the case where the data is not equivalent to those of the
parameters, but they are convertible via equivalencies. In this case, the
equivalencies can either be passed via a dictionary as shown higher up for the
evaluation examples:

.. plot::
   :context:
   :include-source:

    g6 = models.Gaussian1D(mean=110 * u.THz, stddev=10 * u.THz, amplitude=1 * u.Jy)

    g6_fit = fitter(g6, x, y, equivalencies={'x': u.spectral()})

    plt.plot(x, g6_fit(x, equivalencies={'x': u.spectral()}), 'b-')
    plt.xlabel('Wavelength (microns)')
    plt.ylabel('Flux density (mJy)')

In this case, the fit (in blue) is slightly worse, because a Gaussian in
frequency space (blue) is not a Gaussian in wavelength space (red). As mentioned
previously, you can also set input_units_equivalencies on the model itself to
avoid having to pass extra arguments to the fitter::

    g6.input_units_equivalencies = {'x': u.spectral()}
    g6_fit = fitter(g6, x, y)


.. _units-mapping:

Support for units in otherwise unitless models
==============================================

Some models, like polynomials, do not work intrinsically with units. Instead,
the :meth:`~astropy.modeling.core.Model.coerce_units` method provides a way to add input and return units to
unitless models by enclosing the unitless model with two instances of :class:`~astropy.modeling.mappings.UnitsMapping`.
Internally the inputs are stripped of the units before passed
to the model and units are attached to the result if ``return_units`` is specified.
The method returns a new composite model::

    >>> from astropy.modeling import models
    >>> from astropy import units as u
    >>> model = models.Polynomial1D(1, c0=1, c1=2)
    >>> new_model = model.coerce_units(input_units={'x': u.Hz}, return_units={'y': u.s},
    ... input_units_equivalencies={'x':u.spectral()})
    >>> new_model(10 * u.Hz)
    <Quantity 21. s>
.. include:: links.inc

.. _compound-models-intro:

Combining Models
****************

Basics
======

While the Astropy modeling package makes it very easy to define :doc:`new
models <new-model>` either from existing functions, or by writing a
`~astropy.modeling.Model` subclass, an additional way to create new models is
by combining them using arithmetic expressions.  This works with models built
into Astropy, and most user-defined models as well.  For example, it is
possible to create a superposition of two Gaussians like so::

    >>> from astropy.modeling import models
    >>> g1 = models.Gaussian1D(1, 0, 0.2)
    >>> g2 = models.Gaussian1D(2.5, 0.5, 0.1)
    >>> g1_plus_2 = g1 + g2

The resulting object ``g1_plus_2`` is itself a new model.  Evaluating, say,
``g1_plus_2(0.25)`` is the same as evaluating ``g1(0.25) + g2(0.25)``::

    >>> g1_plus_2(0.25)  # doctest: +FLOAT_CMP
    0.5676756958301329
    >>> g1_plus_2(0.25) == g1(0.25) + g2(0.25)
    True

This model can be further combined with other models in new expressions.

These new compound models can also be fitted to data, like most other models
(though this currently requires one of the non-linear fitters):

.. plot::
    :include-source:

    import warnings
    import numpy as np
    import matplotlib.pyplot as plt
    from astropy.modeling import models, fitting

    # Generate fake data
    np.random.seed(42)
    g1 = models.Gaussian1D(1, 0, 0.2)
    g2 = models.Gaussian1D(2.5, 0.5, 0.1)
    x = np.linspace(-1, 1, 200)
    y = g1(x) + g2(x) + np.random.normal(0., 0.2, x.shape)

    # Now to fit the data create a new superposition with initial
    # guesses for the parameters:
    gg_init = models.Gaussian1D(1, 0, 0.1) + models.Gaussian1D(2, 0.5, 0.1)
    fitter = fitting.SLSQPLSQFitter()

    with warnings.catch_warnings():
        # Ignore a warning on clipping to bounds from the fitter
        warnings.filterwarnings('ignore', message='Values in x were outside bounds',
                                category=RuntimeWarning)
        gg_fit = fitter(gg_init, x, y)

    # Plot the data with the best-fit model
    plt.figure(figsize=(8,5))
    plt.plot(x, y, 'ko')
    plt.plot(x, gg_fit(x))
    plt.xlabel('Position')
    plt.ylabel('Flux')

This works for 1-D models, 2-D models, and combinations thereof, though there
are some complexities involved in correctly matching up the inputs and outputs
of all models used to build a compound model.  You can learn more details in
the :doc:`compound-models` documentation.

Astropy models also support convolution through the function
`~astropy.convolution.convolve_models`, which returns a compound model.

For instance, the convolution of two Gaussian functions is also a Gaussian
function in which the resulting mean (variance) is the sum of the means
(variances) of each Gaussian.

.. plot::
    :include-source:

    import numpy as np
    import matplotlib.pyplot as plt
    from astropy.modeling import models
    from astropy.convolution import convolve_models

    g1 = models.Gaussian1D(1, -1, 1)
    g2 = models.Gaussian1D(1, 1, 1)
    g3 = convolve_models(g1, g2)

    x = np.linspace(-3, 3, 50)
    plt.plot(x, g1(x), 'k-')
    plt.plot(x, g2(x), 'k-')
    plt.plot(x, g3(x), 'k-')


.. _compound-models:

A comprehensive description
===========================

Some terminology
----------------

It is possible to create new models just by
combining existing models using the arithmetic operators ``+``, ``-``, ``*``,
``/``, and ``**``, or by model composition using ``|`` and
concatenation (explained below) with ``&``, as well as using :func:`~astropy.modeling.fix_inputs`
for :ref:`reducing the number of inputs to a model <model-reduction>`.


In discussing the compound model feature, it is useful to be clear about a
few terms where there have been points of confusion:

- The term "model" can refer either to a model *class* or a model *instance*.

  - All models in `astropy.modeling`, whether it represents some
    `function <astropy.modeling.functional_models>`, a
    `rotation <astropy.modeling.rotations>`, etc., are represented in the
    abstract by a model *class*--specifically a subclass of
    `~astropy.modeling.Model`--that encapsulates the routine for evaluating the
    model, a list of its required parameters, and other metadata about the
    model.

  - Per typical object-oriented parlance, a model *instance* is the object
    created when when calling a model class with some arguments--in most cases
    values for the model's parameters.

  A model class, by itself, cannot be used to perform any computation because
  most models, at least, have one or more parameters that must be specified
  before the model can be evaluated on some input data. However, we can still
  get some information about a model class from its representation.  For
  example::

      >>> from astropy.modeling.models import Gaussian1D
      >>> Gaussian1D
      <class 'astropy.modeling.functional_models.Gaussian1D'>
      Name: Gaussian1D
      N_inputs: 1
      N_outputs: 1
      Fittable parameters: ('amplitude', 'mean', 'stddev')

  We can then create a model *instance* by passing in values for the three
  parameters::

      >>> my_gaussian = Gaussian1D(amplitude=1.0, mean=0, stddev=0.2)
      >>> my_gaussian  # doctest: +FLOAT_CMP
      <Gaussian1D(amplitude=1.0, mean=0.0, stddev=0.2)>

  We now have an *instance* of `~astropy.modeling.functional_models.Gaussian1D`
  with all its parameters (and in principle other details like fit constraints)
  filled in so that we can perform calculations with it as though it were a
  function::

      >>> my_gaussian(0.2)  # doctest: +FLOAT_CMP
      0.6065306597126334

  In many cases this document just refers to "models", where the class/instance
  distinction is either irrelevant or clear from context.  But a distinction
  will be made where necessary.

- A *compound model* can be created by combining two or more existing model instances
  which can be models that come with Astropy, :doc:`user defined models <new-model>`, or
  other compound models--using Python expressions consisting of one or more of the
  supported binary operators.

- In some places the term *composite model* is used interchangeably with
  *compound model*. However, this document uses the
  term *composite model* to refer *only* to the case of a compound model
  created from the functional composition of two or more models using the pipe
  operator ``|`` as explained below.  This distinction is used consistently
  within this document, but it may be helpful to understand the distinction.


Creating compound models
------------------------

The only way to create compound models is
to combine existing single models and/or compound models using expressions in
Python with the binary operators ``+``, ``-``, ``*``, ``/``, ``**``, ``|``,
and ``&``, each of which is discussed in the following sections.

The result of combining two models is a model instance::

    >>> two_gaussians = Gaussian1D(1.1, 0.1, 0.2) + Gaussian1D(2.5, 0.5, 0.1)
    >>> two_gaussians  # doctest: +FLOAT_CMP
    <CompoundModel...(amplitude_0=1.1, mean_0=0.1, stddev_0=0.2, amplitude_1=2.5, mean_1=0.5, stddev_1=0.1)>

This expression creates a new model instance that is ready to be used for evaluation::

    >>> two_gaussians(0.2)  # doctest: +FLOAT_CMP
    0.9985190841886609

The ``print`` function provides more information about this object::

    >>> print(two_gaussians)
    Model: CompoundModel...
    Inputs: ('x',)
    Outputs: ('y',)
    Model set size: 1
    Expression: [0] + [1]
    Components:
        [0]: <Gaussian1D(amplitude=1.1, mean=0.1, stddev=0.2)>
    <BLANKLINE>
        [1]: <Gaussian1D(amplitude=2.5, mean=0.5, stddev=0.1)>
    Parameters:
        amplitude_0 mean_0 stddev_0 amplitude_1 mean_1 stddev_1
        ----------- ------ -------- ----------- ------ --------
                1.1    0.1      0.2         2.5    0.5      0.1

There are a number of things to point out here:  This model has six
fittable parameters. How parameters are handled is discussed further in the
section on :ref:`compound-model-parameters`.  We also see that there is a
listing of the *expression* that was used to create this compound model, which
in this case is summarized as ``[0] + [1]``.  The ``[0]`` and ``[1]`` refer to
the first and second components of the model listed next (in this case both
components are the `~astropy.modeling.functional_models.Gaussian1D` objects).

Each component of a compound model is a single, non-compound model.  This is
the case even when including an existing compound model in a new expression.
The existing compound model is not treated as a single model--instead the
expression represented by that compound model is extended.  An expression
involving two or more compound models results in a new expression that is the
concatenation of all involved models' expressions::

    >>> four_gaussians = two_gaussians + two_gaussians
    >>> print(four_gaussians)
    Model: CompoundModel...
    Inputs: ('x',)
    Outputs: ('y',)
    Model set size: 1
    Expression: [0] + [1] + [2] + [3]
    Components:
        [0]: <Gaussian1D(amplitude=1.1, mean=0.1, stddev=0.2)>
    <BLANKLINE>
        [1]: <Gaussian1D(amplitude=2.5, mean=0.5, stddev=0.1)>
    <BLANKLINE>
        [2]: <Gaussian1D(amplitude=1.1, mean=0.1, stddev=0.2)>
    <BLANKLINE>
        [3]: <Gaussian1D(amplitude=2.5, mean=0.5, stddev=0.1)>
    Parameters:
        amplitude_0 mean_0 stddev_0 amplitude_1 ... stddev_2 amplitude_3 mean_3 stddev_3
        ----------- ------ -------- ----------- ... -------- ----------- ------ --------
                1.1    0.1      0.2         2.5 ...      0.2         2.5    0.5      0.1


Operators
---------

Arithmetic operators
--------------------

Compound models can be created from expressions that include any
number of the arithmetic operators ``+``, ``-``, ``*``, ``/``, and
``**``, which have the same meanings as they do for other numeric
objects in Python.

.. note::

    In the case of division ``/`` always means floating point division--integer
    division and the ``//`` operator is not supported for models).

As demonstrated in previous examples, for models that have a single output
the result of evaluating a model like ``A + B`` is to evaluate ``A`` and
``B`` separately on the given input, and then return the sum of the outputs of
``A`` and ``B``.  This requires that ``A`` and ``B`` take the same number of
inputs and both have a single output.

It is also possible to use arithmetic operators between models with multiple
outputs.  Again, the number of inputs must be the same between the models, as
must be the number of outputs.  In this case the operator is applied to the
operators element-wise, similarly to how arithmetic operators work on two Numpy
arrays.


.. _compound-model-composition:

Model composition
-----------------

The sixth binary operator that can be used to create compound models is the
composition operator, also known as the "pipe" operator ``|`` (not to be
confused with the boolean "or" operator that this implements for Python numeric
objects).  A model created with the composition operator like ``M = F | G``,
when evaluated, is equivalent to evaluating :math:`g \circ f = g(f(x))`.

.. note::

    The fact that the ``|`` operator has the opposite sense as the functional
    composition operator :math:`\circ` is sometimes a point of confusion.
    This is in part because there is no operator symbol supported in Python
    that corresponds well to this.  The ``|`` operator should instead be read
    like the `pipe operator
    <https://en.wikipedia.org/wiki/Pipeline_%28Unix%29>`_ of UNIX shell syntax:
    It chains together models by piping the output of the left-hand operand to
    the input of the right-hand operand, forming a "pipeline" of models, or
    transformations.

This has different requirements on the inputs/outputs of its operands than do
the arithmetic operators.  For composition all that is required is that the
left-hand model has the same number of outputs as the right-hand model has
inputs.

For simple functional models this is exactly the same as functional
composition, except for the aforementioned caveat about ordering.  For
example, to create the following compound model:

.. graphviz::

    digraph {
        in0 [shape="none", label="input 0"];
        out0 [shape="none", label="output 0"];
        redshift0 [shape="box", label="RedshiftScaleFactor"];
        gaussian0 [shape="box", label="Gaussian1D(1, 0.75, 0.1)"];

        in0 -> redshift0;
        redshift0 -> gaussian0;
        gaussian0 -> out0;
    }

.. plot::
    :include-source:

    import numpy as np
    import matplotlib.pyplot as plt
    from astropy.modeling.models import RedshiftScaleFactor, Gaussian1D

    x = np.linspace(0, 1.2, 100)
    g0 = RedshiftScaleFactor(0) | Gaussian1D(1, 0.75, 0.1)

    plt.figure(figsize=(8, 5))
    plt.plot(x, g0(x), 'g--', label='$z=0$')

    for z in (0.2, 0.4, 0.6):
        g = RedshiftScaleFactor(z) | Gaussian1D(1, 0.75, 0.1)
        plt.plot(x, g(x), color=plt.cm.OrRd(z),
                 label=f'$z={z}$')

    plt.xlabel('Energy')
    plt.ylabel('Flux')
    plt.legend()

If you wish to perform redshifting in the wavelength space instead of energy,
and would also like to conserve flux, here is another way to do it using
model *instances*:

.. plot::
    :include-source:

    import numpy as np
    import matplotlib.pyplot as plt
    from astropy.modeling.models import RedshiftScaleFactor, Gaussian1D, Scale

    x = np.linspace(1000, 5000, 1000)
    g0 = Gaussian1D(1, 2000, 200)  # No redshift is same as redshift with z=0

    plt.figure(figsize=(8, 5))
    plt.plot(x, g0(x), 'g--', label='$z=0$')

    for z in (0.2, 0.4, 0.6):
        rs = RedshiftScaleFactor(z).inverse  # Redshift in wavelength space
        sc = Scale(1. / (1 + z))  # Rescale the flux to conserve energy
        g = rs | g0 | sc
        plt.plot(x, g(x), color=plt.cm.OrRd(z),
                 label=f'$z={z}$')

    plt.xlabel('Wavelength')
    plt.ylabel('Flux')
    plt.legend()

When working with models with multiple inputs and outputs the same idea
applies.  If each input is thought of as a coordinate axis, then this defines a
pipeline of transformations for the coordinates on each axis (though it does
not necessarily guarantee that these transformations are separable).  For
example:

.. graphviz::

    digraph {
        in0 [shape="none", label="input 0"];
        in1 [shape="none", label="input 1"];
        out0 [shape="none", label="output 0"];
        out1 [shape="none", label="output 1"];
        rot0 [shape="box", label="Rotation2D"];
        gaussian0 [shape="box", label="Gaussian2D(1, 0, 0, 0.1, 0.3)"];

        in0 -> rot0;
        in1 -> rot0;
        rot0 -> gaussian0;
        rot0 -> gaussian0;
        gaussian0 -> out0;
        gaussian0 -> out1;
    }

.. plot::
    :include-source:

    import numpy as np
    import matplotlib.pyplot as plt
    from astropy.modeling.models import Rotation2D, Gaussian2D

    x, y = np.mgrid[-1:1:0.01, -1:1:0.01]

    plt.figure(figsize=(8, 2.5))

    for idx, theta in enumerate((0, 45, 90)):
        g = Rotation2D(theta) | Gaussian2D(1, 0, 0, 0.1, 0.3)
        plt.subplot(1, 3, idx + 1)
        plt.imshow(g(x, y), origin='lower')
        plt.xticks([])
        plt.yticks([])
        plt.title(f'Rotated $ {theta}^\circ $')

.. note::

    The above example is a bit contrived in that
    `~astropy.modeling.functional_models.Gaussian2D` already supports an
    optional rotation parameter.  However, this demonstrates how coordinate
    rotation could be added to arbitrary models.

Normally it is not possible to compose, say, a model with two outputs and a
function of only one input::

    >>> from astropy.modeling.models import Rotation2D
    >>> Rotation2D() | Gaussian1D()  # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
    ...
    ModelDefinitionError: Unsupported operands for |: Rotation2D (n_inputs=2, n_outputs=2) and Gaussian1D (n_inputs=1, n_outputs=1); n_outputs for the left-hand model must match n_inputs for the right-hand model.

However, as we will see in the next section,
:ref:`compound-model-concatenation`, provides a means of creating models
that apply transformations to only some of the outputs from a model,
especially when used in concert with :ref:`mappings <compound-model-mappings>`.


.. _compound-model-concatenation:

Model concatenation
-------------------

The concatenation operator ``&``, sometimes also referred to as a "join",
combines two models into a single, fully separable transformation.  That is, it
makes a new model that takes the inputs to the left-hand model, concatenated
with the inputs to the right-hand model, and returns a tuple consisting of the
two models' outputs concatenated together, without mixing in any way.  In other
words, it simply evaluates the two models in parallel--it can be thought of as
something like a tuple of models.

For example, given two coordinate axes, we can scale each coordinate
by a different factor by concatenating two
`~astropy.modeling.functional_models.Scale` models.

.. graphviz::

    digraph {
        in0 [shape="none", label="input 0"];
        in1 [shape="none", label="input 1"];
        out0 [shape="none", label="output 0"];
        out1 [shape="none", label="output 1"];
        scale0 [shape="box", label="Scale(factor=1.2)"];
        scale1 [shape="box", label="Scale(factor=3.4)"];

        in0 -> scale0;
        scale0 -> out0;

        in1 -> scale1;
        scale1 -> out1;
    }

::

    >>> from astropy.modeling.models import Scale
    >>> separate_scales = Scale(factor=1.2) & Scale(factor=3.4)
    >>> separate_scales(1, 2)  # doctest: +FLOAT_CMP
    (1.2, 6.8)

We can also combine concatenation with composition to build chains of
transformations that use both "1D" and "2D" models on two (or more) coordinate
axes:

.. graphviz::

    digraph {
        in0 [shape="none", label="input 0"];
        in1 [shape="none", label="input 1"];
        out0 [shape="none", label="output 0"];
        out1 [shape="none", label="output 1"];
        scale0 [shape="box", label="Scale(factor=1.2)"];
        scale1 [shape="box", label="Scale(factor=3.4)"];
        rot0 [shape="box", label="Rotation2D(90)"];

        in0 -> scale0;
        scale0 -> rot0;

        in1 -> scale1;
        scale1 -> rot0;

        rot0 -> out0;
        rot0 -> out1;
    }

::

    >>> scale_and_rotate = ((Scale(factor=1.2) & Scale(factor=3.4)) |
    ...                     Rotation2D(90))
    >>> scale_and_rotate.n_inputs
    2
    >>> scale_and_rotate.n_outputs
    2
    >>> scale_and_rotate(1, 2)  # doctest: +FLOAT_CMP
    (-6.8, 1.2)

This is of course equivalent to an
`~astropy.modeling.projections.AffineTransformation2D` with the appropriate
transformation matrix::

    >>> from numpy import allclose
    >>> from astropy.modeling.models import AffineTransformation2D
    >>> affine = AffineTransformation2D(matrix=[[0, -3.4], [1.2, 0]])
    >>> # May be small numerical differences due to different implementations
    >>> allclose(scale_and_rotate(1, 2), affine(1, 2))
    True

Other Topics
============

Model names
-----------

In the above two examples another notable feature of the generated compound
model classes is that the class name, as displayed when printing the class at
the command prompt, is not "TwoGaussians", "FourGaussians", etc.  Instead it is
a generated name consisting of "CompoundModel" followed by an essentially
arbitrary integer that is chosen simply so that every compound model has a
unique default name.  This is a limitation at present, due to the limitation
that it is not generally possible in Python when an object is created by an
expression for it to "know" the name of the variable it will be assigned to, if
any.
It is possible to directly assign a name to the compound model instance
by using the `Model.name <astropy.modeling.Model.name>` attribute::

    >>> two_gaussians.name = "TwoGaussians"
    >>> print(two_gaussians)  # doctest: +SKIP
    Model: CompoundModel...
    Name: TwoGaussians
    Inputs: ('x',)
    Outputs: ('y',)
    Model set size: 1
    Expression: [0] + [1]
    Components:
        [0]: <Gaussian1D(amplitude=1.1, mean=0.1, stddev=0.2)>
        <BLANKLINE>
        [1]: <Gaussian1D(amplitude=2.5, mean=0.5, stddev=0.1)>
    Parameters:
        amplitude_0 mean_0 stddev_0 amplitude_1 mean_1 stddev_1
        ----------- ------ -------- ----------- ------ --------
                1.1    0.1      0.2         2.5    0.5      0.1

.. _compound-model-indexing:

Indexing and slicing
--------------------

As seen in some of the previous examples in this document, when creating a
compound model each component of the model is assigned an integer index
starting from zero.  These indices are assigned simply by reading the
expression that defined the model, from left to right, regardless of the order
of operations.  For example::

    >>> from astropy.modeling.models import Const1D
    >>> A = Const1D(1.1, name='A')
    >>> B = Const1D(2.1, name='B')
    >>> C = Const1D(3.1, name='C')
    >>> M = A + B * C
    >>> print(M)
    Model: CompoundModel...
    Inputs: ('x',)
    Outputs: ('y',)
    Model set size: 1
    Expression: [0] + [1] * [2]
    Components:
        [0]: <Const1D(amplitude=1.1, name='A')>
    <BLANKLINE>
        [1]: <Const1D(amplitude=2.1, name='B')>
    <BLANKLINE>
        [2]: <Const1D(amplitude=3.1, name='C')>
    Parameters:
        amplitude_0 amplitude_1 amplitude_2
        ----------- ----------- -----------
                1.1         2.1         3.1


In this example the expression is evaluated ``(B * C) + A``--that is, the
multiplication is evaluated before the addition per usual arithmetic rules.
However, the components of this model are simply read off left to right from
the expression ``A + B * C``, with ``A -> 0``, ``B -> 1``, ``C -> 2``.  If we
had instead defined ``M = C * B + A`` then the indices would be reversed
(though the expression is mathematically equivalent).  This convention is
chosen for simplicity--given the list of components it is not necessary to
jump around when mentally mapping them to the expression.

We can pull out each individual component of the compound model ``M`` by using
indexing notation on it.  Following from the above example, ``M[1]`` should
return the model ``B``::

    >>> M[1]
    <Const1D(amplitude=2.1, name='B')>

We can also take a *slice* of the compound model.  This returns a new compound
model that evaluates the *subexpression* involving the models selected by the
slice.  This follows the same semantics as slicing a `list` or array in Python.
The start point is inclusive and the end point is exclusive.  So a slice like
``M[1:3]`` (or just ``M[1:]``) selects models ``B`` and ``C`` (and all
*operators* between them).  So the resulting model evaluates just the
subexpression ``B * C``::

    >>> print(M[1:])
    Model: CompoundModel
    Inputs: ('x',)
    Outputs: ('y',)
    Model set size: 1
    Expression: [0] * [1]
    Components:
        [0]: <Const1D(amplitude=2.1, name='B')>
    <BLANKLINE>
        [1]: <Const1D(amplitude=3.1, name='C')>
    Parameters:
        amplitude_0 amplitude_1
        ----------- -----------
                2.1         3.1

.. note::

    There is a change in the parameter names of a slice from versions
    prior to 4.0. Previously, the parameter names were identical to that
    of the model being sliced. Now, they are what is expected for a
    compound model of this type apart from the model sliced. That is,
    the sliced model always starts with its own relative index for its
    components, thus the parameter names start with a 0 suffix.

.. note::

    Starting with 4.0, the behavior of slicing is more restrictive than
    previously. For example if::

        m = m1 * m2 + m3

    and one sliced by
    using ``m[1:3]`` previously that would return the model: ``m2 + m3``
    even though there was never any such submodel of m. Starting with 4.0
    a slice must correspond to a submodel (something that corresponds
    to an intermediate result of the computational chain of evaluating
    the compound model). So::

        m1 * m2

    is a submodel (i.e.,``m[:2]``) but
    ``m[1:3]`` is not. Currently this also means that in simpler expressions
    such as::

        m = m1 + m2 + m3 + m4

    where any slice should be valid in
    principle, only slices that include m1 are since it is part of
    all submodules (since the order of evaluation is::

        ((m1 + m2) + m3) + m4

    Anyone creating compound models that wishes submodels to be available
    is advised to use parentheses explicitly  or define intermediate
    models to be used in subsequent expressions so that they can be
    extracted with a slice or simple index depending on the context.
    For example, to make ``m2 + m3`` accessible by slice define ``m`` as::

        m = m1 + (m2 + m3) + m4. In this case ``m[1:3]`` will work.

The new compound model for the subexpression can be evaluated
like any other::

    >>> M[1:](0)  # doctest: +FLOAT_CMP
    6.51

Although the model ``M`` was composed entirely of ``Const1D`` models in this
example, it was useful to give each component a unique name (``A``, ``B``,
``C``) in order to differentiate between them.  This can also be used for
indexing and slicing::

    >>> print(M['B'])
    Model: Const1D
    Name: B
    Inputs: ('x',)
    Outputs: ('y',)
    Model set size: 1
    Parameters:
        amplitude
        ---------
              2.1


In this case ``M['B']`` is equivalent to ``M[1]``.  But by using the name we do
not have to worry about what index that component is in (this becomes
especially useful when combining multiple compound models).  A current
limitation, however, is that each component of a compound model must have a
unique name--if some components have duplicate names then they can only be
accessed by their integer index.

Slicing also works with names.  When using names the start and end points are
*both inclusive*::

    >>> print(M['B':'C'])
    Model: CompoundModel...
    Inputs: ('x',)
    Outputs: ('y',)
    Model set size: 1
    Expression: [0] * [1]
    Components:
        [0]: <Const1D(amplitude=2.1, name='B')>
    <BLANKLINE>
        [1]: <Const1D(amplitude=3.1, name='C')>
    Parameters:
        amplitude_0 amplitude_1
        ----------- -----------
                2.1         3.1

So in this case ``M['B':'C']`` is equivalent to ``M[1:3]``.

.. _compound-model-parameters:

Parameters
----------

A question that frequently comes up when first encountering compound models is
how exactly all the parameters are dealt with.  By now we've seen a few
examples that give some hints, but a more detailed explanation is in order.
This is also one of the biggest areas for possible improvements--the current
behavior is meant to be practical, but is not ideal.  (Some possible
improvements include being able to rename parameters, and providing a means of
narrowing down the number of parameters in a compound model.)

As explained in the general documentation for model :ref:`parameters
<modeling-parameters>`, every model has an attribute called
`~astropy.modeling.Model.param_names` that contains a tuple of all the model's
adjustable parameters.  These names are given in a canonical order that also
corresponds to the order in which the parameters should be specified when
instantiating the model.

The simple scheme used currently for naming parameters in a compound model is
this:  The ``param_names`` from each component model are concatenated with each
other in order from left to right as explained in the section on
:ref:`compound-model-indexing`.  However, each parameter name is appended with
``_<#>``, where ``<#>`` is the index of the component model that parameter
belongs to.  For example::

    >>> Gaussian1D.param_names
    ('amplitude', 'mean', 'stddev')
    >>> (Gaussian1D() + Gaussian1D()).param_names
    ('amplitude_0', 'mean_0', 'stddev_0', 'amplitude_1', 'mean_1', 'stddev_1')

For consistency's sake, this scheme is followed even if not all of the
components have overlapping parameter names::

    >>> from astropy.modeling.models import RedshiftScaleFactor
    >>> (RedshiftScaleFactor() | (Gaussian1D() + Gaussian1D())).param_names
    ('z_0', 'amplitude_1', 'mean_1', 'stddev_1', 'amplitude_2', 'mean_2',
    'stddev_2')

On some level a scheme like this is necessary in order for the compound model
to maintain some consistency with other models with respect to the interface to
its parameters.  However, if one gets lost it is also possible to take
advantage of :ref:`indexing <compound-model-indexing>` to make things easier.
When returning a single component from a compound model the parameters
associated with that component are accessible through their original names, but
are still tied back to the compound model::

    >>> a = Gaussian1D(1, 0, 0.2, name='A')
    >>> b = Gaussian1D(2.5, 0.5, 0.1, name='B')
    >>> m = a + b
    >>> m.amplitude_0
    Parameter('amplitude', value=1.0)

is equivalent to::

    >>> m['A'].amplitude
    Parameter('amplitude', value=1.0)

You can think of these both as different "views" of the same parameter.
Updating one updates the other::

    >>> m.amplitude_0 = 42
    >>> m['A'].amplitude
    Parameter('amplitude', value=42.0)
    >>> m['A'].amplitude = 99
    >>> m.amplitude_0
    Parameter('amplitude', value=99.0)

Note, however, that the original
`~astropy.modeling.functional_models.Gaussian1D` instance ``a`` has been
updated::

    >>> a.amplitude
    Parameter('amplitude', value=99.0)

This is different than the behavior in versions prior to 4.0. Now compound model
parameters share the same Parameter instance as the original model.


.. _compound-model-mappings:

Advanced mappings
-----------------

We have seen in some previous examples how models can be chained together to
form a "pipeline" of transformations by using model :ref:`composition
<compound-model-composition>` and :ref:`concatenation
<compound-model-concatenation>`.  To aid the creation of more complex chains of
transformations (for example for a WCS transformation) a new class of
"`mapping <astropy.modeling.mappings>`" models is provided.

Mapping models do not (currently) take any parameters, nor do they perform any
numeric operation.  They are for use solely with the :ref:`concatenation
<compound-model-concatenation>` (``&``) and :ref:`composition
<compound-model-composition>` (``|``) operators, and can be used to control how
the inputs and outputs of models are ordered, and how outputs from one model
are mapped to inputs of another model in a composition.

Currently there are only two mapping models:
`~astropy.modeling.mappings.Identity`, and (the somewhat generically named)
`~astropy.modeling.mappings.Mapping`.

The `~astropy.modeling.mappings.Identity` mapping simply passes one or more
inputs through, unchanged.  It must be instantiated with an integer specifying
the number of inputs/outputs it accepts.  This can be used to trivially expand
the "dimensionality" of a model in terms of the number of inputs it accepts.
In the section on :ref:`concatenation <compound-model-concatenation>` we saw
an example like::

    >>> m = (Scale(1.2) & Scale(3.4)) | Rotation2D(90)


.. graphviz::

    digraph {
        in0 [shape="none", label="input 0"];
        in1 [shape="none", label="input 1"];
        out0 [shape="none", label="output 0"];
        out1 [shape="none", label="output 1"];
        scale0 [shape="box", label="Scale(factor=1.2)"];
        scale1 [shape="box", label="Scale(factor=3.4)"];
        rot0 [shape="box", label="Rotation2D(90)"];

        in0 -> scale0;
        scale0 -> rot0;

        in1 -> scale1;
        scale1 -> rot0;

        rot0 -> out0;
        rot0 -> out1;
    }

where two coordinate inputs are scaled individually and then rotated into each
other.  However, say we wanted to scale only one of those coordinates.  It
would be fine to simply use ``Scale(1)`` for one them, or any other model that
is effectively a no-op.  But that also adds unnecessary computational overhead,
so we might as well simply specify that that coordinate is not to be scaled or
transformed in any way.  This is a good use case for
`~astropy.modeling.mappings.Identity`:

.. graphviz::

    digraph {
        in0 [shape="none", label="input 0"];
        in1 [shape="none", label="input 1"];
        out0 [shape="none", label="output 0"];
        out1 [shape="none", label="output 1"];
        scale0 [shape="box", label="Scale(factor=1.2)"];
        identity0 [shape="box", label="Identity(1)"];
        rot0 [shape="box", label="Rotation2D(90)"];

        in0 -> scale0;
        scale0 -> rot0;

        in1 -> identity0;
        identity0 -> rot0;

        rot0 -> out0;
        rot0 -> out1;
    }

::

    >>> from astropy.modeling.models import Identity
    >>> m = Scale(1.2) & Identity(1)
    >>> m(1, 2)  # doctest: +FLOAT_CMP
    (1.2, 2.0)


This scales the first input, and passes the second one through unchanged.  We
can use this to build up more complicated steps in a many-axis WCS
transformation.  If for example we had 3 axes and only wanted to scale the
first one:

.. graphviz::

    digraph {
        in0 [shape="none", label="input 0"];
        in1 [shape="none", label="input 1"];
        in2 [shape="none", label="input 2"];
        out0 [shape="none", label="output 0"];
        out1 [shape="none", label="output 1"];
        out2 [shape="none", label="output 2"];
        scale0 [shape="box", label="Scale(1.2)"];
        identity0 [shape="box", label="Identity(2)"];

        in0 -> scale0;
        scale0 -> out0;

        in1 -> identity0;
        in2 -> identity0;
        identity0 -> out1;
        identity0 -> out2;
    }

::

    >>> m = Scale(1.2) & Identity(2)
    >>> m(1, 2, 3)  # doctest: +FLOAT_CMP
    (1.2, 2.0, 3.0)

(Naturally, the last example could also be written out ``Scale(1.2) &
Identity(1) & Identity(1)``.)

The `~astropy.modeling.mappings.Mapping` model is similar in that it does not
modify any of its inputs.  However, it is more general in that it allows inputs
to be duplicated, reordered, or even dropped outright.  It is instantiated with
a single argument: a `tuple`, the number of items of which correspond to the
number of outputs the `~astropy.modeling.mappings.Mapping` should produce.  A
1-tuple means that whatever inputs come in to the
`~astropy.modeling.mappings.Mapping`, only one will be output.  And so on for
2-tuple or higher (though the length of the tuple cannot be greater than the
number of inputs--it will not pull values out of thin air).  The elements of
this mapping are integers corresponding to the indices of the inputs.  For
example, a mapping of ``Mapping((0,))`` is equivalent to ``Identity(1)``--it
simply takes the first (0-th) input and returns it:

.. graphviz::

    digraph G {
        in0 [shape="none", label="input 0"];

        subgraph cluster_A {
            shape=rect;
            color=black;
            label="(0,)";

            a [shape=point, label=""];
        }

        out0 [shape="none", label="output 0"];

        in0 -> a;
        a -> out0;
    }

::

    >>> from astropy.modeling.models import Mapping
    >>> m = Mapping((0,))
    >>> m(1.0)
    1.0

Likewise ``Mapping((0, 1))`` is equivalent to ``Identity(2)``, and so on.
However, `~astropy.modeling.mappings.Mapping` also allows outputs to be
reordered arbitrarily:

.. graphviz::

    digraph G {
        {
            rank=same;
            in0 [shape="none", label="input 0"];
            in1 [shape="none", label="input 1"];
        }

        subgraph cluster_A {
            shape=rect;
            color=black;
            label="(1, 0)";

            {
                rank=same;
                a [shape=point, label=""];
                b [shape=point, label=""];
            }

            {
                rank=same;
                c [shape=point, label=""];
                d [shape=point, label=""];
            }

            a -> c [style=invis];
            a -> d [constraint=false];
            b -> c [constraint=false];
        }

        {
            rank=same;
            out0 [shape="none", label="output 0"];
            out1 [shape="none", label="output 1"];
        }

        in0 -> a;
        in1 -> b;
        c -> out0;
        d -> out1;
    }

::

    >>> m = Mapping((1, 0))
    >>> m(1.0, 2.0)
    (2.0, 1.0)

.. graphviz::

    digraph G {
        {
            rank=same;
            in0 [shape="none", label="input 0"];
            in1 [shape="none", label="input 1"];
            in2 [shape="none", label="input 2"];
        }

        subgraph cluster_A {
            shape=rect;
            color=black;
            label="(1, 0, 2)";

            {
                rank=same;
                a [shape=point, label=""];
                b [shape=point, label=""];
                c [shape=point, label=""];
            }

            {
                rank=same;
                d [shape=point, label=""];
                e [shape=point, label=""];
                f [shape=point, label=""];
            }

            a -> d [style=invis];
            a -> e [constraint=false];
            b -> d [constraint=false];
            c -> f [constraint=false];
        }

        {
            rank=same;
            out0 [shape="none", label="output 0"];
            out1 [shape="none", label="output 1"];
            out2 [shape="none", label="output 2"];
        }

        in0 -> a;
        in1 -> b;
        in2 -> c;
        d -> out0;
        e -> out1;
        f -> out2;
    }

::

    >>> m = Mapping((1, 0, 2))
    >>> m(1.0, 2.0, 3.0)
    (2.0, 1.0, 3.0)

Outputs may also be dropped:

.. graphviz::

    digraph G {
        {
            rank=same;
            in0 [shape="none", label="input 0"];
            in1 [shape="none", label="input 1"];
        }

        subgraph cluster_A {
            shape=rect;
            color=black;
            label="(1,)";

            {
                rank=same;
                a [shape=point, label=""];
                b [shape=point, label=""];
            }

            {
                rank=same;
                c [shape=point, label=""];
            }

            a -> c [style=invis];
            b -> c [constraint=false];
        }

        out0 [shape="none", label="output 0"];

        in0 -> a;
        in1 -> b;
        c -> out0;
    }

::

    >>> m = Mapping((1,))
    >>> m(1.0, 2.0)
    2.0

.. graphviz::

    digraph G {
        {
            rank=same;
            in0 [shape="none", label="input 0"];
            in1 [shape="none", label="input 1"];
            in2 [shape="none", label="input 2"];
        }

        subgraph cluster_A {
            shape=rect;
            color=black;
            label="(0, 2)";

            {
                rank=same;
                a [shape=point, label=""];
                b [shape=point, label=""];
                c [shape=point, label=""];
            }

            {
                rank=same;
                d [shape=point, label=""];
                e [shape=point, label=""];
            }

            a -> d [style=invis];
            a -> d [constraint=false];
            c -> e [constraint=false];
        }

        {
            rank=same;
            out0 [shape="none", label="output 0"];
            out1 [shape="none", label="output 1"];
        }

        in0 -> a;
        in1 -> b;
        in2 -> c;
        d -> out0;
        e -> out1;
    }

::

    >>> m = Mapping((0, 2))
    >>> m(1.0, 2.0, 3.0)
    (1.0, 3.0)

Or duplicated:

.. graphviz::

    digraph G {
        in0 [shape="none", label="input 0"];

        subgraph cluster_A {
            shape=rect;
            color=black;
            label="(0, 0)";

            a [shape=point, label=""];

            {
                rank=same;
                b [shape=point, label=""];
                c [shape=point, label=""];
            }

            a -> b [style=invis];
            a -> b [constraint=false];
            a -> c [constraint=false];
        }

        {
            rank=same;
            out0 [shape="none", label="output 0"];
            out1 [shape="none", label="output 1"];
        }

        in0 -> a;
        b -> out0;
        c -> out1;
    }

::

    >>> m = Mapping((0, 0))
    >>> m(1.0)
    (1.0, 1.0)

.. graphviz::

    digraph G {
        {
            rank=same;
            in0 [shape="none", label="input 0"];
            in1 [shape="none", label="input 1"];
            in2 [shape="none", label="input 2"];
        }

        subgraph cluster_A {
            shape=rect;
            color=black;
            label="(0, 1, 1, 2)";

            {
                rank=same;
                a [shape=point, label=""];
                b [shape=point, label=""];
                c [shape=point, label=""];
            }

            {
                rank=same;
                d [shape=point, label=""];
                e [shape=point, label=""];
                f [shape=point, label=""];
                g [shape=point, label=""];
            }

            a -> d [style=invis];
            a -> d [constraint=false];
            b -> e [constraint=false];
            b -> f [constraint=false];
            c -> g [constraint=false];
        }

        {
            rank=same;
            out0 [shape="none", label="output 0"];
            out1 [shape="none", label="output 1"];
            out2 [shape="none", label="output 2"];
            out3 [shape="none", label="output 3"];
        }

        in0 -> a;
        in1 -> b;
        in2 -> c;
        d -> out0;
        e -> out1;
        f -> out2;
        g -> out3;
    }

::

    >>> m = Mapping((0, 1, 1, 2))
    >>> m(1.0, 2.0, 3.0)
    (1.0, 2.0, 2.0, 3.0)


A complicated example that performs multiple transformations, some separable,
some not, on three coordinate axes might look something like:

.. graphviz::

    digraph G {
        {
            rank=same;
            in0 [shape="none", label="input 0"];
            in1 [shape="none", label="input 1"];
            in2 [shape="none", label="input 2"];
        }

        {
            rank=same;
            poly0 [shape=rect, label="Poly1D(3, c0=1, c3=1)"];
            identity0 [shape=rect, label="Identity(1)"];
            poly1 [shape=rect, label="Poly1D(2, c2=1)"];
        }

        subgraph cluster_A {
            shape=rect;
            color=black;
            label="(0, 2, 1)";

            {
                rank=same;
                a [shape=point, label=""];
                b [shape=point, label=""];
                c [shape=point, label=""];
            }

            {
                rank=same;
                d [shape=point, label=""];
                e [shape=point, label=""];
                f [shape=point, label=""];
            }

            a -> d [style=invis];
            d -> e [style=invis];
            a -> d [constraint=false];
            c -> e [constraint=false];
            b -> f [constraint=false];
        }

        poly2 [shape="rect", label="Poly2D(4, c0_0=1, c1_1=1, c2_2=2)"];
        gaussian0 [shape="rect", label="Gaussian1D(1, 0, 4)"];

        {
            rank=same;
            out0 [shape="none", label="output 0"];
            out1 [shape="none", label="output 1"];
        }

        in0 -> poly0;
        in1 -> identity0;
        in2 -> poly1;
        poly0 -> a;
        identity0 -> b;
        poly1 -> c;
        d -> poly2;
        e -> poly2;
        f -> gaussian0;
        poly2 -> out0;
        gaussian0 -> out1;
    }

::

    >>> from astropy.modeling.models import Polynomial1D as Poly1D
    >>> from astropy.modeling.models import Polynomial2D as Poly2D
    >>> m = ((Poly1D(3, c0=1, c3=1) & Identity(1) & Poly1D(2, c2=1)) |
    ...      Mapping((0, 2, 1)) |
    ...      (Poly2D(4, c0_0=1, c1_1=1, c2_2=2) & Gaussian1D(1, 0, 4)))
    ...
    >>> m(2, 3, 4)  # doctest: +FLOAT_CMP
    (41617.0, 0.7548396019890073)



This expression takes three inputs: :math:`x`, :math:`y`, and :math:`z`.  It
first takes :math:`x \rightarrow x^3 + 1` and :math:`z \rightarrow z^2`.
Then it remaps the axes so that :math:`x` and :math:`z` are passed in to the
`~astropy.modeling.polynomial.Polynomial2D` to evaluate
:math:`2x^2z^2 + xz + 1`, while simultaneously evaluating a Gaussian on
:math:`y`.  The end result is a reduction down to two coordinates.  You can
confirm for yourself that the result is correct.

This opens up the possibility of essentially arbitrarily complex transformation
graphs.  Currently the tools do not exist to make it easy to navigate and
reason about highly complex compound models that use these mappings, but that
is a possible enhancement for future versions.

.. _model-reduction:

Model Reduction
---------------

In order to save much duplication in the construction of complex models, it is
possible to define one complex model that covers all cases where the
variables that distinguish the models are made part of the model's input
variables. The ``fix_inputs`` function allows defining models derived from
the more complex one by setting one or more of the inputs to a constant
value. Examples of this sort of situation arise when working out
the transformations from detector pixel to RA, Dec, and lambda for
spectrographs when the slit locations may be moved (e.g., fiber fed or
commandable slit masks), or different orders may be selected (e.g., Eschelle).
In the case of order, one may have a function of pixel ``x``, ``y``, ``spectral_order``
that map into ``RA``, ``Dec`` and ``lambda``. Without specifying ``spectral_order``, it is
ambiguous what ``RA``, ``Dec`` and ``Lambda`` corresponds to a pixel location. It
is usually possible to define a function of all three inputs. Presuming
this model is ``general_transform`` then ``fix_inputs`` may be used to define
the transform for a specific order as follows:

::
     >>> order1_transform = fix_inputs(general_transform, {'order': 1})  # doctest: +SKIP

creates a new compound model that takes only pixel position and generates
``RA``, ``Dec``, and ``lambda``. The ``fix_inputs`` function can be used to set input
values by position (0 is the first) or by input variable name, and more
than one can be set in the dictionary supplied.

If the input model has a bounding_box, the generated model will have the
bounding for the input coordinate removed.


.. test_replace_submodel

Replace submodels
-----------------


:meth:`~astropy.modeling.core.CompoundModel.replace_submodel` creates a new model by
replacing a submodel with a matching name with another submodel. The number of
inputs and outputs of the old and new submodels should match.
::

    >>> from astropy.modeling import models
    >>> shift = models.Shift(-1) & models.Shift(-1)
    >>> scale = models.Scale(2) & models.Scale(3)
    >>> scale.name = "Scale"
    >>> model = shift | scale
    >>> model(2, 1)  # doctest: +FLOAT_CMP
    (2.0, 0.0)
    >>> new_model = model.replace_submodel('Scale', models.Rotation2D(90, name='Rotation'))
    >>> new_model(2, 1)  # doctest: +FLOAT_CMP
    (6.12e-17, 1.0)
.. include:: links.inc

.. _astropy-modeling:

***************************************
Models and Fitting (`astropy.modeling`)
***************************************

Introduction
============

`astropy.modeling` provides a framework for representing models and performing
model evaluation and fitting.  A number of predefined 1-D and 2-D models are
provided and the capability for custom, user defined models is supported.
Different fitting algorithms can be used with any model.  For those fitters
with the capabilities fitting can be done using uncertainties, parameters with
bounds, and priors.

.. _modeling-using:

Using Modeling
==============

.. toctree::
   :maxdepth: 2

   Models <models.rst>
   Compound Models <compound-models.rst>
   Model Parameters <parameters.rst>
   Fitting <fitting.rst>
   Using Units with Models and Fitting <units.rst>


.. _getting-started-example:

A Simple Example
================

This simple example illustrates defining a model,
calculating values based on input x values, and using fitting data with a model.

   .. plot::
       :include-source:

       import numpy as np
       import matplotlib.pyplot as plt
       from astropy.modeling import models, fitting

       # define a model for a line
       line_orig = models.Linear1D(slope=1.0, intercept=0.5)

       # generate x, y data non-uniformly spaced in x
       # add noise to y measurements
       npts = 30
       np.random.seed(10)
       x = np.random.uniform(0.0, 10.0, npts)
       y = line_orig(x)
       y += np.random.normal(0.0, 1.5, npts)

       # initialize a linear fitter
       fit = fitting.LinearLSQFitter()

       # initialize a linear model
       line_init = models.Linear1D()

       # fit the data with the fitter
       fitted_line = fit(line_init, x, y)

       # plot the model
       plt.figure()
       plt.plot(x, y, 'ko', label='Data')
       plt.plot(x, fitted_line(x), 'k-', label='Fitted Model')
       plt.xlabel('x')
       plt.ylabel('y')
       plt.legend()

.. _advanced_topics:

Advanced Topics
===============

.. toctree::
   :maxdepth: 2

   Performance Tips <performance.rst>
   Extending Models <new-model.rst>
   Extending Fitters <new-fitter.rst>
   Adding support for units to models <add-units.rst>
   Joint Fitting <jointfitter.rst>


Pre-Defined Models
==================

.. To be expanded to include all pre-defined models

Some of the pre-defined models are listed and illustrated.

.. toctree::
   :maxdepth: 2

   1D Models <predef_models1D.rst>
   2D Models <predef_models2D.rst>
   Physical Models <physical_models.rst>
   Polynomial Models <polynomial_models.rst>
   Spline Models <spline_models.rst>

Examples
========

.. toctree::
   :maxdepth: 2

   Fitting a line <example-fitting-line>
   example-fitting-constraints
   example-fitting-model-sets

.. TODO list
    fitting with masks
    fitting with priors
    fitting with units
    defining 1d model
    defining 2d model
    fitting 2d model
    defining and using a WCS/gWCS model
    defining and using a Tabular1D model
    statistics functions and how to make your own
    compound models


Reference/API
=============

.. toctree::
   :maxdepth: 1

   reference_api
.. _predef_models1D:

*********
1D Models
*********

Operations
==========

These models perform simple mathematical operations.

- :class:`~astropy.modeling.functional_models.Const1D` model returns the
  constant replicated by the number of input x values.

- :class:`~astropy.modeling.functional_models.Multiply` model multiples the
  input x values by a factor and propagates units if the factor is
  a :class:`~astropy.units.Quantity`.

- :class:`~astropy.modeling.functional_models.RedshiftScaleFactor` model
  multiples the input x values by a (1 + z) factor.

- :class:`~astropy.modeling.functional_models.Scale` model multiples by a
  factor without changing the units of the result.

- :class:`~astropy.modeling.functional_models.Shift` model adds a constant
  to the input x values.

Shapes
======

These models provide shapes, often used to model general x, y data.

- :class:`~astropy.modeling.functional_models.Linear1D` model provides a
  line parameterizied by the slope and y-intercept

- :class:`~astropy.modeling.functional_models.Sine1D` model provides a sine
  parameterized by an amplitude, frequency, and phase.

- :class:`~astropy.modeling.functional_models.Cosine1D` model provides a
  cosine parameterized by an amplitude, frequency, and phase.

.. plot::

    import numpy as np
    import matplotlib.pyplot as plt

    from astropy.modeling.models import (Linear1D, Sine1D, Cosine1D)

    x = np.linspace(-4.0, 6.0, num=100)

    fig, sax = plt.subplots(ncols=3, figsize=(10, 5))
    ax = sax.flatten()

    linemod = Linear1D(slope=2., intercept=1.)
    ax[0].plot(x, linemod(x), label="Linear1D")

    sinemod = Sine1D(amplitude=10., frequency=0.5, phase=0.)
    ax[1].plot(x, sinemod(x), label="Sine1D")
    ax[1].set_ylim(-11.0, 13.0)

    cosinemod = Cosine1D(amplitude=10., frequency=0.5, phase=0)
    ax[2].plot(x, cosinemod(x), label="Cosine1D")
    ax[2].set_ylim(-11.0, 13.0)

    for k in range(3):
        ax[k].set_xlabel("x")
        ax[k].set_ylabel("y")
        ax[k].legend()

    plt.tight_layout()
    plt.show()

Profiles
========

These models provide profiles, often used for lines in spectra.

- :class:`~astropy.modeling.functional_models.Box1D` model computes a box
  function with an amplitude centered at x_0 with the specified width.

- :class:`~astropy.modeling.functional_models.Gaussian1D` model computes
  a Gaussian with an amplitude centered at x_0 with the specified width.

- :class:`~astropy.modeling.functional_models.KingProjectedAnalytic1D` model
  computes the analytic form of the a King model with an amplitude and
  core and tidal radii.

- :class:`~astropy.modeling.functional_models.Lorentz1D` model computes
  a Lorentzian with an amplitude centered at x_0 with the specified width.

- :class:`~astropy.modeling.functional_models.RickerWavelet1D` model computes
  a RickerWavelet function with an amplitude centered at x_0 with the specified width.

- :class:`~astropy.modeling.functional_models.Moffat1D` model computes a
  Moffat function with an amplitude centered at x_0 with the specified width.

- :class:`~astropy.modeling.functional_models.Sersic1D` model
  computes a Sersic model with an amplitude with an effective radius and
  the specified sersic index.

- :class:`~astropy.modeling.functional_models.Trapezoid1D` model computes a
  box with sloping sides with an amplitude centered at x_0 with the specified
  width and sides with the specified slope.

- :class:`~astropy.modeling.functional_models.Voigt1D` model computes a
  Voigt function with an amplitude centered at x_0 with the specified
  Lorentzian and Gaussian widths.

.. plot::

    import numpy as np
    import matplotlib.pyplot as plt

    from astropy.modeling.models import (
        Box1D,
        Gaussian1D,
        RickerWavelet1D,
        Moffat1D,
        Lorentz1D,
        Sersic1D,
        Trapezoid1D,
        KingProjectedAnalytic1D,
        Voigt1D,
    )

    x = np.linspace(-4.0, 6.0, num=100)
    r = np.logspace(-1.0, 2.0, num=100)

    fig, sax = plt.subplots(nrows=3, ncols=3, figsize=(10, 10))
    ax = sax.flatten()

    mods = [
        Box1D(amplitude=10.0, x_0=1.0, width=1.0),
        Gaussian1D(amplitude=10.0, mean=1.0, stddev=1.0),
        KingProjectedAnalytic1D(amplitude=10.0, r_core=1.0, r_tide=10.0),
        Lorentz1D(amplitude=10.0, x_0=1.0, fwhm=1.0),
        RickerWavelet1D(amplitude=10.0, x_0=1.0, sigma=1.0),
        Moffat1D(amplitude=10.0, x_0=1.0, gamma=1.0, alpha=1.),
        Sersic1D(amplitude=10.0, r_eff=1.0 / 2.0, n=5),
        Trapezoid1D(amplitude=10.0, x_0=1.0, width=1.0, slope=5.0),
        Voigt1D(amplitude_L=10.0, x_0=1.0, fwhm_L=1.0, fwhm_G=1.0),
    ]

    for k, mod in enumerate(mods):
        cname = mod.__class__.__name__
        ax[k].set_title(cname)
        if cname in ["KingProjectedAnalytic1D", "Sersic1D"]:
            ax[k].plot(r, mod(r))
            ax[k].set_xscale("log")
            ax[k].set_yscale("log")
        else:
            ax[k].plot(x, mod(x))

    for k in range(len(mods)):
        ax[k].set_xlabel("x")
        ax[k].set_ylabel("y")

    # remove axis for any plots not used
    for k in range(len(mods), len(ax)):
        ax[k].axis("off")

    plt.tight_layout()
    plt.show()
.. _add_units:

Adding support for units in a model (Advanced)
==============================================

Evaluation
----------

To make it so that your models can accept parameters with units and be evaluated
using inputs with units, you need to make sure that the
:meth:`~astropy.modeling.Model.evaluate` method works correctly with
input values and parameters with units. For simple arithmetic, this may work
out of the box since :class:`~astropy.units.Quantity` objects are understood by
a number of Numpy functions.

If users of your models provide input during evaluation that is not compatible
with the parameter units, they may get cryptic errors such as::

    UnitsError : Can only apply 'subtract' function to dimensionless quantities
    when other argument is not a quantity (unless the latter is all
    zero/infinity/nan)

There are several attributes or properties that can be set on models that adjust
the behavior of models with units. These attributes can be changed from the
defaults in the class definition, e.g.::

    class MyModel(Model):
        input_units = {'x': u.deg}
        ...

Note that these are all optional.

.. _models_input_units:

``input_units``
^^^^^^^^^^^^^^^

You can easily add checking of the input units by adding an ``input_units``
property or attribute on your model class. This should return either `None` (to
indicate no constraints) or a dictionary where the keys are the input names
(e.g. ``x`` for many 1D models) and the values are the units expected, which can
be a function of the parameter units::

    @property
    def input_units(self):
        if self.mean.unit is None:
            return None
        else:
            return {'x': self.mean.unit}

If the user then gives values with incorrect input units, a clear error will be
displayed::

    UnitsError: Units of input 'x', (dimensionless), could not be converted to
    required input units of m (length)

Note that the input units don't have to match exactly those returned by
``input_units``, but be convertible to them. In addition, ``input_units`` can
also be specified as an attribute rather than a property in simple cases::

    input_units = {'x': u.deg}

.. _models_return_units:

``return_units``
^^^^^^^^^^^^^^^^

Similarly to :ref:`models_input_units`, this should be dictionary that maps the return
values of a model to units. If :meth:`~astropy.modeling.Model.evaluate` was called
with quantities but returns unitless values, the units are added to the output.
If the return values are quantities in different units, they are converted to
``return_units``.

``input_units_strict``
^^^^^^^^^^^^^^^^^^^^^^

If set to `True`, values that are passed in compatible units will be converted
to the exact units specified in ``input_units``.

This attribute can also be a
dictionary that maps input names to a Boolean to enable converting of that input
to the specified unit.

``input_units_equivalencies``
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

This can be set to a dictionary that maps the input names to a list of
equivalencies, for example::

    input_units_equivalencies = {'nu': u.spectral()}

``_input_units_allow_dimensionless``
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

If set to `True`, values that are plain scalars or Numpy arrays can be passed to
evaluate even if ``input_units`` specifies that the input should have units. It
is up to the :meth:`~astropy.modeling.Model.evaluate` to then decide how to
handle these dimensionless values. This attribute can also be a dictionary that
maps input names to a Boolean to enable passing dimensionless values to
:meth:`~astropy.modeling.Model.evaluate` for that input.


Fitting
-------

To allow models with parameters that have units to be fitted to data with units,
you will need to add a method called ``_parameter_units_for_data_units`` to your
model class. This should take two arguments ``input_units`` and
``output_units`` - ``input_units`` will be set to a dictionary with
the units of the independent variables in the data, while ``output_units`` will
be set to a dictionary with the units the dependent variables in the data (for
example, for a simple 1D model, ``input_units`` will have one key, ``x``, and
``output_units`` will have one key, ``y``). This method should then return
a dictionary giving for each parameter the units the parameter should be
converted to so that the model could be used on the data if units were removed
from both the models and the data. The following example shows the
implementation for the 1D Gaussian::

    def _parameter_units_for_data_units(self, inputs_unit, outputs_unit):
        return {'mean': inputs_unit['x'],
                'stddev': inputs_unit['x'],
                'amplitude': outputs_unit['y']}

With this method in place, the model can then be fit to data that has units.
.. _modeling-new-classes:

**************************
Defining New Model Classes
**************************

This document describes how to add a model to the package or to create a
user-defined model. In short, one needs to define all model parameters and
write a function which evaluates the model, that is, computes the mathematical
function that implements the model.  If the model is fittable, a function to
compute the derivatives with respect to parameters is required if a linear
fitting algorithm is to be used and optional if a non-linear fitter is to be
used.


Basic custom models
===================

For most cases, the `~astropy.modeling.custom_model` decorator provides an
easy way to make a new `~astropy.modeling.Model` class from an existing Python
callable. The following example demonstrates how to set up a model consisting
of two Gaussians:

.. plot::
   :include-source:

    import numpy as np
    import matplotlib.pyplot as plt
    from astropy.modeling.models import custom_model
    from astropy.modeling.fitting import LevMarLSQFitter

    # Define model
    @custom_model
    def sum_of_gaussians(x, amplitude1=1., mean1=-1., sigma1=1.,
                            amplitude2=1., mean2=1., sigma2=1.):
        return (amplitude1 * np.exp(-0.5 * ((x - mean1) / sigma1)**2) +
                amplitude2 * np.exp(-0.5 * ((x - mean2) / sigma2)**2))

    # Generate fake data
    np.random.seed(0)
    x = np.linspace(-5., 5., 200)
    m_ref = sum_of_gaussians(amplitude1=2., mean1=-0.5, sigma1=0.4,
                             amplitude2=0.5, mean2=2., sigma2=1.0)
    y = m_ref(x) + np.random.normal(0., 0.1, x.shape)

    # Fit model to data
    m_init = sum_of_gaussians()
    fit = LevMarLSQFitter()
    m = fit(m_init, x, y)

    # Plot the data and the best fit
    plt.plot(x, y, 'o', color='k')
    plt.plot(x, m(x))


This decorator also supports setting a model's
`~astropy.modeling.FittableModel.fit_deriv` as well as creating models with
more than one inputs.  Note that when creating a model from a function with
multiple outputs, the keyword argument ``n_outputs`` must be set to the
number of outputs of the function.  It can also be used as a normal factory
function (for example ``SumOfGaussians = custom_model(sum_of_gaussians)``)
rather than as a decorator.  See the `~astropy.modeling.custom_model`
documentation for more examples.


A step by step definition of a 1-D Gaussian model
=================================================

The example described in `Basic custom models`_ can be used for most simple
cases, but the following section describes how to construct model classes in
general.  Defining a full model class may be desirable, for example, to
provide more specialized parameters, or to implement special functionality not
supported by the basic `~astropy.modeling.custom_model` factory function.

The details are explained below with a 1-D Gaussian model as an example.  There
are two base classes for models. If the model is fittable, it should inherit
from `~astropy.modeling.FittableModel`; if not it should subclass
`~astropy.modeling.Model`.

If the model takes parameters they should be specified as class attributes in
the model's class definition using the `~astropy.modeling.Parameter`
descriptor.  All arguments to the Parameter constructor are optional, and may
include a default value for that parameter, a text description of the parameter
(useful for `help` and documentation generation), as well default constraints
and custom getters/setters for the parameter value.  It is also possible to
define a "validator" method for each parameter, enabling custom code to check
whether that parameter's value is valid according to the model definition (for
example if it must be non-negative).  See the example in
`Parameter.validator <astropy.modeling.Parameter.validator>` for more details.

::

    from astropy.modeling import Fittable1DModel, Parameter

    class Gaussian1D(Fittable1DModel):
        n_inputs = 1
        n_outputs = 1

        amplitude = Parameter()
        mean = Parameter()
        stddev = Parameter()

The ``n_inputs`` and ``n_outputs`` class attributes must be integers
indicating the number of independent variables that are input to evaluate the
model, and the number of outputs it returns.  The labels of the inputs and
outputs, ``inputs`` and ``outputs``, are generated automatically. It is possible
to overwrite the default ones by assigning the desired values in the class ``__init__``
method, after calling ``super``. ``outputs`` and ``inputs`` must be tuples of
strings with length ``n_outputs`` and ``n_inputs`` respectively.
Outputs may have the same labels as inputs (eg. ``inputs = ('x', 'y')`` and ``outputs = ('x', 'y')``).
However, inputs must not conflict with each other (eg. ``inputs = ('x', 'x')`` is
incorrect) and likewise for outputs.

There are two helpful base classes in the modeling package that can be used to
avoid specifying ``n_inputs`` and ``n_outputs`` for most common models.  These are
`~astropy.modeling.Fittable1DModel` and `~astropy.modeling.Fittable2DModel`.
For example, the actual `~astropy.modeling.functional_models.Gaussian1D` model is
a subclass of `~astropy.modeling.Fittable1DModel`. This helps cut
down on boilerplate by not having to specify ``n_inputs``, ``n_outputs``, ``inputs``
and ``outputs`` for many models (follow the link to Gaussian1D to see its source code, for
example).

Fittable models can be linear or nonlinear in a regression sense. The default
value of the `~astropy.modeling.Model.linear` attribute is ``False``.  Linear
models should define the ``linear`` class attribute as ``True``.  Because this
model is non-linear we can stick with the default.

Models which inherit from `~astropy.modeling.Fittable1DModel` have the
``Model._separable`` property already set to ``True``.
All other models should define this property to indicate the
:ref:`separability`.

Next, provide methods called ``evaluate`` to evaluate the model and
``fit_deriv``, to compute its derivatives with respect to parameters.  These
may be normal methods, `classmethod`, or `staticmethod`, though the convention
is to use `staticmethod` when the function does not depend on any of the
object's other attributes (i.e., it does not reference ``self``) or any of the
class's other attributes as in the case of `classmethod`.  The evaluation
method takes all input coordinates as separate arguments and all of the model's
parameters in the same order they would be listed by
`~astropy.modeling.Model.param_names`.

For this example::

    @staticmethod
    def evaluate(x, amplitude, mean, stddev):
        return amplitude * np.exp((-(1 / (2. * stddev**2)) * (x - mean)**2))

It should be made clear that the ``evaluate`` method must be designed to take
the model's parameter values as arguments.  This may seem at odds with the fact
that the parameter values are already available via attribute of the model
(eg. ``model.amplitude``).  However, passing the parameter values directly to
``evaluate`` is a more efficient way to use it in many cases, such as fitting.

Users of your model would not generally use ``evaluate`` directly.  Instead
they create an instance of the model and call it on some input.  The
``__call__`` method of models uses ``evaluate`` internally, but users do not
need to be aware of it.  The default ``__call__`` implementation also handles
details such as checking that the inputs are correctly formatted and follow
Numpy's broadcasting rules before attempting to evaluate the model.

Like ``evaluate``, the ``fit_deriv`` method takes as input all coordinates and
all parameter values as arguments.  There is an option to compute numerical
derivatives for nonlinear models in which case the ``fit_deriv`` method should
be ``None``::

    @staticmethod
    def fit_deriv(x, amplitude, mean, stddev):
        d_amplitude = np.exp(- 0.5 / stddev**2 * (x - mean)**2)
        d_mean = (amplitude *
                  np.exp(- 0.5 / stddev**2 * (x - mean)**2) *
                  (x - mean) / stddev**2)
        d_stddev = (2 * amplitude *
                    np.exp(- 0.5 / stddev**2 * (x - mean)**2) *
                    (x - mean)**2 / stddev**3)
        return [d_amplitude, d_mean, d_stddev]


Note that we did *not* have to define an ``__init__`` method or a ``__call__``
method for our model. For most models the ``__init__`` follows the same pattern,
taking the parameter values as positional arguments, followed by several optional
keyword arguments (constraints, etc.).  The modeling framework automatically generates an
``__init__`` for your class that has the correct calling signature (see for
yourself by calling ``help(Gaussian1D.__init__)`` on the example model we just
defined).

There are cases where it might be desirable to define a custom ``__init__``.
For example, the `~astropy.modeling.functional_models.Gaussian2D` model takes
an optional ``cov_matrix`` argument which can be used as an alternative way to
specify the x/y_stddev and theta parameters.  This is perfectly valid so long
as the ``__init__`` determines appropriate values for the actual parameters and
then calls the super ``__init__`` with the standard arguments.  Schematically
this looks something like:

.. code-block:: python

    def __init__(self, amplitude, x_mean, y_mean, x_stddev=None,
                 y_stddev=None, theta=None, cov_matrix=None, **kwargs):
        # The **kwargs here should be understood as other keyword arguments
        # accepted by the basic Model.__init__ (such as constraints)
        if cov_matrix is not None:
            # Set x/y_stddev and theta from the covariance matrix
            x_stddev = ...
            y_stddev = ...
            theta = ...

        # Don't pass on cov_matrix since it doesn't mean anything to the base
        # class
        super().__init__(amplitude, x_mean, y_mean, x_stddev, y_stddev, theta,
                         **kwargs)


Full example
------------

.. code-block:: python

    import numpy as np
    from astropy.modeling import Fittable1DModel, Parameter

    class Gaussian1D(Fittable1DModel):
        amplitude = Parameter()
        mean = Parameter()
        stddev = Parameter()

        @staticmethod
        def evaluate(x, amplitude, mean, stddev):
            return amplitude * np.exp((-(1 / (2. * stddev**2)) * (x - mean)**2))

        @staticmethod
        def fit_deriv(x, amplitude, mean, stddev):
            d_amplitude = np.exp((-(1 / (stddev**2)) * (x - mean)**2))
            d_mean = (2 * amplitude *
                      np.exp((-(1 / (stddev**2)) * (x - mean)**2)) *
                      (x - mean) / (stddev**2))
            d_stddev = (2 * amplitude *
                        np.exp((-(1 / (stddev**2)) * (x - mean)**2)) *
                        ((x - mean)**2) / (stddev**3))
            return [d_amplitude, d_mean, d_stddev]


A full example of a LineModel
=============================

This example demonstrates one other optional feature for model classes, which
is an *inverse*.  An `~astropy.modeling.Model.inverse` implementation should be
a `property` that returns a new model instance (not necessarily of the same
class as the model being inverted) that computes the inverse of that model, so
that for some model instance with an inverse, ``model.inverse(model(*input)) ==
input``.

.. code-block:: python

    import numpy as np
    from astropy.modeling import Fittable1DModel, Parameter

    class LineModel(Fittable1DModel):
        slope = Parameter()
        intercept = Parameter()
        linear = True

        @staticmethod
        def evaluate(x, slope, intercept):
            return slope * x + intercept

        @staticmethod
        def fit_deriv(x, slope, intercept):
            d_slope = x
            d_intercept = np.ones_like(x)
            return [d_slope, d_intercept]

        @property
        def inverse(self):
            new_slope = self.slope ** -1
            new_intercept = -self.intercept / self.slope
            return LineModel(slope=new_slope, intercept=new_intercept)

.. note::

    The above example is essentially equivalent to the built-in
    `~astropy.modeling.functional_models.Linear1D` model.
*******************************************
Emacs setup for following coding guidelines
*******************************************

.. _flycheck: https://www.flycheck.org/
.. _flake8: http://flake8.pycqa.org/

The Astropy coding guidelines are listed in :doc:`codeguide`. Here, we describe
how to configure Emacs to help ensure Python code satisfies the guidelines.

For this setup, we add to the standard ``python-mode`` using flycheck_ and the
flake8_ python style checker.  For installation instructions, see their
respective web sites (or install via your distribution; e.g., in Debian/Ubuntu,
the packages are called ``elpa-flycheck`` and ``flake8``).

.. note:: Emacs can be configured in several different ways. So instead of
          providing a drop in configuration file, only the individual
          configurations are presented below.

          The setup below is on purpose minimal.  In principle, it is possible
          to use `Emacs for Python development
          <https://realpython.com/emacs-the-best-python-editor/>`_,
          with, e.g., `elpy <https://elpy.readthedocs.io/>`_.

No tabs
=======

This setting will cause indentation to use spaces rather than tabs for all
files.  For python files, indentation of 4 spaces will be used if the tab key
is pressed.

.. code-block:: scheme

  ;; Don't use TABS for indentations.
  (setq-default indent-tabs-mode nil)

Delete trailing white spaces
============================

One can `delete trailing whitespace
<https://www.emacswiki.org/emacs/DeletingWhitespace#toc3>`_ with ``M-x
delete-trailing-whitespace``. To ensure this is done every time a python file
is saved, use:

.. code-block:: scheme

  ;; Automatically remove trailing whitespace when file is saved.
  (add-hook 'python-mode-hook
  (lambda () (add-to-list 'write-file-functions 'delete-trailing-whitespace)))

If you want to use this for every type of file, you can use
``(add-hook 'before-save-hook 'delete-trailing-whitespace)``.

Flycheck
========

One can make lines that do not satisfy syntax requirements using flycheck_.
When the cursor is on such a line a message is displayed in the mini-buffer.
When mouse pointer is on such a line a "tool tip" message is also shown. By
default, flycheck_ will check if flake8_ is installed and, if so, use that for
its syntax checking. To ensure flycheck_ starts upon opening python files, add:

.. code-block:: scheme

  (add-hook 'python-mode-hook 'flycheck-mode)

Alternatively, you can just use ``(global-flycheck-mode)`` to run flycheck
for all languages it supports.
**********************************************************************
How to create and maintain a Python package using the Astropy template
**********************************************************************

If you run into any problems, don't hesitate to ask for help on the
astropy-dev mailing list!

The `package-template`_ repository provides a template for Python
packages. This package design mirrors the layout of the main `Astropy`_
repository, as well as reusing much of the helper code used to organize
`Astropy`_. See the
:ref:`package template documentation <packagetemplate:package-template>`
for instructions on using the package template.


Releasing a Python package
**************************

You can release a package using the steps given below. In these
instructions, we assume that the release is made from a fresh clone of the
remote "main" repository and not from a forked copy. We also assume that
the changelog file is named ``CHANGES.rst``, like for the astropy core
package. If instead you use Markdown, then you should replace ``CHANGES.rst``
by ``CHANGES.md`` in the instructions.

.. note:: The instructions below assume that you do not make use of bug fix
          branches in your workflow. If you do wish to create a bug fix branch,
          we recommend that you read over the more complete astropy
          :doc:`releasing` and adapt them for your package.

#. Make sure that continuous integration is passing.

#. Update the ``CHANGES.rst`` file to make sure that all the changes are listed,
   and update the release date, which should currently be set to
   ``unreleased``, to the current date in ``yyyy-mm-dd`` format.

#. Run ``git clean -fxd`` to remove any untracked files (WARNING: this will
   permanently remove any files that have not been previously committed, so
   make sure that you don't need to keep any of these files).

#. At this point, the command to run to build the tar file will depend on
   whether your package has a ``pyproject.toml`` file or not. If it does
   not, then::

        python setup.py build sdist --format=gztar

   If it does, then first make sure the `build <https://pypi.org/project/build/>`_
   package is installed and up-to-date::

        pip install build --upgrade

   then create the source distribution with::

        python -m build --sdist .

   All following instructions will assume you have ``pyproject.toml``.
   If you do not use ``pyproject.toml`` yet, please see
   https://docs.astropy.org/en/v3.2.x/development/astropy-package-template.html
   instead.

   In both cases, make sure that generated file is good to go by going inside
   ``dist``, expanding the tar file, going inside the expanded directory, and
   running the tests with::

        pip install -e .[test]
        pytest

   You may need to add the ``--remote-data`` flag or any other flags that you
   normally add when fully testing your package.

#. Go back to the root of the directory and remove the generated files with::

        git clean -fxd

#. Add the changes to ``CHANGES.rst`` and ``setup.cfg``::

        git add CHANGES.rst setup.cfg

   and commit with message::

        git commit -m "Preparing release <version>"

#. Tag commit with ``v<version>``, optionally signing with the ``-s`` option::

        git tag v<version>

#. Change ``VERSION`` in ``setup.cfg`` to next version number, but with a
   ``.dev`` suffix at the end (e.g. ``0.2.dev``). Add a new section to
   ``CHANGES.rst`` for next version, with a single entry ``No changes yet``, e.g.::

       0.2 (unreleased)
       ----------------

       - No changes yet

#. Add the changes to ``CHANGES.rst`` and ``setup.cfg``::

        git add CHANGES.rst setup.cfg

   and commit with message::

        git commit -m "Back to development: <next_version>"

#. Check out the release commit with ``git checkout v<version>``.
   Run ``git clean -fxd`` to remove any non-committed files.

#. (optional) Run the tests in an environment that mocks up a "typical user"
   scenario. This is not strictly necessary because you ran the tests above, but
   it can sometimes be useful to catch subtle bugs that might come from you
   using a customized developer environment.  For more on setting up virtual
   environments, see :ref:`virtual_envs`, but for the sake of example we will
   assume you're using `Anaconda <https://conda.io/docs/>`_. Do::

       conda create -n myaffilpkg_rel_test astropy <any more dependencies here>
       source activate myaffilpkg_rel_test
       python -m build --sdist .
       cd dist
       pip install myaffilpkg-version.tar.gz
       python -c 'import myaffilpkg; myaffilpkg.test()'
       source deactivate
       cd <back to your source>

   You may want to repeat this for other combinations of dependencies if you think
   your users might have other relevant packages installed.  Assuming the tests
   all pass, you can proceed on.

#. If you did the previous step, do ``git clean -fxd`` again to remove anything
   you made there.  Run ``python -m build --sdist .`` to
   create the files for upload.  Then you can upload to PyPI via ``twine``::

        twine upload dist/*

   as described in `these <https://packaging.python.org/en/latest/tutorials/packaging-projects/#uploading-the-distribution-archives>`_
   instructions. Check that the entry on PyPI is correct, and that
   the tarfile is present.

#. Go back to the main branch and push your changes to github::

        git checkout main
        git push --tags origin main

   Once you have done this, if you use Read the Docs, trigger a ``latest`` build
   then go to the project settings, and under **Versions** you should see the
   tag you just pushed. Select the tag to activate it, and save.

#. If your package is available in the ``conda-forge`` conda channel, you
   should also submit a pull request to update the version number in the
   feedstock of your package.


Modifications for a beta/release candidate release
==================================================

   Before a new release of your package, you may wish do a "pre-release" of the
   code, for example to allow collaborators to independently test the release.
   If the release you are performing is this kind of pre-release,
   some of the above steps need to be modified.

   The primary modifications to the release procedure is:

   * When entering the new version number, instead of just removing the
     ``.dev``, enter "1.2b1" or "1.2rc1".  It is critical that you follow this
     numbering scheme (``X.Yb#`` or ``X.Y.Zrc#``), as it will ensure the release
     is ordered "before" the main release by various automated tools, and also
     tells PyPI that this is a "pre-release".


.. _package-template: https://github.com/astropy/package-template
******************
Release Procedures
******************

The current release procedure for Astropy involves a combination of an
automated release script and some manual steps.  Future versions will automate
more of the process, if not all.

There are several different procedures below, depending on the situation:

* :ref:`release-procedure`
    - :ref:`release-procedure-beta-rc`
* :ref:`release-procedure-new-major`
* :ref:`release-procedure-bug-fix`
    - :ref:`release-procedure-bug-fix-backport`
    - :ref:`release-procedure-bug-fix-direct`
    - :ref:`release-procedure-bug-fix-release`

For a signed release, see :ref:`key-signing-info` for relevant setup
instructions.

.. _release-procedure:

Standard Release Procedure
==========================

This is the standard release procedure for releasing the core astropy package (or other
packages that use the full bugfix/maintenance branch approach.)

#. If you are about to branch a new major version, first follow the
   instructions in :ref:`release-procedure-new-major`.

#. Create a GitHub milestone for the next bugfix version, move any remaining
   issues from the version you are about to release, and close the milestone.
   When releasing a major release, close the last milestone on the previous
   maintenance branch, too.

   .. note::

      Creation of new milestone can be done as early as when you ping
      maintainers about their relevant pull requests, so that the maintainers
      have the option to re-milestone their work.

#. If there are any issues in the GitHub issue tracker that are labeled
   ``affects-dev`` but are issues that apply to this release, update them to
   ``affects-release``.  Similarly, if any issues remain open for this release,
   re-assign them to the next relevant milestone.

#. (astropy specific) Ensure the built-in IERS earth rotation parameter and
   leap second tables are up to date by changing directory to
   ``astropy/utils/iers/data`` and executing ``update_builtin_iers.sh``.
   Check the result with ``git diff`` (do not be surprised to find many lines
   in the ``eopc04_IAU2000.62-now`` file change; those data are reanalyzed
   periodically) and committing. Since in some cases updating the IERS tables
   may result in test failures, this update should be done via a pull request
   to the ``main`` branch, and then backported to the release branch.

#. (Only for major versions) Make sure to update the "What's new"
   section with the stats on the number of issues, PRs, and contributors.
   Since the What's New for the major release is now only present in the release
   branch, you should switch to it to, e.g.::

      $ git checkout v5.0.x

   To find the statistics and contributors, use the `generate_releaserst.xsh`_
   script. This requires `xonsh <https://xon.sh/>`_ and `docopt
   <http://docopt.org/>`_ which you can install with::

      pip install xonsh docopt

   You should then run the script in the root of the astropy repository as follows::

      xonsh generate_releaserst.xsh 4.3 v5.0.dev \
                                    --project-name=astropy \
                                    --pretty-project-name=astropy \
                                    --pat=<a GitHub personal access token>

   The first argument should be the last major version (before any bug fix
   releases, while the second argument should be the ``.dev`` tag that was just
   after the branching of the last major version. Finally, you will need a
   GitHub personal access token with default permissions (no scopes selected).

   The output will look similar to::

      This release of astropy contains 2573 commits in 163 merged pull requests
      closing 104 issues from 98 people, 50 of which are first-time contributors
      to astropy.

      * 2573 commits have been added since 4.3
      * 104 issues have been closed since 4.3
      * 163 pull requests have been merged since 4.3
      * 98 people have contributed since 4.3
      * 50 of which are new contributors

      The people who have contributed to the code for this release are:

      - Name 1 *
      - Name 2 *
      - Name 3

   At this point, you will likely need to update the Astropy ``.mailmap`` file,
   which maps contributor emails to names, as there are often contributors who
   are not careful about using the same e-mail address for every commit, meaning
   that they appear multiple times in the contributor list above, sometimes with
   different spelling, and sometimes you may also just see their GitHub username
   with no full name.

   The easiest way to get a full list of contributors and email addresses is
   to do::

      git shortlog -n -s -e

   Edit the ``.mailmap`` file to add entries for new email addresses for already
   known contributors (matched to the appropriate canonical name/email address).
   You can also try and investigate users with no name to see if you can determine
   their full name from other sources - if you do, add a new entry for them in
   the ``.mailmap`` file. Once you have done this, you can re-run the
   ``generate_releaserst.xsh`` script (you will likely need to iterate a few times).
   Once you are happy with the output, copy it into the 'What's new' page for
   the current release and commit this. E.g., ::

      $ git add docs/whatsnew/5.0.rst
      $ git commit -m "Added contributor statistics and names"

   Update the ``docs/credits.rst`` file to include any new contributors from the
   above step, and commit this and the ``.mailmap`` changes::

      $ git add .mailmap
      $ git add docs/credits.rst
      $ git commit -m "Updated list of contributors and .mailmap file"

   This last commit should be forward-ported to the ``main`` branch.

#. Push the release branch back to GitHub, e.g.::

      $ git push upstream v5.0.x

   and make sure that the CI services mentioned above (includnig the Azure pipeline)
   are still passing.

   .. note::

      You may need to replace ``upstream`` here with ``astropy`` or
      whatever remote name you use for the `astropy core repository`_.

#. Ensure you have a GPG key pair available for when git needs to sign the
   tag you create for the release.  See :ref:`key-signing-info` for more on
   this.

#. Obtain a *clean* version of the `astropy core repository`_.  That is, one
   where you don't have any intermediate build files.  Either use a fresh
   ``git clone`` or do ``git clean -dfx``.

#. Make sure that the continuous integration services (e.g., GitHub Actions or CircleCI) are passing
   for the `astropy core repository`_ branch you are going to release. Also check that
   the `Azure core package pipeline`_ which builds wheels on the ``v*`` branches is passing.
   You may also want to locally run the tests (with remote data on to ensure all
   of the tests actually run), using tox to do a thorough test in an isolated
   environment::

      $ pip install tox --upgrade
      $ TEST_READ_HUGE_FILE=1 tox -e test-alldeps -- --remote-data=any

#. We now need to render the changelog with towncrier. Since it is a good idea to
   review the changelog and fix any line wrap and other issues, we do this on
   a separate branch and open a pull request into the release branch to allow for
   easy review. First, create and switch to a new branch based off the release
   branch, e.g.::

      $ git checkout -b v5.0-changelog

   Next, run towncrier and confirm that the fragments can be deleted::

       towncrier --version 5.0

   Then add and commit those changes with::

      $ git add CHANGES.rst
      $ git commit -m "Finalizing changelog for v<version>"

   Push to GitHub and open a pull request for merging this into the release branch,
   e.g. v5.0.x.

   In cases where an LTS branch and a different release branch are being maintained,
   the changelog should be rendered on both branches separately, and only the
   rendering from the non-LTS release branch should be forward-ported to main.

   .. note::

      We render the changelog on the latest release branch and forward-port it
      rather than rendering on main and backporting, since the latter would
      render all news fragments into the changelog rather than only the ones
      intended for the e.g. v5.0.x release branch.

#. Once the changelog pull request is merged, update your release branch to
   match the upstream version, then (on the release branch), tag the merge
   commit for the changelog changes with ``v<version>``, being certain to sign
   the tag with the ``-s`` option::

      $ git tag -s v<version> -m "Tagging v<version>"

#. Push up the tag to the `astropy core repository`_::

      $ git push upstream v<tag version>

   .. note::

      You may need to replace ``upstream`` here with ``astropy`` or
      whatever remote name you use for the `astropy core repository`_.
      Also, it might be tempting to use the ``--tags`` argument to ``git push``,
      but this should *not* be done, as it might push up some unintended tags.

   At this point if all goes well, the wheels and sdist will be build
   in the `Azure core package pipeline`_ and uploaded to PyPI!

#. In the event there are any issues with the wheel building for the tag
   (which shouldn't really happen if it was passing for the release branch),
   you'll have to fix whatever the problem is. First you will need to back out
   the release procedure by dropping the commits you made for release and
   removing the tag you created::

      $ git reset --hard HEAD^^^^ # you could also use the SHA hash of the commit before your first changelog edit
      $ git tag -d v<version>

   .. note::

      Any re-pushing the same tag back out to GitHub hereafter would be
      a force-push.

  Once the sdist and wheels are uploaded, the release is done!

Congratulations!  You have completed the release! Now there are just a few
clean-up tasks to finalize the process.

.. _post-release-procedure:

Post-Release procedures
-----------------------

#. Push up these changes to the `astropy core repository`_::

      $ git push upstream v<version branch>.x

#. If this is a release of the current release (i.e., not an LTS supported along
   side a more recent version), update the "stable" branch to point to the new
   release::

      $ git checkout stable
      $ git reset --hard v<version>
      $ git push upstream stable --force

#. Update Readthedocs so that it builds docs for the version you just released.
   You'll find this in the "admin" tab, with checkboxes next to each github tag.
   Also verify that the ``stable`` Readthedocs version builds correctly for
   the new version (it should trigger automatically once you've done the
   previous step).

#. When releasing a patch release, also set the previous RTD version in the
   release history to "protected".  For example when releasing v5.0.2, set
   v5.0.1 to "protected".  This prevents the previous releases from
   cluttering the list of versions that users see in the version dropdown
   (the previous versions are still accessible by their URL though).

#. Update the Astropy web site by editing the ``index.html`` page at
   https://github.com/astropy/astropy.github.com by changing the "current
   version" link and/or updating the list of older versions if this is an LTS
   bugfix or a new major version.  You may also need to update the contributor
   list on the web site if you updated the ``docs/credits.rst`` at the outset.

#. Cherry-pick the commit rendering the changelog and deleting the fragments and
   open a PR to the astropy *main* branch. Also make sure you cherry-pick the
   commit updating the ``.mailmap`` and ``docs/credits.rst`` files to the *main*
   branch in a separate PR.

#. ``conda-forge`` has a bot that automatically opens
   a PR from a new PyPI (stable) release, which you need to follow up on and
   merge. Meanwhile, for a LTS release, you still have to manually open a PR
   at `astropy-feedstock <https://github.com/conda-forge/astropy-feedstock/>`_.
   This is similar to the process for wheels.
   When the ``conda-forge`` package is ready, email the Anaconda maintainers
   about the release(s) so they can update the versions in the default channels.
   Typically, you should wait to make sure ``conda-forge`` and possibly
   ``conda`` works before sending out the public announcement
   (so that users who want to try out the new version can do so with ``conda``).

#. Upload the release to Zenodo by creating a GitHub Release off the GitHub tag.
   Click on the tag in https://github.com/astropy/astropy/tags and then click on
   the "Edit tag" button on the upper right. The release title is the same as the
   tag. In the description, you can copy and paste a description from the previous
   release, as it should be a one-liner that points to ``CHANGES.rst``. When you
   are ready, click "Publish release" (the green button on bottom left).
   A webhook to Zenodo will be activated and the release will appear under
   https://doi.org/10.5281/zenodo.4670728 . If you encounter problems during this
   step, please contact the Astropy Coordination Committee.

#. Once the release(s) are available on the default ``conda`` channels,
   prepare the public announcement. Use the previous announcement as a
   template, but link to the release tag instead of ``stable``.
   For a new major release, you should coordinate with the Astropy Coordinators.
   Meanwhile, for a bugfix release, you can proceed to send out an email
   to the ``astropy-dev`` and Astropy mailing lists.

.. _release-procedure-beta-rc:

Modifications for a beta/release candidate release
--------------------------------------------------

For major releases, we do beta and/or release candidates to have a chance to
catch significant bugs before the true release. If the release you are
performing is this kind of pre-release, some of the above steps need to be
modified.

The primary modifications to the release procedure are:

* When entering tagging the release, include a ``b?`` or ``rc??`` suffix after
  the version number, e.g. "5.0b1" or "5.0rc1".  It is critical that you follow this
  numbering scheme (``X.Yb#`` or ``X.Y.Zrc#``), as it will ensure the release
  is ordered "before" the main release by various automated tools, and also
  tells PyPI that this is a "pre-release."
* Do not do steps in :ref:`post-release-procedure`.
* Do not render the changelog with towncrier and open the pull request for these
  changes to the release branch. This should only be done just before the final
  release. However, it is up to the discretion of the release manager whether to
  open 'practice' pull requests to do this as part of the beta/release candidate
  process (but they should not be merged in).

Once a release candidate is available, create a new Wiki page under
`Astropy Project Wiki <https://github.com/astropy/astropy/wiki>`_ with the
title "vX.Y RC testing" (replace "X.Y" with the release number) using the
`wiki of a previous RC <https://github.com/astropy/astropy/wiki/v3.2-RC-testing>`_
as a template.

.. _release-procedure-new-major:

Performing a Feature Freeze/Branching new Major Versions
========================================================

As outlined in
`APE2 <https://github.com/astropy/astropy-APEs/blob/main/APE2.rst>`_, astropy
releases occur at regular intervals, but feature freezes occur well before the
actual release.  Feature freezes are also the time when the main branch's
development separates from the new major version's maintenance branch.  This
allows new development for the next major version to continue while the
soon-to-be-released version can focus on bug fixes and documentation updates.

The procedure for this is straightforward:

#. Update your local main branch to use to the latest version from github::

      $ git fetch upstream --tags
      $ git checkout -B main upstream/main

#. Create a new branch from main at the point you want the feature freeze to
   occur::

      $ git branch v<version>.x

#. Update the "what's new" section of the docs to include a section for the
   next major version.  E.g.::

      $ cp docs/whatsnew/<current_version>.rst docs/whatsnew/<next_version>.rst

   You'll then need to edit ``docs/whatsnew/<next_version>.rst``, removing all
   the content but leaving the basic structure.  You may also need to
   replace the "by the numbers" numbers with "xxx" as a reminder to update them
   before the next release. Then add the new version to the top of
   ``docs/whatsnew/index.rst``, update the reference in ``docs/index.rst`` to
   point to the that version.

#. Update the "what's new" section of the current version,
   ``docs/whatsnew/<current_version>.rst``, and remove all content, replacing it
   with::

      :orphan:

      `What's New in Astropy <current_version>?
      <https://docs.astropy.org/en/v<current_version>/whatsnew/<current_version>.html>`__

   This is because we want to make sure that links in the previous what's new pages continue
   to work and reference the original link they referenced at the time of writing.

#. Commit these changes ::

      $ git add docs/whatsnew/<current_version>.rst
      $ git add docs/whatsnew/<next_version>.rst
      $ git add docs/whatsnew/index.rst
      $ git add docs/index.rst
      $ git commit -m "Added <next_version> what's new page and redirect <current_version> what's new page"

#. Tag this commit using the next major version followed by ``.dev``. For example,
   if you have just branched ``5.0``, create the ``v5.1.dev`` tag::

      $ git tag -s "v<next_version>.dev" -m "Back to development: v<next_version>"

#. Push all of these changes up to github::

      $ git push upstream v<version>.x:v<version>.x
      $ git push upstream main:main

   .. note::

      You may need to replace ``upstream`` here with ``astropy`` or
      whatever remote name you use for the `astropy core repository`_.

#. On the github issue tracker, add a new milestone for the next major version
   and for the next bugfix version, and also create a ``backport-v<version>.x``
   label which can be used to label pull requests that should be backported
   to the new release branch.

#. Inform the Astropy developer community that the branching has occurred.

.. _release-procedure-bug-fix:

Maintaining Bug Fix Releases
============================

.. note::

   Always start with LTS release, followed by, if necessary, a bugfix for
   stable release. If the releases are not done in that order, the change log
   entries on what goes where can get mixed up.

Astropy releases, as recommended for most Python projects, follows a
<major>.<minor>.<micro> version scheme, where the "micro" version is also
known as a "bug fix" release.  Bug fix releases should not change any user-
visible interfaces.  They should only fix bugs on the previous major/minor
release and may also refactor internal APIs or include omissions from previous
releases--that is, features that were documented to exist but were accidentally
left out of the previous release. They may also include changes to docstrings
that enhance clarity but do not describe new features (e.g., more examples,
typo fixes, etc).

Bug fix releases are typically managed by maintaining one or more bug fix
branches separate from the main branch (the release procedure below discusses
creating these branches).  Typically, whenever an issue is fixed on the Astropy
main branch a decision must be made whether this is a fix that should be
included in the Astropy bug fix release.  Usually the answer to this question
is "yes", though there are some issues that may not apply to the bug fix
branch.  For example, it is not necessary to backport a fix to a new feature
that did not exist when the bug fix branch was first created.  New features
are never merged into the bug fix branch--only bug fixes; hence the name.

In rare cases a bug fix may be made directly into the bug fix branch without
going into the main branch first.  This may occur if a fix is made to a
feature that has been removed or rewritten in the development version and no
longer has the issue being fixed.  However, depending on how critical the bug
is it may be worth including in a bug fix release, as some users can be slow to
upgrade to new major/micro versions due to API changes.

Issues are assigned to an Astropy release by way of the Milestone feature in
the GitHub issue tracker.  At any given time there are at least two versions
under development: The next major/minor version, and the next bug fix release.
For example, at the time of writing there are two release milestones open:
v5.1 and v5.0.1.  In this case, v5.0.1 is the next bug fix release and all
issues that should include fixes in that release should be assigned that
milestone.  Any issues that implement new features would go into the v5.1
milestone--this is any work that goes in the main branch that should not
be backported.  For a more detailed set of guidelines on using milestones, see
:ref:`milestones-and-labels`.


.. _release-procedure-bug-fix-backport:

Backporting fixes from main
---------------------------

.. note::

    The changelog script in ``astropy-tools`` (``pr_consistency`` scripts
    in particular) does not know about minor releases, thus please be careful.
    For example, let's say we have two branches (``main`` and ``v5.0.x``).
    Both 5.0.0 and 5.0.1 releases will come out of the same v5.0.x branch.
    If a PR for 5.0.1 is merged into ``main`` before 5.0.0 is released,
    it should not be backported into v5.0.x branch until after 5.0.0 is
    released, despite complaining from the aforementioned script.
    This situation only arises in a very narrow time frame after 5.0.0
    freeze but before its release.

Most fixes are backported using the ``git cherry-pick`` command, which applies
the diff from a single commit like a patch.  For the sake of example, say the
current bug fix branch is 'v5.0.x', and that a bug was fixed in main in a
commit ``abcd1234``.  In order to backport the fix, checkout the v5.0.x
branch (it's also good to make sure it's in sync with the
`astropy core repository`_) and cherry-pick the appropriate commit::

    $ git checkout v5.0.x
    $ git pull upstream v5.0.x
    $ git cherry-pick abcd1234

Sometimes a cherry-pick does not apply cleanly, since the bug fix branch
represents a different line of development.  This can be resolved like any
other merge conflict:  Edit the conflicted files by hand, and then run
``git commit`` and accept the default commit message.  If the fix being
cherry-picked has an associated changelog entry in a separate commit make
sure to backport that as well.

What if the issue required more than one commit to fix?  There are a few
possibilities for this.  The easiest is if the fix came in the form of a
pull request that was merged into the main branch.  Whenever GitHub merges
a pull request it generates a merge commit in the main branch.  This merge
commit represents the *full* difference of all the commits in the pull request
combined.  What this means is that it is only necessary to cherry-pick the
merge commit (this requires adding the ``-m 1`` option to the cherry-pick
command).  For example, if ``5678abcd`` is a merge commit::

    $ git checkout v5.0.x
    $ git pull upstream v5.0.x
    $ git cherry-pick -m 1 5678abcd

In fact, because Astropy emphasizes a pull request-based workflow, this is the
*most* common scenario for backporting bug fixes, and the one requiring the
least thought.  However, if you're not dealing with backporting a fix that was
not brought in as a pull request, read on.

.. seealso::

    :ref:`merge-commits-and-cherry-picks` for further explanation of the
    cherry-pick command and how it works with merge commits.

If not cherry-picking a merge commit there are still other options for dealing
with multiple commits.  The simplest, though potentially tedious, is to
run the cherry-pick command once for each commit in the correct order.
However, as of Git 1.7.2 it is possible to merge a range of commits like so::

    $ git cherry-pick 1234abcd..56789def

This works fine so long as the commits you want to pick are actually congruous
with each other.  In most cases this will be the case, though some bug fixes
will involve followup commits that need to back backported as well.  Most bug
fixes will have an issues associated with it in the issue tracker, so make sure
to reference all commits related to that issue in the commit message.  That way
it's harder for commits that need to be backported from getting lost.


.. _release-procedure-bug-fix-direct:

Making fixes directly to the bug fix branch
-------------------------------------------

As mentioned earlier in this section, in some cases a fix only applies to a bug
fix release, and is not applicable in the mainline development.  In this case
there are two choices:

1. An Astropy developer with commit access to the `astropy core repository`_ may
   check out the bug fix branch and commit and push your fix directly.

2. **Preferable**: You may also make a pull request through GitHub against the
   bug fix branch rather than against main.  Normally when making a pull
   request from a branch on your fork to the `astropy core repository`_, GitHub
   compares your branch to Astropy's main.  If you look on the left-hand
   side of the pull request page, under "base repo: astropy/astropy" there is
   a drop-down list labeled "base branch: main".  You can click on this
   drop-down and instead select the bug fix branch ("v5.0.x" for example). Then
   GitHub will instead compare your fix against that branch, and merge into
   that branch when the PR is accepted.


.. _release-procedure-bug-fix-release:

Preparing the bug fix branch for release
----------------------------------------

There are two primary steps that need to be taken before creating a bug fix
release. The rest of the procedure is the same as any other release as
described in :ref:`release-procedure` (although be sure to provide the
right version number).

1. Any existing fixes to the issues assigned to a release milestone (and older
   LTS releases, if there are any), must be included in the maintenance branch
   before release.

2. The Astropy changelog must be updated to list all issues--especially
   user-visible issues--fixed for the current release.  The changelog should
   be updated in the main branch, and then merged into the bug fix branch.
   Most issues *should* already have changelog entries for them. But
   occasionally these are forgotten, so if doesn't exist yet please add one in
   the process of backporting.  See :ref:`changelog-format` for more details.

To aid this process, there are a series of related scripts in the
`astropy-tools repository`_, in the ``pr_consistency`` directory.  These scripts
essentially check that the above two conditions are met. Detailed documentation
for these scripts is given in their repository, but here we summarize the basic
workflow.  Run the scripts in order (they are numbered ``1.<something>.py``,
``2.<something>.py``, etc.), entering your github login credentials as needed
(if you are going to run them multiple times, using a ``~/.netrc`` file is
recommended - see `this Stack Overflow post
<https://stackoverflow.com/questions/5343068/is-there-a-way-to-cache-github-credentials-for-pushing-commits/18362082#18362082>`_
for more on how to do that, or
`a similar github help page <https://help.github.com/en/articles/caching-your-github-password-in-git>`_).
The script to actually check consistency should be run like::

    $ python 4.check_consistency.py > consistency.html

Which will generate a simple web page that shows all of the areas where either
a pull request was merged into main but is *not* in the relevant release that
it has been milestoned for, as well as any changelog irregularities (i.e., PRs
that are in the wrong section for what the github milestone indicates).  You'll
want to correct those irregularities *first* before starting the backport
process (re-running the scripts in order as needed).

The end of the ``consistency.html`` page will then show a series of
``git cherry-pick`` commands to update the maintenance branch with the PRs that
are needed to make the milestones and branches consistent.  Make sure you're in
the correct maintenance branch with e.g.,

::

    $ git checkout v1.3.x
    $ git pull upstream v1.3.x  # Or possibly a rebase if conflicts exist

if you are doing bugfixes for the 1.3.x series. Go through the commands one at a
time, following the cherry-picking procedure described above. If for some reason
you determine the github milestone was in error and the backporting is
impossible, re-label the issue on github and move on.  Also, whenever you
backport a PR, it's useful to leave a comment in the issue along the lines of
"backported this to v1.3.x as <SHA>" so that it's clear that the backport
happened to others who might later look.

.. warning::

    Automated scripts are never perfect, and can either miss issues that need to
    be backported, or in some cases can report false positives.

    It's always a good idea before finalizing a bug fix release to look on
    GitHub through the list of closed issues in the release milestone and check
    that each one has a fix in the bug fix branch.  Usually a quick way to do
    this is for each issue to run::

        $ git log --oneline <bugfix-branch> | grep #<issue>

    Most fixes will mention their related issue in the commit message, so this
    tends to be pretty reliable.  Some issues won't show up in the commit log,
    however, as their fix is in a separate pull request.  Usually GitHub makes
    this clear by cross-referencing the issue with its PR.

Finally, not all issues assigned to a release milestone need to be fixed before
making that release.  Usually, in the interest of getting a release with
existing fixes out within some schedule, it's best to triage issues that won't
be fixed soon to a new release milestone.  If the upcoming bug fix release is
'v5.0.2', then go ahead and create a 'v5.0.3' milestone and reassign to it any
issues that you don't expect to be fixed in time for 'v5.0.2'.

.. _key-signing-info:

Creating a GPG Signing Key and a Signed Tag
===========================================

One of the main steps in performing a release is to create a tag in the git
repository representing the exact state of the repository that represents the
version being released.  For Astropy we will always use `signed tags`_: A
signed tag is annotated with the name and e-mail address of the signer, a date
and time, and a checksum of the code in the tag.  This information is then
signed with a GPG private key and stored in the repository.

Using a signed tag ensures the integrity of the contents of that tag for the
future.  On a distributed VCS like git, anyone can create a tag of Astropy
called "0.1" in their repository--and where it's easy to monkey around even
after the tag has been created.  But only one "0.1" will be signed by one of
the Astropy Project coordinators and will be verifiable with their public key.

Generating a public/private key pair
------------------------------------

Git uses GPG to created signed tags, so in order to perform an Astropy release
you will need GPG installed and will have to generated a signing key pair.
Most \*NIX installations come with GPG installed by default (as it is used to
verify the integrity of system packages).  If you don't have the ``gpg``
command, consult the documentation for your system on how to install it.

For OSX, GPG can be installed from MacPorts using ``sudo port install gnupg``.

To create a new public/private key pair, run::

    $ gpg --gen-key

This will take you through a few interactive steps. For the encryption
and expiry settings, it should be safe to use the default settings (I use
a key size of 4096 just because what does a couple extra kilobytes
hurt?) Enter your full name, preferably including your middle name or
middle initial, and an e-mail address that you expect to be active for a
decent amount of time. Note that this name and e-mail address must match
the info you provide as your git configuration, so you should either
choose the same name/e-mail address when you create your key, or update
your git configuration to match the key info. Finally, choose a very good
pass phrase that won't be easily subject to brute force attacks.


If you expect to use the same key for some time, it's good to make a backup of
both your public and private key::

    $ gpg --export --armor > public.key
    $ gpg --export-secret-key --armor > private.key

Back up these files to a trusted location--preferably a write-once physical
medium that can be stored safely somewhere.  One may also back up their keys to
a trusted online encrypted storage, though some might not find that secure
enough--it's up to you and what you're comfortable with.

Add your public key to a keyserver
----------------------------------
Now that you have a public key, you can publish this anywhere you like--in your
e-mail, in a public code repository, etc.  You can also upload it to a
dedicated public OpenPGP keyserver.  This will store the public key
indefinitely (until you manually revoke it), and will be automatically synced
with other keyservers around the world.  That makes it easy to retrieve your
public key using the gpg command-line tool.

To do this you will need your public key's keyname.  To find this enter::

    $ gpg --list-keys

This will output something like::

    /path/to/.gnupg/pubring.gpg
    ---------------------------------------------
    pub   4096D/1234ABCD 2012-01-01
    uid                  Your Name <your_email>
    sub   4096g/567890EF 2012-01-01

The 8 digit hex number on the line starting with "pub"--in this example the
"1234ABCD" unique keyname for your public key.  To push it to a keyserver
enter::

    $ gpg --send-keys 1234ABCD

But replace the 1234ABCD with the keyname for your public key.  Most systems
come configured with a sensible default keyserver, so you shouldn't have to
specify any more than that.

Create a tag
------------
Now test creating a signed tag in git.  It's safe to experiment with this--you
can always delete the tag before pushing it to a remote repository::

    $ git tag -s v0.1 -m "Astropy version 0.1"

This will ask for the password to unlock your private key in order to sign
the tag with it.  Confirm that the default signing key selected by git is the
correct one (it will be if you only have one key).

Once the tag has been created, you can verify it with::

    $ git tag -v v0.1

This should output something like::

    object e8e3e3edc82b02f2088f4e974dbd2fe820c0d934
    type commit
    tag v0.1
    tagger Your Name <your_email> 1339779534 -0400

    Astropy version 0.1
    gpg: Signature made Fri 15 Jun 2012 12:59:04 PM EDT using DSA key ID 0123ABCD
    gpg: Good signature from "Your Name <your_email>"

You can use this to verify signed tags from any repository as long as you have
the signer's public key in your keyring.  In this case you signed the tag
yourself, so you already have your public key.

Note that if you are planning to do a release following the steps below, you
will want to delete the tag you just created, because the release script does
that for you.  You can delete this tag by doing::

    $ git tag -d v0.1


.. _astropy core repository: https://github.com/astropy/astropy
.. _signed tags: https://git-scm.com/book/en/v2/Git-Basics-Tagging#Signed-Tags
.. _cython: http://www.cython.org/
.. _astropy-tools repository: https://github.com/astropy/astropy-tools
.. _Anaconda: https://conda.io/docs/
.. _twine: https://packaging.python.org/key_projects/#twine
.. _Azure core package pipeline: https://dev.azure.com/astropy-project/astropy/_build
.. _generate_releaserst.xsh: https://raw.githubusercontent.com/sunpy/sunpy/main/tools/generate_releaserst.xsh
.. _astropy-style-guide:

******************************************************************
Astropy Narrative Style Guide: A Writing Resource for Contributors
******************************************************************

The purpose of this style guide is to provide the Astropy community with a set
of style and formatting guidelines that can be referenced when writing Astropy
documentation. Following the guidelines offered in this style guide will bring
greater consistency and clarity to Astropy's documentation, supporting its
mission to develop a common core package for Astronomy in Python and foster an
ecosystem of interoperable astronomy packages.

This style guide is organized alphabetically by writing topic, with usage
examples in each section, and tone and formatting guidelines at the end.

Abbreviations
=============

Place abbreviations such as i.e. and e.g. within parentheses, where they are
followed by a comma. Alternatively, consider using "that is" and for example
instead, preceded by an em dash or semicolon and followed by a comma, or
contained within em dashes.

Examples
--------
* The only way to modify the data in a frame is by using the ``data`` attribute
  directly and not the aliases for components on the frame (i.e., the following
  will not work).
* There are no plans to support more complex evolution (e.g., non-inertial
  frames or more complex evolution), as that is out of scope for the ``astropy``
  core.
* Once you have a coordinate object you can access the components of that
  coordinate  for example, RA or Dec  to get string representations of the
  full coordinate.

For general use and scientific terms, use abbreviations only when the
abbreviated term is well-known and widely used within the astronomy community.
For less common scientific terms, or terms specific to a given field, write out
the term or link to a resource of explanation. A good rule of thumb to follow
when deciding whether or not something should be abbreviated is: when in doubt,
write it out.

Examples
--------
* 1D, 2D, etc. is preferred over one-dimensional, two-dimensional, etc.
* Units such as SI and CGS can be abbreviated as is more commonly seen in the
  scientific community.
* White dwarf should be written out fully instead of abbreviated as WD.
* Names of organizations or other proper nouns that employ acronyms should be
  written as their known acronym, but with a hyperlink to a website or resource
  for reference, for instance, `CODATA <https://codata.org/>`_.

Capitalization
==============

Capitalize all proper nouns (names) in plain text, except when referring to
package/code names, in which case use lowercase and double backticks. Astropy
capitalized refers to The Astropy Project, while ``astropy`` lowercase and in
backticks refers to the core package.

Examples
--------
* Follow Astropy guidelines for contributing code.
* Affiliated packages are astronomy-related software packages that are not part
  of the ``astropy`` core package.
* Provide a code example along with details of the operating system and the
  Python, ``numpy``, and ``astropy`` versions you are using.

In Documentation materials, title case capitalization is preferred in headings,
meaning capitalize first, last, and all major words in the heading, but
lowercase articles (the, a, an), prepositions (at, to, up, down, with, in,
etc.), and common coordinating conjunctions (and, but, for, or). Sentence case
capitalization is acceptable for longer example headings.

Examples
--------
* Building and Installing
* Frames without Data
* Checklist for Contributing Code
* Astropy Guidelines
* Importing ``astropy`` and Subpackages
* Example: Use velocity to compute sky position at different epochs

In Tutorials and other learning materials, title case capitalization is
preferred in headings of structured introductory/template sections, but within
the tutorial, sentence case (i.e., capitalize first word and proper nouns only)
is acceptable for longer headings designating different learning/code sections.

Contractions
============

Do not use contractions in formal documentation material.

Examples
--------
* If you are making changes that impact ``astropy`` performance, consider adding
  a performance benchmark.
* You do not need to include a changelog entry.

In all other materials, avoid use of contractions only when the tense can be
confused, such as in the case of she is gone versus she has gone, etc.

.. _Hyphenation:

Hyphenation
===========

Phrasal adjectives/compound modifiers placed before a noun should be hyphenated
to avoid confusion.

Examples
--------
* Astronomy-related software packages.
* Astropy provides sustainable, high-level education to the astronomy community.

Hyphenated compound words should contain hyphens in plain text, but no hyphens
in code.

Example
-------
* Do not forget to double-check your formatting.

Numbers
=======

For numbers followed by a unit or as part of a name, use the numeral.

Examples
--------
* 1 arcminute
* 32 degrees
* Gaia data release 2 catalog
* 1D, 2D, etc. is preferred over one-dimensional, two-dimensional, etc.

For all other whole numbers, follow Associated Press (AP) style: spell out
numbers one through nine, and use numerals for 10 and higher, with numeral-word
combinations for millions, billions, and trillions.

Examples
--------
* There are two ways to build Astropy documentation.
* Follow these 11 steps.
* Measuring astrometry for about 2 billion stars.

For casual expressions, spell out the number.

Example
-------
* A picture is worth a thousand words.

Punctuation
===========

For consistency across Astropy materials, non-U.S. punctuation will be edited
to reflect American punctuation preferences.

**Parentheses**: punctuation belonging to parenthetical material will be placed
inside of closing parentheses, with the exception of commas to denote a small
pause coming after parenthetical material, and periods when parenthetical
material is included within another sentence.

Examples
--------
* (For full contributor guidelines, see our documentation.)
* Once you open a pull request (which should be opened against the ``main``
  branch), please make sure to include the following.
* In some cases, most of the required functionality is contained in a single
  class (or a few classes).

**Quotation marks**: periods and commas will be placed inside of closing
quotation marks, whether double or single.

Examples
--------
* Chief among these terms is the concept of a coordinate system.
* Because of the likelihood of confusion between these meanings of coordinate
  system, `~astropy.coordinates` avoids this term wherever possible.

**Hyphens vs. En Dashes vs. Em Dashes**

Hyphens (-) should be used for phrasal adjectives and compound words (see
`Hyphenation`_ above).

En dashes ( longer) should be used for number ranges (dates, times, pages) or
to replace the words to or through, without spaces around the dash.

Examples
--------
* See chapters 1418.
* We have blocked off March 2019May 2019 to develop a new version.

Em dashes ( longest) can be used in place of commas, parentheses, or colons to
set off amplifying or explanatory elements. In Astropy materials, follow
Associated Press (AP) style, which calls for spaces on either side of each em
dash.

Examples
--------
* Several types of input angles  array, scalar, tuple, string  can be used in
  the creation of an Angle object.
* The creation of an Angle object supports a variety of input angle types 
  array, scalar, tuple, string, etc.

Spelling
========

For consistency across Astropy materials, non-U.S. spelling will be edited to
reflect American spelling preferences.

Example
-------
* Cross-matching catalog coordinates (versus catalogue)

Time and Date
=============

Use numerals when exact times are expressed. Use the 24-hour system to express
exact times. For consistency across Astropy materials, all instances of exact
times will be edited to reflect 24-hour time system preferences.

Example
-------
* The presentation starts at 15:00.

Express specific dates as numerals in ISO 8601 format, year-month-day.

Example
-------
* Data from the Gaia mission was released on 2018-04-25.

A Note About Voice and Tone
===========================

Across all Astropy materials in narrative sections, please follow these voice
and tone guidelines.

Write in the present tense.

Example
-------
* In the following section, we are going to make a plot...
* To test if your version of ``astropy`` is running correctly...

Use the first-person inclusive plural.

Example
-------
* We did this the long way, but next we can try it the short way...

Use the generic pronoun you instead of one.

Example
-------
* You can access any of the attributes on a frame by...

Always avoid extraneous or belittling words such as obviously, easily,
simply, just, or straightforward. Avoid extraneous phrases like, we just
have to do one more thing.

Avoid words or phrases that create worry in the mind of the reader. Instead,
use positive language that establishes confidence in the skills being learned.

Examples
--------
* As a best practice...
* One recommended way to...
* An important note to remember is...

Along these lines, use "warning" directives only to note limitations in the
code, not implied limitations in the skills or knowledge of the reader.

Documentation vs. Tutorials vs. Guides
--------------------------------------

Documentation
^^^^^^^^^^^^^
Tone: academic and slightly more formal.

* Use title case capitalization in section headings.
* Do not use contractions.

Tutorials
^^^^^^^^^
Tone: academic but less formal and more friendly.

* Use title case capitalization in introductory/template headings, switch to
  sentence case capitalization for learning/example section headings.
* Section headings should use the imperative mood to form a command or request
  (e.g., Download the data).
* Contractions can be used as long as the tense is clear.

Guides
^^^^^^
Tone: academic but less formal and more friendly.

* Use title case capitalization in introductory/template headings, switch to
  sentence case capitalization for learning/example section headings.
* Contractions can be used as long as the tense is clear.

Formatting Guidelines
=====================

Astropy documentation is written in reStructuredText using the Sphinx
documentation generator. When formatting the different sections of your
documentation files, please follow these guidelines to maintain consistency in
section heading hierarchy across Astropy's RST files.

Section headings in reStructuredText files are created by underlining (and
optionally overlining) the section title with a punctuation character the same
length as the text.

Examples
--------

::

  *************************
  This is a Chapter Heading
  *************************

::

  This is a Section Heading
  =========================

Although there are no formally assigned characters to create heading level
hierarchy, as the hierarchy rendering is determined from the succession of
headings, here is a suggested convention to follow when formatting Astropy
documentation files:

# with overline, for parts
* with overline, for chapters
=, for sections
-, for subsections
^, for subsubsections
", for paragraphs

These guidelines follow Sphinx's recommendation in the `Sections
<https://www.sphinx-doc.org/en/master/usage/restructuredtext/basics.html#sections>`_
chapter of its reStructuredText Primer and Python's convention in the `7.3.6.
Sections <https://devguide.python.org/documenting/#sections>`_ part of its style
guide.

Other Writing Resources
=======================

Some other resources that may be useful when writing Astropy documentation are:

* Python's `Style Guide
  <https://devguide.python.org/documenting/#style-guide>`_
* Sphinx's `reStructuredText Primer
  <https://www.sphinx-doc.org/en/master/usage/restructuredtext/basics.html>`_
* `Quick reStructuredText
  <https://docutils.sourceforge.io/docs/user/rst/quickref.html>`_
:orphan:

***********************
Astropy Docstring Rules
***********************

The rules for Astropy docstrings are now the same as those given in the
[numpydoc documentation](https://numpydoc.readthedocs.io/en/latest/format.html).
.. _building-c-or-cython-extensions:

**********************
C or Cython Extensions
**********************

Astropy supports using C extensions for wrapping C libraries and Cython for
speeding up computationally-intensive calculations. Both Cython and C extension
building can be customized using the ``get_extensions`` function of the
``setup_package.py`` file. If defined, this function must return a list of
``setuptools.Extension`` objects. The creation process is left to the
subpackage designer, and can be customized however is relevant for the
extensions in the subpackage.

While C extensions must always be defined through the ``get_extensions``
mechanism, Cython files (ending in ``.pyx``) are automatically located
by `extension-helpers <https://extension-helpers.readthedocs.io/>`_ and
loaded in separate extensions if they are not in ``get_extensions``. For
Cython extensions located in this way, headers for numpy C functions are
included in the build, but no other external headers are included. ``.pyx``
files present in the extensions returned by ``get_extensions`` are not
included in the list of automatically generated extensions.

.. note::

    If a ``setuptools.Extension`` object is provided for Cython
    source files using the ``get_extensions`` mechanism, it is very
    important that the ``.pyx`` files be given as the ``source``, rather than the
    ``.c`` files generated by Cython.

Using Numpy C headers
=====================

If your C or Cython extensions uses `numpy` at the C level, you probably
need access to the numpy C headers.  When doing this, you should use
``numpy.get_include()`` to specify the include directory to use, for example::

    from setuptools import Extension
    import numpy

    def get_extensions():
        return Extension(name='myextension', sources=['myext.c'],
                         include_dirs=[numpy.get_include()])


Installing C header files
=========================

If your C extension needs to be linked from other third-party C code,
you probably want to install its header files along side the Python module.

    1) Create an ``include`` directory inside of your package for
       all of the header files.

    2) Use the ``[options.package_data]`` section in your ``setup.cfg``
       file to include those header files in the package. For example, the
       `astropy.wcs` package has the following entries in the
       ``[options.package_data]`` section::

           [options.package_data]
           ...
           astropy.wcs = include/*/*.h
           ...

Preventing importing at build time
==================================

It is important to make sure that ``setup_package.py`` files do not trigger an
import of the package they are in - so they should be able to be executed without
relying on imports to other parts of the package.

Speed up your builds with ccache
================================

`ccache <https://en.wikipedia.org/wiki/Ccache>`_ is a tool that caches
compiled sources so that they don't have to be recompiled (so long as they are
unchanged) even if the outputs have been deleted.  This means that if you
switch branches or clean your source checkout you can save a lot of time by
avoiding the majority of re-compiles from scratch.

Because installation and configuration of ccache varies from platform to
platform, please consult the ccache documentation and/or Google to set up
ccache on your system--this is strongly encouraged for anyone doing significant
development of Astropy or scientific programming in general.
.. _documentation-guidelines:

*********************
Writing Documentation
*********************

High-quality, consistent documentation for astronomy code is one of the major
goals of the Astropy Project.  Hence, we describe our documentation procedures
and rules here.  For the astropy core project and coordinated packages we try to
keep to these as closely as possible, and we encourage affiliated packages to
also adhere to these as they encourage useful documentation, a characteristic
often lacking in professional astronomy software.

Adding a Git Commit
===================

When your changes only affect documentation (i.e., docstring or RST files)
and do not include any code snippets that require doctest to run, you may
add a ``[ci skip]`` in your commit message. For example::

    git commit -m "Update documentation about this and that [ci skip]"

When this commit is pushed out to your branch associated with a pull request,
all CI will be skipped because it is not required. This is because the
the documentation build resides in RTD, which currently does not respect the
``[ci skip]`` directive.


Building the Documentation from source
======================================

For information about building the documentation from source, see
the :ref:`builddocs` section in the installation instructions.

Astropy Documentation Rules and Guidelines
==========================================

This section describes the standards for documentation that any contribution
being considered for integration into the core package should follow, as well as
the standard Astropy docstring format.

* All documentation text should follow the :ref:`astropy-style-guide`.

* All documentation should be written using the `Sphinx`_
  documentation tool.

* ReST substitutions are centralized in ``docs/conf.py::rst_epilog`` for
  consistency across the documentation and docstrings. These should be used over
  custom redefinitions; and new substitutions should probably be placed there.

* The `package template <https://github.com/astropy/package-template>`_ provides
  a recommended general structure for documentation.

* Docstrings must be provided for all public classes, methods, and functions.

* Docstrings should follow the `numpydoc format
  <https://numpydoc.readthedocs.io/en/latest/format.html>`_.

* References in docstrings, **including internal Astropy links**, should use the
  `intersphinx format
  <https://www.sphinx-doc.org/en/master/usage/extensions/intersphinx.html>`_.
  For example a link to the Astropy section on unit equivalencies would be
  `` :ref:`astropy:unit_equivalencies` ``.
  When built in Astropy, links starting with 'astropy' resolve to the current
  build. In affiliate packages using ``sphinx-astropy``'s intersphinx mapping,
  the links resolve to the stable version of Astropy. For linking to the
  development version, use the intersphinx target 'astropy-dev'.

* Examples and/or tutorials are strongly encouraged for typical use-cases of a
  particular module or class.

* Any external package dependencies must be explicitly mentioned in the
  documentation. They should also be recorded in the ``setup.cfg`` file in the
  root of the astropy repository using an entry in ``extras_require``,
  under ``all``.

* Configuration options using the :mod:`astropy.config` mechanisms must be
  explicitly mentioned in the documentation.


Sphinx Documentation Themes
===========================

An Astropy Project Sphinx HTML theme is included in the astropy-sphinx-theme_
package. This allows the theme to be used by both Astropy and affiliated
packages. The theme is activated by setting the theme in the global Astropy
sphinx configuration in sphinx-astropy_, which is imported in the sphinx
configuration of both Astropy and affiliated packages.

A different theme can be used by overriding a few sphinx
configuration variables set in the global configuration.

* To use a different theme, set ``html_theme`` to the name of a desired
  builtin Sphinx theme or a custom theme in ``package-name/docs/conf.py``
  (where ``'package-name'`` is "astropy" or the name of the affiliated
  package).

* To use a custom theme, additionally: place the theme in
  ``package-name/docs/_themes`` and add ``'_themes'`` to the
  ``html_theme_path`` variable. See the Sphinx_ documentation for more
  details on theming.

Sphinx extensions
=================

The documentation build process for Astropy uses a number of sphinx extensions
which are all installed automatically when installing sphinx-astropy_. These
facilitate easily documenting code in a homogeneous and readable way.

The main extensions used are:

* sphinx-automodapi_ - an extension that makes it easy to automatically
  generate API documentation.

* sphinx-gallery_ - an extension to generate example galleries

* `numpydoc`_ - an extension to parse docstrings in NumpyDoc format

In addition, the sphinx-astropy_ includes a few small extensions:

* ``sphinx_astropy.ext.edit_on_github`` - an extension to add 'Edit on GitHub'
  links to documentation pages.

* ``sphinx_astropy.ext.changelog_links`` - an extension to add links to
  pull requests when rendering the changelog.

* ``sphinx_astropy.ext.doctest`` - an extension that makes it possible to
  add metadata about doctests inside ``.rst`` files

.. _Sphinx: http://www.sphinx-doc.org/
.. _sphinx-automodapi: https://github.com/astropy/sphinx-automodapi
.. _astropy-sphinx-theme: https://github.com/astropy/astropy-sphinx-theme
.. _sphinx-astropy: https://github.com/astropy/sphinx-astropy
.. _sphinx-gallery: https://sphinx-gallery.readthedocs.io
.. _dev-build-astropy-subpkg:

************************************
Building Astropy and its Subpackages
************************************

The build process currently uses the `setuptools
<https://setuptools.readthedocs.io>`_ package to build and install the astropy
core (and any affiliated packages that use the template). As is typical, there
is a single ``setup.py`` file that is used for the whole ``astropy`` package. To
make it easier to set up C extensions for individual sub-packages, we use
`extension-helpers <https://extension-helpers.readthedocs.io/>`_, which allows
extensions to be defined inside each sub-package.

The way extension-helpers works is that it looks for ``setup_package.py`` files
anywhere in the package, and then looks for a function called ``get_extensions``
inside each of these files. This function should return a list of
``setuptools.Extension`` objects, and these are combined into an
overall list of extensions to build.

For certain string-parsing tasks, Astropy uses the
`PLY <http://www.dabeaz.com/ply/>`_ tool.  PLY generates tables that speed up
the parsing process, which are checked into source code so they don't have to
be regenerated.  These tables can be recognized by having either ``lextab`` or
``parsetab`` in their names.  To regenerate these files (e.g. if a new version
of PLY is bundled with Astropy or some of the parsing code changes), the tables
need to be deleted and the appropriate parts of astropy re-imported and run. For
exact details, see the comments in the headers of the ``parsetab`` and
``lextab`` files.

.. _dev-build-astropy-subpkg-win:

Building on Windows
*******************

The most convenient option is to use Python installation from Miniconda. If you like
Unix-like commands, Git Bash, which comes installed with Git, complements
Miniconda pretty well, as long as Miniconda is installed with the option for
it to be available system-wide (the option that is not recommended by the
installer).

Since ``astropy`` contains C extensions, you also need to install Microsoft
Visual Studio (the latest available should work) so Python can access the
system C compiler.

Once everything is set up as above, you can proceed to build ``astropy``
from source in the ``conda`` environment in an OS-agnostic way. For example:

* Create a new ``conda`` environment.
* Go to the ``astropy`` code checkout directory.
* If you have not already, fetch all of the tags from the main repository.
  If you do not have the latest tag, your developer version number will be
  wrong.
* Run ``pip install -e .`` to build ``astropy``.
.. doctest-skip-all
.. _code-guide:

*****************
Coding Guidelines
*****************

This section describes requirements and guidelines that should be followed both
by the core package and by coordinated packages, and these are also recommended
for affiliated packages.

Interface and Dependencies
==========================

* All code must be compatible with the versions of Python indicated by the
  ``python_requires`` key in the `setup.cfg
  <https://github.com/astropy/astropy/blob/main/setup.cfg>`_ file of the
  core package.

* Usage of ``six``, ``__future__``, and ``2to3`` is no longer acceptable.

* `f-strings <https://docs.python.org/3/reference/lexical_analysis.html#f-strings>`_
  should be used when possible, and if not, Python 3
  formatting should be used (i.e. ``"{0:s}".format("spam")``)
  instead of the ``%`` operator (``"%s" % "spam"``).

* The core package should be importable with no
  dependencies other than components already in the Astropy core, the
  `Python Standard Library <https://docs.python.org/3/library/index.html>`_,
  and NumPy_ |minimum_numpy_version| or later.

* Additional dependencies - such as SciPy_, Matplotlib_, or other
  third-party packages - are allowed for sub-modules or in function
  calls, but they must be noted in the package documentation and
  should only affect the relevant component.  In functions and
  methods, the optional dependency should use a normal ``import``
  statement, which will raise an ``ImportError`` if the dependency is
  not available. In the astropy core package, such optional dependencies should
  be recorded in the ``setup.cfg`` file in the ``extras_require``
  entry, under ``all`` (or ``test_all`` if the dependency is only
  needed for testing).

  At the module level, one can subclass a class from an optional dependency
  like so::

      try:
          from opdep import Superclass
      except ImportError:
          warn(AstropyWarning('opdep is not present, so <functionality>'
                              'will not work.'))
          class Superclass(object): pass

      class Customclass(Superclass):
          ...

* General utilities necessary for but not specific to the package or
  sub-package should be placed in a ``packagename.utils`` module (e.g.
  ``astropy.utils`` for the core package). If a utility is already present in
  :mod:`astropy.utils`, packages should always use that utility instead of
  re-implementing it in ``packagename.utils`` module.

Documentation and Testing
=========================

* Docstrings must be present for all public classes/methods/functions, and
  must follow the form outlined in the :doc:`docguide` document.

* Write usage examples in the docstrings of all classes and functions whenever
  possible. These examples should be short and simple to reproduce--users
  should be able to copy them verbatim and run them. These examples should,
  whenever possible, be in the :ref:`doctest <doctests>` format and will be
  executed as part of the test suite.

* Unit tests should be provided for as many public methods and functions as
  possible, and should adhere to the standards set in the :doc:`testguide`
  document.


Data and Configuration
======================

* Packages can include data in a directory named ``data`` inside a subpackage
  source directory as long as it is less than about 100 kB. These data should
  always be accessed via the :func:`~astropy.utils.data.get_pkg_data_fileobj` or
  :func:`~astropy.utils.data.get_pkg_data_filename` functions. If the data
  exceeds this size, it should be hosted outside the source code repository,
  either at a third-party location on the internet or the `astropy data server
  <https://github.com/astropy/astropy-data>`_.
  In either case, it should always be downloaded using the
  :func:`~astropy.utils.data.get_pkg_data_fileobj` or
  :func:`~astropy.utils.data.get_pkg_data_filename` functions. If a specific
  version of a data file is needed, the hash mechanism described in
  :mod:`astropy.utils.data` should be used.

* All persistent configuration should use the
  :ref:`astropy_config` mechanism.  Such configuration items
  should be placed at the top of the module or package that makes use of them,
  and supply a description sufficient for users to understand what the setting
  changes.

Standard output, warnings, and errors
=====================================

The built-in ``print(...)`` function should only be used for output that
is explicitly requested by the user, for example ``print_header(...)``
or ``list_catalogs(...)``. Any other standard output, warnings, and
errors should follow these rules:

* For errors/exceptions, one should always use ``raise`` with one of the
  built-in exception classes, or a custom exception class. The
  nondescript ``Exception`` class should be avoided as much as possible,
  in favor of more specific exceptions (`IOError`, `ValueError`,
  etc.).

* For warnings, one should always use ``warnings.warn(message,
  warning_class)``. These get redirected to ``log.warning()`` by default,
  but one can still use the standard warning-catching mechanism and custom
  warning classes. The warning class should be either
  :class:`~astropy.utils.exceptions.AstropyUserWarning` or inherit from it.

* For informational and debugging messages, one should always use
  ``log.info(message)`` and ``log.debug(message)``.

The logging system uses the built-in Python :py:mod:`logging`
module. The logger can be imported using::

    from astropy import log

Coding Style/Conventions
========================

* The code should follow the standard `PEP8 Style Guide for Python Code
  <https://www.python.org/dev/peps/pep-0008/>`_. In particular, this includes
  using only 4 spaces for indentation, and never tabs.

* Our testing infrastructure currently enforces a subset of the PEP8 style
  guide. You can check locally whether your changes have followed these by
  running the following `tox <https://tox.readthedocs.io/>`__ command::

    tox -e codestyle

* *Follow the existing coding style* within a subpackage and avoid making
  changes that are purely stylistic.  In particular, there is variation in the
  maximum line length for different subpackages (typically either 80 or 100
  characters).  Please try to maintain the style when adding or modifying code.

* The use of automatic code formatters (e.g.,
  `Black <https://black.readthedocs.io/en/stable/>`_) is strongly discouraged in
  contributions to Astropy.

* Following PEP8's recommendation, absolute imports are to be used in general.
  The exception to this is relative imports of the form
  ``from . import modname``, best when referring to files within the same
  sub-module.  This makes it clearer what code is from the current submodule
  as opposed to from another.

  .. note:: There are multiple options for testing PEP8 compliance of code,
            see :doc:`testguide` for more information.
            See :doc:`codeguide_emacs` for some configuration options for Emacs
            that helps in ensuring conformance to PEP8.

* Astropy source code should contain a comment at the beginning of the file (or
  immediately after the ``#!/usr/bin env python`` command, if relevant)
  pointing to the license for the Astropy source code.  This line should say::

      # Licensed under a 3-clause BSD style license - see LICENSE.rst

* The following naming conventions::

    import numpy as np
    import matplotlib as mpl
    import matplotlib.pyplot as plt

  should be used wherever relevant. On the other hand::

    from packagename import *

  should never be used, except as a tool to flatten the namespace of a module.
  An example of the allowed usage is given in the :ref:`import-star-example`
  example.

* Classes should either use direct variable access, or Pythons property
  mechanism for setting object instance variables. ``get_value``/``set_value``
  style methods should be used only when getting and setting the values
  requires a computationally-expensive operation. The
  :ref:`prop-get-set-example` example below illustrates this guideline.

* Classes should use the builtin `super` function when making calls to
  methods in their super-class(es) unless there are specific reasons not to.
  `super` should be used consistently in all subclasses since it does not
  work otherwise. The :ref:`super-vs-direct-example` example below illustrates
  why this is important.

* Multiple inheritance should be avoided in general without good reason.
  Multiple inheritance is complicated to implement well, which is why many
  object-oriented languages, like Java, do not allow it at all.  Python does
  enable multiple inheritance through use of the
  `C3 Linearization <https://www.python.org/download/releases/2.3/mro/>`_
  algorithm, which provides a consistent method resolution ordering.
  Non-trivial multiple-inheritance schemes should not be attempted without
  good justification, or without understanding how C3 is used to determine
  method resolution order.  However, trivial multiple inheritance using
  orthogonal base classes, known as the 'mixin' pattern, may be used.

* ``__init__.py`` files for modules should not contain any significant
  implementation code. ``__init__.py`` can contain docstrings and code for
  organizing the module layout, however (e.g. ``from submodule import *``
  in accord with the guideline above). If a module is small enough that
  it fits in one file, it should simply be a single file, rather than a
  directory with an ``__init__.py`` file.

* Command-line scripts should follow the form outlined in the :doc:`scripts`
  document.

.. _handling-unicode:

Unicode guidelines
==================

For maximum compatibility, we need to assume that writing non-ASCII
characters to the console or to files will not work.  However, for
those that have a correctly configured Unicode environment, we should
allow them to opt-in to take advantage of Unicode output when
appropriate.  Therefore, there is a global configuration option,
``astropy.conf.unicode_output`` to enable Unicode output of values, set
to `False` by default.

The following conventions should be used for classes that define the
standard string conversion methods (``__str__``, ``__repr__``,
``__bytes__``, and ``__format__``).  In the bullets
below, the phrase "string instance" is used to refer to `str`, while
"bytes instance" is used to refer to `bytes`.

- ``__repr__``: Return a "string instance" containing only 7-bit characters.

- ``__bytes__``: Return a "bytes instance" containing only 7-bit characters.

- ``__str__``: Return a "string instance".
  If ``astropy.conf.unicode_output`` is `False`, it must contain
  only 7-bit characters.  If ``astropy.conf.unicode_output`` is `True`, it
  may contain non-ASCII characters when applicable.

- ``__format__``: Return a "string instance".  If
  ``astropy.conf.unicode_output`` is `False`, it must contain only 7-bit
  characters.  If ``astropy.conf.unicode_output`` is `True`, it may contain
  non-ASCII characters when applicable.

For classes that are expected to roundtrip through strings (unicode or
bytes), the parser must accept the output of ``__str__``.
Additionally, ``__repr__`` should roundtrip when that makes sense.

This design generally follows Postel's Law: "Be liberal in what you
accept, and conservative in what you send."

The following example class shows a way to implement this::

    # -*- coding: utf-8 -*-

    from astropy import conf

    class FloatList(object):
        def __init__(self, init):
            if isinstance(init, str):
                init = init.split('')
            elif isinstance(init, bytes):
                init = init.split(b'|')
            self.x = [float(x) for x in init]

        def __repr__(self):
            # Return unicode object containing no non-ASCII characters
            return f'<FloatList [{", ".join(str(x) for x in self.x)}]>'

        def __bytes__(self):
            return b'|'.join(bytes(x) for x in self.x)

        def __str__(self):
            if astropy.conf.unicode_output:
                return ''.join(str(x) for x in self.x)
            else:
                return self.__bytes__().decode('ascii')

Additionally, there is a test helper,
``astropy.test.helper.assert_follows_unicode_guidelines`` to ensure that a
class follows the Unicode guidelines outlined above.  The following
example test will test that our example class above is compliant::

    def test_unicode_guidelines():
        from astropy.test.helper import assert_follows_unicode_guidelines
        assert_follows_unicode_guidelines(FloatList(b'5|4|3|2'), roundtrip=True)

Including C Code
================

* C extensions are only allowed when they provide a significant performance
  enhancement over pure Python, or a robust C library already exists to
  provided the needed functionality. When C extensions are used, the Python
  interface must meet the aforementioned Python interface guidelines.

* The use of Cython_ is strongly recommended for C extensions. Cython_
  extensions should store ``.pyx`` files in the source code repository,
  but not the generated ``.c`` files.

* If a C extension has a dependency on an external C library, the source code
  for the library should be bundled with the Astropy core, provided the
  license for the C library is compatible with the Astropy license.
  Additionally, the package must be compatible with using a system-installed
  library in place of the library included in Astropy, and a user installing
  the package should be able to opt-in to using the system version using
  a ``ASTROPY_USE_SYSTEM_???`` environment variable, where ``???`` is the name
  of the library, e.g. ``ASTROPY_USE_SYSTEM_WCSLIB`` (see also
  :ref:`external_c_libraries`).

* In cases where C extensions are needed but Cython_ cannot be used, the `PEP 7
  Style Guide for C Code <https://www.python.org/dev/peps/pep-0007/>`_ is
  recommended.

* C extensions (Cython_ or otherwise) should provide the necessary information
  for building the extension via the mechanisms described in
  :ref:`building-c-or-cython-extensions`.


Requirements Specific to Affiliated Packages
============================================

* Affiliated packages implementing many classes/functions not relevant to
  the affiliated package itself (for example leftover code from a previous
  package) will not be accepted - the package should only include the
  required functionality and relevant extensions.

* Affiliated packages must be registered on the `Python Package Index
  <https://pypi.org/>`_, with proper metadata for downloading and
  installing the source package.

* The ``astropy`` root package name should not be used by affiliated
  packages - it is reserved for use by the core package.

Examples
========

This section shows a few examples (not all of which are correct!) to
illustrate points from the guidelines.

.. _prop-get-set-example:

Properties vs. get\_/set\_
--------------------------

This example shows a sample class illustrating the guideline regarding the use
of properties as opposed to getter/setter methods.

Let's assuming you've defined a ``Star`` class and create an instance
like this::

    >>> s = Star(B=5.48, V=4.83)

You should always use attribute syntax like this::

    >>> s.color = 0.4
    >>> print(s.color)
    0.4

Rather than like this::

    >>> s.set_color(0.4)  # Bad form!
    >>> print(s.get_color())  # Bad form!
    0.4

Using Python properties, attribute syntax can still do anything possible with
a get/set method. For lengthy or complex calculations, however, use a method::

    >>> print(s.compute_color(5800, age=5e9))
    0.4

.. _super-vs-direct-example:

super() vs. Direct Calling
--------------------------

This example shows why the use of `super` leads to a more consistent
method resolution order than manually calling methods of the super classes in a
multiple inheritance case::

    # This is dangerous and bug-prone!

    class A(object):
        def method(self):
            print('Doing A')


    class B(A):
        def method(self):
            print('Doing B')
            A.method(self)


    class C(A):
        def method(self):
            print('Doing C')
            A.method(self)

    class D(C, B):
        def method(self):
            print('Doing D')
            C.method(self)
            B.method(self)

if you then do::

    >>> b = B()
    >>> b.method()

you will see::

    Doing B
    Doing A

which is what you expect, and similarly for C. However, if you do::

    >>> d = D()
    >>> d.method()

you might expect to see the methods called in the order D, B, C, A but instead
you see::

    Doing D
    Doing C
    Doing A
    Doing B
    Doing A

because both ``B.method()`` and ``C.method()`` call ``A.method()`` unaware of
the fact that they're being called as part of a chain in a hierarchy.  When
``C.method()`` is called it is unaware that it's being called from a subclass
that inherits from both ``B`` and ``C``, and that ``B.method()`` should be
called next.  By calling `super` the entire method resolution order for
``D`` is precomputed, enabling each superclass to cooperatively determine which
class should be handed control in the next `super` call::

    # This is safer

    class A(object):
        def method(self):
            print('Doing A')

    class B(A):
        def method(self):
            print('Doing B')
            super().method()


    class C(A):
        def method(self):
            print('Doing C')
            super().method()

    class D(C, B):
        def method(self):
            print('Doing D')
            super().method()

::

    >>> d = D()
    >>> d.method()
    Doing D
    Doing C
    Doing B
    Doing A

As you can see, each superclass's method is entered only once.  For this to
work it is very important that each method in a class that calls its
superclass's version of that method use `super` instead of calling the
method directly.  In the most common case of single-inheritance, using
``super()`` is functionally equivalent to calling the superclass's method
directly.  But as soon as a class is used in a multiple-inheritance
hierarchy it must use ``super()`` in order to cooperate with other classes in
the hierarchy.

.. note:: For more information on the the benefits of `super`, see
          https://rhettinger.wordpress.com/2011/05/26/super-considered-super/

.. _import-star-example:

Acceptable use of ``from module import *``
------------------------------------------

``from module import *`` is discouraged in a module that contains
implementation code, as it impedes clarity and often imports unused variables.
It can, however, be used for a package that is laid out in the following
manner::

    packagename
    packagename/__init__.py
    packagename/submodule1.py
    packagename/submodule2.py

In this case, ``packagename/__init__.py`` may be::

    """
    A docstring describing the package goes here
    """
    from submodule1 import *
    from submodule2 import *

This allows functions or classes in the submodules to be used directly as
``packagename.foo`` rather than ``packagename.submodule1.foo``. If this is
used, it is strongly recommended that the submodules make use of the ``__all__``
variable to specify which modules should be imported. Thus, ``submodule2.py``
might read::

    from numpy import array, linspace

    __all__ = ['foo', 'AClass']

    def foo(bar):
        # the function would be defined here
        pass

    class AClass(object):
        # the class is defined here
        pass

This ensures that ``from submodule import *`` only imports ``foo`` and
``AClass``, but not `numpy.array` or `numpy.linspace`.


Additional Resources
====================

Further tips and hints relating to the coding guidelines are included below.

.. toctree::
    :maxdepth: 1

    codeguide_emacs

.. _Numpy: https://numpy.org/
.. _Scipy: https://www.scipy.org/
.. _matplotlib: https://matplotlib.org/
.. _Cython: https://cython.org/
.. _PyPI: https://pypi.org/project
.. doctest-skip-all

.. _testing-guidelines:

******************
Testing Guidelines
******************

This section describes the testing framework and format standards for tests in
Astropy core and coordinated packages, and also serves as recommendations for
affiliated packages.

Testing Framework
*****************

The testing framework used by astropy (and packages using the :doc:`Astropy
package template <astropy-package-template>`) is the `pytest`_ framework.

.. _testing-dependencies:

Testing Dependencies
********************

The dependencies used by the Astropy test runner are provided by a separate
package called `pytest-astropy`_. This package provides the ``pytest``
dependency itself, in addition to several ``pytest`` plugins that are used by
Astropy, and will also be of general use to other packages.

Since the testing dependencies are not actually required to install or use
Astropy, they are not included in ``install_requires`` in ``setup.cfg``.
Instead, they are listed in an ``extras_require`` section called ``test`` in
``setup.cfg``. Developers who want to run the test suite will need to either
install pytest-astropy directly::

    pip install pytest-astropy

or install the core package in 'editable' mode specifying the ``[test]``
option::

    pip install -e .[test]

A detailed description of the plugins can be found in the :ref:`pytest-plugins`
section.

.. _running-tests:

Running Tests
*************

There are currently three different ways to invoke Astropy tests. Each
method invokes `pytest`_ to run the tests but offers different options when
calling. To run the tests, you will need to make sure you have the `pytest`_
package installed.

In addition to running the Astropy tests, these methods can also be called
so that they check Python source code for `PEP8 compliance
<https://www.python.org/dev/peps/pep-0008/>`_. All of the PEP8 testing
options require the `pytest-pep8 plugin
<https://pypi.org/project/pytest-pep8>`_, which must be installed
separately.


tox
===

The most robust way to run the tests (which can also be the slowest) is
to make use of `Tox <https://tox.readthedocs.io/en/latest/>`__, which is a
general purpose tool for automating Python testing. One of the benefits of tox
is that it first creates a source distribution of the package being tested, and
installs it into a new virtual environment, along with any dependencies that are
declared in the package, before running the tests. This can therefore catch
issues related to undeclared package data, or missing dependencies. Since we use
tox to run many of the tests on continuous integration services, it can also be
used in many cases to reproduce issues seen on those services.

To run the tests with tox, first make sure that tox is installed, e.g.::

    pip install tox

then run the basic test suite with::

    tox -e test

or run the test suite with all optional dependencies with::

    tox -e test-alldeps

You can see a list of available test environments with::

    tox -l -v

which will also explain what each of them does.

You can also run checks or commands not directly related to tests - for instance::

    tox -e codestyle

will run checks using the flake8 tool.

Is is possible to pass options to pytest when running tox - to do this, add a
``--`` after the regular tox command, and anything after this will be passed to
pytest, e.g.::

    tox -e test -- -v --pdb

This can be used in conjunction with the ``-P`` option provided by the
`pytest-filter-subpackage <https://github.com/astropy/pytest-filter-subpackage>`_
plugin to run just part of the test suite.

.. _running-pytest:

pytest
======

The test suite can also be run directly from the native ``pytest`` command,
which is generally faster than using tox for iterative development. In
this case, it is important for developers to be aware that they must manually
rebuild any extensions by running::

    pip install -e .[test]

before running the test with pytest with::

    pytest

Instead of calling ``pip install -e .[test]``, you can also build the
extensions with::

    python setup.py build_ext --inplace

which avoids also installing the developer version of astropy into your current
environment - however note that the ``pip`` command is required if you need to
test parts of the package that rely on certain `entry points
<https://setuptools.readthedocs.io/en/latest/pkg_resources.html#entry-points>`_
being installed.

It is possible to run only the tests for a particular subpackage or set of
subpackages.  For example, to run only the ``wcs`` tests from the
commandline::

    pytest -P wcs

Or, to run only the ``wcs`` and ``utils`` tests::

    pytest -P wcs,utils

You can also specify a single directory or file to test from the commandline,
e.g.::

    pytest astropy/modeling

or::

    pytest astropy/wcs/tests/test_wcs.py

and this works for ``.rst`` files too::

    pytest astropy/wcs/index.rst

.. _astropy.test():

astropy.test()
==============

Tests can be run from an installed version of Astropy with::

    import astropy
    astropy.test()

This will run all the default tests for Astropy (but will not run the
documentation tests in the ``.rst`` documentation since those files are
not installed).

Tests for a specific package can be run by specifying the package in the call
to the ``test()`` function::

    astropy.test(package='io.fits')

This method works only with package names that can be mapped to Astropy
directories. As an alternative you can test a specific directory or file
with the ``test_path`` option::

  astropy.test(test_path='wcs/tests/test_wcs.py')

The ``test_path`` must be specified either relative to the working directory
or absolutely.

By default `astropy.test()`_ will skip tests which retrieve data from the
internet. To turn these tests on use the ``remote_data`` flag::

    astropy.test(package='io.fits', remote_data=True)

In addition, the ``test`` function supports any of the options that can be
passed to :ref:`pytest.main() <pytest:pytest.main-usage>`
and convenience options ``verbose=`` and ``pastebin=``.

Enable PEP8 compliance testing with ``pep8=True`` in the call to
``astropy.test``. This will enable PEP8 checking and disable regular tests.

Astropy Test Function
---------------------

.. autofunction:: astropy.test

Test-running options
====================

.. _open-files:

Testing for open files
----------------------

Using the :ref:`openfiles-plugin` plugin (which is installed automatically
when installing pytest-astropy),  we can test whether any of the unit tests
inadvertently leave any files open.  Since this greatly slows down the time it
takes to run the tests, it is turned off by default.

To use it from the commandline, do::

    pytest --open-files

To use it from Python, do::

    >>> import astropy
    >>> astropy.test(open_files=True)

For more information on the ``pytest-openfiles`` plugin see
:ref:`openfiles-plugin`

Test coverage reports
---------------------

Coverage reports can be generated using the `pytest-cov
<https://pypi.org/project/pytest-cov/>`_ plugin (which is installed
automatically when installing pytest-astropy) by using e.g.::

    pytest --cov astropy --cov-report html

There is some configuration inside the ``setup.cfg`` file that
defines files to omit as well as lines to exclude.

Running tests in parallel
-------------------------

It is possible to speed up astropy's tests using the `pytest-xdist
<https://pypi.org/project/pytest-xdist>`_ plugin.

Once installed, tests can be run in parallel using the ``'-n'``
commandline option. For example, to use 4 processes::

    pytest -n 4

Pass ``-n auto`` to create the same number of processes as cores
on your machine.

Similarly, this feature can be invoked from ``astropy.test``::

    >>> import astropy
    >>> astropy.test(parallel=4)

.. _writing-tests:

Writing tests
*************

``pytest`` has the following test discovery rules:

 * ``test_*.py`` or ``*_test.py`` files
 * ``Test`` prefixed classes (without an ``__init__`` method)
 * ``test_`` prefixed functions and methods

Consult the :ref:`test discovery rules <pytest:python test discovery>`
for detailed information on how to name files and tests so that they are
automatically discovered by `pytest`_.

Simple example
==============

The following example shows a simple function and a test to test this
function::

    def func(x):
        """Add one to the argument."""
        return x + 1

    def test_answer():
        """Check the return value of func() for an example argument."""
        assert func(3) == 5

If we place this in a ``test.py`` file and then run::

    pytest test.py

The result is::

    ============================= test session starts ==============================
    python: platform darwin -- Python 3.x.x -- pytest-x.x.x
    test object 1: /Users/username/tmp/test.py

    test.py F

    =================================== FAILURES ===================================
    _________________________________ test_answer __________________________________

        def test_answer():
    >       assert func(3) == 5
    E       assert 4 == 5
    E        +  where 4 = func(3)

    test.py:5: AssertionError
    =========================== 1 failed in 0.07 seconds ===========================

Where to put tests
==================

Package-specific tests
----------------------

Each package should include a suite of unit tests, covering as many of
the public methods/functions as possible. These tests should be
included inside each sub-package, e.g::

    astropy/io/fits/tests/

``tests`` directories should contain an ``__init__.py`` file so that
the tests can be imported and so that they can use relative imports.

Interoperability tests
----------------------

Tests involving two or more sub-packages should be included in::

    astropy/tests/

Regression tests
================

Any time a bug is fixed, and wherever possible, one or more regression tests
should be added to ensure that the bug is not introduced in future. Regression
tests should include the ticket URL where the bug was reported.

.. _data-files:

Working with data files
=======================

Tests that need to make use of a data file should use the
`~astropy.utils.data.get_pkg_data_fileobj` or
`~astropy.utils.data.get_pkg_data_filename` functions.  These functions
search locally first, and then on the astropy data server or an arbitrary
URL, and return a file-like object or a local filename, respectively.  They
automatically cache the data locally if remote data is obtained, and from
then on the local copy will be used transparently.  See the next section for
note specific to dealing with the cache in tests.

They also support the use of an MD5 hash to get a specific version of a data
file.  This hash can be obtained prior to submitting a file to the astropy
data server by using the `~astropy.utils.data.compute_hash` function on a
local copy of the file.

Tests that may retrieve remote data should be marked with the
``@pytest.mark.remote_data`` decorator, or, if a doctest, flagged with the
``REMOTE_DATA`` flag.  Tests marked in this way will be skipped by default by
``astropy.test()`` to prevent test runs from taking too long. These tests can
be run by ``astropy.test()`` by adding the ``remote_data='any'`` flag.  Turn on
the remote data tests at the command line with ``pytest --remote-data=any``.

It is possible to mark tests using
``@pytest.mark.remote_data(source='astropy')``, which can be used to indicate
that the only required data is from the http://data.astropy.org server. To
enable just these tests, you can run the
tests with ``pytest --remote-data=astropy``.

For more information on the ``pytest-remotedata`` plugin, see
:ref:`remotedata-plugin`.

Examples
--------
.. code-block:: python

    from ...config import get_data_filename

    def test_1():
        """Test version using a local file."""
        #if filename.fits is a local file in the source distribution
        datafile = get_data_filename('filename.fits')
        # do the test

    @pytest.mark.remote_data
    def test_2():
        """Test version using a remote file."""
        #this is the hash for a particular version of a file stored on the
        #astropy data server.
        datafile = get_data_filename('hash/94935ac31d585f68041c08f87d1a19d4')
        # do the test

    def doctest_example():
        """
        >>> datafile = get_data_filename('hash/94935')  # doctest: +REMOTE_DATA
        """
        pass

The ``get_remote_test_data`` will place the files in a temporary directory
indicated by the ``tempfile`` module, so that the test files will eventually
get removed by the system. In the long term, once test data files become too
large, we will need to design a mechanism for removing test data immediately.

Tests that use the file cache
-----------------------------

By default, the Astropy test runner sets up a clean file cache in a temporary
directory that is used only for that test run and then destroyed.  This is to
ensure consistency between test runs, as well as to not clutter users' caches
(i.e. the cache directory returned by `~astropy.config.get_cache_dir`) with
test files.

However, some test authors (especially for affiliated packages) may find it
desirable to cache files downloaded during a test run in a more permanent
location (e.g. for large data sets).  To this end the
`~astropy.config.set_temp_cache` helper may be used.  It can be used either as
a context manager within a test to temporarily set the cache to a custom
location, or as a *decorator* that takes effect for an entire test function
(not including setup or teardown, which would have to be decorated separately).

Furthermore, it is possible to change the location of the cache directory
for the duration of the test run by setting the ``XDG_CACHE_HOME``
environment variable.


Tests that create files
=======================

Tests may often be run from directories where users do not have write
permissions so tests which create files should always do so in
temporary directories. This can be done with the
:ref:`pytest 'tmpdir' fixture <pytest:tmpdir handling>` or with
Python's built-in :ref:`tempfile module <python:tempfile-examples>`.


Setting up/Tearing down tests
=============================

In some cases, it can be useful to run a series of tests requiring something
to be set up first. There are four ways to do this:

Module-level setup/teardown
---------------------------

If the ``setup_module`` and ``teardown_module`` functions are specified in a
file, they are called before and after all the tests in the file respectively.
These functions take one argument, which is the module itself, which makes it
very easy to set module-wide variables::

    def setup_module(module):
        """Initialize the value of NUM."""
        module.NUM = 11

    def add_num(x):
        """Add pre-defined NUM to the argument."""
        return x + NUM

    def test_42():
        """Ensure that add_num() adds the correct NUM to its argument."""
        added = add_num(42)
        assert added == 53

We can use this for example to download a remote test data file and have all
the functions in the file access it::

    import os

    def setup_module(module):
        """Store a copy of the remote test file."""
        module.DATAFILE = get_remote_test_data('94935ac31d585f68041c08f87d1a19d4')

    def test():
        """Perform test using cached remote input file."""
        f = open(DATAFILE, 'rb')
        # do the test

    def teardown_module(module):
        """Clean up remote test file copy."""
        os.remove(DATAFILE)

Class-level setup/teardown
--------------------------

Tests can be organized into classes that have their own setup/teardown
functions. In the following ::

    def add_nums(x, y):
        """Add two numbers."""
        return x + y

    class TestAdd42(object):
        """Test for add_nums with y=42."""

        def setup_class(self):
            self.NUM = 42

        def test_1(self):
            """Test behavior for a specific input value."""
            added = add_nums(11, self.NUM)
            assert added == 53

        def test_2(self):
            """Test behavior for another input value."""
            added = add_nums(13, self.NUM)
            assert added == 55

        def teardown_class(self):
            pass

In the above example, the ``setup_class`` method is called first, then all the
tests in the class, and finally the ``teardown_class`` is called.

Method-level setup/teardown
---------------------------

There are cases where one might want setup and teardown methods to be run
before and after *each* test. For this, use the ``setup_method`` and
``teardown_method`` methods::

    def add_nums(x, y):
        """Add two numbers."""
        return x + y

    class TestAdd42(object):
        """Test for add_nums with y=42."""

        def setup_method(self, method):
            self.NUM = 42

        def test_1(self):
        """Test behavior for a specific input value."""
            added = add_nums(11, self.NUM)
            assert added == 53

        def test_2(self):
        """Test behavior for another input value."""
            added = add_nums(13, self.NUM)
            assert added == 55

        def teardown_method(self, method):
            pass

Function-level setup/teardown
-----------------------------

Finally, one can use ``setup_function`` and ``teardown_function`` to define a
setup/teardown mechanism to be run before and after each function in a module.
These take one argument, which is the function being tested::

    def setup_function(function):
        pass

    def test_1(self):
       """First test."""
        # do test

    def test_2(self):
        """Second test."""
        # do test

    def teardown_function(function):
        pass

Property-based tests
====================

`Property-based testing
<https://increment.com/testing/in-praise-of-property-based-testing/>`_
lets you focus on the parts of your test that matter, by making more
general claims - "works for any two numbers" instead of "works for 1 + 2".
Imagine if random testing gave you minimal, non-flaky failing examples,
and a clean way to describe even the most complicated data - that's
property-based testing!

``pytest-astropy`` includes a dependency on `Hypothesis
<https://hypothesis.readthedocs.io/>`_, so installation is easy -
you can just read the docs or `work through the tutorial
<https://github.com/Zac-HD/escape-from-automanual-testing/>`_
and start writing tests like::

    from astropy.coordinates import SkyCoord
    from hypothesis import given, strategies as st

    @given(
        st.builds(SkyCoord, ra=st.floats(0, 360), dec=st.floats(-90, 90))
    )
    def test_coordinate_transform(coord):
        """Test that sky coord can be translated from ICRS to Galactic and back."""
        assert coord == coord.galactic.icrs  # floating-point precision alert!

Other properties that you could test include:

- Round-tripping from image to sky coordinates and back should be lossless
  for distortion-free mappings, and otherwise always below 10^-5 px.
- Take a moment in time, round-trip it through various frames, and check it
  hasn't changed or lost precision. (or at least not by more than a nanosecond)
- IO routines losslessly round-trip data that they are expected to handle
- Optimised routines calculate the same result as unoptimised, within tolerances

This is a great way to start contributing to Astropy, and has already found
bugs in time handling.  See issue #9017 and pull request #9532 for details!

(and if you find Hypothesis useful in your research,
`please cite it <https://doi.org/10.21105/joss.01891>`_!)


Parametrizing tests
===================

If you want to run a test several times for slightly different values,
you can use ``pytest`` to avoid writing separate tests.
For example, instead of writing::

    def test1():
        assert type('a') == str

    def test2():
        assert type('b') == str

    def test3():
        assert type('c') == str

You can use the ``@pytest.mark.parametrize`` decorator to concisely
create a test function for each input::

    @pytest.mark.parametrize(('letter'), ['a', 'b', 'c'])
    def test(letter):
        """Check that the input is a string."""
        assert type(letter) == str

As a guideline, use ``parametrize`` if you can enumerate all possible
test cases and each failure would be a distinct issue, and Hypothesis
when there are many possible inputs or you only want a single simple
failure to be reported.

Tests requiring optional dependencies
=====================================

For tests that test functions or methods that require optional dependencies
(e.g., Scipy), pytest should be instructed to skip the test if the dependencies
are not present, as the ``astropy`` tests should succeed even if an optional
dependency is not present. ``astropy`` provides a list of boolean flags that
test whether optional dependencies are installed (at import time). For example,
to load the corresponding flag for Scipy and mark a test to skip if Scipy is not
present, use::

    import pytest
    from astropy.utils.compat.optional_deps import HAS_SCIPY

    @pytest.mark.skipif(not HAS_SCIPY, reason='scipy is required')
    def test_that_uses_scipy():
        ...

These variables should exist for all of Astropy's optional dependencies; a
complete list of supported flags can be found in
``astropy.utils.compat.optional_deps``.

Any new optional dependencies should be added to that file, as well as to
relevant entries in ``setup.cfg`` under ``options.extras_require``:
typically, under ``all`` for dependencies used in user-facing code
(e.g., ``h5py``, which is used to write tables to HDF5 format),
and in ``test_all`` for dependencies only used in tests (e.g.,
``skyfield``, which is used to cross-check the accuracy of coordinate
transforms).

Using pytest helper functions
=============================

If your tests need to use `pytest helper functions
<https://docs.pytest.org/en/latest/reference/reference.html#functions>`_, such as
``pytest.raises``, import ``pytest`` into your test module like so::

    import pytest

Testing warnings
================

In order to test that warnings are triggered as expected in certain
situations,
`pytest`_ provides its own context manager
:ref:`pytest.warns <pytest:warns>` that, completely
analogously to ``pytest.raises`` (see below) allows to probe explicitly
for specific warning classes and, through the optional ``match`` argument,
messages. Note that when no warning of the specified type is
triggered, this will make the test fail. When checking for optional,
but not mandatory warnings, ``pytest.warns(None)`` can be used to catch and
inspect them.

.. note::

   With `pytest`_ there is also the option of using the
   :ref:`recwarn <pytest:recwarn>` function argument to test that
   warnings are triggered within the entire embedding function.
   This method has been found to be problematic in at least one case
   (`pull request 1174 <https://github.com/astropy/astropy/pull/1174#issuecomment-20249309>`_).

Testing exceptions
==================

Just like the handling of warnings described above, tests that are
designed to trigger certain errors should verify that an exception of
the expected type is raised in the expected place.  This is efficiently
done by running the tested code inside the
:ref:`pytest.raises <pytest:assertraises>`
context manager.  Its optional ``match`` argument allows to check the
error message for any patterns using ``regex`` syntax.  For example the
matches ``pytest.raises(OSError, match=r'^No such file')`` and
``pytest.raises(OSError, match=r'or directory$')`` would be equivalent
to ``assert str(err).startswith(No such file)`` and ``assert
str(err).endswith(or directory)``, respectively, on the raised error
message ``err``.
For matching multi-line messages you need to pass the ``(?s)``
:ref:`flag <python:re-syntax>`
to the underlying ``re.search``, as in the example below::

  with pytest.raises(fits.VerifyError, match=r'(?s)not upper.+ Illegal key') as excinfo:
      hdu.verify('fix+exception')
  assert str(excinfo.value).count('Card') == 2

This invocation also illustrates how to get an ``ExceptionInfo`` object
returned to perform additional diagnostics on the info.

Testing configuration parameters
================================

In order to ensure reproducibility of tests, all configuration items
are reset to their default values when the test runner starts up.

Sometimes you'll want to test the behavior of code when a certain
configuration item is set to a particular value.  In that case, you
can use the `astropy.config.ConfigItem.set_temp` context manager to
temporarily set a configuration item to that value, test within that
context, and have it automatically return to its original value.

For example::

    def test_pprint():
        from ... import conf
        with conf.set_temp('max_lines', 6):
            # ...

Marking blocks of code to exclude from coverage
===============================================

Blocks of code may be ignored by the coverage testing by adding a
comment containing the phrase ``pragma: no cover`` to the start of the
block::

    if this_rarely_happens:  # pragma: no cover
        this_call_is_ignored()

.. _image-tests:

Image tests with pytest-mpl
===========================

Running image tests
-------------------

We make use of the `pytest-mpl <https://pypi.org/project/pytest-mpl>`_
plugin to write tests where we can compare the output of plotting commands
with reference files on a pixel-by-pixel basis (this is used for instance in
:ref:`astropy.visualization.wcsaxes <wcsaxes>`).

To run the Astropy tests with the image comparison, use::

    pytest --mpl --remote-data=astropy

However, note that the output can be very sensitive to the version of Matplotlib
as well as all its dependencies (e.g., freetype), so we recommend running the
image tests inside a `Docker <https://www.docker.com/>`__ container which has a
frozen set of package versions (Docker containers can be thought of as mini
virtual machines). See our ``.circleci/config.yml`` for reference.

Writing image tests
-------------------

The `README.rst <https://github.com/matplotlib/pytest-mpl/blob/master/README.rst>`__
for the plugin contains information on writing tests with this plugin. The only
key addition compared to those instructions is that you should set
``baseline_dir``::

    from astropy.tests.image_tests import IMAGE_REFERENCE_DIR

    @pytest.mark.mpl_image_compare(baseline_dir=IMAGE_REFERENCE_DIR)

This is because since the reference image files would contribute significantly
to the repository size, we instead store them on the http://data.astropy.org
site. The downside is that it is a little more complicated to create or
re-generate reference files, but we describe the process here.

Generating reference images
---------------------------

Any failed test on CircleCI would provide you with the old and the new reference
images, along with the difference image. After you have determined that the
new reference image is acceptable, you could download it from the "artifacts"
tab on the CircleCI dashboard.

Uploading the reference images
------------------------------

Next, we need to add these images to the http://data.astropy.org server. To do
this, open a pull request to `this <https://github.com/astropy/astropy-data>`_
repository. The reference images for Astropy tests should go inside the
`testing/astropy <https://github.com/astropy/astropy-data/tree/gh-pages/testing/astropy>`_
directory. In that directory are folders named as timestamps. If you are simply
adding new tests, add the reference files to the most recent directory.

If you are re-generating baseline images due to changes in Astropy, make a new
timestamp directory by copying one the most recent one, then replace any
baseline images that have changed. Note that due to changes between Matplotlib
versions, we need to add the whole set of reference images for each major
Matplotlib version. Therefore, in each timestamp folder, there are folders named
e.g. ``1.4.x`` and ``1.5.x``.

Once the reference images are merged in and available on
http://data.astropy.org, update the timestamp in the ``IMAGE_REFERENCE_DIR``
variable in the ``astropy.tests.image_tests`` sub-module. Because the timestamp
is hard-coded, adding a new timestamp directory will not mess with testing for
released versions of Astropy, so you can easily add and tweak a new timestamp
directory while still working on a pull request to Astropy.

.. _doctests:

Writing doctests
****************

A doctest in Python is a special kind of test that is embedded in a
function, class, or module's docstring, or in the narrative Sphinx
documentation, and is formatted to look like a Python interactive
session--that is, they show lines of Python code entered at a ``>>>``
prompt followed by the output that would be expected (if any) when
running that code in an interactive session.

The idea is to write usage examples in docstrings that users can enter
verbatim and check their output against the expected output to confirm that
they are using the interface properly.

Furthermore, Python includes a :mod:`doctest` module that can detect these
doctests and execute them as part of a project's automated test suite.  This
way we can automatically ensure that all doctest-like examples in our
docstrings are correct.

The Astropy test suite automatically detects and runs any doctests in the
astropy source code or documentation, or in packages using the Astropy test
running framework. For example doctests and detailed documentation on how to
write them, see the full :mod:`doctest` documentation.

.. note::

   Since the narrative Sphinx documentation is not installed alongside the
   astropy source code, it can only be tested by running ``pytest`` directly (or
   via tox), not by ``import astropy; astropy.test()``.

For more information on the ``pytest-doctestplus`` plugin used by Astropy, see
:ref:`doctestplus-plugin`.

.. _skipping-doctests:

Skipping doctests
=================

Sometimes it is necessary to write examples that look like doctests but that
are not actually executable verbatim. An example may depend on some external
conditions being fulfilled, for example. In these cases there are a few ways to
skip a doctest:

1. Next to the example add a comment like: ``# doctest: +SKIP``.  For example:

   .. code-block:: none

     >>> import os
     >>> os.listdir('.')  # doctest: +SKIP

   In the above example we want to direct the user to run ``os.listdir('.')``
   but we don't want that line to be executed as part of the doctest.

   To skip tests that require fetching remote data, use the ``REMOTE_DATA``
   flag instead.  This way they can be turned on using the
   ``--remote-data`` flag when running the tests:

   .. code-block:: none

     >>> datafile = get_data_filename('hash/94935')  # doctest: +REMOTE_DATA

2. Astropy's test framework adds support for a special ``__doctest_skip__``
   variable that can be placed at the module level of any module to list
   functions, classes, and methods in that module whose doctests should not
   be run.  That is, if it doesn't make sense to run a function's example
   usage as a doctest, the entire function can be skipped in the doctest
   collection phase.

   The value of ``__doctest_skip__`` should be a list of wildcard patterns
   for all functions/classes whose doctests should be skipped.  For example::

       __doctest_skip__ = ['myfunction', 'MyClass', 'MyClass.*']

   skips the doctests in a function called ``myfunction``, the doctest for a
   class called ``MyClass``, and all *methods* of ``MyClass``.

   Module docstrings may contain doctests as well.  To skip the module-level
   doctests include the string ``'.'`` in ``__doctest_skip__``.

   To skip all doctests in a module::

       __doctest_skip__ = ['*']

3. In the Sphinx documentation, a doctest section can be skipped by
   making it part of a ``doctest-skip`` directive::

       .. doctest-skip::

           >>> # This is a doctest that will appear in the documentation,
           >>> # but will not be executed by the testing framework.
           >>> 1 / 0  # Divide by zero, ouch!

   It is also possible to skip all doctests below a certain line using
   a ``doctest-skip-all`` comment.  Note the lack of ``::`` at the end
   of the line here::

       .. doctest-skip-all

       All doctests below here are skipped...

4. ``__doctest_requires__`` is a way to list dependencies for specific
   doctests.  It should be a dictionary mapping wildcard patterns (in the same
   format as ``__doctest_skip__``) to a list of one or more modules that should
   be *importable* in order for the tests to run.  For example, if some tests
   require the scipy module to work they will be skipped unless ``import
   scipy`` is possible.  It is also possible to use a tuple of wildcard
   patterns as a key in this dict::

            __doctest_requires__ = {('func1', 'func2'): ['scipy']}

   Having this module-level variable will require ``scipy`` to be importable
   in order to run the doctests for functions ``func1`` and ``func2`` in that
   module.

   In the Sphinx documentation, a doctest requirement can be notated with the
   ``doctest-requires`` directive::

       .. doctest-requires:: scipy

           >>> import scipy
           >>> scipy.hamming(...)


Skipping output
===============

One of the important aspects of writing doctests is that the example output
can be accurately compared to the actual output produced when running the
test.

The doctest system compares the actual output to the example output verbatim
by default, but this not always feasible.  For example the example output may
contain the ``__repr__`` of an object which displays its id (which will change
on each run), or a test that expects an exception may output a traceback.

The simplest way to generalize the example output is to use the ellipses
``...``.  For example::

    >>> 1 / 0
    Traceback (most recent call last):
    ...
    ZeroDivisionError: integer division or modulo by zero

This doctest expects an exception with a traceback, but the text of the
traceback is skipped in the example output--only the first and last lines
of the output are checked.  See the :mod:`doctest` documentation for
more examples of skipping output.

Ignoring all output
-------------------

Another possibility for ignoring output is to use the
``# doctest: +IGNORE_OUTPUT`` flag.  This allows a doctest to execute (and
check that the code executes without errors), but allows the entire output
to be ignored in cases where we don't care what the output is.  This differs
from using ellipses in that we can still provide complete example output, just
without the test checking that it is exactly right.  For example::

    >>> print('Hello world')  # doctest: +IGNORE_OUTPUT
    We don't really care what the output is as long as there were no errors...

.. _handling-float-output:

Handling float output
=====================

Some doctests may produce output that contains string representations of
floating point values.  Floating point representations are often not exact and
contain roundoffs in their least significant digits.  Depending on the platform
the tests are being run on (different Python versions, different OS, etc.) the
exact number of digits shown can differ.  Because doctests work by comparing
strings this can cause such tests to fail.

To address this issue, the ``pytest-doctestplus`` plugin provides support for a
``FLOAT_CMP`` flag that can be used with doctests.  For example:

.. code-block:: none

  >>> 1.0 / 3.0  # doctest: +FLOAT_CMP
  0.333333333333333311

When this flag is used, the expected and actual outputs are both parsed to find
any floating point values in the strings.  Those are then converted to actual
Python `float` objects and compared numerically.  This means that small
differences in representation of roundoff digits will be ignored by the
doctest.  The values are otherwise compared exactly, so more significant
(albeit possibly small) differences will still be caught by these tests.

Continuous integration
**********************

Overview
========

Astropy uses the following continuous integration (CI) services:

* `GitHub Actions <https://github.com/astropy/astropy/actions>`_ for
  Linux, OS X, and Windows setups
  (Note: GitHub Actions does not have "allowed failures" yet, so you might
  see a fail job reported for your PR with "(Allowed Failure)" in its name.
  Still, some failures might be real and related to your changes, so check
  it anyway!)
* `CircleCI <https://circleci.com>`_ for visualization tests

These continuously test the package for each commit and pull request that is
pushed to GitHub to notice when something breaks.

In some cases, you may see failures on continuous integration services that
you do not see locally, for example because the operating system is different,
or because the failure happens with only 32-bit Python.

.. _pytest-plugins:

Pytest Plugins
**************

The following ``pytest`` plugins are maintained and used by Astropy. They are
included as dependencies to the ``pytest-astropy`` package, which is now
required for testing Astropy. More information on all of the  plugins provided
by the ``pytest-astropy`` package (including dependencies not maintained by
Astropy) can be found `here <https://github.com/astropy/pytest-astropy>`__.

.. _remotedata-plugin:

pytest-remotedata
=================

The `pytest-remotedata`_ plugin allows developers to control whether to run
tests that access data from the internet. The plugin provides two decorators
that can be used to mark individual test functions or entire test classes:

* ``@pytest.mark.remote_data`` for tests that require data from the internet
* ``@pytest.mark.internet_off`` for tests that should run only when there is no
  internet access. This is useful for testing local data caches or fallbacks
  for when no network access is available.

The plugin also adds the ``--remote-data`` option to the ``pytest`` command
(which is also made available through the Astropy test runner).

If the ``--remote-data`` option is not provided when running the test suite, or
if ``--remote-data=none`` is provided, all tests that are marked with
``remote_data`` will be skipped. All tests that are marked with
``internet_off`` will be executed. Any test that attempts to access the
internet but is not marked with ``remote_data`` will result in a failure.

Providing either the ``--remote-data`` option, or ``--remote-data=any``, will
cause all tests marked with ``remote_data`` to be executed. Any tests that are
marked with ``internet_off`` will be skipped.

Running the tests with ``--remote-data=astropy`` will cause only tests that
receive remote data from Astropy data sources to be run. Tests with any other
data sources will be skipped. This is indicated in the test code by marking
test functions with ``@pytest.mark.remote_data(source='astropy')``. Tests
marked with ``internet_off`` will also be skipped in this case.

Also see :ref:`data-files`.

.. _doctestplus-plugin:

pytest-doctestplus
==================

The `pytest-doctestplus`_ plugin provides advanced doctest features, including:

* handling doctests that use remote data in conjunction with the
  ``pytest-remotedata`` plugin above (see :ref:`data-files`)
* approximate floating point comparison for doctests that produce floating
  point results (see :ref:`handling-float-output`)
* skipping particular classes, methods, and functions when running doctests
  (see :ref:`skipping-doctests`)
* optional inclusion of ``*.rst`` files for doctests

This plugin provides two command line options: ``--doctest-plus`` for enabling
the advanced features mentioned above, and ``--doctest-rst`` for including
``*.rst`` files in doctest collection.

The Astropy test runner enables both of these options by default. When running
the test suite directly from ``pytest`` (instead of through the Astropy test
runner), it is necessary to explicitly provide these options when they are
needed.

.. _openfiles-plugin:

pytest-openfiles
================

The `pytest-openfiles`_ plugin allows for the detection of open I/O resources
at the end of unit tests. This plugin adds the ``--open-files`` option to the
``pytest`` command (which is also exposed through the Astropy test runner).

When running tests with ``--open-files``, if a file is opened during the course
of a unit test but that file  not closed before the test finishes, the test
will fail. This is particularly useful for testing code that manipulates file
handles or other I/O resources. It allows developers to ensure that this kind
of code properly cleans up I/O resources when they are no longer needed.

Also see :ref:`open-files`.
*********************************
When to rebase and squash commits
*********************************

This page describes recommendations for when to rebase pull requests and when to
combine/squash commits.

When to remove or combine/squash commits
========================================

Pull requests **must** be rebased and at least partially squashed (but not
necessarily squashed to a single commit) if large (approximately >10KB)
non-source code files (e.g. images, data files, etc.) are added and then removed
or modified in the PR commit history (The squashing should remove all but the
last addition of the file to not use extra space in the repository).

Combining/squashing commits is **encouraged** when the number of commits
is excessive for the changes made. The definition of 'excessive' is
subjective, but in general one should attempt to have individual commits be
units of change, and not include reversions.
As a concrete example, for a change affecting < 10 lines of source code and
including a changelog entry, more than a few commits would be excessive.
For a larger pull request adding significant functionality, however, more
commits may well be appropriate.

As another guideline, squashing should remove extraneous information but
should not be used to remove useful information for how a PR was developed.  For
example, 4 commits that are testing  changes and have a commit message of just
"debug" should be squashed.  But a series of commit messages that are
"Implemented feature X", "added test for feature X", "fixed bugs revealed by
tests for feature X" are useful information and should not be squashed away
without reason.

When squashing, extra care should be taken to keep authorship credit to all
individuals who provided substantial contribution to the given PR,
e.g. only squash commits made by the same author.

In all cases, be mindful of maintaining a welcoming environment and be helpful
with advice, especially for new contributors.  E.g., It is expected that a
maintainer offer to help a contributor who is a novice git user do any squashing
that that maintainer asks for, or do the squash themselves by directly pushing
to the PR branch.

When to rebase
==============

Pull requests **must** be rebased (but not necessarily squashed to a single
commit) if at least one of the following conditions is met:

* There are conflicts with main
* There are merge commits from upstream/main in the PR commit history (merge
  commits from PRs to the user's fork are fine)
* There are commit messages include offensive language or violate the code of
  conduct (in this case the rebase must also edit the commit messages)

Github 'Squash and Merge' button
================================

We should never use or enable the GitHub 'Squash and Merge' button since this
creates problems when dealing with identifying backports.
:orphan:

.. _vision:

********************************************
Vision for a Common Astronomy Python Package
********************************************

The following document summarizes a vision for a common Astronomy Python
package, and how we can best all work together to achieve this. In the
following document, this common package will be referred to as the core
package. This vision is not set in stone, and we are committed to adapting it
to whatever process and guidelines work in practice.

The ultimate goal that we seek is a package that would contain much of the core
functionality and some common tools required across Astronomy, but not
*everything* Astronomers will ever need. The aim is primarily to avoid
duplication for common core tasks, and to provide a robust framework upon which
to build more complex tools.

Such a common package should not preclude any other Astronomy package from
existing, because there will always be more complex and/or specialized tools
required. These tools will be able to rely on a single core library for many
tasks, and thus reduce the number of dependencies, reduce duplication of
functionality, and increase consistency of their interfaces.

Procedure
=========

With the help of the community, the coordination committee will start by
identifying a few of key areas where initial development/consolidation will be
needed (such as FITS, WCS, coordinates, tables, photometry, spectra, etc.) and
will encourage teams to be formed to build standalone packages implementing
this functionality. These packages will be referred to as affiliated packages
(meaning that they are intended for future integration in the core package).

A set of requirements will be set out concerning the interfaces and
classes/methods that affiliated packages will need to make available in order
to ensure consistency between the different components. As the core package
grows, new potential areas/components for the core package will be identified.
Competition cannot be avoided, and will not be actively discouraged, but
whenever possible, developers should strive to work as a team to provide a
single and robust affiliated package, for the benefit of the community.

The affiliated packages will be developed outside the core package in
independent repositories, which will allow the teams the choice of tool and
organization. Once an affiliated package has implemented the desired
functionality, and satisfies quality criteria for coding style, documentation,
and testing, it will be considered for inclusion in the core package, and
further development will be done directly in the core package either via direct
access to the repository, or via patches/pull requests (exactly how this will
be done will be decided later).

To ensure uniformity across affiliated packages, and to facilitate integration
with the core package, developers who wish to submit their affiliated packages
for inclusion in the core will need to follow the layout of a template
package that will be provided before development starts.

Dependencies
============

Affiliated packages should be able to be imported with only the following
dependencies:

* The Python Standard Library NumPy, SciPy, and Matplotlib Components already
  * in the core Astronomy package

Other packages may be used, but must be imported as needed rather than during
the initial import of the package.

If a dependency is needed, but is an affiliated package, the dependent package
will need to wait until the dependency is integrated into the core package
before being itself considered for inclusion. In the mean time, it can make use
of the other affiliated package in its current form, or other packages, so as
not to stall development. Thus, the first packages to be included in the core
will be those only requiring the standard library, NumPy, SciPy, and
Matplotlib.

If the required dependency will never be part of a main package, then by
default the dependency can be included but should be imported as needed
(meaning that it only prevents the importing of that component, not the entire
core package), unless a strong case is made and a general consensus is reached
by the community that this dependency is important enough to be required at a
higher level.

This system means that packages will be integrated into the core package in an
order depending on the dependency tree, and also ensures that the interfaces of
packages being integrated into the core package are consistent with those
already in the core package.

Initially, no dependency on GUI toolkits will be allowed in the core package.
If the community reaches an agreement on a single toolkit that could be used,
then this toolkit will be allowed (but will only be imported as needed).

Keeping track of affiliated packages
====================================

Affiliated packages will be listed in a central location (in addition to PyPI)
that will allow an easy installation of all the affiliated packages, for
example with a script that will seamlessly download and install all the
affiliated packages. The core package will also include mechanisms to
facilitate this installation process.

Existing Packages
=================

Developers who already have existing packages will be encouraged to continue
supporting them for the benefit of users until the core library is considered
stable, contains this functionality, and is released to the community.
Thereafter, developers should encourage users to transition to using the
functionality in the core package, and eventually phase out their own packages,
unless they provide added value over the core package.
****************************
Writing Command-Line Scripts
****************************

Command-line scripts in Astropy should follow a consistent scheme to promote
readability and compatibility.

Setuptools' `"entry points"`_ are used to automatically generate wrappers with
the correct extension. The scripts can live in their own module, or be part of
a larger module that implements a class or function for astropy library use.
They should have a ``main`` function to parse the arguments and pass those
arguments on to some library function so that the library function can be used
programmatically when needed. The ``main`` function should accept an optional
single argument that holds the ``sys.argv`` list, except for the script name
(e.g., ``argv[1:]``). It must then be added to the list of entry points in the
``setup.py`` file (see the example below).

Command-line options can be parsed however desired, but the :mod:`argparse`
module is recommended when possible, due to its simpler and more flexible
interface relative to the older :mod:`optparse`.

.. _"entry points": https://setuptools.readthedocs.io/en/latest/setuptools.html#automatic-script-creation

Example
=======

Contents of ``/astropy/somepackage/somemod.py`` ::

    def do_something(args, option=False):
        for a in args:
            if option:
                ...do something...
            else:
                ...do something else...

    def main(args=None):

        import argparse

        parser = argparse.ArgumentParser(description='Process some integers.')
        parser.add_argument('-o', '--option', dest='op',action='store_true',
                            help='Some option that turns something on.')
        parser.add_argument('stuff', metavar='S', nargs='+',
                            help='Some input I should be able to get lots of.')

        res = parser.parse_args(args)

        do_something(res.stuff,res.op)

Then add the script to the ``setup.cfg`` under this section::

    [options.entry_points]
    console_scripts =
        somescript = astropy.somepackage.somemod:main
:orphan:

.. include:: links.inc
.. _virtual_envs:

***************************
Python virtual environments
***************************

If you plan to do regular work on Astropy you should do your development in
a Python virtual environment. Conceptually a virtual environment is a
duplicate of the Python environment you normally work in, but sandboxed from
your default Python environment in the sense that packages installed in the
virtual environment do not affect your normal working environment in any way.
This allows you to install, for example, a development version of Astropy
and its dependencies without it conflicting with your day-to-day work with
Astropy and other Python packages.

.. note::

    "Default Python environment" here means whatever Python you are using
    when you log in; i.e. the default Python installation on your system,
    which is not in a Conda environment or virtualenv.

    More specifically, in UNIX-like platforms it creates a parallel root
    "prefix" with its own ``bin/``, ``lib/``, etc. directories.  When you
    :ref:`activate <activate_env>` the virtual environment it places this
    ``bin/`` at the head of your ``$PATH`` environment variable.

    This works similarly on Windows but the details depend on how you
    installed Python and whether or not you're using Anaconda.

There are a few options for using virtual environments; the choice of method
is dictated by the Python distribution you use:

* If you use the Anaconda/Conda Python distribution you must use the
  `conda`_ command to make and manage your virtual environments.

* If you do not use Anaconda you can use `virtualenv`_ and the conda-like
  helper commands provided by `virtualenvwrapper`_; you *can not* use this
  with `conda`_. As the name suggests, `virtualenvwrapper`_ is a wrapper
  around `virtualenv`_.

* A third, more recent option which is growing in popularity is `pipenv`_
  which builds on top of `virtualenv`_ to provide project-specific Python
  environments and dependency management.

In both cases you will go through the same basic steps; the commands to
accomplish each step are given for both `conda`_ and `virtualenvwrapper`_:

* :ref:`setup_for_env`
* :ref:`list_env`
* :ref:`create_env`
* :ref:`activate_env`
* :ref:`deactivate_env`
* :ref:`delete_env`

Another well-maintained guide to Python virtualenvs (specifically `pipenv`_
and `virtualenv`_, though it does not discuss `conda`_) which has been
translated into multiple languages is the `Hitchhiker's Guide to Python
<https://docs.python-guide.org/dev/virtualenvs/>`_ chapter on the subject.


.. _setup_for_env:


Set up for virtual environments
===============================

* `virtualenvwrapper`_:

  + First, install `virtualenvwrapper`_, which will also install `virtualenv`_,
    with::

        pip install --user virtualenvwrapper

  + From the `documentation for virtualenvwrapper`_, you also need to::

      export WORKON_HOME=$HOME/.virtualenvs
      export PROJECT_HOME=$HOME/
      source /usr/local/bin/virtualenvwrapper.sh

* `conda`_: No setup is necessary beyond installing the Anaconda Python
  distribution.

* `pipenv`_: Install the ``pipenv`` command using your default pip (the
  pip in the default Python environment)::

      pip install --user pipenv

.. _list_env:

List virtual environments
=========================

You do not need to list the virtual environments you have created before using
them...but sooner or later you will forget what environments you have defined
and this is the easy way to find out.

* `virtualenvwrapper`_: ``workon``
    + If this displays nothing you have no virtual environments
    + If this displays ``workon: command not found`` then you haven't done
      the :ref:`setup_for_env`; do that.
    + For more detailed information about installed environments use
      ``lsvirtualenv``.

* `conda`_: ``conda info -e``
    + you will always have at least one environment, called ``root``
    + your active environment is indicated by a ``*``

* `pipenv`_ does not have a concept of listing virtualenvs; it instead
  automatically generates the virtualenv associated with a project directory
  (e.g. the Astropy source repository on your computer).

.. _create_env:

Create a new virtual environment
================================

This needs to be done once for each virtual environment you want. There is one
important choice you need to make when you create a virtual environment:
which, if any, of the packages installed in your default Python environment do
you want in your virtual environment?

Including them in your virtual environment doesn't take much extra space--they
are linked into the virtual environment instead of being copied. Within the
virtual environment you can install new versions of packages like Numpy or
Astropy that override the versions installed in your default Python environment.

The easiest way to get started is to include in your virtual environment the
packages installed in your your default Python environment; the instructions
below do that.

In everything that follows, ``ENV`` represents the name you give your virtual
environment.

**The name you choose cannot have spaces in it.**

* `virtualenvwrapper`_:
    + Make an environment called ``ENV`` with all of the packages in your
      default Python environment::

         mkvirtualenv --system-site-packages ENV

    + Omit the option ``--system-site-packages`` to create an environment
      without the Python packages installed in your default Python environment.
    + Environments created with `virtualenvwrapper`_ always include `pip`_
      and `setuptools <https://setuptools.readthedocs.io>`_ so that you
      can install packages within the virtual environment.
    + More details and examples are in the
      `virtualenvwrapper command documentation`_.

* `conda`_:
    + Make an environment called ``ENV`` with all of the packages in your main
      Anaconda environment::

        conda create -n ENV anaconda

    + More details, and examples that start with none of the packages from
      your default Python environment, are in the
      `documentation for the conda command`_ and the
      `guide on how to manage environments`_.

    + Next activate the environment ``ENV`` with::

        conda activate ENV

    + Your command-line prompt will contain ``ENV`` in parentheses by default.

    + If Astropy is installed in your ``ENV`` environment, you may need to uninstall it
      in order for the development version to install properly. You can do this
      with the following command::

        conda uninstall astropy

* `pipenv`_:
    + Make sure you are in the Astropy source directory.  See
      :ref:`get_devel` if you are unsure how to get the source code.  After
      running ``git clone <your-astropy-fork>`` run ``cd astropy/`` then::

        pipenv install -e .

    + This both creates the virtual environment for the project
      automatically, and also installs all of Astropy's dependencies, and
      adds your Astropy repository as the version of Astropy to use in the
      environment.

    + You can activate the environment any time you're in the top-level
      ``astropy/`` directory (cloned from git) by running::

        pipenv shell

      This will open a new shell with the appropriate virtualenv enabled.

      You can also run individual commands from the virtualenv without
      activating it in the shell like::

        pipenv run python

.. _activate_env:

Activate a virtual environment
==============================

To use a new virtual environment you may need to activate it;
`virtualenvwrapper`_ will try to automatically activate your new environment
when you create it. Activation does two things (either of which you could do
manually, though it would be inconvenient):

* Puts the ``bin`` directory for the virtual environment at the front of your
  ``$PATH``.

* Adds the name of the virtual environment to your command prompt. If you
  have successfully switched to a new environment called ``ENV`` your prompt
  should look something like this: ``(ENV)[~] $``

The commands below allow you to switch between virtual environments in
addition to activating new ones.

* `virtualenvwrapper`_: Activate the environment ``ENV`` with::

      workon ENV

* `conda`_: Activate the environment ``ENV`` with::

      conda activate ENV

* `pipenv`_: Activate the environment by changing into the project
  directory (i.e. the copy of the Astropy repository on your computer) and
  running::

      pipenv shell


.. _deactivate_env:

Deactivate a virtual environment
================================

At some point you may want to go back to your default Python environment. Do
that with:

* `virtualenvwrapper`_: ``deactivate``
    + Note that in ``virtualenvwrapper 4.1.1`` the output of
      ``mkvirtualenv`` says you should use ``source deactivate``; that does
      not seem to actually work.

* `conda`_: ``conda deactivate``

* `pipenv`_: ``exit``

  .. note::

    Unlike ``virtualenv`` and ``conda``, ``pipenv`` does not manipulate
    environment variables in your current shell session.  Instead it
    launches a *subshell* which is a copy of your previous shell, in which
    it can then change some environment variables.  Therefore, any
    environment variables you change in the ``pipenv`` shell will be
    restored to their previous value (or lost entirely) when ``exit``-ing
    the subshell.

.. _delete_env:

Delete a virtual environment
============================

In both `virtualenvwrapper`_ and `conda`_ you can simply delete the
directory in which the ``ENV`` is located; both also provide commands to
make that a bit easier.  `pipenv`_ includes a command for deleting the
virtual environment associated with the current directory:

* `virtualenvwrapper`_: ``rmvirtualenv ENV``

* `conda`_: ``conda remove --all -n ENV``

* `pipenv`_: ``pipenv --rm``: As with other ``pipenv`` commands this is
  run from within the project directory.

.. _documentation for virtualenvwrapper: https://virtualenvwrapper.readthedocs.io/en/latest/install.html
.. _virtualenvwrapper command documentation: https://virtualenvwrapper.readthedocs.io/en/latest/command_ref.html
.. _documentation for the conda command: https://docs.conda.io/projects/conda/en/latest/commands.html
.. _guide on how to manage environments: https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html
.. _development-workflow:

*******************************
How to make a code contribution
*******************************

This document outlines the process for contributing code to the Astropy
project.

**Already experienced with git? Contributed before?** Jump right to
:ref:`astropy-git`.

Pre-requisites
**************

Before following the steps in this document you need:

+ an account on `GitHub`_
+ a local copy of the astropy source. Instructions for doing that, including the
  basics you need for setting up git and GitHub, are at :ref:`get_devel`.

Strongly Recommended, but not required
**************************************

You cannot easily work on the development version of astropy in a python
environment in which you also use the stable version. It can be done |emdash|
but can only be done *successfully* if you always remember whether the
development version or stable version is the active one.

:ref:`virtual_envs` offer a better solution and take only a few minutes to set
up. It is well worth your time.

Not sure what your first contribution should be? Take a look at the `Astropy
issue list`_ and grab one labeled `"package-novice" <https://github.com/astropy/astropy/issues?q=is%3Aissue+is%3Aopen+label%3Apackage-novice>`_.
These issues are the most accessible ones if you are not familiar with the
Astropy source code. Issues labeled as `"effort-low" <https://github.com/astropy/astropy/issues?q=is%3Aissue+is%3Aopen+label%3Aeffort-low>`_
are expected to take a few hours (at most) to address, while the
`"effort-medium" <https://github.com/astropy/astropy/issues?q=is%3Aissue+is%3Aopen+label%3Aeffort-medium>`_
ones may take a few days. The developers are friendly and want you to help, so
don't be shy about asking questions on the `astropy-dev mailing list`_.

New to `git`_?
**************

Some `git`_ resources
=====================

If you have never used git or have limited experience with it, take a few
minutes to look at these resources:

* `Interactive tutorial`_ that runs in a browser
* `Git Basics`_, part of a much longer `git book`_.

In practice, you need only a handful of `git`_ commands to make contributions
to Astropy. There is a more extensive list of :ref:`git-resources` if you
want more background.

Double check your setup
=======================

Before going further, make sure you have set up astropy as described in
:ref:`get_devel`.

In a terminal window, change directory to the one containing your clone of
Astropy. Then, run ``git remote``; the output should look something like this::

    your-github-username
    astropy

If that works, also run ``git fetch --all``. If it runs without errors then
your installation is working and you have a complete list of all branches in
your clone, ``your-github-username`` and ``astropy``.

About names in `git`_
=====================

`git`_ is designed to be a *distributed* version control system. Each clone of
a repository is, itself, a repository. That can lead to some confusion,
especially for the branch called ``main``. If you list all of the branches
your clone of git knows about with ``git branch -a`` you will see there are
*three* different branches called ``main``::

    * main                              # this is main in your local repo
    remotes/your-github-username/main   # main on your fork of Astropy on GitHub
    remotes/astropy/main                # the official development branch of Astropy

The naming scheme used by `git`_ will also be used here. A plain branch name,
like ``main`` means a branch in your local copy of Astropy. A branch on a
remote, like ``astropy`` , is labeled by that remote, ``astropy/main``.

This duplication of names can get very confusing when working with pull
requests, especially when the official main branch, ``astropy/main``,
changes due to other contributions before your contributions are merged in.
As a result, you should never do any work in your main
branch, ``main``. Always work on a branch instead.

Essential `git`_ commands
=========================

A full `git`_ tutorial is beyond the scope of this document but this list
describes the few ``git`` commands you are likely to encounter in contributing
to Astropy:

* ``git fetch`` gets the latest development version of Astropy, which you will
  use as the basis for making your changes.
* ``git branch`` makes a logically separate copy of Astropy to keep track of
  your changes.
* ``git add`` stages files you have changed or created for addition to `git`_.
* ``git commit`` adds your staged changes to the repository.
* ``git push`` copies the changes you committed to GitHub
* ``git status`` to see a list of files that have been modified or created.

.. note::
    A good graphical interface to git makes some of these steps much
    easier. Some options are described in :ref:`git_gui_options`.

If something goes wrong
=======================

`git`_ provides a number of ways to recover from errors. If you end up making a
`git`_ mistake, do not hesitate to ask for help. An additional resource that
walks you through recovering from `git`_ mistakes is the
`git choose-your-own-adventure`_.

.. _astropy-git:

Astropy Guidelines for `git`_
*****************************

* Don't use your ``main`` branch for anything. Consider :ref:`delete-main`.
* Make a new branch, called a *feature branch*, for each separable set of
  changes: "one task, one branch" (`ipython git workflow`_).
* Start that new *feature branch* from the most current development version
  of astropy (instructions are below).
* Name your branch for the purpose of the changes, for example
  ``bugfix-for-issue-14`` or ``refactor-database-code``.
* Make frequent commits, and always include a commit message. Each commit
  should represent one logical set of changes.
* Ask on the `astropy-dev mailing list`_ if you get stuck.
* Never merge changes from ``astropy/main`` into your feature branch. If
  changes in the development version require changes to our code you can
  :ref:`rebase`.

In addition there are a couple of `git`_ naming conventions used in this
document:

* Change the name of the remote ``origin`` to ``your-github-username``.
* Name the remote that is the primary Astropy repository
  ``astropy``; in prior versions of this documentation it was referred to as
  ``upstream``.

Workflow
********

These, conceptually, are the steps you will follow in contributing to Astropy:

#. :ref:`fetch-latest`
#. :ref:`make-feature-branch`; you will make your changes on this branch.
#. :ref:`install-branch`
#. Follow :ref:`edit-flow` to write/edit/document/test code - make
   frequent, small commits.
#. :ref:`add-changelog`
#. :ref:`push-to-github`
#. From GitHub, :ref:`pull-request` to let the Astropy maintainers know
   you have contributions to review.
#. :ref:`revise and push` in response to comments on the pull
   request. Pushing those changes to GitHub automatically updates the
   pull request.

This way of working helps to keep work well organized, with readable history.
This in turn makes it easier for project maintainers (that might be you) to
see what you've done, and why you did it.

A worked example that follows these steps for fixing an Astropy issue is at
:ref:`astropy-fix-example`.

Some additional topics related to `git`_ are in :ref:`additional-git`.

.. _delete-main:

Deleting your main branch
=========================

It may sound strange, but deleting your own ``main`` branch can help reduce
confusion about which branch you are on.  See `deleting main on github`_ for
details.

.. _fetch-latest:

Fetch the latest Astropy
************************

From time to time you should fetch the development version (i.e. Astropy
``astropy/main``) changes from GitHub::

   git fetch astropy --tags

This will pull down any commits you don't have, and set the remote branches to
point to the latest commit. For example, 'trunk' is the branch referred to by
``astropy/main``, and if there have been commits since
you last checked, ``astropy/main`` will change after you do the fetch.

.. _make-feature-branch:

Make a new feature branch
*************************

Make the new branch
===================

When you are ready to make some changes to the code, you should start a new
branch. Branches that are for a collection of related edits are often called
'feature branches'.

Making a new branch for each set of related changes will make it easier for
someone reviewing your branch to see what you are doing.

Choose an informative name for the branch to remind yourself and the rest of us
what the changes in the branch are for. Branch names like ``add-ability-to-fly``
or ``buxfix-for-issue-42`` clearly describe the purpose of the branch.

Always make your branch from ``astropy/main`` so that you are basing your
changes on the latest version of Astropy::

    # Update the mirror of trunk
    git fetch astropy --tags

    # Make new feature branch starting at astropy/main
    git branch my-new-feature astropy/main
    git checkout my-new-feature

Connect the branch to GitHub
============================

At this point you have made and checked out a new branch, but `git`_ does not
know it should be connected to your fork on GitHub. You need that connection
for your proposed changes to be managed by the Astropy maintainers on GitHub.

The most convenient way for connecting your local branch to GitHub is to `git
push`_ this new branch up to your GitHub repo with the ``--set-upstream``
option::

   git push --set-upstream your-github-username my-new-feature

From now on git will know that ``my-new-feature`` is related to the
``your-github-username/my-new-feature`` branch in your GitHub fork of Astropy.

You will still need to ``git push`` your changes to GitHub periodically. The
setup in this section will make that easier because any following pushes of
this branch can be performed without having to write out the remote and branch
names.

.. _install-branch:

Install your branch
*******************

Ideally you should set up a Python virtual environment just for this fix;
instructions for doing to are at :ref:`virtual_envs`. Doing so ensures you
will not corrupt your main ``astropy`` install and makes it very easy to recover
from mistakes.

Once you have activated that environment, you need to install the version of
``astropy`` you are working on. Do that with:

.. code-block:: bash

    pip install -e .

For more details on building ``astropy`` from source, see
:ref:`dev-build-astropy-subpkg`.

.. _edit-flow:

The editing workflow
********************

Conceptually, you will:

#. Make changes to one or more files and/or add a new file.
#. Check that your changes do not break existing code.
#. Add documentation to your code and, as appropriate, to the Astropy
   documentation.
#. Ideally, also make sure your changes do not break the documentation.
#. Add tests of the code you contribute.
#. Commit your changes in `git`_
#. Repeat as necessary.


In more detail
==============

#. Make some changes to one or more files. You should follow the Astropy
   :ref:`code-guide`. Each logical set of changes should be treated as one
   commit. For example, if you are fixing a known bug in Astropy and notice
   a different bug while implementing your fix, implement the fix to that new
   bug as a different set of changes.

#. Test that your changes do not lead to *regressions*, i.e. that your
   changes do not break existing code, by running the Astropy tests. You can
   run all of the Astropy tests from ipython with::

     import astropy
     astropy.test()

   If your change involves only a small part of Astropy, e.g. Time, you can
   run just those tests::

     import astropy
     astropy.test(package='time')

   Tests can also be run from the command line while in the package
   root directory, e.g.::

     pytest

   To run the tests in only a single package, e.g. Time, you can do::

     pytest -P time

   For more details on running tests, please see :ref:`testing-guidelines`.

#. Make sure your code includes appropriate docstrings, in the
   `Numpydoc format`_.
   If appropriate, as when you are adding a new feature,
   you should update the appropriate documentation in the ``docs`` directory;
   a detailed description is in :ref:`documentation-guidelines`.

#. If you have sphinx installed, you can also check that
   the documentation builds and looks correct by running, from the
   ``astropy`` directory::

     cd docs
     make html

   The last line should just state ``build succeeded``, and should not mention
   any warnings.  (For more details, see :ref:`documentation-guidelines`.)

#. Add tests of your new code, if appropriate. Some changes (e.g. to
   documentation) do not need tests. Detailed instructions are at
   :ref:`writing-tests`, but if you have no experience writing tests or
   with the `pytest`_ testing framework submit your changes without adding
   tests, but mention in the pull request that you have not written tests.
   An example of writing a test is in
   :ref:`astropy-fix-add-tests`.

#. Stage your changes using ``git add`` and commit them using ``git commit``.
   An example of doing that, based on the fix for an actual Astropy issue, is
   at :ref:`astropy-fix-example`.

   .. note::
        Make your `git`_ commit messages short and descriptive. If a commit
        fixes an issue, include, on the second or later line of the commit
        message, the issue number in the commit message, like this:
        ``Closes #123``. Doing so will automatically close the issue when the
        pull request is accepted.

#. Some modifications require more than one commit; if in doubt, break
   your changes into a few, smaller, commits rather than one large commit
   that does many things at once. Repeat the steps above as necessary!

.. _add-changelog:

Add a changelog entry
*********************

Add a changelog fragment briefly describing the change you made by creating
a new file in ``docs/changes/<sub-package>/``. The file should be named like
``<PULL REQUEST>.<TYPE>.rst``, where ``<PULL REQUEST>`` is a pull request
number, and ``<TYPE>`` is one of:

* ``feature``: New feature.
* ``api``: API change.
* ``bugfix``: Bug fix.
* ``other``: Other changes and additions.

An example entry, for the changes in `PR 1845
<https://github.com/astropy/astropy/pull/1845>`_, the file would be
``docs/changes/wcs/1845.bugfix.rst`` and would contain::

    ``astropy.wcs.Wcs.printwcs`` will no longer warn that ``cdelt`` is
    being ignored when none was present in the FITS file.

If you are opening a new pull request, you may not know its number yet, but you
can add it *after* you make the pull request. If you're not sure where to
put the changelog entry, wait at least until a maintainer has reviewed your
PR and assigned it to a milestone.

When writing changelog entries, do not attempt to make API reference links
by using single-backticks.  This is because the changelog (in its current
format) runs for the history of the project, and API references you make today
may not be valid in a future version of Astropy.  However, use of
double-backticks for monospace rendering of module/class/function/argument
names and the like is encouraged.

.. _push-to-github:

Copy your changes to GitHub
***************************

If you followed the instructions to `Connect the branch to GitHub`_ then you
can simply use::

    git push

If you skipped that step then you need to write out the remote and branch
names::

    git push your-github-username my-new-feature

.. _pull-request:

Ask for your changes to be reviewed
***********************************

A *pull request* on GitHub is a request to merge the changes you have made into
another repository.

When you are ready to ask for someone to review your code and consider merging
it into Astropy:

#. Go to the URL of your fork of Astropy, e.g.,
   ``https://github.com/your-user-name/astropy``.

#. Use the 'Switch Branches' dropdown menu to select the branch with your
   changes:

   .. image:: branch_dropdown.png

#. Click on the 'Pull request' button:

   .. image:: pull_button.png

   Enter a title for the set of changes, and some explanation of what you've
   done. If there is anything you'd like particular attention for, like a
   complicated change or some code you are not happy with, add the details
   here.

   If you don't think your request is ready to be merged, just say so in your
   pull request message.  This is still a good way to start a preliminary
   code review.

   You may also opt to open a work-in-progress pull request.
   If you do so, instead of clicking "Create pull request", click on the small
   down arrow next to it and select "Create draft pull request". This will let
   the maintainers know that your work is not ready for a full review nor to be
   merged yet. In addition, if your commits are not ready for CI testing, you
   should also use ``[ci skip]`` or ``[skip ci]`` directive in your commit message.

.. _revise and push:

Revise and push as necessary
****************************

You may be asked to make changes in the discussion of the pull request. Make
those changes in your local copy, commit them to your local repo and push them
to GitHub. GitHub will automatically update your pull request.

.. _no git pull:

Do Not Create a Merge Commit
****************************

If your branch associated with the pull request falls behind the ``main``
branch of https://github.com/astropy/astropy, GitHub might offer you the option
to catch up or resolve conflicts via its web interface, but do not use this. Using
the web interface might create a "merge commit" in your commit history, which is
undesirable, as a "merge commit" can introduce maintenance overhead for the
release manager as well as undesirable branch structure complexity. Do not use the ``git pull`` command either.

Instead, in your local checkout, do a ``fetch`` and then a ``rebase``, and
resolve conflicts as necessary. See :ref:`rebase` and :ref:`howto_rebase`
for further information.

.. _rebase:

Rebase, but only if asked
*************************

Sometimes the maintainers of Astropy may ask a pull request to be *rebased*
or *squashed* in the process of reviewing a pull request for merging into
the main Astropy *main* repository.

The decisions of when to request a *squash* or *rebase* are left to
individual maintainers.  These may be requested to reduce the number of
visible commits saved in the repository history, or because of code changes
in Astropy in the meantime.  A rebase may be necessary to allow the Continuous
Integration tests to run.  Both involve rewriting the `git`_ history, meaning
that commit hashes will change, which is why you should do it only if asked.

Conceptually, rebasing means taking your changes and applying them to the latest
version of the development branch of the official Astropy as though that was the
version you had originally branched from. Each individual commit remains
visible, but with new metadata/commit hashes. Squashing commits changes the
metadata/commit hash, and also removes separate visibility of individual
commits; a new commit and commit message will only contain a textual
list of the earlier commits.

It is easier to make mistakes rebasing than other areas of `git`_, so before you
start make a branch to serve as a backup copy of your work::

    git branch tmp my-new-feature # make temporary branch--will be deleted later

After altering the history, e.g. with ``git rebase``, a normal ``git push``
is prevented, and a ``git push --force`` will be required.

.. warning::

    Do not update your branch with ``git pull``. Pulling changes from
    ``astropy/main`` includes merging the branches, which combines them in a
    way that preserves the commit history of both. The purpose of rebasing is
    rewriting the commit history of your branch, not preserving it.

.. _howto_rebase:

How to rebase
*************

Behind the scenes, `git`_ is deleting the changes and branch you made, making the
changes others made to the development branch of Astropy, then re-making your
branch from the development branch and applying your changes to your branch.

The actual rebasing is usually easy::

    git fetch astropy main # get the latest development astropy
    git rebase astropy/main my-new-feature

You are more likely to run into *conflicts* here  places where the changes you
made conflict with changes that someone else made  than anywhere else. Ask for
help if you need it. Instructions are available on how to
`resolve merge conflicts after a Git rebase <https://help.github.com/en/articles/resolving-merge-conflicts-after-a-git-rebase>`_.

.. _howto_squash:

How to squash
*************

Typically we ask to *squash* when there was a fair amount of trial
and error, but the final patch remains quite small, or when files were added
and removed (especially binary files or files that should not remain in the
repository) or if the number of commits in the history is disproportionate
compared to the work being carried out (for example 30 commits gradually
refining a final 10-line change).  Conceptually this is equivalent to
exporting the final diff from a feature branch, then starting a new branch and
applying only that patch.

Many of us find that is it actually easiest to squash using rebase. In particular,
you can rebase and squash within the existing branch using::

  git fetch astropy
  git rebase -i astropy/main

The last command will open an editor with all your commits, allowing you to
squash several commits together, rename them, etc. Helpfully, the file you are
editing has the instructions on what to do.

.. _howto_push_force:

How to push
***********

After using ``git rebase`` you will still need to push your changes to
GitHub so that they are visible to others and the pull request can be
updated.  Use of a simple ``git push`` will be prevented because of the
changed history, and will need to be manually overridden using::

    git push --force

If you run into any problems, do not hesitate to ask. A more detailed conceptual
discussing of rebasing is at :ref:`rebase-on-trunk`.

Once the modifications and new git history are successfully pushed to GitHub you
can delete any backup branches that may have been created::

    git branch -D tmp

.. include:: links.inc

.. _Interactive tutorial: http://try.github.io/
.. _Git Basics: https://git-scm.com/book/en/Getting-Started-Git-Basics
.. _git book: https://git-scm.com/book/
.. _Astropy issue list: https://github.com/astropy/astropy/issues
.. _git choose-your-own-adventure: http://sethrobertson.github.io/GitFixUm/fixup.html
.. _numpydoc format: https://numpydoc.readthedocs.io/en/latest/format.html
.. _get_devel:

***************************
Try the development version
***************************

.. note::
    `git`_ is the name of a source code management system. It is used to keep
    track of changes made to code and to manage contributions coming from
    several different people. If you want to read more about `git`_ right now
    take a look at `Git Basics`_.

    If you have never used `git`_ before, allow one hour the first time you do
    this. If you find this taking more than one hour, post in one of the
    `astropy forums <http://www.astropy.org/help.html>`_ to get help.


Trying out the development version of astropy is useful in three ways:

* More users testing new features helps uncover bugs before the feature is
  released.
* A bug in the most recent stable release might have been fixed in the
  development version. Knowing whether that is the case can make your bug
  reports more useful.
* You will need to go through all of these steps before contributing any
  code to `Astropy`_. Practicing now will save you time later if you plan to
  contribute.

Overview
========

Conceptually, there are several steps to getting a working copy of the latest
version of astropy on your computer:

#. :ref:`fork_a_copy`; this copy is called a *fork* (if you don't have an
   account on `github`_ yet, go there now and make one).
#. :ref:`check_git_install`
#. :ref:`clone_your_fork`; this is called making a *clone* of the repository.
#. :ref:`set_upstream_main`
#. :ref:`make_a_branch`; this is called making a *branch*.
#. :ref:`activate_development_astropy`
#. :ref:`test_installation`
#. :ref:`try_devel`
#. :ref:`deactivate_development`

Step-by-step instructions
=========================

.. _fork_a_copy:

Make your own copy of Astropy on GitHub
---------------------------------------

In the language of `GitHub`_, making a copy of someone's code is called making
a *fork*. A fork is a complete copy of the code and all of its revision
history.

#. Log into your `GitHub`_ account.

#. Go to the `Astropy GitHub`_ home page.

#. Click on the *fork* button:

   .. image:: ../workflow/forking_button.png

   After a short pause and an animation of Octocat scanning a book on a
   flatbed scanner, you should find yourself at the home page for your own
   forked copy of astropy.

.. _check_git_install:

Make sure git is installed and configured on your computer
----------------------------------------------------------

**Check that git is installed:**

Check by typing, in a terminal::

    $ git --version
    # if git is installed, will get something like: git version 2.20.1

If `git`_ is not installed, `get it <https://git-scm.com/downloads>`_.

**Basic git configuration:**

Follow the instructions at `Set Up Git at GitHub`_ to take care of two
essential items:

+ Set your user name and email in your copy of `git`_

+ Set up authentication so you don't have to type your github password every
  time you need to access github from the command line. The default method at
  `Set Up Git at GitHub`_ may require administrative privileges; if that is a
  problem, set up authentication
  `using SSH keys instead <https://help.github.com/en/articles/connecting-to-github-with-ssh>`_

We also recommend setting up `git`_ so that when you copy changes from your
computer to `GitHub`_ only the copy (called a *branch*) of astropy that you are
working on gets pushed up to GitHub.  *If* your version of git is 1.7.11 or,
greater, you can do that with::

    git config --global push.default simple

If you skip this step now it is not a problem; `git`_ will remind you to do it in
those cases when it is relevant.  If your version of git is less than 1.7.11,
you can still continue without this, but it may lead to confusion later, as you
might push up branches you do not intend to push.

.. note::

    Make sure you make a note of which authentication method you set up
    because it affects the command you use to copy your GitHub fork to your
    computer.

    If you set up password caching (the default method) the URLs will look like
    ``https://github.com/your-user-name/astropy.git``.

    If you set up SSH keys the URLs you use for making copies will look
    something like ``git@github.com:your-user-name/astropy.git``.


.. _clone_your_fork:

Copy your fork of Astropy from GitHub to your computer
------------------------------------------------------

One of the commands below will make a complete copy of your `GitHub`_ fork
of `Astropy`_ in a directory called ``astropy``; which form you use depends
on what kind of authentication you set up in the previous step::

    # Use this form if you setup SSH keys...
    $ git clone --recursive git@github.com:your-user-name/astropy.git
    # ...otherwise use this form:
    $ git clone --recursive https://github.com/your-user-name/astropy.git

If there is an error at this stage it is probably an error in setting up
authentication.

.. _set_upstream_main:

Tell git where to look for changes in the development version of Astropy
------------------------------------------------------------------------

Right now your local copy of astropy doesn't know where the development
version of astropy is. There is no easy way to keep your local copy up to
date. In `git`_ the name for another location of the same repository is a
*remote*. The repository that contains the latest "official" development
version is traditionally called the *upstream* remote, but here we use a
more meaningful name for the remote: *astropy*.

Change into the ``astropy`` directory you created in the previous step and
let `git`_ know about about the astropy remote::

    cd astropy
    git remote add astropy git://github.com/astropy/astropy.git

You can check that everything is set up properly so far by asking `git`_ to
show you all of the remotes it knows about for your local repository of
`Astropy`_ with ``git remote -v``, which should display something like::

    astropy   git://github.com/astropy/astropy.git (fetch)
    astropy   git://github.com/astropy/astropy.git (push)
    origin     git@github.com:your-user-name/astropy.git (fetch)
    origin     git@github.com:your-user-name/astropy.git (push)

Note that `git`_ already knew about one remote, called *origin*; that is your
fork of `Astropy`_ on `GitHub`_.

To make more explicit that origin is really *your* fork of `Astropy`_, rename that
remote to your `GitHub`_ user name::

  git remote rename origin your-user-name

.. _make_a_branch:

Create your own private workspace
---------------------------------

One of the nice things about `git`_ is that it is easy to make what is
essentially your own private workspace to try out coding ideas. `git`_
calls these workspaces *branches*.

Your repository already has several branches; see them if you want by running
``git branch -a``. Most of them are on ``remotes/origin``; in other words,
they exist on your remote copy of astropy on GitHub.

There is one special branch, called *main*. Right now it is the one you are
working on; you can tell because it has a marker next to it in your list of
branches: ``* main``.

To make a long story short, you never want to work on main. Always work on a branch.

To avoid potential confusion down the road, make your own branch now; this
one you can call anything you like (when making contributions you should use
a meaningful more name)::

    git branch my-own-astropy

You are *not quite* done yet. Git knows about this new branch; run
``git branch`` and you get::

    * main
      my-own-astropy

The ``*`` indicates you are still working on main. To work on your branch
instead you need to *check out* the branch ``my-own-astropy``. Do that with::

    git checkout my-own-astropy

and you should be rewarded with::

    Switched to branch 'my-own-astropy'

.. _activate_development_astropy:

"Activate" the development version of astropy
---------------------------------------------

Right now you have the development version of astropy, but python will not
see it. Though there are more sophisticated ways of managing multiple versions
of astropy, for now this straightforward way will work (if you want to jump
ahead to the more sophisticated method look at :ref:`virtual_envs`).

.. note::
    If you want to work on C or Cython code in `Astropy`_, this quick method
    of activating your copy of astropy will *not* work _ -- you need to go
    straight to using a virtual python environment.

If you have decided to use the recommended "activation" method with
``pip``, please note the following: Before trying to install,
check that you have the required dependencies: "cython" and "jinja2".
If not, install them with ``pip``. Note that on some platforms,
the pip command is ``pip3`` instead of ``pip``, so be sure to use
this instead in the examples below if that is the case.
If you have any problem with different versions of ``pip`` installed,
try aliasing to resolve the issue. If you are unsure about which ``pip``
version you are using, try the command ``which pip`` on the terminal.

In the directory where your copy of astropy is type::

    pip install -e .

Several pages of output will follow the first time you do this; this wouldn't
be a bad time to get a fresh cup of coffee. At the end of it you should see
something like  ``Finished processing dependencies for astropy==3.2.dev6272``.

To make sure it has been activated **change to a different directory outside of
the astropy distribution** and try this in python::

    >>> import astropy
    >>> astropy.__version__  # doctest: +SKIP
    '3.2.dev6272'

The actual version number will be different than in this example, but it
should have ``'dev'`` in the name.

.. warning::
    Right now every time you run Python, the development version of astropy
    will be used. That is fine for testing but you should make sure you change
    back to the stable version unless you are developing astropy. If you want
    to develop astropy, there is a better way of separating the development
    version from the version you do science with. That method, using a
    `virtualenv`_, is discussed at :ref:`virtual_envs`.

    For now **remember to change back to your usual version** when you are
    done with this.

.. _test_installation:

Test your development copy
--------------------------

Testing is an important part of making sure astropy produces reliable,
reproducible results. Before you try out a new feature or think you have found
a bug make sure the tests run properly on your system.

Before running your tests, please see :ref:`testing-dependencies`.

If the test *don't* complete successfully, that is itself a bug--please
`report it <https://github.com/astropy/astropy/issues>`_.

To run the tests, navigate back to the directory your copy of astropy is in on
your computer, then, at the shell prompt, type::

    pytest

This is another good time to get some coffee or tea. The number of test is
large. When the test are done running you will see a message something like
this::

    4741 passed, 85 skipped, 11 xfailed

Skips and xfails are fine, but if there are errors or failures please
`report them <https://github.com/astropy/astropy/issues>`_.

.. _try_devel:

Try out the development version
-------------------------------

If you are going through this to ramp up to making more contributions to
`Astropy`_ you don't actually have to do anything here.

If you are doing this because you have found a bug and are checking that it
still exists in the development version, try running your code.

Or, just for fun, try out one of the :ref:`new features <changelog>` in
the development version.

Either way, once you are done, make sure you do the next step.

.. _deactivate_development:

"Deactivate" the development version
------------------------------------

Be sure to turn the development version off before you go back to doing
science work with astropy.

Navigate to the directory where your local copy of the development version is,
then run::

    pip uninstall astropy

This should remove the development version only. Once again,
it is important to check that you are using the proper version of
``pip`` corresponding to the Python executable desired.

You should really confirm it is deactivated by **changing to a different
directory outside of the astropy distribution** and running this in python::

    >>> import astropy
    >>> astropy.__version__  # doctest: +SKIP
    '3.1.1'

The actual version number you see will likely be different than this example,
but it should not have ``'dev'`` in it.


.. include:: links.inc
.. _Git Basics: https://git-scm.com/book/en/Getting-Started-Git-Basics
.. _Set Up Git at GitHub: https://help.github.com/en/articles/set-up-git#set-up-git
:orphan:

.. include:: links.inc
.. _install-git:

**************************
 Install and configure git
**************************


Get git
=======

Installers and instructions for all platforms are available at
https://git-scm.com/downloads

.. _essential_config:

Essential configuration
=======================

Though technically not required to install `git`_ and get it running, configure `git`_ so that you get credit for your contributions::

    git config --global user.name "Your Name"
    git config --global user.email you@yourdomain.example.com

.. note::
    Use the same email address here that you used for setting up your GitHub
    account to save yourself a couple of steps later, when you connect your
    git to GitHub.

Check it with::

    $ git config --list
    user.name=Your Name
    user.email=you@yourdomain.example.com
    # ...likely followed by many other configuration values

.. _git_gui_options:

Get a git GUI (optional)
========================

There are several good, free graphical interfaces for git.
Even if you are proficient with `git`_ at the command line a GUI can be useful.

Mac and Windows:

+ `SourceTree`_
+ The github client for `Mac`_ or `Windows`_

Linux, Mac and Windows:

+ `git-cola`_

There is a more extensive list of `git GUIs`_, including non-free options, for
all platforms.

.. _git GUIs: https://git-scm.com/downloads/guis
.. _SourceTree: https://www.sourcetreeapp.com/
.. _Mac: https://desktop.github.com/
.. _Windows: https://desktop.github.com/
.. _git-cola: http://git-cola.github.io/
.. _maintainer-workflow:

************************
Workflow for Maintainers
************************

This page is for maintainers |emdash| those of us who merge our own or other
peoples' changes into the upstream repository.

Being as how you're a maintainer, you are completely on top of the basic stuff
in :ref:`development-workflow`.

=======================================================
Integrating changes via the web interface (recommended)
=======================================================

Whenever possible, merge pull requests automatically via the pull request manager on GitHub. Merging should only be done manually if there is a really good reason to do this!

Make sure that pull requests do not contain a messy history with merges, etc. If this is the case, then follow the manual instructions, and make sure the fork is rebased to tidy the history before committing.

============================
Integrating changes manually
============================

First, check out the ``astropy`` repository. The instructions in :ref:`set_upstream_main` add a remote that has read-only
access to the upstream repo.  Being a maintainer, you've got read-write access.

It's good to have your upstream remote have a scary name, to remind you that
it's a read-write remote::

    git remote add upstream-rw git@github.com:astropy/astropy.git
    git fetch upstream-rw --tags

Let's say you have some changes that need to go into trunk
(``upstream-rw/main``).

The changes are in some branch that you are currently on. For example, you are
looking at someone's changes like this::

    git remote add someone git://github.com/someone/astropy.git
    git fetch someone
    git branch cool-feature --track someone/cool-feature
    git checkout cool-feature

So now you are on the branch with the changes to be incorporated upstream. The
rest of this section assumes you are on this branch.

A few commits
-------------

If there are only a few commits, consider rebasing to upstream::

    # Fetch upstream changes
    git fetch upstream-rw

    # Rebase
    git rebase upstream-rw/main

Remember that, if you do a rebase, and push that, you'll have to close any
github pull requests manually, because github will not be able to detect the
changes have already been merged.

A long series of commits
------------------------

If there are a longer series of related commits, consider a merge instead::

    git fetch upstream-rw
    git merge --no-ff upstream-rw/main

The merge will be detected by github, and should close any related pull
requests automatically.

Note the ``--no-ff`` above. This forces git to make a merge commit, rather
than doing a fast-forward, so that these set of commits branch off trunk then
rejoin the main history with a merge, rather than appearing to have been made
directly on top of trunk.

Check the history
-----------------

Now, in either case, you should check that the history is sensible and you
have the right commits::

    git log --oneline --graph
    git log -p upstream-rw/main..

The first line above just shows the history in a compact way, with a text
representation of the history graph. The second line shows the log of commits
excluding those that can be reached from trunk (``upstream-rw/main``), and
including those that can be reached from current HEAD (implied with the ``..``
at the end). So, it shows the commits unique to this branch compared to trunk.
The ``-p`` option shows the diff for these commits in patch form.

Push to trunk
-------------

::

    git push upstream-rw my-new-feature:main

This pushes the ``my-new-feature`` branch in this repository to the ``main``
branch in the ``upstream-rw`` repository.


.. _milestones-and-labels:

===========================
Using Milestones and Labels
===========================

General guidelines for milestones:

* 100% of pull requests should have a milestone

* Issues are not milestoned unless they block a given release

* Only the following criteria should result in a pull request being closed without a milestone:

  * Invalid (user error, etc.)

  * Duplicate of an existing pull request

  * A pull request superseded by a new pull request providing an alternate implementation

* In general there should be the following open milestones:

  * The next bug fix releases for any still-supported version lines; for example if 0.4 is in development and
    0.2.x and 0.3.x are still supported there should be milestones for the next 0.2.x and 0.3.x releases.

  * The next X.Y release, i.e. the next minor release; this is generally the next release that all development in
    main is aimed toward.

  * The next X.Y release +1; for example if 0.3 is the next release, there should also be a milestone for 0.4 for
    issues that are important, but that we know won't be resolved in the next release.

* We have `Rolling reminder: update wcslib and cfitsio and leap second/IERS B table to the latest version <https://github.com/astropy/astropy/issues/9018>`_.
  The milestone for this issue should be updated as part of the release
  procedures.

General guidelines for labels:

* Issues: Maintainer should be proactive in labeling issues as they come in.
  At the very least, label the subpackage(s) involved and whether the issue
  is a bug.

* Pull requests: We have GitHub Actions to automatically apply labels using
  some simple rules when a pull request is opened. Once that is done, a
  maintainer can then manually apply any other labels that apply.


.. _changelog-format:

======================================
Updating and Maintaining the Changelog
======================================

The Astropy "changelog" is managed with
`towncrier <https://pypi.org/project/towncrier/>`_, which is used to generate
the ``CHANGES.rst`` file at the root of the repository. The changelog fragment
files should be added with each PR as described in
`docs/changes/README.rst <https://github.com/astropy/astropy/blob/main/docs/changes/README.rst>`_.
The purpose of this file is to give a technical, but still user (and developer)
oriented overview of what changes were made to Astropy between each public
release.  The idea is that it's a little more to the point and easier to follow
than trying to read through full git log.  It lists all new features added
between versions, so that a user can easily find out from reading the changelog
when a feature was added.  Likewise it lists any features or APIs that were
changed (and how they were changed) or removed.  It also lists all bug fixes.
Affiliated packages are encouraged to maintain a similar changelog.

.. _stale-policies:

==============
Stale Policies
==============

The ``astropy`` GitHub repository has the following stale policies, which are
enforced by `action-astropy-stalebot <https://github.com/pllim/action-astropy-stalebot/>`_
in `.github/workflows/stalebot.yml <https://github.com/astropy/astropy/blob/main/.github/workflows/stalebot.yml>`_
that runs on a schedule. Hereafter, we refer to this automated enforcer as stale-bot.

All the timing mentioned depends on a successful stale-bot run. GitHub API limits,
spam protection, or server maintenance could affect the run. The former might
especially be relevant when there is a significant backlog of stale issues and
pull requests accumulated.

If you notice unintended stale-bot behaviors, please report them to the Astropy
maintainers.

Issues
------

A maintainer applies the "Closed?" label to mark an issue as stale, otherwise it
stays open until someone manually closes it. Once marked as stale, a warning will
be issued.

A maintainer can apply "keep-open" label or remove "Closed?" label to remove the
stale status. Otherwise, stale-bot will close the issue after about a week and
apply a "closed-by-bot" label.

When both "keep-open" and "Close?" labels exist, the former will take precedence
and the latter will be removed from the issue.

Pull Requests
-------------

A pull request becomes stale after about 4-5 months since the last commit (stale-bot
counts in seconds and naively assumes 30 days per month). When this happens, stale-bot
applies the "Close?" label to it. A maintainer can also fast-track its staleness by
manually applying the "Close?" label. Once marked as stale, a warning will be issued.

A maintainer can apply "keep-open" label or remove "Closed?" label to remove the
stale status. The pull request author (or maintainer) can reset the stale timer by
pushing out a commit (e.g., by rebasing). Otherwise, stale-bot will close the
pull request after about a month and apply a "closed-by-bot" label.

.. note::

    The "keep-open" label should be used very sparingly, only for pull requests that
    *must* be kept open. For example, a pull request that has been completed but cannot be
    merged until a blocker is removed can use this label. An abandoned or incomplete
    pull request should not use this label as it can be re-opened later when the author
    has a renewed interest to wrap it up.

When both "keep-open" and "Close?" labels exist, the former will take precedence
and the latter will be removed from the pull request. If maintainer removes "Close?"
without applying "keep-open" or pushing a new commit, stale-bot will mark it as
stale again in the next run.

If a new commit is pushed but the "Close?" label remains, stale-bot will close
it without another warning after another 4-5 months.

In short, to truly reset the stale timer for a pull request, it is recommended
that a new commit be pushed *and* the "Close?" label be removed.

.. include:: links.inc
:orphan:

.. _basic-workflow:

****************
Creating patches
****************

Overview
========

If you haven't already configured git::

    git config --global user.name "Your Name"
    git config --global user.email you@yourdomain.example.com

Then, the workflow is the following::

   # Get the repository if you don't have it
   git clone --recursive git://github.com/astropy/astropy.git

   # Make a branch for your patching
   cd astropy
   git branch the-fix-im-thinking-of
   git checkout the-fix-im-thinking-of

   # hack, hack, hack

   # Tell git about any new files you've made
   git add somewhere/tests/test_my_bug.py

   # Commit work in progress as you go
   git commit -am 'BF - added tests for Funny bug'

   # hack hack, hack

   # Commit work
   git commit -am 'BF - added fix for Funny bug'

   # Make the patch files
   git format-patch -M -C main

Then, send the generated patch files to the `astropy-dev mailing list`_ |emdash|
where we will thank you warmly.

In detail
=========

#. Tell git who you are so it can label the commits you've
   made::

    git config --global user.name "Your Name"
    git config --global user.email you@yourdomain.example.com

   This is only necessary if you haven't already done this, and you haven't
   checked to :ref:`check_git_install`.

#. If you don't already have one, clone a copy of the
   Astropy_ repository::

      git clone --recursive git://github.com/astropy/astropy.git
      cd astropy

#. Make a 'feature branch'. This will be where you work on your bug fix. It's
   nice and safe and leaves you with access to an unmodified copy of the code
   in the main branch::

      git branch the-fix-im-thinking-of
      git checkout the-fix-im-thinking-of

#. Do some edits, and commit them as you go::

      # hack, hack, hack

      # Tell git about any new files you've made
      git add somewhere/tests/test_my_bug.py

      # Commit work in progress as you go
      git commit -am 'BF - added tests for Funny bug'

      # hack hack, hack

      # Commit work
      git commit -am 'BF - added fix for Funny bug'

   Note the ``-am`` options to ``commit``. The ``m`` flag just
   signals that you're going to type a message on the command
   line.  The ``a`` flag |emdash| you can just take on faith |emdash|
   or see `why the -a flag?`_.

#. When you have finished, check you have committed all your changes::

      git status

#. Finally, make your commits into patches. You want all the commits since you
   branched from the ``main`` branch::

      git format-patch -M -C main

   You will now have several files named for the commits::

      0001-BF-added-tests-for-Funny-bug.patch
      0002-BF-added-fix-for-Funny-bug.patch

   Send these files to the `astropy-dev mailing list`_.

When you are done, to switch back to the main copy of the
code, just return to the ``main`` branch::

   git checkout main

.. include:: links.inc
:orphan:

.. include:: links.inc
.. _astropy-fix-example:

**********************************************
Contributing code to Astropy, a worked example
**********************************************

This example is based on fixing `Issue 1761`_ from the list
of `astropy issues on GitHub <https://github.com/astropy/astropy/issues>`_.
It resulted in `pull request 1917`_.

The issue title was "len() does not work for coordinates" with description
"It would be nice to be able to use ``len`` on coordinate arrays to know how
many coordinates are present."

This particular example was chosen because it was tagged as easy in GitHub;
seemed like the best place to start out!

Before you begin
================

Make sure you have a local copy of astropy set up as described in
:ref:`get_devel`. In a nutshell, the output of ``git remote -v``, run in the
directory where your local of ``astropy`` resides, should be something like this::

    astropy   git@github.com:astropy/astropy.git (fetch)
    astropy   git@github.com:astropy/astropy.git (push)
    your-user-name     git@github.com:your-user-name/astropy.git (fetch)
    your-user-name     git@github.com:your-user-name/astropy.git (push)

The precise form of the URLs for ``your-user-name`` depends on the
authentication method you set up with GitHub.

The important point is that ``astropy`` should point to the official ``astropy``
repo and ``your-user-name`` should point to *your* copy of ``astropy`` on GitHub.


Grab the latest updates to astropy
==================================

A few steps in this tutorial take only a single command. They are broken out
separately to outline the process in words as well as code.

Inform your local copy of ``astropy`` about the latest changes in the development
version with::

    git fetch astropy --tags

Set up an isolated workspace
============================

+ Make a new `git`_ branch for fixing this issue and switch to the branch::

    git checkout astropy/main -b fix-1761

+ Make a Python environment just for this fix and switch to that environment.
  The example below shows the necessary steps in the Miniconda/Anaconda Python
  distribution::

    conda create -n apy-1761 python=3.9 # replace 3.9 with desired version
    conda activate apy-1761

  If you are using a different distribution, see :ref:`virtual_envs` for
  instructions for creating and activating a new environment.

+ Install our branch in this environment with::

    pip install -e .[test]

Do you really have to set up a separate Python environment for each fix? No,
but you definitely want to have a Python environment for your work on code
contributions. Making new environments is fast, does not take much space, and
provide a way to keep your work organized.

If installation fails, try to upgrade ``pip`` using ``pip install pip -U``
command. It is also a good practice to keep your ``conda`` up-to-date by
running ``conda update conda -n base`` when prompted to do so; maybe ``git`` too.

Test first, please
==================

It would be hard to overstate the importance of testing in Astropy. Tests are
what gives you confidence that new code does what it should and that it
does not break old code.

You should at least run the relevant tests before you make any changes to make
sure that your Python environment is set up properly.

The first challenge is figuring out where to look for relevant tests. `Issue
1761`_ is a problem in the `~astropy.coordinates` package, so the tests for
it are in ``astropy/coordinates/tests``. The rest of ``astropy`` has a similar
layout, as described at :ref:`testing-guidelines`.

Run the current tests in that directory with::

    pytest astropy/coordinates/tests

If the bug you are working on involves remote data access, you need to run
the tests with an extra flag, i.e., ``pytest ... --remote-data``.

In the event where all the tests passed with the bug present, new tests are
needed to expose this bug.

A subpackage organizes its tests into multiple test modules; e.g.::

    $ ls astropy/coordinates/tests
    test_angles.py
    test_angular_separation.py
    test_api_ape5.py
    test_arrays.py
    ...

`Issue 1761`_ affects arrays of coordinates, so it seems sensible to put the
new test in ``test_arrays.py``. As with all of the steps, when in doubt,
please ask on the `astropy-dev mailing list`_.

The goal at this point may be a little counter-intuitive: write a test that we
know will fail with the current code. This test allows ``astropy`` to check,
in an automated way, whether our fix actually works and to prevent
regression (i.e., make sure future changes to code do not break our fix).

Looking over the existing code in ``test_arrays.py``, each test is a function
with a name that starts with ``test_``. An appropriate place to add the test is
after the last test function in the file.

Give the test a reasonably clear name; e.g., ``test_array_len``. The
easiest way to figure out what you need to import and how to set up the test
is to look at other tests. The full test is in the traceback below and in
`pull request 1917`_.

Write the test, then see if it works as expected; remember, in this case we
expect it to *fail* without the patch from `pull request 1917`_.
Running ``pytest astropy/coordinates/tests/test_arrays.py`` would give the expected failure;
an excerpt from the output is::

    ================= FAILURES =============================
    ______________ test_array_len __________________________

        def test_array_len():
            from .. import ICRS

            input_length = 5
            ra = np.linspace(0, 360, input_length)
            dec = np.linspace(0, 90, input_length)

            c = ICRS(ra, dec, unit=(u.degree, u.degree))

    >       assert len(c) == input_length
    E       TypeError: object of type 'ICRS' has no len()

    astropy/coordinates/tests/test_arrays.py:291: TypeError

Success!

Add this test to your local `git`_ repo
=======================================

Keep `git`_ commits small and focused on one logical piece at a time. The test
we just wrote is one logical change, so we will commit it. You could, if you
prefer, wait and commit this test along with your fix.

For this tutorial, we will commit the test separately. If you are not sure what to
do, ask on `astropy-dev mailing list`_.

Check what was changed
----------------------

We can see what has changed with ``git status``::

    $ git status
    On branch fix-1761
    Your branch is up-to-date with 'astropy/main'.

    Changes not staged for commit:
      (use "git add <file>..." to update what will be committed)
      (use "git checkout -- <file>..." to discard changes in working directory)

        modified:   astropy/coordinates/tests/test_arrays.py

    no changes added to commit (use "git add" and/or "git commit -a")

There are two bits of information here:

+ one file changed; i.e., ``astropy/coordinates/tests/test_arrays.py``
+ this file has not been added to git's staging area yet, so it is listed
  under ``Changes not staged for commit``.

Use ``git diff`` to see what changes have been made::

    $ git diff
    diff --git a/astropy/coordinates/tests/test_arrays.py b/astropy/coordinates/test
    index 2785b59..7eecfbb 100644
    --- a/astropy/coordinates/tests/test_arrays.py
    +++ b/astropy/coordinates/tests/test_arrays.py
    @@ -278,3 +278,14 @@ def test_array_indexing():
         assert c2.equinox == c1.equinox
         assert c3.equinox == c1.equinox
         assert c4.equinox == c1.equinox
    +
    +
    +def test_array_len():
    +    from .. import ICRS
    +
    +    input_length = 5
    +    ra = np.linspace(0, 360, input_length)
    +    dec = np.linspace(0, 90, input_length)
    +
    +    c = ICRS(ra, dec, unit=(u.degree, u.degree))
    +
    +    assert len(c) == input_length

A graphical interface to git makes keeping track of these sorts of changes
even easier; see :ref:`git_gui_options` if you are interested.

Stage the change
----------------

`git`_ requires you to add changes in two steps:

+ stage the change with ``git add ...``; this adds the file to
  the list of items that will be added to the repo when you are ready to
  commit.
+ commit the change with ``git commit ...``; this actually adds the changes to
  your repo.

These can be combined into one step (not recommended); the advantage of doing it in two steps
is that it is easier to undo staging than committing. As we will see later,
``git status`` even tells you how to do it.

Staging can be very handy if you are making changes in a couple of different
places that you want to commit at the same time. Make your first changes,
stage it, then make your second change and stage that. Once everything is
staged, commit the changes as one commit.

In this case, first stage the change::

    git add astropy/coordinates/tests/test_arrays.py

You get no notice at the command line that anything has changed, but
``git status`` will let you know::

    $ git status
    On branch fix-1761
    Your branch is up-to-date with 'astropy/main'.

    Changes to be committed:
      (use "git reset HEAD <file>..." to unstage)

        modified:   astropy/coordinates/tests/test_arrays.py

Note that `git`_ helpfully includes the command necessary to unstage the
change if you want to.

Commit your change
------------------

Next, we will commit the test without the fix::

    $ git commit -m "Add test for array coordinate length (issue #1761)"
    [fix-1761 dd4ef8c] Add test for array coordinate length (issue #1761)
     1 file changed, 12 insertions(+)

Commit messages should be concise. Including the GitHub issue
number allows GitHub to automatically create links to the relevant issue.

Use ``git status`` to get a recap of where we are so far::

    $ git status
    On branch fix-1761
    Your branch is ahead of 'astropy/main' by 1 commit.
      (use "git push" to publish your local commits)

    nothing to commit, working directory clean

In other words, we have made a change to our local copy of ``astropy`` but we
have not pushed (transferred) that change to our GitHub account.

Fix the issue
=============

Write the code
--------------

Now that we have a test written, we will fix the issue. A full discussion of
the fix is beyond the scope of this tutorial, but the fix is to add a
``__len__`` method to ``astropy.coordinates.SphericalCoordinatesBase`` in
``coordsystems.py`` (the code has since been refactored, if you try to look
for it). All of the spherical coordinate systems inherit from
this base class and it is this base class that implements the
``__getitem__`` method that allows indexing of coordinate arrays.

See `pull request 1917`_ to view the changes to the code.

.. _test_changes:

Test your change
----------------

There are a few levels at which you want to test:

+ Does this code change make the test we wrote succeed now? Check
  by running ``pytest astropy/coordinates/tests/test_arrays.py``.
  In this case, yes!
+ Do the rest of the coordinate tests still pass? Check by running
  ``pytest astropy/coordinates/``. In this case, yes, we have not broken
  anything!
+ Do all of the astropy tests still succeed? Check by running ``pytest``
  from the top-level directory.
  This may take a while depending on the speed of your system.
  Success again!

.. note::
    Tests that are skipped or xfailed are fine. A fail or an error is not
    fine. If you get stuck, ask on `astropy-dev mailing list`_ for help!

Stage and commit your change
----------------------------

Add the file to your `git`_ repo in two steps: stage, then commit.

To make this a little different than the commit we did above, make sure you
are still in the top level directory and check the ``git status``::

    $ git status
    On branch fix-1761
    Your branch is ahead of 'astropy/main' by 1 commit.
      (use "git push" to publish your local commits)

    Changes not staged for commit:
      (use "git add <file>..." to update what will be committed)
      (use "git checkout -- <file>..." to discard changes in working directory)

        modified:   astropy/coordinates/coordsystems.py

    no changes added to commit (use "git add" and/or "git commit -a")

Note that git knows what has changed no matter what directory you are in (as
long as you are in one of the directories in the repo, that is).

Stage the change with::

    git add astropy/coordinates/coordsystems.py

For this commit, it is helpful to use a multi-line commit message that will
automatically close the issue on GitHub when this change is accepted. The
snippet below accomplishes that in bash (and similar shells)::

    $ git commit -m"
    > Add len() to coordinates
    >
    > Closes #1761"
    [fix-1761 f196771] Add len() to coordinates
     1 file changed, 4 insertions(+)

Another option for multi-line commit message is to use a Git GUI or to
run ``git commit`` without a message to get prompted by an editor.

The message after committing should look like this when you inspect with
``git log``::

    Add len() to coordinates

    Closes #1761

If the commit message does not look right, run ``git commit --amend``.
If you still run into problems, please ask about fixing it at
`astropy-dev mailing list`_.

At this point, none of the Astropy maintainers know anything about
your changes.

We will take care of that in a moment with a "pull request", but first,
see :ref:`astropy-fix-add-tests`.

.. _astropy-fix-add-tests:

Stop and think: Any more tests or other changes?
================================================

It never hurts to pause at this point and review whether your proposed
changes are complete. In this case, there are some more tests that could
be included, such as:

+ What happens when ``len()`` is called on a coordinate that is *not* an
  array?
+ Does ``len()`` work when the coordinate is an array with one entry?

Both of these are mentioned in `pull request 1917`_, so it does not hurt to check
them. In this case, they also provide an opportunity to illustrate a feature
of the `pytest`_ framework.

The second case is easier, so it will be handled first following the
development cycle we used above:

+ Make the change in ``astropy/coordinates/tests/test_arrays.py``
+ Test the change

The test passed; but rather than committing this one change, we will also
implement the check for the scalar case.

One could imagine two different desirable outcomes here:

+ ``len(scalar_coordinate)`` behaves just like ``len(scalar_angle)``, raising
  a `TypeError` for a scalar coordinate.
+ ``len(scalar_coordinate)`` returns 1 since there is one coordinate.

If you encounter a case like this and are not sure what to do, ask. The best
place to ask is on GitHub on the page for the issue you are fixing.

Alternatively, make a choice and be clear in your pull request on GitHub what
you chose and why; instructions for that are below.

Testing for an expected error
-----------------------------

In this case, we opted for raising a `TypeError`, because
the user needs to know that the coordinate they created is not going to
behave like an array of one coordinate if they try to index it later on.

The `pytest`_ framework makes testing for an exception relatively
easy; you put the code you expect to fail in a ``with`` block::

    c = ICRS(0, 0, unit=(u.degree, u.degree))

    with pytest.raises(TypeError):
        len(c)

A test like this can be added ``test_array_len`` in ``test_arrays.py``.
In your own work, you may also choose to put that into a new test function,
if you wish.

Aside: Python lesson--let others do your work
---------------------------------------------

The actual fix to this issue was very, very short. In ``coordsystems.py``, two
lines were added::

    def __len__(self):
        return len(self.lonangle)

``lonangle`` contains the ``Angle``s that represent longitude (sometimes this
is an RA, sometimes a longitude). By simply calling ``len()`` on one of the
angles in the array you get, for free, whatever behavior has been defined in
the ``Angle`` class for handling the case of a scalar.

Adding an explicit check for the case of a scalar here would have the very
big downside of having two things that need to be kept in sync: handling of
scalars in ``Angle`` and in coordinates.

Commit any additional changes
=============================

Continue to follow the development cycle above for other files that you need
to modify, including changelog (see :ref:`git_edit_changelog`) and
documentation, as needed:

+ Check that **all** ``astropy`` tests still pass; see :ref:`test_changes`
+ ``git status`` to see what needs to be staged and committed
+ ``git add ...`` to stage the changes
+ ``git commit ...`` to commit the changes
+ ``git log`` to inspect the change history

The `git`_ commands, without their output, are::

    git status
    git add astropy/coordinates/tests/test_arrays.py
    git commit -m "Add tests of len() for scalar coordinate and length 1 coordinate"
    git log

Push your changes to your GitHub fork of astropy
================================================

Use this command to push your local changes out to your copy of ``astropy``
on GitHub before asking for the changes to be reviewed::

    git push your-user-name fix-1761

Propose your changes as a pull request
======================================

This stage requires going to your GitHub account and navigate to *your* copy
of ``astropy``; the url will be something like
``https://github.com/your-user-name/astropy``.

Once there, select the branch that contains your fix from the branches
dropdown:

    .. image:: worked_example_switch_branch.png

After selecting the correct branch, click on the "Pull Request" button,
as shown below:

    .. image:: pull_button.png

Name your pull request something sensible. Include the issue number with a
leading ``#`` in the description of the pull request so that a link is
created to the original issue, as stated in ``astropy``'s pull request template.

Please see `pull request 1917`_ for the pull request showcased in this tutorial.

.. _git_edit_changelog:

Edit the changelog
==================

Keeping the list of changes up to date is nearly impossible unless each
contributor makes the appropriate updates as they propose changes.

Create a file ``docs/changes/coordinates/<PULL REQUEST>.feature.rst``, where
``<PULL REQUEST>`` is the pull request number (1917 for this example).  The
content of this file should summarize what you did. For writing changelog
entries, you do not need to know much about the markup language being used
(though you can read as much as you want about it at the `Sphinx primer`_); look
at other entries and emulate.

For this issue, the file would contain::

    Implemented ``len()`` for coordinate objects.

Putting ``len()`` in double-backtick makes that text render in a monospaced
font.

Commit your changes and push
----------------------------

You can use ``git status`` as above or jump right to staging and committing::

    git add docs/changes/coordinates/<PULL REQUEST>.feature.rst
    git commit -m "Add changelog entry"
    git push

Revise and push as necessary
============================

You may be asked to make changes in the discussion of the pull request. Make
those changes in your local copy, commit them to your local repo, and push them
to GitHub. GitHub will automatically update your pull request.

.. _Issue 1761: https://github.com/astropy/astropy/issues/1761
.. _pull request 1917: https://github.com/astropy/astropy/pull/1917
.. _Sphinx primer: https://www.sphinx-doc.org/
.. _test commit: https://github.com/mwcraig/astropy/commit/cf7d5ac15d7c63ae28dac638c6484339bac5f8de
:orphan:

.. _git-resources:

*************
Git resources
*************

Tutorials and summaries
***********************

* `GitHub Help`_ has an excellent series of how-to guides.
* `learn.github`_ has an excellent series of tutorials
* The `pro git book`_ is a good in-depth book on git.
* A `git cheat sheet`_ is a page giving summaries of common commands.
* The `git user manual`_
* The `git tutorial`_
* The `git community book`_
* `git ready`_ |emdash| a nice series of tutorials
* `git casts`_ |emdash| video snippets giving git how-tos.
* `git magic`_ |emdash| extended introduction with intermediate detail
* The `git parable`_ is an easy read explaining the concepts behind git.
* `git foundation`_ expands on the `git parable`_.
* Fernando Perez' git page |emdash| `Fernando's git page`_ |emdash| many
  links and tips
* Fernando Perez's `ipython notebook on using git in science`_
* A good but technical page on `git concepts`_
* `git svn crash course`_: git for those of us used to subversion

Manual pages online
*******************

You can get these on your own machine with (e.g) ``git help push`` or
(same thing) ``git push --help``, but, for convenience, here are the
online manual pages for some common commands:

* `git add`_
* `git branch`_
* `git checkout`_
* `git clone`_
* `git commit`_
* `git config`_
* `git diff`_
* `git log`_
* `git pull`_
* `git push`_
* `git remote`_
* `git status`_

.. include:: links.inc
:orphan:

.. _additional-git:

Some other things you might want to do
**************************************

Delete a branch on GitHub
=========================

`git`_ strongly encourages making a new branch each time you make a change in the
code. At some point you will need to clean up the branches you no longer need--
that point is *after* your changes have been accepted if you made a pull request
for those changes.

There are two places to delete the branch: in your local repo and on GitHub.

You can do these independent of each other.

To delete both your local copy AND the GitHub copy from the command line follow
these instructions::

   # change to the main branch (if you still have one, otherwise change to
   # another branch)
   git checkout main

   # delete branch locally
   # Note: -d tells git to check whether your branch has been merged somewhere
   # if it hasn't, and you delete it, it is gone forever.
   #
   # Use -D instead to force deletion regardless of merge status
   git branch -d my-unwanted-branch

   # delete branch on GitHub
   git push origin :my-unwanted-branch

(Note the colon ``:`` before ``test-branch``.) See `Github's instructions for
deleting a branch
<https://help.github.com/en/articles/creating-and-deleting-branches-within-your-repository>`_
if you want to delete the GitHub copy through GitHub.

Several people sharing a single repository
==========================================

If you want to work on some stuff with other people, where you are all
committing into the same repository, or even the same branch, then just
share it via GitHub.

First fork Astropy into your account, as from :ref:`fork_a_copy`.

Then, go to your forked repository GitHub page, e.g.,
``https://github.com/your-user-name/astropy``

Click on the 'Admin' button, and add anyone else to the repo as a
collaborator:

   .. image:: pull_button.png

Now all those people can do::

    git clone --recursive git@githhub.com:your-user-name/astropy.git

Remember that links starting with ``git@`` use the ssh protocol and are
read-write; links starting with ``git://`` are read-only.

Your collaborators can then commit directly into that repo with the
usual::

     git commit -am 'ENH - much better code'
     git push origin main # pushes directly into your repo

Explore your repository
=======================

To see a graphical representation of the repository branches and
commits::

   gitk --all

To see a linear list of commits for this branch::

   git log

You can also look at the `network graph visualizer`_ for your GitHub
repo.

.. _rebase-on-trunk:

Rebasing on trunk
=================

Let's say you thought of some work you'd like to do. You
:ref:`fetch-latest` and :ref:`make-feature-branch` called
``cool-feature``. At this stage trunk is at some commit, let's call it E. Now
you make some new commits on your ``cool-feature`` branch, let's call them A,
B, C. Maybe your changes take a while, or you come back to them after a while.
In the meantime, trunk has progressed from commit E to commit (say) G::

          A---B---C cool-feature
         /
    D---E---F---G trunk

At this stage you consider merging trunk into your feature branch, and you
remember that this here page sternly advises you not to do that, because the
history will get messy. Most of the time you can just ask for a review, and
not worry that trunk has got a little ahead. But sometimes, the changes in
trunk might affect your changes, and you need to harmonize them. In this
situation you may prefer to do a rebase.

Rebase takes your changes (A, B, C) and replays them as if they had been made
to the current state of ``trunk``. In other words, in this case, it takes the
changes represented by A, B, C and replays them on top of G. After the rebase,
your history will look like this::

                  A'--B'--C' cool-feature
                 /
    D---E---F---G trunk

See `rebase without tears`_ for more detail.

To do a rebase on trunk::

    # Update the mirror of trunk
    git fetch upstream

    # Go to the feature branch
    git checkout cool-feature

    # Make a backup in case you mess up
    git branch tmp cool-feature

    # Rebase cool-feature onto trunk
    git rebase --onto upstream/main upstream/main cool-feature

In this situation, where you are already on branch ``cool-feature``, the last
command can be written more succinctly as::

    git rebase upstream/main

When all looks good you can delete your backup branch::

   git branch -D tmp

If it doesn't look good you may need to have a look at
:ref:`recovering-from-mess-up`.

If you have made changes to files that have also changed in trunk, this may
generate merge conflicts that you need to resolve - see the `git rebase`_ man
page for some instructions at the end of the "Description" section. There is
some related help on merging in the git user manual - see `resolving a
merge`_.

If your feature branch is already on GitHub and you rebase, you will have to
force push the branch; a normal push would give an error. If the branch you
rebased is called ``cool-feature`` and your GitHub fork is available as the
remote called ``origin``, you use this command to force-push::

   git push -f origin cool-feature

Note that this will overwrite the branch on GitHub, i.e. this is one of the few
ways you can actually lose commits with git. Also note that it is never allowed
to force push to the main astropy repo (typically called ``upstream``), because
this would re-write commit history and thus cause problems for all others.

.. _recovering-from-mess-up:

Recovering from mess-ups
========================

Sometimes, you mess up merges or rebases. Luckily, in git it is relatively
straightforward to recover from such mistakes.

If you mess up during a rebase::

   git rebase --abort

If you notice you messed up after the rebase::

   # Reset branch back to the saved point
   git reset --hard tmp

If you forgot to make a backup branch::

   # Look at the reflog of the branch
   git reflog show cool-feature

   8630830 cool-feature@{0}: commit: BUG: io: close file handles immediately
   278dd2a cool-feature@{1}: rebase finished: refs/heads/my-feature-branch onto 11ee694744f2552d
   26aa21a cool-feature@{2}: commit: BUG: lib: make seek_gzip_factory not leak gzip obj
   ...

   # Reset the branch to where it was before the botched rebase
   git reset --hard cool-feature@{2}

.. _rewriting-commit-history:

Rewriting commit history
========================

.. note::

   Do this only for your own feature branches.

There's an embarrassing typo in a commit you made? Or perhaps the you
made several false starts you would like the posterity not to see.

This can be done via *interactive rebasing*.

Suppose that the commit history looks like this::

    git log --oneline
    eadc391 Fix some remaining bugs
    a815645 Modify it so that it works
    2dec1ac Fix a few bugs + disable
    13d7934 First implementation
    6ad92e5 * masked is now an instance of a new object, MaskedConstant
    29001ed Add pre-nep for a couple of structured_array_extensions.
    ...

and ``6ad92e5`` is the last commit in the ``cool-feature`` branch. Suppose we
want to make the following changes:

* Rewrite the commit message for ``13d7934`` to something more sensible.
* Combine the commits ``2dec1ac``, ``a815645``, ``eadc391`` into a single one.

We do as follows::

    # make a backup of the current state
    git branch tmp HEAD
    # interactive rebase
    git rebase -i 6ad92e5

This will open an editor with the following text in it::

    pick 13d7934 First implementation
    pick 2dec1ac Fix a few bugs + disable
    pick a815645 Modify it so that it works
    pick eadc391 Fix some remaining bugs

    # Rebase 6ad92e5..eadc391 onto 6ad92e5
    #
    # Commands:
    #  p, pick = use commit
    #  r, reword = use commit, but edit the commit message
    #  e, edit = use commit, but stop for amending
    #  s, squash = use commit, but meld into previous commit
    #  f, fixup = like "squash", but discard this commit's log message
    #
    # If you remove a line here THAT COMMIT WILL BE LOST.
    # However, if you remove everything, the rebase will be aborted.
    #

To achieve what we want, we will make the following changes to it::

    r 13d7934 First implementation
    pick 2dec1ac Fix a few bugs + disable
    f a815645 Modify it so that it works
    f eadc391 Fix some remaining bugs

This means that (i) we want to edit the commit message for ``13d7934``, and
(ii) collapse the last three commits into one. Now we save and quit the
editor.

Git will then immediately bring up an editor for editing the commit message.
After revising it, we get the output::

    [detached HEAD 721fc64] FOO: First implementation
     2 files changed, 199 insertions(+), 66 deletions(-)
    [detached HEAD 0f22701] Fix a few bugs + disable
     1 files changed, 79 insertions(+), 61 deletions(-)
    Successfully rebased and updated refs/heads/my-feature-branch.

and the history looks now like this::

     0f22701 Fix a few bugs + disable
     721fc64 ENH: Sophisticated feature
     6ad92e5 * masked is now an instance of a new object, MaskedConstant

If it went wrong, recovery is again possible as explained :ref:`above
<recovering-from-mess-up>`.

.. _merge-commits-and-cherry-picks:

Merge commits and cherry picks
==============================

Let's say that you have a fork (origin) on GitHub of the main Astropy
repository (upstream).  Your fork is up to date with upstream's main branch
and you've made some commits branching off from it on your own branch::

    upstream:

       main
          |
    A--B--C

    origin:

     upstream/main
          |
    A--B--C
           \
            D--E
               |
           issue-branch

Then say you make a pull request of issue-branch against Astroy's main, and
the pull request is accepted and merged.  When GitHub merges the pull request
it's basically doing the following in the upstream repository::

    $ git checkout main
    $ git remote add yourfork file:///path/to/your/fork/astropy
    $ git fetch yourfork
    $ git merge --no-ff yourfork/issue-branch


Because it always uses ``--no-ff`` we always get a merge commit (it is possible
to manually do a fast-forward merge of a pull request, but we rarely ever do
that).  Now the main Astropy repository looks like this::


    upstream:

              main
                 |
    A--B--C------F
           \    /
            D--E
               |
        yourfork/issue-branch

where "F" is the merge commit GitHub just made in upstream.

When you do cherry-pick of a non-merge commit, say you want to just cherry-pick
"D" from the branch, what happens is it does a diff of "D" with its parent (in
this case "C") and applies that diff as a patch to whatever your HEAD is.

The problem with a merge commit, such as "F", is that "F" has two parents: "C"
and "E".  It doesn't know whether to apply the diff of "F" with "C" or the diff
of "F" with "E".  Clearly in this case of backporting a pull request to a bug
fix branch we want to apply everything that changed on main from the merge,
so we want the diff of "F" with "C".

Since GitHub was on ``main`` when it did ``git merge yourfork/issue-branch``, the
last commit in ``main`` is the first parent.  Basically whatever HEAD you're on
when you do the merge is the first parent, and the tip you're merging from is
the second parent (octopus merge gets more complicated but only a little, and
that doesn't apply to pull requests).  Since parents are numbered starting from
"1" then we will always cherry-pick merge commits with ``-m 1`` in this case.

That's not to say that the cherry-pick will always apply cleanly.  Say in
upstream we also have a backport branch that we want to cherry pick "F" onto::

    upstream:

      backport
         |
         G       main
        /          |
    A--B----C------F
             \    /
              D--E

We would do::

    $ git checkout backport
    $ git cherry-pick -m 1 F

But this applies the diff of "F" with "C", not of "F" with "G".  So clearly
there's potential for conflicts and incongruity here.  But this will work like
any merge that has conflicts--you can resolve any conflicts manually and then
commit.  As long as the fix being merged is reasonably self-contained this
usually requires little effort.

.. include:: links.inc
.. _timeseries-masking:

Masking Values in Time Series
*****************************

.. warning:: Note that masking does not yet work for columns that have units.

Masking values is done in the same way as for |Table| objects (see
:ref:`masking_and_missing_values`). The most convenient way to use masking is to
initialize a |TimeSeries| object using the ``masked=True`` option.

Example
-------

.. EXAMPLE START: Masking Values in TimeSeries Objects

We start by initializing a |TimeSeries| object with ``masked=True``::

    >>> from astropy import units as u
    >>> from astropy.timeseries import TimeSeries
    >>> ts = TimeSeries(time_start='2016-03-22T12:30:31',
    ...                 time_delta=3 * u.s,
    ...                 n_samples=5, masked=True)

We can now add some data to our time series::

    >>> ts['flux'] = [1., -2., 5., -1., 4.]

As you can see, some of the values are negative. We can mask these using::

    >>> ts['flux'].mask = ts['flux'] < 0
    >>> ts
    <TimeSeries masked=True length=5>
              time            flux
              Time          float64
    ----------------------- -------
    2016-03-22T12:30:31.000     1.0
    2016-03-22T12:30:34.000      --
    2016-03-22T12:30:37.000     5.0
    2016-03-22T12:30:40.000      --
    2016-03-22T12:30:43.000     4.0

We can also access the mask values::

    >>> ts['flux'].mask
    array([False,  True, False,  True, False]...)

Masks are column-based, so masking a single cell does not mask the whole row.
Having masked cells then allows functions that normally understand masked values
and operate on columns to ignore the masked entries::

    >>> import numpy as np
    >>> np.min(ts['flux'])
    1.0
    >>> np.ma.median(ts['flux'])
    4.0

.. EXAMPLE END
.. _stats-bls:

***********************************
Box Least Squares (BLS) Periodogram
***********************************

The "box least squares" (BLS) periodogram [1]_ is a statistical tool used for
detecting transiting exoplanets and eclipsing binaries in time series
photometric data. The main interface to this implementation is the
`~astropy.timeseries.BoxLeastSquares` class.

Mathematical Background
=======================

The BLS method finds transit candidates by modeling a transit as a periodic
upside down top hat with four parameters: period, duration, depth, and a
reference time. In this implementation, the reference time is chosen to be the
mid-transit time of the first transit in the observational baseline. These
parameters are shown in the following sketch:

.. plot::

    import numpy as np
    import matplotlib.pyplot as plt

    period = 6
    t0 = -3
    duration = 2.5
    depth = 0.1
    x = np.linspace(-5, 5, 50000)
    y = np.ones_like(x)
    y[np.abs((x-t0+0.5*period)%period-0.5*period)<0.5*duration] = 1.0 - depth
    plt.figure(figsize=(7, 4))
    plt.axvline(t0, color="k", ls="dashed", lw=0.75)
    plt.axvline(t0+period, color="k", ls="dashed", lw=0.75)
    plt.axhline(1.0-depth, color="k", ls="dashed", lw=0.75)
    plt.plot(x, y)

    kwargs = dict(
        va="center", arrowprops=dict(arrowstyle="->", lw=0.5),
        bbox={"fc": "w", "ec": "none"},
    )
    plt.annotate("period", xy=(t0+period, 1.01), xytext=(t0+0.5*period, 1.01), ha="center", **kwargs)
    plt.annotate("period", xy=(t0, 1.01), xytext=(t0+0.5*period, 1.01), ha="center", **kwargs)
    plt.annotate("duration", xy=(t0-0.5*duration, 1.0-0.5*depth), xytext=(t0, 1.0-0.5*depth), ha="center", **kwargs)
    plt.annotate("duration", xy=(t0+0.5*duration, 1.0-0.5*depth), xytext=(t0, 1.0-0.5*depth), ha="center", **kwargs)
    plt.annotate("reference time", xy=(t0, 1.0-depth-0.01), xytext=(t0+0.25*duration, 1.0-depth-0.01), ha="left", **kwargs)
    plt.annotate("depth", xy=(0.0, 1.0), xytext=(0.0, 1.0-0.5*depth), ha="center", rotation=90, **kwargs)
    plt.annotate("depth", xy=(0.0, 1.0-depth), xytext=(0.0, 1.0-0.5*depth), ha="center", rotation=90, **kwargs)


    plt.ylim(1.0 - depth - 0.02, 1.02)
    plt.xlim(-5, 5)
    plt.gca().set_yticks([])
    plt.gca().set_xticks([])
    plt.ylabel("brightness")
    plt.xlabel("time")

    # ****

Assuming that the uncertainties on the measured flux are known, independent,
and Gaussian, the maximum likelihood in-transit flux can be computed as

.. math::

    y_\mathrm{in} = \frac{\sum_\mathrm{in} y_n/{\sigma_n}^2}{\sum_\mathrm{in} 1/{\sigma_n}^2}

where :math:`y_n` are the brightness measurements, :math:`\sigma_n` are the
associated uncertainties, and both sums are computed over the in-transit data
points.

Similarly, the maximum likelihood out-of-transit flux is

.. math::

    y_\mathrm{out} = \frac{\sum_\mathrm{out} y_n/{\sigma_n}^2}{\sum_\mathrm{out} 1/{\sigma_n}^2}

where these sums are over the out-of-transit observations. Using these results,
the log likelihood of a transit model (maximized over depth) at a given period
:math:`P`, duration :math:`\tau`, and reference time :math:`t_0` is

.. math::

    \log \mathcal{L}(P,\,\tau,\,t_0) =
    -\frac{1}{2}\,\sum_\mathrm{in}\frac{(y_n-y_\mathrm{in})^2}{{\sigma_n}^2}
    -\frac{1}{2}\,\sum_\mathrm{out}\frac{(y_n-y_\mathrm{out})^2}{{\sigma_n}^2}
    + \mathrm{constant}

This equation might be familiar because it is proportional to the "chi
squared" :math:`\chi^2` for this model and this is a direct consequence of our
assumption of Gaussian uncertainties.

This :math:`\chi^2` is called the "signal residue" by [1]_, so maximizing the
log likelihood over duration and reference time is equivalent to computing the
box least squares spectrum from [1]_.

In practice, this is achieved by finding the maximum likelihood model over a
grid in duration and reference time as specified by the ``durations`` and
``oversample`` parameters for the
`~astropy.timeseries.BoxLeastSquares.power` method.

Behind the scenes, this implementation minimizes the number of required
calculations by pre-binning the observations onto a fine grid following [1]_
and [2]_.

Basic Usage
===========

The transit periodogram takes as input time series observations where the
timestamps ``t`` and the observations ``y`` (usually brightness) are stored as
``numpy`` arrays or :class:`~astropy.units.Quantity` objects. If known, error
bars ``dy`` can also optionally be provided.

Example
-------

.. EXAMPLE START: Evaluating BLS Periodograms

To evaluate the periodogram for a simulated data set:

>>> import numpy as np
>>> import astropy.units as u
>>> from astropy.timeseries import BoxLeastSquares
>>> np.random.seed(42)
>>> t = np.random.uniform(0, 20, 2000)
>>> y = np.ones_like(t) - 0.1*((t%3)<0.2) + 0.01*np.random.randn(len(t))
>>> model = BoxLeastSquares(t * u.day, y, dy=0.01)
>>> periodogram = model.autopower(0.2)

The output of the `.astropy.timeseries.BoxLeastSquares.autopower` method
is a `~astropy.timeseries.BoxLeastSquaresResults` object with several
useful attributes, the most useful of which are generally the ``period`` and
``power`` attributes.

This result can be plotted using matplotlib:

>>> import matplotlib.pyplot as plt                  # doctest: +SKIP
>>> plt.plot(periodogram.period, periodogram.power)  # doctest: +SKIP

.. plot::

    import numpy as np
    import astropy.units as u
    import matplotlib.pyplot as plt
    from astropy.timeseries import BoxLeastSquares

    np.random.seed(42)
    t = np.random.uniform(0, 20, 2000)
    y = np.ones_like(t) - 0.1*((t%3)<0.2) + 0.01*np.random.randn(len(t))
    model = BoxLeastSquares(t * u.day, y, dy=0.01)
    periodogram = model.autopower(0.2)

    plt.figure(figsize=(8, 4))
    plt.plot(periodogram.period, periodogram.power, "k")
    plt.xlabel("period [day]")
    plt.ylabel("power")

In this figure, you can see the peak at the correct period of three days.

.. EXAMPLE END

Objectives
==========

By default, the `~astropy.timeseries.BoxLeastSquares.power` method computes the
log likelihood of the model fit and maximizes over reference time and duration.
It is also possible to use the signal-to-noise ratio with which the transit
depth is measured as an objective function.

Example
-------

.. EXAMPLE START: Transit Search with BoxLeastSquares.power and Signal-to-Noise

To compute the log likelihood of the model fit, call
`~astropy.timeseries.BoxLeastSquares.power` or
`~astropy.timeseries.BoxLeastSquares.autopower` with ``objective='snr'`` as
follows:

>>> model = BoxLeastSquares(t * u.day, y, dy=0.01)
>>> periodogram = model.autopower(0.2, objective="snr")

.. plot::

    import numpy as np
    import astropy.units as u
    import matplotlib.pyplot as plt
    from astropy.timeseries import BoxLeastSquares

    np.random.seed(42)
    t = np.random.uniform(0, 20, 2000)
    y = np.ones_like(t) - 0.1*((t%3)<0.2) + 0.01*np.random.randn(len(t))
    model = BoxLeastSquares(t * u.day, y, dy=0.01)
    periodogram = model.autopower(0.2, objective="snr")

    plt.figure(figsize=(8, 4))
    plt.plot(periodogram.period, periodogram.power, "k")
    plt.xlabel("period [day]")
    plt.ylabel("depth S/N")

This objective will generally produce a periodogram that is qualitatively
similar to the log likelihood spectrum, but it has been used to improve the
reliability of transit search in the presence of correlated noise.

.. EXAMPLE END

Period Grid
===========

The transit periodogram is always computed on a grid of periods and the results
can be sensitive to the sampling. As discussed in [1]_, the performance of the
transit periodogram method is more sensitive to the period grid than the
`~astropy.timeseries.LombScargle` periodogram.

This implementation of the transit periodogram includes a conservative heuristic
for estimating the required period grid that is used by the
`~astropy.timeseries.BoxLeastSquares.autoperiod` and
`~astropy.timeseries.BoxLeastSquares.autopower` methods and the details of this
method are given in the API documentation for
`~astropy.timeseries.BoxLeastSquares.autoperiod`.

Example
-------

.. EXAMPLE START: Computing Transit Periodograms on a Grid of Periods

It is possible to provide a specific period grid as follows:

>>> model = BoxLeastSquares(t * u.day, y, dy=0.01)
>>> periods = np.linspace(2.5, 3.5, 1000) * u.day
>>> periodogram = model.power(periods, 0.2)

.. plot::

    import numpy as np
    import astropy.units as u
    import matplotlib.pyplot as plt
    from astropy.timeseries import BoxLeastSquares

    np.random.seed(42)
    t = np.random.uniform(0, 20, 2000)
    y = np.ones_like(t) - 0.1*((t%3)<0.2) + 0.01*np.random.randn(len(t))
    model = BoxLeastSquares(t * u.day, y, dy=0.01)
    periods = np.linspace(2.5, 3.5, 1000) * u.day
    periodogram = model.power(periods, 0.2)

    plt.figure(figsize=(8, 4))
    plt.plot(periodogram.period, periodogram.power, "k")
    plt.xlabel("period [day]")
    plt.ylabel("power")

However, if the period grid is too coarse, the correct period might be missed.

>>> model = BoxLeastSquares(t * u.day, y, dy=0.01)
>>> periods = np.linspace(0.5, 10.5, 15) * u.day
>>> periodogram = model.power(periods, 0.2)

.. plot::

    import numpy as np
    import astropy.units as u
    import matplotlib.pyplot as plt
    from astropy.timeseries import BoxLeastSquares

    np.random.seed(42)
    t = np.random.uniform(0, 20, 2000)
    y = np.ones_like(t) - 0.1*((t%3)<0.2) + 0.01*np.random.randn(len(t))
    model = BoxLeastSquares(t * u.day, y, dy=0.01)
    periods = np.linspace(0.5, 10.5, 15) * u.day
    periodogram = model.power(periods, 0.2)

    plt.figure(figsize=(8, 4))
    plt.plot(periodogram.period, periodogram.power, "k")
    plt.xlabel("period [day]")
    plt.ylabel("power")

.. EXAMPLE END

Peak Statistics
===============

To help in the transit vetting process and to debug problems with candidate
peaks, the `~astropy.timeseries.BoxLeastSquares.compute_stats` method can be
used to calculate several statistics of a candidate transit.

Many of these statistics are based on the VARTOOLS package described in [2]_.
This will often be used as follows to compute stats for the maximum point in
the periodogram:

>>> model = BoxLeastSquares(t * u.day, y, dy=0.01)
>>> periodogram = model.autopower(0.2)
>>> max_power = np.argmax(periodogram.power)
>>> stats = model.compute_stats(periodogram.period[max_power],
...                             periodogram.duration[max_power],
...                             periodogram.transit_time[max_power])

This calculates a dictionary with statistics about this candidate.
Each entry in this dictionary is described in the documentation for
`~astropy.timeseries.BoxLeastSquares.compute_stats`.


Literature References
=====================

.. [1] Kovacs, Zucker, & Mazeh (2002), A&A, 391, 369 (arXiv:astro-ph/0206099)
.. [2] Hartman & Bakos (2016), Astronomy & Computing, 17, 1 (arXiv:1605.06811)
.. _timeseries-pandas:

Interfacing with the Pandas Package
***********************************

The `astropy.timeseries` package is not the only package to provide
functionality related to time series. Another notable package is `pandas
<https://pandas.pydata.org/>`_, which provides a :class:`pandas.DataFrame`
class. The main benefits of `astropy.timeseries` in the context of astronomical
research are the following:

* The time column is a |Time| object that supports very high precision
  representation of times, and makes it easy to convert between different
  time scales and formats (e.g., ISO 8601 timestamps, Julian Dates, and so on).
* The data columns can include |Quantity| objects with units.
* The |BinnedTimeSeries| class includes variable-width time bins.
* There are built-in readers for common time series file formats, as well as
  the ability to define custom readers/writers.

Nevertheless, there are cases where using pandas :class:`~pandas.DataFrame`
objects might make sense, so we provide methods to convert to/from
:class:`~pandas.DataFrame` objects.

Example
-------

.. EXAMPLE START: Interfacing between Time Series and the Pandas DataFrame

Consider a concise example starting from a :class:`~pandas.DataFrame`:

.. doctest-requires:: pandas

    >>> import pandas
    >>> import numpy as np
    >>> df = pandas.DataFrame()
    >>> df['a'] = [1, 2, 3]
    >>> times = np.array(['2015-07-04', '2015-07-05', '2015-07-06'], dtype=np.datetime64)
    >>> df.set_index(pandas.DatetimeIndex(times), inplace=True)
    >>> df
        a
    2015-07-04  1
    2015-07-05  2
    2015-07-06  3

We can convert this to an ``astropy`` |TimeSeries| using
:meth:`~astropy.timeseries.TimeSeries.from_pandas`:

.. doctest-requires:: pandas

    >>> from astropy.timeseries import TimeSeries
    >>> ts = TimeSeries.from_pandas(df)
    >>> ts
    <TimeSeries length=3>
                 time               a
                 Time             int64
    ----------------------------- -----
    2015-07-04T00:00:00.000000000     1
    2015-07-05T00:00:00.000000000     2
    2015-07-06T00:00:00.000000000     3

Converting to :class:`~pandas.DataFrame` can also be done with
:meth:`~astropy.timeseries.TimeSeries.to_pandas`:

.. doctest-requires:: pandas

    >>> ts['b'] = [1.2, 3.4, 5.4]
    >>> df_new = ts.to_pandas()
    >>> df_new
                a    b
    time
    2015-07-04  1  1.2
    2015-07-05  2  3.4
    2015-07-06  3  5.4

Missing values in the time column are supported and correctly converted to a
pandas' NaT object:

.. doctest-requires:: pandas

    >>> ts.time[2] = np.nan
    >>> ts
    <TimeSeries length=3>
                 time               a      b
                 Time             int64 float64
    ----------------------------- ----- -------
    2015-07-04T00:00:00.000000000     1     1.2
    2015-07-05T00:00:00.000000000     2     3.4
                               --     3     5.4
    >>> df_missing = ts.to_pandas()
    >>> df_missing
               a    b
    time
    2015-07-04  1  1.2
    2015-07-05  2  3.4
    NaT         3  5.4

.. EXAMPLE END
.. _stats-lombscargle:

*************************
Lomb-Scargle Periodograms
*************************

The Lomb-Scargle periodogram (after Lomb [1]_, and Scargle [2]_) is a commonly
used statistical tool designed to detect periodic signals in unevenly spaced
observations. The :class:`~astropy.timeseries.LombScargle` class is a unified
interface to several implementations of the Lomb-Scargle periodogram, including
a fast *O[NlogN]* implementation following the algorithm presented by Press &
Rybicki [3]_.

The code here is adapted from the `astroml`_ package ([4]_, [5]_) and the
`gatspy`_ package ([6]_, [7]_).  For a detailed practical discussion of the
Lomb-Scargle periodogram, with code examples based on ``astropy``, see
*Understanding the Lomb-Scargle Periodogram* [11]_, with associated code at
https://github.com/jakevdp/PracticalLombScargle/.

.. _gatspy: https://www.astroml.org/gatspy/
.. _astroml: https://www.astroml.org/

Basic Usage
===========

.. Note::
   All frequencies in :class:`~astropy.timeseries.LombScargle` are **not**
   angular frequencies, but rather frequencies of oscillation (i.e., number of
   cycles per unit time).

The Lomb-Scargle periodogram is designed to detect periodic signals in
unevenly spaced observations.

Example
-------

.. EXAMPLE START: Using the Lomb-Scargle Periodogram to Detect Periodic Signals

To detect periodic signals in unevenly spaced observations, consider the
following data:

>>> import numpy as np
>>> rand = np.random.default_rng(42)
>>> t = 100 * rand.random(100)
>>> y = np.sin(2 * np.pi * t) + 0.1 * rand.standard_normal(100)

These are 100 noisy measurements taken at irregular times, with a frequency
of 1 cycle per unit time.

The Lomb-Scargle periodogram, evaluated at frequencies chosen
automatically based on the input data, can be computed as follows
using the :class:`~astropy.timeseries.LombScargle` class:

>>> from astropy.timeseries import LombScargle
>>> frequency, power = LombScargle(t, y).autopower()

Plotting the result with Matplotlib gives:

>>> import matplotlib.pyplot as plt  # doctest: +SKIP
>>> plt.plot(frequency, power)       # doctest: +SKIP

.. plot::

    from astropy.timeseries import LombScargle

    import numpy as np
    import matplotlib.pyplot as plt

    rand = np.random.default_rng(42)
    t = 100 * rand.random(100)
    y = np.sin(2 * np.pi * t) + 0.1 * rand.standard_normal(100)

    frequency, power = LombScargle(t, y).autopower()
    fig = plt.figure(figsize=(6, 4.5))
    plt.plot(frequency, power)

The periodogram shows a clear spike at a frequency of 1 cycle per unit time,
as we would expect from the data we constructed.

.. EXAMPLE END

Measurement Uncertainties
-------------------------

The :class:`~astropy.timeseries.LombScargle` interface can also handle data with
measurement uncertainties.

Example
^^^^^^^

.. EXAMPLE START: Using the Lomb-Scargle Periodogram with Measurement Uncertainties

If all uncertainties are the same, you can pass a scalar:

>>> dy = 0.1
>>> frequency, power = LombScargle(t, y, dy).autopower()

If uncertainties vary from observation to observation, you can pass them as
an array:

>>> dy = 0.1 * (1 + rand.random(100))
>>> y = np.sin(2 * np.pi * t) + dy * rand.standard_normal(100)
>>> frequency, power = LombScargle(t, y, dy).autopower()

Gaussian uncertainties are assumed, and ``dy`` here specifies the standard
deviation (not the variance).

.. EXAMPLE END

Periodograms and Units
----------------------

The :class:`~astropy.timeseries.LombScargle` interface properly handles
:class:`~astropy.units.Quantity` objects with units attached,
and will validate the inputs to make sure units are appropriate.

Example
^^^^^^^

.. EXAMPLE START: Using the LombScargle Class with Quantity Objects

To use the :class:`~astropy.timeseries.LombScargle` for
:class:`~astropy.units.Quantity` objects with units attached:

>>> import astropy.units as u
>>> t_days = t * u.day
>>> y_mags = y * u.mag
>>> dy_mags = y * u.mag
>>> frequency, power = LombScargle(t_days, y_mags, dy_mags).autopower()
>>> frequency.unit
Unit("1 / d")
>>> power.unit
Unit(dimensionless)

We see that the output is dimensionless, which is always the case for the
standard normalized periodogram (for more on normalizations,
see :ref:`lomb-scargle-normalization` below). If you include arguments to
autopower such as ``minimum_frequency`` or ``maximum_frequency``, make sure to
specify units as well:

>>> frequency, power = LombScargle(t_days, y_mags, dy_mags).autopower(minimum_frequency=1e-5*u.Hz)

.. EXAMPLE END

Specifying the Frequency
------------------------

With the :func:`~astropy.timeseries.LombScargle.autopower` method used above, a
heuristic is applied to select a suitable frequency grid. By default, the
heuristic assumes that the width of peaks is inversely proportional to the
observation baseline, and that the maximum frequency is a factor of five larger
than the so-called "average Nyquist frequency," with computation based on the
average observation spacing.

This heuristic is not universally useful, as the frequencies probed by
irregularly sampled data can be much higher than the average Nyquist frequency.
For this reason, the heuristic can be tuned through keywords passed to the
:func:`~astropy.timeseries.LombScargle.autopower` method.

Example
^^^^^^^

.. EXAMPLE START: Specifying the Frequency with the LombScargle.autopower method

To tune the heuristic using keywords passed to the
:func:`~astropy.timeseries.LombScargle.autopower` method:

>>> frequency, power = LombScargle(t, y, dy).autopower(nyquist_factor=2)
>>> len(frequency), frequency.min(), frequency.max()  # doctest: +FLOAT_CMP
(500, 0.0010327803641893758, 1.0317475838251864)

Here the highest frequency is two times the average Nyquist frequency.
If we increase the ``nyquist_factor``, we can probe higher frequencies:

>>> frequency, power = LombScargle(t, y, dy).autopower(nyquist_factor=10)
>>> len(frequency), frequency.min(), frequency.max()  # doctest: +FLOAT_CMP
(2500, 0.0010327803641893758, 5.16286904058269)

Alternatively, we can use the :func:`~astropy.timeseries.LombScargle.power`
method to evaluate the periodogram at a user-specified set of frequencies:

>>> frequency = np.linspace(0.5, 1.5, 1000)
>>> power = LombScargle(t, y, dy).power(frequency)

Note that the fastest Lomb-Scargle implementation requires regularly spaced
frequencies; if frequencies are irregularly spaced, a slower method will be
used instead.

.. EXAMPLE END

Frequency Grid Spacing
^^^^^^^^^^^^^^^^^^^^^^

One common issue with user-specified frequencies is inadvertently choosing
too coarse a grid, such that significant peaks lie between grid points and
are missed entirely.

Example
"""""""

.. EXAMPLE START: Frequency Grid Spacing in Periodograms

Imagine you chose to evaluate your periodogram at 100 points:

>>> frequency = np.linspace(0.1, 1.9, 100)
>>> power = LombScargle(t, y, dy).power(frequency)
>>> plt.plot(frequency, power)   # doctest: +SKIP

.. plot::

    import numpy as np
    import matplotlib.pyplot as plt
    from astropy.timeseries import LombScargle

    rand = np.random.default_rng(42)
    t = 100 * rand.random(100)
    dy = 0.1
    y = np.sin(2 * np.pi * t) + dy * rand.standard_normal(100)

    frequency = np.linspace(0.1, 1.9, 100)
    power = LombScargle(t, y, dy).power(frequency)

    plt.figure(figsize=(6, 4.5))
    plt.plot(frequency, power)
    plt.xlabel('frequency')
    plt.ylabel('Lomb-Scargle Power')
    plt.ylim(0, 1)

From this plot alone, you might conclude that no clear periodic signal exists in
the data.  But this conclusion is in error: there is in fact a strong periodic
signal, but the periodogram peak falls in the gap between the chosen grid
points!

A more reliable approach is to use the frequency heuristic to decide on the
appropriate grid spacing, optionally passing a minimum and maximum frequency to
the :func:`~astropy.timeseries.LombScargle.autopower` method:

>>> frequency, power = LombScargle(t, y, dy).autopower(minimum_frequency=0.1,
...                                                    maximum_frequency=1.9)
>>> len(frequency)
872
>>> plt.plot(frequency, power)   # doctest: +SKIP

.. plot::

    import numpy as np
    import matplotlib.pyplot as plt
    from astropy.timeseries import LombScargle

    rand = np.random.default_rng(42)
    t = 100 * rand.random(100)
    dy = 0.1
    y = np.sin(2 * np.pi * t) + dy * rand.standard_normal(100)

    frequency, power = LombScargle(t, y, dy).autopower(minimum_frequency=0.1,
                                                       maximum_frequency=1.9)

    plt.figure(figsize=(6, 4.5))
    plt.plot(frequency, power)
    plt.xlabel('frequency')
    plt.ylabel('Lomb-Scargle Power')
    plt.ylim(0, 1)

With a finer grid (here 884 points between 0.1 and 1.9),
it is clear that there is a very strong periodic signal in the data.

.. EXAMPLE END

By default, the heuristic aims to have roughly five grid points across each
significant periodogram peak; this can be increased by changing the
``samples_per_peak`` argument:

>>> frequency, power = LombScargle(t, y, dy).autopower(minimum_frequency=0.1,
...                                                    maximum_frequency=1.9,
...                                                    samples_per_peak=10)
>>> len(frequency)
1744

Keep in mind that the width of the peak scales inversely with the baseline of
the observations (i.e., the difference between the maximum and minimum time),
and the required number of grid points will scale linearly with the size of
the baseline.

The Lomb-Scargle Model
----------------------

The Lomb-Scargle periodogram fits a sinusoidal model to the data at each
frequency, with a larger power reflecting a better fit. With this in mind, it is
often helpful to plot the best-fit sinusoid over the phased data.

Example
^^^^^^^

.. EXAMPLE START: Computing a Best-Fit Sinusoid Using the LombScargle Class

This best-fit sinusoid can be computed using the
:func:`~astropy.timeseries.LombScargle.model` method of the
:class:`~astropy.timeseries.LombScargle` object:

>>> best_frequency = frequency[np.argmax(power)]
>>> t_fit = np.linspace(0, 1)
>>> ls = LombScargle(t, y, dy)
>>> y_fit = ls.model(t_fit, best_frequency)

We can then phase the data and plot the Lomb-Scargle model fit:

.. plot::

    import numpy as np
    import matplotlib.pyplot as plt

    from astropy.timeseries import LombScargle

    rand = np.random.default_rng(42)
    t = 100 * rand.random(100)
    dy = 0.1
    y = np.sin(2 * np.pi * t) + dy * rand.standard_normal(100)

    frequency, power = LombScargle(t, y, dy).autopower(minimum_frequency=0.1,
                                                       maximum_frequency=1.9)
    best_frequency = frequency[np.argmax(power)]
    phase_fit = np.linspace(0, 1)
    y_fit = LombScargle(t, y, dy).model(t=phase_fit / best_frequency,
                                        frequency=best_frequency)
    phase = (t * best_frequency) % 1

    fig, ax = plt.subplots(figsize=(6, 4.5))
    ax.errorbar(phase, y, dy, fmt='o', mew=0, capsize=0, elinewidth=1.5)
    ax.plot(phase_fit, y_fit, color='black')
    ax.invert_yaxis()
    ax.set(xlabel='phase',
           ylabel='magnitude',
           title=f'phased data at frequency={best_frequency:.2f}')

The best-fit model parameters can be computed with the
:func:`~astropy.timeseries.LombScargle.model_parameters` method of the
:class:`~astropy.timeseries.LombScargle` object at a given frequency:

>>> theta = ls.model_parameters(best_frequency)
>>> theta.round(2)
array([-0.01,  0.99,  0.11])

These parameters :math:`\vec{\theta}` are fit using the following model:

.. math::

    y(t; f, \vec{\theta}) = \theta_0 + \sum_{n=1}^{\tt nterms} [\theta_{2n-1}\sin(2\pi n f t) + \theta_{2n}\cos(2\pi n f t)]

The model can be constructed from these parameters by computing the associated
:func:`~astropy.timeseries.LombScargle.offset`, which accounts for the
pre-centering of data (i.e., the ``center_data`` argument), and
:func:`~astropy.timeseries.LombScargle.design_matrix`, which computes the sine
and cosine terms for you:

>>> offset = ls.offset()
>>> design_matrix = ls.design_matrix(best_frequency, t_fit)
>>> np.allclose(y_fit, offset + design_matrix.dot(theta))
True

.. EXAMPLE END

Additional Arguments
--------------------

On initialization, :class:`~astropy.timeseries.LombScargle` takes a few
additional arguments which control the model for the data:

- ``center_data`` (``True`` by default) controls whether the ``y`` values are
  pre-centered before the algorithm fits the data.  The only time it is really
  warranted to change the default is if you are computing the periodogram of a
  sequence of constant values to, for example, estimate the window power
  spectrum for a series of observations.
- ``fit_mean`` (``True`` by default) controls whether the model fits for the
  mean of the data, rather than assuming the mean is zero. When
  ``fit_mean=True``, the periodogram is more robust than the original
  Lomb-Scargle formalism, particularly in the case of smaller sample sizes
  and/or data with nontrivial selection bias. In the literature, this model has
  variously been called the *date-compensated discrete Fourier transform*, the
  *floating-mean periodogram*, the *generalized Lomb-Scargle method*, and likely
  other names as well.
- ``nterms`` (``1`` by default) controls how many Fourier terms are used in the
  model. As seen above, the standard Lomb-Scargle periodogram is equivalent to
  a single-term sinusoidal fit to the data at each frequency; the
  generalization is to expand this to a truncated Fourier series with multiple
  frequencies. While this can be very useful in some cases, in others the
  additional model complexity can lead to spurious periodogram peaks that
  outweigh the benefit of the more flexible model.

.. _lomb-scargle-normalization:

Periodogram Normalizations
==========================

There are several normalizations of the Lomb-Scargle periodogram found in the
literature. :class:`~astropy.timeseries.LombScargle` makes four options
available via the ``normalization`` argument: ``normalization='standard'`` (the
default), ``normalization='model'``, ``normalization='log'``, and
``normalization='psd'``. These normalizations can be thought of in terms of
least-squares fits around a constant reference model :math:`M_{ref}` and a
periodic model :math:`M(f)` at each frequency, with best-fit sum of residuals
that we will denote by :math:`\chi^2_{ref}` and :math:`\chi^2(f)` respectively.

Standard Normalization
----------------------

The default, the standard normalized periodogram is normalized by the residuals
of the data around the constant reference model:

.. math::

   P_{standard}(f) = \frac{\chi^2_{ref} - \chi^2(f)}{\chi^2_{ref}}

This form of the normalization (``normalization='standard'``) is the default
choice used in :class:`~astropy.timeseries.LombScargle`. The resulting power
*P* is a dimensionless quantity that lies in the range *0  P  1*.

Model Normalization
-------------------

Alternatively, the periodogram is sometimes normalized instead by the residuals
around the periodic model:

.. math::

   P_{model}(f) = \frac{\chi^2_{ref} - \chi^2(f)}{\chi^2(f)}

This form of the normalization can be specified with ``normalization='model'``.
As above, the resulting power is a dimensionless quantity that lies in the
range *0  P  *.

Logarithmic Normalization
-------------------------

Another form of normalization is to scale the periodogram logarithmically:

.. math::

   P_{log}(f) = \log \frac{\chi^2_{ref}}{\chi^2(f)}

This normalization can be specified with ``normalization='log'``, and the
resulting power is a dimensionless quantity in the range *0  P  *.

PSD Normalization (Unnormalized)
--------------------------------

Finally, it is sometimes useful to compute an unnormalized periodogram
(``normalization='psd'``):

.. math::

   P_{psd}(f) = \frac{1}{2}\left(\chi^2_{ref} - \chi^2(f)\right)

Which, in the case of no-uncertainty, will have units ``y.unit ** 2``.
This normalization is constructed to be comparable to the standard Fourier
power spectral density (PSD):

>>> ls = LombScargle(t_days, y_mags, normalization='psd')
>>> frequency, power = ls.autopower()
>>> power.unit
Unit("mag2")

Note, however, that the ``normalization='psd'`` result only has these units
*if uncertainties are not specified*. In the presence of uncertainties,
even the unnormalized PSD periodogram will be dimensionless; this is due to
the scaling of data by uncertainty within the Lomb-Scargle computation:

>>> # with uncertainties, PSD power is unitless
>>> ls = LombScargle(t_days, y_mags, dy_mags, normalization='psd')
>>> frequency, power = ls.autopower()
>>> power.unit
Unit(dimensionless)

The equivalence of the PSD-normalized periodogram and the Fourier PSD
in the unnormalized, no-uncertainty case can be confirmed by comparing
results directly for uniformly sampled inputs.

We will first define a convenience function to compute the basic
Fourier periodogram for uniformly sampled quantities:

>>> def fourier_periodogram(t, y):
...     N = len(t)
...     frequency = np.fft.fftfreq(N, t[1] - t[0])
...     y_fft = np.fft.fft(y.value) * y.unit
...     positive = (frequency > 0)
...     return frequency[positive], (1. / N) * abs(y_fft[positive]) ** 2

Next we compute the two versions of the PSD from uniformly sampled data:

>>> t_days = np.arange(100) * u.day
>>> y_mags = rand.standard_normal(100) * u.mag
>>> frequency, PSD_fourier = fourier_periodogram(t_days, y_mags)
>>> ls = LombScargle(t_days, y_mags, normalization='psd')
>>> PSD_LS = ls.power(frequency)

Examining the results, we see that the two outputs match:

>>> u.allclose(PSD_fourier, PSD_LS)
True

This equivalence is one reason that the Lomb-Scargle periodogram is considered
to be an extension of the Fourier PSD.

For more information on the statistical properties of these normalizations,
see, for example, Baluev 2008 [8]_.

Peak Significance and False Alarm Probabilities
===============================================

.. Note::
   Interpretation of Lomb-Scargle peak significance via false alarm
   probabilities is a subtle subject, and the quantities computed below are
   commonly misinterpreted or misused. For a detailed discussion of periodogram
   peak significance, see [11]_.

When using the Lomb-Scargle periodogram to decide whether a signal contains a
periodic component, an important consideration is the significance of the
periodogram peak. This significance is usually expressed in terms of a
false alarm probability, which encodes the probability of measuring a
peak of a given height (or higher) conditioned on the assumption that
the data consists of Gaussian noise with no periodic component.

Example
-------

.. EXAMPLE START: Lomb-Scargle Peak Significance via False Alarm Probabilities

To use the Lomb-Scargle periodogram to decide if our signal contains a periodic
component, we can start by simulating 60 observations of a sine wave with noise:

>>> t = 100 * rand.random(60)
>>> dy = 1.0
>>> y = np.sin(2 * np.pi * t) + dy * rand.standard_normal(60)
>>> ls = LombScargle(t, y, dy)
>>> freq, power = ls.autopower()
>>> print(power.max())  # doctest: +FLOAT_CMP
0.29154492887882927

The peak of the periodogram has a value of 0.33, but how significant is
this peak? We can address this question using the
:func:`~astropy.timeseries.LombScargle.false_alarm_probability` method:

.. doctest-requires:: scipy

  >>> ls.false_alarm_probability(power.max())  # doctest: +FLOAT_CMP
  0.028959671719328808

What this tells us is that under the assumption that there is no periodic
signal in the data, we will observe a peak this high or higher approximately
0.4% of the time, which gives a strong indication that a periodic signal is
present in the data.

.. Note::
  Users must interpret this probability carefully: it is a measurement
  conditioned on the assumption of the null hypothesis of no signal; in symbols,
  you might write :math:`P({\rm data} \mid {\rm noise-only})`.

  Although it may seem like this quantity could be interpreted with a statement
  such as "there is an 0.4% chance that this data is noise only," this is *not*
  a correct statement; in symbols, this statement describes the quantity
  :math:`P({\rm noise-only} \mid {\rm data})`, and in general :math:`P(A\mid B)
  \ne P(B\mid A)`.

  See [11]_ for a more detailed discussion of such caveats.

We might also wish to compute the required peak height to attain any given
false alarm probability, which can be done with the
:func:`~astropy.timeseries.LombScargle.false_alarm_level` method:

.. doctest-requires:: scipy

  >>> probabilities = [0.1, 0.05, 0.01]
  >>> ls.false_alarm_level(probabilities)  # doctest: +FLOAT_CMP
  array([0.25681381, 0.27663466, 0.31928202])

This tells us that to attain a 10% false alarm probability requires the highest
periodogram peak to be approximately 0.25; 5% requires 0.27, and 1% requires
0.32.

.. EXAMPLE END

False Alarm Approximations
--------------------------

Although the false alarm probability at any particular frequency is analytically
computable, there is no closed-form analytic expression for the more relevant
quantity of the false alarm level of the *highest* peak in a particular
periodogram. This must be either determined through bootstrap simulations, or
approximated by various means.

``astropy`` provides four options for approximating the false alarm probability,
which can be chosen using the ``method`` keyword:

- ``method="baluev"`` (the default) implements the approximation proposed by
  Baluev 2008 [8]_, which employs extreme value statistics to compute an upper
  bound of the false alarm probability for the alias-free case. Experiments show
  that the bound is also useful even for highly aliased observing patterns.

.. doctest-requires:: scipy

    >>> ls.false_alarm_probability(power.max(), method='baluev')  # doctest: +FLOAT_CMP
    0.028959671719328808

- ``method="bootstrap"`` implements a bootstrap simulation: effectively it
  computes many Lomb-Scargle periodograms on simulated data at the same
  observation times. The bootstrap approach can very accurately determine
  the false alarm probability, but is very computationally expensive.
  To estimate the level corresponding to a false alarm probability
  :math:`P_{false}`, it requires on order :math:`n_{boot} \approx 10/P_{false}`
  individual periodograms to be computed for the dataset.

.. doctest-requires:: scipy

    >>> ls.false_alarm_probability(power.max(), method='bootstrap')  # doctest: +SKIP
    0.0030000000000000027

- ``method="davies"`` is related to the Baluev method, but loses accuracy
  at large false alarm probabilities.

.. doctest-requires:: scipy

    >>> ls.false_alarm_probability(power.max(), method='davies')  # doctest: +FLOAT_CMP
    0.029387277355227746

- ``method="naive"`` is a basic method based on the assumption that
  well-separated areas in the periodogram are independent. In general, it
  provides a very poor estimate of the false alarm probability and should
  not be used in practice, but is included for completeness.

.. doctest-requires:: scipy

    >>> ls.false_alarm_probability(power.max(), method='naive')  # doctest: +FLOAT_CMP
    0.00810080828660202

The following figure compares these false alarm estimates at a range of
peak heights for 100 observations with a heavily aliased observing pattern:

.. plot::

    import numpy as np
    import matplotlib.pyplot as plt

    from astropy.timeseries import LombScargle

    rng = np.random.default_rng(42)

    N = 100
    t = 5 * rng.random(N)
    t -= 0.5 * (t % 1)  # create alias-inducing structure in the window function
    dy = 0.5 * (1 + rng.random(N))
    y = dy * rng.standard_normal(N)

    ls = LombScargle(t, y, dy, normalization='standard')
    z = np.linspace(1E-3, 0.15, 1000)

    def false_alarm(method):
        return ls.false_alarm_probability(z, method=method, maximum_frequency=5)

    fa_boot = ls.false_alarm_probability(z, method='bootstrap',
                                         maximum_frequency=5,
                                         method_kwds=dict(random_seed=42))

    fig, ax = plt.subplots(figsize=(6, 4.5))

    ax.plot(z, false_alarm('naive'), label='naive estimate')
    ax.plot(z, false_alarm('baluev'), label='Baluev estimate')
    ax.plot(z, false_alarm('davies'), ':k', label='Davies bound')
    ax.plot(z, fa_boot, '-k', label='bootstrap estimate')

    ax.legend(loc='lower left')
    ax.set(yscale='log',
           title='False Alarm Estimates (N=100)',
           xlim=(0, 0.15), ylim=(0.01, 1.5),
           xlabel='Value of Highest Periodogram Peak',
           ylabel='False Alarm Probability');

In general, users should use the bootstrap approach when computationally
feasible, and the Baluev approach otherwise.

In all of this, it is important to keep in mind a few caveats:

- False alarm probabilities are computed relative to a particular set of
  observing times, and a particular choice of frequency grid.
- False alarm probabilities are conditioned upon the null hypothesis of
  data with no periodic component, and in particular say nothing
  quantitative about whether the data are actually consistent with a
  periodic model.
- False alarm probabilities are not related to the question of whether the
  highest peak in a periodogram is the *correct* peak, and in particular
  are not especially useful in the case of observations with a strong
  aliasing pattern.

For a detailed discussion of these caveats and others when computing and
interpreting false alarm probabilities, please refer to [11]_.

Periodogram Algorithms
======================

The :class:`~astropy.timeseries.LombScargle` class makes available
several complementary implementations of the Lomb-Scargle periodogram,
which can be selected using the ``method`` keyword of the Lomb-Scargle power.
By design all methods will return the same results (some approximate),
and each has its advantages and disadvantages.

For example, to compute a periodogram using the Fast Chi-squared method
of Palmer (2009) [9]_, you can specify ``method='fastchi2'``:

    >>> frequency, power = LombScargle(t, y).autopower(method='fastchi2')

There are currently six methods available in the package:

``method='auto'``
-----------------

The ``auto`` method is the default, and will attempt to select the best option
from the following methods using heuristics driven by the input data.

``method='slow'``
-----------------

The ``slow`` method is a pure Python implementation of the original Lomb-Scargle
periodogram ([1]_, [2]_), enhanced to account for observational noise,
and to allow a floating mean (sometimes called the *generalized periodogram*;
see [10]_). The method is not particularly fast, scaling approximately
as :math:`O[NM]` for :math:`N` data points and :math:`M` frequencies.

``method='cython'``
-------------------

The ``cython`` method is a Cython implementation of the same algorithm used for
``method='slow'``. It is slightly faster than the pure Python implementation,
but much more memory-efficient as the size of the inputs grow. The computational
scaling is approximately :math:`O[NM]` for :math:`N` data points and
:math:`M` frequencies.

``method='scipy'``
------------------

The ``scipy`` method wraps the C implementation of the original Lomb-Scargle
periodogram which is available in :func:`scipy.signal.lombscargle`. This is
slightly faster than the ``slow`` method, but does not allow for errors in
data or extensions such as the floating mean. The scaling is approximately
:math:`O[NM]` for :math:`N` data points and :math:`M` frequencies.

``method='fast'``
-----------------

The ``fast`` method is a pure Python implementation of the fast periodogram of
Press & Rybicki [3]_. It uses an *extrapolation* approach to approximate the
periodogram frequencies using a fast Fourier transform. As with the ``slow``
method, it can handle data errors and floating mean.  The scaling is
approximately :math:`O[N\log M]` for :math:`N` data points and :math:`M`
frequencies. The fast algorithm trades accuracy for speed, and produces a close
approximation to the true periodogram. In particular, you may observe powers
less than zero in some cases.

``method='chi2'``
-----------------

The ``chi2`` method is a pure Python implementation based on matrix algebra
(see [7]_). It utilizes the fact that the Lomb-Scargle periodogram at
each frequency is equivalent to the least-squares fit of a sinusoid to the
data. The advantage of the ``chi2`` method is that it allows extensions of
the periodogram to multiple Fourier terms, specified by the ``nterms``
parameter. For the standard problem, it is slightly slower than
``method='slow'`` and scales as :math:`O[n_fNM]` for :math:`N` data points,
:math:`M` frequencies, and :math:`n_f` Fourier terms.

``method='fastchi2'``
---------------------

The Fast Chi-squared method of Palmer (2009) [9]_ is equivalent to the ``chi2``
method, but the matrices are constructed using an FFT-based approach similar to
that of the ``fast`` method. The result is a relatively efficient periodogram
(though not nearly as efficient as the ``fast`` method) which can be extended to
multiple terms. The scaling is approximately :math:`O[n_f(M + N\log M)]` for
:math:`N` data points, :math:`M` frequencies, and :math:`n_f` Fourier terms.

Summary
-------

The following table summarizes the features of the above algorithms:

==============  ============================  =============  ===============  ========
Method          Computational                 Observational  Bias Term        Multiple
                Scaling                       Uncertainties  (Floating Mean)  Terms
==============  ============================  =============  ===============  ========
``"slow"``      :math:`O[NM]`                 Yes            Yes              No
``"cython"``    :math:`O[NM]`                 Yes            Yes              No
``"scipy"``     :math:`O[NM]`                 No             No               No
``"fast"``      :math:`O[N\log M]`            Yes            Yes              No
``"chi2"``      :math:`O[n_fNM]`              Yes            Yes              Yes
``"fastchi2"``  :math:`O[n_f(M + N\log M)]`   Yes            Yes              Yes
==============  ============================  =============  ===============  ========

In the Computational Scaling column, :math:`N` is the number of data points,
:math:`M` is the number of frequencies, and :math:`n_f` is the number of
Fourier terms for a multi-term fit.

.. _lomb-scargle-example:

RR Lyrae Example
================

.. EXAMPLE START: Computing a Periodogram for RR Lyrae Data

An example of computing the periodogram for a more realistic dataset is shown in
the following figure. The data here consists of 50 nightly observations of a
simulated RR Lyrae-like variable star, with a lightcurve shape that is more
complicated than a simple sine wave:

.. plot::

    import numpy as np
    import matplotlib.pyplot as plt

    from astropy.timeseries import LombScargle


    def simulated_data(N, rseed=2, period=0.41, phase=0.0):
        """Simulate data based from a pre-computed empirical fit"""

        # coefficients from a 5-term Fourier fit to SDSS object 1019544
        coeffs = [-0.0191, 0.1375, -0.1968, 0.0959, 0.075,
                  -0.0686, 0.0307, -0.0045, -0.0421, 0.0216, 0.0041]

        rand = np.random.default_rng(rseed)
        t = phase + np.arange(N, dtype=float)
        t += 0.1 * rand.standard_normal(N)
        dmag = 0.01 + 0.03 * rand.random(N)

        omega = 2 * np.pi / period
        n = np.arange(1 + len(coeffs) // 2)[:, None]

        mag = (15 + dmag * rand.standard_normal(N)
               + np.dot(coeffs[::2], np.cos(n * omega * t)) +
               + np.dot(coeffs[1::2], np.sin(n[1:] * omega * t)))

        return t, mag, dmag


    # generate data and compute the periodogram
    t, mag, dmag = simulated_data(50)
    ls = LombScargle(t, mag, dmag, normalization='standard')
    freq, PLS = ls.autopower(minimum_frequency=1 / 1.2,
                             maximum_frequency=1 / 0.2)
    best_freq = freq[np.argmax(PLS)]
    phase = (t * best_freq) % 1

    # compute the best-fit model
    phase_fit = np.linspace(0, 1)
    mag_fit = ls.model(t=phase_fit / best_freq,
                       frequency=best_freq)

    # set up the figure & axes for plotting
    fig, ax = plt.subplots(1, 2, figsize=(12, 5))
    fig.suptitle('Lomb-Scargle Periodogram (period=0.41 days)')
    fig.subplots_adjust(bottom=0.12, left=0.07, right=0.95)
    inset = fig.add_axes([0.78, 0.56, 0.15, 0.3])

    # plot the raw data
    ax[0].errorbar(t, mag, dmag, fmt='ok', elinewidth=1.5, capsize=0)
    ax[0].invert_yaxis()
    ax[0].set(xlim=(0, 50),
              xlabel='Observation time (days)',
              ylabel='Observed Magnitude')

    # plot the periodogram
    ax[1].plot(1. / freq, PLS)
    ax[1].set(xlabel='period (days)',
              ylabel='Lomb-Scargle Power',
              xlim=(0.2, 1.2),
              ylim=(0, 1));

    # plot the false-alarm levels
    z_false = ls.false_alarm_level(0.01, maximum_frequency=1 / 0.2,
                                   method='baluev')
    ax[1].axhline(z_false, linestyle='dotted', color='black')

    # plot the phased data & model in the inset
    inset.errorbar(phase, mag, dmag, fmt='.k', capsize=0)
    inset.plot(phase_fit, mag_fit)
    inset.invert_yaxis()
    inset.set_xlabel('phase')
    inset.set_ylabel('mag')


The dotted line shows the periodogram level corresponding to a maximum peak
false alarm probability of 1%. This example demonstrates that for irregularly
sampled data, the Lomb-Scargle periodogram can be sensitive to frequencies
higher than the average Nyquist frequency: the above data are sampled at an
average rate of roughly one observation per night, and the periodogram
relatively cleanly reveals the true period of 0.41 days.

Still, the periodogram has many spurious peaks, which are due to several
factors:

1. Errors in observations lead to leakage of power from the true peaks.
2. The signal is not a perfect sinusoid, so additional peaks can indicate
   higher frequency components in the signal.
3. The observations take place only at night, meaning that the survey window has
   non-negligible power at a frequency of 1 cycle per day.  Thus we expect
   aliases to appear at :math:`f_{\rm alias} = f_{\rm true} + n f_{\rm window}`
   for integer values of :math:`n`. With a true period of 0.41 days and a 1-day
   signal in the observing window, the :math:`n=+1` and :math:`n=-1` aliases to
   lie at periods of 0.29 and 0.69 days, respectively: these aliases are
   prominent in the above plot.

The interaction of these effects means that in practice there is no absolute
guarantee that the highest peak corresponds to the best frequency, and results
must be interpreted carefully.  For a detailed discussion of these effects, see
[11]_.

.. EXAMPLE END

Literature References
=====================

.. [1] Lomb, N.R. *Least-squares frequency analysis of unequally spaced data*.
       Ap&SS 39 pp. 447-462 (1976)
.. [2] Scargle, J. D. *Studies in astronomical time series analysis. II -
       Statistical aspects of spectral analysis of unevenly spaced data*.
       ApJ 1:263 pp. 835-853 (1982)
.. [3] Press W.H. and Rybicki, G.B, *Fast algorithm for spectral analysis
       of unevenly sampled data*. ApJ 1:338, p. 277 (1989)
.. [4] Vanderplas, J., Connolly, A. Ivezic, Z. & Gray, A. *Introduction to
       astroML: Machine learning for astrophysics*. Proceedings of the
       Conference on Intelligent Data Understanding (2012)
.. [5]  Vanderplas, J., Connolly, A. Ivezic, Z. & Gray, A. *Statistics,
	Data Mining and Machine Learning in Astronomy*. Princeton Press (2014)}
.. [6] VanderPlas, J. *Gatspy: General Tools for Astronomical Time Series
       in Python* (2015) https://zenodo.org/record/14833
.. [7] VanderPlas, J. & Ivezic, Z. *Periodograms for Multiband Astronomical
       Time Series*. ApJ 812.1:18 (2015)
.. [8] Baluev, R.V. *Assessing Statistical Significance of Periodogram Peaks*
       MNRAS 385, 1279 (2008)
.. [9] Palmer, D. *A Fast Chi-squared Technique for Period Search of
       Irregularly Sampled Data*. ApJ 695.1:496 (2009)
.. [10] Zechmeister, M. and Kurster, M. *The generalised Lomb-Scargle
       periodogram. A new formalism for the floating-mean and Keplerian
       periodograms*, A&A 496, 577-584 (2009)
.. [11] VanderPlas, J. *Understanding the Lomb-Scargle Periodogram*
	ApJS 236.1:16 (2018)
	https://ui.adsabs.harvard.edu/abs/2018ApJS..236...16V
.. _timeseries-analysis:

Manipulation and Analysis of Time Series
****************************************

Combining Time Series
=====================

The :func:`~astropy.table.vstack` and :func:`~astropy.table.hstack` functions
from the :mod:`astropy.table` module can be used to stack time series in
different ways.

Examples
--------

.. EXAMPLE START: Stacking Time Series Row-Wise Using table.vstack

Time series can be stacked "vertically" or row-wise using the
:func:`~astropy.table.vstack` function (although note that sampled time
series cannot be combined with binned time series and vice versa)::

    >>> from astropy.table import vstack
    >>> from astropy import units as u
    >>> from astropy.timeseries import TimeSeries
    >>> ts_a = TimeSeries(time_start='2016-03-22T12:30:31',
    ...                   time_delta=3 * u.s,
    ...                   data={'flux': [1, 4, 5, 3, 2] * u.mJy})
    >>> ts_b = TimeSeries(time_start='2016-03-22T12:50:31',
    ...                   time_delta=3 * u.s,
    ...                   data={'flux': [4, 3, 1, 2, 3] * u.mJy})
    >>> ts_ab = vstack([ts_a, ts_b])
    >>> ts_ab
    <TimeSeries length=10>
              time            flux
                              mJy
              Time          float64
    ----------------------- -------
    2016-03-22T12:30:31.000     1.0
    2016-03-22T12:30:34.000     4.0
    2016-03-22T12:30:37.000     5.0
    2016-03-22T12:30:40.000     3.0
    2016-03-22T12:30:43.000     2.0
    2016-03-22T12:50:31.000     4.0
    2016-03-22T12:50:34.000     3.0
    2016-03-22T12:50:37.000     1.0
    2016-03-22T12:50:40.000     2.0
    2016-03-22T12:50:43.000     3.0

Note that :func:`~astropy.table.vstack` does not automatically sort, nor get rid
of duplicates  this is something you would need to do explicitly afterwards.

.. EXAMPLE END

.. EXAMPLE START: Stacking Time Series Column-Wise Using table.vstack

Time series can also be combined "horizontally" or column-wise with other tables
using the :func:`~astropy.table.hstack` function, though these should not be
time series (as having multiple time columns would be confusing)::

    >>> from astropy.table import Table, hstack
    >>> data = Table(data={'temperature': [40., 41., 40., 39., 30.] * u.K})
    >>> ts_a_data = hstack([ts_a, data])
    >>> ts_a_data
    <TimeSeries length=5>
              time            flux  temperature
                              mJy          K
              Time          float64    float64
    ----------------------- ------- -----------
    2016-03-22T12:30:31.000     1.0        40.0
    2016-03-22T12:30:34.000     4.0        41.0
    2016-03-22T12:30:37.000     5.0        40.0
    2016-03-22T12:30:40.000     3.0        39.0
    2016-03-22T12:30:43.000     2.0        30.0

.. EXAMPLE END

Sorting Time Series
===================

.. EXAMPLE START: Sorting Time Series

Sorting time series in place can be done using the
:meth:`~astropy.table.Table.sort` method, as for |Table|::

    >>> ts = TimeSeries(time_start='2016-03-22T12:30:31',
    ...                 time_delta=3 * u.s,
    ...                 data={'flux': [1., 4., 5., 3., 2.]})
    >>> ts
    <TimeSeries length=5>
              time            flux
              Time          float64
    ----------------------- -------
    2016-03-22T12:30:31.000     1.0
    2016-03-22T12:30:34.000     4.0
    2016-03-22T12:30:37.000     5.0
    2016-03-22T12:30:40.000     3.0
    2016-03-22T12:30:43.000     2.0
    >>> ts.sort('flux')
    >>> ts
    <TimeSeries length=5>
              time            flux
              Time          float64
    ----------------------- -------
    2016-03-22T12:30:31.000     1.0
    2016-03-22T12:30:43.000     2.0
    2016-03-22T12:30:40.000     3.0
    2016-03-22T12:30:34.000     4.0
    2016-03-22T12:30:37.000     5.0

.. EXAMPLE END

Resampling
==========

We provide a :func:`~astropy.timeseries.aggregate_downsample` function
that can be used to bin values from a time series into equal-size or uneven bins,
and contiguous and non-contiguous bins, using a custom function (mean, median, etc.).
This operation returns a |BinnedTimeSeries|. Note that this is a basic function in
the sense that it does not, for example, know how to treat columns with uncertainties
differently from other values, and it will blindly apply the custom function
specified to all columns.

Example
-------

.. EXAMPLE START: Creating a BinnedTimeSeries with even contiguous bins

The following example shows how to use
:func:`~astropy.timeseries.aggregate_downsample` to bin a light curve from the
Kepler mission into 20 minute contiguous bins using a median function. First,
we read in the data using:

.. plot::
   :include-source:
   :context: reset
   :nofigs:

    from astropy.timeseries import TimeSeries
    from astropy.utils.data import get_pkg_data_filename
    example_data = get_pkg_data_filename('timeseries/kplr010666592-2009131110544_slc.fits')
    kepler = TimeSeries.read(example_data, format='kepler.fits')

(See :ref:`timeseries-io` for more details about reading in data). We can then
downsample using:

.. plot::
   :context:
   :nofigs:

   import warnings
   warnings.filterwarnings('ignore', message='All-NaN slice encountered')

.. plot::
   :include-source:
   :context:
   :nofigs:

    import numpy as np
    from astropy import units as u
    from astropy.timeseries import aggregate_downsample
    kepler_binned = aggregate_downsample(kepler, time_bin_size=20 * u.min, aggregate_func=np.nanmedian)

We can take a look at the results:

.. plot::
   :include-source:
   :context:

    import matplotlib.pyplot as plt
    plt.plot(kepler.time.jd, kepler['sap_flux'], 'k.', markersize=1)
    plt.plot(kepler_binned.time_bin_start.jd, kepler_binned['sap_flux'], 'r-', drawstyle='steps-pre')
    plt.xlabel('Julian Date')
    plt.ylabel('SAP Flux (e-/s)')

.. EXAMPLE END

.. EXAMPLE START: Creating a BinnedTimeSeries with uneven contiguous bins

The :func:`~astropy.timeseries.aggregate_downsample` can also be used
to bin the light curve into custom bins. The following example shows
the case of uneven-size contiguous bins:

.. plot::
   :context: reset
   :nofigs:

    import numpy as np
    from astropy import units as u
    import matplotlib.pyplot as plt
    from astropy.timeseries import TimeSeries
    from astropy.timeseries import aggregate_downsample
    from astropy.utils.data import get_pkg_data_filename

    example_data = get_pkg_data_filename('timeseries/kplr010666592-2009131110544_slc.fits')
    kepler = TimeSeries.read(example_data, format='kepler.fits')

    import warnings
    warnings.filterwarnings('ignore', message='All-NaN slice encountered')

.. plot::
   :include-source:
   :context:

    kepler_binned = aggregate_downsample(kepler, time_bin_size=[1000, 125, 80, 25, 150, 210, 273] * u.min,
                                         aggregate_func=np.nanmedian)

    plt.plot(kepler.time.jd, kepler['sap_flux'], 'k.', markersize=1)
    plt.plot(kepler_binned.time_bin_start.jd, kepler_binned['sap_flux'], 'r-', drawstyle='steps-pre')
    plt.xlabel('Julian Date')
    plt.ylabel('SAP Flux (e-/s)')

To learn more about the custom binning functionality in
:func:`~astropy.timeseries.aggregate_downsample`, see
:ref:`timeseries-binned-initializing`.

Folding
=======

.. EXAMPLE START: Phase Folding a Time Series

The |TimeSeries| class has a
:meth:`~astropy.timeseries.TimeSeries.fold` method that can be used to
return a new time series with a relative and folded time axis. This method
takes the period as a :class:`~astropy.units.Quantity`, and optionally takes
an epoch as a :class:`~astropy.time.Time`, which defines a zero time offset:

.. plot::
   :context: reset
   :nofigs:

   import numpy as np
   from astropy import units as u
   import matplotlib.pyplot as plt
   from astropy.timeseries import TimeSeries
   from astropy.utils.data import get_pkg_data_filename

   example_data = get_pkg_data_filename('timeseries/kplr010666592-2009131110544_slc.fits')
   kepler = TimeSeries.read(example_data, format='kepler.fits')

.. plot::
   :include-source:
   :context:

    kepler_folded = kepler.fold(period=2.2 * u.day, epoch_time='2009-05-02T20:53:40')

    plt.plot(kepler_folded.time.jd, kepler_folded['sap_flux'], 'k.', markersize=1)
    plt.xlabel('Time from midpoint epoch (days)')
    plt.ylabel('SAP Flux (e-/s)')

Note that in this example we happened to know the period and midpoint from a
previous periodogram analysis. See the example in :doc:`index` for how you
might do this.

.. EXAMPLE END

Arithmetic
==========

.. EXAMPLE START: Arithmetic with Time Series

Since |TimeSeries| objects are subclasses of |Table|, they naturally support
arithmetic on any of the data columns. As an example, we can take the folded
Kepler time series we have seen in previous examples, and normalize it to the
sigma-clipped median value.

.. plot::
   :context: reset
   :nofigs:

   import numpy as np
   from astropy import units as u
   import matplotlib.pyplot as plt
   from astropy.timeseries import TimeSeries
   from astropy.utils.data import get_pkg_data_filename

   example_data = get_pkg_data_filename('timeseries/kplr010666592-2009131110544_slc.fits')
   kepler = TimeSeries.read(example_data, format='kepler.fits')
   kepler_folded = kepler.fold(period=2.2 * u.day, epoch_time='2009-05-02T20:53:40')

.. plot::
   :context:
   :nofigs:

   import warnings
   warnings.filterwarnings('ignore', message='Input data contains invalid values')

.. plot::
   :include-source:
   :context:

    from astropy.stats import sigma_clipped_stats

    mean, median, stddev = sigma_clipped_stats(kepler_folded['sap_flux'])

    kepler_folded['sap_flux_norm'] = kepler_folded['sap_flux'] / median

    plt.plot(kepler_folded.time.jd, kepler_folded['sap_flux_norm'], 'k.', markersize=1)
    plt.xlabel('Time from midpoint epoch (days)')
    plt.ylabel('Normalized flux')

.. EXAMPLE END
.. _astropy-timeseries:

**********************************
Time Series (`astropy.timeseries`)
**********************************

Introduction
============

From sampling a continuous variable at fixed times to counting events binned
into time windows, many different areas of astrophysics require the manipulation
of 1D time series data. To address this need, the `astropy.timeseries`
subpackage provides classes to represent and manipulate time series.

The time series classes presented below are |QTable| subclasses that have
special columns to represent times using the |Time| class. Therefore, much of
the functionality described in :ref:`astropy-table` applies here. But the main
purpose of the new classes are to provide time series-specific functionality
above and beyond |QTable|.

Getting Started
===============

In this section, we take a quick look at how to read in a time series, access
the data, and carry out some basic analysis. For more details about creating and
using time series, see the full documentation in :ref:`using-timeseries`.

The most basic time series class is |TimeSeries|  it represents a time series
as a collection of values at specific points in time. If you are interested in
representing time series as measurements in discrete time bins, you will likely
be interested in the |BinnedTimeSeries| subclass which we show in
:ref:`using-timeseries`).

.. EXAMPLE START: Using the TimeSeries Class

To start off, we retrieve a FITS file containing a Kepler light curve for a
source::

    >>> from astropy.utils.data import get_pkg_data_filename
    >>> filename = get_pkg_data_filename('timeseries/kplr010666592-2009131110544_slc.fits')  # doctest: +REMOTE_DATA

.. note::
    The light curve provided here is handpicked for example purposes. For
    more information about the Kepler FITS format, see
    `Section 2.3.1 of the Kepler Archive Manual <https://archive.stsci.edu/files/live/sites/mast/files/home/missions-and-data/k2/_documents/MAST_Kepler_Archive_Manual_2020.pdf>`_.
    To get other light curves for science purposes using Python, see the
    `astroquery <https://astroquery.readthedocs.io>`_ affiliated package.

We can then use the |TimeSeries| class to read in this file::

    >>> from astropy.timeseries import TimeSeries
    >>> ts = TimeSeries.read(filename, format='kepler.fits')  # doctest: +REMOTE_DATA +IGNORE_WARNINGS

Time series are specialized kinds of |Table| objects::

    >>> ts  # doctest: +REMOTE_DATA
    <TimeSeries length=14280>
              time             timecorr   ...   pos_corr1      pos_corr2
                                  d       ...      pix            pix
              Time             float32    ...    float32        float32
    ----------------------- ------------- ... -------------- --------------
    2009-05-02T00:41:40.338  6.630610e-04 ...  1.5822421e-03 -1.4463664e-03
    2009-05-02T00:42:39.188  6.630857e-04 ...  1.5743829e-03 -1.4540013e-03
    2009-05-02T00:43:38.045  6.631103e-04 ...  1.5665225e-03 -1.4616371e-03
    2009-05-02T00:44:36.894  6.631350e-04 ...  1.5586632e-03 -1.4692718e-03
    2009-05-02T00:45:35.752  6.631597e-04 ...  1.5508028e-03 -1.4769078e-03
    2009-05-02T00:46:34.601  6.631844e-04 ...  1.5429436e-03 -1.4845425e-03
    2009-05-02T00:47:33.451  6.632091e-04 ...  1.5350844e-03 -1.4921773e-03
    2009-05-02T00:48:32.291  6.632337e-04 ...  1.5272264e-03 -1.4998110e-03
    2009-05-02T00:49:31.149  6.632584e-04 ...  1.5193661e-03 -1.5074468e-03
                        ...           ... ...            ...            ...
    2009-05-11T17:58:22.526  1.014493e-03 ...  3.6121816e-03  3.1950327e-03
    2009-05-11T17:59:21.376  1.014518e-03 ...  3.6102540e-03  3.1872767e-03
    2009-05-11T18:00:20.225  1.014542e-03 ...  3.6083264e-03  3.1795206e-03
    2009-05-11T18:01:19.065  1.014567e-03 ...  3.6063993e-03  3.1717657e-03
    2009-05-11T18:02:17.923  1.014591e-03 ...  3.6044715e-03  3.1640085e-03
    2009-05-11T18:03:16.772  1.014615e-03 ...  3.6025438e-03  3.1562524e-03
    2009-05-11T18:04:15.630  1.014640e-03 ...  3.6006160e-03  3.1484952e-03
    2009-05-11T18:05:14.479  1.014664e-03 ...  3.5986886e-03  3.1407392e-03
    2009-05-11T18:06:13.328  1.014689e-03 ...  3.5967610e-03  3.1329831e-03
    2009-05-11T18:07:12.186  1.014713e-03 ...  3.5948332e-03  3.1252259e-03

.. EXAMPLE END

.. EXAMPLE START: Slicing TimeSeries Objects Using Index Notation

In the same way as for |Table| objects, the various columns and rows of
|TimeSeries| objects can be accessed and sliced using index notation::

    >>> ts['sap_flux']  # doctest: +REMOTE_DATA
    <Quantity [1027045.06, 1027184.44, 1027076.25, ..., 1025451.56, 1025468.5 ,
               1025930.9 ] electron / s>

    >>> ts['time', 'sap_flux']  # doctest: +REMOTE_DATA
    <TimeSeries length=14280>
              time             sap_flux
                             electron / s
              Time             float32
    ----------------------- --------------
    2009-05-02T00:41:40.338  1.0270451e+06
    2009-05-02T00:42:39.188  1.0271844e+06
    2009-05-02T00:43:38.045  1.0270762e+06
    2009-05-02T00:44:36.894  1.0271414e+06
    2009-05-02T00:45:35.752  1.0271569e+06
    2009-05-02T00:46:34.601  1.0272296e+06
    2009-05-02T00:47:33.451  1.0273199e+06
    2009-05-02T00:48:32.291  1.0271497e+06
    2009-05-02T00:49:31.149  1.0271755e+06
                        ...            ...
    2009-05-11T17:58:22.526  1.0234769e+06
    2009-05-11T17:59:21.376  1.0234574e+06
    2009-05-11T18:00:20.225  1.0238128e+06
    2009-05-11T18:01:19.065  1.0243234e+06
    2009-05-11T18:02:17.923  1.0244257e+06
    2009-05-11T18:03:16.772  1.0248654e+06
    2009-05-11T18:04:15.630  1.0250156e+06
    2009-05-11T18:05:14.479  1.0254516e+06
    2009-05-11T18:06:13.328  1.0254685e+06
    2009-05-11T18:07:12.186  1.0259309e+06

    >>> ts[0:4]  # doctest: +REMOTE_DATA
    <TimeSeries length=4>
              time             timecorr   ...   pos_corr1      pos_corr2
                                  d       ...      pix            pix
              Time             float32    ...    float32        float32
    ----------------------- ------------- ... -------------- --------------
    2009-05-02T00:41:40.338  6.630610e-04 ...  1.5822421e-03 -1.4463664e-03
    2009-05-02T00:42:39.188  6.630857e-04 ...  1.5743829e-03 -1.4540013e-03
    2009-05-02T00:43:38.045  6.631103e-04 ...  1.5665225e-03 -1.4616371e-03
    2009-05-02T00:44:36.894  6.631350e-04 ...  1.5586632e-03 -1.4692718e-03

.. EXAMPLE END

.. EXAMPLE START: Accessing the Time Column in TimeSeries Objects

As seen in the previous examples, |TimeSeries| objects have a ``time``
column, which is always the first column. This column can also be accessed using
the ``.time`` attribute::

    >>> ts.time  # doctest: +REMOTE_DATA
    <Time object: scale='tdb' format='isot' value=['2009-05-02T00:41:40.338' '2009-05-02T00:42:39.188'
      '2009-05-02T00:43:38.045' ... '2009-05-11T18:05:14.479'
      '2009-05-11T18:06:13.328' '2009-05-11T18:07:12.186']>

The first column is always a |Time| object (see :ref:`Times and Dates
<astropy-time>`), which therefore supports the ability to convert to different
time scales and formats::

    >>> ts.time.mjd  # doctest: +REMOTE_DATA
    array([54953.0289391 , 54953.02962023, 54953.03030145, ...,
           54962.7536398 , 54962.75432093, 54962.75500215])

    >>> ts.time.unix  # doctest: +REMOTE_DATA
    array([1.24122483e+09, 1.24122489e+09, 1.24122495e+09, ...,
           1.24206505e+09, 1.24206511e+09, 1.24206517e+09])

We can also check what time scale the time is defined on::

    >>> ts.time.scale  # doctest: +REMOTE_DATA
    'tdb'

This is the Barycentric Dynamical Time scale (see :ref:`astropy-time` for more
details). We can use what we have seen so far to make a plot:

.. plot::
   :context: reset
   :nofigs:

    from astropy.utils.data import get_pkg_data_filename
    filename = get_pkg_data_filename('timeseries/kplr010666592-2009131110544_slc.fits')
    from astropy.timeseries import TimeSeries
    ts = TimeSeries.read(filename, format='kepler.fits')

.. plot::
   :include-source:
   :context:

   import matplotlib.pyplot as plt
   plt.plot(ts.time.jd, ts['sap_flux'], 'k.', markersize=1)
   plt.xlabel('Julian Date')
   plt.ylabel('SAP Flux (e-/s)')

It looks like there are a few transits! We can use the
:class:`~astropy.timeseries.BoxLeastSquares` class to estimate the
period, using the "box least squares" (BLS) algorithm::

    >>> import numpy as np
    >>> from astropy import units as u
    >>> from astropy.timeseries import BoxLeastSquares
    >>> periodogram = BoxLeastSquares.from_timeseries(ts, 'sap_flux')  # doctest: +REMOTE_DATA

To run the periodogram analysis, we use a box with a duration of 0.2 days::

    >>> results = periodogram.autopower(0.2 * u.day)  # doctest: +REMOTE_DATA
    >>> best = np.argmax(results.power)  # doctest: +REMOTE_DATA
    >>> period = results.period[best]  # doctest: +REMOTE_DATA
    >>> period  # doctest: +REMOTE_DATA
    <Quantity 2.20551724 d>
    >>> transit_time = results.transit_time[best]  # doctest: +REMOTE_DATA
    >>> transit_time  # doctest: +REMOTE_DATA
    <Time object: scale='tdb' format='isot' value=2009-05-02T20:51:16.338>

.. EXAMPLE END

For more information on available periodogram algorithms, see
:ref:`periodogram-algorithms`.

.. plot::
   :context:
   :nofigs:

   import numpy as np
   from astropy import units as u
   from astropy.timeseries import BoxLeastSquares
   periodogram = BoxLeastSquares.from_timeseries(ts, 'sap_flux')
   results = periodogram.autopower(0.2 * u.day)
   best = np.argmax(results.power)
   period = results.period[best]
   transit_time = results.transit_time[best]

We can now fold the time series using the period we found above using the
:meth:`~astropy.timeseries.TimeSeries.fold` method::

    >>> ts_folded = ts.fold(period=period, epoch_time=transit_time)  # doctest: +REMOTE_DATA

.. plot::
   :context:
   :nofigs:

   ts_folded = ts.fold(period=period, epoch_time=transit_time)

Now we can take a look at the folded time series:

.. plot::
   :context:
   :nofigs:

   plt.clf()

.. plot::
   :context:
   :include-source:

   plt.plot(ts_folded.time.jd, ts_folded['sap_flux'], 'k.', markersize=1)
   plt.xlabel('Time (days)')
   plt.ylabel('SAP Flux (e-/s)')

Using the :ref:`stats` module, we can normalize the flux by sigma-clipping
the data to determine the baseline flux::

    >>> from astropy.stats import sigma_clipped_stats
    >>> mean, median, stddev = sigma_clipped_stats(ts_folded['sap_flux'])  # doctest: +REMOTE_DATA +IGNORE_WARNINGS
    >>> ts_folded['sap_flux_norm'] = ts_folded['sap_flux'] / median  # doctest: +REMOTE_DATA

.. plot::
   :context:
   :nofigs:

   import warnings
   warnings.filterwarnings('ignore', message='Input data contains invalid values')

   from astropy.stats import sigma_clipped_stats
   mean, median, stddev = sigma_clipped_stats(ts_folded['sap_flux'])
   ts_folded['sap_flux_norm'] = ts_folded['sap_flux'] / median

And we can downsample the time series by binning the points into bins of equal
time  this returns a |BinnedTimeSeries|::

    >>> from astropy.timeseries import aggregate_downsample
    >>> ts_binned = aggregate_downsample(ts_folded, time_bin_size=0.03 * u.day)  # doctest: +REMOTE_DATA +IGNORE_WARNINGS
    >>> ts_binned  # doctest: +FLOAT_CMP +REMOTE_DATA
    <BinnedTimeSeries length=74>
       time_bin_start   time_bin_size ...        pos_corr2          sap_flux_norm
                              d       ...           pix
         TimeDelta         float64    ...         float64              float64
    ------------------- ------------- ... ----------------------- ------------------
    -1.1022116370482966          0.03 ...  0.00031207725987769663 0.9998741745948792
    -1.0722116370482966          0.03 ...  0.00041217938996851444 0.9999074339866638
    -1.0422116370482966          0.03 ...  0.00039273229776881635  0.999972939491272
    -1.0122116370482965          0.03 ...   0.0002928022004198283 1.0000077486038208
    -0.9822116370482965          0.03 ...   0.0003891147789545357 0.9999921917915344
    -0.9522116370482965          0.03 ...   0.0003491091774776578 1.0000101327896118
    -0.9222116370482966          0.03 ...   0.0002824827388394624 1.0000121593475342
    -0.8922116370482965          0.03 ...  0.00016335179680027068 0.9999905228614807
    -0.8622116370482965          0.03 ...   0.0001397567830281332 1.0000263452529907
                    ...           ... ...                     ...                ...
      0.817788362951705          0.03 ... -2.2798192730988376e-05 1.0000624656677246
      0.847788362951705          0.03 ...  0.00022221534163691103 1.0000633001327515
      0.877788362951705          0.03 ...  0.00019213277846574783 1.0000433921813965
     0.9077883629517051          0.03 ...   0.0002187517675338313  1.000024676322937
     0.9377883629517049          0.03 ...  0.00016979355132207274 1.0000224113464355
     0.9677883629517047          0.03 ...  0.00014231358363758773 1.0000698566436768
     0.9977883629517045          0.03 ...   0.0001224415173055604 0.9999606013298035
     1.0277883629517042          0.03 ...  0.00027701034559868276 0.9999635815620422
      1.057788362951704          0.03 ...   0.0003093520936090499 0.9999105930328369
     1.0877883629517038          0.03 ...  0.00022884277859702706 0.9998687505722046


.. plot::
   :context:
   :nofigs:

   import warnings
   warnings.filterwarnings('ignore', message='Mean of empty slice')

   from astropy.timeseries import aggregate_downsample
   ts_binned = aggregate_downsample(ts_folded, time_bin_size=0.03 * u.day)

Now we can take a look at the final result:

.. plot::
   :context:
   :nofigs:

   plt.clf()

.. plot::
   :context:
   :include-source:

   plt.plot(ts_folded.time.jd, ts_folded['sap_flux_norm'], 'k.', markersize=1)
   plt.plot(ts_binned.time_bin_start.jd, ts_binned['sap_flux_norm'], 'r-', drawstyle='steps-post')
   plt.xlabel('Time (days)')
   plt.ylabel('Normalized flux')

To learn more about the capabilities in the `astropy.timeseries` module, you can
find links to the full documentation in the next section.

.. _using-timeseries:

Using ``timeseries``
====================

The details of using `astropy.timeseries` are provided in the following
sections:

Initializing and Reading in Time Series
---------------------------------------

.. toctree::
   :maxdepth: 2

   initializing
   io

Accessing Data and Manipulating Time Series
-------------------------------------------

.. toctree::
   :maxdepth: 2

   data_access
   times
   analysis
   masking
   pandas

.. _periodogram-algorithms:

Periodogram Algorithms
----------------------

.. toctree::
   :maxdepth: 2

   lombscargle
   bls

Reference/API
=============

.. automodapi:: astropy.timeseries
   :inherited-members:

.. automodapi:: astropy.timeseries.io
   :inherited-members:
.. _timeseries-io:

Reading and Writing Time Series
*******************************

Built-in Readers
================

Since |TimeSeries| and |BinnedTimeSeries| are subclasses of |Table|, they have
:meth:`~astropy.table.Table.read` and :meth:`~astropy.table.Table.write` methods
that can be used to read and write time series from files. We include a few readers for
well-defined formats in `astropy.timeseries`. For instance we have readers for
light curves in FITS format from the `Kepler
<https://www.nasa.gov/mission_pages/kepler/main/index.html>`_ and `TESS
<https://tess.gsfc.nasa.gov/>`_ missions.

Example
-------

.. EXAMPLE START: Reading and Writing Kepler and TESS TimeSeries

In this demonstration of using Kepler FITS time series, we start off by fetching
an example file:

.. plot::
   :include-source:
   :context: reset
   :nofigs:

   from astropy.utils.data import get_pkg_data_filename
   example_data = get_pkg_data_filename('timeseries/kplr010666592-2009131110544_slc.fits')

.. note::
    The light curve provided here is handpicked for example purposes. To get
    other Kepler light curves for science purposes using Python, see the
    `astroquery <https://astroquery.readthedocs.io>`_ affiliated package.

This will set ``example_data`` to the filename of the downloaded file (so you
can replace this by the filename for the file you want to read in). We can then
read in the time series using:

.. plot::
   :include-source:
   :context:
   :nofigs:

   from astropy.timeseries import TimeSeries
   kepler = TimeSeries.read(example_data, format='kepler.fits')

Now we can check that the time series has been read in correctly:

.. plot::
   :include-source:
   :context:

   import matplotlib.pyplot as plt

   plt.plot(kepler.time.jd, kepler['sap_flux'], 'k.', markersize=1)
   plt.xlabel('Julian Date')
   plt.ylabel('SAP Flux (e-/s)')

.. EXAMPLE END

Reading Common Light Curve Formats
==================================

At the moment only a few formats are defined in ``astropy`` itself, in part
because there are not many well-documented formats for storing time series. So
in many cases, you will likely have to first read in your files using the more
generic |Table| class (see :ref:`read_write_tables`). In fact, the
:meth:`TimeSeries.read <astropy.timeseries.TimeSeries.read>` and
:meth:`BinnedTimeSeries.read <astropy.timeseries.BinnedTimeSeries.read>` methods
can do this behind the scenes. If the table cannot be read by any of the time
series readers, these methods will try to use some of the default
:class:`~astropy.table.Table` readers and then require users to specify the
names of the important columns.

Examples
--------

.. EXAMPLE START: Reading Common Light Curve Formats for Storing Time Series

If you are reading in a file called :download:`sampled.csv <sampled.csv>` where
the time column is called ``Date`` and is an ISO string, you can do::

    >>> from astropy.timeseries import TimeSeries
    >>> from astropy.utils.data import get_pkg_data_filename
    >>> sampled_filename = get_pkg_data_filename('data/sampled.csv',
    ...                                          package='astropy.timeseries.tests')
    >>> ts = TimeSeries.read(sampled_filename, format='ascii.csv',
    ...                      time_column='Date')
    >>> ts[:3]
    <TimeSeries length=3>
              time             A       B       C       D       E       F       G
              Time          float64 float64 float64 float64 float64 float64 float64
    ----------------------- ------- ------- ------- ------- ------- ------- -------
    2008-03-18 00:00:00.000   24.68  164.93  114.73   26.27   19.21   28.87   63.44
    2008-03-19 00:00:00.000   24.18  164.89  114.75   26.22   19.07   27.76   59.98
    2008-03-20 00:00:00.000   23.99  164.63  115.04   25.78   19.01   27.04   59.61

If you are reading in a binned time series from a file called
:download:`binned.csv <binned.csv>` and with a column ``time_start`` giving the
start time and ``bin_size`` giving the size of each bin, you can do::

    >>> from astropy import units as u
    >>> from astropy.timeseries import BinnedTimeSeries
    >>> binned_filename = get_pkg_data_filename('data/binned.csv',
    ...                                          package='astropy.timeseries.tests')
    >>> ts = BinnedTimeSeries.read(binned_filename, format='ascii.csv',
    ...                            time_bin_start_column='time_start',
    ...                            time_bin_size_column='bin_size',
    ...                            time_bin_size_unit=u.s)
    >>> ts[:3]
    <BinnedTimeSeries length=3>
         time_bin_start     time_bin_size ...    E       F
                                  s       ...
              Time             float64    ... float64 float64
    ----------------------- ------------- ... ------- -------
    2016-03-22T12:30:31.000           3.0 ...   28.87   63.44
    2016-03-22T12:30:34.000           3.0 ...   27.76   59.98
    2016-03-22T12:30:37.000           3.0 ...   27.04   59.61

See the documentation for :meth:`TimeSeries.read
<astropy.timeseries.TimeSeries.read>` and :meth:`BinnedTimeSeries.read
<astropy.timeseries.BinnedTimeSeries.read>` for more details.

.. EXAMPLE END

Alternatively, you can read in the table using your own code then construct the
|TimeSeries| object as described in :ref:`timeseries-initializing`, although
then you cannot write out another time series in the same format.

If you have written a reader/writer for a commonly used format, please feel free
to contribute it to ``astropy``!
.. _timeseries-initializing:

Creating Time Series
********************

Initializing a Time Series
==========================

The first type of time series that we will look at here is a |TimeSeries|
object, which can be used for a time series which samples a continuous variable
at discrete, instantaneous times. Initializing a |TimeSeries| object can be done
in the same ways as initializing a |Table| object (see :ref:`Data Tables
<astropy-table>`), but additional arguments related to the times should be
specified.

Evenly Sampled Time Series
--------------------------

.. EXAMPLE START: Constructing an Evenly Sampled TimeSeries

The most convenient way to construct an evenly sampled |TimeSeries| is to
specify the start time, the time interval, and the number of samples::

    >>> from astropy import units as u
    >>> from astropy.timeseries import TimeSeries
    >>> ts1 = TimeSeries(time_start='2016-03-22T12:30:31',
    ...                  time_delta=3 * u.s,
    ...                  n_samples=5)
    >>> ts1
    <TimeSeries length=5>
              time
              Time
    -----------------------
    2016-03-22T12:30:31.000
    2016-03-22T12:30:34.000
    2016-03-22T12:30:37.000
    2016-03-22T12:30:40.000
    2016-03-22T12:30:43.000

The ``time`` keyword argument can be set to anything that can be passed to the
|Time| class (see also :ref:`Time and Dates <astropy-time>`) or |Time| objects
directly. Note that the ``n_samples`` argument is only needed if you are not
also passing in data during initialization (see `Passing Data During
Initialization`_).

.. EXAMPLE END

Arbitrarily Sampled Time Series
-------------------------------

.. EXAMPLE START: Constructing an Arbitrarily Sampled TimeSeries

To construct a sampled time series with samples at arbitrary times, you can
pass multiple times to the ``time`` argument::

    >>> ts2 = TimeSeries(time=['2016-03-22T12:30:31',
    ...                        '2016-03-22T12:30:38',
    ...                        '2016-03-22T12:34:40'])
    >>> ts2
    <TimeSeries length=3>
              time
              Time
    -----------------------
    2016-03-22T12:30:31.000
    2016-03-22T12:30:38.000
    2016-03-22T12:34:40.000

You can also specify a vector |Time| object directly as the ``time=`` argument,
or a vector |TimeDelta| argument or a quantity array to the ``time_delta=``
argument.::

    >>> TimeSeries(time_start="2011-01-01T00:00:00",
    ...            time_delta=[0.1, 0.2, 0.1, 0.3, 0.2]*u.s)
    <TimeSeries length=5>
             time
             Time
    -----------------------
    2011-01-01T00:00:00.000
    2011-01-01T00:00:00.100
    2011-01-01T00:00:00.300
    2011-01-01T00:00:00.400
    2011-01-01T00:00:00.700

.. EXAMPLE END

.. _timeseries-binned-initializing:

Initializing a Binned Time Series
=================================

The |BinnedTimeSeries| can be used to represent time series where each entry
corresponds to measurements taken over a range in time  for instance, a light
curve constructed by binning X-ray photon events. This class supports equal-size
or uneven bins, and contiguous and non-contiguous bins. As for |TimeSeries|,
initializing a |BinnedTimeSeries| can be done in the same ways as initializing a
|Table| object (see :ref:`Data Tables <astropy-table>`), but additional
arguments related to the times should be specified as described below.

Equal-Sized Contiguous Bins
---------------------------

.. EXAMPLE START: Initializing a Binned Time Series with Equal Contiguous Bins

To create a binned time series with equal-size contiguous bins, it is sufficient
to specify a start time as well as a bin size::

    >>> from astropy.timeseries import BinnedTimeSeries
    >>> ts3 = BinnedTimeSeries(time_bin_start='2016-03-22T12:30:31',
    ...                        time_bin_size=3 * u.s, n_bins=10)
    >>> ts3
    <BinnedTimeSeries length=10>
        time_bin_start     time_bin_size
                                 s
              Time             float64
    ----------------------- -------------
    2016-03-22T12:30:31.000           3.0
    2016-03-22T12:30:34.000           3.0
    2016-03-22T12:30:37.000           3.0
    2016-03-22T12:30:40.000           3.0
    2016-03-22T12:30:43.000           3.0
    2016-03-22T12:30:46.000           3.0
    2016-03-22T12:30:49.000           3.0
    2016-03-22T12:30:52.000           3.0
    2016-03-22T12:30:55.000           3.0
    2016-03-22T12:30:58.000           3.0

Note that the ``n_bins`` argument is only needed if you are not also passing in
data during initialization (see `Passing Data During Initialization`_).

.. EXAMPLE END

Uneven Contiguous Bins
----------------------

.. EXAMPLE START: Initializing a Binned Time Series with Uneven Contiguous Bins

When creating a binned time series with uneven contiguous bins, the bin size can
be changed to give multiple values (note that in this case ``n_bins`` is not
required)::

    >>> ts4 = BinnedTimeSeries(time_bin_start='2016-03-22T12:30:31',
    ...                        time_bin_size=[3, 3, 2, 3] * u.s)
    >>> ts4
    <BinnedTimeSeries length=4>
         time_bin_start     time_bin_size
                                  s
              Time             float64
    ----------------------- -------------
    2016-03-22T12:30:31.000           3.0
    2016-03-22T12:30:34.000           3.0
    2016-03-22T12:30:37.000           2.0
    2016-03-22T12:30:39.000           3.0

Alternatively, you can create the same time series by giving an array of start
times as well as a single end time::

    >>> ts5 = BinnedTimeSeries(time_bin_start=['2016-03-22T12:30:31',
    ...                                        '2016-03-22T12:30:34',
    ...                                        '2016-03-22T12:30:37',
    ...                                        '2016-03-22T12:30:39'],
    ...                        time_bin_end='2016-03-22T12:30:42')
    >>> ts5  # doctest: +FLOAT_CMP
    <BinnedTimeSeries length=4>
         time_bin_start        time_bin_size
                                    s
             Time                float64
    ----------------------- -----------------
    2016-03-22T12:30:31.000               3.0
    2016-03-22T12:30:34.000               3.0
    2016-03-22T12:30:37.000               2.0
    2016-03-22T12:30:39.000               3.0

.. EXAMPLE END

Uneven Non-Contiguous Bins
--------------------------

.. EXAMPLE START: Initializing a Binned Time Series with Uneven Non-Contiguous
   Bins

To create a binned time series with non-contiguous bins, you can either
specify an array of start times and bin widths::

    >>> ts6 = BinnedTimeSeries(time_bin_start=['2016-03-22T12:30:31',
    ...                                        '2016-03-22T12:30:38',
    ...                                        '2016-03-22T12:34:40'],
    ...                        time_bin_size=[5, 100, 2]*u.s)
    >>> ts6
    <BinnedTimeSeries length=3>
         time_bin_start     time_bin_size
                                  s
              Time             float64
    ----------------------- -------------
    2016-03-22T12:30:31.000           5.0
    2016-03-22T12:30:38.000         100.0
    2016-03-22T12:34:40.000           2.0

Or in the most general case, you can also specify multiple times for
``time_bin_start`` and ``time_bin_end``::

    >>> ts7 = BinnedTimeSeries(time_bin_start=['2016-03-22T12:30:31',
    ...                                        '2016-03-22T12:30:33',
    ...                                        '2016-03-22T12:30:40'],
    ...                        time_bin_end=['2016-03-22T12:30:32',
    ...                                      '2016-03-22T12:30:35',
    ...                                      '2016-03-22T12:30:41'])
    >>> ts7  # doctest: +FLOAT_CMP
    <BinnedTimeSeries length=3>
        time_bin_start        time_bin_size
                                    s
              Time               float64
    ----------------------- ------------------
    2016-03-22T12:30:31.000                1.0
    2016-03-22T12:30:33.000                2.0
    2016-03-22T12:30:40.000                1.0

.. EXAMPLE END

Adding Data to the Time Series
==============================

The above examples show how to initialize |TimeSeries| objects, but these do not
include any data aside from the times. There are different ways of adding data,
as with the |Table| class.

Passing Data During Initialization
----------------------------------

.. EXAMPLE START: Adding Data to a TimeSeries Object During Initialization

It is possible to pass data during the initialization of a |TimeSeries|
object, as for |Table| objects. For instance::

    >>> ts8 = BinnedTimeSeries(time_bin_start=['2016-03-22T12:30:31',
    ...                                        '2016-03-22T12:30:34',
    ...                                        '2016-03-22T12:30:37',
    ...                                        '2016-03-22T12:30:39'],
    ...                        time_bin_end='2016-03-22T12:30:42',
    ...                        data={'flux': [1., 4., 5., 6.] * u.mJy})
    >>> ts8  # doctest: +FLOAT_CMP
    <BinnedTimeSeries length=4>
          time_bin_start     time_bin_size     flux
                                   s            mJy
              Time              float64       float64
    ----------------------- ----------------- -------
    2016-03-22T12:30:31.000               3.0     1.0
    2016-03-22T12:30:34.000               3.0     4.0
    2016-03-22T12:30:37.000               2.0     5.0
    2016-03-22T12:30:39.000               3.0     6.0

.. EXAMPLE END

Adding Data After Initialization
--------------------------------

.. EXAMPLE START: Adding Data to a TimeSeries Object After Initialization

Once a |TimeSeries| object is initialized, you can add columns/fields to it as
you would for a |Table| object::

    >>> from astropy import units as u
    >>> ts1['flux'] = [1., 4., 5., 6., 4.] * u.mJy
    >>> ts1
    <TimeSeries length=5>
              time            flux
                              mJy
              Time          float64
    ----------------------- -------
    2016-03-22T12:30:31.000     1.0
    2016-03-22T12:30:34.000     4.0
    2016-03-22T12:30:37.000     5.0
    2016-03-22T12:30:40.000     6.0
    2016-03-22T12:30:43.000     4.0

.. EXAMPLE END

Adding Rows
-----------

.. EXAMPLE START: Adding Rows to a TimeSeries or BinnedTimeSeries

Adding rows to |TimeSeries| or |BinnedTimeSeries| can be done using the
:meth:`~astropy.table.Table.add_row` method, as for |Table| and |QTable|. This
method takes a dictionary where the keys are column names::

    >>> ts8.add_row({'time_bin_start': '2016-03-22T12:30:44.000',
    ...              'time_bin_size': 2 * u.s,
    ...              'flux': 3 * u.mJy})
    >>> ts8  # doctest: +FLOAT_CMP
    <BinnedTimeSeries length=5>
        time_bin_start       time_bin_size      flux
                                    s           mJy
              Time               float64      float64
    ----------------------- ----------------- -------
    2016-03-22T12:30:31.000               3.0     1.0
    2016-03-22T12:30:34.000               3.0     4.0
    2016-03-22T12:30:37.000               2.0     5.0
    2016-03-22T12:30:39.000               3.0     6.0
    2016-03-22T12:30:44.000               2.0     3.0

If you want to be able to skip some values when adding rows, you should make
sure that masking is enabled  see :ref:`timeseries-masking` for more details.

.. EXAMPLE END
.. _timeseries-data-access:

Accessing Data in Time Series
*****************************

.. |time_attr| replace:: :attr:`~astropy.timeseries.TimeSeries.time`
.. |time_bin_start| replace:: :attr:`~astropy.timeseries.BinnedTimeSeries.time_bin_start`
.. |time_bin_center| replace:: :attr:`~astropy.timeseries.BinnedTimeSeries.time_bin_center`
.. |time_bin_end| replace:: :attr:`~astropy.timeseries.BinnedTimeSeries.time_bin_end`
.. |time_bin_size| replace:: :attr:`~astropy.timeseries.BinnedTimeSeries.time_bin_size`

Accessing Data
==============

.. EXAMPLE START: Accessing Data in Time Series

For the examples in this page, we will consider a sampled time series
with two data columns  ``flux`` and ``temp``::

    >>> from astropy import units as u
    >>> from astropy.timeseries import TimeSeries
    >>> ts = TimeSeries(time_start='2016-03-22T12:30:31',
    ...                 time_delta=3 * u.s,
    ...                 data={'flux': [1., 4., 5., 3., 2.] * u.Jy,
    ...                       'temp': [40., 41., 39., 24., 20.] * u.K},
    ...                 names=('flux', 'temp'))

As for |Table|, columns can be accessed by name::

    >>> ts['flux']  # doctest: +FLOAT_CMP
    <Quantity [ 1., 4., 5., 3., 2.] Jy>
    >>> ts['time']
    <Time object: scale='utc' format='isot' value=['2016-03-22T12:30:31.000' '2016-03-22T12:30:34.000'
     '2016-03-22T12:30:37.000' '2016-03-22T12:30:40.000'
     '2016-03-22T12:30:43.000']>

And rows can be accessed by index::

    >>> ts[0]
    <Row index=0>
              time            flux    temp
                               Jy      K
              Time          float64 float64
    ----------------------- ------- -------
    2016-03-22T12:30:31.000     1.0    40.0

Accessing individual values can then be done either by accessing a column and
then a row, or vice versa::

    >>> ts[0]['flux']  # doctest: +FLOAT_CMP
    <Quantity 1. Jy>

    >>> ts['temp'][2]  # doctest: +FLOAT_CMP
    <Quantity 39. K>

.. EXAMPLE END

.. _timeseries-accessing-times:

Accessing Times
===============

.. duplicate example from index.rst

For |TimeSeries|, the ``time`` column can be accessed using the regular column
access notation, as shown in `Accessing Data`_, but it can also be accessed
more conveniently using the |time_attr| attribute::

    >>> ts.time
    <Time object: scale='utc' format='isot' value=['2016-03-22T12:30:31.000' '2016-03-22T12:30:34.000'
     '2016-03-22T12:30:37.000' '2016-03-22T12:30:40.000'
     '2016-03-22T12:30:43.000']>

.. EXAMPLE START: Accessing the Time Column in BinnedTimeSeries

For |BinnedTimeSeries|, we provide three attributes: |time_bin_start|,
|time_bin_center|, and |time_bin_end|::

    >>> from astropy.timeseries import BinnedTimeSeries
    >>> bts = BinnedTimeSeries(time_bin_start='2016-03-22T12:30:31',
    ...                        time_bin_size=3 * u.s, n_bins=5)
    >>> bts.time_bin_start
    <Time object: scale='utc' format='isot' value=['2016-03-22T12:30:31.000' '2016-03-22T12:30:34.000'
     '2016-03-22T12:30:37.000' '2016-03-22T12:30:40.000'
     '2016-03-22T12:30:43.000']>
    >>> bts.time_bin_center
    <Time object: scale='utc' format='isot' value=['2016-03-22T12:30:32.500' '2016-03-22T12:30:35.500'
     '2016-03-22T12:30:38.500' '2016-03-22T12:30:41.500'
     '2016-03-22T12:30:44.500']>
    >>> bts.time_bin_end
    <Time object: scale='utc' format='isot' value=['2016-03-22T12:30:34.000' '2016-03-22T12:30:37.000'
     '2016-03-22T12:30:40.000' '2016-03-22T12:30:43.000'
     '2016-03-22T12:30:46.000']>

In addition, the |time_bin_size| attribute can be used to access the bin sizes::

    >>> bts.time_bin_size  # doctest: +SKIP
    <Quantity [3., 3., 3., 3., 3.] s>

Note that only |time_bin_start| and |time_bin_size| are available as actual
columns, and |time_bin_center| and |time_bin_end| are computed on the fly.

.. EXAMPLE END

See :ref:`timeseries-times` for more information about changing between
different representations of time.

Extracting a Subset of Columns
==============================

.. EXAMPLE START: Extracting a Subset of Columns in TimeSeries

We can create a new time series with just the ``flux`` column by doing::

   >>> ts['time', 'flux']
   <TimeSeries length=5>
             time            flux
                              Jy
             Time          float64
   ----------------------- -------
   2016-03-22T12:30:31.000     1.0
   2016-03-22T12:30:34.000     4.0
   2016-03-22T12:30:37.000     5.0
   2016-03-22T12:30:40.000     3.0
   2016-03-22T12:30:43.000     2.0

Note that the new columns will be copies (not views) of the original columns.
We can also create a plain |QTable| by extracting just the ``flux`` and
``temp`` columns::

   >>> ts['flux', 'temp']
   <QTable length=5>
     flux    temp
       Jy      K
   float64 float64
   ------- -------
       1.0    40.0
       4.0    41.0
       5.0    39.0
       3.0    24.0
       2.0    20.0

.. EXAMPLE END

Extracting a Subset of Rows
===========================

.. EXAMPLE START: Extracting a Subset of Rows in TimeSeries

|TimeSeries| objects can be sliced by rows, using the same syntax as for |Time|,
for example::

   >>> ts[0:2]
   <TimeSeries length=2>
             time            flux    temp
                              Jy      K
             Time          float64 float64
   ----------------------- ------- -------
   2016-03-22T12:30:31.000     1.0    40.0
   2016-03-22T12:30:34.000     4.0    41.0

|TimeSeries| objects are also automatically indexed using the functionality
described in :ref:`table-indexing`. This provides the ability to access rows and
a subset of rows using the :attr:`~astropy.timeseries.TimeSeries.loc` and
:attr:`~astropy.timeseries.TimeSeries.iloc` attributes.

.. EXAMPLE END

.. EXAMPLE START: Slicing TimeSeries by Time

The :attr:`~astropy.timeseries.TimeSeries.loc` attribute can be used to slice
|TimeSeries| objects by time. For example, the following can be used to extract
all entries for a given timestamp::

   >>> from astropy.time import Time
   >>> ts.loc[Time('2016-03-22T12:30:31.000')]  # doctest: +SKIP
   <Row index=0>
             time            flux    temp
                              Jy      K
             Time          float64 float64
   ----------------------- ------- -------
   2016-03-22T12:30:31.000     1.0    40.0

Or within a time range::

   >>> ts.loc['2016-03-22T12:30:30':'2016-03-22T12:30:41']
   <TimeSeries length=4>
             time            flux    temp
                              Jy      K
             Time          float64 float64
   ----------------------- ------- -------
   2016-03-22T12:30:31.000     1.0    40.0
   2016-03-22T12:30:34.000     4.0    41.0
   2016-03-22T12:30:37.000     5.0    39.0
   2016-03-22T12:30:40.000     3.0    24.0

.. EXAMPLE END

Note that in this case we did not specify |Time|  this is not needed if the
string is an ISO 8601 time string. As for the |QTable| and |Table| class ``loc``
attribute, in order to be consistent with `pandas
<https://pandas.pydata.org/>`_, the last item in the ``loc`` range is inclusive.

Also note that the result will always be sorted by time. Similarly, the
:attr:`~astropy.timeseries.TimeSeries.iloc` attribute can be used to fetch
rows from the time series *sorted by time*, so for example, the first two
entries (by time) can be accessed with::

   >>> ts.iloc[0:2]
   <TimeSeries length=2>
             time            flux    temp
                              Jy      K
             Time          float64 float64
   ----------------------- ------- -------
   2016-03-22T12:30:31.000     1.0    40.0
   2016-03-22T12:30:34.000     4.0    41.0
.. _timeseries-times:

Converting between Different Time Representations
*************************************************

In :ref:`timeseries-accessing-times`, we saw how to access the time
columns/attributes of the |TimeSeries| and |BinnedTimeSeries| classes. Here we
look in more detail at how to manipulate the resulting times.

Converting Times
================

Since the time column in time series is always a |Time| object, it is possible
to use the usual attributes on |Time| to convert the time to different formats
or scales.

Example
-------

.. EXAMPLE START: Converting the Time Column to Different Time Formats

To get the times as modified Julian Dates from a minimal time series::

    >>> from astropy import units as u
    >>> from astropy.timeseries import TimeSeries
    >>> ts = TimeSeries(time_start='2016-03-22T12:30:31', time_delta=3 * u.s,
    ...                 data={'flux': [1., 3., 4., 2., 4.]})
    >>> ts.time.mjd  # doctest: +FLOAT_CMP
    array([57469.52119213, 57469.52122685, 57469.52126157, 57469.5212963 ,
           57469.52133102])

Or to convert the times to the Temps Atomique International (TAI) scale::

    >>> ts.time.tai
    <Time object: scale='tai' format='isot' value=['2016-03-22T12:31:07.000' '2016-03-22T12:31:10.000'
     '2016-03-22T12:31:13.000' '2016-03-22T12:31:16.000'
     '2016-03-22T12:31:19.000']>

To find the current time scale of the data, you can do::

    >>> ts.time.scale
    'utc'

See :ref:`astropy-time` for more documentation on how to access and convert
times.

.. EXAMPLE END

Formatting Times
================

Since the various time columns are |Time| objects, the default format and scale
to use for the display of the time series can be changed using the ``format``
and ``scale`` attributes.

Example
-------

.. EXAMPLE START: Formatting the Time Column in Time Series

To change the display of the time series::

    >>> ts.time.format = 'isot'
    >>> ts
    <TimeSeries length=5>
              time            flux
              Time          float64
    ----------------------- -------
    2016-03-22T12:30:31.000     1.0
    2016-03-22T12:30:34.000     3.0
    2016-03-22T12:30:37.000     4.0
    2016-03-22T12:30:40.000     2.0
    2016-03-22T12:30:43.000     4.0
    >>> ts.time.format = 'unix'
    >>> ts  # doctest: +FLOAT_CMP
    <TimeSeries length=5>
        time       flux
        Time     float64
    ------------ -------
    1458649831.0     1.0
    1458649834.0     3.0
    1458649837.0     4.0
    1458649840.0     2.0
    1458649843.0     4.0

.. EXAMPLE END

Times Relative to Other Times
=============================

In some cases, it can be useful to use relative rather than absolute times.
This can be done by using the |TimeDelta| class instead of the |Time| class,
for example, by subtracting a reference time from an existing |Time| object.

Example
-------

.. EXAMPLE START: Times Relative to Other Times in Time Series

To use a relative rather than an absolute time::

    >>> ts_rel = TimeSeries(time=ts.time - ts.time[0])
    >>> ts_rel  # doctest: +FLOAT_CMP
    <TimeSeries length=5>
             time
           TimeDelta
    ----------------------
                       0.0
     3.472222222222765e-05
      6.94444444444553e-05
    0.00010416666666657193
    0.00013888888888879958

The |TimeDelta| values can be converted to a different time unit (e.g., second)
using::

    >>> ts_rel.time.to('second')
    <Quantity [ 0.,  3.,  6.,  9., 12.] s>

.. EXAMPLE END
.. _nddata_subclassing:

Subclassing
***********

`~astropy.nddata.NDData`
========================

This class serves as the base for subclasses that use a `numpy.ndarray` (or
something that presents a ``numpy``-like interface) as the ``data`` attribute.

.. note::
  Each attribute is saved as an attribute with one leading underscore. For
  example, the ``data`` is saved as ``_data`` and the ``mask`` as ``_mask``,
  and so on.

Adding Another Property
-----------------------

    >>> from astropy.nddata import NDData

    >>> class NDDataWithFlags(NDData):
    ...     def __init__(self, *args, **kwargs):
    ...         # Remove flags attribute if given and pass it to the setter.
    ...         self.flags = kwargs.pop('flags') if 'flags' in kwargs else None
    ...         super().__init__(*args, **kwargs)
    ...
    ...     @property
    ...     def flags(self):
    ...         return self._flags
    ...
    ...     @flags.setter
    ...     def flags(self, value):
    ...         self._flags = value

    >>> ndd = NDDataWithFlags([1,2,3])
    >>> ndd.flags is None
    True

    >>> ndd = NDDataWithFlags([1,2,3], flags=[0, 0.2, 0.3])
    >>> ndd.flags
    [0, 0.2, 0.3]

.. note::
  To simplify subclassing, each setter (except for ``data``) is called during
  ``__init__`` so putting restrictions on any attribute can be done inside
  the setter and will also apply during instance creation.

Customize the Setter for a Property
-----------------------------------

    >>> import numpy as np

    >>> class NDDataMaskBoolNumpy(NDData):
    ...
    ...     @NDData.mask.setter
    ...     def mask(self, value):
    ...         # Convert mask to boolean numpy array.
    ...         self._mask = np.array(value, dtype=np.bool_)

    >>> ndd = NDDataMaskBoolNumpy([1,2,3])
    >>> ndd.mask = [True, False, True]
    >>> ndd.mask
    array([ True, False,  True]...)

Extend the Setter for a Property
--------------------------------

``unit``, ``meta``, and ``uncertainty`` implement some additional logic in their
setter so subclasses might define a call to the superclass and let the
super property set the attribute afterwards::

    >>> import numpy as np

    >>> class NDDataUncertaintyShapeChecker(NDData):
    ...
    ...     @NDData.uncertainty.setter
    ...     def uncertainty(self, value):
    ...         value = np.asarray(value)
    ...         if value.shape != self.data.shape:
    ...             raise ValueError('uncertainty must have the same shape as the data.')
    ...         # Call the setter of the super class in case it might contain some
    ...         # important logic (only True for meta, unit and uncertainty)
    ...         super(NDDataUncertaintyShapeChecker, self.__class__).uncertainty.fset(self, value)
    ...         # Unlike "super(cls_name, cls_name).uncertainty.fset" or
    ...         # or "NDData.uncertainty.fset" this will respect Pythons method
    ...         # resolution order.

    >>> ndd = NDDataUncertaintyShapeChecker([1,2,3], uncertainty=[2,3,4])
    INFO: uncertainty should have attribute uncertainty_type. [astropy.nddata.nddata]
    >>> ndd.uncertainty
    UnknownUncertainty([2, 3, 4])

Having a Setter for the Data
----------------------------

    >>> class NDDataWithDataSetter(NDData):
    ...
    ...     @NDData.data.setter
    ...     def data(self, value):
    ...         self._data = np.asarray(value)

    >>> ndd = NDDataWithDataSetter([1,2,3])
    >>> ndd.data = [3,2,1]
    >>> ndd.data
    array([3, 2, 1])

.. _NDDataRef:

`~astropy.nddata.NDDataRef`
===========================

`~astropy.nddata.NDDataRef` itself inherits from `~astropy.nddata.NDData` so
any of the possibilities there also apply to NDDataRef. But NDDataRef also
inherits from the Mixins:

- `~astropy.nddata.NDSlicingMixin`
- `~astropy.nddata.NDArithmeticMixin`
- `~astropy.nddata.NDIOMixin`

Which allow additional operations.

Add Another Arithmetic Operation
--------------------------------

Adding another operation is possible provided the ``data`` and ``unit`` allow
it within the framework of `~astropy.units.Quantity`.

Examples
^^^^^^^^

..
  EXAMPLE START
  Adding Operations When Working with NDDataRef

To add a power function::

    >>> from astropy.nddata import NDDataRef
    >>> import numpy as np
    >>> from astropy.utils import sharedmethod

    >>> class NDDataPower(NDDataRef):
    ...     @sharedmethod # sharedmethod to allow it also as classmethod
    ...     def pow(self, operand, operand2=None, **kwargs):
    ...         # the uncertainty doesn't allow propagation so set it to None
    ...         kwargs['propagate_uncertainties'] = None
    ...         # Call the _prepare_then_do_arithmetic function with the
    ...         # numpy.power ufunc.
    ...         return self._prepare_then_do_arithmetic(np.power, operand,
    ...                                                 operand2, **kwargs)

This can be used like the other arithmetic methods similar to
:meth:`~astropy.nddata.NDArithmeticMixin.add`. So it works when calling it
on the class or the instance::

    >>> ndd = NDDataPower([1,2,3])

    >>> # using it on the instance with one operand
    >>> ndd.pow(3)
    NDDataPower([ 1,  8, 27])

    >>> # using it on the instance with two operands
    >>> ndd.pow([1,2,3], [3,4,5])
    NDDataPower([  1,  16, 243])

    >>> # or using it as classmethod
    >>> NDDataPower.pow(6, [1,2,3])
    NDDataPower([  6,  36, 216])

To allow propagation also with ``uncertainty`` see subclassing
`~astropy.nddata.NDUncertainty`.

..
  EXAMPLE END

The ``_prepare_then_do_arithmetic`` implements the relevant checks if it was
called on the class or the instance, and if one or two operands were given,
converts the operands, if necessary, to the appropriate classes. Overriding
``_prepare_then_do_arithmetic`` in subclasses should be avoided if
possible.

Arithmetic on an Existing Property
----------------------------------

Customizing how an existing property is handled during arithmetic is possible
with some arguments to the function calls such as
:meth:`~astropy.nddata.NDArithmeticMixin.add`, but it is possible to hardcode
behavior too. The actual operation on the attribute (except for ``unit``) is
done in a method ``_arithmetic_*`` where ``*`` is the name of the property.

Examples
^^^^^^^^

..
  EXAMPLE START
  Customizing Existing Properties During Arithmetic in NDData

To customize how the ``meta`` will be affected during arithmetics::

    >>> from astropy.nddata import NDDataRef

    >>> from copy import deepcopy
    >>> class NDDataWithMetaArithmetics(NDDataRef):
    ...
    ...     def _arithmetic_meta(self, operation, operand, handle_mask, **kwds):
    ...         # the function must take the arguments:
    ...         # operation (numpy-ufunc like np.add, np.subtract, ...)
    ...         # operand (the other NDData-like object, already wrapped as NDData)
    ...         # handle_mask (see description for "add")
    ...
    ...         # The meta is dict like but we want the keywords exposure to change
    ...         # Anticipate that one or both might have no meta and take the first one that has
    ...         result_meta = deepcopy(self.meta) if self.meta else deepcopy(operand.meta)
    ...         # Do the operation on the keyword if the keyword exists
    ...         if result_meta and 'exposure' in result_meta:
    ...             result_meta['exposure'] = operation(result_meta['exposure'], operand.data)
    ...         return result_meta # return it

To trigger this method, the ``handle_meta`` argument to arithmetic methods can
be anything except ``None`` or ``"first_found"``::

    >>> ndd = NDDataWithMetaArithmetics([1,2,3], meta={'exposure': 10})
    >>> ndd2 = ndd.add(10, handle_meta='')
    >>> ndd2.meta
    {'exposure': 20}

    >>> ndd3 = ndd.multiply(0.5, handle_meta='')
    >>> ndd3.meta
    {'exposure': 5.0}

.. warning::
  To use these internal `_arithmetic_*` methods there are some restrictions on
  the attributes when calling the operation:

  - ``mask``: ``handle_mask`` must not be ``None``, ``"ff"``, or
    ``"first_found"``.
  - ``wcs``: ``compare_wcs`` argument with the same restrictions as mask.
  - ``meta``: ``handle_meta`` argument with the same restrictions as mask.
  - ``uncertainty``: ``propagate_uncertainties`` must be ``None`` or evaluate
    to ``False``. ``arithmetic_uncertainty`` must also accept different
    arguments: ``operation``, ``operand``, ``result``, ``correlation``,
    ``**kwargs``.

..
  EXAMPLE END

Changing the Default Argument for Arithmetic Operations
-------------------------------------------------------

If the goal is to change the default value of an existing parameter for
arithmetic methods, such as when explicitly specifying the parameter each
time you call an arithmetic operation is too much effort, you can change the
default value of existing parameters by changing it in the method signature of
``_arithmetic``.

Example
^^^^^^^

..
  EXAMPLE START
  Changing the Default Argument for Arithmetic Operations in NDData

To change the default value of an existing parameter for arithmetic methods::

    >>> from astropy.nddata import NDDataRef
    >>> import numpy as np

    >>> class NDDDiffAritDefaults(NDDataRef):
    ...     def _arithmetic(self, *args, **kwargs):
    ...         # Changing the default of handle_mask to None
    ...         if 'handle_mask' not in kwargs:
    ...             kwargs['handle_mask'] = None
    ...         # Call the original with the updated kwargs
    ...         return super()._arithmetic(*args, **kwargs)

    >>> ndd1 = NDDDiffAritDefaults(1, mask=False)
    >>> ndd2 = NDDDiffAritDefaults(1, mask=True)
    >>> ndd1.add(ndd2).mask is None  # it will be None
    True

    >>> # But giving other values is still possible:
    >>> ndd1.add(ndd2, handle_mask=np.logical_or).mask
    True

    >>> ndd1.add(ndd2, handle_mask="ff").mask
    False

The parameter controlling how properties are handled are all keyword-only
so using the ``*args``, ``**kwargs`` approach allows you to only alter one
default without needing to care about the positional order of arguments.

..
  EXAMPLE END

Arithmetic with an Additional Property
--------------------------------------

This also requires overriding the ``_arithmetic`` method. Suppose we have a
``flags`` attribute again::

    >>> from copy import deepcopy
    >>> import numpy as np

    >>> class NDDataWithFlags(NDDataRef):
    ...     def __init__(self, *args, **kwargs):
    ...         # Remove flags attribute if given and pass it to the setter.
    ...         self.flags = kwargs.pop('flags') if 'flags' in kwargs else None
    ...         super().__init__(*args, **kwargs)
    ...
    ...     @property
    ...     def flags(self):
    ...         return self._flags
    ...
    ...     @flags.setter
    ...     def flags(self, value):
    ...         self._flags = value
    ...
    ...     def _arithmetic(self, operation, operand, *args, **kwargs):
    ...         # take all args and kwargs to allow arithmetic on the other properties
    ...         # to work like before.
    ...
    ...         # do the arithmetics on the flags (pop the relevant kwargs, if any!!!)
    ...         if self.flags is not None and operand.flags is not None:
    ...             result_flags = np.logical_or(self.flags, operand.flags)
    ...             # np.logical_or is just a suggestion you can do what you want
    ...         else:
    ...             if self.flags is not None:
    ...                 result_flags = deepcopy(self.flags)
    ...             else:
    ...                 result_flags = deepcopy(operand.flags)
    ...
    ...         # Let the superclass do all the other attributes note that
    ...         # this returns the result and a dictionary containing other attributes
    ...         result, kwargs = super()._arithmetic(operation, operand, *args, **kwargs)
    ...         # The arguments for creating a new instance are saved in kwargs
    ...         # so we need to add another keyword "flags" and add the processed flags
    ...         kwargs['flags'] = result_flags
    ...         return result, kwargs # these must be returned

    >>> ndd1 = NDDataWithFlags([1,2,3], flags=np.array([1,0,1], dtype=bool))
    >>> ndd2 = NDDataWithFlags([1,2,3], flags=np.array([0,0,1], dtype=bool))
    >>> ndd3 = ndd1.add(ndd2)
    >>> ndd3.flags
    array([ True, False,  True]...)

Slicing an Existing Property
----------------------------

Suppose you have a class expecting a 2D ``data`` but the mask is
only 1D. This would lead to problems if you were to slice in two dimensions.

    >>> from astropy.nddata import NDDataRef
    >>> import numpy as np

    >>> class NDDataMask1D(NDDataRef):
    ...     def _slice_mask(self, item):
    ...         # Multidimensional slices are represented by tuples:
    ...         if isinstance(item, tuple):
    ...             # only use the first dimension of the slice
    ...             return self.mask[item[0]]
    ...         # Let the superclass deal with the other cases
    ...         return super()._slice_mask(item)

    >>> ndd = NDDataMask1D(np.ones((3,3)), mask=np.ones(3, dtype=bool))
    >>> nddsliced = ndd[1:3,1:3]
    >>> nddsliced.mask
    array([ True,  True]...)

.. note::
  The methods slicing the attributes are prefixed by a ``_slice_*`` where ``*``
  can be ``mask``, ``uncertainty``, or ``wcs``. So overriding them is the
  most convenient way to customize how the attributes are sliced.

.. note::
  If slicing should affect the ``unit`` or ``meta`` see the next example.


Slicing an Additional Property
------------------------------

Building on the added property ``flags``, we want them to be sliceable:

    >>> class NDDataWithFlags(NDDataRef):
    ...     def __init__(self, *args, **kwargs):
    ...         # Remove flags attribute if given and pass it to the setter.
    ...         self.flags = kwargs.pop('flags') if 'flags' in kwargs else None
    ...         super().__init__(*args, **kwargs)
    ...
    ...     @property
    ...     def flags(self):
    ...         return self._flags
    ...
    ...     @flags.setter
    ...     def flags(self, value):
    ...         self._flags = value
    ...
    ...     def _slice(self, item):
    ...         # slice all normal attributes
    ...         kwargs = super()._slice(item)
    ...         # The arguments for creating a new instance are saved in kwargs
    ...         # so we need to add another keyword "flags" and add the sliced flags
    ...         kwargs['flags'] = self.flags[item]
    ...         return kwargs # these must be returned

    >>> ndd = NDDataWithFlags([1,2,3], flags=[0, 0.2, 0.3])
    >>> ndd2 = ndd[1:3]
    >>> ndd2.flags
    [0.2, 0.3]

If you wanted to keep just the original ``flags`` instead of the sliced ones,
you could use ``kwargs['flags'] = self.flags`` and omit the ``[item]``.

`~astropy.nddata.NDDataBase`
============================

The class `~astropy.nddata.NDDataBase` is a metaclass  when subclassing it,
all properties of `~astropy.nddata.NDDataBase` *must* be overridden in the
subclass.

Subclassing from `~astropy.nddata.NDDataBase` gives you complete flexibility
in how you implement data storage and the other properties. If your data is
stored in a ``numpy`` array (or something that behaves like a ``numpy`` array),
it may be more convenient to subclass `~astropy.nddata.NDData` instead of
`~astropy.nddata.NDDataBase`.

Example
-------

..
  EXAMPLE START
  Implementing the NDDataBase Interface

To implement the NDDataBase interface by creating a read-only container::

    >>> from astropy.nddata import NDDataBase

    >>> class NDDataReadOnlyNoRestrictions(NDDataBase):
    ...     def __init__(self, data, unit, mask, uncertainty, meta, wcs):
    ...         self._data = data
    ...         self._unit = unit
    ...         self._mask = mask
    ...         self._uncertainty = uncertainty
    ...         self._meta = meta
    ...         self._wcs = wcs
    ...
    ...     @property
    ...     def data(self):
    ...         return self._data
    ...
    ...     @property
    ...     def unit(self):
    ...         return self._unit
    ...
    ...     @property
    ...     def mask(self):
    ...         return self._mask
    ...
    ...     @property
    ...     def uncertainty(self):
    ...         return self._uncertainty
    ...
    ...     @property
    ...     def meta(self):
    ...         return self._meta
    ...
    ...     @property
    ...     def wcs(self):
    ...         return self._wcs

    >>> # A meaningless test to show that creating this class is possible:
    >>> NDDataReadOnlyNoRestrictions(1,2,3,4,5,6) is not None
    True

.. note::
  Actually defining an ``__init__`` is not necessary and the properties could
  return arbitrary values but the properties **must** be defined.

..
  EXAMPLE END

Subclassing `~astropy.nddata.NDUncertainty`
===========================================

.. warning::
    The internal interface of NDUncertainty and subclasses is experimental and
    might change in future versions.

Subclasses deriving from `~astropy.nddata.NDUncertainty` need in order to
implement:

- Property ``uncertainty_type`` should return a string describing the
  uncertainty, for example, ``"ivar"`` for inverse variance.
- Methods for propagation: `_propagate_*` where ``*`` is the name of the
  universal function (ufunc) that is used on the ``NDData`` parent.

Creating an Uncertainty without Propagation
-------------------------------------------

`~astropy.nddata.UnknownUncertainty` is a minimal working implementation
without error propagation. We can create an uncertainty by storing
systematic uncertainties::

    >>> from astropy.nddata import NDUncertainty

    >>> class SystematicUncertainty(NDUncertainty):
    ...     @property
    ...     def uncertainty_type(self):
    ...         return 'systematic'
    ...
    ...     def _data_unit_to_uncertainty_unit(self, value):
    ...         return None
    ...
    ...     def _propagate_add(self, other_uncert, *args, **kwargs):
    ...         return None
    ...
    ...     def _propagate_subtract(self, other_uncert, *args, **kwargs):
    ...         return None
    ...
    ...     def _propagate_multiply(self, other_uncert, *args, **kwargs):
    ...         return None
    ...
    ...     def _propagate_divide(self, other_uncert, *args, **kwargs):
    ...         return None

    >>> SystematicUncertainty([10])
    SystematicUncertainty([10])
*********************************************
Decorating Functions to Accept NDData Objects
*********************************************

The `astropy.nddata` module includes a decorator
:func:`~astropy.nddata.support_nddata` that makes it convenient for developers
and users to write functions that can accept :class:`~astropy.nddata.NDData`
objects and also separate arguments.

Consider the following function::

    def test(data, wcs=None, unit=None, n_iterations=3):
        ...

Now say that we want to be able to call the function as ``test(nd)``
where ``nd`` is an :class:`~astropy.nddata.NDData` instance. We can decorate
this function using :func:`~astropy.nddata.support_nddata`::

    from astropy.nddata import support_nddata

    @support_nddata
    def test(data, wcs=None, unit=None, n_iterations=3):
        ...

Which makes it so that when the user calls ``test(nd)``, the function would
automatically be called with::

    test(nd.data, wcs=nd.wcs, unit=nd.unit)

The decorator looks at the signature of the function and checks if any
of the arguments are also properties of the ``NDData`` object, and passes them
as individual arguments. The function can also be called with separate
arguments as if it was not decorated.

A warning is emitted if an ``NDData`` property is set but the function does
not accept it  for example, if ``wcs`` is set, but the function cannot support
WCS objects. On the other hand, if an argument in the function does not exist
in the ``NDData`` object or is not set, it is left to its default value.

If the function call succeeds, then the decorator returns the values from the
function unmodified by default. However, in some cases we may want to return
separate ``data``, ``wcs``, etc. if these were passed in separately, and a new
:class:`~astropy.nddata.NDData` instance otherwise. To do this, you can specify
``repack=True`` in the decorator and provide a list of the names of the output
arguments from the function::

    @support_nddata(repack=True, returns=['data', 'wcs'])
    def test(data, wcs=None, unit=None, n_iterations=3):
        ...

With this, the function will return separate values if ``test`` is called with
separate arguments, and an object with the same class type as the input if the
input is an :class:`~astropy.nddata.NDData` or subclass instance.

Finally, the decorator can be made to restrict input to specific ``NDData``
subclasses (and the subclasses of those) using the ``accepts`` option::

    @support_nddata(accepts=CCDImage)
    def test(data, wcs=None, unit=None, n_iterations=3):
        ...
.. _nddata_details:

NDData
******

Overview
========

:class:`~astropy.nddata.NDData` is based on `numpy.ndarray`-like ``data`` with
additional meta attributes:

+  ``meta`` for general metadata
+ ``unit`` represents the physical unit of the data
+ ``uncertainty`` for the uncertainty of the data
+ ``mask`` indicates invalid points in the data
+ ``wcs`` represents the relationship between the data grid and world
  coordinates

Each of these attributes can be set during initialization or directly on the
instance. Only the ``data`` cannot be directly set after creating the instance.

Data
====

The data is the base of `~astropy.nddata.NDData` and is required to be
`numpy.ndarray`-like. It is the only property that is required to create an
instance and it cannot be directly set on the instance.

Example
-------

..
  EXAMPLE START
  Creating Instances with NumPy NDarray-like Data

To create an instance::

    >>> import numpy as np
    >>> from astropy.nddata import NDData
    >>> array = np.array([[0, 1, 0], [1, 0, 1], [0, 1, 0]])
    >>> ndd = NDData(array)
    >>> ndd
    NDData([[0, 1, 0],
            [1, 0, 1],
            [0, 1, 0]])

And access by the ``data`` attribute::

    >>> ndd.data
    array([[0, 1, 0],
           [1, 0, 1],
           [0, 1, 0]])

As already mentioned, it is not possible to set the data directly. So
``ndd.data = np.arange(9)`` will raise an exception. But the data can be
modified in place::

    >>> ndd.data[1,1] = 100
    >>> ndd.data
    array([[  0,   1,   0],
           [  1, 100,   1],
           [  0,   1,   0]])

..
  EXAMPLE END

Data During Initialization
--------------------------

During initialization it is possible to provide data that is not a
`numpy.ndarray` but convertible to one.

Examples
^^^^^^^^

..
  EXAMPLE START
  Data Convertible to a NumPy NDarray During Initialization

To provide data that is convertible to a `numpy.ndarray`, you can pass a `list`
containing numerical values::

    >>> alist = [1, 2, 3, 4]
    >>> ndd = NDData(alist)
    >>> ndd.data  # data will be a numpy-array:
    array([1, 2, 3, 4])

A nested `list` or `tuple` is possible, but if these contain non-numerical
values the conversion might fail.

Besides input that is convertible to such an array, you can also use the
``data`` parameter to pass implicit additional information. For example, if the
data is another `~astropy.nddata.NDData` object it implicitly uses its
properties::

    >>> ndd = NDData(ndd, unit = 'm')
    >>> ndd2 = NDData(ndd)
    >>> ndd2.data  # It has the same data as ndd
    array([1, 2, 3, 4])
    >>> ndd2.unit  # but it also has the same unit as ndd
    Unit("m")

Another possibility is to use a `~astropy.units.Quantity` as a ``data``
parameter::

    >>> import astropy.units as u
    >>> quantity = np.ones(3) * u.cm  # this will create a Quantity
    >>> ndd3 = NDData(quantity)
    >>> ndd3.data  # doctest: +FLOAT_CMP
    array([1., 1., 1.])
    >>> ndd3.unit
    Unit("cm")

Or a `numpy.ma.MaskedArray`::

    >>> masked_array = np.ma.array([5,10,15], mask=[False, True, False])
    >>> ndd4 = NDData(masked_array)
    >>> ndd4.data
    array([ 5, 10, 15])
    >>> ndd4.mask
    array([False,  True, False]...)

If such an implicitly passed property conflicts with an explicit parameter, the
explicit parameter will be used and an info message will be issued::

    >>> quantity = np.ones(3) * u.cm
    >>> ndd6 = NDData(quantity, unit='m')
    INFO: overwriting Quantity's current unit with specified unit. [astropy.nddata.nddata]
    >>> ndd6.data  # doctest: +FLOAT_CMP
    array([1., 1., 1.])
    >>> ndd6.unit
    Unit("m")

The unit of the `~astropy.units.Quantity` is being ignored and the unit is set
to the explicitly passed one.

It might be possible to pass other classes as a ``data`` parameter as long as
they have the properties ``shape``, ``dtype``, ``__getitem__``, and
``__array__``.

The purpose of this mechanism is to allow considerable flexibility in the
objects used to store the data while providing a useful default (``numpy``
array).

..
  EXAMPLE END

Mask
====

The ``mask`` is being used to indicate if data points are valid or invalid.
`~astropy.nddata.NDData` does not restrict this mask in any way but it is
expected to follow the `numpy.ma.MaskedArray` convention in that the mask:

+ Returns ``True`` for data points that are considered **invalid**.
+ Returns ``False`` for those points that are **valid**.

Examples
--------

..
  EXAMPLE START
  Masks Used to Indicate Valid or Invalid Data Points in NDData

One possibility is to create a mask by using ``numpy``'s comparison operators::

    >>> array = np.array([0, 1, 4, 0, 2])

    >>> mask = array == 0  # Mask points containing 0
    >>> mask
    array([ True, False, False,  True, False]...)

    >>> other_mask = array > 1  # Mask points with a value greater than 1
    >>> other_mask
    array([False, False,  True, False,  True]...)

And initialize the `~astropy.nddata.NDData` instance using the ``mask``
parameter::

    >>> ndd = NDData(array, mask=mask)
    >>> ndd.mask
    array([ True, False, False,  True, False]...)

Or by replacing the mask::

    >>> ndd.mask = other_mask
    >>> ndd.mask
    array([False, False,  True, False,  True]...)

There is no requirement that the mask actually be a ``numpy`` array; for
example, a function which evaluates a mask value as needed is acceptable as
long as it follows the convention that ``True`` indicates a value that should
be ignored.

..
  EXAMPLE END

Unit
====

The ``unit`` represents the unit of the data values. It is required to be
`~astropy.units.Unit`-like or a string that can be converted to such a
`~astropy.units.Unit`::

    >>> import astropy.units as u
    >>> ndd = NDData([1, 2, 3, 4], unit="meter")  # using a string
    >>> ndd.unit
    Unit("m")

..note::
    Setting the ``unit`` on an instance is not possible.

Uncertainties
=============

The ``uncertainty`` represents an arbitrary representation of the error of the
data values. To indicate which kind of uncertainty representation is used, the
``uncertainty`` should have an ``uncertainty_type`` property. If no such
property is found it will be wrapped inside a
`~astropy.nddata.UnknownUncertainty`.

The ``uncertainty_type`` should follow the `~astropy.nddata.StdDevUncertainty`
convention in that it returns a short string like ``"std"`` for an uncertainty
given in standard deviation. Other examples are
`~astropy.nddata.VarianceUncertainty` and `~astropy.nddata.InverseVariance`.

Examples
--------

..
  EXAMPLE START
  Setting Uncertainties During Initialization in NDData

Like the other properties the ``uncertainty`` can be set during
initialization::

    >>> from astropy.nddata import StdDevUncertainty
    >>> array = np.array([10, 7, 12, 22])
    >>> uncert = StdDevUncertainty(np.sqrt(array))
    >>> ndd = NDData(array, uncertainty=uncert)
    >>> ndd.uncertainty  # doctest: +FLOAT_CMP
    StdDevUncertainty([3.16227766, 2.64575131, 3.46410162, 4.69041576])

Or on the instance directly::

    >>> other_uncert = StdDevUncertainty([2,2,2,2])
    >>> ndd.uncertainty = other_uncert
    >>> ndd.uncertainty
    StdDevUncertainty([2, 2, 2, 2])

But it will print an info message if there is no ``uncertainty_type``::

    >>> ndd.uncertainty = np.array([5, 1, 2, 10])
    INFO: uncertainty should have attribute uncertainty_type. [astropy.nddata.nddata]
    >>> ndd.uncertainty
    UnknownUncertainty([ 5,  1,  2, 10])

..
  EXAMPLE END

WCS
---

The ``wcs`` should contain a mapping from the gridded data to world
coordinates. There are no restrictions placed on the property currently but it
may be restricted to an `~astropy.wcs.WCS` object or a more generalized WCS
object in the future.

.. note::
    Like the unit the ``wcs`` cannot be set on an instance.

Metadata
=========

The ``meta`` property contains all further meta information that does not fit
any other property.

Examples
--------

..
  EXAMPLE START
  Metadata in NDData

If the ``meta`` property is given it must be `dict`-like::

    >>> ndd = NDData([1,2,3], meta={'observer': 'myself'})
    >>> ndd.meta
    {'observer': 'myself'}

`dict`-like means it must be a mapping from some keys to some values. This
also includes `~astropy.io.fits.Header` objects::

    >>> from astropy.io import fits
    >>> header = fits.Header()
    >>> header['observer'] = 'Edwin Hubble'
    >>> ndd = NDData(np.zeros([10, 10]), meta=header)
    >>> ndd.meta['observer']
    'Edwin Hubble'

If the ``meta`` property is not provided or explicitly set to ``None``, it will
default to an empty `collections.OrderedDict`::

    >>> ndd.meta = None
    >>> ndd.meta
    OrderedDict()

    >>> ndd = NDData([1,2,3])
    >>> ndd.meta
    OrderedDict()

The ``meta`` object therefore supports adding or updating these values::

    >>> ndd.meta['exposure_time'] = 340.
    >>> ndd.meta['filter'] = 'J'

Elements of the metadata dictionary can be set to any valid Python object::

    >>> ndd.meta['history'] = ['calibrated', 'aligned', 'flat-fielded']

..
  EXAMPLE END

Initialization with Copy
========================

The default way to create an `~astropy.nddata.NDData` instance is to try saving
the parameters as references to the original rather than as copy. Sometimes
this is not possible because the internal mechanics do not allow for this.

Examples
--------

..
  EXAMPLE START
  Creating an NDData Instance with Copy

If the ``data`` is a `list` then during initialization this is copied
while converting to a `~numpy.ndarray`. But it is also possible to enforce
copies during initialization by setting the ``copy`` parameter to ``True``::

    >>> array = np.array([1, 2, 3, 4])
    >>> ndd = NDData(array)
    >>> ndd.data[2] = 10
    >>> array[2]  # Original array has changed
    10

    >>> ndd2 = NDData(array, copy=True)
    >>> ndd2.data[2] = 3
    >>> array[2]  # Original array hasn't changed.
    10

.. note::
    In some cases setting ``copy=True`` will copy the ``data`` twice. Known
    cases are if the ``data`` is a `list` or `tuple`.

..
  EXAMPLE END

Converting NDData to Other Classes
==================================

There is limited support to convert a `~astropy.nddata.NDData` instance to
other classes. In the process some properties might be lost.

    >>> data = np.array([1, 2, 3, 4])
    >>> mask = np.array([True, False, False, True])
    >>> unit = 'm'
    >>> ndd = NDData(data, mask=mask, unit=unit)

`numpy.ndarray`
---------------

Converting the ``data`` to an array::

    >>> array = np.asarray(ndd.data)
    >>> array
    array([1, 2, 3, 4])

Though using ``np.asarray`` is not required, in most cases it will ensure that
the result is always a `numpy.ndarray`

`numpy.ma.MaskedArray`
----------------------

Converting the ``data`` and ``mask`` to a MaskedArray::


    >>> masked_array = np.ma.array(ndd.data, mask=ndd.mask)
    >>> masked_array
    masked_array(data=[--, 2, 3, --],
                 mask=[ True, False, False,  True],
           fill_value=999999)

`~astropy.units.Quantity`
-------------------------

Converting the ``data`` and ``unit`` to a Quantity::

    >>> quantity = u.Quantity(ndd.data, unit=ndd.unit)
    >>> quantity  # doctest: +FLOAT_CMP
    <Quantity [1., 2., 3., 4.] m>

.. note::
    Ideally, you would construct masked quantities, but these are not properly
    supported: many operations on them fail.
.. note that if this is changed from the default approach of using an *include*
   (in index.rst) to a separate performance page, the header needs to be changed
   from === to ***, the filename extension needs to be changed from .inc.rst to
   .rst, and a link needs to be added in the subpackage toctree

.. _astropy-nddata-performance:

Performance Tips
================

+ Using the uncertainty class `~astropy.nddata.VarianceUncertainty` will
  be somewhat more efficient than the other two uncertainty classes,
  `~astropy.nddata.InverseVariance` and `~astropy.nddata.StdDevUncertainty`.
  The latter two are converted to variance for the purposes of error
  propagation and then converted from variance back to the original
  uncertainty type. The performance difference should be small.
+ When possible, mask values by setting them to ``np.nan`` and use the
  ``numpy`` functions and methods that automatically exclude ``np.nan``,
  like ``np.nanmedian`` and ``np.nanstd``. This will typically be much
  faster than using `numpy.ma.MaskedArray`.
.. _nddata_utils:

Image Utilities
***************

Overview
========

The `astropy.nddata.utils` module includes general utility functions
for array operations.

.. _cutout_images:

2D Cutout Images
================

Getting Started
---------------

The `~astropy.nddata.utils.Cutout2D` class can be used to create a
postage stamp cutout image from a 2D array. If an optional
`~astropy.wcs.WCS` object is input to
`~astropy.nddata.utils.Cutout2D`, then the
`~astropy.nddata.utils.Cutout2D` object will contain an updated
`~astropy.wcs.WCS` corresponding to the cutout array.

First, we simulate a single source on a 2D data array. If you would like to
simulate many sources, see :ref:`bounding-boxes`.

Note: The pair convention is different for **size** and **position**! The
position is specified as (x,y), but the size is specified as (y,x).

    >>> import numpy as np
    >>> from astropy.modeling.models import Gaussian2D
    >>> y, x = np.mgrid[0:500, 0:500]
    >>> data = Gaussian2D(1, 50, 100, 10, 5, theta=0.5)(x, y)

Now, we can display the image:

.. doctest-skip::

    >>> import matplotlib.pyplot as plt
    >>> plt.imshow(data, origin='lower')

.. plot::

    import numpy as np
    import matplotlib.pyplot as plt
    from astropy.modeling.models import Gaussian2D
    y, x = np.mgrid[0:500, 0:500]
    data = Gaussian2D(1, 50, 100, 10, 5, theta=0.5)(x, y)
    plt.imshow(data, origin='lower')

Next we can create a cutout for the single object in this image. We
create a cutout centered at position ``(x, y) = (49.7, 100.1)`` with a
size of ``(ny, nx) = (41, 51)`` pixels::

    >>> from astropy.nddata import Cutout2D
    >>> from astropy import units as u
    >>> position = (49.7, 100.1)
    >>> size = (41, 51)     # pixels
    >>> cutout = Cutout2D(data, position, size)

The ``size`` keyword can also be a `~astropy.units.Quantity` object::

    >>> size = u.Quantity((41, 51), u.pixel)
    >>> cutout = Cutout2D(data, position, size)

Or contain `~astropy.units.Quantity` objects::

    >>> size = (41*u.pixel, 51*u.pixel)
    >>> cutout = Cutout2D(data, position, size)

A square cutout image can be generated by passing an integer or
a scalar `~astropy.units.Quantity`::

    >>> size = 41
    >>> cutout2 = Cutout2D(data, position, size)

    >>> size = 41 * u.pixel
    >>> cutout2 = Cutout2D(data, position, size)

The cutout array is stored in the ``data`` attribute of the
`~astropy.nddata.utils.Cutout2D` instance. If the ``copy`` keyword is
`False` (default), then ``cutout.data`` will be a view into the
original ``data`` array. If ``copy=True``, then ``cutout.data`` will
hold a copy of the original ``data``. Now we display the cutout
image:

.. doctest-skip::

    >>> cutout = Cutout2D(data, position, (41, 51))
    >>> plt.imshow(cutout.data, origin='lower')

.. plot::

    import numpy as np
    import matplotlib.pyplot as plt
    from astropy.modeling.models import Gaussian2D
    from astropy.nddata import Cutout2D
    y, x = np.mgrid[0:500, 0:500]
    data = Gaussian2D(1, 50, 100, 10, 5, theta=0.5)(x, y)
    position = (49.7, 100.1)
    cutout = Cutout2D(data, position, (41, 51))
    plt.imshow(cutout.data, origin='lower')

The cutout object can plot its bounding box on the original data using
the :meth:`~astropy.nddata.utils.Cutout2D.plot_on_original` method:

.. doctest-skip::

    >>> plt.imshow(data, origin='lower')
    >>> cutout.plot_on_original(color='white')

.. plot::

    import numpy as np
    import matplotlib.pyplot as plt
    from astropy.modeling.models import Gaussian2D
    from astropy.nddata import Cutout2D
    y, x = np.mgrid[0:500, 0:500]
    data = Gaussian2D(1, 50, 100, 10, 5, theta=0.5)(x, y)
    position = (49.7, 100.1)
    size = (41, 51)
    cutout = Cutout2D(data, position, size)
    plt.imshow(data, origin='lower')
    cutout.plot_on_original(color='white')

Many properties of the cutout array are also stored as attributes,
including::

    >>> # shape of the cutout array
    >>> print(cutout.shape)
    (41, 51)

    >>> # rounded pixel index of the input position
    >>> print(cutout.position_original)
    (50, 100)

    >>> # corresponding position in the cutout array
    >>> print(cutout.position_cutout)
    (25, 20)

    >>> # (non-rounded) input position in both the original and cutout arrays
    >>> print((cutout.input_position_original, cutout.input_position_cutout))  # doctest: +FLOAT_CMP
    ((49.7, 100.1), (24.700000000000003, 20.099999999999994))

    >>> # the origin pixel in both arrays
    >>> print((cutout.origin_original, cutout.origin_cutout))
    ((25, 80), (0, 0))

    >>> # tuple of slice objects for the original array
    >>> print(cutout.slices_original)
    (slice(80, 121, None), slice(25, 76, None))

    >>> # tuple of slice objects for the cutout array
    >>> print(cutout.slices_cutout)
    (slice(0, 41, None), slice(0, 51, None))

There are also two `~astropy.nddata.utils.Cutout2D` methods to convert
pixel positions between the original and cutout arrays::

    >>> print(cutout.to_original_position((2, 1)))
    (27, 81)

    >>> print(cutout.to_cutout_position((27, 81)))
    (2, 1)


2D Cutout Modes
---------------

There are three modes for creating cutout arrays: ``'trim'``,
``'partial'``, and ``'strict'``. For the ``'partial'`` and ``'trim'``
modes, a partial overlap of the cutout array and the input ``data``
array is sufficient. For the ``'strict'`` mode, the cutout array has
to be fully contained within the ``data`` array, otherwise an
`~astropy.nddata.utils.PartialOverlapError` is raised. In all modes,
non-overlapping arrays will raise a
`~astropy.nddata.utils.NoOverlapError`. In ``'partial'`` mode,
positions in the cutout array that do not overlap with the ``data``
array will be filled with ``fill_value``. In ``'trim'`` mode only the
overlapping elements are returned, thus the resulting cutout array may
be smaller than the requested ``size``.

The default uses ``mode='trim'``, which can result in cutout arrays
that are smaller than the requested ``size``::

    >>> data2 = np.arange(20.).reshape(5, 4)
    >>> cutout1 = Cutout2D(data2, (0, 0), (3, 3), mode='trim')
    >>> print(cutout1.data)  # doctest: +FLOAT_CMP
    [[0. 1.]
     [4. 5.]]
    >>> print(cutout1.shape)
    (2, 2)
    >>> print((cutout1.position_original, cutout1.position_cutout))
    ((0, 0), (0, 0))

With ``mode='partial'``, the cutout will never be trimmed. Instead it
will be filled with ``fill_value`` (the default is ``numpy.nan``) if
the cutout is not fully contained in the data array::

    >>> cutout2 = Cutout2D(data2, (0, 0), (3, 3), mode='partial')
    >>> print(cutout2.data)  # doctest: +FLOAT_CMP
    [[nan nan nan]
     [nan  0.  1.]
     [nan  4.  5.]]

Note that for the ``'partial'`` mode, the positions (and several other
attributes) are calculated for on the *valid* (non-filled) cutout
values::

    >>> print((cutout2.position_original, cutout2.position_cutout))
    ((0, 0), (1, 1))
    >>> print((cutout2.origin_original, cutout2.origin_cutout))
    ((0, 0), (1, 1))
    >>> print(cutout2.slices_original)
    (slice(0, 2, None), slice(0, 2, None))
    >>> print(cutout2.slices_cutout)
    (slice(1, 3, None), slice(1, 3, None))

Using ``mode='strict'`` will raise an exception if the cutout is not
fully contained in the data array:

.. doctest-skip::

    >>> cutout3 = Cutout2D(data2, (0, 0), (3, 3), mode='strict')
    PartialOverlapError: Arrays overlap only partially.


2D Cutout from a `~astropy.coordinates.SkyCoord` Position
---------------------------------------------------------

The input ``position`` can also be specified as a
`~astropy.coordinates.SkyCoord`, in which case a `~astropy.wcs.WCS`
object must be input via the ``wcs`` keyword.

First, we define a `~astropy.coordinates.SkyCoord` position and a
`~astropy.wcs.WCS` object for our data (usually this would come from
your FITS header)::

    >>> from astropy.coordinates import SkyCoord
    >>> from astropy.wcs import WCS
    >>> position = SkyCoord('13h11m29.96s -01d19m18.7s', frame='icrs')
    >>> wcs = WCS(naxis=2)
    >>> rho = np.pi / 3.
    >>> scale = 0.05 / 3600.
    >>> wcs.wcs.cd = [[scale*np.cos(rho), -scale*np.sin(rho)],
    ...               [scale*np.sin(rho), scale*np.cos(rho)]]
    >>> wcs.wcs.ctype = ['RA---TAN', 'DEC--TAN']
    >>> wcs.wcs.crval = [position.ra.to_value(u.deg),
    ...                  position.dec.to_value(u.deg)]
    >>> wcs.wcs.crpix = [50, 100]

Now we can create the cutout array using the
`~astropy.coordinates.SkyCoord` position and ``wcs`` object::

    >>> cutout = Cutout2D(data, position, (30, 40), wcs=wcs)
    >>> plt.imshow(cutout.data, origin='lower')   # doctest: +SKIP

.. plot::

    import numpy as np
    import matplotlib.pyplot as plt
    from astropy.modeling.models import Gaussian2D
    from astropy.nddata import Cutout2D
    from astropy.coordinates import SkyCoord
    from astropy.wcs import WCS
    y, x = np.mgrid[0:500, 0:500]
    data = Gaussian2D(1, 50, 100, 10, 5, theta=0.5)(x, y)
    position = SkyCoord('13h11m29.96s -01d19m18.7s', frame='icrs')
    wcs = WCS(naxis=2)
    rho = np.pi / 3.
    scale = 0.05 / 3600.
    wcs.wcs.cd = [[scale*np.cos(rho), -scale*np.sin(rho)],
                  [scale*np.sin(rho), scale*np.cos(rho)]]
    wcs.wcs.ctype = ['RA---TAN', 'DEC--TAN']
    wcs.wcs.crval = [position.ra.value, position.dec.value]
    wcs.wcs.crpix = [50, 100]
    cutout = Cutout2D(data, position, (30, 40), wcs=wcs)
    plt.imshow(cutout.data, origin='lower')

The ``wcs`` attribute of the `~astropy.nddata.utils.Cutout2D` object now
contains the propagated `~astropy.wcs.WCS` for the cutout array.
Now we can find the sky coordinates for a given pixel in the cutout array.
Note that we need to use the ``cutout.wcs`` object for the cutout
positions::

    >>> from astropy.wcs.utils import pixel_to_skycoord
    >>> x_cutout, y_cutout = (5, 10)
    >>> pixel_to_skycoord(x_cutout, y_cutout, cutout.wcs)    # doctest: +FLOAT_CMP
    <SkyCoord (ICRS): (ra, dec) in deg
        ( 197.8747893, -1.32207626)>

We now find the corresponding pixel in the original ``data`` array and
its sky coordinates::

    >>> x_data, y_data = cutout.to_original_position((x_cutout, y_cutout))
    >>> pixel_to_skycoord(x_data, y_data, wcs)    # doctest: +FLOAT_CMP
    <SkyCoord (ICRS): (ra, dec) in deg
        ( 197.8747893, -1.32207626)>

As expected, the sky coordinates in the original ``data`` and the
cutout array agree.


2D Cutout Using an Angular ``size``
-----------------------------------

The input ``size`` can also be specified as a
`~astropy.units.Quantity` in angular units (e.g., degrees, arcminutes,
arcseconds, etc.). For this case, a `~astropy.wcs.WCS` object must be
input via the ``wcs`` keyword.

For this example, we will use the data, `~astropy.coordinates.SkyCoord`
position, and ``wcs`` object from above to create a cutout with size
1.5 x 2.5 arcseconds::

    >>> size = u.Quantity((1.5, 2.5), u.arcsec)
    >>> cutout = Cutout2D(data, position, size, wcs=wcs)
    >>> plt.imshow(cutout.data, origin='lower')   # doctest: +SKIP

.. plot::

    import numpy as np
    import matplotlib.pyplot as plt
    from astropy.modeling.models import Gaussian2D
    from astropy.nddata import Cutout2D
    from astropy.coordinates import SkyCoord
    from astropy.wcs import WCS
    from astropy import units as u
    y, x = np.mgrid[0:500, 0:500]
    data = Gaussian2D(1, 50, 100, 10, 5, theta=0.5)(x, y)
    position = SkyCoord('13h11m29.96s -01d19m18.7s', frame='icrs')
    wcs = WCS(naxis=2)
    rho = np.pi / 3.
    scale = 0.05 / 3600.
    wcs.wcs.cd = [[scale*np.cos(rho), -scale*np.sin(rho)],
                  [scale*np.sin(rho), scale*np.cos(rho)]]
    wcs.wcs.ctype = ['RA---TAN', 'DEC--TAN']
    wcs.wcs.crval = [position.ra.value, position.dec.value]
    wcs.wcs.crpix = [50, 100]
    size = u.Quantity((1.5, 2.5), u.arcsec)
    cutout = Cutout2D(data, position, size, wcs=wcs)
    plt.imshow(cutout.data, origin='lower')


Saving a 2D Cutout to a FITS File with an Updated WCS
=====================================================

A `~astropy.nddata.utils.Cutout2D` object can be saved to a FITS file,
including the updated WCS object for the cutout region. In this example, we
download an example FITS image and create a cutout image. The resulting
`~astropy.nddata.utils.Cutout2D` object is then saved to a new FITS file with
the updated WCS for the cutout region.

.. literalinclude:: examples/cutout2d_tofits.py
   :language: python
.. _astropy_nddata:

*****************************************
N-Dimensional Datasets (`astropy.nddata`)
*****************************************

Introduction
============

The `~astropy.nddata` package provides classes to represent images and other
gridded data, some essential functions for manipulating images, and the
infrastructure for package developers who wish to include support for the
image classes. This subpackage was developed based on `APE 7`_.

.. _astropy_nddata_getting_started:

Getting Started
===============

NDData
------

The primary purpose of `~astropy.nddata.NDData` is to act as a *container* for
data, metadata, and other related information like a mask.

An `~astropy.nddata.NDData` object can be instantiated by passing it an
n-dimensional `numpy` array::

    >>> import numpy as np
    >>> from astropy.nddata import NDData
    >>> array = np.zeros((12, 12, 12))  # a 3-dimensional array with all zeros
    >>> ndd1 = NDData(array)

Or something that can be converted to a `numpy.ndarray`::

    >>> ndd2 = NDData([1, 2, 3, 4])
    >>> ndd2
    NDData([1, 2, 3, 4])

And can be accessed again via the ``data`` attribute::

    >>> ndd2.data
    array([1, 2, 3, 4])

It also supports additional properties like a ``unit`` or ``mask`` for the
data, a ``wcs`` (World Coordinate System) and ``uncertainty`` of the data and
additional ``meta`` attributes:

    >>> data = np.array([1,2,3,4])
    >>> mask = data > 2
    >>> unit = 'erg / s'
    >>> from astropy.nddata import StdDevUncertainty
    >>> uncertainty = StdDevUncertainty(np.sqrt(data)) # representing standard deviation
    >>> meta = {'object': 'fictional data.'}
    >>> ndd = NDData(data, mask=mask, unit=unit, uncertainty=uncertainty,
    ...              meta=meta)
    >>> ndd
    NDData([1, 2, 3, 4], unit='erg / s')

The representation only displays the ``data``; the other attributes need to be
accessed directly, for example, ``ndd.mask`` to access the mask.


NDDataRef
---------

Building upon this pure container, `~astropy.nddata.NDDataRef` implements:

+ A ``read`` and ``write`` method to access ``astropy``'s unified file I/O
  interface.
+ Simple arithmetics like addition, subtraction, division, and multiplication.
+ Slicing.

Instances are created in the same way::

    >>> from astropy.nddata import NDDataRef
    >>> ndd = NDDataRef(ndd)
    >>> ndd
    NDDataRef([1, 2, 3, 4], unit='erg / s')

But also support arithmetic (:ref:`nddata_arithmetic`) like addition::

    >>> import astropy.units as u
    >>> ndd2 = ndd.add([4, -3.5, 3, 2.5] * u.erg / u.s)
    >>> ndd2
    NDDataRef([ 5. , -1.5,  6. ,  6.5], unit='erg / s')

Because these operations have a wide range of options, these are not available
using arithmetic operators like ``+``.

Slicing or indexing (:ref:`nddata_slicing`) is possible (with warnings issued if
some attribute cannot be sliced)::

    >>> ndd2[2:]  # discard the first two elements  # doctest: +FLOAT_CMP
    NDDataRef([6. , 6.5], unit='erg / s')
    >>> ndd2[1]   # get the second element  # doctest: +FLOAT_CMP
    NDDataRef(-1.5, unit='erg / s')


Working with Two-Dimensional Data Like Images
---------------------------------------------

Though the `~astropy.nddata` package supports any kind of gridded data, this
introduction will focus on the use of `~astropy.nddata` for two-dimensional
images. To get started, we will construct a two-dimensional image with a few
sources, some Gaussian noise, and a "cosmic ray" which we will later mask out.

Examples
^^^^^^^^

..
  EXAMPLE START
  Working with Two-Dimensional Data Using NDData

First, construct a two-dimensional image with a few sources, some Gaussian
noise, and a "cosmic ray"::

    >>> import numpy as np
    >>> from astropy.modeling.models import Gaussian2D
    >>> y, x = np.mgrid[0:500, 0:600]
    >>> data = (Gaussian2D(1, 150, 100, 20, 10, theta=0.5)(x, y) +
    ...         Gaussian2D(0.5, 400, 300, 8, 12, theta=1.2)(x,y) +
    ...         Gaussian2D(0.75, 250, 400, 5, 7, theta=0.23)(x,y) +
    ...         Gaussian2D(0.9, 525, 150, 3, 3)(x,y) +
    ...         Gaussian2D(0.6, 200, 225, 3, 3)(x,y))
    >>> data += 0.01 * np.random.randn(500, 600)
    >>> cosmic_ray_value = 0.997
    >>> data[100, 300:310] = cosmic_ray_value

This image has a large "galaxy" in the lower left and the "cosmic ray" is the
horizontal line in the lower middle of the image:

.. doctest-skip::

    >>> import matplotlib.pyplot as plt
    >>> plt.imshow(data, origin='lower')

.. plot::

    import numpy as np
    import matplotlib.pyplot as plt
    from astropy.modeling.models import Gaussian2D
    y, x = np.mgrid[0:500, 0:600]
    data = (Gaussian2D(1, 150, 100, 20, 10, theta=0.5)(x, y) +
            Gaussian2D(0.5, 400, 300, 8, 12, theta=1.2)(x,y) +
            Gaussian2D(0.75, 250, 400, 5, 7, theta=0.23)(x,y) +
            Gaussian2D(0.9, 525, 150, 3, 3)(x,y) +
            Gaussian2D(0.6, 200, 225, 3, 3)(x,y))
    np.random.seed(123456)
    data += 0.01 * np.random.randn(500, 600)
    cosmic_ray_value = 0.997
    data[100, 300:310] = cosmic_ray_value
    plt.imshow(data, origin='lower')


The "cosmic ray" can be masked out in this test image, like this::

    >>> mask = (data == cosmic_ray_value)

..
  EXAMPLE END

`~astropy.nddata.CCDData` Class for Images
------------------------------------------

The `~astropy.nddata.CCDData` object, like the other objects in this package,
can store the data, a mask, and metadata. The `~astropy.nddata.CCDData` object
requires that a unit be specified::

    >>> from astropy.nddata import CCDData
    >>> ccd = CCDData(data, mask=mask,
    ...               meta={'object': 'fake galaxy', 'filter': 'R'},
    ...               unit='adu')

Slicing
-------

Slicing works the way you would expect with the mask and, if present,
WCS, sliced appropriately::

    >>> ccd2 = ccd[:200, :]
    >>> ccd2.data.shape
    (200, 600)
    >>> ccd2.mask.shape
    (200, 600)
    >>> # Show the mask in a region around the cosmic ray:
    >>> ccd2.mask[99:102, 299:311]
    array([[False, False, False, False, False, False, False, False, False,
            False, False, False],
           [False,  True,  True,  True,  True,  True,  True,  True,  True,
             True,  True, False],
           [False, False, False, False, False, False, False, False, False,
            False, False, False]]...)

For many applications it may be more convenient to use
`~astropy.nddata.Cutout2D`, described in `image_utilities`_.

Image Arithmetic, Including Uncertainty
---------------------------------------

Methods are provided for basic arithmetic operations between images, including
propagation of uncertainties. Three uncertainty types are supported: variance
(`~astropy.nddata.VarianceUncertainty`), standard deviation
(`~astropy.nddata.StdDevUncertainty`), and inverse variance
(`~astropy.nddata.InverseVariance`).

Examples
^^^^^^^^

..
  EXAMPLE START
  Image Arithmetic Including Uncertainty in NDData

This example creates an uncertainty that is Poisson error, stored as a
variance::

    >>> from astropy.nddata import VarianceUncertainty
    >>> poisson_noise = np.ma.sqrt(np.ma.abs(ccd.data))
    >>> ccd.uncertainty = VarianceUncertainty(poisson_noise ** 2)

As a convenience, the uncertainty can also be set with a ``numpy`` array. In
that case, the uncertainty is assumed to be the standard deviation::

    >>> ccd.uncertainty = poisson_noise
    INFO: array provided for uncertainty; assuming it is a StdDevUncertainty. [astropy.nddata.ccddata]

If we make a copy of the image and add that to the original, the uncertainty
changes as expected::

    >>> ccd2 = ccd.copy()
    >>> added_ccds = ccd.add(ccd2, handle_meta='first_found')
    >>> added_ccds.uncertainty.array[0, 0] / ccd.uncertainty.array[0, 0] / np.sqrt(2) # doctest: +FLOAT_CMP
    0.99999999999999989

..
  EXAMPLE END

Reading and Writing
-------------------

A `~astropy.nddata.CCDData` can be saved to a FITS file::

    >>> ccd.write('test_file.fits')

And can also be read in from a FITS file::

    >>> ccd2 = CCDData.read('test_file.fits')

Note the unit is stored in the ``BUNIT`` keyword in the header on saving, and is
read from the header if it is present.

Detailed help on the available keyword arguments for reading and writing
can be obtained via the ``help()`` method as follows:

.. doctest-skip::

    >>> CCDData.read.help('fits')  # Get help on the CCDData FITS reader
    >>> CCDData.writer.help('fits')  # Get help on the CCDData FITS writer

.. _image_utilities:

Image Utilities
---------------

Cutouts
^^^^^^^

Though slicing directly is one way to extract a subframe,
`~astropy.nddata.Cutout2D` provides more convenient access to cutouts from the
data.

Examples
~~~~~~~~

..
  EXAMPLE START
  Accessing Cutouts in NDData

This example pulls out the large "galaxy" in the lower left of the image, with
the center of the cutout at ``position``::

    >>> from astropy.nddata import Cutout2D
    >>> position = (149.7, 100.1)
    >>> size = (81, 101)     # pixels
    >>> cutout = Cutout2D(ccd, position, size)
    >>> plt.imshow(cutout.data, origin='lower') # doctest: +SKIP

.. plot::

    import numpy as np
    import matplotlib.pyplot as plt
    from astropy.modeling.models import Gaussian2D
    from astropy.nddata import CCDData
    from astropy.nddata import Cutout2D
    y, x = np.mgrid[0:500, 0:600]
    data = (Gaussian2D(1, 150, 100, 20, 10, theta=0.5)(x, y) +
            Gaussian2D(0.5, 400, 300, 8, 12, theta=1.2)(x,y) +
            Gaussian2D(0.75, 250, 400, 5, 7, theta=0.23)(x,y) +
            Gaussian2D(0.9, 525, 150, 3, 3)(x,y) +
            Gaussian2D(0.6, 200, 225, 3, 3)(x,y))
    np.random.seed(123456)
    data += 0.01 * np.random.randn(500, 600)
    cosmic_ray_value = 0.997
    data[100, 300:310] = cosmic_ray_value
    mask = (data == cosmic_ray_value)
    ccd = CCDData(data, mask=mask,
                  meta={'object': 'fake galaxy', 'filter': 'R'},
                  unit='adu')
    position = (149.7, 100.1)
    size = (81, 101)     # pixels
    cutout = Cutout2D(ccd, position, size)
    plt.imshow(cutout.data, origin='lower')

This cutout can also plot itself on the original image::

    >>> plt.imshow(ccd, origin='lower')  # doctest: +SKIP
    >>> cutout.plot_on_original(color='white') # doctest: +SKIP

.. plot::

    import numpy as np
    import matplotlib.pyplot as plt
    from astropy.modeling.models import Gaussian2D
    from astropy.nddata import CCDData, Cutout2D
    y, x = np.mgrid[0:500, 0:600]
    data = (Gaussian2D(1, 150, 100, 20, 10, theta=0.5)(x, y) +
            Gaussian2D(0.5, 400, 300, 8, 12, theta=1.2)(x,y) +
            Gaussian2D(0.75, 250, 400, 5, 7, theta=0.23)(x,y) +
            Gaussian2D(0.9, 525, 150, 3, 3)(x,y) +
            Gaussian2D(0.6, 200, 225, 3, 3)(x,y))
    np.random.seed(123456)
    data += 0.01 * np.random.randn(500, 600)
    cosmic_ray_value = 0.997
    data[100, 300:310] = cosmic_ray_value
    mask = (data == cosmic_ray_value)
    ccd = CCDData(data, mask=mask,
                  meta={'object': 'fake galaxy', 'filter': 'R'},
                  unit='adu')
    position = (149.7, 100.1)
    size = (81, 101)     # pixels
    cutout = Cutout2D(ccd, position, size)
    plt.imshow(ccd, origin='lower')
    cutout.plot_on_original(color='white')

The cutout also provides methods for finding pixel coordinates in the original
or in the cutout; recall that ``position`` is the center of the cutout in the
original image::

    >>> position
    (149.7, 100.1)
    >>> cutout.to_cutout_position(position)  # doctest: +FLOAT_CMP
    (49.7, 40.099999999999994)
    >>> cutout.to_original_position((49.7, 40.099999999999994))  # doctest: +FLOAT_CMP
     (149.7, 100.1)

For more details, including constructing a cutout from World Coordinates and
the options for handling cutouts that go beyond the bounds of the original
image, see :ref:`cutout_images`.

..
  EXAMPLE END

Image Resizing
^^^^^^^^^^^^^^

The functions `~astropy.nddata.block_reduce` and
`~astropy.nddata.block_replicate` resize images.

Example
~~~~~~~

..
  EXAMPLE START
  Image Resizing in NDData

This example reduces the size of the image by a factor of 4. Note that the
result is a `numpy.ndarray`; the mask, metadata, etc. are discarded:

.. doctest-requires:: skimage

    >>> from astropy.nddata import block_reduce, block_replicate
    >>> smaller = block_reduce(ccd, 4)  # doctest: +IGNORE_WARNINGS
    >>> smaller
    array(...)
    >>> plt.imshow(smaller, origin='lower')  # doctest: +SKIP

.. plot::

    import numpy as np
    import matplotlib.pyplot as plt
    from astropy.modeling.models import Gaussian2D
    from astropy.nddata import block_reduce, block_replicate
    from astropy.nddata import CCDData, Cutout2D
    y, x = np.mgrid[0:500, 0:600]
    data = (Gaussian2D(1, 150, 100, 20, 10, theta=0.5)(x, y) +
            Gaussian2D(0.5, 400, 300, 8, 12, theta=1.2)(x,y) +
            Gaussian2D(0.75, 250, 400, 5, 7, theta=0.23)(x,y) +
            Gaussian2D(0.9, 525, 150, 3, 3)(x,y) +
            Gaussian2D(0.6, 200, 225, 3, 3)(x,y))
    np.random.seed(123456)
    data += 0.01 * np.random.randn(500, 600)
    cosmic_ray_value = 0.997
    data[100, 300:310] = cosmic_ray_value
    mask = (data == cosmic_ray_value)
    ccd = CCDData(data, mask=mask,
                  meta={'object': 'fake galaxy', 'filter': 'R'},
                  unit='adu')
    smaller = block_reduce(ccd.data, 4)
    plt.imshow(smaller, origin='lower')

By default, both `~astropy.nddata.block_reduce` and
`~astropy.nddata.block_replicate` conserve flux.

..
  EXAMPLE END

Other Image Classes
-------------------

There are two less restrictive classes, `~astropy.nddata.NDDataArray` and
`~astropy.nddata.NDDataRef`, that can be used to hold image data. They are
primarily of interest to those who may want to create their own image class by
subclassing from one of the classes in the `~astropy.nddata` package. The main
differences between them are:

+ `~astropy.nddata.NDDataRef` can be sliced and has methods for basic
  arithmetic operations, but the user needs to use one of the uncertainty
  classes to define an uncertainty. See :ref:`NDDataRef` for more detail.
  Most of its properties must be set when the object is created because they
  are not mutable.
+ `~astropy.nddata.NDDataArray` extends `~astropy.nddata.NDDataRef` by adding
  the methods necessary for it to behave like a ``numpy`` array in expressions
  and adds setters for several properties. It lacks the ability to
  automatically recognize and read data from FITS files and does not attempt
  to automatically set the WCS property.
+ `~astropy.nddata.CCDData` extends `~astropy.nddata.NDDataArray` by setting
  up a default uncertainty class, setting up straightforward read/write to FITS
  files, and automatically setting up a WCS property.

More General Gridded Data Classes
---------------------------------

There are two additional classes in the ``nddata`` package that are of
interest primarily to users who either need a custom image class that goes
beyond the classes discussed so far, or who are working with gridded data that
is not an image.

+ `~astropy.nddata.NDData` is a container class for holding general gridded
  data. It includes a handful of basic attributes, but no slicing or arithmetic.
  More information about this class is in :ref:`nddata_details`.
+ `~astropy.nddata.NDDataBase` is an abstract base class that developers of new
  gridded data classes can subclass to declare that the new class follows the
  `~astropy.nddata.NDData` interface. More details are in
  :ref:`nddata_subclassing`.

Additional Examples
===================

The list of packages below that use the ``nddata`` framework is intended to be
useful to either users writing their own image classes or those looking
for an image class that goes beyond what `~astropy.nddata.CCDData` does.

+ The `SunPy project <https://sunpy.org/>`_ uses `~astropy.nddata.NDData` as the
  foundation for its
  `Map classes <https://docs.sunpy.org/en/stable/code_ref/map.html>`_.
+ The class `~astropy.nddata.NDDataRef` is used in
  `specutils <https://specutils.readthedocs.io/en/latest/>`_ as the basis for
  `Spectrum1D <https://specutils.readthedocs.io/en/latest/api/specutils.Spectrum1D.html>`_, which adds several methods useful for
  spectra.
+ The package `ndmapper <https://ndmapper.readthedocs.io/en/latest/>`_, which
  makes it easy to build reduction pipelines for optical data, uses
  `~astropy.nddata.NDDataArray` as its image object.
+ The package `ccdproc <https://ccdproc.readthedocs.io/en/latest/>`_ uses the
  `~astropy.nddata.CCDData` class throughout for implementing optical/IR image
  reduction.

Using ``nddata``
================

.. toctree::
   :maxdepth: 2

   ccddata.rst
   utils.rst
   bitmask.rst
   decorator.rst
   nddata.rst
   mixins/index.rst
   subclassing.rst

.. note that if this section gets too long, it should be moved to a separate
   doc page - see the top of performance.inc.rst for the instructions on how to do
   that
.. include:: performance.inc.rst

Reference/API
=============

.. automodapi:: astropy.nddata
    :no-inheritance-diagram:

.. automodapi:: astropy.nddata.bitmask
    :no-inheritance-diagram:

.. automodapi:: astropy.nddata.utils
    :no-inheritance-diagram:

.. _APE 7: https://github.com/astropy/astropy-APEs/blob/main/APE7.rst
.. _ccddata:


CCDData Class
=============

Getting Started
---------------

Getting Data In
+++++++++++++++

Creating a `~astropy.nddata.CCDData` object from any array-like data using
`astropy.nddata` is convenient:

    >>> import numpy as np
    >>> from astropy.nddata import CCDData
    >>> ccd = CCDData(np.arange(10), unit="adu")

Note that behind the scenes, this creates references to (not copies of) your
data when possible, so modifying the data in ``ccd`` will modify the
underlying data.

You are **required** to provide a unit for your data. The most frequently used
units for these objects are likely to be ``adu``, ``photon``, and ``electron``,
which can be set either by providing the string name of the unit (as in the
example above) or from unit objects:

    >>> from astropy import units as u
    >>> ccd_photon = CCDData([1, 2, 3], unit=u.photon)
    >>> ccd_electron = CCDData([1, 2, 3], unit="electron")

If you prefer *not* to use the unit functionality, then use the special unit
``u.dimensionless_unscaled`` when you create your `~astropy.nddata.CCDData`
images:

    >>> ccd_unitless = CCDData(np.zeros((10, 10)),
    ...                        unit=u.dimensionless_unscaled)

A `~astropy.nddata.CCDData` object can also be initialized from a FITS filename
or URL:

    >>> ccd = CCDData.read('my_file.fits', unit="adu")  # doctest: +SKIP
    >>> ccd = CCDData.read('http://data.astropy.org/tutorials/FITS-images/HorseHead.fits', unit="adu", cache=True)  # doctest: +REMOTE_DATA +IGNORE_WARNINGS

If there is a unit in the FITS file (in the ``BUNIT`` keyword), that will be
used, but explicitly providing a unit in ``read`` will override any unit in the
FITS file.

There is no restriction at all on what the unit can be  any unit in
`astropy.units` or another that you create yourself will work.

In addition, the user can specify the extension in a FITS file to use:

    >>> ccd = CCDData.read('my_file.fits', hdu=1, unit="adu")  # doctest: +SKIP

If ``hdu`` is not specified, it will assume the data is in the primary
extension. If there is no data in the primary extension, the first extension
with image data will be used.

Metadata
++++++++

When initializing from a FITS file, the ``header`` property is initialized using
the header of the FITS file. Metadata is optional, and can be provided by any
dictionary or dict-like object:

    >>> ccd_simple = CCDData(np.arange(10), unit="adu")
    >>> my_meta = {'observer': 'Edwin Hubble', 'exposure': 30.0}
    >>> ccd_simple.header = my_meta  # or use ccd_simple.meta = my_meta

Whether the metadata is case-sensitive or not depends on how it is
initialized. A FITS header, for example, is not case-sensitive, but a Python
dictionary is.

Getting Data Out
++++++++++++++++

A `~astropy.nddata.CCDData` object behaves like a ``numpy`` array (masked if the
`~astropy.nddata.CCDData` mask is set) in expressions, and the underlying
data (ignoring any mask) is accessed through the ``data`` attribute:

    >>> ccd_masked = CCDData([1, 2, 3], unit="adu", mask=[0, 0, 1])
    >>> 2 * np.ones(3) * ccd_masked   # one return value will be masked
    masked_array(data=[2.0, 4.0, --],
                 mask=[False, False,  True],
           fill_value=1e+20)
    >>> 2 * np.ones(3) * ccd_masked.data   # ignores the mask  # doctest: +FLOAT_CMP
    array([2., 4., 6.])

You can force conversion to a ``numpy`` array with:

    >>> np.asarray(ccd_masked)
    array([1, 2, 3])
    >>> np.ma.array(ccd_masked.data, mask=ccd_masked.mask)
    masked_array(data=[1, 2, --],
                 mask=[False, False,  True],
           fill_value=999999)

A method for converting a `~astropy.nddata.CCDData` object to a FITS HDU list
is also available. It converts the metadata to a FITS header:

    >>> hdulist = ccd_masked.to_hdu()

You can also write directly to a FITS file:

    >>> ccd_masked.write('my_image.fits')

Masks and Flags
+++++++++++++++

Although it is not required when a `~astropy.nddata.CCDData` image is created,
you can also specify a mask and/or flags.

A mask is a boolean array the same size as the data in which a value of
``True`` indicates that a particular pixel should be masked (*i.e.*, not be
included in arithmetic operations or aggregation).

Flags are one or more additional arrays (of any type) whose shape matches the
shape of the data. One particularly useful type of flag is a bit planes; for
more details about bit planes and the functions ``astropy`` provides for
converting them to binary masks, see :ref:`bitmask_details`. For more details
on setting flags, see `~astropy.nddata.NDData`.

WCS
+++

The ``wcs`` attribute of a `~astropy.nddata.CCDData` object can be set two ways.

+ If the `~astropy.nddata.CCDData` object is created from a FITS file that has
  WCS keywords in the header, the ``wcs`` attribute is set to a
  `~astropy.wcs.WCS` object using the information in the FITS header.

+ The WCS can also be provided when the `~astropy.nddata.CCDData` object is
  constructed with the ``wcs`` argument.

Either way, the ``wcs`` attribute is kept up to date if the
`~astropy.nddata.CCDData` image is trimmed.

Uncertainty
-----------

You can set the uncertainty directly, either by creating a
`~astropy.nddata.StdDevUncertainty` object first:

    >>> data = np.random.normal(size=(10, 10), loc=1.0, scale=0.1)
    >>> ccd = CCDData(data, unit="electron")
    >>> from astropy.nddata.nduncertainty import StdDevUncertainty
    >>> uncertainty = 0.1 * ccd.data  # can be any array whose shape matches the data
    >>> my_uncertainty = StdDevUncertainty(uncertainty)
    >>> ccd.uncertainty = my_uncertainty

Or by providing a `~numpy.ndarray` with the same shape as the data:

    >>> ccd.uncertainty = 0.1 * ccd.data  # doctest: +ELLIPSIS
    INFO: array provided for uncertainty; assuming it is a StdDevUncertainty. [...]

In this case, the uncertainty is assumed to be
`~astropy.nddata.StdDevUncertainty`.

Two other uncertainty classes are available for which error propagation is
also supported: `~astropy.nddata.VarianceUncertainty` and
`~astropy.nddata.InverseVariance`. Using one of these three uncertainties is
required to enable error propagation in `~astropy.nddata.CCDData`.

If you want access to the underlying uncertainty, use its ``.array`` attribute:

    >>> ccd.uncertainty.array  # doctest: +ELLIPSIS
    array(...)

Arithmetic with Images
----------------------

Methods are provided to perform arithmetic operations with a
`~astropy.nddata.CCDData` image and a number, an ``astropy``
`~astropy.units.Quantity` (a number with units), or another
`~astropy.nddata.CCDData` image.

Using these methods propagates errors correctly (if the errors are
uncorrelated), takes care of any necessary unit conversions, and applies masks
appropriately. Note that the metadata of the result is *not* set if the
operation is between two `~astropy.nddata.CCDData` objects.

    >>> result = ccd.multiply(0.2 * u.adu)
    >>> uncertainty_ratio = result.uncertainty.array[0, 0]/ccd.uncertainty.array[0, 0]
    >>> round(uncertainty_ratio, 5)   # doctest: +FLOAT_CMP
    0.2
    >>> result.unit
    Unit("adu electron")

.. note::
    The affiliated package `ccdproc <https://ccdproc.readthedocs.io>`_ provides
    functions for many common data reduction operations. Those functions try to
    construct a sensible header for the result and provide a mechanism for
    logging the action of the function in the header.


The arithmetic operators ``*``, ``/``, ``+``, and ``-`` are *not* overridden.

.. note::
   If two images have different WCS values, the ``wcs`` on the first
   `~astropy.nddata.CCDData` object will be used for the resultant object.
.. _bitmask_details:

********************************************************
Utility Functions for Handling Bit Masks and Mask Arrays
********************************************************

It is common to use `bit fields <https://en.wikipedia.org/wiki/Bit_field>`_,
such as integer variables whose individual bits represent some attributes, to
characterize the state of data. For example, Hubble Space Telescope (HST) uses
arrays of bit fields to characterize data quality (DQ) of HST images. See, for
example, DQ field values for `WFPC2 image data (see Table 3.3) <https://www.stsci.edu/files/live/sites/www/files/home/hst/instrumentation/legacy/wfpc2/_documents/wfpc2_dhb.pdf>`_ and `WFC3 image data (see Table 3.3) <https://hst-docs.stsci.edu/wfc3dhb/chapter-3-wfc3-data-calibration/3-3-ir-data-calibration-steps#id-3.3IRDataCalibrationSteps-3.3.1DataQualityInitialization>`_.
As you can see, the meaning assigned to various *bit flags* for the two
instruments is generally different.

Bit fields can be thought of as tightly packed collections of bit flags. Using
`masking <https://en.wikipedia.org/wiki/Mask_(computing)>`_ we can "inspect"
the status of individual bits.

One common operation performed on bit field arrays is their conversion to
boolean masks, for example, by assigning boolean `True` (in the boolean
mask) to those elements that correspond to non-zero-valued bit fields
(bit fields with at least one bit set to ``1``) or, oftentimes, by assigning
`True` to elements whose corresponding bit fields have only *specific fields*
set (to ``1``). This more sophisticated analysis of bit fields can be
accomplished using *bit masks* and the aforementioned masking operation.

The `~astropy.nddata.bitmask` module provides two functions that facilitate
conversion of bit field arrays (i.e., DQ arrays) to boolean masks:
`~astropy.nddata.bitmask.bitfield_to_boolean_mask` converts an input bit
field array to a boolean mask using an input bit mask (or list of individual
bit flags) and `~astropy.nddata.bitmask.interpret_bit_flags` creates a bit mask
from an input list of individual bit flags.

Creating Boolean Masks
**********************

Overview
========

`~astropy.nddata.bitmask.bitfield_to_boolean_mask` by default assumes that
all input bit fields that have at least one bit turned "ON" corresponds to
"bad" data (i.e., pixels) and converts them to boolean `True` in the output
boolean mask (otherwise output boolean mask values are set to `False`).

Often, for specific algorithms and situations, some bit flags are okay and
can be ignored. `~astropy.nddata.bitmask.bitfield_to_boolean_mask` accepts
lists of bit flags that *by default must be ignored* in the input bit fields
when creating boolean masks.

Fundamentally, *by default*, `~astropy.nddata.bitmask.bitfield_to_boolean_mask`
performs the following operation:

.. _main_eq:

``(1)    boolean_mask = (bitfield & ~bit_mask) != 0``

(Here ``&`` is bitwise ``and`` while ``~`` is the bitwise ``not``
operation.) In the previous formula, ``bit_mask`` is a bit mask created from
individual bit flags that need to be ignored in the bit field.

Example
-------

..
  EXAMPLE START
  Creating Boolean Masks from Bit Field Arrays

.. _table1:

.. table:: Table 1: Examples of Boolean Mask Computations \
           (default parameters and 8-bit data type)

    +--------------+--------------+--------------+--------------+------------+
    | Bit Field    |  Bit Mask    | ~(Bit Mask)  | Bit Field &  |Boolean Mask|
    |              |              |              | ~(Bit Mask)  |            |
    +==============+==============+==============+==============+============+
    |11011001 (217)|01010000 (80) |10101111 (175)|10001001 (137)|   True     |
    +--------------+--------------+--------------+--------------+------------+
    |11011001 (217)|10101111 (175)|01010000 (80) |01010000 (80) |   True     |
    +--------------+--------------+--------------+--------------+------------+
    |00001001 (9)  |01001001 (73) |10110110 (182)|00000000 (0)  |   False    |
    +--------------+--------------+--------------+--------------+------------+
    |00001001 (9)  |00000000 (0)  |11111111 (255)|00001001 (9)  |   True     |
    +--------------+--------------+--------------+--------------+------------+
    |00001001 (9)  |11111111 (255)|00000000 (0)  |00000000 (0)  |   False    |
    +--------------+--------------+--------------+--------------+------------+

..
  EXAMPLE END

Specifying Bit Flags
====================

`~astropy.nddata.bitmask.bitfield_to_boolean_mask` accepts either an integer
bit mask or lists of bit flags. Lists of bit flags will be combined into a
bit mask and can be provided either as a Python list of
**integer bit flag values** or as a comma-separated (or ``+``-separated)
list of integer bit flag values. Consider the bit mask from the first example
in `Table 1 <table1_>`_. In this case ``ignore_flags`` can be set either to:

    - An integer value bit mask 80
    - A Python list indicating individual non-zero
      *bit flag values:* ``[16, 64]``
    - A string of comma-separated *bit flag values or mnemonic names*: ``'16,64'``, ``'CR,WARM'``
    - A string of ``+``-separated *bit flag values or mnemonic names*: ``'16+64'``, ``'CR+WARM'``

Example
-------

..
  EXAMPLE START
  Specifying Bit Flags in NDData

To specify bit flags:

    >>> from astropy.nddata import bitmask
    >>> import numpy as np
    >>> bitmask.bitfield_to_boolean_mask(217, ignore_flags=80)
    array(True...)
    >>> bitmask.bitfield_to_boolean_mask(217, ignore_flags='16,64')
    array(True...)
    >>> bitmask.bitfield_to_boolean_mask(217, ignore_flags=[16, 64])
    array(True...)
    >>> bitmask.bitfield_to_boolean_mask(9, ignore_flags=[1, 8, 64])
    array(False...)
    >>> bitmask.bitfield_to_boolean_mask([9, 10, 73, 217], ignore_flags='1,8,64')
    array([False,  True, False,  True]...)

It is also possible to specify the type of the output mask:

    >>> bitmask.bitfield_to_boolean_mask([9, 10, 73, 217], ignore_flags='1,8,64', dtype=np.uint8)
    array([0, 1, 0, 1], dtype=uint8)

In order to use lists of mnemonic bit flags names, one must provide a map,
a subclass of `~astropy.nddata.bitmask.BitFlagNameMap`, that can be
used to map mnemonic names to bit flag values. Normally these maps should be
provided by a third-party package supporting a specific instrument. Each bit
flag in the map may also contain a string comment following the flag value.
In the example below we define a simple mask map:

    >>> from astropy.nddata.bitmask import BitFlagNameMap
    >>> class ST_DQ(BitFlagNameMap):
    ...     CR = 1
    ...     CLOUDY = 4
    ...     RAINY = 8, 'Dome closed'
    ...     HOT = 32
    ...     DEAD = 64
    >>> bitmask.bitfield_to_boolean_mask([9, 10, 73, 217], ignore_flags='CR,RAINY,DEAD',
    ...                                  dtype=np.uint8, flag_name_map=ST_DQ)
    array([0, 1, 0, 1], dtype=uint8)

..
  EXAMPLE END

Using Bit Flags Name Maps
=========================

..
  EXAMPLE START

In order to allow the use of mnemonic bit flag names to describe the flags
to be taken into consideration or ignored when creating a *boolean* mask, we
use bit flag name maps. These maps perform case-insensitive translation of
mnemonic bit flag names to the corresponding integer value.

Bit flag name maps are subclasses of `~astropy.nddata.bitmask.BitFlagNameMap`
and can be constructed in two ways, either by directly subclassing
`~astropy.nddata.bitmask.BitFlagNameMap`, e.g.,

    >>> from astropy.nddata.bitmask import BitFlagNameMap
    >>> class ST_DQ(BitFlagNameMap):
    ...     CR = 1
    ...     CLOUDY = 4
    ...     RAINY = 8
    ...
    >>> class ST_CAM1_DQ(ST_DQ):
    ...     HOT = 16
    ...     DEAD = 32

or by using the `~astropy.nddata.bitmask.extend_bit_flag_map` class factory:

    >>> from astropy.nddata.bitmask import extend_bit_flag_map
    >>> ST_DQ = extend_bit_flag_map('ST_DQ', CR=1, CLOUDY=4, RAINY=8)
    >>> ST_CAM1_DQ = extend_bit_flag_map('ST_CAM1_DQ', ST_DQ, HOT=16, DEAD=32)

.. note::

    Bit flag values must be integer numbers that are powers of 2.

Once constructed, bit flag values of a map cannot be modified, deleted, or
added. Adding flags to a map is allowed only through subclassing using one of
the two methods shown above or by adding lists of tuples of
the form ``('NAME', value)`` to the class. This will create a new map class
subclassed from the original map but containing the additional flags

    >>> ST_CAM1_DQ = ST_DQ + [('HOT', 16), ('DEAD', 32)]

would result in an equivalent map as in the subclassing or class factory
examples shown above.

Once a bit flag name map was created, the bit flag values can be accessed
either as *case-insensitive* class attributes or keys in a dictionary:

    >>> ST_CAM1_DQ.cloudy
    4
    >>> ST_CAM1_DQ['Rainy']
    8

..
  EXAMPLE END

Modifying the Formula for Creating Boolean Masks
================================================

`~astropy.nddata.bitmask.bitfield_to_boolean_mask` provides several parameters
that can be used to modify the formula used to create boolean masks.

Inverting Bit Masks
-------------------

Sometimes it is more convenient to be able to specify those bit
flags that *must be considered* when creating the boolean mask, and all other
flags should be ignored.

Example
^^^^^^^

..
  EXAMPLE START
  Inverting Bit Masks in NDData

In `~astropy.nddata.bitmask.bitfield_to_boolean_mask` specifying bit flags that
must be considered when creating the boolean mask can be accomplished by
setting the parameter ``flip_bits`` to `True`. This effectively modifies
`equation (1) <main_eq_>`_ to:

.. _modif_eq2:

``(2)    boolean_mask = (bitfield & bit_mask) != 0``

So, instead of:

    >>> bitmask.bitfield_to_boolean_mask([9, 10, 73, 217], ignore_flags=[1, 8, 64])
    array([False,  True, False,  True]...)

You can obtain the same result as:

    >>> bitmask.bitfield_to_boolean_mask(
    ...     [9, 10, 73, 217], ignore_flags=[2, 4, 16, 32, 128], flip_bits=True
    ... )
    array([False,  True, False,  True]...)

Note however, when ``ignore_flags`` is a comma-separated list of bit flag
values, ``flip_bits`` cannot be set to either `True` or `False`. Instead,
to flip bits of the bit mask formed from a string list of comma-separated
bit flag values, you can prepend a single ``~`` to the list:

    >>> bitmask.bitfield_to_boolean_mask([9, 10, 73, 217], ignore_flags='~2+4+16+32+128')
    array([False,  True, False,  True]...)

..
  EXAMPLE END

Inverting Boolean Masks
-----------------------

Other times, it may be more convenient to obtain an inverted mask in which
flagged data are converted to `False` instead of `True`:

.. _modif_eq3:

``(3)    boolean_mask = (bitfield & ~bit_mask) == 0``

This can be accomplished by changing the ``good_mask_value`` parameter from
its default value (`False`) to `True`.

Example
^^^^^^^

..
  EXAMPLE START
  Inverting Boolean Masks in NDData

To obtain an inverted mask in which flagged data are converted to `False`
instead of `True`:

    >>> bitmask.bitfield_to_boolean_mask([9, 10, 73, 217], ignore_flags=[1, 8, 64],
    ...                                  good_mask_value=True)
    array([ True, False,  True, False]...)

..
  EXAMPLE END
.. _nddata_io:

I/O Mixin
*********

The I/O mixin, `~astropy.nddata.NDIOMixin`, adds ``read`` and ``write``
methods that use the ``astropy`` I/O registry.

The mixin itself creates the read/write methods; it does not register
any readers or writers with the I/O registry. Subclasses of
`~astropy.nddata.NDDataBase` or `~astropy.nddata.NDData` need to include this
mixin, implement a reader and writer, *and* register it with the I/O
framework. See :ref:`io_registry` for details.
.. _nddata_arithmetic:

NDData Arithmetic
*****************

Introduction
============

`~astropy.nddata.NDDataRef` implements the following arithmetic operations:

- Addition: :meth:`~astropy.nddata.NDArithmeticMixin.add`
- Subtraction: :meth:`~astropy.nddata.NDArithmeticMixin.subtract`
- Multiplication: :meth:`~astropy.nddata.NDArithmeticMixin.multiply`
- Division: :meth:`~astropy.nddata.NDArithmeticMixin.divide`

Using Basic Arithmetic Methods
==============================

Using the standard arithmetic methods requires that the first operand
is an `~astropy.nddata.NDDataRef` instance:

    >>> from astropy.nddata import NDDataRef
    >>> from astropy.wcs import WCS
    >>> import numpy as np
    >>> ndd1 = NDDataRef([1, 2, 3, 4])

While the requirement for the second operand is that it must be convertible
to the first operand. It can be a number::

    >>> ndd1.add(3)
    NDDataRef([4, 5, 6, 7])

Or a `list`::

    >>> ndd1.subtract([1,1,1,1])
    NDDataRef([0, 1, 2, 3])

Or a `numpy.ndarray`::

    >>> ndd1.multiply(np.arange(4, 8))
    NDDataRef([ 4, 10, 18, 28])
    >>> ndd1.divide(np.arange(1,13).reshape(3,4))  # a 3 x 4 numpy array  # doctest: +FLOAT_CMP
    NDDataRef([[1.        , 1.        , 1.        , 1.        ],
               [0.2       , 0.33333333, 0.42857143, 0.5       ],
               [0.11111111, 0.2       , 0.27272727, 0.33333333]])

Here, broadcasting takes care of the different dimensions. Several other
classes are also possible.

Using Arithmetic Classmethods
=============================

Here both operands do not need to be `~astropy.nddata.NDDataRef`-like::

    >>> NDDataRef.add(1, 3)
    NDDataRef(4)

To wrap the result of an arithmetic operation between two Quantities::

    >>> import astropy.units as u
    >>> ndd = NDDataRef.multiply([1,2] * u.m, [10, 20] * u.cm)
    >>> ndd  # doctest: +FLOAT_CMP
    NDDataRef([10., 40.], unit='cm m')
    >>> ndd.unit
    Unit("cm m")

Or take the inverse of an `~astropy.nddata.NDDataRef` object::

    >>> NDDataRef.divide(1, ndd1)  # doctest: +FLOAT_CMP
    NDDataRef([1.        , 0.5       , 0.33333333, 0.25      ])


Possible Operands
-----------------

The possible types of input for operands are:

+ Scalars of any type
+ Lists containing numbers (or nested lists)
+ ``numpy`` arrays
+ ``numpy`` masked arrays
+ ``astropy`` quantities
+ Other ``nddata`` classes or subclasses

Advanced Options
================

The normal Python operators ``+``, ``-``, etc. are not implemented because
the methods provide several options on how to proceed with the additional
attributes.

Data and Unit
-------------

For ``data`` and ``unit`` there are no parameters. Every arithmetic
operation lets the `astropy.units.Quantity`-framework evaluate the result
or fail and abort the operation.

Adding two `~astropy.nddata.NDData` objects with the same unit works::

    >>> ndd1 = NDDataRef([1,2,3,4,5], unit='m')
    >>> ndd2 = NDDataRef([100,150,200,50,500], unit='m')

    >>> ndd = ndd1.add(ndd2)
    >>> ndd.data  # doctest: +FLOAT_CMP
    array([101., 152., 203.,  54., 505.])
    >>> ndd.unit
    Unit("m")

Adding two `~astropy.nddata.NDData` objects with compatible units also works::

    >>> ndd1 = NDDataRef(ndd1, unit='pc')
    INFO: overwriting NDData's current unit with specified unit. [astropy.nddata.nddata]
    >>> ndd2 = NDDataRef(ndd2, unit='lyr')
    INFO: overwriting NDData's current unit with specified unit. [astropy.nddata.nddata]

    >>> ndd = ndd1.subtract(ndd2)
    >>> ndd.data  # doctest: +FLOAT_CMP
    array([ -29.66013938,  -43.99020907,  -58.32027876,  -11.33006969,
           -148.30069689])
    >>> ndd.unit
    Unit("pc")

This will keep by default the unit of the first operand. However, units will
not be decomposed during division::

    >>> ndd = ndd2.divide(ndd1)
    >>> ndd.data  # doctest: +FLOAT_CMP
    array([100.        ,  75.        ,  66.66666667,  12.5       , 100.        ])
    >>> ndd.unit
    Unit("lyr / pc")

Mask
----

The ``handle_mask`` parameter for the arithmetic operations implements what the
resulting mask will be. There are several options.

- ``None``, the result will have no ``mask``::

      >>> ndd1 = NDDataRef(1, mask=True)
      >>> ndd2 = NDDataRef(1, mask=False)
      >>> ndd1.add(ndd2, handle_mask=None).mask is None
      True

- ``"first_found"`` or ``"ff"``, the result will have the ``mask`` of the first
  operand or if that is ``None``, the ``mask`` of the second operand::

      >>> ndd1 = NDDataRef(1, mask=True)
      >>> ndd2 = NDDataRef(1, mask=False)
      >>> ndd1.add(ndd2, handle_mask="first_found").mask
      True
      >>> ndd3 = NDDataRef(1)
      >>> ndd3.add(ndd2, handle_mask="first_found").mask
      False

- A function (or an arbitrary callable) that takes at least two arguments.
  For example, `numpy.logical_or` is the default::

      >>> ndd1 = NDDataRef(1, mask=np.array([True, False, True, False]))
      >>> ndd2 = NDDataRef(1, mask=np.array([True, False, False, True]))
      >>> ndd1.add(ndd2).mask
      array([ True, False,  True,  True]...)

  This defaults to ``"first_found"`` in case only one ``mask`` is not None::

      >>> ndd1 = NDDataRef(1)
      >>> ndd2 = NDDataRef(1, mask=np.array([True, False, False, True]))
      >>> ndd1.add(ndd2).mask
      array([ True, False, False,  True]...)

  Custom functions are also possible::

      >>> def take_alternating_values(mask1, mask2, start=0):
      ...     result = np.zeros(mask1.shape, dtype=np.bool_)
      ...     result[start::2] = mask1[start::2]
      ...     result[start+1::2] = mask2[start+1::2]
      ...     return result

  This function is nonsense, but we can still see how it performs::

      >>> ndd1 = NDDataRef(1, mask=np.array([True, False, True, False]))
      >>> ndd2 = NDDataRef(1, mask=np.array([True, False, False, True]))
      >>> ndd1.add(ndd2, handle_mask=take_alternating_values).mask
      array([ True, False,  True,  True]...)

  Additional parameters can be given by prefixing them with ``mask_``
  (which will be stripped before passing it to the function)::

      >>> ndd1.add(ndd2, handle_mask=take_alternating_values, mask_start=1).mask
      array([False, False, False, False]...)
      >>> ndd1.add(ndd2, handle_mask=take_alternating_values, mask_start=2).mask
      array([False, False,  True,  True]...)

Meta
----

The ``handle_meta`` parameter for the arithmetic operations implements what the
resulting ``meta`` will be. The options are the same as for the ``mask``:

- If ``None`` the resulting ``meta`` will be an empty `collections.OrderedDict`.

      >>> ndd1 = NDDataRef(1, meta={'object': 'sun'})
      >>> ndd2 = NDDataRef(1, meta={'object': 'moon'})
      >>> ndd1.add(ndd2, handle_meta=None).meta
      OrderedDict()

  For ``meta`` this is the default so you do not need to pass it in this case::

      >>> ndd1.add(ndd2).meta
      OrderedDict()

- If ``"first_found"`` or ``"ff"``, the resulting ``meta`` will be the ``meta``
  of the first operand or if that contains no keys, the ``meta`` of the second
  operand is taken.

      >>> ndd1 = NDDataRef(1, meta={'object': 'sun'})
      >>> ndd2 = NDDataRef(1, meta={'object': 'moon'})
      >>> ndd1.add(ndd2, handle_meta='ff').meta
      {'object': 'sun'}

- If it is a ``callable`` it must take at least two arguments. Both ``meta``
  attributes will be passed to this function (even if one or both of them are
  empty) and the callable evaluates the result's ``meta``. For example, a
  function that merges these two::

      >>> # It's expected with arithmetics that the result is not a reference,
      >>> # so we need to copy
      >>> from copy import deepcopy

      >>> def combine_meta(meta1, meta2):
      ...     if not meta1:
      ...         return deepcopy(meta2)
      ...     elif not meta2:
      ...         return deepcopy(meta1)
      ...     else:
      ...         meta_final = deepcopy(meta1)
      ...         meta_final.update(meta2)
      ...         return meta_final

      >>> ndd1 = NDDataRef(1, meta={'time': 'today'})
      >>> ndd2 = NDDataRef(1, meta={'object': 'moon'})
      >>> ndd1.subtract(ndd2, handle_meta=combine_meta).meta # doctest: +SKIP
      {'object': 'moon', 'time': 'today'}

  Here again additional arguments for the function can be passed in using
  the prefix ``meta_`` (which will be stripped away before passing it to this
  function). See the description for the mask-attribute for further details.

World Coordinate System (WCS)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The ``compare_wcs`` argument will determine what the result's ``wcs`` will be
or if the operation should be forbidden. The possible values are identical to
``mask`` and ``meta``:

- If ``None`` the resulting ``wcs`` will be an empty ``None``.

      >>> ndd1 = NDDataRef(1, wcs=None)
      >>> ndd2 = NDDataRef(1, wcs=WCS())
      >>> ndd1.add(ndd2, compare_wcs=None).wcs is None
      True

- If ``"first_found"`` or ``"ff"`` the resulting ``wcs`` will be the ``wcs`` of
  the first operand or if that is ``None``, the ``meta`` of the second operand
  is taken.

      >>> wcs = WCS()
      >>> ndd1 = NDDataRef(1, wcs=wcs)
      >>> ndd2 = NDDataRef(1, wcs=None)
      >>> str(ndd1.add(ndd2, compare_wcs='ff').wcs) == str(wcs)
      True

- If it is a ``callable`` it must take at least two arguments. Both ``wcs``
  attributes will be passed to this function (even if one or both of them are
  ``None``) and the callable should return ``True`` if these ``wcs`` are
  identical (enough) to allow the arithmetic operation or ``False`` if the
  arithmetic operation should be aborted with a ``ValueError``. If ``True`` the
  ``wcs`` are identical and the first one is used for the result::

      >>> def compare_wcs_scalar(wcs1, wcs2, allowed_deviation=0.1):
      ...     if wcs1 is None and wcs2 is None:
      ...         return True  # both have no WCS so they are identical
      ...     if wcs1 is None or wcs2 is None:
      ...         return False  # one has WCS, the other doesn't not possible
      ...     else:
      ...         # Consider wcs close if centers are close enough
      ...         return all(abs(wcs1.wcs.crpix - wcs2.wcs.crpix) < allowed_deviation)

      >>> ndd1 = NDDataRef(1, wcs=None)
      >>> ndd2 = NDDataRef(1, wcs=None)
      >>> ndd1.subtract(ndd2, compare_wcs=compare_wcs_scalar).wcs


  Additional arguments can be passed in prefixing them with ``wcs_`` (this
  prefix will be stripped away before passing it to the function)::

      >>> ndd1 = NDDataRef(1, wcs=WCS())
      >>> ndd1.wcs.wcs.crpix = [1, 1]
      >>> ndd2 = NDDataRef(1, wcs=WCS())
      >>> ndd1.subtract(ndd2, compare_wcs=compare_wcs_scalar, wcs_allowed_deviation=2).wcs.wcs.crpix
      array([1., 1.])

  If you are using `~astropy.wcs.WCS` objects, a very handy function to use
  might be::

      >>> def wcs_compare(wcs1, wcs2, *args, **kwargs):
      ...     return wcs1.wcs.compare(wcs2.wcs, *args, **kwargs)

  See :meth:`astropy.wcs.Wcsprm.compare` for the arguments this comparison
  allows.

Uncertainty
-----------

The ``propagate_uncertainties`` argument can be used to turn the propagation
of uncertainties on or off.

- If ``None`` the result will have no uncertainty::

      >>> from astropy.nddata import StdDevUncertainty
      >>> ndd1 = NDDataRef(1, uncertainty=StdDevUncertainty(0))
      >>> ndd2 = NDDataRef(1, uncertainty=StdDevUncertainty(1))
      >>> ndd1.add(ndd2, propagate_uncertainties=None).uncertainty is None
      True

- If ``False`` the result will have the first found uncertainty.

  .. note::
      Setting ``propagate_uncertainties=False`` is generally not
      recommended.

- If ``True`` both uncertainties must be ``NDUncertainty`` subclasses that
  implement propagation. This is possible for
  `~astropy.nddata.StdDevUncertainty`::

      >>> ndd1 = NDDataRef(1, uncertainty=StdDevUncertainty([10]))
      >>> ndd2 = NDDataRef(1, uncertainty=StdDevUncertainty([10]))
      >>> ndd1.add(ndd2, propagate_uncertainties=True).uncertainty  # doctest: +FLOAT_CMP
      StdDevUncertainty([14.14213562])

Uncertainty with Correlation
----------------------------

If ``propagate_uncertainties`` is ``True`` you can also give an argument
for ``uncertainty_correlation``. `~astropy.nddata.StdDevUncertainty` cannot
keep track of its correlations by itself, but it can evaluate the correct
resulting uncertainty if the correct ``correlation`` is given.

The default (``0``) represents uncorrelated while ``1`` means correlated and
``-1`` anti-correlated. If given a `numpy.ndarray` it should represent the
element-wise correlation coefficient.

Examples
^^^^^^^^

..
  EXAMPLE START
  Uncertainty with Correlation in NDData

Without correlation, subtracting an `~astropy.nddata.NDDataRef` instance from
itself results in a non-zero uncertainty::

    >>> ndd1 = NDDataRef(1, uncertainty=StdDevUncertainty([10]))
    >>> ndd1.subtract(ndd1, propagate_uncertainties=True).uncertainty  # doctest: +FLOAT_CMP
    StdDevUncertainty([14.14213562])

Given a correlation of ``1`` (because they clearly correlate) gives the
correct uncertainty of ``0``::

    >>> ndd1 = NDDataRef(1, uncertainty=StdDevUncertainty([10]))
    >>> ndd1.subtract(ndd1, propagate_uncertainties=True,
    ...               uncertainty_correlation=1).uncertainty  # doctest: +FLOAT_CMP
    StdDevUncertainty([0.])

Which would be consistent with the equivalent operation ``ndd1 * 0``::

    >>> ndd1.multiply(0, propagate_uncertainties=True).uncertainty # doctest: +FLOAT_CMP
    StdDevUncertainty([0.])

.. warning::
    The user needs to calculate or know the appropriate value or array manually
    and pass it to ``uncertainty_correlation``. The implementation follows
    general first order error propagation formulas. See, for example:
    `Wikipedia <https://en.wikipedia.org/wiki/Propagation_of_uncertainty#Example_formulas>`_.

You can also give element-wise correlations::

    >>> ndd1 = NDDataRef([1,1,1,1], uncertainty=StdDevUncertainty([1,1,1,1]))
    >>> ndd2 = NDDataRef([2,2,2,2], uncertainty=StdDevUncertainty([2,2,2,2]))
    >>> ndd1.add(ndd2,uncertainty_correlation=np.array([1,0.5,0,-1])).uncertainty  # doctest: +FLOAT_CMP
    StdDevUncertainty([3.        , 2.64575131, 2.23606798, 1.        ])

The correlation ``np.array([1, 0.5, 0, -1])`` would indicate that the first
element is fully correlated and the second element partially correlates, while
the third element is uncorrelated, and the fourth is anti-correlated.

..
  EXAMPLE END

Uncertainty with Unit
---------------------

`~astropy.nddata.StdDevUncertainty` implements correct error propagation even
if the unit of the data differs from the unit of the uncertainty::

    >>> ndd1 = NDDataRef([10], unit='m', uncertainty=StdDevUncertainty([10], unit='cm'))
    >>> ndd2 = NDDataRef([20], unit='m', uncertainty=StdDevUncertainty([10]))
    >>> ndd1.subtract(ndd2, propagate_uncertainties=True).uncertainty  # doctest: +FLOAT_CMP
    StdDevUncertainty([10.00049999])

But it needs to be convertible to the unit for the data.
.. _nddata_slicing:

Slicing and Indexing NDData
***************************

Introduction
============

This page only deals with peculiarities that apply to
`~astropy.nddata.NDData`-like classes. For a tutorial about slicing/indexing see the
`python documentation <https://docs.python.org/3/tutorial/introduction.html#lists>`_
and `numpy documentation <https://numpy.org/doc/stable/reference/arrays.indexing.html>`_.

.. warning::
    `~astropy.nddata.NDData` and `~astropy.nddata.NDDataRef` enforce almost no
    restrictions on the properties, so it might happen that some **valid but
    unusual** combinations of properties always result in an IndexError or
    incorrect results. In this case, see :ref:`nddata_subclassing` on how to
    customize slicing for a particular property.


Slicing NDDataRef
=================

Unlike `~astropy.nddata.NDData` the class `~astropy.nddata.NDDataRef`
implements slicing or indexing. The result will be wrapped inside the same
class as the sliced object.

Getting one element::

    >>> import numpy as np
    >>> from astropy.nddata import NDDataRef

    >>> data = np.array([1, 2, 3, 4])
    >>> ndd = NDDataRef(data)
    >>> ndd[1]
    NDDataRef(2)

Getting a sliced portion of the original::

    >>> ndd[1:3]  # Get element 1 (inclusive) to 3 (exclusive)
    NDDataRef([2, 3])

This will return a reference (and as such **not a copy**) of the original
properties, so changing a slice will affect the original::

    >>> ndd_sliced = ndd[1:3]
    >>> ndd_sliced.data[0] = 5
    >>> ndd_sliced
    NDDataRef([5, 3])
    >>> ndd
    NDDataRef([1, 5, 3, 4])

But only the one element that was indexed is affected (for example,
``ndd_sliced = ndd[1]``). The element is a scalar and changes will not
propagate to the original.

Slicing NDDataRef Including Attributes
======================================

In the case that a ``mask``, or ``uncertainty`` is present, this
attribute will be sliced too::

    >>> from astropy.nddata import StdDevUncertainty
    >>> data = np.array([1, 2, 3, 4])
    >>> mask = data > 2
    >>> uncertainty = StdDevUncertainty(np.sqrt(data))
    >>> ndd = NDDataRef(data, mask=mask, uncertainty=uncertainty)
    >>> ndd_sliced = ndd[1:3]

    >>> ndd_sliced.data
    array([2, 3])

    >>> ndd_sliced.mask
    array([False,  True]...)

    >>> ndd_sliced.uncertainty  # doctest: +FLOAT_CMP
    StdDevUncertainty([1.41421356, 1.73205081])

``unit`` and ``meta``, however, will be unaffected.

If any of the attributes are set but do not implement slicing, an info will be
printed and the property will be kept as is::

    >>> data = np.array([1, 2, 3, 4])
    >>> mask = False
    >>> uncertainty = StdDevUncertainty(0)
    >>> ndd = NDDataRef(data, mask=mask, uncertainty=uncertainty)
    >>> ndd_sliced = ndd[1:3]
    INFO: uncertainty cannot be sliced. [astropy.nddata.mixins.ndslicing]
    INFO: mask cannot be sliced. [astropy.nddata.mixins.ndslicing]

    >>> ndd_sliced.mask
    False


Slicing NDData with World Coordinates
-------------------------------------

If ``wcs`` is set, it must be either implement
`~astropy.wcs.wcsapi.BaseLowLevelWCS` or `~astropy.wcs.wcsapi.BaseHighLevelWCS`.
This means that only integer or range slices without a step are supported. So
slices like ``[::10]`` or array or boolean based slices will not work.

If you want to slice an ``NDData`` object called ``ndd`` without the WCS you can remove the
WCS from the ``NDData`` object by running:

    >>> ndd.wcs = None


Removing Masked Data
--------------------

.. warning::
    If ``wcs`` is set this will **NOT** be possible. But you can work around
    this by setting the wcs attribute to `None` with ``ndd.wcs = None`` before slicing.

By convention, the ``mask`` attribute indicates if a point is valid or invalid.
So we are able to get all valid data points by slicing with the mask.

Examples
^^^^^^^^

..
  EXAMPLE START
  Removing Masked Data in NDDataRef

To get all of the valid data points by slicing with the mask::

    >>> data = np.array([[1,2,3],[4,5,6],[7,8,9]])
    >>> mask = np.array([[0,1,0],[1,1,1],[0,0,1]], dtype=bool)
    >>> uncertainty = StdDevUncertainty(np.sqrt(data))
    >>> ndd = NDDataRef(data, mask=mask, uncertainty=uncertainty)
    >>> # don't forget that ~ or you'll get the invalid points
    >>> ndd_sliced = ndd[~ndd.mask]
    >>> ndd_sliced
    NDDataRef([1, 3, 7, 8])

    >>> ndd_sliced.mask
    array([False, False, False, False]...)

    >>> ndd_sliced.uncertainty  # doctest: +FLOAT_CMP
    StdDevUncertainty([1.        , 1.73205081, 2.64575131, 2.82842712])

Or all invalid points::

    >>> ndd_sliced = ndd[ndd.mask] # without the ~ now!
    >>> ndd_sliced
    NDDataRef([2, 4, 5, 6, 9])

    >>> ndd_sliced.mask
    array([ True,  True,  True,  True,  True]...)

    >>> ndd_sliced.uncertainty  # doctest: +FLOAT_CMP
    StdDevUncertainty([1.41421356, 2.        , 2.23606798, 2.44948974, 3.        ])

.. note::
    The result of this kind of indexing (boolean indexing) will always be
    one-dimensional!

..
  EXAMPLE END
Mixins for Added Functionality
******************************

.. toctree::
    :maxdepth: 2

    ndslicing.rst
    ndarithmetic.rst
    ndio.rst
.. doctest-skip-all

.. _vo-samp-example_hub:

Starting and Stopping a SAMP Hub Server
***************************************

There are several ways you can start up a SAMP hub:

Using an Existing Hub
=====================

You can start up another application that includes a hub, such as
`TOPCAT <http://www.star.bris.ac.uk/~mbt/topcat/>`_,
`SAO Ds9 <http://ds9.si.edu/>`_, or
`Aladin Desktop <http://aladin.u-strasbg.fr>`_.

Using the Command-Line Hub Utility
==================================

You can make use of the ``samp_hub`` command-line utility, which is included in
``astropy``::

    $ samp_hub

To get more help on available options for ``samp_hub``::

    $ samp_hub -h

To stop the server, press control-C.

Starting a Hub Programmatically (Advanced)
==========================================

You can start up a hub by creating a |SAMPHubServer| instance and starting it,
either from the interactive Python prompt, or from a Python script::

    >>> from astropy.samp import SAMPHubServer
    >>> hub = SAMPHubServer()
    >>> hub.start()

You can then stop the hub by calling::

    >>> hub.stop()

However, this method is generally not recommended for average users because it
does not work correctly when web SAMP clients try to connect. Instead, this
should be reserved for developers who want to embed a SAMP hub in a GUI, for
example. For more information, see :doc:`advanced_embed_samp_hub`.
.. note that if this is changed from the default approach of using an *include*
   (in index.rst) to a separate performance page, the header needs to be changed
   from === to ***, the filename extension needs to be changed from .inc.rst to
   .rst, and a link needs to be added in the subpackage toctree

.. _astropy-samp-performance:

.. Performance Tips
.. ================
..
.. Here we provide some tips and tricks for how to optimize performance of code
.. using `astropy.samp`.
.. doctest-skip-all

.. _vo-samp-example_clients:


Communication between Integrated Clients Objects
************************************************

As shown in :doc:`example_table_image`, the |SAMPIntegratedClient| class can be
used to communicate with other SAMP-enabled tools such as `TOPCAT
<http://www.star.bris.ac.uk/~mbt/topcat/>`_, `SAO DS9
<http://ds9.si.edu/>`_, or `Aladin Desktop
<http://aladin.u-strasbg.fr>`_.

In this section, we look at how we can set up two |SAMPIntegratedClient|
instances and communicate between them.

First, start up a SAMP hub as described in :doc:`example_hub`.

Next, we create two clients and connect them to the hub::

   >>> from astropy import samp
   >>> client1 = samp.SAMPIntegratedClient(name="Client 1", description="Test Client 1",
   ...                                     metadata = {"client1.version":"0.01"})
   >>> client2 = samp.SAMPIntegratedClient(name="Client 2", description="Test Client 2",
   ...                                     metadata = {"client2.version":"0.25"})
   >>> client1.connect()
   >>> client2.connect()

We now define functions to call when receiving a notification, call or
response::

   >>> def test_receive_notification(private_key, sender_id, mtype, params, extra):
   ...     print("Notification:", private_key, sender_id, mtype, params, extra)

   >>> def test_receive_call(private_key, sender_id, msg_id, mtype, params, extra):
   ...     print("Call:", private_key, sender_id, msg_id, mtype, params, extra)
   ...     client1.ereply(msg_id, samp.SAMP_STATUS_OK, result = {"txt": "printed"})

   >>> def test_receive_response(private_key, sender_id, msg_id, response):
   ...     print("Response:", private_key, sender_id, msg_id, response)

We subscribe client 1 to ``"samp.app.*"`` and bind it to the
related functions::

   >>> client1.bind_receive_notification("samp.app.*", test_receive_notification)
   >>> client1.bind_receive_call("samp.app.*", test_receive_call)

We now bind message tags received by client 2 to suitable functions::

   >>> client2.bind_receive_response("my-dummy-print", test_receive_response)
   >>> client2.bind_receive_response("my-dummy-print-specific", test_receive_response)

We are now ready to test out the clients and callback functions. Client 2
notifies all clients using the "samp.app.echo" message type via the hub::

   >>> client2.enotify_all("samp.app.echo", txt="Hello world!")
   ['cli#2']
   Notification: 0d7f4500225981c104a197c7666a8e4e cli#2 samp.app.echo {'txt':
   'Hello world!'} {'host': 'antigone.lambrate.inaf.it', 'user': 'unknown'}

We can also find a dictionary that specifies which clients would currently
receive ``samp.app.echo`` messages::

   >>> print(client2.get_subscribed_clients("samp.app.echo"))
   {'cli#2': {}}

Client 2 calls all clients with the ``"samp.app.echo"`` message type using
``"my-dummy-print"`` as a message-tag::

   >>> print(client2.call_all("my-dummy-print",
   ...                        {"samp.mtype": "samp.app.echo",
   ...                         "samp.params": {"txt": "Hello world!"}}))
   {'cli#1': 'msg#1;;cli#hub;;cli#2;;my-dummy-print'}
   Call: 8c8eb53178cb95e168ab17ec4eac2353 cli#2
   msg#1;;cli#hub;;cli#2;;my-dummy-print samp.app.echo {'txt': 'Hello world!'}
   {'host': 'antigone.lambrate.inaf.it', 'user': 'unknown'}
   Response: d0a28636321948ccff45edaf40888c54 cli#1 my-dummy-print
   {'samp.status': 'samp.ok', 'samp.result': {'txt': 'printed'}}

Client 2 then calls client 1 using the ``"samp.app.echo"`` message type,
tagging the message as ``"my-dummy-print-specific"``::

   >>> try:
   ...     print(client2.call(client1.get_public_id(),
   ...                        "my-dummy-print-specific",
   ...                        {"samp.mtype": "samp.app.echo",
   ...                         "samp.params": {"txt": "Hello client 1!"}}))
   ... except samp.SAMPProxyError as e:
   ...     print("Error ({0}): {1}".format(e.faultCode, e.faultString))
   msg#2;;cli#hub;;cli#2;;my-dummy-print-specific
   Call: 8c8eb53178cb95e168ab17ec4eac2353 cli#2
   msg#2;;cli#hub;;cli#2;;my-dummy-print-specific samp.app.echo {'txt': 'Hello
   Cli 1!'} {'host': 'antigone.lambrate.inaf.it', 'user': 'unknown'}
   Response: d0a28636321948ccff45edaf40888c54 cli#1 my-dummy-print-specific
   {'samp.status': 'samp.ok', 'samp.result': {'txt': 'printed'}}

We can now define a function called to test synchronous calls::

   >>> def test_receive_sync_call(private_key, sender_id, msg_id, mtype, params, extra):
   ...     import time
   ...     print("SYNC Call:", sender_id, msg_id, mtype, params, extra)
   ...     time.sleep(2)
   ...     client1.reply(msg_id, {"samp.status": samp.SAMP_STATUS_OK,
   ...                            "samp.result": {"txt": "printed sync"}})

We now bind the ``samp.test`` message type to ``test_receive_sync_call``::

   >>> client1.bind_receive_call("samp.test", test_receive_sync_call)
   >>> try:
   ...     # Sync call
   ...     print(client2.call_and_wait(client1.get_public_id(),
   ...                                 {"samp.mtype": "samp.test",
   ...                                  "samp.params": {"txt": "Hello SYNCRO client 1!"}},
   ...                                  "10"))
   ... except samp.SAMPProxyError as e:
   ...     # If timeout expires than a SAMPProxyError is returned
   ...     print("Error ({0}): {1}".format(e.faultCode, e.faultString))
   SYNC Call: cli#2 msg#3;;cli#hub;;cli#2;;sampy::sync::call samp.test {'txt':
   'Hello SYNCRO Cli 1!'} {'host': 'antigone.lambrate.inaf.it', 'user':
   'unknown'}
   {'samp.status': 'samp.ok', 'samp.result': {'txt': 'printed sync'}}

Finally, we disconnect the clients from the hub at the end::

   >>> client1.disconnect()
   >>> client2.disconnect()
.. doctest-skip-all

.. _vo-samp-example-table-image:

Sending and Receiving Tables and Images over SAMP
*************************************************

In the following examples, we make use of:

* `TOPCAT <http://www.star.bris.ac.uk/~mbt/topcat/>`_, which is a tool to
  explore tabular data.
* `SAO DS9 <http://ds9.si.edu/>`_, which is an image
  visualization tool that can overplot catalogs.
* `Aladin Desktop <http://aladin.u-strasbg.fr>`_, which is another tool that
  can visualize images and catalogs.

TOPCAT and Aladin will run a SAMP Hub if none is found, so for the following
examples you can either start up one of these applications first, or you can
start up the `astropy.samp` hub. You can start this using the following
command::

    $ samp_hub

Sending a Table to TOPCAT and DS9
=================================

The easiest way to send a VO table to TOPCAT is to make use of the
|SAMPIntegratedClient| class. Once TOPCAT is open, first instantiate a
|SAMPIntegratedClient| instance and then connect to the hub::

    >>> from astropy.samp import SAMPIntegratedClient
    >>> client = SAMPIntegratedClient()
    >>> client.connect()

Next, we have to set up a dictionary that contains details about the table to
send. This should include ``url``, which is the URL to the file, and ``name``,
which is a human-readable name for the table. The URL can be a local URL
(starting with ``file:///``)::

    >>> params = {}
    >>> params["url"] = 'file:///Users/tom/Desktop/aj285677t3_votable.xml'
    >>> params["name"] = "Robitaille et al. (2008), Table 3"

.. note:: To construct a local URL, you can also make use of ``urlparse`` as
          follows::

                >>> import urlparse
                >>> params["url"] = urlparse.urljoin('file:', os.path.abspath("aj285677t3_votable.xml"))

Now we can set up the message itself. This includes the type of message (here
we use ``table.load.votable``, which indicates that a VO table should be loaded
and the details of the table that we set above)::

    >>> message = {}
    >>> message["samp.mtype"] = "table.load.votable"
    >>> message["samp.params"] = params

Finally, we can broadcast this to all clients that are listening for
``table.load.votable`` messages using
:meth:`~astropy.samp.integrated_client.SAMPIntegratedClient.notify_all`::

    >>> client.notify_all(message)

The above message will actually be broadcast to all applications connected via
SAMP. For example, if we open `SAO DS9 <http://ds9.si.edu/>`_ in
addition to TOPCAT, and we run the above command, both applications will load
the table. We can use the
:meth:`~astropy.samp.integrated_client.SAMPIntegratedClient.get_registered_clients` method to
find all of the clients connected to the hub::

    >>> client.get_registered_clients()
    ['hub', 'c1', 'c2']

These IDs do not mean much, but we can find out more using::

   >>> client.get_metadata('c1')
   {'author.affiliation': 'Astrophysics Group, Bristol University',
    'author.email': 'm.b.taylor@bristol.ac.uk',
    'author.name': 'Mark Taylor',
    'home.page': 'http://www.starlink.ac.uk/topcat/',
    'samp.description.text': 'Tool for OPerations on Catalogues And Tables',
    'samp.documentation.url': 'http://127.0.0.1:2525/doc/sun253/index.html',
    'samp.icon.url': 'http://127.0.0.1:2525/doc/images/tc_sok.gif',
    'samp.name': 'topcat',
    'topcat.version': '4.0-1'}

We can see that ``c1`` is the TOPCAT client. We can now resend the data, but
this time only to TOPCAT, using the
:meth:`~astropy.samp.integrated_client.SAMPIntegratedClient.notify` method::

    >>> client.notify('c1', message)

Once finished, we should make sure we disconnect from the hub::

    >>> client.disconnect()

Receiving a Table from TOPCAT
=============================

To receive a table from TOPCAT, we have to set up a client that listens for
messages from the hub. As before, we instantiate a |SAMPIntegratedClient|
instance and connect to the hub::

    >>> from astropy.samp import SAMPIntegratedClient
    >>> client = SAMPIntegratedClient()
    >>> client.connect()

We now set up a receiver class which will handle any received messages. We need
to take care to write handlers for both notifications and calls (the difference
between the two being that calls expect a reply)::

    >>> class Receiver(object):
    ...     def __init__(self, client):
    ...         self.client = client
    ...         self.received = False
    ...     def receive_call(self, private_key, sender_id, msg_id, mtype, params, extra):
    ...         self.params = params
    ...         self.received = True
    ...         self.client.reply(msg_id, {"samp.status": "samp.ok", "samp.result": {}})
    ...     def receive_notification(self, private_key, sender_id, mtype, params, extra):
    ...         self.params = params
    ...         self.received = True

And we instantiate it:

    >>> r = Receiver(client)

We can now use the
:meth:`~astropy.samp.integrated_client.SAMPIntegratedClient.bind_receive_call`
and
:meth:`~astropy.samp.integrated_client.SAMPIntegratedClient.bind_receive_notification`
methods to tell our receiver to listen to all ``table.load.votable`` messages::

    >>> client.bind_receive_call("table.load.votable", r.receive_call)
    >>> client.bind_receive_notification("table.load.votable", r.receive_notification)

We can now check that the message has not been received yet::

    >>> r.received
    False

We can now broadcast the table from TOPCAT. After a few seconds, we can check
again if the message has been received::

    >>> r.received
    True

Success! The table URL should now be available in ``r.params['url']``, so we
can do::

    >>> from astropy.table import Table
    >>> t = Table.read(r.params['url'])
    Downloading http://127.0.0.1:2525/dynamic/4/t12.vot [Done]
    >>> t
               col1             col2     col3    col4     col5    col6 col7  col8 col9 col10
    ------------------------- -------- ------- -------- -------- ----- ---- ----- ---- -----
    SSTGLMC G000.0046+01.1431   0.0046  1.1432 265.2992 -28.3321  6.67 5.04  6.89 5.22     N
    SSTGLMC G000.0106-00.7315   0.0106 -0.7314 267.1274 -29.3063  7.18 6.07   nan 5.17     Y
    SSTGLMC G000.0110-01.0237   0.0110 -1.0236 267.4151 -29.4564  8.32 6.30  8.34 6.32     N
    ...

As before, we should remember to disconnect from the hub once we are done::

    >>> client.disconnect()

Example
=======

..
  EXAMPLE START
  Receiving and Reading a Table over SAMP

The following is a full example of a script that can be used to receive and
read a table. It includes a loop that waits until the message is received, and
reads the table once it has::

    import time

    from astropy.samp import SAMPIntegratedClient
    from astropy.table import Table

     # Instantiate the client and connect to the hub
    client=SAMPIntegratedClient()
    client.connect()

    # Set up a receiver class
    class Receiver(object):
        def __init__(self, client):
            self.client = client
            self.received = False
        def receive_call(self, private_key, sender_id, msg_id, mtype, params, extra):
            self.params = params
            self.received = True
            self.client.reply(msg_id, {"samp.status": "samp.ok", "samp.result": {}})
        def receive_notification(self, private_key, sender_id, mtype, params, extra):
            self.params = params
            self.received = True

    # Instantiate the receiver
    r = Receiver(client)

    # Listen for any instructions to load a table
    client.bind_receive_call("table.load.votable", r.receive_call)
    client.bind_receive_notification("table.load.votable", r.receive_notification)

    # We now run the loop to wait for the message in a try/finally block so that if
    # the program is interrupted e.g. by control-C, the client terminates
    # gracefully.

    try:

        # We test every 0.1s to see if the hub has sent a message
        while True:
            time.sleep(0.1)
            if r.received:
                t = Table.read(r.params['url'])
                break

    finally:

        client.disconnect()

    # Print out table
    print t

..
  EXAMPLE END

Sending an Image to DS9 and Aladin
==================================

As for tables, the most convenient way to send a FITS image over SAMP is to
make use of the |SAMPIntegratedClient| class. Once Aladin or DS9 are open,
first instantiate a |SAMPIntegratedClient| instance and then connect to the hub
as before::

    >>> from astropy.samp import SAMPIntegratedClient
    >>> client = SAMPIntegratedClient()
    >>> client.connect()

Next, we have to set up a dictionary that contains details about the image to
send. This should include ``url``, which is the URL to the file, and ``name``,
which is a human-readable name for the table. The URL can be a local URL
(starting with ``file:///``)::

    >>> params = {}
    >>> params["url"] = 'file:///Users/tom/Desktop/MSX_E.fits'
    >>> params["name"] = "MSX Band E Image of the Galactic Center"

See `Sending a Table to TOPCAT and DS9`_ for an example of a recommended way to
construct local URLs. Now we can set up the message itself. This includes the
type of message (here we use ``image.load.fits`` which indicates that a FITS
image should be loaded, and the details of the table that we set above)::

    >>> message = {}
    >>> message["samp.mtype"] = "image.load.fits"
    >>> message["samp.params"] = params

Finally, we can broadcast this to all clients that are listening for
``table.load.votable`` messages::

    >>> client.notify_all(message)

As for `Sending a Table to TOPCAT and DS9`_, the
:meth:`~astropy.samp.integrated_client.SAMPIntegratedClient.notify_all`
method will broadcast the image to all listening clients, and for tables it
is possible to instead use the
:meth:`~astropy.samp.integrated_client.SAMPIntegratedClient.notify` method
to send it to a specific client.

Once finished, we should make sure we disconnect from the hub::

    >>> client.disconnect()

Receiving a Table from DS9 or Aladin
====================================

Receiving images over SAMP is identical to `Receiving a Table from TOPCAT`_,
with the exception that the message type should be ``image.load.fits`` instead
of ``table.load.votable``. Once the URL has been received, the FITS image can
be opened with::

    >>> from astropy.io import fits
    >>> fits.open(r.params['url'])
.. doctest-skip-all

.. _vo-samp:

*************************************************************
SAMP (Simple Application Messaging Protocol) (`astropy.samp`)
*************************************************************

`astropy.samp` is a Python implementation of the SAMP messaging system.

Simple Application Messaging Protocol (SAMP) is an inter-process communication
system that allows different client programs, usually running on the same
computer, to communicate with each other by exchanging short messages that may
reference external data files. The protocol has been developed within the
International Virtual Observatory Alliance (IVOA) and is understood by many
desktop astronomy tools, including `TOPCAT
<http://www.star.bris.ac.uk/~mbt/topcat/>`_, `SAO DS9 <http://ds9.si.edu/>`_,
and `Aladin <http://aladin.u-strasbg.fr>`_.

So by using the classes in `astropy.samp`, Python code can interact with
other running desktop clients, for instance displaying a named FITS file in DS9,
prompting Aladin to recenter on a given sky position, or receiving a message
identifying the row when a user highlights a plotted point in TOPCAT.

The way the protocol works is that a SAMP "Hub" process must be running on the
local host, and then various client programs can connect to it. Once connected,
these clients can send messages to each other via the hub. The details are
described in the `SAMP standard <http://www.ivoa.net/documents/SAMP/>`_.

`astropy.samp` provides classes both to set up such a hub process, and to
help implement a client that can send and receive messages. It also provides a
stand-alone program ``samp_hub`` which can run a persistent hub in its own
process. Note that setting up the hub from Python is not always necessary, since
various other SAMP-aware applications may start up a hub independently; in most
cases, only one running hub is used during a SAMP session.

The following classes are available in `astropy.samp`:

* |SAMPHubServer|, which is used to instantiate a hub server that clients can
  then connect to.
* |SAMPHubProxy|, which is used to connect to an existing hub (including hubs
  started from other applications such as
  `TOPCAT <http://www.star.bris.ac.uk/~mbt/topcat/>`_).
* |SAMPClient|, which is used to create a SAMP client.
* |SAMPIntegratedClient|, which is the same as |SAMPClient| except that it has
  a self-contained |SAMPHubProxy| to provide a simpler user interface.

`astropy.samp` is a full implementation of `SAMP V1.3
<http://www.ivoa.net/documents/SAMP/20120411/>`_. As well as the Standard
Profile, it supports the Web Profile, which means that it can be used to also
communicate with web SAMP clients; see the `sampjs
<http://astrojs.github.io/sampjs/>`_ library examples for more details.

.. _IVOA Simple Application Messaging Protocol: http://www.ivoa.net/documents/latest/SAMP.html

Using `astropy.samp`
====================

.. toctree::
   :maxdepth: 2

   example_hub
   example_table_image
   example_clients
   advanced_embed_samp_hub

.. note that if this section gets too long, it should be moved to a separate
   doc page - see the top of performance.inc.rst for the instructions on how to do
   that
.. include:: performance.inc.rst

Reference/API
=============

.. automodapi:: astropy.samp

Acknowledgments
===============

This code is adapted from the `SAMPy <https://pypi.org/project/sampy>`__
package written by Luigi Paioro, who has granted the Astropy Project permission
to use the code under a BSD license.
.. doctest-skip-all

Embedding a SAMP Hub in a GUI
*****************************

Overview
========

If you wish to embed a SAMP hub in your Python Graphical User Interface (GUI)
tool, you will need to start the hub programmatically using::

    from astropy.samp import SAMPHubServer
    hub = SAMPHubServer()
    hub.start()

This launches the hub in a thread and is non-blocking. If you are not
interested in connections from web SAMP clients, then you can use::

    from astropy.samp import SAMPHubServer
    hub = SAMPHubServer(web_profile=False)
    hub.start()

This should be all you need to do. However, if you want to keep the Web
Profile active, there is an additional consideration: when a web
SAMP client connects, you will need to ask the user whether they accept the
connection (for security reasons). By default, the confirmation message is a
text-based message in the terminal, but if you have a GUI tool, you will
likely want to open a GUI dialog instead.

To do this, you will need to define a class that handles the dialog, and then
pass an **instance** of the class to |SAMPHubServer| (not the class itself).
This class should inherit from `astropy.samp.WebProfileDialog` and add the
following:

    1) A GUI timer callback that periodically calls
       ``WebProfileDialog.handle_queue`` (available as
       ``self.handle_queue``).

    2) A ``show_dialog`` method to display a consent dialog.
       It should take the following arguments:

           - ``samp_name``: The name of the application making the request.

           - ``details``: A dictionary of details about the client
             making the request. The only key in this dictionary required by
             the SAMP standard is ``samp.name`` which gives the name of the
             client making the request.

           - ``client``: A hostname, port pair containing the client
             address.

           - ``origin``: A string containing the origin of the
             request.

    3) Based on the user response, the ``show_dialog`` should call
       ``WebProfileDialog.consent`` or ``WebProfileDialog.reject``.
       This may, in some cases, be the result of another GUI callback.

Example of embedding a SAMP hub in a Tk application
---------------------------------------------------

..
  EXAMPLE START
  Embedding a SAMP Hub in a Tk Application

The following code is a full example of a Tk application that watches for web
SAMP connections and opens the appropriate dialog::

    import tkinter as tk
    import tkinter.messagebox as tkMessageBox

    from astropy.samp import SAMPHubServer
    from astropy.samp.hub import WebProfileDialog

    MESSAGE = """
    A Web application which declares to be

    Name: {name}
    Origin: {origin}

    is requesting to be registered with the SAMP Hub.  Pay attention
    that if you permit its registration, such application will acquire
    all current user privileges, like file read/write.

    Do you give your consent?
    """

    class TkWebProfileDialog(WebProfileDialog):
        def __init__(self, root):
            self.root = root
            self.wait_for_dialog()

        def wait_for_dialog(self):
            self.handle_queue()
            self.root.after(100, self.wait_for_dialog)

        def show_dialog(self, samp_name, details, client, origin):
            text = MESSAGE.format(name=samp_name, origin=origin)

            response = tkMessageBox.askyesno(
                'SAMP Hub', text,
                default=tkMessageBox.NO)

            if response:
                self.consent()
            else:
                self.reject()

    # Start up Tk application
    root = tk.Tk()
    tk.Label(root, text="Example SAMP Tk application",
             font=("Helvetica", 36), justify=tk.CENTER).pack(pady=200)
    root.geometry("500x500")
    root.update()

    # Start up SAMP hub
    h = SAMPHubServer(web_profile_dialog=TkWebProfileDialog(root))
    h.start()

    try:
        # Main GUI loop
        root.mainloop()
    except KeyboardInterrupt:
        pass

    h.stop()

If you run the above script, a window will open that says "Example SAMP Tk
application." If you then go to the following page, for example:

http://astrojs.github.io/sampjs/examples/pinger.html

and click on the Ping button, you will see the dialog open in the Tk
application. Once you click on "CONFIRM," future "Ping" calls will no longer
bring up the dialog.

..
  EXAMPLE END
.. note that if this is changed from the default approach of using an *include*
   (in index.rst) to a separate performance page, the header needs to be changed
   from === to ***, the filename extension needs to be changed from .inc.rst to
   .rst, and a link needs to be added in the sub-package toctree

.. _astropy-convolution-performance:

Performance Tips
================

The :func:`~astropy.convolution.convolve` function is best suited to small
kernels, and can become very slow for larger kernels. In this case, consider
using :func:`~astropy.convolution.convolve_fft` (though note that this function
uses more memory, and consider the different padding options).
Convolution Kernels
*******************

Introduction and Concept
========================

The convolution module provides several built-in kernels to cover the most
common applications in astronomy. It is also possible to define custom kernels
from arrays or combine existing kernels to match specific applications.

Every filter kernel is characterized by its response function. For time series
we speak of an "impulse response function" or for images we call it "point
spread function." This response function is given for every kernel by a
`~astropy.modeling.FittableModel`, which is evaluated on a grid with
:func:`~astropy.convolution.discretize_model` to obtain a kernel
array, which can be used for discrete convolution with the binned data.


Examples
========

1D Kernels
----------

..
  EXAMPLE START
  Using 1D Kernels to Smooth Noisy Data

One application of filtering is to smooth noisy data. In this case we
consider a noisy Lorentz curve:

>>> import numpy as np
>>> from astropy.modeling.models import Lorentz1D
>>> from astropy.convolution import convolve, Gaussian1DKernel, Box1DKernel
>>> lorentz = Lorentz1D(1, 0, 1)
>>> x = np.linspace(-5, 5, 100)
>>> data_1D = lorentz(x) + 0.1 * (np.random.rand(100) - 0.5)

Smoothing the noisy data with a `~astropy.convolution.Gaussian1DKernel`
with a standard deviation of 2 pixels:

>>> gauss_kernel = Gaussian1DKernel(2)
>>> smoothed_data_gauss = convolve(data_1D, gauss_kernel)

Smoothing the same data with a `~astropy.convolution.Box1DKernel` of width 5
pixels:

>>> box_kernel = Box1DKernel(5)
>>> smoothed_data_box = convolve(data_1D, box_kernel)

The following plot illustrates the results:

.. plot::

    import numpy as np
    import matplotlib.pyplot as plt
    from astropy.modeling.models import Lorentz1D
    from astropy.convolution import convolve, Gaussian1DKernel, Box1DKernel

    # Fake Lorentz data including noise
    lorentz = Lorentz1D(1, 0, 1)
    x = np.linspace(-5, 5, 100)
    data_1D = lorentz(x) + 0.1 * (np.random.rand(100) - 0.5)

    # Smooth data
    gauss_kernel = Gaussian1DKernel(2)
    smoothed_data_gauss = convolve(data_1D, gauss_kernel)
    box_kernel = Box1DKernel(5)
    smoothed_data_box = convolve(data_1D, box_kernel)

    # Plot data and smoothed data
    plt.plot(x, data_1D, label='Original')
    plt.plot(x, smoothed_data_gauss, label='Smoothed with Gaussian1DKernel')
    plt.plot(x, smoothed_data_box, label='Smoothed with Box1DKernel')
    plt.xlabel('x [a.u.]')
    plt.ylabel('amplitude [a.u.]')
    plt.xlim(-5, 5)
    plt.ylim(-0.1, 1.5)
    plt.legend(prop={'size':12})
    plt.show()


Beside the ``astropy`` convolution functions
`~astropy.convolution.convolve` and
`~astropy.convolution.convolve_fft`, it is also possible to use
the kernels with ``numpy`` or ``scipy`` convolution by passing the ``array``
attribute. This will be faster in most cases than the ``astropy`` convolution,
but will not work properly if NaN values are present in the data.

>>> smoothed = np.convolve(data_1D, box_kernel.array)

..
  EXAMPLE END

2D Kernels
----------

..
  EXAMPLE START
  Using 2D Kernels to Smooth Noisy Data

As all 2D kernels are symmetric, it is sufficient to specify the width in one
direction. Therefore the use of 2D kernels is basically the same as for 1D
kernels. Here we consider a small Gaussian-shaped source of amplitude 1 in the
middle of the image and add 10% noise:

>>> import numpy as np
>>> from astropy.convolution import convolve, Gaussian2DKernel, Tophat2DKernel
>>> from astropy.modeling.models import Gaussian2D
>>> gauss = Gaussian2D(1, 0, 0, 3, 3)
>>> # Fake image data including noise
>>> x = np.arange(-100, 101)
>>> y = np.arange(-100, 101)
>>> x, y = np.meshgrid(x, y)
>>> data_2D = gauss(x, y) + 0.1 * (np.random.rand(201, 201) - 0.5)

Smoothing the noisy data with a
:class:`~astropy.convolution.Gaussian2DKernel` with a standard
deviation of 2 pixels:

>>> gauss_kernel = Gaussian2DKernel(2)
>>> smoothed_data_gauss = convolve(data_2D, gauss_kernel)

Smoothing the noisy data with a
:class:`~astropy.convolution.Tophat2DKernel` of width 5 pixels:

>>> tophat_kernel = Tophat2DKernel(5)
>>> smoothed_data_tophat = convolve(data_2D, tophat_kernel)

This is what the original image looks like:

.. plot::

    import numpy as np
    import matplotlib.pyplot as plt
    from astropy.modeling.models import Gaussian2D
    gauss = Gaussian2D(1, 0, 0, 2, 2)
    # Fake image data including noise
    x = np.arange(-100, 101)
    y = np.arange(-100, 101)
    x, y = np.meshgrid(x, y)
    data_2D = gauss(x, y) + 0.1 * (np.random.rand(201, 201) - 0.5)
    plt.imshow(data_2D, origin='lower')
    plt.xlabel('x [pixels]')
    plt.ylabel('y [pixels]')
    plt.colorbar()
    plt.show()

The following plot illustrates the differences between several 2D kernels
applied to the simulated data. Note that it has a slightly different color
scale compared to the original image.

.. plot::

    import numpy as np
    import matplotlib.pyplot as plt

    from astropy.convolution import *
    from astropy.modeling.models import Gaussian2D

    # Small Gaussian source in the middle of the image
    gauss = Gaussian2D(1, 0, 0, 2, 2)
    # Fake data including noise
    x = np.arange(-100, 101)
    y = np.arange(-100, 101)
    x, y = np.meshgrid(x, y)
    data_2D = gauss(x, y) + 0.1 * (np.random.rand(201, 201) - 0.5)

    # Setup kernels, including unity kernel for original image
    # Choose normalization for linear scale space for RickerWavelet

    kernels = [TrapezoidDisk2DKernel(11, slope=0.2),
               Tophat2DKernel(11),
               Gaussian2DKernel(11),
               Box2DKernel(11),
               11 ** 2 * RickerWavelet2DKernel(11),
               AiryDisk2DKernel(11)]

    fig, axes = plt.subplots(nrows=2, ncols=3)

    # Plot kernels
    for kernel, ax in zip(kernels, axes.flat):
        smoothed = convolve(data_2D, kernel, normalize_kernel=False)
        im = ax.imshow(smoothed, vmin=-0.01, vmax=0.08, origin='lower',
                       interpolation='None')
        title = kernel.__class__.__name__
        ax.set_title(title, fontsize=12)
        ax.set_yticklabels([])
        ax.set_xticklabels([])

    cax = fig.add_axes([0.9, 0.1, 0.03, 0.8])
    fig.colorbar(im, cax=cax)
    plt.subplots_adjust(left=0.05, right=0.85, top=0.95, bottom=0.05)
    plt.show()


The Gaussian kernel has better smoothing properties compared to the Box and the
Top Hat. The Box filter is not isotropic and can produce artifacts (the source
appears rectangular). The Ricker Wavelet filter removes noise and slowly varying
structures (i.e., background), but produces a negative ring around the source.
The best choice for the filter strongly depends on the application.

..
  EXAMPLE END

Available Kernels
=================

.. currentmodule:: astropy.convolution

.. autosummary::

   AiryDisk2DKernel
   Box1DKernel
   Box2DKernel
   CustomKernel
   Gaussian1DKernel
   Gaussian2DKernel
   RickerWavelet1DKernel
   RickerWavelet2DKernel
   Model1DKernel
   Model2DKernel
   Moffat2DKernel
   Ring2DKernel
   Tophat2DKernel
   Trapezoid1DKernel
   TrapezoidDisk2DKernel

Kernel Arithmetics
==================

Addition and Subtraction
------------------------

As convolution is a linear operation, kernels can be added or subtracted from
each other. They can also be multiplied with some number.

Examples
^^^^^^^^

..
  EXAMPLE START
  Adding and Subtracting Kernels in astropy.convolution

One basic example of subtracting kernels would be the definition of a
Difference of Gaussian filter:

>>> from astropy.convolution import Gaussian1DKernel
>>> gauss_1 = Gaussian1DKernel(10)
>>> gauss_2 = Gaussian1DKernel(16)
>>> DoG = gauss_2 - gauss_1

Another application is to convolve faked data with an instrument response
function model. For example, if the response function can be described by
the weighted sum of two Gaussians:

>>> gauss_1 = Gaussian1DKernel(10)
>>> gauss_2 = Gaussian1DKernel(16)
>>> SoG = 4 * gauss_1 + gauss_2

Most times it will be necessary to normalize the resulting kernel by calling
explicitly:

>>> SoG.normalize()

..
  EXAMPLE END

Convolution
-----------

Furthermore, two kernels can be convolved with each other, which is useful when
data is filtered with two different kinds of kernels or to create a new,
special kernel.

Examples
^^^^^^^^

..
  EXAMPLE START
  Convolving Kernels in astropy.convolution

To convolve two kernels with each other:

>>> from astropy.convolution import Gaussian1DKernel, convolve
>>> gauss_1 = Gaussian1DKernel(10)
>>> gauss_2 = Gaussian1DKernel(16)
>>> broad_gaussian = convolve(gauss_2,  gauss_1)  # doctest: +IGNORE_WARNINGS

Or in case of multistage smoothing:

>>> import numpy as np
>>> from astropy.modeling.models import Lorentz1D
>>> from astropy.convolution import convolve, Gaussian1DKernel, Box1DKernel
>>> lorentz = Lorentz1D(1, 0, 1)
>>> x = np.linspace(-5, 5, 100)
>>> data_1D = lorentz(x) + 0.1 * (np.random.rand(100) - 0.5)

>>> gauss = Gaussian1DKernel(3)
>>> box = Box1DKernel(5)
>>> smoothed_gauss = convolve(data_1D, gauss)
>>> smoothed_gauss_box = convolve(smoothed_gauss, box)

You would rather do the following:

>>> gauss = Gaussian1DKernel(3)
>>> box = Box1DKernel(5)
>>> smoothed_gauss_box = convolve(data_1D, convolve(box, gauss))  # doctest: +IGNORE_WARNINGS

Which, in most cases, will also be faster than the first method because only
one convolution with the often times larger data array will be necessary.

..
  EXAMPLE END

Discretization
==============

To obtain the kernel array for discrete convolution, the kernel's response
function is evaluated on a grid with
:func:`~astropy.convolution.discretize_model`. For the
discretization step the following modes are available:

* Mode ``'center'`` (default) evaluates the response function on the grid by
  taking the value at the center of the bin.

   >>> from astropy.convolution import Gaussian1DKernel
   >>> gauss_center = Gaussian1DKernel(3, mode='center')

* Mode ``'linear_interp'`` takes the values at the corners of the bin and
  linearly interpolates the value at the center:

  >>> gauss_interp = Gaussian1DKernel(3, mode='linear_interp')

* Mode ``'oversample'`` evaluates the response function by taking the mean on an
  oversampled grid. The oversample factor can be specified with the ``factor``
  argument. If the oversample factor is too large, the evaluation becomes slow.

 >>> gauss_oversample = Gaussian1DKernel(3, mode='oversample', factor=10)

* Mode ``'integrate'`` integrates the function over the pixel using
  ``scipy.integrate.quad`` and ``scipy.integrate.dblquad``. This mode is very
  slow and is only recommended when the highest accuracy is required.

.. doctest-requires:: scipy

    >>> gauss_integrate = Gaussian1DKernel(3, mode='integrate')

Especially in the range where the kernel width is in order of only a few pixels,
it can be advantageous to use the mode ``oversample`` or ``integrate`` to
conserve the integral on a subpixel scale.


Normalization
=============

The kernel models are normalized per default (i.e.,
:math:`\int_{-\infty}^{\infty} f(x) dx = 1`). But because of the limited kernel
array size, the normalization for kernels with an infinite response can differ
from one. The value of this deviation is stored in the kernel's ``truncation``
attribute.

The normalization can also differ from one, especially for small kernels, due
to the discretization step. This can be partly controlled by the ``mode``
argument, when initializing the kernel. (See also
:func:`~astropy.convolution.discretize_model`.) Setting the
``mode`` to ``'oversample'`` allows us to conserve the normalization even on the
subpixel scale.

The kernel arrays can be renormalized explicitly by calling either the
``normalize()`` method or by setting the ``normalize_kernel`` argument in the
:func:`~astropy.convolution.convolve` and
:func:`~astropy.convolution.convolve_fft` functions. The latter
method leaves the kernel itself unchanged but works with an internal normalized
version of the kernel.

Note that for :class:`~astropy.convolution.RickerWavelet1DKernel`
and :class:`~astropy.convolution.RickerWavelet2DKernel` there is
:math:`\int_{-\infty}^{\infty} f(x) dx = 0`. To define a proper normalization,
both filters are derived from a normalized Gaussian function.
Using the Convolution Functions
*******************************

Overview
========

Two convolution functions are provided. They are imported as::

    >>> from astropy.convolution import convolve, convolve_fft

and are both used as::

    >>> result = convolve(image, kernel)  # doctest: +SKIP
    >>> result = convolve_fft(image, kernel)  # doctest: +SKIP

:func:`~astropy.convolution.convolve` is implemented as a
direct convolution algorithm, while
:func:`~astropy.convolution.convolve_fft` uses a Fast Fourier
Transform (FFT). Thus, the former is better for small kernels, while the latter
is much more efficient for larger kernels.

The input images and kernels should be lists or ``numpy`` arrays with either 1,
2, or 3 dimensions (and the number of dimensions should be the same for the
image and kernel). The result is a ``numpy`` array with the same dimensions as
the input image. The convolution is always done as floating point.

The :func:`~astropy.convolution.convolve` function takes an
optional ``boundary=`` argument describing how to perform the convolution at
the edge of the array. The values for ``boundary`` can be:

* ``None``: set the result values to zero where the kernel extends beyond the
  edge of the array (default).

* ``'fill'``: set values outside the array boundary to a constant. If this
  option is specified, the constant should be specified using the
  ``fill_value=`` argument, which defaults to zero.

* ``'wrap'``: assume that the boundaries are periodic.

* ``'extend'`` : set values outside the array to the nearest array value.

By default, the kernel is not normalized. To normalize it prior to convolution,
use::

    >>> result = convolve(image, kernel, normalize_kernel=True)  # doctest: +SKIP

Examples
--------

..
  EXAMPLE START
  Smoothing Arrays with Custom Kernels

Smooth a 1D array with a custom kernel and no boundary treatment::

    >>> import numpy as np
    >>> convolve([1, 4, 5, 6, 5, 7, 8], [0.2, 0.6, 0.2])  # doctest: +FLOAT_CMP
    array([1.4, 3.6, 5. , 5.6, 5.6, 6.8, 6.2])

As above, but using the 'extend' algorithm for boundaries::

    >>> convolve([1, 4, 5, 6, 5, 7, 8], [0.2, 0.6, 0.2], boundary='extend')  # doctest: +FLOAT_CMP
    array([1.6, 3.6, 5. , 5.6, 5.6, 6.8, 7.8])

If a NaN value is present in the original array, it will be
interpolated using the kernel::

    >>> import numpy as np
    >>> convolve([1, 4, 5, 6, np.nan, 7, 8], [0.2, 0.6, 0.2], boundary='extend')  # doctest: +FLOAT_CMP
    array([1.6 , 3.6 , 5.  , 5.75, 6.5 , 7.25, 7.8 ])

..
  EXAMPLE END

..
  EXAMPLE START
  Constructing Kernels from Lists

Kernels and arrays can be specified either as lists or as ``numpy``
arrays. The following examples show how to construct a 1D array as a
list::

    >>> kernel = [0, 1, 0]
    >>> result = convolve(spectrum, kernel)  # doctest: +SKIP

A 2D array as a list::

    >>> kernel = [[0, 1, 0],
    ...           [1, 2, 1],
    ...           [0, 1, 0]]
    >>> result = convolve(image, kernel)  # doctest: +SKIP

And a 3D array as a list::

    >>> kernel = [[[0, 0, 0], [0, 2, 0], [0, 0, 0]],
    ...           [[0, 1, 0], [2, 3, 2], [0, 1, 0]],
    ...           [[0, 0, 0], [0, 2, 0], [0, 0, 0]]]
    >>> result = convolve(cube, kernel)  # doctest: +SKIP

..
  EXAMPLE END

Kernels
=======

The above examples use custom kernels, but `astropy.convolution` also
includes a number of built-in kernels, which are described in
:doc:`kernels`.
************************************
Convolving with Unnormalized Kernels
************************************

There are some tasks, such as source finding, where you want to apply a filter
with a kernel that is not normalized.

For data that are well-behaved (contain no missing or infinite values), this
can be done in one step::

    convolve(image, kernel)

Examples
--------

..
  EXAMPLE START
  Convolving with Unnormalized Kernels

For an example of applying a filter with a kernel that is not normalized, we
can try to run a commonly used peak enhancing kernel:

.. plot::
   :context: reset
   :include-source:
   :align: center

    import numpy as np
    import matplotlib.pyplot as plt

    from astropy.io import fits
    from astropy.utils.data import get_pkg_data_filename
    from astropy.convolution import CustomKernel
    from scipy.signal import convolve as scipy_convolve
    from astropy.convolution import convolve, convolve_fft


    # Load the data from data.astropy.org
    filename = get_pkg_data_filename('galactic_center/gc_msx_e.fits')
    hdu = fits.open(filename)[0]

    # Scale the file to have reasonable numbers
    # (this is mostly so that colorbars don't have too many digits)
    # Also, we crop it so you can see individual pixels
    img = hdu.data[50:90, 60:100] * 1e5

    kernel = CustomKernel([[-1,-1,-1], [-1, 8, -1], [-1,-1,-1]])

    astropy_conv = convolve(img, kernel, normalize_kernel=False, nan_treatment='fill')
    #astropy_conv_fft = convolve_fft(img, kernel, normalize_kernel=False, nan_treatment='fill')

    plt.figure(1, figsize=(12, 12)).clf()
    ax1 = plt.subplot(1, 2, 1)
    im = ax1.imshow(img, vmin=-6., vmax=5.e1, origin='lower',
                    interpolation='nearest', cmap='viridis')

    ax2 = plt.subplot(1, 2, 2)
    im = ax2.imshow(astropy_conv, vmin=-6., vmax=5.e1, origin='lower',
                    interpolation='nearest', cmap='viridis')

..
  EXAMPLE END

..
  EXAMPLE START
  Replacing NaN Values with Interpolated Values Using Kernels

If you have an image with missing values (NaNs), you have to replace them with
real values first. Often, the best way to do this is to replace the NaN values
with interpolated values. In the example below, we use a Gaussian kernel
with a size similar to that of our peak-finding kernel to replace the bad data
before applying the peak-finding kernel.

.. plot::
   :context:
   :include-source:
   :align: center

   from astropy.convolution import Gaussian2DKernel, interpolate_replace_nans

   # Select a random set of pixels that were affected by some sort of artifact
   # and replaced with NaNs (e.g., cosmic-ray-affected pixels)
   np.random.seed(42)
   yinds, xinds = np.indices(img.shape)
   img[np.random.choice(yinds.flat, 50), np.random.choice(xinds.flat, 50)] = np.nan

   # We smooth with a Gaussian kernel with x_stddev=1 (and y_stddev=1)
   # It is a 9x9 array
   kernel = Gaussian2DKernel(x_stddev=1)

   # interpolate away the NaNs
   reconstructed_image = interpolate_replace_nans(img, kernel)


   # apply peak-finding
   kernel = CustomKernel([[-1,-1,-1], [-1, 8, -1], [-1,-1,-1]])

   # Use the peak-finding kernel
   # We have to turn off kernel normalization and set nan_treatment to "fill"
   # here because `nan_treatment='interpolate'` is incompatible with non-
   # normalized kernels
   peaked_image = convolve(reconstructed_image, kernel,
                           normalize_kernel=False,
                           nan_treatment='fill')

   plt.figure(1, figsize=(12, 12)).clf()
   ax1 = plt.subplot(1, 3, 1)
   ax1.set_title("Image with missing data")
   im = ax1.imshow(img, vmin=-6., vmax=5.e1, origin='lower',
                   interpolation='nearest', cmap='viridis')

   ax2 = plt.subplot(1, 3, 2)
   ax2.set_title("Interpolated")
   im = ax2.imshow(reconstructed_image, vmin=-6., vmax=5.e1, origin='lower',
                   interpolation='nearest', cmap='viridis')

   ax3 = plt.subplot(1, 3, 3)
   ax3.set_title("Peak-Finding")
   im = ax3.imshow(peaked_image, vmin=-6., vmax=5.e1, origin='lower',
                   interpolation='nearest', cmap='viridis')

..
  EXAMPLE END
.. _astropy_convolve:

*************************************************
Convolution and Filtering (`astropy.convolution`)
*************************************************

Introduction
============

`astropy.convolution` provides convolution functions and kernels that offer
improvements compared to the SciPy `scipy.ndimage` convolution routines,
including:

* Proper treatment of NaN values (ignoring them during convolution and
  replacing NaN pixels with interpolated values)

* A single function for 1D, 2D, and 3D convolution

* Improved options for the treatment of edges

* Both direct and Fast Fourier Transform (FFT) versions

* Built-in kernels that are commonly used in Astronomy

The following thumbnails show the difference between ``scipy`` and
``astropy`` convolve functions on an astronomical image that contains NaN
values. ``scipy``'s function essentially returns NaN for all pixels that are
within a kernel of any NaN value, which is often not the desired result.

.. plot::
   :context: reset
   :include-source:
   :align: center

    import numpy as np
    import matplotlib.pyplot as plt

    from astropy.io import fits
    from astropy.utils.data import get_pkg_data_filename
    from astropy.convolution import Gaussian2DKernel
    from scipy.signal import convolve as scipy_convolve
    from astropy.convolution import convolve


    # Load the data from data.astropy.org
    filename = get_pkg_data_filename('galactic_center/gc_msx_e.fits')
    hdu = fits.open(filename)[0]

    # Scale the file to have reasonable numbers
    # (this is mostly so that colorbars do not have too many digits)
    # Also, we crop it so you can see individual pixels
    img = hdu.data[50:90, 60:100] * 1e5

    # This example is intended to demonstrate how astropy.convolve and
    # scipy.convolve handle missing data, so we start by setting the
    # brightest pixels to NaN to simulate a "saturated" data set
    img[img > 2e1] = np.nan

    # We also create a copy of the data and set those NaNs to zero.  We will
    # use this for the scipy convolution
    img_zerod = img.copy()
    img_zerod[np.isnan(img)] = 0

    # We smooth with a Gaussian kernel with x_stddev=1 (and y_stddev=1)
    # It is a 9x9 array
    kernel = Gaussian2DKernel(x_stddev=1)

    # Convolution: scipy's direct convolution mode spreads out NaNs (see
    # panel 2 below)
    scipy_conv = scipy_convolve(img, kernel, mode='same', method='direct')

    # scipy's direct convolution mode run on the 'zero'd' image will not
    # have NaNs, but will have some very low value zones where the NaNs were
    # (see panel 3 below)
    scipy_conv_zerod = scipy_convolve(img_zerod, kernel, mode='same',
                                      method='direct')

    # astropy's convolution replaces the NaN pixels with a kernel-weighted
    # interpolation from their neighbors
    astropy_conv = convolve(img, kernel)


    # Now we do a bunch of plots.  In the first two plots, the originally masked
    # values are marked with red X's
    plt.figure(1, figsize=(12, 12)).clf()
    ax1 = plt.subplot(2, 2, 1)
    im = ax1.imshow(img, vmin=-2., vmax=2.e1, origin='lower',
                    interpolation='nearest', cmap='viridis')
    y, x = np.where(np.isnan(img))
    ax1.set_autoscale_on(False)
    ax1.plot(x, y, 'rx', markersize=4)
    ax1.set_title("Original")
    ax1.set_xticklabels([])
    ax1.set_yticklabels([])

    ax2 = plt.subplot(2, 2, 2)
    im = ax2.imshow(scipy_conv, vmin=-2., vmax=2.e1, origin='lower',
                    interpolation='nearest', cmap='viridis')
    ax2.set_autoscale_on(False)
    ax2.plot(x, y, 'rx', markersize=4)
    ax2.set_title("Scipy")
    ax2.set_xticklabels([])
    ax2.set_yticklabels([])

    ax3 = plt.subplot(2, 2, 3)
    im = ax3.imshow(scipy_conv_zerod, vmin=-2., vmax=2.e1, origin='lower',
                    interpolation='nearest', cmap='viridis')
    ax3.set_title("Scipy nan->zero")
    ax3.set_xticklabels([])
    ax3.set_yticklabels([])

    ax4 = plt.subplot(2, 2, 4)
    im = ax4.imshow(astropy_conv, vmin=-2., vmax=2.e1, origin='lower',
                    interpolation='nearest', cmap='viridis')
    ax4.set_title("Default astropy")
    ax4.set_xticklabels([])
    ax4.set_yticklabels([])

    # we make a second plot of the amplitudes vs offset position to more
    # clearly illustrate the value differences
    plt.figure(2).clf()
    plt.plot(img[:, 25], label='input', drawstyle='steps-mid', linewidth=2,
             alpha=0.5)
    plt.plot(scipy_conv[:, 25], label='scipy', drawstyle='steps-mid',
             linewidth=2, alpha=0.5, marker='s')
    plt.plot(scipy_conv_zerod[:, 25], label='scipy nan->zero',
             drawstyle='steps-mid', linewidth=2, alpha=0.5, marker='s')
    plt.plot(astropy_conv[:, 25], label='astropy', drawstyle='steps-mid',
             linewidth=2, alpha=0.5)
    plt.ylabel("Amplitude")
    plt.ylabel("Position Offset")
    plt.legend(loc='best')
    plt.show()


The following sections describe how to make use of the convolution functions,
and how to use built-in convolution kernels:

Getting Started
===============

Two convolution functions are provided. They are imported as::

    from astropy.convolution import convolve, convolve_fft

and are both used as::

    result = convolve(image, kernel)
    result = convolve_fft(image, kernel)

:func:`~astropy.convolution.convolve` is implemented as a direct convolution
algorithm, while :func:`~astropy.convolution.convolve_fft` uses a Fast Fourier
Transform (FFT). Thus, the former is better for small kernels, while the latter
is much more efficient for larger kernels.

Example
-------

..
  EXAMPLE START
  Convolution for User-Specified Kernels

To convolve a 1D dataset with a user-specified kernel, you can do::

    >>> from astropy.convolution import convolve
    >>> convolve([1, 4, 5, 6, 5, 7, 8], [0.2, 0.6, 0.2])  # doctest: +FLOAT_CMP
    array([1.4, 3.6, 5. , 5.6, 5.6, 6.8, 6.2])

Notice that the end points are set to zero  by default, points that are too
close to the boundary to have a convolved value calculated are set to zero.
However, the :func:`~astropy.convolution.convolve` function allows for a
``boundary`` argument that can be used to specify alternate behaviors. For
example, setting ``boundary='extend'`` causes values near the edges to be
computed, assuming the original data is simply extended using a constant
extrapolation beyond the boundary::

    >>> from astropy.convolution import convolve
    >>> convolve([1, 4, 5, 6, 5, 7, 8], [0.2, 0.6, 0.2], boundary='extend')  # doctest: +FLOAT_CMP
    array([1.6, 3.6, 5. , 5.6, 5.6, 6.8, 7.8])

The values at the end are computed assuming that any value below the first
point is ``1``, and any value above the last point is ``8``. For a more
detailed discussion of boundary treatment, see :doc:`using`.

..
  EXAMPLE END

Example
-------

..
  EXAMPLE START
  Convolution for Built-In Kernels

The convolution module also includes built-in kernels that can be imported as,
for example::

    >>> from astropy.convolution import Gaussian1DKernel

To use a kernel, first create a specific instance of the kernel::

    >>> gauss = Gaussian1DKernel(stddev=2)

``gauss`` is not an array, but a kernel object. The underlying array can be
retrieved with::

    >>> gauss.array  # doctest: +FLOAT_CMP
    array([6.69151129e-05, 4.36341348e-04, 2.21592421e-03,
           8.76415025e-03, 2.69954833e-02, 6.47587978e-02,
           1.20985362e-01, 1.76032663e-01, 1.99471140e-01,
           1.76032663e-01, 1.20985362e-01, 6.47587978e-02,
           2.69954833e-02, 8.76415025e-03, 2.21592421e-03,
           4.36341348e-04, 6.69151129e-05])

The kernel can then be used directly when calling
:func:`~astropy.convolution.convolve`:

.. plot::
   :include-source:

    import numpy as np
    import matplotlib.pyplot as plt

    from astropy.convolution import Gaussian1DKernel, convolve

    plt.figure(3).clf()

    # Generate fake data
    x = np.arange(1000).astype(float)
    y = np.sin(x / 100.) + np.random.normal(0., 1., x.shape)
    y[::3] = np.nan

    # Create kernel
    g = Gaussian1DKernel(stddev=50)

    # Convolve data
    z = convolve(y, g)

    # Plot data before and after convolution
    plt.plot(x, y, 'k-', label='Before')
    plt.plot(x, z, 'b-', label='After', alpha=0.5, linewidth=2)
    plt.legend(loc='best')
    plt.show()

..
  EXAMPLE END

Using ``astropy``'s Convolution to Replace Bad Data
---------------------------------------------------

``astropy``'s convolution methods can be used to replace bad data with values
interpolated from their neighbors. Kernel-based interpolation is useful for
handling images with a few bad pixels or for interpolating sparsely sampled
images.

The interpolation tool is implemented and used as::

    from astropy.convolution import interpolate_replace_nans
    result = interpolate_replace_nans(image, kernel)

Some contexts in which you might want to use kernel-based interpolation include:

 * Images with saturated pixels. Generally, these are the highest-intensity
   regions in the imaged area, and the interpolated values are not reliable,
   but this can be useful for display purposes.
 * Images with flagged pixels (e.g., a few small regions affected by cosmic
   rays or other spurious signals that require those pixels to be flagged out).
   If the affected region is small enough, the resulting interpolation will have
   a small effect on source statistics and may allow for robust source-finding
   algorithms to be run on the resulting data.
 * Sparsely sampled images such as those constructed with single-pixel
   detectors. Such images will only have a few discrete points sampled across
   the imaged area, but an approximation of the extended sky emission can still
   be constructed.

.. note::
    Care must be taken to ensure that the kernel is large enough to completely
    cover potential contiguous regions of NaN values.
    An ``AstropyUserWarning`` is raised if NaN values are detected post-
    convolution, in which case the kernel size should be increased.

Example
^^^^^^^

..
  EXAMPLE START
  Kernel Interpolation to Fill in Flagged-Out Pixels

The script below shows an example of kernel interpolation to fill in
flagged-out pixels:

.. plot::
   :context:
   :include-source:
   :align: center

   import numpy as np
   import matplotlib.pyplot as plt

   from astropy.io import fits
   from astropy.utils.data import get_pkg_data_filename
   from astropy.convolution import Gaussian2DKernel, interpolate_replace_nans

   # Load the data from data.astropy.org
   filename = get_pkg_data_filename('galactic_center/gc_msx_e.fits')

   hdu = fits.open(filename)[0]
   img = hdu.data[50:90, 60:100] * 1e5

   # This example is intended to demonstrate how astropy.convolve and
   # scipy.convolve handle missing data, so we start by setting the brightest
   # pixels to NaN to simulate a "saturated" data set
   img[img > 2e1] = np.nan

   # We smooth with a Gaussian kernel with x_stddev=1 (and y_stddev=1)
   # It is a 9x9 array
   kernel = Gaussian2DKernel(x_stddev=1)

   # create a "fixed" image with NaNs replaced by interpolated values
   fixed_image = interpolate_replace_nans(img, kernel)

   # Now we do a bunch of plots.  In the first two plots, the originally masked
   # values are marked with red X's
   plt.figure(1, figsize=(12, 6)).clf()
   plt.close(2) # close the second plot from above

   ax1 = plt.subplot(1, 2, 1)
   im = ax1.imshow(img, vmin=-2., vmax=2.e1, origin='lower',
                   interpolation='nearest', cmap='viridis')
   y, x = np.where(np.isnan(img))
   ax1.set_autoscale_on(False)
   ax1.plot(x, y, 'rx', markersize=4)
   ax1.set_title("Original")
   ax1.set_xticklabels([])
   ax1.set_yticklabels([])

   ax2 = plt.subplot(1, 2, 2)
   im = ax2.imshow(fixed_image, vmin=-2., vmax=2.e1, origin='lower',
                   interpolation='nearest', cmap='viridis')
   ax2.set_title("Fixed")
   ax2.set_xticklabels([])
   ax2.set_yticklabels([])

..
  EXAMPLE END

Example
^^^^^^^

..
  EXAMPLE START
  Kernel Interpolation to Reconstruct Images from Sparse Sampling.

This script shows the power of this technique for reconstructing images from
sparse sampling. Note that the image is not perfect: the pointlike sources
are sometimes missed, but the extended structure is very well recovered by
eye.

.. plot::
   :context:
   :include-source:
   :align: center

   import numpy as np
   import matplotlib.pyplot as plt

   from astropy.io import fits
   from astropy.utils.data import get_pkg_data_filename
   from astropy.convolution import Gaussian2DKernel, interpolate_replace_nans

   # Load the data from data.astropy.org
   filename = get_pkg_data_filename('galactic_center/gc_msx_e.fits')

   hdu = fits.open(filename)[0]
   img = hdu.data[50:90, 60:100] * 1e5

   indices = np.random.randint(low=0, high=img.size, size=300)

   sampled_data = img.flat[indices]

   # Build a new, sparsely sampled version of the original image
   new_img = np.tile(np.nan, img.shape)
   new_img.flat[indices] = sampled_data

   # We smooth with a Gaussian kernel with x_stddev=1 (and y_stddev=1)
   # It is a 9x9 array
   kernel = Gaussian2DKernel(x_stddev=1)

   # create a "reconstructed" image with NaNs replaced by interpolated values
   reconstructed_image = interpolate_replace_nans(new_img, kernel)

   # Now we do a bunch of plots.  In the first two plots, the originally masked
   # values are marked with red X's
   plt.figure(1, figsize=(12, 6)).clf()
   ax1 = plt.subplot(1, 3, 1)
   im = ax1.imshow(img, vmin=-2., vmax=2.e1, origin='lower',
                   interpolation='nearest', cmap='viridis')
   y, x = np.where(np.isnan(img))
   ax1.set_autoscale_on(False)
   ax1.set_title("Original")
   ax1.set_xticklabels([])
   ax1.set_yticklabels([])

   ax2 = plt.subplot(1, 3, 2)
   im = ax2.imshow(new_img, vmin=-2., vmax=2.e1, origin='lower',
                   interpolation='nearest', cmap='viridis')
   ax2.set_title("Sparsely Sampled")
   ax2.set_xticklabels([])
   ax2.set_yticklabels([])

   ax2 = plt.subplot(1, 3, 3)
   im = ax2.imshow(reconstructed_image, vmin=-2., vmax=2.e1, origin='lower',
                   interpolation='nearest', cmap='viridis')
   ax2.set_title("Reconstructed")
   ax2.set_xticklabels([])
   ax2.set_yticklabels([])

..
  EXAMPLE END

Using `astropy.convolution`
===========================

.. toctree::
   :maxdepth: 2

   using.rst
   kernels.rst
   non_normalized_kernels.rst

.. note that if this section gets too long, it should be moved to a separate
   doc page - see the top of performance.inc.rst for the instructions on how to
   do that
.. include:: performance.inc.rst

Reference/API
=============

.. automodapi:: astropy.convolution
    :no-inheritance-diagram:
    :skip: MexicanHat1DKernel
    :skip: MexicanHat2DKernel
.. _stats-ripley:

******************************
Ripley's K Function Estimators
******************************

Spatial correlation functions have been used in the astronomical
context to estimate the probability of finding an object (e.g., a galaxy)
within a given distance of another object [1]_.

Ripley's K function is a type of estimator used to characterize the correlation
of such spatial point processes
[2]_, [3]_, [4]_, [5]_, [6]_.
More precisely, it describes correlation among objects in a given field.
The `~astropy.stats.RipleysKEstimator` class implements some
estimators for this function which provides several methods for
edge effects correction.

Basic Usage
===========

The actual implementation of Ripley's K function estimators lie in the method
``evaluate``, which take the following arguments: ``data``, ``radii``, and
optionally, ``mode``.

The ``data`` argument is a 2D array which represents the set of observed
points (events) in the area of study. The ``radii`` argument corresponds to a
set of distances for which the estimator will be evaluated. The ``mode``
argument takes a value on the following linguistic set
``{none, translation, ohser, var-width, ripley}``; each keyword represents a
different method to perform correction due to edge effects. See the API
documentation and references for details about these methods.

Instances of `~astropy.stats.RipleysKEstimator` can also be used as
callables (which is equivalent to calling the ``evaluate`` method).

Example
-------

..
  EXAMPLE START
  Using Ripley's K Function Estimators

To use Ripley's K Function Estimators from ``astropy``'s stats sub-package:

.. plot::
    :include-source:

    import numpy as np
    from matplotlib import pyplot as plt
    from astropy.stats import RipleysKEstimator

    z = np.random.uniform(low=5, high=10, size=(100, 2))
    Kest = RipleysKEstimator(area=25, x_max=10, y_max=10, x_min=5, y_min=5)

    r = np.linspace(0, 2.5, 100)
    plt.plot(r, Kest.poisson(r), color='green', ls=':', label=r'$K_{pois}$')
    plt.plot(r, Kest(data=z, radii=r, mode='none'), color='red', ls='--',
             label=r'$K_{un}$')
    plt.plot(r, Kest(data=z, radii=r, mode='translation'), color='black',
             label=r'$K_{trans}$')
    plt.plot(r, Kest(data=z, radii=r, mode='ohser'), color='blue', ls='-.',
             label=r'$K_{ohser}$')
    plt.plot(r, Kest(data=z, radii=r, mode='var-width'), color='green',
             label=r'$K_{var-width}$')
    plt.plot(r, Kest(data=z, radii=r, mode='ripley'), color='yellow',
             label=r'$K_{ripley}$')
    plt.legend()

..
  EXAMPLE END

References
==========
.. [1] Peebles, P.J.E. *The large scale structure of the universe*.
       <https://ui.adsabs.harvard.edu/abs/1980lssu.book.....P>
.. [2] Ripley, B.D. *The second-order analysis of stationary point processes*.
       Journal of Applied Probability. 13: 255266, 1976.
.. [3] *Spatial descriptive statistics*.
       <https://en.wikipedia.org/wiki/Spatial_descriptive_statistics>
.. [4] Cressie, N.A.C. *Statistics for Spatial Data*, Wiley, New York.
.. [5] Stoyan, D., Stoyan, H. *Fractals, Random Shapes and Point Fields*,
       Akademie Verlag GmbH, Chichester, 1992.
.. [6] *Correlation function*.
       <https://en.wikipedia.org/wiki/Correlation_function_(astronomy)>
.. note that if this is changed from the default approach of using an *include*
   (in index.rst) to a separate performance page, the header needs to be changed
   from === to ***, the filename extension needs to be changed from .inc.rst to
   .rst, and a link needs to be added in the subpackage toctree

.. _astropy-stats-performance:

Performance Tips
================

If you are finding sigma clipping to be slow, and if you have not already done
so, consider installing the `bottleneck <https://pypi.org/project/Bottleneck/>`_
package, which will speed up some of the internal computations. In addition, if
you are using standard functions for ``cenfunc`` and/or ``stdfunc``, make sure
you specify these as strings rather than passing a NumPy function  that is,
use::

    >>> sigma_clip(array, cenfunc='median')  # doctest: +SKIP

instead of::

    >>> sigma_clip(array, cenfunc=np.nanmedian)  # doctest: +SKIP

Using strings will allow the sigma-clipping algorithm to pick the fastest
implementation available for finding the median.
.. _stats-circular:

*******************
Circular Statistics
*******************

.. automodapi:: astropy.stats.circstats


References
----------
.. [1] S. R. Jammalamadaka, A. SenGupta. "Topics in Circular Statistics".
       Series on Multivariate Analysis, Vol. 5, 2001.
.. [2] C. Agostinelli, U. Lund. "Circular Statistics from 'Topics in
       Circular Statistics (2001)'". 2015.
.. _stats:

***************************************
Astrostatistics Tools (`astropy.stats`)
***************************************

Introduction
============

The `astropy.stats` package holds statistical functions or algorithms
used in astronomy.  While the `scipy.stats` and `statsmodels
<http://www.statsmodels.org/stable/index.html>`_ packages contains a
wide range of statistical tools, they are general-purpose packages and
are missing some tools that are particularly useful or specific to
astronomy. This package is intended to provide such functionality,
but *not* to replace `scipy.stats` if its implementation satisfies
astronomers' needs.


Getting Started
===============

A number of different tools are contained in the stats package, and
they can be accessed by importing them::

    >>> from astropy import stats

A full list of the different tools are provided below. Please see the
documentation for their different usages. For example, sigma clipping,
which is a common way to estimate the background of an image, can be
performed with the :func:`~astropy.stats.sigma_clip` function. By
default, the function returns a masked array where outliers are
masked.

Examples
--------

..
  EXAMPLE START
  Sigma Clipping with Astropy Stats sigma_clip Function

To estimate the background of an image::

    >>> data = [1, 5, 6, 8, 100, 5, 3, 2]
    >>> stats.sigma_clip(data, sigma=2, maxiters=5)
    masked_array(data=[1, 5, 6, 8, --, 5, 3, 2],
                 mask=[False, False, False, False,  True, False, False, False],
           fill_value=999999)

..
  EXAMPLE END

..
  EXAMPLE START
  Sigma Clipping with Astropy Stats SigmaClip Class

Alternatively, the :class:`~astropy.stats.SigmaClip` class provides an
object-oriented interface to sigma clipping, which also returns a
masked array by default::

    >>> sigclip = stats.SigmaClip(sigma=2, maxiters=5)
    >>> sigclip(data)
    masked_array(data=[1, 5, 6, 8, --, 5, 3, 2],
                 mask=[False, False, False, False,  True, False, False, False],
           fill_value=999999)

..
  EXAMPLE END

..
  EXAMPLE START
  Calculating Sigma Clipping Statistics

In addition, there are also several convenience functions for making
the calculation of statistics even more convenient. For example,
:func:`~astropy.stats.sigma_clipped_stats` will return the mean,
median, and standard deviation of a sigma-clipped array::

     >>> stats.sigma_clipped_stats(data, sigma=2, maxiters=5)  # doctest: +FLOAT_CMP
     (4.2857142857142856, 5.0, 2.2497165354319457)

There are also tools for calculating :ref:`robust statistics
<stats-robust>`, sampling the data, :ref:`circular statistics
<stats-circular>`, confidence limits, spatial statistics, and adaptive
histograms.

..
  EXAMPLE END

Most tools are fairly self-contained, and include relevant examples in
their docstrings.


Using `astropy.stats`
=====================

More detailed information on using the package is provided on separate pages,
listed below.

.. toctree::
   :maxdepth: 2

   robust.rst
   circ.rst
   ripley.rst
   ../visualization/histogram.rst


Constants
=========

The `astropy.stats` package defines two constants useful for
converting between Gaussian sigma and full width at half maximum
(FWHM):

.. data:: gaussian_sigma_to_fwhm

    Factor with which to multiply Gaussian 1-sigma standard deviation
    to convert it to full width at half maximum (FWHM).

    >>> from astropy.stats import gaussian_sigma_to_fwhm
    >>> gaussian_sigma_to_fwhm  # doctest: +FLOAT_CMP
    2.3548200450309493

.. data:: gaussian_fwhm_to_sigma

    Factor with which to multiply Gaussian full width at half maximum
    (FWHM) to convert it to 1-sigma standard deviation.

    >>> from astropy.stats import gaussian_fwhm_to_sigma
    >>> gaussian_fwhm_to_sigma  # doctest: +FLOAT_CMP
    0.42466090014400953


See Also
========

* :mod:`scipy.stats`
    This SciPy package contains a variety of useful statistical functions
    and classes. The functionality in `astropy.stats` is intended to supplement
    this, *not* replace it.

* `statsmodels <http://www.statsmodels.org/stable/index.html>`_
    The statsmodels package provides functionality for estimating
    different statistical models, tests, and data exploration.

* `astroML <https://www.astroml.org/>`_
    The astroML package is a Python module for machine learning and
    data mining. Some of the tools from this package have been
    migrated here, but there are still a number of tools there that
    are useful for astronomy and statistical analysis.


* :func:`astropy.visualization.hist`
    The :func:`~astropy.stats.histogram` routine and related functionality
    defined here are used within the :func:`astropy.visualization.hist`
    function. For a discussion of these methods for determining histogram
    binnings, see :ref:`astropy-visualization-hist`.

.. note that if this section gets too long, it should be moved to a separate
   doc page - see the top of performance.inc.rst for the instructions on how to do
   that
.. include:: performance.inc.rst

Reference/API
=============

.. automodapi:: astropy.stats
.. _stats-robust:

*****************************
Robust Statistical Estimators
*****************************

Robust statistics provides reliable estimates of basic statistics for complex
distributions. The statistics package includes several robust statistical
functions that are commonly used in astronomy. This includes methods for
rejecting outliers as well as statistical description of the underlying
distributions.

In addition to the functions mentioned here, models can be fit with outlier
rejection using :func:`~astropy.modeling.fitting.FittingWithOutlierRemoval`.

Sigma Clipping
==============

Sigma clipping provides a fast method for identifying outliers in a
distribution. For a distribution of points, a center and a standard
deviation are calculated. Values which are less or more than a
specified number of standard deviations from a center value are
rejected. The process can be iterated to further reject outliers.

The `astropy.stats` package provides both a functional and
object-oriented interface for sigma clipping. The function is called
:func:`~astropy.stats.sigma_clip` and the class is called
:class:`~astropy.stats.SigmaClip`. By default, they both return a
masked array where the rejected points are masked.

Examples
--------

..
  EXAMPLE START
  Functional Sigma Clipping with astropy.stats.sigma_clip

We can start by generating some data that has a mean of 0 and standard
deviation of 0.2, but with outliers:

.. doctest-requires:: scipy

     >>> import numpy as np
     >>> import scipy.stats as stats
     >>> np.random.seed(0)
     >>> x = np.arange(200)
     >>> y = np.zeros(200)
     >>> c = stats.bernoulli.rvs(0.35, size=x.shape)
     >>> y += (np.random.normal(0., 0.2, x.shape) +
     ...       c*np.random.normal(3.0, 5.0, x.shape))

Now we can use :func:`~astropy.stats.sigma_clip` to perform sigma
clipping on the data:

.. doctest-requires:: scipy

     >>> from astropy.stats import sigma_clip
     >>> filtered_data = sigma_clip(y, sigma=3, maxiters=10)

The output masked array then can be used to calculate statistics on
the data, fit models to the data, or otherwise explore the data.

..
  EXAMPLE END

..
  EXAMPLE START
  Object-Oriented Sigma Clipping with the astropy.stats.SigmaClip Class

To perform the same sigma clipping with the
:class:`~astropy.stats.SigmaClip` class:

.. doctest-requires:: scipy

     >>> from astropy.stats import SigmaClip
     >>> sigclip = SigmaClip(sigma=3, maxiters=10)
     >>> print(sigclip)  # doctest: +SKIP
     <SigmaClip>
        sigma: 3
        sigma_lower: None
        sigma_upper: None
        maxiters: 10
        cenfunc: <function median at 0x108dbde18>
        stdfunc: <function std at 0x103ab52f0>
     >>> filtered_data = sigclip(y)

Note that once the ``sigclip`` instance is defined above, it can be
applied to other data using the same already defined sigma-clipping
parameters.

..
  EXAMPLE END

For basic statistics, :func:`~astropy.stats.sigma_clipped_stats` is a
convenience function to calculate the sigma-clipped mean, median, and
standard deviation of an array. As can be seen, rejecting the
outliers returns accurate values for the underlying distribution.

..
  EXAMPLE START
  Calculating the Sigma-Clipped Mean, Median, and Standard Deviation of an Array

To use :func:`~astropy.stats.sigma_clipped_stats` for sigma-clipped statistics
calculation:

.. doctest-requires:: scipy

     >>> from astropy.stats import sigma_clipped_stats
     >>> y.mean(), np.median(y), y.std()  # doctest: +FLOAT_CMP
     (0.86586417693378226, 0.03265864495523732, 3.2913811977676444)
     >>> sigma_clipped_stats(y, sigma=3, maxiters=10)  # doctest: +FLOAT_CMP
     (-0.0020337793767186197, -0.023632809025713953, 0.19514652532636906)

:func:`~astropy.stats.sigma_clip` and
:class:`~astropy.stats.SigmaClip` can be combined with other robust
statistics to provide improved outlier rejection as well.

.. plot::
    :include-source:

    import numpy as np
    import scipy.stats as stats
    from matplotlib import pyplot as plt
    from astropy.stats import sigma_clip, mad_std

    # Generate fake data that has a mean of 0 and standard deviation of 0.2 with outliers
    np.random.seed(0)
    x = np.arange(200)
    y = np.zeros(200)
    c = stats.bernoulli.rvs(0.35, size=x.shape)
    y += (np.random.normal(0., 0.2, x.shape) +
          c*np.random.normal(3.0, 5.0, x.shape))

    filtered_data = sigma_clip(y, sigma=3, maxiters=1, stdfunc=mad_std)

    # plot the original and rejected data
    plt.figure(figsize=(8,5))
    plt.plot(x, y, '+', color='#1f77b4', label="original data")
    plt.plot(x[filtered_data.mask], y[filtered_data.mask], 'x',
             color='#d62728', label="rejected data")
    plt.xlabel('x')
    plt.ylabel('y')
    plt.legend(loc=2, numpoints=1)

.. automodapi:: astropy.stats.sigma_clipping

..
  EXAMPLE END

Median Absolute Deviation
=========================

The median absolute deviation (MAD) is a measure of the spread of a
distribution and is defined as ``median(abs(a - median(a)))``. The
MAD can be calculated using `~astropy.stats.median_absolute_deviation`. For a
normal distribution, the MAD is related to the standard deviation by a factor
of 1.4826, and a convenience function, `~astropy.stats.mad_std`, is
available to apply the conversion.

.. note::

   A function can be supplied to the
   `~astropy.stats.median_absolute_deviation` to specify the median
   function to be used in the calculation. Depending on the version
   of NumPy and whether the array is masked or contains irregular
   values, significant performance increases can be had by
   preselecting the median function. If the median function is not
   specified, `~astropy.stats.median_absolute_deviation` will attempt
   to select the most relevant function according to the input data.


Biweight Estimators
===================

A set of functions are included in the `astropy.stats` package that use the
biweight formalism. These functions have long been used in astronomy,
particularly to calculate the velocity dispersion of galaxy clusters [1]_. The
following set of tasks are available for biweight measurements:

.. automodapi:: astropy.stats.biweight


References
----------

.. [1] Beers, Flynn, and Gebhardt (1990; AJ 100, 32) (https://ui.adsabs.harvard.edu/abs/1990AJ....100...32B)
.. note that if this is changed from the default approach of using an *include*
   (in index.rst) to a separate performance page, the header needs to be changed
   from === to ***, the filename extension needs to be changed from .inc.rst to
   .rst, and a link needs to be added in the sub-package toctree

.. _astropy-constants-performance:

.. Performance Tips
.. ================
..
.. Here we provide some tips and tricks for how to optimize performance of code
.. using `astropy.constants`.
.. _astropy-constants:

*******************************
Constants (`astropy.constants`)
*******************************

.. currentmodule:: astropy.constants

Introduction
============

`astropy.constants` contains a number of physical constants useful in
Astronomy. A `~astropy.constants.Constant` is a |Quantity| object with
additional metadata describing its provenance and uncertainty.

Getting Started
===============

You can import a :class:`~astropy.constants.Constant` directly from the
:mod:`astropy.constants` sub-package::

    >>> from astropy.constants import G
    >>> print(G)
      Name   = Gravitational constant
      Value  = 6.6743e-11
      Uncertainty  = 1.5e-15
      Unit  = m3 / (kg s2)
      Reference = CODATA 2018

Or, if you want to avoid having to explicitly import all of the constants you
need, you can do::

    >>> from astropy import constants as const
    >>> print(const.G)
      Name   = Gravitational constant
      ...

Constants can be used in :ref:`quantity_arithmetic` operations and
:ref:`quantity_and_numpy` just like any other |Quantity|::

    >>> from astropy import units as u
    >>> F = (const.G * 3. * const.M_sun * 100 * u.kg) / (2.2 * u.au) ** 2
    >>> print(F.to(u.N))  # doctest: +FLOAT_CMP
    0.3675671602160826 N

Unit Conversion
===============

..
  EXAMPLE START
  Converting Constants to Different Units

Explicitly :ref:`quantity_unit_conversion` is often not necessary, but can be
done if needed::

    >>> print(const.c)
      Name   = Speed of light in vacuum
      Value  = 299792458.0
      Uncertainty  = 0.0
      Unit  = m / s
      Reference = CODATA 2018

    >>> print(const.c.to('km/s'))
    299792.458 km / s

    >>> print(const.c.to('pc/yr'))  # doctest: +FLOAT_CMP
    0.306601393788 pc / yr

It is possible to convert most constants to `Centimeter-Gram-Second (CGS)
<https://en.wikipedia.org/wiki/Centimetre-gram-second_system_of_units>`_ units
using, for example::

    >>> const.c.cgs  # doctest: +FLOAT_CMP
    <Quantity   2.99792458e+10 cm / s>

However, some constants are defined with `different physical dimensions in CGS
<https://en.wikipedia.org/wiki/Centimetre-gram-second_system_of_units#Alternative_derivations_of_CGS_units_in_electromagnetism>`_
and cannot be directly converted. Because of this ambiguity, such constants
cannot be used in expressions without specifying a system::

    >>> 100 * const.e
    Traceback (most recent call last):
        ...
    TypeError: Constant u'e' does not have physically compatible units
    across all systems of units and cannot be combined with other
    values without specifying a system (eg. e.emu)
    >>> 100 * const.e.esu  # doctest: +FLOAT_CMP
    <Quantity 4.8032045057134676e-08 Fr>

..
  EXAMPLE END

.. _astropy-constants-prior:

Collections of Constants (and Prior Versions)
=============================================

Constants are organized into version modules. The constants for
``astropy`` 2.0 can be accessed in the ``astropyconst20`` module.
For example::

    >>> from astropy.constants import astropyconst20 as const
    >>> print(const.e)
      Name   = Electron charge
      Value  = 1.6021766208e-19
      Uncertainty  = 9.8e-28
      Unit  = C
      Reference = CODATA 2014

The version modules contain physical and astronomical constants, and both sets
can also be chosen independently from each other. Physical `CODATA constants
<https://physics.nist.gov/cuu/Constants/index.html>`_ are in modules with names
like ``codata2010``, ``codata2014``, or ``codata2018``::

    >>> from astropy.constants import codata2014 as const
    >>> print(const.h)
      Name   = Planck constant
      Value  = 6.62607004e-34
      Uncertainty  = 8.1e-42
      Unit  = J s
      Reference = CODATA 2014

Astronomical constants defined (primarily) by the International Astronomical
Union (IAU) are collected in modules with names like ``iau2012`` or ``iau2015``::

    >>> from astropy.constants import iau2012 as const
    >>> print(const.L_sun)
      Name   = Solar luminosity
      Value  = 3.846e+26
      Uncertainty  = 5e+22
      Unit  = W
      Reference = Allen's Astrophysical Quantities 4th Ed.

    >>> from astropy.constants import iau2015 as const
    >>> print(const.L_sun)
      Name   = Nominal solar luminosity
      Value  = 3.828e+26
      Uncertainty  = 0.0
      Unit  = W
      Reference = IAU 2015 Resolution B 3

However, importing these prior version modules directly will lead to
inconsistencies with other subpackages that have already imported
`astropy.constants`. Notably, `astropy.units` will have already used
the default version of constants. When using prior versions of the constants
in this manner, quantities should be constructed with constants instead of units.

To ensure consistent use of a prior version of constants in other ``astropy``
packages (such as :mod:`astropy.units`) that import :mod:`astropy.constants`,
the physical and astronomical constants versions should be set via
:class:`~astropy.utils.state.ScienceState` classes. These must be set before
the first import of either :mod:`astropy.constants` or :mod:`astropy.units`.
For example, you can use the CODATA2010 physical constants together with the
IAU 2012 astronomical constants::

    >>> from astropy import physical_constants, astronomical_constants
    >>> physical_constants.set('codata2010')  # doctest: +SKIP
    <ScienceState physical_constants: 'codata2010'>
    >>> physical_constants.get()  # doctest: +SKIP
    'codata2010'
    >>> astronomical_constants.set('iau2012')  # doctest: +SKIP
    <ScienceState astronomical_constants: 'iau2012'>
    >>> astronomical_constants.get()  # doctest: +SKIP
    'iau2012'

Then all other packages that import `astropy.constants` will self-consistently
initialize with these prior versions of constants.

The versions may also be set using values referring to the version modules::

    >>> from astropy import physical_constants, astronomical_constants
    >>> physical_constants.set('astropyconst13')  # doctest: +SKIP
    <ScienceState physical_constants: 'codata2010'>
    >>> physical_constants.get()  # doctest: +SKIP
    'codata2010'
    >>> astronomical_constants.set('astropyconst13')  # doctest: +SKIP
    <ScienceState astronomical_constants: 'iau2012'>
    >>> astronomical_constants.get()  # doctest: +SKIP
    'iau2012'

.. The doctest should not be skipped, ideally. See https://github.com/astropy/astropy/issues/8781

If :mod:`astropy.constants` or :mod:`astropy.units` have already been imported,
a :class:`RuntimeError` will be raised::

    >>> import astropy.units
    >>> from astropy import physical_constants, astronomical_constants
    >>> astronomical_constants.set('astropyconst13')
    Traceback (most recent call last):
        ...
    RuntimeError: astropy.units is already imported

.. note that if this section gets too long, it should be moved to a separate
   doc page - see the top of performance.inc.rst for the instructions on how to
   do that
.. include:: performance.inc.rst

Reference/API
=============

.. automodapi:: astropy.constants
.. _structured_units:

Structured Units
****************

Numpy arrays can be :doc:`structured arrays <numpy:user/basics.rec>`, where
each element consists of multiple fields. These can be used with |Quantity|
using a |StructuredUnit|, which provides a |Unit| for each field. For example,
this allows constructing a single |Quantity| object with position and velocity
fields that have different units, but are contained within the same object
(as is needed to support units in the PyERFA_ wrappers around the ERFA_
routines that use position-velocity arrays).

Creating Structured Quantities
==============================

You can create structured quantities either directly or by multiplication with
a |StructuredUnit|, with the latter in turn either created directly, or
through `~astropy.units.Unit`.

Example
-------

.. EXAMPLE START: Creating Structured Quantities

To create a structured quantity containing a position and velocity::

  >>> import astropy.units as u, numpy as np
  >>> pv_values = np.array([([1., 0., 0.], [0., 0.125, 0.]),
  ...                       ([0., 1., 0.], [-0.125, 0., 0.])],
  ...                      dtype=[('p', '(3,)f8'), ('v', '(3,)f8')])
  >>> pv = u.Quantity(pv_values, u.StructuredUnit((u.km, u.km/u.s)))
  >>> pv
  <Quantity [([1., 0., 0.], [ 0.   ,  0.125,  0.   ]),
             ([0., 1., 0.], [-0.125,  0.   ,  0.   ])] (km, km / s)>
  >>> pv_values * u.Unit('AU, AU/day')
  <Quantity [([1., 0., 0.], [ 0.   ,  0.125,  0.   ]),
             ([0., 1., 0.], [-0.125,  0.   ,  0.   ])] (AU, AU / d)>

As for normal |Quantity| objects, you can access the value and the unit with the
`~astropy.units.Quantity.value` and `~astropy.units.Quantity.unit` attribute,
respectively. In addition, you can index any given field using its name::

  >>> pv = pv_values * u.Unit('km, km/s')
  >>> pv.value
  array([([1., 0., 0.], [ 0.   ,  0.125,  0.   ]),
         ([0., 1., 0.], [-0.125,  0.   ,  0.   ])],
        dtype=[('p', '<f8', (3,)), ('v', '<f8', (3,))])
  >>> pv.unit
  Unit("(km, km / s)")
  >>> pv['v']
  <Quantity [[ 0.   ,  0.125,  0.   ],
             [-0.125,  0.   ,  0.   ]] km / s>

Structures can be nested, as in this example taken from an PyERFA_ test case
for :func:`erfa.ldn`::

  >>> ldbody = [
  ...     (0.00028574, 3e-10, ([-7.81014427, -5.60956681, -1.98079819],
  ...                          [0.0030723249, -0.00406995477, -0.00181335842])),
  ...     (0.00095435, 3e-9, ([0.738098796, 4.63658692, 1.9693136],
  ...                         [-0.00755816922, 0.00126913722, 0.000727999001])),
  ...     (1.0, 6e-6, ([-0.000712174377, -0.00230478303, -0.00105865966],
  ...                  [6.29235213e-6, -3.30888387e-7, -2.96486623e-7]))
  ...     ] * u.Unit('Msun,radian,(AU,AU/day)')
  >>> ldbody  # doctest: +FLOAT_CMP
  <Quantity [(2.8574e-04, 3.e-10, ([-7.81014427e+00, -5.60956681e+00, -1.98079819e+00], [ 3.07232490e-03, -4.06995477e-03, -1.81335842e-03])),
             (9.5435e-04, 3.e-09, ([ 7.38098796e-01,  4.63658692e+00,  1.96931360e+00], [-7.55816922e-03,  1.26913722e-03,  7.27999001e-04])),
             (1.0000e+00, 6.e-06, ([-7.12174377e-04, -2.30478303e-03, -1.05865966e-03], [ 6.29235213e-06, -3.30888387e-07, -2.96486623e-07]))] (solMass, rad, (AU, AU / d))>

.. EXAMPLE END

Converting to Different Units
=============================

Like regular |Quantity| objects, structured quantities can be converted to
different units, as long as they have the same structure and each unit is
equivalent.

Example
-------

.. EXAMPLE START: Converting Structured Quantities to Different Units

To convert a structured quantity to a different unit::

  >>> pv.to((u.m, u.m / u.s))  # doctest: +FLOAT_CMP
  <Quantity [([1000.,    0.,    0.], [   0.,  125.,    0.]),
             ([   0., 1000.,    0.], [-125.,    0.,    0.])] (m, m / s)>
  >>> pv.cgs
  <Quantity [([100000.,      0.,      0.], [     0.,  12500.,      0.]),
             ([     0., 100000.,      0.], [-12500.,      0.,      0.])] (cm, cm / s)>

.. EXAMPLE END

Use with ERFA
=============

The ERFA_ C routines make use of structured types, and these are exposed in
the PyERFA_ interface.

.. warning:: Not all PyERFA_ routines are wrapped yet. Help with adding
             wrappers will be appreciated.

Example
-------

.. EXAMPLE START: Using Structured Quantities with ERFA

To use a position-velocity structured array with PyERFA_::

  >>> import erfa
  >>> pv_values = np.array([([1., 0., 0.], [0., 0.125, 0.]),
  ...                       ([0., 1., 0.], [-0.125, 0., 0.])],
  ...                      dtype=erfa.dt_pv)
  >>> pv = pv_values << u.Unit('AU,AU/day')
  >>> erfa.pvu(86400*u.s, pv)
  <Quantity [([ 1.   ,  0.125,  0.   ], [ 0.   ,  0.125,  0.   ]),
             ([-0.125,  1.   ,  0.   ], [-0.125,  0.   ,  0.   ])] (AU, AU / d)>
  >>> erfa.pv2s(pv)  # doctest: +FLOAT_CMP
  (<Quantity [0.        , 1.57079633] rad>,
   <Quantity [0., 0.] rad>,
   <Quantity [1., 1.] AU>,
   <Quantity [0.125, 0.125] rad / d>,
   <Quantity [0., 0.] rad / d>,
   <Quantity [0., 0.] AU / d>)
  >>> z_axis = np.array(([0, 0, 1], [0, 0, 0]), erfa.dt_pv) * u.Unit('1,1/s')
  >>> erfa.pvxpv(pv, z_axis)
  <Quantity [([ 0., -1.,  0.], [0.125, 0.   , 0.   ]),
             ([ 1.,  0.,  0.], [0.   , 0.125, 0.   ])] (AU, AU / d)>

.. EXAMPLE END
.. note that if this is changed from the default approach of using an *include*
   (in index.rst) to a separate performance page, the header needs to be changed
   from === to ***, the filename extension needs to be changed from .inc.rst to
   .rst, and a link needs to be added in the subpackage toctree

.. _astropy-units-performance:

Performance Tips
================

If you are attaching units to arrays to make |Quantity| objects, multiplying
arrays by units will result in the array being copied in memory, which will slow
things down. Furthermore, if you are multiplying an array by a composite unit,
the array will be copied for each individual multiplication. Thus, in the
following case, the array is copied four successive times::

    In [1]: array = np.random.random(10000000)

    In [2]: %timeit array * u.m / u.s / u.kg / u.sr
    92.5 ms  2.52 ms per loop (mean  std. dev. of 7 runs, 10 loops each)

There are several ways to speed this up. First, when you are using composite
units, ensure that the entire unit gets evaluated first, then attached to the
array. You can do this by using parentheses as for any other operation::

    In [3]: %timeit array * (u.m / u.s / u.kg / u.sr)
    21.5 ms  886 s per loop (mean  std. dev. of 7 runs, 10 loops each)

In this case, this has sped things up by a factor of 4. If you use a composite
unit several times in your code then you can define a variable for it::

    In [4]: UNIT_MSKGSR = u.m / u.s / u.kg / u.sr

    In [5]: %timeit array * UNIT_MSKGSR
    22.2 ms  551 s per loop (mean  std. dev. of 7 runs, 10 loops each)

In this case and the case with brackets, the array is still copied once when
creating the |Quantity|. If you want to avoid any copies altogether, you can
make use of the ``<<`` operator to attach the unit to the array::

    In [6]: %timeit array << u.m / u.s / u.kg / u.sr
    47.1 s  5.77 s per loop (mean  std. dev. of 7 runs, 10000 loops each)

Note that these are now **microseconds**, so this is 2000x faster than the
original case with no brackets. Note that brackets are not needed when using
``<<`` since ``*`` and ``/`` have a higher precedence, so the unit will be
evaluated first. When using ``<<``, be aware that because the data is not being
copied, changing the original array will also change the |Quantity| object.

Note that for composite units, you will definitely see an
impact if you can pre-compute the composite unit::

    In [7]: %timeit array << UNIT_MSKGSR
    6.51 s  112 ns per loop (mean  std. dev. of 7 runs, 100000 loops each)

Which is over 10000x faster than the original example. See
:ref:`astropy-units-quantity-no-copy` for more details about the ``<<``
operator.
.. _physical_types:

Physical Types
**************

A physical type corresponds to physical quantities with dimensionally
compatible units. For example, the physical type *mass* corresponds to
physical quantities with units that can be converted to kilograms.
Physical types are represented as instances of the |PhysicalType| class.

Accessing Physical Types
========================

.. EXAMPLE START: Accessing Physical Types

Using :func:`~astropy.units.get_physical_type` lets us acquire |PhysicalType|
instances from strings with a name of a physical type, units, |Quantity|
instances, objects that can become quantities (e.g., numbers), and
|PhysicalType| instances.

  >>> import astropy.units as u
  >>> u.get_physical_type('speed')  # from the name of a physical type
  PhysicalType({'speed', 'velocity'})
  >>> u.get_physical_type(u.meter)  # from a unit
  PhysicalType('length')
  >>> u.get_physical_type(1 * u.barn * u.Mpc)  # from a Quantity
  PhysicalType('volume')
  >>> u.get_physical_type(42)  # from a number
  PhysicalType('dimensionless')

The physical type of a unit can be accessed via its
:attr:`~astropy.units.UnitBase.physical_type` attribute::

  >>> u.coulomb.physical_type
  PhysicalType('electrical charge')
  >>> (u.meter ** 2).physical_type
  PhysicalType('area')

.. EXAMPLE END

Using Physical Types
====================

.. EXAMPLE START: Using Physical Types

An equality comparison between a |PhysicalType| and a string will return
`True` if the string is a name of the |PhysicalType|::

  >>> acceleration = u.get_physical_type(u.m / u.s ** 2)
  >>> acceleration == 'acceleration'
  True

Some units may correspond to multiple physical types because compatible
units can be used to quantify different phenomena::

  >>> u.get_physical_type('pressure')
  PhysicalType({'energy density', 'pressure', 'stress'})

We can iterate through the names of a |PhysicalType|::

  >>> for name in u.J.physical_type: print(name)
  energy
  torque
  work

We can test for membership or equality with a string that has the name
of a |PhysicalType|::

  >>> 'energy' == u.J.physical_type
  True
  >>> 'work' in u.J.physical_type
  True

.. EXAMPLE END

Dimensional Analysis
====================

.. EXAMPLE START: Dimensional Analysis With Physical Types

|PhysicalType| instances support multiplication, division,
and exponentiation. Because of this, they can be used for
dimensional analysis::

  >>> length = u.get_physical_type('length')
  >>> time = u.get_physical_type('time')
  >>> length ** 2
  PhysicalType('area')
  >>> 1 / time
  PhysicalType('frequency')

Dimensional analysis can be performed between a |PhysicalType| and a
unit or between a |PhysicalType| and a string with a name of a
|PhysicalType|::

  >>> length ** 2 / u.s
  PhysicalType({'diffusivity', 'kinematic viscosity'})
  >>> length / 'time'
  PhysicalType({'speed', 'velocity'})

.. EXAMPLE END
Unit-Aware Type Annotations
***************************

Python supports static type analysis using the type syntax of `PEP 484
<https://www.python.org/dev/peps/pep-0484/>`_. For a detailed guide on type
hints, function annotations, and other related syntax see the `Real Python Guide
<https://realpython.com/python-type-checking/#type-aliases>`_. Below we describe
how you can be use Quantity type hints and annotations and also include metadata
about the associated units.


We assume the following imports:

::

   >>> import typing as T
   >>> import astropy.units as u
   >>> from astropy.units import Quantity


.. _quantity_type_annotation:

Quantity Type Annotation
========================

A |Quantity| can be used as a type annotation,::

   >>> x: Quantity = 2 * u.km

or as a function annotation.::

   >>> def func(x: Quantity) -> Quantity:
   ...     return x


Preserving Units
^^^^^^^^^^^^^^^^

While the above annotations are useful for annotating the value's type, it
does not inform us of the other most important attribute of a |Quantity|:
the unit.

Unit information may be included by the syntax
``Quantity[unit or "physical_type", shape, numpy.dtype]``.:

..
   All following doctests can be unskipped when py3.9+

.. doctest-skip::

   >>> Quantity[u.m]
   typing.Annotated[astropy.units.quantity.Quantity, Unit("m")]
   >>>
   >>> Quantity["length"]
   typing.Annotated[astropy.units.quantity.Quantity, PhysicalType('length')]

See ``typing.Annotated`` for explanation of ``Annotated``

These can also be used on functions

.. doctest-skip::

   >>> def func(x: Quantity[u.kpc]) -> Quantity[u.m]:
   ...     return x << u.m


.. _multiple_annotation:

Multiple Annotations
====================

Multiple Quantity and unit-aware |Quantity| annotations are supported using
:class:`~typing.Union` or :class:`~typing.Optional`

.. doctest-skip::

   >>> T.Union[Quantity[u.m], None]
   typing.Optional[typing.Annotated[astropy.units.quantity.Quantity, Unit("m")]]
   >>>
   >>> T.Union[Quantity[u.m], Quantity["time"]]
   typing.Union[typing.Annotated[astropy.units.quantity.Quantity, Unit("m")],
                typing.Annotated[astropy.units.quantity.Quantity, PhysicalType('time')]]
Low-Level Unit Conversion
*************************

Conversion of quantities from one unit to another is handled using the
`Quantity.to() <astropy.units.quantity.Quantity.to>` method. This page
describes some low-level features for handling unit conversion that
are rarely required in user code.

Direct Conversion
=================

.. EXAMPLE START: Direct Conversions Between Units

In this case, given a source and destination unit, the values in the
new units are returned.

  >>> from astropy import units as u
  >>> u.pc.to(u.m, 3.26)
  1.0059308915661856e+17

This converts 3.26 parsecs to meters.

Arrays are permitted as arguments.

  >>> u.h.to(u.s, [1, 2, 5, 10.1])
  array([  3600.,   7200.,  18000.,  36360.])

.. EXAMPLE END

Incompatible Conversions
========================

.. EXAMPLE START: Conversions Between Incompatible Units

If you attempt to convert to a incompatible unit, a
:class:`~astropy.units.UnitConversionError` will result:

  >>> cms = u.cm / u.s
  >>> cms.to(u.km)  # doctest: +IGNORE_EXCEPTION_DETAIL
  Traceback (most recent call last):
    ...
  UnitConversionError: 'cm / s' (speed) and 'km' (length) are not convertible

You can check whether a particular conversion is possible using the
:meth:`~astropy.units.core.UnitBase.is_equivalent` method::

  >>> u.m.is_equivalent(u.pc)
  True
  >>> u.m.is_equivalent("second")
  False
  >>> (u.m ** 3).is_equivalent(u.l)
  True

.. EXAMPLE END
Using Prior Versions of Constants
*********************************

By default, `astropy.units` are initialized upon first import to use
the current versions of `astropy.constants`. For units to initialize
properly to a prior version of constants, the constants versions must
be set before the first import of `astropy.units` or `astropy.constants`.

This is accomplished using :class:`~astropy.utils.state.ScienceState` classes
in the top-level package. Setting the prior versions at the start of a Python
session will allow consistent units.

Example
=======

.. EXAMPLE START: Using Prior Versions of Constants

To initialize units to a prior version of constants:

>>> import astropy
>>> astropy.physical_constants.set('codata2010')  # doctest: +SKIP
<ScienceState physical_constants: 'codata2010'>
>>> astropy.astronomical_constants.set('iau2012')  # doctest: +SKIP
<ScienceState astronomical_constants: 'iau2012'>
>>> import astropy.units as u
>>> import astropy.constants as const
>>> (const.M_sun / u.M_sun).to(u.dimensionless_unscaled) - 1  # doctest: +SKIP
<Quantity 0.>
>>> print(const.M_sun)  # doctest: +SKIP
  Name   = Solar mass
  Value  = 1.9891e+30
  Uncertainty  = 5e+25
  Unit  = kg
  Reference = Allen's Astrophysical Quantities 4th Ed.

If :mod:`astropy.units` has already been imported, a :class:`RuntimeError` is
raised.

.. EXAMPLE END
.. _astropy-units-format:

String Representations of Units
*******************************

Converting Units to String Representations
==========================================

You can control the way that |Quantity| and |Unit| objects are rendered as
strings using the `Python Format String Syntax
<https://docs.python.org/3/library/string.html#format-string-syntax>`_
(demonstrated below using `f-strings
<https://www.python.org/dev/peps/pep-0498/>`_).

For quantities, format specifiers, like ``0.003f`` will be applied to
the |Quantity| value, without affecting the unit. Specifiers like
``20s``, which would only apply to a string, will be applied to the
whole string representation of the |Quantity|.

Examples
--------

.. EXAMPLE START: Converting Units to String Representations

To render |Quantity| or |Unit| objects as strings::

    >>> from astropy import units as u
    >>> import numpy as np
    >>> q = 10.5 * u.km
    >>> q
    <Quantity  10.5 km>
    >>> f"{q}"
    '10.5 km'
    >>> f"{q:+0.03f}"
    '+10.500 km'
    >>> f"{q:20s}"
    '10.5 km             '

To format both the value and the unit separately, you can access the |Quantity|
class attributes within format strings::

    >>> q = 10.5 * u.km
    >>> q
    <Quantity  10.5 km>
    >>> f"{q.value:0.003f} in {q.unit:s}"
    '10.500 in km'

Because ``numpy`` arrays do not accept most format specifiers, using specifiers
like ``0.003f`` will not work when applied to a ``numpy`` array or non-scalar
|Quantity|. Use :func:`numpy.array_str` instead. For instance::

    >>> q = np.linspace(0,1,10) * u.m
    >>> f"{np.array_str(q.value, precision=1)} {q.unit}"  # doctest: +FLOAT_CMP
    '[0.  0.1 0.2 0.3 0.4 0.6 0.7 0.8 0.9 1. ] m'

Examine the NumPy documentation for more examples with :func:`numpy.array_str`.

.. EXAMPLE END

Units, or the unit part of a quantity, can also be formatted in a number of
different styles. By default, the string format used is referred to as the
"generic" format, which is based on syntax of the `FITS standard
<https://fits.gsfc.nasa.gov/fits_standard.html>`_ format for representing
units, but supports all of the units defined within the :mod:`astropy.units`
framework, including user-defined units. The format specifier (and
:meth:`~astropy.units.core.UnitBase.to_string`) functions also take an optional
parameter to select a different format, including ``"latex"``, ``"unicode"``,
``"cds"``, and others, defined below::

    >>> q = 10 * u.km
    >>> f"{q.value:0.003f} in {q.unit:latex}"
    '10.000 in $\\mathrm{km}$'
    >>> fluxunit = u.erg / (u.cm ** 2 * u.s)
    >>> f"{fluxunit}"
    u'erg / (cm2 s)'
    >>> print(f"{fluxunit:console}")
     erg
    ------
    s cm^2
    >>> f"{fluxunit:latex}"
    u'$\\mathrm{\\frac{erg}{s\\,cm^{2}}}$'
    >>> f"{fluxunit:>20s}"
    u'       erg / (cm2 s)'

The :meth:`~astropy.units.core.UnitBase.to_string` method is an alternative way
to format units as strings, and is the underlying implementation of the
`format`-style usage::

    >>> fluxunit = u.erg / (u.cm ** 2 * u.s)
    >>> fluxunit.to_string('latex')
    u'$\\mathrm{\\frac{erg}{s\\,cm^{2}}}$'

Creating Units from Strings
===========================

.. EXAMPLE START: Creating Units from Strings

Units can also be created from strings in a number of different
formats using the `~astropy.units.Unit` class::

  >>> u.Unit("m")
  Unit("m")
  >>> u.Unit("erg / (s cm2)")
  Unit("erg / (cm2 s)")
  >>> u.Unit("erg.s-1.cm-2", format="cds")
  Unit("erg / (cm2 s)")

.. note::

   Creating units from strings requires the use of a specialized
   parser for the unit language, which results in a performance
   penalty if units are created using strings. Thus, it is much
   faster to use unit objects directly (e.g., ``unit = u.degree /
   u.minute``) instead of via string parsing (``unit =
   u.Unit('deg/min')``). This parser is very useful, however, if your
   unit definitions are coming from a file format such as FITS or
   VOTable.

.. EXAMPLE END

Built-In Formats
================

`astropy.units` includes support for parsing and writing the following
formats:

  - ``"fits"``: This is the format defined in the Units section of the
    `FITS Standard <https://fits.gsfc.nasa.gov/fits_standard.html>`__.
    Unlike the "generic" string format, this will only accept or
    generate units defined in the FITS standard.

  - ``"vounit"``: The `Units in the VO 1.0
    <http://www.ivoa.net/documents/VOUnits/>`__ standard for
    representing units in the VO. Again, based on the FITS syntax,
    but the collection of supported units is different.

  - ``"cds"``: `Standards for astronomical catalogues from Centre de
    Donnes astronomiques de Strasbourg
    <http://vizier.u-strasbg.fr/vizier/doc/catstd-3.2.htx>`_: This is the
    standard used by `Vizier tables <http://vizier.u-strasbg.fr/>`__,
    as well as what is used by VOTable versions 1.3 and earlier.

  - ``"ogip"``: A standard for storing units as recommended by the
    `Office of Guest Investigator Programs (OGIP)
    <https://heasarc.gsfc.nasa.gov/docs/heasarc/ofwg/docs/general/ogip_93_001/>`_.

`astropy.units` is also able to write, but not read, units in the
following formats:

  - ``"latex"``: Writes units out using LaTeX math syntax using the
    `IAU Style Manual
    <https://www.iau.org/static/publications/stylemanual1989.pdf>`_
    recommendations for unit presentation. This format is
    automatically used when printing a unit in the IPython notebook::

      >>> fluxunit  # doctest: +SKIP

    .. math::

       \mathrm{\frac{erg}{s\,cm^{2}}}

  - ``"latex_inline"``: Writes units out using LaTeX math syntax using the
    `IAU Style Manual
    <https://www.iau.org/static/publications/stylemanual1989.pdf>`_
    recommendations for unit presentation, using negative powers instead of
    fractions, as required by some journals (e.g., `Apj and AJ
    <https://journals.aas.org/manuscript-preparation/>`_).
    Best suited for unit representation inline with text::

      >>> fluxunit.to_string('latex_inline')  # doctest: +SKIP

    .. math::

       \mathrm{erg\,s^{-1}\,cm^{-2}}

  - ``"console"``: Writes a multiline representation of the unit
    useful for display in a text console::

      >>> print(fluxunit.to_string('console'))
       erg
      ------
      s cm^2

  - ``"unicode"``: Same as ``"console"``, except uses Unicode
    characters::

      >>> print(u.Ry.decompose().to_string('unicode'))  # doctest: +FLOAT_CMP
                      m kg
      2.179872410 
                       s

.. _astropy-units-format-unrecognized:

Dealing with Unrecognized Units
===============================

Since many files found in the wild have unit strings that do not
correspond to any given standard, `astropy.units` also has a
consistent way to store and pass around unit strings that did not
parse.  In addition, it provides tools for transforming non-standard,
legacy or misspelt unit strings into their standardized form,
preventing the further propagation of these unit strings.

By default, passing an unrecognized unit string raises an exception::

  >>> # The FITS standard uses 'angstrom', not 'Angstroem'
  >>> u.Unit("Angstroem", format="fits")
  Traceback (most recent call last):
    ...
  ValueError: 'Angstroem' did not parse as fits unit: At col 0, Unit
  'Angstroem' not supported by the FITS standard. Did you mean Angstrom
  or angstrom? If this is meant to be a custom unit, define it with
  'u.def_unit'. To have it recognized inside a file reader or other
  code, enable it with 'u.add_enabled_units'. For details, see
  https://docs.astropy.org/en/latest/units/combining_and_defining.html

However, the `~astropy.units.Unit` constructor has the keyword
argument ``parse_strict`` that can take one of three values to control
this behavior:

  - ``'raise'``: (default) raise a :class:`ValueError`.

  - ``'warn'``: emit a :class:`~astropy.units.UnitsWarning`, and return an
    `~astropy.units.UnrecognizedUnit` instance.

  - ``'silent'``: return an `~astropy.units.UnrecognizedUnit`
    instance.

By either adding additional unit aliases for the misspelt units with
:func:`~astropy.units.set_enabled_aliases` (e.g., 'Angstroms' for 'Angstrom';
as demonstrated below), or defining new units via
:func:`~astropy.units.def_unit` and :func:`~astropy.units.add_enabled_units`,
we can use ``parse_strict='raise'`` to rapidly find issues with the units used,
while also being able to read in older datasets where the unit usage may have
been less standard.


Examples
--------

.. EXAMPLE START: Define Aliases for Units

To set unit aliases, pass :func:`~astropy.units.set_enabled_aliases` a
:class:`dict` mapping the misspelt string to an astropy unit. The following
code snippet shows how to set up Angstroem -> Angstrom::

    >>> u.set_enabled_aliases({"Angstroem": u.Angstrom})
    <astropy.units.core._UnitContext object at 0x...>
    >>> u.Unit("Angstroem")
    Unit("Angstrom")
    >>> u.Unit("Angstroem") == u.Angstrom
    True

You can also set multiple aliases up at once or add to existing ones::

    >>> u.set_enabled_aliases({"Angstroem": u.Angstrom, "Angstroms": u.Angstrom})
    <astropy.units.core._UnitContext object at 0x...>
    >>> u.add_enabled_aliases({"angstroem": u.Angstrom})
    <astropy.units.core._UnitContext object at 0x...>
    >>> u.Unit("Angstroem") == u.Unit("Angstroms") == u.Unit("angstroem") == u.Angstrom
    True

The aliases can be reset by passing an empty dictionary::

    >>> u.set_enabled_aliases({})
    <astropy.units.core._UnitContext object at 0x...>

You can use both :func:`~astropy.units.set_enabled_aliases` and
:func:`~astropy.units.add_enabled_aliases` as a `context manager
<https://docs.python.org/3/reference/datamodel.html#context-managers>`_,
limiting where a particular alias is used::

    >>> with u.add_enabled_aliases({"Angstroem": u.Angstrom}):
    ...     print(u.Unit("Angstroem") == u.Angstrom)
    True
    >>> u.Unit("Angstroem") == u.Angstrom
    Traceback (most recent call last):
      ...
    ValueError: 'Angstroem' did not parse as unit: At col 0, Angstroem is not
    a valid unit. Did you mean Angstrom, angstrom, mAngstrom or mangstrom? If
    this is meant to be a custom unit, define it with 'u.def_unit'. To have it
    recognized inside a file reader or other code, enable it with
    'u.add_enabled_units'. For details, see
    https://docs.astropy.org/en/latest/units/combining_and_defining.html

.. EXAMPLE END

.. EXAMPLE START: Using `~astropy.units.UnrecognizedUnit`

To pass an unrecognized unit string::

   >>> x = u.Unit("Angstroem", format="fits", parse_strict="warn")  # doctest: +SHOW_WARNINGS
   UnitsWarning: 'Angstroem' did not parse as fits unit: At col 0, Unit
   'Angstroem' not supported by the FITS standard. Did you mean Angstrom or
   angstrom? If this is meant to be a custom unit, define it with 'u.def_unit'.
   To have it recognized inside a file reader or other code, enable it with
   'u.add_enabled_units'. For details, see
   https://docs.astropy.org/en/latest/units/combining_and_defining.html

This `~astropy.units.UnrecognizedUnit` object remembers the
original string it was created with, so it can be written back out,
but any meaningful operations on it, such as converting to another
unit or composing with other units, will fail.

   >>> x.to_string()
   'Angstroem'
   >>> x.to(u.km)
   Traceback (most recent call last):
     ...
   ValueError: The unit 'Angstroem' is unrecognized.  It can not be
   converted to other units.
   >>> x / u.m
   Traceback (most recent call last):
     ...
   ValueError: The unit 'Angstroem' is unrecognized, so all arithmetic
   operations with it are invalid.

.. EXAMPLE END
.. _doc_standard_units:

Standard Units
**************

Standard units are defined in the `astropy.units` package as object
instances.

All units are defined in terms of basic "irreducible" units. The
irreducible units include:

  - Length (meter)
  - Time (second)
  - Mass (kilogram)
  - Current (ampere)
  - Temperature (Kelvin)
  - Angular distance (radian)
  - Solid angle (steradian)
  - Luminous intensity (candela)
  - Stellar magnitude (mag)
  - Amount of substance (mole)
  - Photon count (photon)

(There are also some more obscure base units required by the `FITS Standard
<https://fits.gsfc.nasa.gov/fits_standard.html>`_ that are no longer
recommended for use.)

Units that involve combinations of fundamental units are instances of
`~astropy.units.CompositeUnit`. In most cases, you do not need
to worry about the various kinds of unit classes unless you want to
design a more complex case.

There are many units already predefined in the module. You may use the
:meth:`~astropy.units.core.UnitBase.find_equivalent_units` method to list
all of the existing predefined units of a given type::

  >>> from astropy import units as u
  >>> u.g.find_equivalent_units()
    Primary name | Unit definition | Aliases
  [
    M_e          | 9.10938e-31 kg  |                                  ,
    M_p          | 1.67262e-27 kg  |                                  ,
    earthMass    | 5.97217e+24 kg  | M_earth, Mearth                  ,
    g            | 0.001 kg        | gram                             ,
    jupiterMass  | 1.89812e+27 kg  | M_jup, Mjup, M_jupiter, Mjupiter ,
    kg           | irreducible     | kilogram                         ,
    solMass      | 1.98841e+30 kg  | M_sun, Msun                      ,
    t            | 1000 kg         | tonne                            ,
    u            | 1.66054e-27 kg  | Da, Dalton                       ,
  ]


Prefixes
========

Most units can be used with prefixes, with both the standard `SI
<https://www.bipm.org/documents/20126/41483022/SI-Brochure-9-EN.pdf>`_ prefixes
and the `IEEE 1514-2002
<https://ieeexplore.ieee.org/servlet/opac?punumber=5254929>`_ binary prefixes
(for ``bit`` and ``byte``) supported:

+------------------------------+
|  Available decimal prefixes  |
+--------+-------------+-------+
| Symbol |    Prefix   | Value |
+========+=============+=======+
|    Y   |    yotta-   |  1e24 |
+--------+-------------+-------+
|    Z   |    zetta-   |  1e21 |
+--------+-------------+-------+
|    E   |     exa-    |  1e18 |
+--------+-------------+-------+
|    P   |    peta-    |  1e15 |
+--------+-------------+-------+
|    T   |    tera-    |  1e12 |
+--------+-------------+-------+
|    G   |    giga-    |  1e9  |
+--------+-------------+-------+
|    M   |    mega-    |  1e6  |
+--------+-------------+-------+
|    k   |    kilo-    |  1e3  |
+--------+-------------+-------+
|    h   |    hecto-   |  1e2  |
+--------+-------------+-------+
|   da   | deka-, deca |  1e1  |
+--------+-------------+-------+
|    d   |    deci-    |  1e-1 |
+--------+-------------+-------+
|    c   |    centi-   |  1e-2 |
+--------+-------------+-------+
|    m   |    milli-   |  1e-3 |
+--------+-------------+-------+
|    u   |    micro-   |  1e-6 |
+--------+-------------+-------+
|    n   |    nano-    |  1e-9 |
+--------+-------------+-------+
|    p   |    pico-    | 1e-12 |
+--------+-------------+-------+
|    f   |    femto-   | 1e-15 |
+--------+-------------+-------+
|    a   |    atto-    | 1e-18 |
+--------+-------------+-------+
|    z   |    zepto-   | 1e-21 |
+--------+-------------+-------+
|    y   |    yocto-   | 1e-24 |
+--------+-------------+-------+

+---------------------------+
| Available binary prefixes |
+--------+--------+---------+
| Symbol | Prefix |  Value  |
+========+========+=========+
|   Ki   |  kibi- | 2 ** 10 |
+--------+--------+---------+
|   Mi   |  mebi- | 2 ** 20 |
+--------+--------+---------+
|   Gi   |  gibi- | 2 ** 30 |
+--------+--------+---------+
|   Ti   |  tebi- | 2 ** 40 |
+--------+--------+---------+
|   Pi   |  pebi- | 2 ** 50 |
+--------+--------+---------+
|   Ei   |  exbi- | 2 ** 60 |
+--------+--------+---------+


.. _doc_dimensionless_unit:

The Dimensionless Unit
======================

In addition to these units, `astropy.units` includes the concept of
the dimensionless unit, used to indicate quantities that do not have a
physical dimension. This is distinct in concept from a unit that is
equal to `None`: that indicates that no unit was specified in the data
or by the user.

For convenience, there is a unit that is both dimensionless and
unscaled: the ``dimensionless_unscaled`` object::

   >>> u.dimensionless_unscaled
   Unit(dimensionless)

Dimensionless quantities are often defined as products or ratios of
quantities that are not dimensionless, but whose dimensions cancel out
when their powers are multiplied.

Examples
--------

.. EXAMPLE START: Dimensionless Units

To use the ``dimensionless_unscaled`` object::

   >>> u.m / u.m
   Unit(dimensionless)

For compatibility with the :ref:`astropy-units-format`, this is
equivalent to ``Unit('')`` and ``Unit(1)``, though using
``u.dimensionless_unscaled`` in Python code is preferred for
readability::

   >>> u.dimensionless_unscaled == u.Unit('')
   True
   >>> u.dimensionless_unscaled == u.Unit(1)
   True

Note that in many cases, a dimensionless unit may also have a scale.
For example::

   >>> (u.km / u.m).decompose()
   Unit(dimensionless with a scale of 1000.0)
   >>> (u.km / u.m).decompose() == u.dimensionless_unscaled
   False

As an example of why you might want to create a scaled dimensionless
quantity, say you will be doing many calculations with some big
unit-less number, ``big_unitless_num = 20000000  # 20 million``,
but you want all of your answers to be in multiples of a million. This
can be done by dividing ``big_unitless_num`` by ``1e6``, but this
requires you to remember that this scaling factor has been applied,
which may be difficult to do after many calculations. Instead, create
a scaled dimensionless quantity by multiplying a value by ``Unit(scale)``
to keep track of the scaling factor. For example::

   >>> scale = 1e6
   >>> big_unitless_num = 20 * u.Unit(scale)  # 20 million

   >>> some_measurement = 5.0 * u.cm
   >>> some_measurement * big_unitless_num  # doctest: +FLOAT_CMP
   <Quantity 100. 1e+06 cm>

To determine if a unit is dimensionless (but regardless of the scale),
use the `~astropy.units.core.UnitBase.physical_type` property::

   >>> (u.km / u.m).physical_type
   PhysicalType('dimensionless')
   >>> # This also has a scale, so it is not the same as u.dimensionless_unscaled
   >>> (u.km / u.m) == u.dimensionless_unscaled
   False
   >>> # However, (u.m / u.m) has a scale of 1.0, so it is the same
   >>> (u.m / u.m) == u.dimensionless_unscaled
   True

.. EXAMPLE END

.. _enabling-other-units:

Enabling Other Units
====================

By default, only the "default" units are searched by
:meth:`~astropy.units.core.UnitBase.find_equivalent_units` and similar methods
that do searching. This includes `SI
<https://www.bipm.org/documents/20126/41483022/SI-Brochure-9-EN.pdf>`_, `CGS
<https://en.wikipedia.org/wiki/Centimetre-gram-second_system_of_units>`_, and
astrophysical units. However, you may wish to enable the `Imperial
<https://en.wikipedia.org/wiki/Imperial_units>`_ or other user-defined units.

Example
-------

.. EXAMPLE START: Enabling Other Units

To enable Imperial units, do::

    >>> from astropy.units import imperial
    >>> imperial.enable()
    <astropy.units.core._UnitContext object at ...>
    >>> u.m.find_equivalent_units()
      Primary name | Unit definition | Aliases
    [
      AU           | 1.49598e+11 m   | au, astronomical_unit            ,
      Angstrom     | 1e-10 m         | AA, angstrom                     ,
      cm           | 0.01 m          | centimeter                       ,
      earthRad     | 6.3781e+06 m    | R_earth, Rearth                  ,
      ft           | 0.3048 m        | foot                             ,
      fur          | 201.168 m       | furlong                          ,
      inch         | 0.0254 m        |                                  ,
      jupiterRad   | 7.1492e+07 m    | R_jup, Rjup, R_jupiter, Rjupiter ,
      lsec         | 2.99792e+08 m   | lightsecond                      ,
      lyr          | 9.46073e+15 m   | lightyear                        ,
      m            | irreducible     | meter                            ,
      mi           | 1609.34 m       | mile                             ,
      micron       | 1e-06 m         |                                  ,
      mil          | 2.54e-05 m      | thou                             ,
      nmi          | 1852 m          | nauticalmile, NM                 ,
      pc           | 3.08568e+16 m   | parsec                           ,
      solRad       | 6.957e+08 m     | R_sun, Rsun                      ,
      yd           | 0.9144 m        | yard                             ,
    ]


This may also be used with the `Python "with" statement
<https://docs.python.org/3/reference/compound_stmts.html#with>`_, to
temporarily enable additional units::

    >>> with imperial.enable():
    ...     print(u.m.find_equivalent_units())
          Primary name | Unit definition | Aliases
    ...

To enable only specific units, use :func:`~astropy.units.add_enabled_units`::

    >>> with u.add_enabled_units([imperial.knot]):
    ...     print(u.m.find_equivalent_units())
          Primary name | Unit definition | Aliases
    ...

.. EXAMPLE END
.. _unit_equivalencies:

Equivalencies
*************

The unit module has machinery for supporting equivalences between
different units in certain contexts, namely when equations can
uniquely relate a value in one unit to a different unit. A good
example is the equivalence between wavelength, frequency, and energy
for specifying a wavelength of radiation. Normally these units are not
convertible, but when understood as representing light, they are
convertible in certain contexts. Here we describe how to use the
equivalencies included in `astropy.units` and how to
define new equivalencies.

Equivalencies are used by passing a list of equivalency pairs to the
``equivalencies`` keyword argument of `Quantity.to()
<astropy.units.quantity.Quantity.to>` or `Unit.to()
<astropy.units.core.UnitBase.to>` methods. The list can be supplied directly,
but ``astropy`` contains several functions that return appropriate lists so
constructing them is often not necessary. Alternatively, if a larger piece of
code needs the same equivalencies, you can set them for a :ref:`given context
<equivalency-context>`.

Built-In Equivalencies
======================

How to Convert Parallax to Distance
-----------------------------------

The length unit *parsec* is defined such that a star one parsec away
will exhibit a 1-arcsecond parallax. (Think of the name as a contraction
between *parallax* and *arcsecond*.)

The :func:`~astropy.units.equivalencies.parallax` function handles
conversions between parallax angles and length.

.. EXAMPLE START: Converting Parallax to Distance

In general, you should not be able to change units of length into
angles or vice versa, so :meth:`~astropy.units.core.UnitBase.to`
raises an exception::

  >>> from astropy import units as u
  >>> (0.8 * u.arcsec).to(u.parsec)  # doctest: +IGNORE_EXCEPTION_DETAIL
  Traceback (most recent call last):
    ...
  UnitConversionError: 'arcsec' (angle) and 'pc' (length) are not convertible

To trigger the conversion between parallax angle and distance, provide
:func:`~astropy.units.equivalencies.parallax` as the optional keyword
argument (``equivalencies=``) to the
:meth:`~astropy.units.core.UnitBase.to` method.

    >>> (0.8 * u.arcsec).to(u.parsec, equivalencies=u.parallax())
    <Quantity 1.25 pc>

.. EXAMPLE END

Angles as Dimensionless Units
-----------------------------

Angles are treated as a physically distinct type, which usually helps to avoid
mistakes. However, this is not very handy when working with units related to
rotational energy or the small angle approximation. (Indeed, this
double-sidedness underlies why radians went from a `supplementary to derived unit
<https://www.bipm.org/en/committees/cg/cgpm/20-1995/resolution-8>`__.) The function
:func:`~astropy.units.equivalencies.dimensionless_angles` provides the required
equivalency list that helps convert between angles and dimensionless units. It
is somewhat different from all others in that it allows an arbitrary change in
the number of powers to which radians is raised (i.e., including zero and
thus dimensionless).

Examples
^^^^^^^^

.. EXAMPLE START: Angles as Dimensionless Units

Normally the following would raise exceptions::

  >>> u.degree.to('')  # doctest: +IGNORE_EXCEPTION_DETAIL
  Traceback (most recent call last):
    ...
  UnitConversionError: 'deg' (angle) and '' (dimensionless) are not convertible
  >>> (u.kg * u.m**2 * (u.cycle / u.s)**2).to(u.J)  # doctest: +IGNORE_EXCEPTION_DETAIL
  Traceback (most recent call last):
    ...
  UnitConversionError: 'cycle2 kg m2 / s2' and 'J' (energy) are not convertible

But when passing the proper conversion function,
:func:`~astropy.units.equivalencies.dimensionless_angles`, it works.

  >>> u.deg.to('', equivalencies=u.dimensionless_angles())  # doctest: +FLOAT_CMP
  0.017453292519943295
  >>> (0.5e38 * u.kg * u.m**2 * (u.cycle / u.s)**2).to(u.J,
  ...                            equivalencies=u.dimensionless_angles())  # doctest: +FLOAT_CMP
  <Quantity 1.9739208802178715e+39 J>
  >>> import numpy as np
  >>> np.exp((1j*0.125*u.cycle).to('', equivalencies=u.dimensionless_angles())) # doctest: +FLOAT_CMP
  <Quantity  0.70710678+0.70710678j>

.. EXAMPLE END

In an example with complex numbers you may well be doing a fair
number of similar calculations. For such situations, there is the
option to :ref:`set default equivalencies <equivalency-context>`.

In some situations, this equivalency may behave differently than
anticipated. For instance, it might at first seem reasonable to use it
for converting from an angular velocity :math:`\omega` in radians per
second to the corresponding frequency :math:`f` in hertz (i.e., to
implement :math:`f=\omega/2\pi`). However, attempting this yields:

  >>> (1*u.rad/u.s).to(u.Hz, equivalencies=u.dimensionless_angles())  # doctest: +FLOAT_CMP
  <Quantity 1. Hz>
  >>> (1*u.cycle/u.s).to(u.Hz, equivalencies=u.dimensionless_angles())  # doctest: +FLOAT_CMP
  <Quantity 6.283185307179586 Hz>

Here, we might have expected ~0.159 Hz in the first example and 1 Hz in
the second. However, :func:`~astropy.units.equivalencies.dimensionless_angles`
converts to radians per second and then drops radians as a unit. The
implicit mistake made in these examples is that the unit Hz is taken to be
equivalent to cycles per second, which it is not (it is just "per second").
This realization also leads to the solution: to use an explicit equivalency
between cycles per second and hertz:

  >>> (1*u.rad/u.s).to(u.Hz, equivalencies=[(u.cy/u.s, u.Hz)])  # doctest: +FLOAT_CMP
  <Quantity 0.15915494309189535 Hz>
  >>> (1*u.cy/u.s).to(u.Hz, equivalencies=[(u.cy/u.s, u.Hz)])  # doctest: +FLOAT_CMP
  <Quantity 1. Hz>

.. _astropy-units-spectral-equivalency:

Spectral Units
--------------

:func:`~astropy.units.equivalencies.spectral` is a function that returns
an equivalency list to handle conversions between wavelength,
frequency, energy, and wave number.

.. EXAMPLE START: Using Spectral Units for Conversions

As mentioned with parallax units, we pass a list of equivalencies (in this case,
the result of :func:`~astropy.units.equivalencies.spectral`) as the second
argument to the :meth:`~astropy.units.quantity.Quantity.to` method and
wavelength, and then frequency and energy can be converted.

  >>> ([1000, 2000] * u.nm).to(u.Hz, equivalencies=u.spectral())  # doctest: +FLOAT_CMP
  <Quantity [2.99792458e+14, 1.49896229e+14] Hz>
  >>> ([1000, 2000] * u.nm).to(u.eV, equivalencies=u.spectral())  # doctest: +FLOAT_CMP
  <Quantity [1.23984193, 0.61992096] eV>

These equivalencies even work with non-base units::

  >>> # Inches to calories
  >>> from astropy.units import imperial
  >>> imperial.inch.to(imperial.Cal, equivalencies=u.spectral())  # doctest: +FLOAT_CMP
  1.869180759162485e-27

.. EXAMPLE END

.. _astropy-units-doppler-equivalencies:

Spectral (Doppler) Equivalencies
--------------------------------

Spectral equivalencies allow you to convert between wavelength,
frequency, energy, and wave number, but not to velocity, which is
frequently the quantity of interest.

It is fairly convenient to define the equivalency, but note that there are
different `conventions <https://www.gb.nrao.edu/~fghigo/gbtdoc/doppler.html>`__.
In these conventions :math:`f_0` is the rest frequency, :math:`f` is the
observed frequency, :math:`V` is the velocity, and :math:`c` is the speed of
light:

    * Radio         :math:`V = c \frac{f_0 - f}{f_0}  ;  f(V) = f_0 ( 1 - V/c )`
    * Optical       :math:`V = c \frac{f_0 - f}{f  }  ;  f(V) = f_0 ( 1 + V/c )^{-1}`
    * Relativistic  :math:`V = c \frac{f_0^2 - f^2}{f_0^2 + f^2} ;  f(V) = f_0 \frac{\left(1 - (V/c)^2\right)^{1/2}}{(1+V/c)}`

These three conventions are implemented in
:mod:`astropy.units.equivalencies` as
:func:`~astropy.units.equivalencies.doppler_optical`,
:func:`~astropy.units.equivalencies.doppler_radio`, and
:func:`~astropy.units.equivalencies.doppler_relativistic`.

Example
^^^^^^^

.. EXAMPLE START: Using Spectral (Doppler) Equivalencies

To define an equivalency::

    >>> restfreq = 115.27120 * u.GHz  # rest frequency of 12 CO 1-0 in GHz
    >>> freq_to_vel = u.doppler_radio(restfreq)
    >>> (116e9 * u.Hz).to(u.km / u.s, equivalencies=freq_to_vel)  # doctest: +FLOAT_CMP
    <Quantity -1895.4321928669085 km / s>

.. EXAMPLE END

Spectral Flux and Luminosity Density Units
------------------------------------------

There is also support for spectral flux and luminosity density units,
their equivalent surface brightness units, and integrated flux units. Their use
is more complex, since it is necessary to also supply the location in the
spectrum for which the conversions will be done, and the units of those spectral
locations. The function that handles these unit conversions is
:func:`~astropy.units.equivalencies.spectral_density`. This function takes as
its arguments the |Quantity| for the spectral location.

Example
^^^^^^^

.. EXAMPLE START: Converting Spectral Flux and Luminosity Density Units

To perform unit conversions with
:func:`~astropy.units.equivalencies.spectral_density`::

    >>> (1.5 * u.Jy).to(u.photon / u.cm**2 / u.s / u.Hz,
    ...                 equivalencies=u.spectral_density(3500 * u.AA)) # doctest: +FLOAT_CMP
    <Quantity 2.6429114293019694e-12 ph / (cm2 Hz s)>
    >>> (1.5 * u.Jy).to(u.photon / u.cm**2 / u.s / u.micron,
    ...                 equivalencies=u.spectral_density(3500 * u.AA))  # doctest: +FLOAT_CMP
    <Quantity 6467.9584789120845 ph / (cm2 micron s)>
    >>> a = 1. * (u.photon / u.s / u.angstrom)
    >>> a.to(u.erg / u.s / u.Hz,
    ...      equivalencies=u.spectral_density(5500 * u.AA))  # doctest: +FLOAT_CMP
    <Quantity 3.6443382634999996e-23 erg / (Hz s)>
    >>> w = 5000 * u.AA
    >>> a = 1. * (u.erg / u.cm**2 / u.s)
    >>> b = a.to(u.photon / u.cm**2 / u.s, u.spectral_density(w))
    >>> b  # doctest: +FLOAT_CMP
    <Quantity 2.51705828e+11 ph / (cm2 s)>
    >>> b.to(a.unit, u.spectral_density(w))  # doctest: +FLOAT_CMP
    <Quantity 1. erg / (cm2 s)>

.. EXAMPLE END

Brightness Temperature and Surface Brightness Equivalency
---------------------------------------------------------

There is an equivalency between surface brightness (flux density per area) and
brightness temperature. This equivalency is often referred to as "Antenna Gain"
since, at a given frequency, telescope brightness sensitivity is unrelated to
aperture size, but flux density sensitivity is, so this equivalency is only
dependent on the aperture size. See `Tools of Radio Astronomy
<https://books.google.com/books?id=9KHw6R8rQEMC&pg=PA179&source=gbs_toc_r&cad=4#v=onepage&q&f=false>`_
for details.

.. note:: The brightness temperature mentioned here is the Rayleigh-Jeans
          equivalent temperature, which results in a linear relation between
          flux and temperature. This is the convention that is most often used
          in relation to observations, but if you are interested in computing
          the *exact* temperature of a blackbody function that would produce a
          given flux, you should not use this equivalency.

Examples
^^^^^^^^

.. EXAMPLE START: Converting Brightness Temperature and Surface Brightness
   Equivalency

The :func:`~astropy.units.equivalencies.brightness_temperature` equivalency
requires the beam area and frequency as arguments. Recalling that the area of a
2D Gaussian is :math:`2 \pi \sigma^2` (see `wikipedia
<https://en.wikipedia.org/wiki/Gaussian_function#Two-dimensional_Gaussian_function>`_),
here is an example::

    >>> beam_sigma = 50*u.arcsec
    >>> omega_B = 2 * np.pi * beam_sigma**2
    >>> freq = 5 * u.GHz
    >>> (1*u.Jy/omega_B).to(u.K, equivalencies=u.brightness_temperature(freq))  # doctest: +FLOAT_CMP
    <Quantity 3.526295144567176 K>

If you have beam full-width half-maxima (FWHM), which are often quoted and are
the values stored in the FITS header keywords BMAJ and BMIN, a more appropriate
example converts the FWHM to sigma::

    >>> beam_fwhm = 50*u.arcsec
    >>> fwhm_to_sigma = 1. / (8 * np.log(2))**0.5
    >>> beam_sigma = beam_fwhm * fwhm_to_sigma
    >>> omega_B = 2 * np.pi * beam_sigma**2
    >>> (1*u.Jy/omega_B).to(u.K, equivalencies=u.brightness_temperature(freq))  # doctest: +FLOAT_CMP
    <Quantity 19.553932298231704 K>

You can also convert between ``Jy/beam`` and ``K`` by specifying the beam area::

    >>> (1*u.Jy/u.beam).to(u.K, u.brightness_temperature(freq, beam_area=omega_B))  # doctest: +FLOAT_CMP
    <Quantity 19.553932298231704 K>

.. EXAMPLE END

Beam Equivalency
----------------

Radio data, especially from interferometers, is often produced in units of
``Jy/beam``. Converting this number to a beam-independent value (e.g.,
``Jy/sr``), can be done with the
:func:`~astropy.units.equivalencies.beam_angular_area` equivalency.

Example
^^^^^^^

.. EXAMPLE START: Converting Radio Data to a Beam-Independent Value

To convert units of ``Jy/beam`` to ``Jy/sr``::

    >>> beam_fwhm = 50*u.arcsec
    >>> fwhm_to_sigma = 1. / (8 * np.log(2))**0.5
    >>> beam_sigma = beam_fwhm * fwhm_to_sigma
    >>> omega_B = 2 * np.pi * beam_sigma**2
    >>> (1*u.Jy/u.beam).to(u.MJy/u.sr, equivalencies=u.beam_angular_area(omega_B))  # doctest: +FLOAT_CMP
    <Quantity 15.019166691021288 MJy / sr>


Note that the `radio_beam <https://github.com/radio-astro-tools/radio-beam>`_
package deals with beam input/output and various operations more directly.

.. EXAMPLE END

Temperature Energy Equivalency
------------------------------

The :func:`~astropy.units.equivalencies.temperature_energy` equivalency allows
conversion between temperature and its equivalent in energy (i.e., the
temperature multiplied by the Boltzmann constant), usually expressed in
electronvolts. This is used frequently for observations at high-energy, be it
for solar or X-ray astronomy.

Example
^^^^^^^

.. EXAMPLE START: Temperature Energy Equivalency

To convert between temperature and its equivalent in energy::

    >>> t_k = 1e6 * u.K
    >>> t_k.to(u.eV, equivalencies=u.temperature_energy())  # doctest: +FLOAT_CMP
    <Quantity 86.17332384960955 eV>

.. EXAMPLE END

.. _tcmb-equivalency:

Thermodynamic Temperature Equivalency
-------------------------------------

This :func:`~astropy.units.equivalencies.thermodynamic_temperature`
equivalency allows conversion between ``Jy/beam`` and "thermodynamic
temperature", :math:`T_{CMB}`, in Kelvins.

Examples
^^^^^^^^

.. EXAMPLE START: Thermodynamic Temperature Equivalency

To convert between ``Jy/beam`` and thermodynamic temperature::

    >>> nu = 143 * u.GHz
    >>> t_k = 0.002632051878 * u.K
    >>> t_k.to(u.MJy / u.sr, equivalencies=u.thermodynamic_temperature(nu))  # doctest: +FLOAT_CMP
    <Quantity 1. MJy / sr>

By default, this will use the :math:`T_{CMB}` value for the default
:ref:`cosmology <astropy-cosmology>` in ``astropy``, but it is possible to
specify a custom :math:`T_{CMB}` value for a specific cosmology as the second
argument to the equivalency::

    >>> from astropy.cosmology import WMAP9
    >>> t_k.to(u.MJy / u.sr, equivalencies=u.thermodynamic_temperature(nu, T_cmb=WMAP9.Tcmb0))  # doctest: +FLOAT_CMP
    <Quantity 0.99982392 MJy / sr>

.. EXAMPLE END

Molar Mass AMU Equivalency
--------------------------

The :func:`~astropy.units.equivalencies.molar_mass_amu` equivalency allows
conversion between the atomic mass unit and the equivalent g/mol. For context,
refer to the `NIST definition of SI Base Units
<https://www.nist.gov/si-redefinition/definitions-si-base-units>`_.

Example
^^^^^^^

.. EXAMPLE START: Molar Mass AMU Equivalency

To convert between atomic mass unit and the equivalent g/mol::

    >>> x = 1 * (u.g / u.mol)
    >>> y = 1 * u.u
    >>> x.to(u.u, equivalencies=u.molar_mass_amu()) # doctest: +FLOAT_CMP
    <Quantity 1.0 u>
    >>> y.to(u.g/u.mol, equivalencies=u.molar_mass_amu()) # doctest: +FLOAT_CMP
    <Quantity 1.0 g / mol>

.. EXAMPLE END

Pixel and Plate Scale Equivalencies
-----------------------------------

These equivalencies are for converting between angular scales and either linear
scales in the focal plane or distances in units of the number of pixels.

Examples
^^^^^^^^

.. EXAMPLE START: Pixel and Plate Scale Equivalencies

Suppose you are working with cutouts from the Sloan Digital Sky Survey,
which defaults to a pixel scale of 0.4 arcseconds per pixel, and want to know
the true size of something that you measure to be 240 pixels across in the
cutout image::

    >>> sdss_pixelscale = u.pixel_scale(0.4*u.arcsec/u.pixel)
    >>> (240*u.pixel).to(u.arcmin, sdss_pixelscale)  # doctest: +FLOAT_CMP
    <Quantity 1.6 arcmin>

Or maybe you are designing an instrument for a telescope that someone told you
has an inverse plate scale of 7.8 meters per radian (for your desired focus),
and you want to know how big your pixels need to be to cover half an arcsecond.
Using :func:`~astropy.units.equivalencies.plate_scale`::

    >>> tel_platescale = u.plate_scale(7.8*u.m/u.radian)
    >>> (0.5*u.arcsec).to(u.micron, tel_platescale)  # doctest: +FLOAT_CMP
    <Quantity 18.9077335632719 micron>

The :func:`~astropy.units.equivalencies.pixel_scale` equivalency can also work
in more general context, where the scale is specified as any quantity that is
reducible to ``<composite unit>/u.pix`` or ``u.pix/<composite unit>`` (that is,
the dimensionality of ``u.pix`` is 1 or -1). For instance, you may define the
dots per inch (DPI) for a digital image to calculate its physical size::

    >>> dpi = u.pixel_scale(100 * u.pix / u.imperial.inch)
    >>> (1024 * u.pix).to(u.cm, dpi)  # doctest: +FLOAT_CMP
    <Quantity 26.0096 cm>

.. EXAMPLE END

Photometric Zero Point Equivalency
----------------------------------

The :func:`~astropy.units.zero_point_flux` equivalency provides a way to move
between photometric systems (i.e., those defined relative to a particular
zero-point flux) and absolute fluxes. This is most useful in conjunction with
support for :ref:`logarithmic_units`.

Example
^^^^^^^

.. EXAMPLE START: Photometric Zero Point Equivalency

Suppose you are observing a target with a filter with a reported standard zero
point of 3631.1 Jy::

    >>> target_flux = 1.2 * u.nanomaggy
    >>> zero_point_star_equiv = u.zero_point_flux(3631.1 * u.Jy)
    >>> u.Magnitude(target_flux.to(u.AB, zero_point_star_equiv))  # doctest: +FLOAT_CMP
    <Magnitude 22.30195136 mag(AB)>

.. EXAMPLE END

Temperature Equivalency
-----------------------

The :func:`~astropy.units.temperature` equivalency allows conversion
between the Celsius, Fahrenheit, Rankine and Kelvin.

Example
^^^^^^^

.. EXAMPLE START: Using the Temperature Equivalency

To convert between temperature scales::

    >>> temp_C = 0 * u.Celsius
    >>> temp_Kelvin = temp_C.to(u.K, equivalencies=u.temperature())
    >>> temp_Kelvin  # doctest: +FLOAT_CMP
    <Quantity 273.15 K>
    >>> temp_F = temp_C.to(u.imperial.deg_F, equivalencies=u.temperature())
    >>> temp_F  # doctest: +FLOAT_CMP
    <Quantity 32. deg_F>
    >>> temp_R = temp_C.to(u.imperial.deg_R, equivalencies=u.temperature())
    >>> temp_R  # doctest: +FLOAT_CMP
    <Quantity 491.67 deg_R>

.. note:: You can also use ``u.deg_C`` instead of ``u.Celsius``.

.. EXAMPLE END

Mass-Energy Equivalency
-----------------------

.. EXAMPLE START: Using the Mass-Energy Equivalency

In a special relativity context it can be convenient to use the
:func:`~astropy.units.equivalencies.mass_energy` equivalency. For instance::

    >>> (1 * u.g).to(u.eV, u.mass_energy())  # doctest: +FLOAT_CMP
    <Quantity 5.60958865e+32 eV>

.. EXAMPLE END

Doppler Redshift Equivalency
----------------------------

Conversion between Doppler redshift and radial velocity can be done with the
:func:`~astropy.units.equivalencies.doppler_redshift` equivalency.

Example
^^^^^^^

.. EXAMPLE START: Converting Doppler redshift to radial velocity

To convert Doppler redshift (unitless) to ``km/s``::

    >>> z = 0.1 * u.dimensionless_unscaled
    >>> z.to(u.km / u.s, u.doppler_redshift())  # doctest: +FLOAT_CMP
    <Quantity 28487.0661448 km / s>

However, it cannot take the cosmological redshift unit from `astropy.cosmology.units`
because the latter should not be interpreted the same since the recessional
velocity from the expansion of space can exceed the speed of light; see
`Hubble's law: Redshift velocity and recessional velocity <https://en.wikipedia.org/wiki/Hubble%27s_law#Redshift_velocity_and_recessional_velocity>`_
for more information.

.. EXAMPLE END

Writing New Equivalencies
=========================

An equivalence list is a :class:`list` of tuples, where each :class:`tuple` has
four elements::

  (from_unit, to_unit, forward, backward)

``from_unit`` and ``to_unit`` are the equivalent units. ``forward`` and
``backward`` are functions that convert values between those units. ``forward``
and ``backward`` are optional, and if omitted then the equivalency declares
that the two units should be taken as equivalent. The functions must take and
return non-|Quantity| objects to avoid infinite recursion; See
:ref:`complicated-equiv-example` for more details.

Examples
--------

.. EXAMPLE START: Writing New Equivalencies

Until 1964, the metric liter was defined as the volume of 1kg of water at 4C at
760mm mercury pressure. Volumes and masses are not normally directly
convertible, but if we hold the constants in the 1964 definition of the liter as
true, we could build an equivalency for them::

  >>> liters_water = [
  ...    (u.l, u.g, lambda x: 1000.0 * x, lambda x: x / 1000.0)
  ... ]
  >>> u.l.to(u.kg, 1, equivalencies=liters_water)
  1.0

Note that the equivalency can be used with any other compatible unit::

  >>> imperial.gallon.to(imperial.pound, 1, equivalencies=liters_water)  # doctest: +FLOAT_CMP
  8.345404463333525

And it also works in the other direction::

  >>> imperial.lb.to(imperial.pint, 1, equivalencies=liters_water)  # doctest: +FLOAT_CMP
  0.9586114172355459

.. EXAMPLE END

.. _complicated-equiv-example:

A More Complex Example: Spectral Doppler Equivalencies
------------------------------------------------------

.. EXAMPLE START: Writing Spectral Doppler Equivalencies

We show how to define an equivalency using the radio convention for CO 1-0.
This function is already defined in
:func:`~astropy.units.equivalencies.doppler_radio`, but this example is
illustrative::

    >>> from astropy.constants import si
    >>> restfreq = 115.27120  # rest frequency of 12 CO 1-0 in GHz
    >>> freq_to_vel = [(u.GHz, u.km/u.s,
    ... lambda x: (restfreq-x) / restfreq * si.c.to_value('km/s'),
    ... lambda x: (1-x/si.c.to_value('km/s')) * restfreq )]
    >>> u.Hz.to(u.km / u.s, 116e9, equivalencies=freq_to_vel)  # doctest: +FLOAT_CMP
    -1895.4321928669262
    >>> (116e9 * u.Hz).to(u.km / u.s, equivalencies=freq_to_vel)  # doctest: +FLOAT_CMP
    <Quantity -1895.4321928669262 km / s>

.. EXAMPLE END

Note that once this is defined for GHz and km/s, it will work for all other
units of frequency and velocity. ``x`` is converted from the input frequency
unit (e.g., Hz) to GHz before being passed to ``lambda x:``. Similarly, the
return value is assumed to be in units of ``km/s``, which is why the ``value``
of ``c`` is used instead of the :class:`~astropy.constants.Constant`.

Displaying Available Equivalencies
==================================

The :meth:`~astropy.units.core.UnitBase.find_equivalent_units` method also
understands equivalencies.

Example
-------

.. EXAMPLE START: Displaying Available Equivalencies

Without passing equivalencies, there are three compatible units for ``Hz`` in
the standard set::

  >>> u.Hz.find_equivalent_units()
    Primary name | Unit definition | Aliases
  [
    Bq           | 1 / s           | becquerel    ,
    Ci           | 3.7e+10 / s    | curie        ,
    Hz           | 1 / s           | Hertz, hertz ,
  ]

However, when passing the spectral equivalency, you can see there are
all kinds of things that ``Hz`` can be converted to::

  >>> u.Hz.find_equivalent_units(equivalencies=u.spectral())
    Primary name | Unit definition        | Aliases
  [
    AU           | 1.49598e+11 m          | au, astronomical_unit ,
    Angstrom     | 1e-10 m                | AA, angstrom          ,
    Bq           | 1 / s                  | becquerel             ,
    Ci           | 3.7e+10 / s            | curie                 ,
    Hz           | 1 / s                  | Hertz, hertz          ,
    J            | kg m2 / s2             | Joule, joule          ,
    Ry           | 2.17987e-18 kg m2 / s2 | rydberg               ,
    cm           | 0.01 m                 | centimeter            ,
    eV           | 1.60218e-19 kg m2 / s2 | electronvolt          ,
    earthRad     | 6.3781e+06 m           | R_earth, Rearth       ,
    erg          | 1e-07 kg m2 / s2       |                       ,
    jupiterRad   | 7.1492e+07 m           | R_jup, Rjup, R_jupiter, Rjupiter ,
    k            | 100 / m                | Kayser, kayser        ,
    lsec         | 2.99792e+08 m          | lightsecond           ,
    lyr          | 9.46073e+15 m          | lightyear             ,
    m            | irreducible            | meter                 ,
    micron       | 1e-06 m                |                       ,
    pc           | 3.08568e+16 m          | parsec                ,
    solRad       | 6.957e+08 m            | R_sun, Rsun           ,
  ]

.. EXAMPLE END

.. _equivalency-context:

Using Equivalencies in Larger Pieces of Code
============================================

Sometimes you may have an involved calculation where you are regularly switching
back and forth between equivalent units. For these cases, you can set
equivalencies that will by default be used, in a way similar to how you can
:ref:`enable other units <enabling-other-units>`.

Examples
--------

.. EXAMPLE START: Using Equivalencies in Larger Pieces of Code

To enable radians to be treated as a dimensionless unit use
:func:`~astropy.units.set_enabled_equivalencies` as a `context manager
<https://docs.python.org/3/reference/datamodel.html#context-managers>`_::

  >>> with u.set_enabled_equivalencies(u.dimensionless_angles()):
  ...    phase = 0.5 * u.cycle
  ...    c = np.exp(1j*phase)
  >>> c  # doctest: +FLOAT_CMP
  <Quantity -1.+1.2246468e-16j>

To permanently and globally enable radians to be treated as a dimensionless
unit use :func:`~astropy.units.set_enabled_equivalencies` not as a context
manager:

.. doctest-skip::

  >>> u.set_enabled_equivalencies(u.dimensionless_angles())
  <astropy.units.core._UnitContext object at ...>
  >>> u.deg.to('')  # doctest: +FLOAT_CMP
  0.017453292519943295

The disadvantage of the above approach is that you may forget to turn the
default off (done by giving an empty argument).

:func:`~astropy.units.set_enabled_equivalencies` accepts any list of
equivalencies, so you could add, for example,
:func:`~astropy.units.equivalencies.spectral` and
:func:`~astropy.units.equivalencies.spectral_density` (since these return
lists, they should indeed be combined by adding them together).

.. EXAMPLE END
.. _logarithmic_units:

Magnitudes and Other Logarithmic Units
**************************************

Magnitudes and logarithmic units such as ``dex`` and ``dB`` are used as the
logarithm of values relative to some reference value. Quantities with such
units are supported in ``astropy`` via the :class:`~astropy.units.Magnitude`,
:class:`~astropy.units.Dex`, and :class:`~astropy.units.Decibel` classes.

Creating Logarithmic Quantities
===============================

You can create logarithmic quantities either directly or by multiplication with
a logarithmic unit.

Example
-------

.. EXAMPLE START: Creating Logarithmic Quantities

To create a logarithmic quantity::

  >>> import astropy.units as u, astropy.constants as c, numpy as np
  >>> u.Magnitude(-10.)  # doctest: +FLOAT_CMP
  <Magnitude -10. mag>
  >>> u.Magnitude(10 * u.ct / u.s)  # doctest: +FLOAT_CMP
  <Magnitude -2.5 mag(ct / s)>
  >>> u.Magnitude(-2.5, "mag(ct/s)")  # doctest: +FLOAT_CMP
  <Magnitude -2.5 mag(ct / s)>
  >>> -2.5 * u.mag(u.ct / u.s)  # doctest: +FLOAT_CMP
  <Magnitude -2.5 mag(ct / s)>
  >>> u.Dex((c.G * u.M_sun / u.R_sun**2).cgs)  # doctest: +FLOAT_CMP
  <Dex 4.438067627303133 dex(cm / s2)>
  >>> np.linspace(2., 5., 7) * u.Unit("dex(cm/s2)")  # doctest: +FLOAT_CMP
  <Dex [2. , 2.5, 3. , 3.5, 4. , 4.5, 5. ] dex(cm / s2)>

Above, we make use of the fact that the units ``mag``, ``dex``, and
``dB`` are special in that, when used as functions, they return a
:class:`~astropy.units.function.logarithmic.LogUnit` instance
(:class:`~astropy.units.function.logarithmic.MagUnit`,
:class:`~astropy.units.function.logarithmic.DexUnit`, and
:class:`~astropy.units.function.logarithmic.DecibelUnit`,
respectively). The same happens as required when strings are parsed
by :class:`~astropy.units.Unit`.

.. EXAMPLE END

As for normal |Quantity| objects, you can access the value with the
`~astropy.units.Quantity.value` attribute. In addition, you can convert to a
|Quantity| with the physical unit using the
`~astropy.units.function.FunctionQuantity.physical` attribute::

    >>> logg = 5. * u.dex(u.cm / u.s**2)
    >>> logg.value
    5.0
    >>> logg.physical  # doctest: +FLOAT_CMP
    <Quantity 100000. cm / s2>

Converting to Different Units
=============================

Like |Quantity| objects, logarithmic quantities can be converted to different
units, be it another logarithmic unit or a physical one.

Example
-------

.. EXAMPLE START: Converting Logarithmic Quantities to Different Units

To convert a logarithmic quantity to a different unit::

    >>> logg = 5. * u.dex(u.cm / u.s**2)
    >>> logg.to(u.m / u.s**2)  # doctest: +FLOAT_CMP
    <Quantity 1000. m / s2>
    >>> logg.to('dex(m/s2)')  # doctest: +FLOAT_CMP
    <Dex 3. dex(m / s2)>

For convenience, the :attr:`~astropy.units.function.FunctionQuantity.si` and
:attr:`~astropy.units.function.FunctionQuantity.cgs` attributes can be used to
convert the |Quantity| to base `SI
<https://www.bipm.org/documents/20126/41483022/SI-Brochure-9-EN.pdf>`_ or `CGS
<https://en.wikipedia.org/wiki/Centimetre-gram-second_system_of_units>`_
units::

    >>> logg.si  # doctest: +FLOAT_CMP
    <Dex 3. dex(m / s2)>

.. EXAMPLE END

Arithmetic and Photometric Applications
=======================================

Addition and subtraction work as expected for logarithmic quantities,
multiplying and dividing the physical units as appropriate. It may be best
seen through an example of a photometric reduction.

Example
-------

.. EXAMPLE START: Photometric Reduction with Logarithmic Quantities

First, calculate instrumental magnitudes assuming some count rates for three
objects::

    >>> tint = 1000.*u.s
    >>> cr_b = ([3000., 100., 15.] * u.ct) / tint
    >>> cr_v = ([4000., 90., 25.] * u.ct) / tint
    >>> b_i, v_i = u.Magnitude(cr_b), u.Magnitude(cr_v)
    >>> b_i, v_i  # doctest: +FLOAT_CMP
    (<Magnitude [-1.19280314,  2.5       ,  4.55977185] mag(ct / s)>,
     <Magnitude [-1.50514998,  2.61439373,  4.00514998] mag(ct / s)>)

Then, the instrumental B-V color is::

    >>> b_i - v_i  # doctest: +FLOAT_CMP
    <Magnitude [ 0.31234684, -0.11439373,  0.55462187] mag>

Note that the physical unit has become dimensionless. The following step might
be used to correct for atmospheric extinction::

    >>> atm_ext_b, atm_ext_v = 0.12 * u.mag, 0.08 * u.mag
    >>> secz = 1./np.cos(45 * u.deg)
    >>> b_i0 = b_i - atm_ext_b * secz
    >>> v_i0 = v_i - atm_ext_b * secz
    >>> b_i0, v_i0  # doctest: +FLOAT_CMP
    (<Magnitude [-1.36250876,  2.33029437,  4.39006622] mag(ct / s)>,
     <Magnitude [-1.67485561,  2.4446881 ,  3.83544435] mag(ct / s)>)

Since the extinction is dimensionless, the units do not change. Now suppose the
first star has a known ST magnitude, so we can calculate zero points::

    >>> b_ref, v_ref = 17.2 * u.STmag, 17.0 * u.STmag
    >>> b_ref, v_ref  # doctest: +FLOAT_CMP
    (<Magnitude 17.2 mag(ST)>, <Magnitude 17. mag(ST)>)
    >>> zp_b, zp_v = b_ref - b_i0[0], v_ref - v_i0[0]
    >>> zp_b, zp_v  # doctest: +FLOAT_CMP
    (<Magnitude 18.56250876 mag(s ST / ct)>,
     <Magnitude 18.67485561 mag(s ST / ct)>)

Here, ``ST`` is shorthand for the ST zero-point flux::

    >>> (0. * u.STmag).to(u.erg/u.s/u.cm**2/u.AA)  # doctest: +FLOAT_CMP
    <Quantity 3.63078055e-09 erg / (Angstrom cm2 s)>
    >>> (-21.1 * u.STmag).to(u.erg/u.s/u.cm**2/u.AA)  # doctest: +FLOAT_CMP
    <Quantity 1. erg / (Angstrom cm2 s)>

.. Note::

    At present, only magnitudes defined in terms of luminosity or flux are
    implemented, since those do not depend on the filter with which the
    measurement was made. They include absolute and apparent bolometric [M15]_,
    ST [H95]_, and AB [OG83]_ magnitudes.

Now applying the calibration, we find (note the proper change in units)::

    >>> B, V = b_i0 + zp_b, v_i0 + zp_v
    >>> B, V  # doctest: +FLOAT_CMP
    (<Magnitude [17.2       , 20.89280314, 22.95257499] mag(ST)>,
     <Magnitude [17.        , 21.1195437 , 22.51029996] mag(ST)>)

We could convert these magnitudes to another system, for example, ABMag, using
appropriate :ref:`equivalency <unit_equivalencies>`::

    >>> V.to(u.ABmag, u.spectral_density(5500.*u.AA))  # doctest: +FLOAT_CMP
    <Magnitude [16.99023831, 21.10978201, 22.50053827] mag(AB)>

This is particularly useful for converting magnitude into flux density. ``V``
is currently in ST magnitudes, which is based on flux densities per unit
wavelength (:math:`f_\lambda`). Therefore, we can directly convert ``V`` into
flux density per unit wavelength using the
:meth:`~astropy.units.quantity.Quantity.to` method::

    >>> flam = V.to(u.erg/u.s/u.cm**2/u.AA)
    >>> flam  # doctest: +FLOAT_CMP
    <Quantity [5.75439937e-16, 1.29473986e-17, 3.59649961e-18] erg / (Angstrom cm2 s)>

To convert ``V`` to flux density per unit frequency (:math:`f_\nu`), we again
need the appropriate :ref:`equivalency <unit_equivalencies>`, which in this case
is the central wavelength of the magnitude band, 5500 Angstroms::

    >>> lam = 5500 * u.AA
    >>> fnu = V.to(u.erg/u.s/u.cm**2/u.Hz, u.spectral_density(lam))
    >>> fnu  # doctest: +FLOAT_CMP
    <Quantity [5.80636959e-27, 1.30643316e-28, 3.62898099e-29] erg / (cm2 Hz s)>

We could have used the central frequency instead::

    >>> nu = 5.45077196e+14 * u.Hz
    >>> fnu = V.to(u.erg/u.s/u.cm**2/u.Hz, u.spectral_density(nu))
    >>> fnu  # doctest: +FLOAT_CMP
    <Quantity [5.80636959e-27, 1.30643316e-28, 3.62898099e-29] erg / (cm2 Hz s)>

.. Note::

    When converting magnitudes to flux densities, the order of operations
    matters; the value of the unit needs to be established *before* the
    conversion. For example, ``21 * u.ABmag.to(u.erg/u.s/u.cm**2/u.Hz)`` will
    give you 21 times :math:`f_\nu` for an AB mag of 1, whereas ``(21 *
    u.ABmag).to(u.erg/u.s/u.cm**2/u.Hz)`` will give you :math:`f_\nu` for an AB
    mag of 21.

Suppose we also knew the intrinsic color of the first star, then we can
calculate the reddening::

    >>> B_V0 = -0.2 * u.mag
    >>> EB_V = (B - V)[0] - B_V0
    >>> R_V = 3.1
    >>> A_V = R_V * EB_V
    >>> A_B = (R_V+1) * EB_V
    >>> EB_V, A_V, A_B  # doctest: +FLOAT_CMP
    (<Magnitude 0.4 mag>, <Magnitude 1.24 mag>, <Magnitude 1.64 mag>)

Here, you see that the extinctions have been converted to quantities. This
happens generally for division and multiplication, since these processes
work only for dimensionless magnitudes (otherwise, the physical unit would have
to be raised to some power), and |Quantity| objects, unlike logarithmic
quantities, allow units like ``mag / d``.

.. EXAMPLE END

Note that you can take the automatic unit conversion quite far (perhaps too
far, but it is fun). For instance, suppose we also knew the bolometric
correction and absolute bolometric magnitude, then we can calculate the
distance modulus::

    >>> BC_V = -0.3 * (u.m_bol - u.STmag)
    >>> M_bol = 5.46 * u.M_bol
    >>> DM = V[0] - A_V + BC_V - M_bol
    >>> BC_V, M_bol, DM  # doctest: +FLOAT_CMP
    (<Magnitude -0.3 mag(bol / ST)>,
     <Magnitude 5.46 mag(Bol)>,
     <Magnitude 10. mag(bol / Bol)>)

With a proper :ref:`equivalency <unit_equivalencies>`, we can also convert to
distance without remembering the 5-5log rule (but you might find the
:class:`~astropy.coordinates.Distance` class to be even more convenient)::

    >>> radius_and_inverse_area = [(u.pc, u.pc**-2,
    ...                            lambda x: 1./(4.*np.pi*x**2),
    ...                            lambda x: np.sqrt(1./(4.*np.pi*x)))]
    >>> DM.to(u.pc, equivalencies=radius_and_inverse_area)  # doctest: +FLOAT_CMP
    <Quantity 1000. pc>

NumPy Functions
===============

For logarithmic quantities, most ``numpy`` functions and many array methods do
not make sense, hence they are disabled. But you can use those you would expect
to work::

    >>> np.max(v_i)  # doctest: +FLOAT_CMP
    <Magnitude 4.00514998 mag(ct / s)>
    >>> np.std(v_i)  # doctest: +FLOAT_CMP
    <Magnitude 2.33971149 mag>

.. note::

    This is implemented by having a list of supported ufuncs in
    ``units/function/core.py`` and by explicitly disabling some array methods in
    :class:`~astropy.units.function.FunctionQuantity`.  If you believe a
    function or method is incorrectly treated, please `let us know
    <http://www.astropy.org/contribute.html>`_.

Dimensionless Logarithmic Quantities
====================================

Dimensionless quantities are treated somewhat specially in that, if needed,
logarithmic quantities will be converted to normal |Quantity| objects with the
appropriate unit of ``mag``, ``dB``, or ``dex``.  With this, it is possible to
use composite units like ``mag/d`` or ``dB/m``, which cannot conveniently be
supported as logarithmic units. For instance::

    >>> dBm = u.dB(u.mW)
    >>> signal_in, signal_out = 100. * dBm, 50 * dBm
    >>> cable_loss = (signal_in - signal_out) / (100. * u.m)
    >>> signal_in, signal_out, cable_loss  # doctest: +FLOAT_CMP
    (<Decibel 100. dB(mW)>, <Decibel 50. dB(mW)>, <Quantity 0.5 dB / m>)
    >>> better_cable_loss = 0.2 * u.dB / u.m
    >>> signal_in - better_cable_loss * 100. * u.m  # doctest: +FLOAT_CMP
    <Decibel 80. dB(mW)>

**References**

.. [M15] Mamajek et al., 2015, `arXiv:1510.06262
	  <https://ui.adsabs.harvard.edu/abs/2015arXiv151006262M>`_
.. [H95] E.g., Holtzman et al., 1995, `PASP 107, 1065
          <https://ui.adsabs.harvard.edu/abs/1995PASP..107.1065H>`_
.. [OG83] Oke, J.B., & Gunn, J. E., 1983, `ApJ 266, 713
	  <https://ui.adsabs.harvard.edu/abs/1983ApJ...266..713O>`_
Decomposing and Composing Units
*******************************

.. _decomposing:

Reducing a Unit to Its Irreducible Parts
========================================

A unit or quantity can be decomposed into its irreducible parts using
the `Unit.decompose() <astropy.units.core.UnitBase.decompose>` or
`Quantity.decompose() <astropy.units.quantity.Quantity.decompose>`
methods.

Examples
--------

.. EXAMPLE START: Reducing a Unit to Its Irreducible Parts

To decompose a unit with :meth:`~astropy.units.core.UnitBase.decompose`::

  >>> from astropy import units as u
  >>> u.Ry
  Unit("Ry")
  >>> u.Ry.decompose()
  Unit("2.17987e-18 kg m2 / s2")

You can limit the selection of units that you want to decompose by
using the ``bases`` keyword argument::

  >>> u.Ry.decompose(bases=[u.m, u.N])
  Unit("2.17987e-18 m N")

This is also useful to decompose to a particular system. For example,
to decompose the Rydberg unit of energy in terms of `CGS
<https://en.wikipedia.org/wiki/Centimetre-gram-second_system_of_units>`_
units::

  >>> u.Ry.decompose(bases=u.cgs.bases)
  Unit("2.17987e-11 cm2 g / s2")

Finally, if you want to know how a unit was defined::

  >>> u.Ry.represents
  Unit("13.6057 eV")

.. EXAMPLE END

Automatically Composing a Unit into More Complex Units
======================================================

Conversely, a unit may be recomposed back into more complex units
using the :meth:`~astropy.units.core.UnitBase.compose` method. Since there
may be multiple equally good results, a list is always returned.

Examples
--------

.. EXAMPLE START: Recomposing a Unit into More Complex Units

To recompose a unit with :meth:`~astropy.units.core.UnitBase.compose`::

  >>> x = u.Ry.decompose()
  >>> x.compose()
  [Unit("Ry"),
   Unit("2.17987e-18 J"),
   Unit("2.17987e-11 erg"),
   Unit("13.6057 eV")]

Some other interesting examples::

   >>> (u.s ** -1).compose()  # doctest: +SKIP
   [Unit("Bq"), Unit("Hz"), Unit("2.7027e-11 Ci")]

Composition can be combined with :ref:`unit_equivalencies`::

   >>> (u.s ** -1).compose(equivalencies=u.spectral())  # doctest: +SKIP
   [Unit("m"),
    Unit("Hz"),
    Unit("J"),
    Unit("Bq"),
    Unit("3.24078e-17 pc"),
    Unit("1.057e-16 lyr"),
    Unit("6.68459e-12 AU"),
    Unit("1.4378e-09 solRad"),
    Unit("0.01 k"),
    Unit("100 cm"),
    Unit("1e+06 micron"),
    Unit("1e+07 erg"),
    Unit("1e+10 Angstrom"),
    Unit("3.7e+10 Ci"),
    Unit("4.58743e+17 Ry"),
    Unit("6.24151e+18 eV")]

A name does not exist for every arbitrary derived unit
imaginable. In that case, the system will do its best to reduce the
unit to the fewest possible symbols::

   >>> (u.cd * u.sr * u.V * u.s).compose()
   [Unit("lm Wb")]

.. EXAMPLE END

Converting Between Systems
==========================

Built on top of this functionality is a convenience method to convert
between unit systems.

Examples
--------

.. EXAMPLE START: Converting Between Unit Systems

To convert between unit systems::

   >>> u.Pa.to_system(u.cgs)
   [Unit("10 P / s"), Unit("10 Ba")]

There is also a shorthand for this which only returns the first of
many possible matches::

   >>> u.Pa.cgs
   Unit("10 P / s")

This is equivalent to decomposing into the new system and then
composing into the most complex units possible, though
:meth:`~astropy.units.core.UnitBase.to_system` adds some extra logic to
return the results sorted in the most useful order::

   >>> u.Pa.decompose(bases=u.cgs.bases)
   Unit("10 g / (cm s2)")
   >>> _.compose(units=u.cgs)
   [Unit("10 Ba"), Unit("10 P / s")]

.. EXAMPLE END
Combining and Defining Units
****************************

Basic example
=============

.. EXAMPLE START: Combining Units and Quantities

Units and quantities can be combined together using the regular Python
numeric operators::

  >>> from astropy import units as u
  >>> fluxunit = u.erg / (u.cm ** 2 * u.s)
  >>> fluxunit
  Unit("erg / (cm2 s)")
  >>> 52.0 * fluxunit  # doctest: +FLOAT_CMP
  <Quantity  52. erg / (cm2 s)>
  >>> 52.0 * fluxunit / u.s  # doctest: +FLOAT_CMP
  <Quantity  52. erg / (cm2 s2)>

.. EXAMPLE END

Fractional powers
=================

.. EXAMPLE START: Using Fractional Powers with Units

Units support fractional powers, which retain their precision through
complex operations. To do this, it is recommended to use
:class:`fractions.Fraction` objects::

  >>> from fractions import Fraction
  >>> Franklin = u.g ** Fraction(1, 2) * u.cm ** Fraction(3, 2) * u.s ** -1

.. note::

    Floating-point powers that are effectively the same as fractions
    with a denominator less than 10 are implicitly converted to
    `~fractions.Fraction` objects under the hood. Therefore, the
    following are equivalent::

        >>> x = u.m ** Fraction(1, 3)
        >>> x.powers
        [Fraction(1, 3)]
        >>> x = u.m ** (1. / 3.)
        >>> x.powers
        [Fraction(1, 3)]

.. EXAMPLE END

Defining units
==============

.. EXAMPLE START: Defining New Units

Users are free to define new units, either fundamental or compound,
using the :func:`~astropy.units.def_unit` function::

  >>> bakers_fortnight = u.def_unit('bakers_fortnight', 13 * u.day)

The addition of a string gives the new unit a name that will show up
when the unit is printed::

  >>> 10. * bakers_fortnight  # doctest: +FLOAT_CMP
  <Quantity  10. bakers_fortnight>

Creating a new fundamental unit is also possible::

  >>> titter = u.def_unit('titter')
  >>> chuckle = u.def_unit('chuckle', 5 * titter)
  >>> laugh = u.def_unit('laugh', 4 * chuckle)
  >>> guffaw = u.def_unit('guffaw', 3 * laugh)
  >>> rofl = u.def_unit('rofl', 4 * guffaw)
  >>> death_by_laughing = u.def_unit('death_by_laughing', 10 * rofl)
  >>> (1. * rofl).to(titter)  # doctest: +FLOAT_CMP
  <Quantity  240. titter>

Users can see the definition of a unit and its :ref:`decomposition
<decomposing>` via::

  >>> rofl.represents
  Unit("4 guffaw")
  >>> rofl.decompose()
  Unit("240 titter")

By default, custom units are not searched by methods such as
:meth:`~astropy.units.core.UnitBase.find_equivalent_units`. However, they
can be enabled by calling :func:`~astropy.units.add_enabled_units`::

  >>> kmph = u.def_unit('kmph', u.km / u.h)
  >>> (u.m / u.s).find_equivalent_units()
  There are no equivalent units
  >>> u.add_enabled_units([kmph])
  <astropy.units.core._UnitContext object at ...>
  >>> (u.m / u.s).find_equivalent_units()
    Primary name | Unit definition | Aliases
  [
    kmph         | 0.277778 m / s  |         ,
  ]

.. EXAMPLE END
.. _astropy-units:

**************************************
Units and Quantities (`astropy.units`)
**************************************

.. currentmodule:: astropy.units

Introduction
============

`astropy.units` handles defining, converting between, and performing
arithmetic with physical quantities, such as meters, seconds, Hz,
etc. It also handles logarithmic units such as magnitude and decibel.

`astropy.units` does not know spherical geometry or sexagesimal
(hours, min, sec): if you want to deal with celestial coordinates,
see the `astropy.coordinates` package.

Getting Started
===============

Most users of the `astropy.units` package will work with :ref:`Quantity objects
<quantity>`: the combination of a value and a unit. The most convenient way to
create a |Quantity| is to multiply or divide a value by one of the built-in
units. It works with scalars, sequences, and ``numpy`` arrays.

Examples
--------

.. EXAMPLE START: Creating and Combining Quantities with Units

To create a |Quantity| object::

    >>> from astropy import units as u
    >>> 42.0 * u.meter  # doctest: +FLOAT_CMP
    <Quantity  42. m>
    >>> [1., 2., 3.] * u.m  # doctest: +FLOAT_CMP
    <Quantity [1., 2., 3.] m>
    >>> import numpy as np
    >>> np.array([1., 2., 3.]) * u.m  # doctest: +FLOAT_CMP
    <Quantity [1., 2., 3.] m>

You can get the unit and value from a |Quantity| using the unit and
value members::

    >>> q = 42.0 * u.meter
    >>> q.value
    42.0
    >>> q.unit
    Unit("m")

From this basic building block, it is possible to start combining
quantities with different units::

    >>> 15.1 * u.meter / (32.0 * u.second)  # doctest: +FLOAT_CMP
    <Quantity 0.471875 m / s>
    >>> 3.0 * u.kilometer / (130.51 * u.meter / u.second)  # doctest: +FLOAT_CMP
    <Quantity 0.022986744310780783 km s / m>
    >>> (3.0 * u.kilometer / (130.51 * u.meter / u.second)).decompose()  # doctest: +FLOAT_CMP
    <Quantity 22.986744310780782 s>

Unit conversion is done using the
:meth:`~astropy.units.quantity.Quantity.to` method, which returns a new
|Quantity| in the given unit::

    >>> x = 1.0 * u.parsec
    >>> x.to(u.km)  # doctest: +FLOAT_CMP
    <Quantity 30856775814671.914 km>

.. EXAMPLE END

.. EXAMPLE START: Creating Custom Units for Quantity Objects

It is also possible to work directly with units at a lower level, for
example, to create custom units::

    >>> from astropy.units import imperial

    >>> cms = u.cm / u.s
    >>> # ...and then use some imperial units
    >>> mph = imperial.mile / u.hour

    >>> # And do some conversions
    >>> q = 42.0 * cms
    >>> q.to(mph)  # doctest: +FLOAT_CMP
    <Quantity 0.939513242662849 mi / h>

Units that "cancel out" become a special unit called the
"dimensionless unit":

    >>> u.m / u.m
    Unit(dimensionless)

To create a basic :ref:`dimensionless quantity <doc_dimensionless_unit>`,
multiply a value by the unscaled dimensionless unit::

    >>> q = 1.0 * u.dimensionless_unscaled
    >>> q.unit
    Unit(dimensionless)

.. EXAMPLE END

.. EXAMPLE START: Matching and Converting Between Units

`astropy.units` is able to match compound units against the units it already
knows about::

    >>> (u.s ** -1).compose()  # doctest: +SKIP
    [Unit("Bq"), Unit("Hz"), Unit("2.7027e-11 Ci")]

And it can convert between unit systems, such as `SI
<https://www.bipm.org/documents/20126/41483022/SI-Brochure-9-EN.pdf>`_ or `CGS
<https://en.wikipedia.org/wiki/Centimetre-gram-second_system_of_units>`_::

    >>> (1.0 * u.Pa).cgs
    <Quantity 10. P / s>

The units ``mag``, ``dex``, and ``dB`` are special, being :ref:`logarithmic
units <logarithmic_units>`, for which a value is the logarithm of a physical
quantity in a given unit. These can be used with a physical unit in
parentheses to create a corresponding logarithmic quantity::

    >>> -2.5 * u.mag(u.ct / u.s)
    <Magnitude -2.5 mag(ct / s)>
    >>> from astropy import constants as c
    >>> u.Dex((c.G * u.M_sun / u.R_sun**2).cgs)  # doctest: +FLOAT_CMP
    <Dex 4.438067627303133 dex(cm / s2)>

`astropy.units` also handles :ref:`equivalencies <unit_equivalencies>`, such as
that between wavelength and frequency. To use that feature, equivalence objects
are passed to the :meth:`~astropy.units.quantity.Quantity.to` conversion
method. For instance, a conversion from wavelength to frequency does not
normally work:

    >>> (1000 * u.nm).to(u.Hz)  # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
      ...
    UnitConversionError: 'nm' (length) and 'Hz' (frequency) are not convertible

But by passing an equivalency list, in this case
:func:`~astropy.units.equivalencies.spectral`, it does:

    >>> (1000 * u.nm).to(u.Hz, equivalencies=u.spectral())  # doctest: +FLOAT_CMP
    <Quantity  2.99792458e+14 Hz>

.. EXAMPLE END

.. EXAMPLE START: Printing Quantities and Units to Strings

Quantities and units can be :ref:`printed nicely to strings
<astropy-units-format>` using the `Format String Syntax
<https://docs.python.org/3/library/string.html#format-string-syntax>`_. Format
specifiers (like ``0.03f``) in strings will be used to format the quantity
value::

    >>> q = 15.1 * u.meter / (32.0 * u.second)
    >>> q  # doctest: +FLOAT_CMP
    <Quantity 0.471875 m / s>
    >>> f"{q:0.03f}"
    '0.472 m / s'

The value and unit can also be formatted separately. Format specifiers
for units can be used to choose the unit formatter::

    >>> q = 15.1 * u.meter / (32.0 * u.second)
    >>> q  # doctest: +FLOAT_CMP
    <Quantity 0.471875 m / s>
    >>> f"{q.value:0.03f} {q.unit:FITS}"
    '0.472 m s-1'

.. EXAMPLE END

Using `astropy.units`
=====================

.. toctree::
   :maxdepth: 2

   quantity
   type_hints
   standard_units
   combining_and_defining
   decomposing_and_composing
   logarithmic_units
   structured_units
   format
   equivalencies
   physical_types
   constants_versions
   conversion

Acknowledgments
===============

This code was originally based on the `pynbody
<https://github.com/pynbody/pynbody>`__ units module written by Andrew
Pontzen, who has granted the Astropy Project permission to use the code
under a BSD license.

See Also
========

- `FITS Standard <https://fits.gsfc.nasa.gov/fits_standard.html>`_ for
  units in FITS.

- The `Units in the VO 1.0 Standard
  <http://www.ivoa.net/documents/VOUnits/>`_ for representing units in
  the VO.

- OGIP Units: A standard for storing units in `OGIP FITS files
  <https://heasarc.gsfc.nasa.gov/docs/heasarc/ofwg/docs/general/ogip_93_001/>`_.

- `Standards for astronomical catalogues: units
  <http://vizier.u-strasbg.fr/vizier/doc/catstd-3.2.htx>`_.

- `IAU Style Manual
  <https://www.iau.org/static/publications/stylemanual1989.pdf>`_.

- `A table of astronomical unit equivalencies
  <https://www.stsci.edu/~strolger/docs/UNITS.txt>`_.

.. note that if this section gets too long, it should be moved to a separate
   doc page - see the top of performance.inc.rst for the instructions on how to do
   that
.. include:: performance.inc.rst

Reference/API
=============

.. automodapi:: astropy.units.quantity

.. automodapi:: astropy.units

.. automodapi:: astropy.units.format

.. automodapi:: astropy.units.si

.. automodapi:: astropy.units.cgs

.. automodapi:: astropy.units.astrophys

.. automodapi:: astropy.units.misc

.. automodapi:: astropy.units.function.units

.. automodapi:: astropy.units.photometric

.. automodapi:: astropy.units.imperial

.. automodapi:: astropy.units.cds

.. automodapi:: astropy.units.physical

.. automodapi:: astropy.units.equivalencies

.. automodapi:: astropy.units.function

.. automodapi:: astropy.units.function.logarithmic
   :include-all-objects:

.. automodapi:: astropy.units.deprecated

.. automodapi:: astropy.units.required_by_vounit
.. _quantity:

Quantity
********

The |Quantity| object is meant to represent a value that has some unit
associated with the number.

Creating Quantity Instances
===========================

|Quantity| objects are normally created through multiplication with
:class:`~astropy.units.Unit` objects.

Examples
--------

.. EXAMPLE START: Creating Quantity Instances Through Multiplication

To create a |Quantity| to represent 15 m/s:

    >>> import astropy.units as u
    >>> 15 * u.m / u.s  # doctest: +FLOAT_CMP
    <Quantity 15. m / s>

This extends as expected to division by a unit, or using ``numpy`` arrays or
`Python sequences <https://docs.python.org/3/library/stdtypes.html#typesseq>`_:

    >>> 1.25 / u.s
    <Quantity 1.25 1 / s>
    >>> [1, 2, 3] * u.m  # doctest: +FLOAT_CMP
    <Quantity [1., 2., 3.] m>
    >>> import numpy as np
    >>> np.array([1, 2, 3]) * u.m  # doctest: +FLOAT_CMP
    <Quantity [1., 2., 3.] m>

.. EXAMPLE END

.. EXAMPLE START: Creating Quantity Instances Using the Quantity Constructor

You can also create instances using the |Quantity| constructor directly, by
specifying a value and unit:

    >>> u.Quantity(15, u.m / u.s)  # doctest: +FLOAT_CMP
    <Quantity 15. m / s>

The constructor gives a few more options. In particular, it allows you to
merge sequences of |Quantity| objects (as long as all of their units are
equivalent), and to parse simple strings (which may help, for example, to parse
configuration files, etc.):

    >>> qlst = [60 * u.s, 1 * u.min]
    >>> u.Quantity(qlst, u.minute)  # doctest: +FLOAT_CMP
    <Quantity [1.,  1.] min>
    >>> u.Quantity('15 m/s')  # doctest: +FLOAT_CMP
    <Quantity 15. m / s>

The current unit and value can be accessed via the
`~astropy.units.quantity.Quantity.unit` and
`~astropy.units.quantity.Quantity.value` attributes:

    >>> q = 2.5 * u.m / u.s
    >>> q.unit
    Unit("m / s")
    >>> q.value
    2.5

.. note:: |Quantity| objects are converted to float by default. Furthermore, any
          data passed in are copied, which for large arrays may not be optimal.
          As discussed :ref:`further below <astropy-units-quantity-no-copy>`,
          you can instead obtain a `view
          <https://numpy.org/doc/stable/glossary.html#term-view>`_ by passing
          ``copy=False`` to |Quantity| or by using the ``<<`` operator.

.. EXAMPLE END

.. _quantity_unit_conversion:

Converting to Different Units
=============================

|Quantity| objects can be converted to different units using the
:meth:`~astropy.units.quantity.Quantity.to` method.

Examples
--------

.. EXAMPLE START: Converting Quantity Objects to Different Units

To convert |Quantity| objects to different units:

    >>> q = 2.3 * u.m / u.s
    >>> q.to(u.km / u.h)  # doctest: +FLOAT_CMP
    <Quantity 8.28 km / h>

For convenience, the :attr:`~astropy.units.quantity.Quantity.si` and
:attr:`~astropy.units.quantity.Quantity.cgs` attributes can be used to convert
the |Quantity| to base `SI
<https://www.bipm.org/documents/20126/41483022/SI-Brochure-9-EN.pdf>`_ or `CGS
<https://en.wikipedia.org/wiki/Centimetre-gram-second_system_of_units>`_ units:

    >>> q = 2.4 * u.m / u.s
    >>> q.si  # doctest: +FLOAT_CMP
    <Quantity 2.4 m / s>
    >>> q.cgs  # doctest: +FLOAT_CMP
    <Quantity 240. cm / s>

If you want the value of the quantity in a different unit, you can use
:meth:`~astropy.units.Quantity.to_value` as a shortcut:

    >>> q = 2.5 * u.m
    >>> q.to_value(u.cm)
    250.0

.. note:: You could get the value in ``cm`` also by using ``q.to(u.cm).value``.
          The difference is that :meth:`~astropy.units.Quantity.to_value` does
          no copying if the unit is already the correct one, instead
          returning a `view
          <https://numpy.org/doc/stable/glossary.html#term-view>`_  of the data
          (just as if you had done ``q.value``). In contrast,
          :meth:`~astropy.units.Quantity.to` always returns a copy (which also
          means it is slower for the case where no conversion is necessary).
          As discussed :ref:`further below <astropy-units-quantity-no-copy>`,
          you can avoid the copying by using the ``<<`` operator.

Comparing Quantities
====================

The equality of |Quantity| objects is best tested using the
:func:`~astropy.units.allclose` and :func:`~astropy.units.isclose` functions,
which are unit-aware analogues of the ``numpy`` functions with the same name::

    >>> u.allclose([1, 2] * u.m, [100, 200] * u.cm)
    True
    >>> u.isclose([1, 2] * u.m, [100, 20] * u.cm)
    array([ True, False])

The use of `Python comparison operators
<https://docs.python.org/3/reference/expressions.html#comparisons>`_ is also
supported::

    >>> 1*u.m < 50*u.cm
    False

Plotting Quantities
===================

|Quantity| objects can be conveniently plotted using `Matplotlib`_  see
:ref:`plotting-quantities` for more details.

.. _quantity_arithmetic:

Arithmetic
==========

Addition and Subtraction
------------------------

Addition or subtraction between |Quantity| objects is supported when their
units are equivalent.

Examples
^^^^^^^^

.. EXAMPLE START: Addition and Subtraction Between Quantity Objects

When the units are equal, the resulting object has the same unit:

    >>> 11 * u.s + 30 * u.s  # doctest: +FLOAT_CMP
    <Quantity 41. s>
    >>> 30 * u.s - 11 * u.s  # doctest: +FLOAT_CMP
    <Quantity 19. s>

If the units are equivalent, but not equal (e.g., kilometer and meter), the
resulting object **has units of the object on the left**:

    >>> 1100.1 * u.m + 13.5 * u.km
    <Quantity 14600.1 m>
    >>> 13.5 * u.km + 1100.1 * u.m  # doctest: +FLOAT_CMP
    <Quantity 14.6001 km>
    >>> 1100.1 * u.m - 13.5 * u.km
    <Quantity -12399.9 m>
    >>> 13.5 * u.km - 1100.1 * u.m  # doctest: +FLOAT_CMP
    <Quantity 12.3999 km>

Addition and subtraction are not supported between |Quantity| objects and basic
numeric types, except for dimensionless quantities (see `Dimensionless
Quantities`_) or special values like zero and infinity::

    >>> 13.5 * u.km + 19.412  # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
      ...
    UnitConversionError: Can only apply 'add' function to dimensionless
    quantities when other argument is not a quantity (unless the
    latter is all zero/infinity/nan)

.. EXAMPLE END

Multiplication and Division
---------------------------

Multiplication and division are supported between |Quantity| objects with any
units, and with numeric types. For these operations between objects with
equivalent units, the **resulting object has composite units**.

Examples
^^^^^^^^

.. EXAMPLE START: Multiplication and Division Between Quantity Objects

To perform these operations on |Quantity| objects:

    >>> 1.1 * u.m * 140.3 * u.cm  # doctest: +FLOAT_CMP
    <Quantity 154.33 cm m>
    >>> 140.3 * u.cm * 1.1 * u.m  # doctest: +FLOAT_CMP
    <Quantity 154.33 cm m>
    >>> 1. * u.m / (20. * u.cm)  # doctest: +FLOAT_CMP
    <Quantity 0.05 m / cm>
    >>> 20. * u.cm / (1. * u.m)  # doctest: +FLOAT_CMP
    <Quantity 20. cm / m>

For multiplication, you can change how to represent the resulting object by
using the :meth:`~astropy.units.quantity.Quantity.to` method:

    >>> (1.1 * u.m * 140.3 * u.cm).to(u.m**2)  # doctest: +FLOAT_CMP
    <Quantity 1.5433 m2>
    >>> (1.1 * u.m * 140.3 * u.cm).to(u.cm**2)  # doctest: +FLOAT_CMP
    <Quantity 15433. cm2>

For division, if the units are equivalent, you may want to make the resulting
object dimensionless by reducing the units. To do this, use the
:meth:`~astropy.units.quantity.Quantity.decompose()` method:

    >>> (20. * u.cm / (1. * u.m)).decompose()  # doctest: +FLOAT_CMP
    <Quantity 0.2>

This method is also useful for more complicated arithmetic:

    >>> 15. * u.kg * 32. * u.cm * 15 * u.m / (11. * u.s * 1914.15 * u.ms)  # doctest: +FLOAT_CMP
    <Quantity 0.34195097 cm kg m / (ms s)>
    >>> (15. * u.kg * 32. * u.cm * 15 * u.m / (11. * u.s * 1914.15 * u.ms)).decompose()  # doctest: +FLOAT_CMP
    <Quantity 3.41950973 kg m2 / s2>

.. EXAMPLE END

.. _quantity_and_numpy:

NumPy Functions
===============

|Quantity| objects are actually full ``numpy`` arrays (the |Quantity| class
inherits from and extends :class:`numpy.ndarray`), and we have tried to ensure
that ``numpy`` functions behave properly with quantities:

    >>> q = np.array([1., 2., 3., 4.]) * u.m / u.s
    >>> np.mean(q)
    <Quantity 2.5 m / s>
    >>> np.std(q)  # doctest: +FLOAT_CMP
    <Quantity 1.11803399 m / s>

This includes functions that only accept specific units such as angles:

    >>> q = 30. * u.deg
    >>> np.sin(q)  # doctest: +FLOAT_CMP
    <Quantity 0.5>

Or `Dimensionless Quantities`_::

    >>> from astropy.constants import h, k_B
    >>> nu = 3 * u.GHz
    >>> T = 30 * u.K
    >>> np.exp(-h * nu / (k_B * T))  # doctest: +FLOAT_CMP
    <Quantity 0.99521225>

.. note:: Support for functions from other packages, such as `scipy`_, is more
          incomplete (contributions to improve this are welcomed!).

Dimensionless Quantities
========================

Dimensionless quantities have the characteristic that if they are
added to or subtracted from a Python scalar or unitless `~numpy.ndarray`,
or if they are passed to a ``numpy`` function that takes dimensionless
quantities, the units are simplified so that the quantity is
dimensionless and scale-free. For example:

    >>> 1. + 1. * u.m / u.km  # doctest: +FLOAT_CMP
    <Quantity 1.001>

Which is different from:

    >>> 1. + (1. * u.m / u.km).value
    2.0

In the latter case, the result is ``2.0`` because the unit of ``(1. * u.m /
u.km)`` is not scale-free by default:

    >>> q = (1. * u.m / u.km)
    >>> q.unit
    Unit("m / km")
    >>> q.unit.decompose()
    Unit(dimensionless with a scale of 0.001)

However, when combining with an object that is not a |Quantity|, the unit is
automatically decomposed to be scale-free, giving the expected result.

This also occurs when passing dimensionless quantities to functions that take
dimensionless quantities:

    >>> nu = 3 * u.GHz
    >>> T = 30 * u.K
    >>> np.exp(- h * nu / (k_B * T))  # doctest: +FLOAT_CMP
    <Quantity 0.99521225>

The result is independent from the units in which the different quantities were
specified:

    >>> nu = 3.e9 * u.Hz
    >>> T = 30 * u.K
    >>> np.exp(- h * nu / (k_B * T))  # doctest: +FLOAT_CMP
    <Quantity 0.99521225>

Converting to Plain Python Scalars
==================================

Converting |Quantity| objects does not work for non-dimensionless quantities:

    >>> float(3. * u.m)
    Traceback (most recent call last):
      ...
    TypeError: only dimensionless scalar quantities can be converted
    to Python scalars

Only dimensionless values can be converted to plain Python scalars:

    >>> float(3. * u.m / (4. * u.m))
    0.75
    >>> float(3. * u.km / (4. * u.m))
    750.0
    >>> int(6. * u.km / (2. * u.m))
    3000

Functions that Accept Quantities
================================

If a function accepts a |Quantity| as an argument then it can be a good idea to
check that the provided |Quantity| belongs to one of the expected
:ref:`physical_types`. This can be done with the `decorator
<https://docs.python.org/3/glossary.html#term-decorator>`_
:func:`~astropy.units.quantity_input`.

The decorator does not convert the input |Quantity| to the desired unit, say
arcseconds to degrees in the example below, it merely checks that such a
conversion is possible, thus verifying that the `~astropy.units.Quantity`
argument can be used in calculations.

Keyword arguments to :func:`~astropy.units.quantity_input` specify which
arguments should be validated and what unit they are expected to be compatible
with.

Examples
--------

.. EXAMPLE START: Functions that Accept Quantities

To verify if a |Quantity| argument can be used in calculations::

    >>> @u.quantity_input(myarg=u.deg)
    ... def myfunction(myarg):
    ...     return myarg.unit

    >>> myfunction(100*u.arcsec)
    Unit("arcsec")
    >>> myfunction(2*u.m)  # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
    ...
    UnitsError: Argument 'myarg' to function 'myfunction' must be in units
    convertible to 'deg'.

It is also possible to instead specify the :ref:`physical type
<physical_types>` of the desired unit::

    >>> @u.quantity_input(myarg='angle')
    ... def myfunction(myarg):
    ...     return myarg.unit

    >>> myfunction(100*u.arcsec)
    Unit("arcsec")

Optionally, `None` keyword arguments are also supported; for such cases, the
input is only checked when a value other than `None` is passed::

    >>> @u.quantity_input(a='length', b='angle')
    ... def myfunction(a, b=None):
    ...     return a, b

    >>> myfunction(1.*u.km)  # doctest: +FLOAT_CMP
    (<Quantity 1. km>, None)
    >>> myfunction(1.*u.km, 1*u.deg)  # doctest: +FLOAT_CMP
    (<Quantity 1. km>, <Quantity 1. deg>)

Alternatively, you can use the `annotations syntax
<https://docs.python.org/3/library/typing.html>`_ to provide the units.
While the raw unit or string can be used, the preferred method is with the
unit-aware Quantity-annotation syntax.
This requires Python 3.9 or the package ``typing_extensions``.

``Quantity[unit or "string", metadata, ...]``
.. doctest-skip::

    >>> @u.quantity_input
    ... def myfunction(myarg: u.Quantity[u.arcsec]):
    ...     return myarg.unit
    >>>
    >>> myfunction(100*u.arcsec)
    Unit("arcsec")

You can also annotate for different types in non-unit expecting arguments:
.. doctest-skip::

    >>> @u.quantity_input
    ... def myfunction(myarg: u.Quantity[u.arcsec], nice_string: str):
    ...     return myarg.unit, nice_string
    >>> myfunction(100*u.arcsec, "a nice string")
    (Unit("arcsec"), 'a nice string')

The output can be specified to have a desired unit with a function annotation,
for example
.. doctest-skip::

    >>> @u.quantity_input
    ... def myfunction(myarg: u.Quantity[u.arcsec]) -> u.deg:
    ...     return myarg*1000
    >>>
    >>> myfunction(100*u.arcsec)  # doctest: +FLOAT_CMP
    <Quantity 27.77777778 deg>

This both checks that the return value of your function is consistent with what
you expect and makes it much neater to display the results of the function.

.. EXAMPLE END

Specifying a list of valid equivalent units or :ref:`physical_types` is
supported for functions that should accept inputs with multiple valid units:

    >>> @u.quantity_input(a=['length', 'speed'])
    ... def myfunction(a):
    ...     return a.unit

    >>> myfunction(1.*u.km)
    Unit("km")
    >>> myfunction(1.*u.km/u.s)
    Unit("km / s")

Representing Vectors with Units
===============================

|Quantity| objects can, like ``numpy`` arrays, be used to represent vectors or
matrices by assigning specific dimensions to represent the coordinates or
matrix elements, but that implies tracking those dimensions carefully. For
vectors :ref:`astropy-coordinates-representations` can be more convenient as
doing so allows you to use representations other than Cartesian (such as
spherical or cylindrical), as well as simple vector arithmetic.

.. _astropy-units-quantity-no-copy:

Creating and Converting Quantities without Copies
=================================================

When creating a |Quantity| using multiplication with a unit, a copy of the
underlying data is made. This can be avoided by passing on ``copy=False`` in
the initializer.

Examples
--------

.. EXAMPLE START: Creating and Converting Quantities without Copies

To avoid duplication using ``copy=False``::

    >>> a = np.arange(5.)
    >>> q = u.Quantity(a, u.m, copy=False)
    >>> q  # doctest: +FLOAT_CMP
    <Quantity [0., 1., 2., 3., 4.] m>
    >>> np.may_share_memory(a, q)
    True
    >>> a[0] = -1.
    >>> q  # doctest: +FLOAT_CMP
    <Quantity [-1.,  1.,  2.,  3.,  4.] m>

This may be particularly useful in functions which do not change their input
while ensuring that if a user passes in a |Quantity| then it will be converted
to the desired unit.

.. EXAMPLE END

As a shortcut, you can "shift" to the requested unit using the ``<<``
operator::

    >>> q = a << u.m
    >>> np.may_share_memory(a, q)
    True
    >>> q  # doctest: +FLOAT_CMP
    <Quantity [-1.,  1.,  2.,  3.,  4.] m>

The operator works identically to the initialization with ``copy=False``
mentioned above::

    >>> q << u.cm  # doctest: +FLOAT_CMP
    <Quantity [-100.,  100.,  200.,  300.,  400.] cm>

It can also be used for in-place conversion::

    >>> q <<= u.cm
    >>> q  # doctest: +FLOAT_CMP
    <Quantity [-100.,  100.,  200.,  300.,  400.] cm>
    >>> a  # doctest: +FLOAT_CMP
    array([-100.,  100.,  200.,  300.,  400.])

QTable
======

It is possible to use |Quantity| objects as columns in :mod:`astropy.table`.
See :ref:`quantity_and_qtable` for more details.

Subclassing Quantity
====================

To subclass |Quantity|, you generally proceed as you would when subclassing
|ndarray| (i.e., you typically need to override ``__new__()``, rather than
``__init__()``, and use the ``numpy.ndarray.__array_finalize__()`` method to
update attributes). For details, see the `NumPy documentation on subclassing
<https://numpy.org/doc/stable/user/basics.subclassing.html>`_.  To get a sense
of what is involved, have a look at |Quantity| itself, where, for example, the
``astropy.units.Quantity.__array_finalize__()`` method is used to pass on the
``unit``, at :class:`~astropy.coordinates.Angle`, where strings are parsed as
angles in the ``astropy.coordinates.Angle.__new__()`` method and at
:class:`~astropy.coordinates.Longitude`, where the
``astropy.coordinates.Longitude.__array_finalize__()`` method is used to pass
on the angle at which longitudes wrap.

Another method that is meant to be overridden by subclasses, specific to
|Quantity|, is ``astropy.units.Quantity.__quantity_subclass__()``. This is
called to decide which type of subclass to return, based on the unit of the
|Quantity| that is to be created. It is used, for example, in
:class:`~astropy.coordinates.Angle` to return a |Quantity| if a calculation
returns a unit other than an angular one. The implementation of this is via
:class:`~astropy.units.SpecificTypeQuantity`, which more generally allows users
to construct |Quantity| subclasses that have methods that are useful only for a
specific physical type.
.. note that if this is changed from the default approach of using an *include*
   (in index.rst) to a separate performance page, the header needs to be changed
   from === to ***, the filename extension needs to be changed from .inc.rst to
   .rst, and a link needs to be added in the subpackage toctree

.. _astropy-time-performance:

.. Performance Tips
.. ================
..
.. Here we provide some tips and tricks for how to optimize performance of code
.. using `astropy.time`.
.. _astropy-uncertainty:

*******************************************************
Uncertainties and Distributions (`astropy.uncertainty`)
*******************************************************

Introduction
============

.. note:: This subpackage is still in development.

``astropy`` provides a |Distribution| object to represent statistical
distributions in a form that acts as a drop-in replacement for a |Quantity|
object or a regular |ndarray|. Used in this manner, |Distribution| provides
uncertainty propagation at the cost of additional computation. It can also more
generally represent sampled distributions for Monte Carlo calculation
techniques, for instance.

The core object for this feature is the |Distribution|. Currently, all
such distributions are Monte Carlo sampled. While this means each distribution
may take more memory, it allows arbitrarily complex operations to be performed
on distributions while maintaining their correlation structure. Some specific
well-behaved distributions (e.g., the normal distribution) have
analytic forms which may eventually enable a more compact and efficient
representation. In the future, these may provide a coherent uncertainty
propagation mechanism to work with `~astropy.nddata.NDData`. However, this is
not currently implemented. Hence, details of storing uncertainties for
`~astropy.nddata.NDData` objects can be found in the :ref:`astropy_nddata`
section.

Getting Started
===============

To demonstrate a basic use case for distributions, consider the problem of
uncertainty propagation of normal distributions. Assume there are two
measurements you wish to add, each with normal uncertainties. We start
with some initial imports and setup::

  >>> import numpy as np
  >>> from astropy import units as u
  >>> from astropy import uncertainty as unc
  >>> np.random.seed(12345)  # ensures reproducible example numbers

Now we create two |Distribution| objects to represent our distributions::

  >>> a = unc.normal(1*u.kpc, std=30*u.pc, n_samples=10000)
  >>> b = unc.normal(2*u.kpc, std=40*u.pc, n_samples=10000)

For normal distributions, the centers should add as expected, and the standard
deviations add in quadrature. We can check these results (to the limits of our
Monte Carlo sampling) trivially with |Distribution| arithmetic and attributes::

  >>> c = a + b
  >>> c # doctest: +ELLIPSIS
  <QuantityDistribution [...] kpc with n_samples=10000>
  >>> c.pdf_mean() # doctest: +FLOAT_CMP
  <Quantity 2.99970555 kpc>
  >>> c.pdf_std().to(u.pc) # doctest: +FLOAT_CMP
  <Quantity 50.07120457 pc>

Indeed these are close to the expectations. While this may seem unnecessary for
the basic Gaussian case, for more complex distributions or arithmetic
operations where error analysis becomes untenable, |Distribution| still powers
through::

  >>> d = unc.uniform(center=3*u.kpc, width=800*u.pc, n_samples=10000)
  >>> e = unc.Distribution(((np.random.beta(2,5, 10000)-(2/7))/2 + 3)*u.kpc)
  >>> f = (c * d * e) ** (1/3)
  >>> f.pdf_mean() # doctest: +FLOAT_CMP
  <Quantity 2.99786227 kpc>
  >>> f.pdf_std() # doctest: +FLOAT_CMP
  <Quantity 0.08330476 kpc>
  >>> from matplotlib import pyplot as plt # doctest: +SKIP
  >>> from astropy.visualization import quantity_support # doctest: +SKIP
  >>> with quantity_support():
  ...     plt.hist(f.distribution, bins=50) # doctest: +SKIP

.. plot::

  import numpy as np
  from astropy import units as u
  from astropy import uncertainty as unc
  from astropy.visualization import quantity_support
  from matplotlib import pyplot as plt
  np.random.seed(12345)
  a = unc.normal(1*u.kpc, std=30*u.pc, n_samples=10000)
  b = unc.normal(2*u.kpc, std=40*u.pc, n_samples=10000)
  c = a + b
  d = unc.uniform(center=3*u.kpc, width=800*u.pc, n_samples=10000)
  e = unc.Distribution(((np.random.beta(2,5, 10000)-(2/7))/2 + 3)*u.kpc)
  f = (c * d * e) ** (1/3)
  with quantity_support():
      plt.hist(f.distribution, bins=50)


Using `astropy.uncertainty`
===========================

Creating Distributions
----------------------

.. EXAMPLE START: Creating Distributions Using Arrays or Quantities

The most direct way to create a distribution is to use an array or |Quantity|
that carries the samples in the *last* dimension::

  >>> import numpy as np
  >>> from astropy import units as u
  >>> from astropy import uncertainty as unc
  >>> np.random.seed(123456)  # ensures "random" numbers match examples below
  >>> unc.Distribution(np.random.poisson(12, (1000)))  # doctest: +ELLIPSIS
  NdarrayDistribution([..., 12,...]) with n_samples=1000
  >>> pq = np.random.poisson([1, 5, 30, 400], (1000, 4)).T * u.ct # note the transpose, required to get the sampling on the *last* axis
  >>> distr = unc.Distribution(pq)
  >>> distr # doctest: +ELLIPSIS
  <QuantityDistribution [[...],
             [...],
             [...],
             [...]] ct with n_samples=1000>

Note the distinction for these two distributions: the first is built from an
array and therefore does not have |Quantity| attributes like ``unit``, while the
latter does have these attributes. This is reflected in how they interact with
other objects, for example, the ``NdarrayDistribution`` will not combine with
|Quantity| objects containing units.

.. EXAMPLE END

.. EXAMPLE START: Creating Distributions Using Helper Functions

For commonly used distributions, helper functions exist to make creating them
more convenient. The examples below demonstrate several equivalent ways to
create a normal/Gaussian distribution::

  >>> center = [1, 5, 30, 400]
  >>> n_distr = unc.normal(center*u.kpc, std=[0.2, 1.5, 4, 1]*u.kpc, n_samples=1000)
  >>> n_distr = unc.normal(center*u.kpc, var=[0.04, 2.25, 16, 1]*u.kpc**2, n_samples=1000)
  >>> n_distr = unc.normal(center*u.kpc, ivar=[25, 0.44444444, 0.625, 1]*u.kpc**-2, n_samples=1000)
  >>> n_distr.distribution.shape
  (4, 1000)
  >>> unc.normal(center*u.kpc, std=[0.2, 1.5, 4, 1]*u.kpc, n_samples=100).distribution.shape
  (4, 100)
  >>> unc.normal(center*u.kpc, std=[0.2, 1.5, 4, 1]*u.kpc, n_samples=20000).distribution.shape
  (4, 20000)

Additionally, Poisson and uniform |Distribution| creation functions exist::

  >>> unc.poisson(center*u.count, n_samples=1000) # doctest: +ELLIPSIS
  <QuantityDistribution [[...],
               [...],
               [...],
               [...]] ct with n_samples=1000>
  >>> uwidth = [10, 20, 10, 55]*u.pc
  >>> unc.uniform(center=center*u.kpc, width=uwidth, n_samples=1000) # doctest: +ELLIPSIS
  <QuantityDistribution [[...],
               [...],
               [...],
               [...]] kpc with n_samples=1000>
  >>> unc.uniform(lower=center*u.kpc - uwidth/2,  upper=center*u.kpc + uwidth/2, n_samples=1000)  # doctest: +ELLIPSIS
  <QuantityDistribution [[...],
               [...],
               [...],
               [...]] kpc with n_samples=1000>

.. EXAMPLE END

Users are free to create their own distribution classes following similar
patterns.

Using Distributions
-------------------

.. EXAMPLE START: Accessing Properties of Distributions

This object now acts much like a |Quantity| or |ndarray| for all but the
non-sampled dimension, but with additional statistical operations that work on
the sampled distributions::

  >>> distr.shape
  (4,)
  >>> distr.size
  4
  >>> distr.unit
  Unit("ct")
  >>> distr.n_samples
  1000
  >>> distr.pdf_mean() # doctest: +FLOAT_CMP
  <Quantity [  0.998,   5.017,  30.085, 400.345] ct>
  >>> distr.pdf_std() # doctest: +FLOAT_CMP
  <Quantity [ 0.97262326,  2.32222114,  5.47629208, 20.6328373 ] ct>
  >>> distr.pdf_var() # doctest: +FLOAT_CMP
  <Quantity [  0.945996,   5.392711,  29.989775, 425.713975] ct2>
  >>> distr.pdf_median()
  <Quantity [   1.,   5.,  30., 400.] ct>
  >>> distr.pdf_mad()  # Median absolute deviation # doctest: +FLOAT_CMP
  <Quantity [ 1.,  2.,  4., 14.] ct>
  >>> distr.pdf_smad()  # Median absolute deviation, rescaled to match std for normal # doctest: +FLOAT_CMP
  <Quantity [ 1.48260222,  2.96520444,  5.93040887, 20.75643106] ct>
  >>> distr.pdf_percentiles([10, 50, 90])
  <Quantity [[  0. ,   2. ,  23. , 374. ],
             [  1. ,   5. ,  30. , 400. ],
             [  2. ,   8. ,  37.1, 427. ]] ct>
  >>> distr.pdf_percentiles([.1, .5, .9]*u.dimensionless_unscaled)
  <Quantity [[  0. ,   2. ,  23. , 374. ],
            [  1. ,   5. ,  30. , 400. ],
            [  2. ,   8. ,  37.1, 427. ]] ct>

If need be, the underlying array can then be accessed from the ``distribution``
attribute::

  >>> distr.distribution  # doctest: +ELLIPSIS
  <Quantity [[...1...],
             [...5...],
             [...27...],
             [...405...]] ct>
  >>> distr.distribution.shape
  (4, 1000)

.. EXAMPLE END

.. EXAMPLE START: Interaction Between Quantity Objects and Distributions

A |Quantity| distribution interacts naturally with non-|Distribution|
|Quantity| objects, assuming the |Quantity| is a Dirac delta distribution::

  >>> distr_in_kpc = distr * u.kpc/u.count  # for the sake of round numbers in examples
  >>> distrplus = distr_in_kpc + [2000,0,0,500]*u.pc
  >>> distrplus.pdf_median()
  <Quantity [   3. ,   5. ,  30. , 400.5] kpc>
  >>> distrplus.pdf_var() # doctest: +FLOAT_CMP
  <Quantity [  0.945996,   5.392711,  29.989775, 425.713975] kpc2>

It also operates as expected with other distributions (but see below for a
discussion of covariances)::

  >>> another_distr = unc.Distribution((np.random.randn(1000,4)*[1000,.01 , 3000, 10] + [2000, 0, 0, 500]).T * u.pc)
  >>> combined_distr = distr_in_kpc + another_distr
  >>> combined_distr.pdf_median()  # doctest: +FLOAT_CMP
  <Quantity [  3.01847755,   4.99999576,  29.60559788, 400.49176321] kpc>
  >>> combined_distr.pdf_var()  # doctest: +FLOAT_CMP
  <Quantity [  1.8427705 ,   5.39271147,  39.5343726 , 425.71324244] kpc2>

.. EXAMPLE END

Covariance in Distributions and Discrete Sampling Effects
---------------------------------------------------------

One of the main applications for distributions is uncertainty propagation, which
critically requires proper treatment of covariance. This comes naturally in the
Monte Carlo sampling approach used by the |Distribution| class, as long as
proper care is taken with sampling error.

.. EXAMPLE START: Covariance in Distributions

To start with a basic example, two un-correlated distributions should produce
an un-correlated joint distribution plot:

.. plot::
  :context: close-figs
  :include-source:

  >>> import numpy as np
  >>> np.random.seed(12345)  # produce repeatable plots
  >>> from astropy import units as u
  >>> from astropy import uncertainty as unc
  >>> from matplotlib import pyplot as plt # doctest: +SKIP
  >>> n1 = unc.normal(center=0., std=1, n_samples=10000)
  >>> n2 = unc.normal(center=0., std=2, n_samples=10000)
  >>> plt.scatter(n1.distribution, n2.distribution, s=2, lw=0, alpha=.5) # doctest: +SKIP
  >>> plt.xlim(-4, 4) # doctest: +SKIP
  >>> plt.ylim(-4, 4) # doctest: +SKIP

Indeed, the distributions are independent. If we instead construct a covariant
pair of Gaussians, it is immediately apparent:

.. plot::
  :context: close-figs
  :include-source:

  >>> ncov = np.random.multivariate_normal([0, 0], [[1, .5], [.5, 2]], size=10000)
  >>> n1 = unc.Distribution(ncov[:, 0])
  >>> n2 = unc.Distribution(ncov[:, 1])
  >>> plt.scatter(n1.distribution, n2.distribution, s=2, lw=0, alpha=.5) # doctest: +SKIP
  >>> plt.xlim(-4, 4) # doctest: +SKIP
  >>> plt.ylim(-4, 4) # doctest: +SKIP

Most importantly, the proper correlated structure is preserved or generated as
expected by appropriate arithmetic operations. For example, ratios of
uncorrelated normal distribution gain covariances if the axes are not
independent, as in this simulation of iron, hydrogen, and oxygen abundances in
a hypothetical collection of stars:

.. plot::
  :context: close-figs
  :include-source:

  >>> fe_abund = unc.normal(center=-2, std=.25, n_samples=10000)
  >>> o_abund = unc.normal(center=-6., std=.5, n_samples=10000)
  >>> h_abund = unc.normal(center=-0.7, std=.1, n_samples=10000)
  >>> feh = fe_abund - h_abund
  >>> ofe = o_abund - fe_abund
  >>> plt.scatter(ofe.distribution, feh.distribution, s=2, lw=0, alpha=.5) # doctest: +SKIP
  >>> plt.xlabel('[Fe/H]') # doctest: +SKIP
  >>> plt.ylabel('[O/Fe]') # doctest: +SKIP

This demonstrates that the correlations naturally arise from the variables, but
there is no need to explicitly account for it: the sampling process naturally
recovers correlations that are present.

.. EXAMPLE END

.. EXAMPLE START: Preserving Covariance in Distributions

An important note of warning, however, is that the covariance is only preserved
if the sampling axes are exactly matched sample by sample. If they are not, all
covariance information is (silently) lost:

.. plot::
  :context: close-figs
  :include-source:

  >>> n2_wrong = unc.Distribution(ncov[::-1, 1])  #reverse the sampling axis order
  >>> plt.scatter(n1.distribution, n2_wrong.distribution, s=2, lw=0, alpha=.5) # doctest: +SKIP
  >>> plt.xlim(-4, 4) # doctest: +SKIP
  >>> plt.ylim(-4, 4) # doctest: +SKIP

Moreover, an insufficiently sampled distribution may give poor estimates or
hide correlations. The example below is the same as the covariant Gaussian
example above, but with 200x fewer samples:


.. plot::
  :context: close-figs
  :include-source:

  >>> ncov = np.random.multivariate_normal([0, 0], [[1, .5], [.5, 2]], size=50)
  >>> n1 = unc.Distribution(ncov[:, 0])
  >>> n2 = unc.Distribution(ncov[:, 1])
  >>> plt.scatter(n1.distribution, n2.distribution, s=5, lw=0) # doctest: +SKIP
  >>> plt.xlim(-4, 4) # doctest: +SKIP
  >>> plt.ylim(-4, 4) # doctest: +SKIP
  >>> np.cov(n1.distribution, n2.distribution) # doctest: +FLOAT_CMP
  array([[1.04667972, 0.19391617],
         [0.19391617, 1.50899902]])

The covariance structure is much less apparent by eye, and this is reflected
in significant discrepancies between the input and output covariance matrix.
In general this is an intrinsic trade-off using sampled distributions: a smaller
number of samples is computationally more efficient, but leads to larger
uncertainties in any of the relevant quantities. These tend to be of order
:math:`\sqrt{n_{\rm samples}}` in any derived quantity, but that depends on the
complexity of the distribution in question.

.. EXAMPLE END

.. note that if this section gets too long, it should be moved to a separate
   doc page - see the top of performance.inc.rst for the instructions on how to do
   that
.. include:: performance.inc.rst

Reference/API
=============

.. automodapi:: astropy.uncertainty
.. include:: references.rst
.. doctest-skip-all
.. _note_sip: :orphan:


Note about SIP and WCS
**********************

`astropy.wcs` supports the Simple Imaging Polynomial (`SIP`_) convention.
The SIP distortion is defined in FITS headers by the presence of the
SIP specific keywords **and** a ``-SIP`` suffix in ``CTYPE``, for example
``RA---TAN-SIP``, ``DEC--TAN-SIP``.

This has not been a strict convention in the past and the default in
`astropy.wcs` is to always include the SIP distortion if the SIP coefficients
are present, even if ``-SIP`` is not included in CTYPE.
The presence of a ``-SIP`` suffix in CTYPE is not used as a trigger
to initialize the SIP distortion.

It is important that headers implement correctly the SIP convention.
If the intention is to use the SIP distortion, a header should have
the SIP coefficients and the ``-SIP`` suffix in CTYPE.

`astropy.wcs` prints INFO messages when inconsistent headers are detected,
for example when SIP coefficients are present but CTYPE is missing a ``-SIP`` suffix,
see examples below.
`astropy.wcs` will print a message about the inconsistent header
but will create and use the SIP distortion and it will be used in
calls to `~astropy.wcs.wcs.WCS.all_pix2world`. If this was not the intended use
(e.g. it's a drizzled image and has no distortions) it is best to remove the SIP
coefficients from the header. They can be removed temporarily from a WCS object by

>>> wcsobj.sip = None

In addition, if SIP is the only distortion in the header, the two methods,
`~astropy.wcs.wcs.WCS.wcs_pix2world` and `~astropy.wcs.wcs.WCS.wcs_world2pix`,
may be used to transform from pixels to world coordinate system while omitting distortions.

Another consequence of the inconsistent header is that if
`~astropy.wcs.wcs.WCS.to_header()` is called with ``relax=True`` it will return a header
with SIP coefficients and a ``-SIP`` suffix in CTYPE and will not reproduce the original header.

**In conclusion, when astropy.wcs detects inconsistent headers, the recommendation
is that the header is inspected and corrected to match the data.**

Below is an example of a header with SIP coefficients when ``-SIP`` is missing from CTYPE.
The data is drizzled, i.e. distortion free, so the intention is **not** to include the
SIP distortion.

>>> wcsobj = wcs.WCS(header)

INFO::

        Inconsistent SIP distortion information is present in the FITS header and the WCS object:
        SIP coefficients were detected, but CTYPE is missing a "-SIP" suffix.
        astropy.wcs is using the SIP distortion coefficients,
        therefore the coordinates calculated here might be incorrect.

        If you do not want to apply the SIP distortion coefficients,
        please remove the SIP coefficients from the FITS header or the
        WCS object.  As an example, if the image is already distortion-corrected
        (e.g., drizzled) then distortion components should not apply and the SIP
        coefficients should be removed.

        While the SIP distortion coefficients are being applied here, if that was indeed the intent,
        for consistency please append "-SIP" to the CTYPE in the FITS header or the WCS object.


>>> hdr = wcsobj.to_header(relax=True)

INFO::

        Inconsistent SIP distortion information is present in the current WCS:
        SIP coefficients were detected, but CTYPE is missing "-SIP" suffix,
        therefore the current WCS is internally inconsistent.

        Because relax has been set to True, the resulting output WCS will have
        "-SIP" appended to CTYPE in order to make the header internally consistent.

        However, this may produce incorrect astrometry in the output WCS, if
        in fact the current WCS is already distortion-corrected.

        Therefore, if current WCS is already distortion-corrected (eg, drizzled)
        then SIP distortion components should not apply. In that case, for a WCS
        that is already distortion-corrected, please remove the SIP coefficients
        from the header.
.. _relax:

The ``relax`` keyword argument controls the handling of non-standard
FITS WCS keywords.

Note that the default value of ``relax`` is `True` for reading (to
accept all non standard keywords), and `False` for writing (to write
out only standard keywords), in accordance with `Postel's prescription
<http://catb.org/jargon/html/P/Postels-Prescription.html>`_:

    Be liberal in what you accept, and conservative in what you send.

.. _relaxread:

Header-reading relaxation constants
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

`~astropy.wcs.WCS`, `~astropy.wcs.Wcsprm` and
`~astropy.wcs.find_all_wcs` have a *relax* argument, which may be
either `True`, `False` or an `int`.

- If `True`, (default), all non-standard WCS extensions recognized by the parser
  will be handled.

- If `False`, none of the extensions (even those in the
  errata) will be handled.  Non-conformant keywords will be handled in
  the same way as non-WCS keywords in the header, i.e. by simply
  ignoring them.

- If an `int`, is is a bit field to provide fine-grained control over
  what non-standard WCS keywords to accept.  The flag bits are subject
  to change in future and should be set by using the constants
  beginning with ``WCSHDR_`` in the `astropy.wcs` module.

  For example, to accept ``CD00i00j`` and ``PC00i00j`` use::

      relax = astropy.wcs.WCSHDR_CD00i00j | astropy.wcs.WCSHDR_PC00i00j

  The parser always treats ``EPOCH`` as subordinate to ``EQUINOXa`` if
  both are present, and ``VSOURCEa`` is always subordinate to
  ``ZSOURCEa``.

  Likewise, ``VELREF`` is subordinate to the formalism of WCS Paper
  III.

The flag bits are:

- ``WCSHDR_none``: Don't accept any extensions (not even those in the
  errata).  Treat non-conformant keywords in the same way as non-WCS
  keywords in the header, i.e. simply ignore them.  (This is
  equivalent to passing `False`)

- ``WCSHDR_all``: Accept all extensions recognized by the parser.  (This
  is equivalent to the default behavior or passing `True`).

- ``WCSHDR_reject``: Reject non-standard keyrecords (that are not
  otherwise explicitly accepted by one of the flags below).  A warning
  will be displayed by default.

  This flag may be used to signal the presence of non-standard
  keywords, otherwise they are simply passed over as though they did
  not exist in the header.  It is mainly intended for testing
  conformance of a FITS header to the WCS standard.

  Keyrecords may be non-standard in several ways:

  - The keyword may be syntactically valid but with keyvalue of
    incorrect type or invalid syntax, or the keycomment may be
    malformed.

  - The keyword may strongly resemble a WCS keyword but not, in fact,
    be one because it does not conform to the standard.  For example,
    ``CRPIX01`` looks like a ``CRPIXja`` keyword, but in fact the
    leading zero on the axis number violates the basic FITS standard.
    Likewise, ``LONPOLE2`` is not a valid ``LONPOLEa`` keyword in the
    WCS standard, and indeed there is nothing the parser can sensibly
    do with it.

  - Use of the keyword may be deprecated by the standard.  Such will
    be rejected if not explicitly accepted via one of the flags below.

- ``WCSHDR_CROTAia``: Accept ``CROTAia``, ``iCROTna``, ``TCROTna``
- ``WCSHDR_EPOCHa``:  Accept ``EPOCHa``.
- ``WCSHDR_VELREFa``: Accept ``VELREFa``.

        The constructor always recognizes the AIPS-convention
        keywords, ``CROTAn``, ``EPOCH``, and ``VELREF`` for the
        primary representation ``(a = ' ')`` but alternates are
        non-standard.

        The constructor accepts ``EPOCHa`` and ``VELREFa`` only if
        ``WCSHDR_AUXIMG`` is also enabled.

- ``WCSHDR_CD00i00j``: Accept ``CD00i00j``.
- ``WCSHDR_PC00i00j``: Accept ``PC00i00j``.
- ``WCSHDR_PROJPn``: Accept ``PROJPn``.

        These appeared in early drafts of WCS Paper I+II (before they
        were split) and are equivalent to ``CDi_ja``, ``PCi_ja``, and
        ``PVi_ma`` for the primary representation ``(a = ' ')``.
        ``PROJPn`` is equivalent to ``PVi_ma`` with ``m`` = ``n`` <=
        9, and is associated exclusively with the latitude axis.


- ``WCSHDR_CD0i_0ja``: Accept ``CD0i_0ja`` (wcspih()).
- ``WCSHDR_PC0i_0ja``: Accept ``PC0i_0ja`` (wcspih()).
- ``WCSHDR_PV0i_0ma``: Accept ``PV0i_0ja`` (wcspih()).
- ``WCSHDR_PS0i_0ma``: Accept ``PS0i_0ja`` (wcspih()).

        Allow the numerical index to have a leading zero in doubly-
        parameterized keywords, for example, ``PC01_01``.  WCS Paper I
        (Sects 2.1.2 & 2.1.4) explicitly disallows leading zeroes.
        The FITS 3.0 standard document (Sect. 4.1.2.1) states that the
        index in singly-parameterized keywords (e.g. ``CTYPEia``) "shall
        not have leading zeroes", and later in Sect. 8.1 that "leading
        zeroes must not be used" on ``PVi_ma`` and ``PSi_ma``.  However, by an
        oversight, it is silent on ``PCi_ja`` and ``CDi_ja``.

        Only available if built with wcslib 5.0 or later.

- ``WCSHDR_RADECSYS``: Accept ``RADECSYS``.  This appeared in early
  drafts of WCS Paper I+II and was subsequently replaced by
  ``RADESYSa``.  The constructor accepts ``RADECSYS`` only if
  ``WCSHDR_AUXIMG`` is also enabled.

- ``WCSHDR_VSOURCE``: Accept ``VSOURCEa`` or ``VSOUna``.  This appeared
  in early drafts of WCS Paper III and was subsequently dropped in
  favor of ``ZSOURCEa`` and ``ZSOUna``.  The constructor accepts
  ``VSOURCEa`` only if ``WCSHDR_AUXIMG`` is also enabled.

- ``WCSHDR_DOBSn``: Allow ``DOBSn``, the column-specific analogue of
  ``DATE-OBS``.  By an oversight this was never formally defined in
  the standard.

- ``WCSHDR_LONGKEY``: Accept long forms of the alternate binary table
  and pixel list WCS keywords, i.e. with "a" non- blank.
  Specifically::

        jCRPXna  TCRPXna  :  jCRPXn  jCRPna  TCRPXn  TCRPna  CRPIXja
           -     TPCn_ka  :    -     ijPCna    -     TPn_ka  PCi_ja
           -     TCDn_ka  :    -     ijCDna    -     TCn_ka  CDi_ja
        iCDLTna  TCDLTna  :  iCDLTn  iCDEna  TCDLTn  TCDEna  CDELTia
        iCUNIna  TCUNIna  :  iCUNIn  iCUNna  TCUNIn  TCUNna  CUNITia
        iCTYPna  TCTYPna  :  iCTYPn  iCTYna  TCTYPn  TCTYna  CTYPEia
        iCRVLna  TCRVLna  :  iCRVLn  iCRVna  TCRVLn  TCRVna  CRVALia
        iPVn_ma  TPVn_ma  :    -     iVn_ma    -     TVn_ma  PVi_ma
        iPSn_ma  TPSn_ma  :    -     iSn_ma    -     TSn_ma  PSi_ma

  where the primary and standard alternate forms together with the
  image-header equivalent are shown rightwards of the colon.

  The long form of these keywords could be described as quasi-
  standard.  ``TPCn_ka``, ``iPVn_ma``, and ``TPVn_ma`` appeared by
  mistake in the examples in WCS Paper II and subsequently these and
  also ``TCDn_ka``, ``iPSn_ma`` and ``TPSn_ma`` were legitimized by
  the errata to the WCS papers.

  Strictly speaking, the other long forms are non-standard and in fact
  have never appeared in any draft of the WCS papers nor in the
  errata.  However, as natural extensions of the primary form they are
  unlikely to be written with any other intention.  Thus it should be
  safe to accept them provided, of course, that the resulting keyword
  does not exceed the 8-character limit.

  If ``WCSHDR_CNAMn`` is enabled then also accept::

        iCNAMna  TCNAMna  :   ---   iCNAna    ---   TCNAna  CNAMEia
        iCRDEna  TCRDEna  :   ---   iCRDna    ---   TCRDna  CRDERia
        iCSYEna  TCSYEna  :   ---   iCSYna    ---   TCSYna  CSYERia

  Note that ``CNAMEia``, ``CRDERia``, ``CSYERia``, and their variants
  are not used by `astropy.wcs` but are stored as auxiliary information.

- ``WCSHDR_CNAMn``: Accept ``iCNAMn``, ``iCRDEn``, ``iCSYEn``,
  ``TCNAMn``, ``TCRDEn``, and ``TCSYEn``, i.e. with ``a`` blank.
  While non-standard, these are the analogues of ``iCTYPn``,
  ``TCTYPn``, etc.

- ``WCSHDR_AUXIMG``: Allow the image-header form of an auxiliary WCS
  keyword with representation-wide scope to provide a default value
  for all images.  This default may be overridden by the
  column-specific form of the keyword.

  For example, a keyword like ``EQUINOXa`` would apply to all image
  arrays in a binary table, or all pixel list columns with alternate
  representation ``a`` unless overridden by ``EQUIna``.

  Specifically the keywords are::

        LATPOLEa  for LATPna
        LONPOLEa  for LONPna
        RESTFREQ  for RFRQna
        RESTFRQa  for RFRQna
        RESTWAVa  for RWAVna

  whose keyvalues are actually used by WCSLIB, and also keywords that
  provide auxiliary information that is simply stored in the wcsprm
  struct::

        EPOCH         -       ... (No column-specific form.)
        EPOCHa        -       ... Only if WCSHDR_EPOCHa is set.
        EQUINOXa  for EQUIna
        RADESYSa  for RADEna
        RADECSYS  for RADEna  ... Only if WCSHDR_RADECSYS is set.
        SPECSYSa  for SPECna
        SSYSOBSa  for SOBSna
        SSYSSRCa  for SSRCna
        VELOSYSa  for VSYSna
        VELANGLa  for VANGna
        VELREF        -       ... (No column-specific form.)
        VELREFa       -       ... Only if WCSHDR_VELREFa is set.
        VSOURCEa  for VSOUna  ... Only if WCSHDR_VSOURCE is set.
        WCSNAMEa  for WCSNna  ... Or TWCSna (see below).
        ZSOURCEa  for ZSOUna

        DATE-AVG  for DAVGn
        DATE-OBS  for DOBSn
        MJD-AVG   for MJDAn
        MJD-OBS   for MJDOBn
        OBSGEO-X  for OBSGXn
        OBSGEO-Y  for OBSGYn
        OBSGEO-Z  for OBSGZn

  where the image-header keywords on the left provide default values
  for the column specific keywords on the right.

  Keywords in the last group, such as ``MJD-OBS``, apply to all
  alternate representations, so ``MJD-OBS`` would provide a default
  value for all images in the header.

  This auxiliary inheritance mechanism applies to binary table image
  arrays and pixel lists alike.  Most of these keywords have no
  default value, the exceptions being ``LONPOLEa`` and ``LATPOLEa``,
  and also ``RADESYSa`` and ``EQUINOXa`` which provide defaults for
  each other.  Thus the only potential difficulty in using
  ``WCSHDR_AUXIMG`` is that of erroneously inheriting one of these four
  keywords.

  Unlike ``WCSHDR_ALLIMG``, the existence of one (or all) of these
  auxiliary WCS image header keywords will not by itself cause a
  `~astropy.wcs.Wcsprm` object to be created for alternate
  representation ``a``.  This is because they do not provide
  sufficient information to create a non-trivial coordinate
  representation when used in conjunction with the default values of
  those keywords, such as ``CTYPEia``, that are parameterized by axis
  number.

- ``WCSHDR_ALLIMG``: Allow the image-header form of *all* image header
  WCS keywords to provide a default value for all image arrays in a
  binary table (n.b. not pixel list).  This default may be overridden
  by the column-specific form of the keyword.

  For example, a keyword like ``CRPIXja`` would apply to all image
  arrays in a binary table with alternate representation ``a``
  unless overridden by ``jCRPna``.

  Specifically the keywords are those listed above for ``WCSHDR_AUXIMG``
  plus::

        WCSAXESa  for WCAXna

  which defines the coordinate dimensionality, and the following
  keywords which are parameterized by axis number::

        CRPIXja   for jCRPna
        PCi_ja    for ijPCna
        CDi_ja    for ijCDna
        CDELTia   for iCDEna
        CROTAi    for iCROTn
        CROTAia        -      ... Only if WCSHDR_CROTAia is set.
        CUNITia   for iCUNna
        CTYPEia   for iCTYna
        CRVALia   for iCRVna
        PVi_ma    for iVn_ma
        PSi_ma    for iSn_ma

        CNAMEia   for iCNAna
        CRDERia   for iCRDna
        CSYERia   for iCSYna

  where the image-header keywords on the left provide default values
  for the column specific keywords on the right.

  This full inheritance mechanism only applies to binary table image
  arrays, not pixel lists, because in the latter case there is no
  well-defined association between coordinate axis number and column
  number.

  Note that ``CNAMEia``, ``CRDERia``, ``CSYERia``, and their variants
  are not used by pywcs but are stored in the `~astropy.wcs.Wcsprm`
  object as auxiliary information.

  Note especially that at least one `~astropy.wcs.Wcsprm` object will
  be returned for each ``a`` found in one of the image header keywords
  listed above:

    - If the image header keywords for ``a`` **are not** inherited by
      a binary table, then the struct will not be associated with any
      particular table column number and it is up to the user to
      provide an association.

    - If the image header keywords for ``a`` **are** inherited by a
      binary table image array, then those keywords are considered to
      be "exhausted" and do not result in a separate
      `~astropy.wcs.Wcsprm` object.

.. _relaxwrite:

Header-writing relaxation constants
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

`~astropy.wcs.wcs.WCS.to_header` and `~astropy.wcs.wcs.WCS.to_header_string`
has a *relax* argument which may be either `True`, `False` or an
`int`.

- If `True`, write all recognized extensions.

- If `False` (default), write all extensions that are considered to be
  safe and recommended, equivalent to ``WCSHDO_safe`` (described below).

- If an `int`, is is a bit field to provide fine-grained control over
  what non-standard WCS keywords to accept.  The flag bits are subject
  to change in future and should be set by using the constants
  beginning with ``WCSHDO_`` in the `astropy.wcs` module.

The flag bits are:

- ``WCSHDO_none``: Don't use any extensions.

- ``WCSHDO_all``: Write all recognized extensions, equivalent to setting
  each flag bit.

- ``WCSHDO_safe``: Write all extensions that are considered to be safe
  and recommended.

- ``WCSHDO_DOBSn``: Write ``DOBSn``, the column-specific analogue of
  ``DATE-OBS`` for use in binary tables and pixel lists.  WCS Paper
  III introduced ``DATE-AVG`` and ``DAVGn`` but by an oversight
  ``DOBSn`` was never formally defined by the
  standard.  The alternative to using ``DOBSn`` is to write
  ``DATE-OBS`` which applies to the whole table.  This usage is
  considered to be safe and is recommended.

- ``WCSHDO_TPCn_ka``: WCS Paper I defined

  - ``TPn_ka`` and ``TCn_ka`` for pixel lists

    but WCS Paper II uses ``TPCn_ka`` in one example and subsequently
    the errata for the WCS papers legitimized the use of

  - ``TPCn_ka`` and ``TCDn_ka`` for pixel lists

    provided that the keyword does not exceed eight characters.  This
    usage is considered to be safe and is recommended because of the
    non-mnemonic terseness of the shorter forms.

- ``WCSHDO_PVn_ma``: WCS Paper I defined

  - ``iVn_ma`` and ``iSn_ma`` for bintables and
  - ``TVn_ma`` and ``TSn_ma`` for pixel lists

    but WCS Paper II uses ``iPVn_ma`` and ``TPVn_ma`` in the examples
    and subsequently the errata for the WCS papers legitimized the use
    of

  - ``iPVn_ma`` and ``iPSn_ma`` for bintables and
  - ``TPVn_ma`` and ``TPSn_ma`` for pixel lists

    provided that the keyword does not exceed eight characters.  This
    usage is considered to be safe and is recommended because of the
    non-mnemonic terseness of the shorter forms.

- ``WCSHDO_CRPXna``: For historical reasons WCS Paper I defined

  - ``jCRPXn``, ``iCDLTn``, ``iCUNIn``, ``iCTYPn``, and ``iCRVLn`` for
    bintables and
  - ``TCRPXn``, ``TCDLTn``, ``TCUNIn``, ``TCTYPn``, and ``TCRVLn`` for
    pixel lists

    for use without an alternate version specifier.  However, because
    of the eight-character keyword constraint, in order to accommodate
    column numbers greater than 99 WCS Paper I also defined

  - ``jCRPna``, ``iCDEna``, ``iCUNna``, ``iCTYna`` and ``iCRVna`` for
    bintables and
  - ``TCRPna``, ``TCDEna``, ``TCUNna``, ``TCTYna`` and ``TCRVna`` for
    pixel lists

    for use with an alternate version specifier (the ``a``).  Like the
    ``PC``, ``CD``, ``PV``, and ``PS`` keywords there is a
    tendency to confuse these two forms for column numbers up to 99.
    It is very unlikely that any parser would reject keywords in the
    first set with a non-blank alternate version specifier so this
    usage is considered to be safe and is recommended.

- ``WCSHDO_CNAMna``: WCS Papers I and III defined

  - ``iCNAna``,  ``iCRDna``,  and ``iCSYna``  for bintables and
  - ``TCNAna``,  ``TCRDna``,  and ``TCSYna``  for pixel lists

    By analogy with the above, the long forms would be

  - ``iCNAMna``, ``iCRDEna``, and ``iCSYEna`` for bintables and
  - ``TCNAMna``, ``TCRDEna``, and ``TCSYEna`` for pixel lists

    Note that these keywords provide auxiliary information only, none
    of them are needed to compute world coordinates.  This usage is
    potentially unsafe and is not recommended at this time.

- ``WCSHDO_WCSNna``: Write ``WCSNna`` instead of ``TWCSna`` for pixel
  lists.  While the constructor treats ``WCSNna`` and ``TWCSna`` as
  equivalent, other parsers may not.  Consequently, this usage is
  potentially unsafe and is not recommended at this time.

- ``WCSHDO_SIP``: Write out Simple Imaging Polynomial (SIP) keywords.

- ``WCSHDO_P12``, ``WCSHDO_P13``, ``WCSHDO_P14``, ``WCSHDO_P15``, ``WCSHDO_P16``, ``WCSHDO_P17``, ``WCSHDO_EFMT``

  These constants control the precision of the WCS keywords returned by `~astropy.wcs.WCS.to_header`.

  - ``WCSHDO_P12`` : Use "%20.12G" format for all floating-point keyvalues (12 significant digits)
  - ``WCSHDO_P13`` : Use "%21.13G" format for all floating-point keyvalues (13 significant digits)
  - ``WCSHDO_P14`` : Use "%22.14G" format for all floating-point keyvalues (14 significant digits)
  - ``WCSHDO_P15`` : Use "%23.15G" format for all floating-point keyvalues (15 significant digits)
  - ``WCSHDO_P16`` : Use "%24.16G" format for all floating-point keyvalues (16 significant digits)
  - ``WCSHDO_P17`` : Use "%25.17G" format for all floating-point keyvalues (17 significant digits)
  - ``WCSHDO_EFMT`` : Use "%E" format instead of the default "%G" format above
.. note that if this is changed from the default approach of using an *include*
   (in index.rst) to a separate performance page, the header needs to be changed
   from === to ***, the filename extension needs to be changed from .inc.rst to
   .rst, and a link needs to be added in the subpackage toctree

.. _astropy-wcs-performance:

.. Performance Tips
.. ================
..
.. Here we provide some tips and tricks for how to optimize performance of code
.. using `astropy.wcs`.
astropy.wcs History
*******************

`astropy.wcs` began life as ``pywcs``.  Earlier version numbers refer to
that package.

pywcs Version 1.11
==================

- Updated to wcslib version 4.8, which gives much more detailed error
  messages.

- Added functions get_pc() and get_cdelt().  These provide a way to
  always get the canonical representation of the linear transformation
  matrix, whether the header specified it in PC, CD or CROTA form.

- Long-running process will now release the Python GIL to better
  support Python multithreading.

- The dimensions of the `~astropy.wcs.Wcsprm.cd` and
  `~astropy.wcs.Wcsprm.pc` matrices were always returned as 2x2.  They
  now are sized according to naxis.

- Supports Python 3.x

- Builds on Microsoft Windows without severely patching wcslib.

- Lots of new unit tests

- ``pywcs`` will now run without ``pyfits``, though the SIP and distortion
  lookup table functionality is unavailable.

- Setting `~astropy.wcs.Wcsprm.cunit` will now verify that the values
  are valid unit strings.

pywcs Version 1.10
==================

- Adds a ``UnitConversion`` class, which gives access to wcslib's unit
  conversion functionality.  Given two convertible unit strings, pywcs
  can convert arrays of values from one to the other.

- Now uses wcslib 4.7

- Changes to some wcs values would not always calculate secondary values.

pywcs Version 1.9
=================

- Support binary image arrays and pixel list format WCS by presenting
  a way to call wcslib's ``wcsbth()``

- Updated underlying wcslib to version 4.5, which fixes the following:

    - Fixed the interpretation of VELREF when translating
      AIPS-convention spectral types.  Such translation is now handled
      by a new special- purpose function, spcaips().  The wcsprm
      struct has been augmented with an entry for velref which is
      filled by wcspih() and wcsbth().  Previously, selection by
      VELREF of the radio or optical velocity convention for type VELO
      was not properly handled.

Bugs
----

- The `~astropy.wcs.Wcsprm.pc` member is now available with a default
  raw `~astropy.wcs.Wcsprm` object.

- Make properties that return arrays read-only, since modifying a
  (mutable) array could result in secondary values not being
  recomputed based on those changes.

- `float` properties can now be set using `int` values

pywcs Version 1.3a1
===================

Earlier versions of pywcs had two versions of every conversion method::

  X(...)      -- treats the origin of pixel coordinates at (0, 0)
  X_fits(...) -- treats the origin of pixel coordinates at (1, 1)

From version 1.3 onwards, there is only one method for each
conversion, with an 'origin' argument:

  - 0: places the origin at (0, 0), which is the C/Numpy convention.

  - 1: places the origin at (1, 1), which is the Fortran/FITS
    convention.
.. include:: references.txt

.. supported_projections:

Supported projections
---------------------

As `astropy.wcs` is based on `wcslib`_, it supports the standard
projections defined in the `FITS WCS standard`_.  These projection
codes are three letter strings specified in the second part of the ``CTYPEn`` keywords
(accessible through `Wcsprm.ctype <astropy.wcs.Wcsprm.ctype>`). For
example, a tangent projection with RA, DEC coordinates is defined by
``CTYPE1 = RA---TAN`` and ``CTYPE2 = DEC--TAN``. If a SIP distortion is present the
keywords become ``CTYPE1 = RA---TAN-SIP`` and ``CTYPE2 = DEC--TAN-SIP``.

The supported projection codes are:

- ``AZP``: zenithal/azimuthal perspective
- ``SZP``: slant zenithal perspective
- ``TAN``: gnomonic
- ``STG``: stereographic
- ``SIN``: orthographic/synthesis
- ``ARC``: zenithal/azimuthal equidistant
- ``ZPN``: zenithal/azimuthal polynomial
- ``ZEA``: zenithal/azimuthal equal area
- ``AIR``: Airy's projection
- ``CYP``: cylindrical perspective
- ``CEA``: cylindrical equal area
- ``CAR``: plate carre
- ``MER``: Mercator's projection
- ``COP``: conic perspective
- ``COE``: conic equal area
- ``COD``: conic equidistant
- ``COO``: conic orthomorphic
- ``SFL``: Sanson-Flamsteed ("global sinusoid")
- ``PAR``: parabolic
- ``MOL``: Mollweide's projection
- ``AIT``: Hammer-Aitoff
- ``BON``: Bonne's projection
- ``PCO``: polyconic
- ``TSC``: tangential spherical cube
- ``CSC``: COBE quadrilateralized spherical cube
- ``QSC``: quadrilateralized spherical cube
- ``HPX``: HEALPix
- ``XPH``: HEALPix polar, aka "butterfly"

And, if built with wcslib 5.0 or later, the following polynomial
distortions are supported:

- ``TPV``: Polynomial distortion
- ``TUV``: Polynomial distortion

.. note::

    Though wcslib 5.4 and later handles ``SIP`` polynomial distortion,
    for backward compatibility, ``SIP`` is handled by astropy itself
    and methods exist to handle it specially.
Loading WCS Information from a FITS File
----------------------------------------

This example loads a FITS file (supplied on the command line) and uses
the FITS keywords in its primary header to create a WCS and transform.

.. literalinclude:: examples/from_file.py
   :language: python
.. _validation:

Validation and Bounds checking
******************************

Bounds checking is enabled by default, and any computed world
coordinates outside of [-180, 180] for longitude and [-90, 90] in
latitude are marked as invalid.  To disable this behavior, use
`astropy.wcs.Wcsprm.bounds_check`.
.. _example_create_imaging:

First Example
^^^^^^^^^^^^^

This example, rather than starting from a FITS header, sets WCS values
programmatically, uses those settings to transform some points, and then
saves those settings to a new FITS header.

.. literalinclude:: examples/programmatic.py
   :language: python

.. note::
    The members of the WCS object correspond roughly to the key/value
    pairs in the FITS header.  However, they are adjusted and
    normalized in a number of ways that make performing the WCS
    transformation easier.  Therefore, they can not be relied upon to
    get the original values in the header.  To build up a FITS header
    directly and specifically, use `astropy.io.fits.Header` directly.
Reference/API
=============

.. automodapi:: astropy.wcs
   :inherited-members:

.. automodapi:: astropy.wcs.utils
   :inherited-members:

.. automodapi:: astropy.wcs.wcsapi
   :inherited-members:
.. _wcstools:

Subsetting and Pixel Scales
^^^^^^^^^^^^^^^^^^^^^^^^^^^

WCS objects can be broken apart into their constituent axes using the
`~astropy.wcs.WCS.sub` function.  There is also a `~astropy.wcs.WCS.celestial`
convenience function that will return a WCS object with only the celestial axes
included.

The pixel scales of a celestial image or the pixel dimensions of a non-celestial
image can be extracted with the utility functions
`~astropy.wcs.utils.proj_plane_pixel_scales` and
`~astropy.wcs.utils.non_celestial_pixel_scales`. Likewise, celestial pixel
area can be extracted with the utility function
`~astropy.wcs.utils.proj_plane_pixel_area`.

Matplotlib plots with correct WCS projection
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The :ref:`WCSAxes <wcsaxes>` framework, previously a standalone package, allows
the :class:`~astropy.wcs.WCS` to be used to define projections in Matplotlib.
More information on using WCSAxes can be found :ref:`here <wcsaxes>`.

.. plot::
    :context: reset
    :include-source:
    :align: center

    import warnings
    from matplotlib import pyplot as plt
    from astropy.io import fits
    from astropy.wcs import WCS, FITSFixedWarning
    from astropy.utils.data import get_pkg_data_filename

    filename = get_pkg_data_filename('tutorials/FITS-images/HorseHead.fits')

    hdu = fits.open(filename)[0]
    with warnings.catch_warnings():
        # Ignore a warning on using DATE-OBS in place of MJD-OBS
        warnings.filterwarnings('ignore', message="'datfix' made the change",
                                category=FITSFixedWarning)
        wcs = WCS(hdu.header)

    fig = plt.figure()
    fig.add_subplot(111, projection=wcs)
    plt.imshow(hdu.data, origin='lower', cmap=plt.cm.viridis)
    plt.xlabel('RA')
    plt.ylabel('Dec')
.. _wcsapi:

Shared Python Interface for World Coordinate Systems
****************************************************

Background
^^^^^^^^^^

The :class:`~astropy.wcs.WCS` class implements what is considered the
most common 'standard' for representing world coordinate systems in
FITS files, but it cannot represent arbitrarily complex transformations
and there is no agreement on how to use the standard beyond FITS files.
Therefore, other world coordinate system transformation approaches exist,
such as the `gwcs <https://gwcs.readthedocs.io/>`_ package being developed
for the James Webb Space Telescope (which is also applicable to other data).

Since one of the goals of the Astropy Project is to improve interoperability
between packages, we have collaboratively defined a standardized application
programming interface (API) for world coordinate system objects to be used
in Python. This API is described in the Astropy Proposal for Enhancements (APE) 14:
`A shared Python interface for World Coordinate Systems
<https://doi.org/10.5281/zenodo.1188874>`_.

The core astropy package provides base classes that define the low- and high-
level APIs described in APE 14 in the :mod:`astropy.wcs.wcsapi` module, and
these are listed in the :ref:`wcs-reference-api` section below.

Overview
^^^^^^^^

While the full  details and motivation for the API are detailed in APE 14,  this
documentation summarizes the elements that are implemented directly in the
astropy core package.  The high-level interface is likely of most interest to
the average user.  In particular, the most important methods are the
:meth:`~astropy.wcs.wcsapi.BaseHighLevelWCS.pixel_to_world` and
:meth:`~astropy.wcs.wcsapi.BaseHighLevelWCS.world_to_pixel` methods. These
provide the essential elements of WCS: mapping to and from world coordinates.
The remainder generally provide information about the *kind* of world
coordinates or similar information about the structure of the WCS.

In a bit more detail, the key classes implemented here are a high-level that
provides the main user interface (:class:`~astropy.wcs.wcsapi.BaseHighLevelWCS` and
subclasses), and a lower-level interface (:class:`~astropy.wcs.wcsapi.BaseLowLevelWCS`
and subclasses).  These can be distinct objects *or* the same one.  For
FITS-WCS, the `~astropy.wcs.WCS` object meant for FITS-WCS follows both
interfaces, allowing immediate use of this API with files that already contain
FITS-WCS. More concrete examples are outlined below.

Basic usage
^^^^^^^^^^^

Let's start off by looking at the shared Python interface for WCS by using a
simple image with two celestial axes (Right Ascension and Declination)::

    >>> from astropy.wcs import WCS
    >>> from astropy.utils.data import get_pkg_data_filename
    >>> from astropy.io import fits
    >>> filename = get_pkg_data_filename('galactic_center/gc_2mass_k.fits')  # doctest: +REMOTE_DATA
    >>> hdulist = fits.open(filename)  # doctest: +REMOTE_DATA
    >>> hdu = hdulist[0]  # doctest: +REMOTE_DATA
    >>> wcs = WCS(hdu.header)  # doctest: +REMOTE_DATA
    >>> wcs  # doctest: +REMOTE_DATA
    WCS Keywords
    Number of WCS axes: 2
    CTYPE : 'RA---TAN'  'DEC--TAN'
    CRVAL : 266.4  -28.93333
    CRPIX : 361.0  360.5
    NAXIS : 721  720

We can check how many pixel and world axes are in the transformation as well
as the shape of the data the WCS applies to::

    >>> wcs.pixel_n_dim  # doctest: +REMOTE_DATA
    2
    >>> wcs.world_n_dim  # doctest: +REMOTE_DATA
    2
    >>> wcs.array_shape  # doctest: +REMOTE_DATA
    (720, 721)

Note that the array shape should match that of the data::

    >>> hdu.data.shape  # doctest: +REMOTE_DATA
    (720, 721)

As mentioned in :ref:`pixel_conventions`, what would normally be
considered the 'y-axis' of the image (when looking at it visually) is the first
dimension, while the 'x-axis' of the image is the second dimension. Thus
:attr:`~astropy.wcs.WCS.array_shape` returns the shape in the *opposite* order
to the NAXIS keywords in the FITS header (in the case of FITS-WCS). If you are
interested in the data shape in the reverse order (which would match the NAXIS
order in the case of FITS-WCS), then you can use
:attr:`~astropy.wcs.WCS.pixel_shape`::

    >>> wcs.pixel_shape  # doctest: +REMOTE_DATA
    (721, 720)

Let's now check what the physical type of each axis is::

    >>> wcs.world_axis_physical_types  # doctest: +REMOTE_DATA
    ['pos.eq.ra', 'pos.eq.dec']

This is indeed an image with two celestial axes.

The main part of the new interface defines standard methods for transforming
coordinates. The most convenient way is to use the high-level methods
:meth:`~astropy.wcs.wcsapi.BaseHighLevelWCS.pixel_to_world` and
:meth:`~astropy.wcs.wcsapi.BaseHighLevelWCS.world_to_pixel`, which can
transform directly to astropy objects::

    >>> coord = wcs.pixel_to_world([1, 2], [4, 3])  # doctest: +REMOTE_DATA
    >>> coord  # doctest: +REMOTE_DATA
    <SkyCoord (FK5: equinox=2000.0): (ra, dec) in deg
        [(266.97242993, -29.42584415), (266.97084321, -29.42723968)]>

Similarly, we can transform astropy objects back - we can test this by creating
Galactic coordinates and these will automatically be converted::

    >>> from astropy.coordinates import SkyCoord
    >>> coord = SkyCoord('00h00m00s +00d00m00s', frame='galactic')
    >>> pixels = wcs.world_to_pixel(coord)  # doctest: +REMOTE_DATA
    >>> pixels  # doctest: +REMOTE_DATA
    (array(356.85179997), array(357.45340331))

If you are looking to index the original data using these pixel coordinates,
be sure to instead use
:meth:`~astropy.wcs.wcsapi.BaseHighLevelWCS.world_to_array_index` which returns
the coordinates in the correct order to index Numpy arrays, and also rounds to
the nearest integer values::

    >>> index = wcs.world_to_array_index(coord)  # doctest: +REMOTE_DATA
    >>> index  # doctest: +REMOTE_DATA
    (357, 357)
    >>> hdu.data[index]  # doctest: +REMOTE_DATA +FLOAT_CMP
    563.7532
    >>> hdulist.close()  # doctest: +REMOTE_DATA

Advanced usage
^^^^^^^^^^^^^^

Let's now take a look at a WCS for a spectral cube (two celestial axes and one
spectral axis)::

    >>> filename = get_pkg_data_filename('l1448/l1448_13co.fits')  # doctest: +REMOTE_DATA
    >>> hdulist = fits.open(filename)  # doctest: +REMOTE_DATA
    >>> hdu = hdulist[0]  # doctest: +REMOTE_DATA
    >>> wcs = WCS(hdu.header)  # doctest: +REMOTE_DATA
    >>> wcs  # doctest: +REMOTE_DATA
    WCS Keywords
    Number of WCS axes: 3
    CTYPE : 'RA---SFL'  'DEC--SFL'  'VOPT'
    CRVAL : 57.6599999999  0.0  -9959.44378305
    CRPIX : -799.0  -4741.913  -187.0
    PC1_1 PC1_2 PC1_3  : 1.0  0.0  0.0
    PC2_1 PC2_2 PC2_3  : 0.0  1.0  0.0
    PC3_1 PC3_2 PC3_3  : 0.0  0.0  1.0
    CDELT : -0.006388889  0.006388889  66.42361
    NAXIS : 105  105  53

As before we can check how many pixel and world axes are in the transformation
as well as the shape of the data the WCS applies to, as well as the physical
types of each axis::

    >>> wcs.pixel_n_dim  # doctest: +REMOTE_DATA
    3
    >>> wcs.world_n_dim  # doctest: +REMOTE_DATA
    3
    >>> wcs.array_shape  # doctest: +REMOTE_DATA
    (53, 105, 105)
    >>> wcs.world_axis_physical_types  # doctest: +REMOTE_DATA
    ['pos.eq.ra', 'pos.eq.dec', 'spect.dopplerVeloc.opt']

This is indeed a spectral cube, with RA/Dec and a velocity axis.

As before, we can convert between pixels and high-level Astropy objects::

    >>> celestial, spectral = wcs.pixel_to_world([1, 2], [4, 3], [2, 3])  # doctest: +REMOTE_DATA
    >>> celestial  # doctest: +REMOTE_DATA
    <SkyCoord (ICRS): (ra, dec) in deg
        [(51.73115731, 30.32750025), (51.72414268, 30.32111136)]>
    >>> spectral  # doctest: +REMOTE_DATA
    <SpectralCoord
       (target: <ICRS Coordinate: (ra, dec, distance) in (deg, deg, kpc)
                    (57.66, 0., 1000.)
                 (pm_ra_cosdec, pm_dec, radial_velocity) in (mas / yr, mas / yr, km / s)
                    (0., 0., 0.)>)
      [2661.04211695, 2727.46572695] m / s>

and back::

    >>> from astropy import units as u
    >>> coord = SkyCoord('03h26m36.4901s +30d45m22.2012s')
    >>> pixels = wcs.world_to_pixel(coord, 3000 * u.m / u.s)  # doctest: +REMOTE_DATA +IGNORE_WARNINGS
    >>> pixels  # doctest: +REMOTE_DATA
    (array(8.11341207), array(71.0956641), array(7.10297292))

And as before we can index array values using::

    >>> index = wcs.world_to_array_index(coord, 3000 * u.m / u.s)  # doctest: +REMOTE_DATA +IGNORE_WARNINGS
    >>> index  # doctest: +REMOTE_DATA
    (7, 71, 8)
    >>> hdu.data[index]  # doctest: +REMOTE_DATA +FLOAT_CMP
    0.22262384
    >>> hdulist.close()  # doctest: +REMOTE_DATA

If you are interested in converting to/from world values as simple Python scalars
or Numpy arrays without using high-level astropy objects, there are methods
such as :meth:`~astropy.wcs.wcsapi.BaseLowLevelWCS.pixel_to_world_values` to
do this - see :ref:`wcs-reference-api` section for more details.

Extending the physical types in FITS-WCS
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

As shown above, the :attr:`~astropy.wcs.WCS.world_axis_physical_types` property
returns the list of physical types for each axis. For FITS-WCS, this is
determined from the CTYPE values in the header. In cases where the physical
type is not known, `None` is returned. However, it is possible to override the
physical types returned by using the
:class:`~astropy.wcs.wcsapi.fitswcs.custom_ctype_to_ucd_mapping` context
manager. Consider a WCS with the following CTYPE::

    >>> from astropy.wcs import WCS
    >>> wcs = WCS(naxis=1)
    >>> wcs.wcs.ctype = ['SPAM']
    >>> wcs.world_axis_physical_types
    [None]

We can specify that for this CTYPE, the physical type should be
``'food.spam'``::

    >>> from astropy.wcs.wcsapi.fitswcs import custom_ctype_to_ucd_mapping
    >>> with custom_ctype_to_ucd_mapping({'SPAM': 'food.spam'}):
    ...     wcs.world_axis_physical_types
    ['food.spam']

Slicing of WCS objects
^^^^^^^^^^^^^^^^^^^^^^

A common operation when dealing with data with WCS information attached is to
slice the WCS - this can be either to extract the WCS for a sub-region of the
data, preserving the overall number of dimensions (e.g. a cutout from an image)
or it can be reducing the dimensionality of the data and associated WCS (e.g.
extracting a slice from a spectral cube).

The :class:`~astropy.wcs.wcsapi.SlicedLowLevelWCS` class can be used to slice
any WCS object that conforms to the :class:`~astropy.wcs.wcsapi.BaseLowLevelWCS`
API. To demonstrate this, let's start off by reading in a spectral cube file::

    >>> filename = get_pkg_data_filename('l1448/l1448_13co.fits')  # doctest: +REMOTE_DATA
    >>> wcs = WCS(fits.getheader(filename, ext=0))  # doctest: +REMOTE_DATA

The ``wcs`` object is an instance of :class:`~astropy.wcs.WCS` which conforms to the
:class:`~astropy.wcs.wcsapi.BaseLowLevelWCS` API. We can then use the
:class:`~astropy.wcs.wcsapi.SlicedLowLevelWCS` class to slice the cube::

    >>> from astropy.wcs.wcsapi import SlicedLowLevelWCS
    >>> slices = [10, slice(30, 100), slice(30, 100)]  # doctest: +REMOTE_DATA
    >>> subwcs = SlicedLowLevelWCS(wcs, slices=slices)  # doctest: +REMOTE_DATA

The ``slices`` argument takes any combination of slices, integer values, and
ellipsis which would normally slice a Numpy array. In the above case, we are
extracting a spectral slice, and in that slice we are extracting a sub-region
on the sky.

If you are implementing your own WCS class, you could choose to implement
``__getitem__`` and have it internally use
:class:`~astropy.wcs.wcsapi.SlicedLowLevelWCS`. In fact, the
:class:`~astropy.wcs.WCS` class does this - the example above can be written
more succinctly as::

    >>> wcs[10, 30:100, 30:100]  # doctest: +REMOTE_DATA +ELLIPSIS
    <...>
    SlicedFITSWCS Transformation
    <BLANKLINE>
    This transformation has 2 pixel and 2 world dimensions
    <BLANKLINE>
    Array shape (Numpy order): (70, 70)
    <BLANKLINE>
    Pixel Dim  Axis Name  Data size  Bounds
            0  None              70  None
            1  None              70  None
    <BLANKLINE>
    World Dim  Axis Name  Physical Type  Units
            0  None       pos.eq.ra      deg
            1  None       pos.eq.dec     deg
    <BLANKLINE>
    Correlation between pixel and world axes:
    <BLANKLINE>
               Pixel Dim
    World Dim    0    1
            0  yes  yes
            1  yes  yes

This slicing infrastructure is able to deal with slicing of WCS objects which
have correlated axes - in this case, you may end up with a WCS that has a
different number of pixel and world coordinates. For example, if we slice
a spectral cube to extract a 1D dataset corresponding to a row in the
image plane of a spectral slice, the final WCS will have one pixel dimension
and two world dimensions (since both RA/Dec vary over the extracted 1D slice)::

    >>> wcs[10, 40, :]  # doctest: +REMOTE_DATA +ELLIPSIS
    <...>
    SlicedFITSWCS Transformation
    <BLANKLINE>
    This transformation has 1 pixel and 2 world dimensions
    <BLANKLINE>
    Array shape (Numpy order): (105,)
    <BLANKLINE>
    Pixel Dim  Axis Name  Data size  Bounds
            0  None             105  None
    <BLANKLINE>
    World Dim  Axis Name  Physical Type  Units
            0  None       pos.eq.ra      deg
            1  None       pos.eq.dec     deg
    <BLANKLINE>
    Correlation between pixel and world axes:
    <BLANKLINE>
               Pixel Dim
    World Dim    0
            0  yes
            1  yes
.. _example_cube_wcs:

Second Example
^^^^^^^^^^^^^^

Another way of creating a WCS object is via the use of a Python
dictionary. This affords us more control over the ``NAXISn``
FITS header keyword which is otherwise automatically default to zero
as in the case of the First Example shown above.

.. literalinclude:: examples/cube_wcs.py
   :language: python
.. include:: references.txt
.. _astropy-wcs:

***************************************
World Coordinate System (`astropy.wcs`)
***************************************

Introduction
============

World Coordinate Systems (WCSs) describe the geometric transformations
between one set of coordinates and another. A common application is to
map the pixels in an image onto the celestial sphere. Another common
application is to map pixels to wavelength in a spectrum.

`astropy.wcs` contains utilities for managing World Coordinate System
(WCS) transformations defined in several elaborate `FITS WCS standard`_ conventions.
These transformations work both forward (from pixel to world) and backward
(from world to pixel).

For historical reasons and to support legacy software, `astropy.wcs` maintains
two separate application interfaces. The ``High-Level API`` should be used by
most applications. It abstracts out the underlying object and works transparently
with other packages which support the
`Common Python Interface for WCS <https://zenodo.org/record/1188875#.XnpOtJNKjyI>`_,
allowing for a more flexible approach to the problem and avoiding the `limitations
of the FITS WCS standard <https://ui.adsabs.harvard.edu/abs/2015A%26C....12..133T/abstract>`_.

The ``Low Level API`` is the original `astropy.wcs` API and originally developed as ``pywcs``.
It ties applications to the `astropy.wcs` package and limits the transformations to the three distinct
types supported by it:

- Core WCS, as defined in the `FITS WCS standard`_, based on Mark
  Calabretta's `wcslib`_.  (Also includes ``TPV`` and ``TPD``
  distortion, but not ``SIP``).

- Simple Imaging Polynomial (`SIP`_) convention. (See :doc:`note about SIP in headers <note_sip>`.)

- Table lookup distortions as defined in the FITS WCS `distortion
  paper`_.

.. _pixel_conventions:

Pixel Conventions and Definitions
---------------------------------

Both APIs assume that integer pixel values fall at the center of pixels (as assumed in
the `FITS WCS standard`_, see Section 2.1.4 of `Greisen et al., 2002,
A&A 446, 747 <https://doi.org/10.1051/0004-6361:20053818>`_).

However, theres a difference in what is considered to be the first pixel. The
``High Level API`` follows the Python and C convention that the first pixel is
the 0-th one, i.e. the first pixel spans pixel values -0.5 to + 0.5. The
``Low Level API`` takes an additional ``origin`` argument with values of 0 or 1
indicating whether the input arrays are 0- or 1-based.
The Low-level interface assumes Cartesian order (x, y) of the input coordinates,
however the Common Interface for World Coordinate System accepts both conventions.
The order of the pixel coordinates ((x, y) vs (row, column)) in the Common API
depends on the method or property used, and this can normally be determined from
the property or method name. Properties and methods containing pixel assume (x, y)
ordering, while properties and methods containing array assume (row, column) ordering.

A Simple Example
================

One example of the use of the high-level WCS API is to use the
`~astropy.wcs.wcs.WCS.pixel_to_world` to yield the simplest WCS
with default values, converting from pixel to world coordinates::

    >>> from astropy.io import fits
    >>> from astropy.wcs import WCS
    >>> from astropy.utils.data import get_pkg_data_filename
    >>> fn = get_pkg_data_filename('data/j94f05bgq_flt.fits', package='astropy.wcs.tests')
    >>> f = fits.open(fn)
    >>> w = WCS(f[1].header)
    >>> sky = w.pixel_to_world(30, 40)
    >>> print(sky)  # doctest: +FLOAT_CMP
    <SkyCoord (ICRS): (ra, dec) in deg
        (5.52844243, -72.05207809)>
    >>> f.close()

Similarly, another use of the high-level API is to use the
`~astropy.wcs.wcs.WCS.world_to_pixel` to yield another simple WCS, while
converting from world to pixel coordinates::

    >>> from astropy.io import fits
    >>> from astropy.wcs import WCS
    >>> from astropy.utils.data import get_pkg_data_filename
    >>> fn = get_pkg_data_filename('data/j94f05bgq_flt.fits', package='astropy.wcs.tests')
    >>> f = fits.open(fn)
    >>> w = WCS(f[1].header)
    >>> x, y = w.world_to_pixel(sky)
    >>> print(x, y)  # doctest: +FLOAT_CMP
    30.00000214673885 39.999999958235094
    >>> f.close()

Using `astropy.wcs`
===================

.. toctree::
   :maxdepth: 2

   Shared Python Interface for World Coordinate Systems <wcsapi.rst>
   Legacy Interface <legacy_interface.rst>
   Supported Projections <supported_projections>

Examples creating a WCS programmatically
========================================

.. toctree::
   :maxdepth: 2

   Example of Imaging WCS <example_create_imaging.rst>
   Example of Cube WCS <example_cube_wcs.rst>
   Loading From a FITS File <loading_from_fits.rst>

.. _wcslint:



WCS Tools
=========

.. toctree::
   :maxdepth: 1

   wcstools.rst

Relax Constants
===============

.. toctree::
   :maxdepth: 1

   relax

Other Information
=================

.. toctree::
   :maxdepth: 1

   history
   validation

.. note that if this section gets too long, it should be moved to a separate
   doc page - see the top of performance.inc.rst for the instructions on how to do
   that
.. include:: performance.inc.rst


Reference/API
=============

.. toctree::
   :maxdepth: 1

   reference_api

See Also
========

- `wcslib`_

.. _wcs-reference-api:


Acknowledgments and Licenses
============================

`wcslib`_ is licenced under the `GNU Lesser General Public License
<http://www.gnu.org/licenses/lgpl.html>`_.
:orphan:

.. _wcslib: https://www.atnf.csiro.au/people/mcalabre/WCS/wcslib/index.html
.. _distortion paper: https://www.atnf.csiro.au/people/mcalabre/WCS/dcs_20040422.pdf
.. _SIP: https://irsa.ipac.caltech.edu/data/SPITZER/docs/files/spitzer/shupeADASS.pdf
.. _ds9: http://hea-www.harvard.edu/RD/ds9/
.. _FITS WCS standard: http://fits.gsfc.nasa.gov/fits_wcs.html
.. _paper_I: https://arxiv.org/pdf/astro-ph/0207407.pdf
.. _paper_II: https://arxiv.org/pdf/astro-ph/0207413.pdf
.. _paper_III: https://arxiv.org/pdf/astro-ph/0507293.pdf
.. _paper_IV: https://arxiv.org/pdf/1409.7583.pdf
.. include:: references.txt
.. _legacy_interface:

Legacy Interface
****************

astropy.wcs API
^^^^^^^^^^^^^^^

The ``Low Level API`` or ``Legacy Interface`` is the original `astropy.wcs` API.
It supports three types of transforms:

- Core WCS, as defined in the `FITS WCS standard`_, based on Mark
  Calabretta's `wcslib`_.  (Also includes ``TPV`` and ``TPD``
  distortion, but not ``SIP``).

- Simple Imaging Polynomial (`SIP`_) convention. (See :doc:`note about SIP in headers <note_sip>`.)

- Table lookup distortions as defined in the FITS WCS `distortion paper`_.

Each of these transformations can be used independently or together in a standard pipeline.
All methods support scalar and array inputs. Note, that all methods require an additional
positional argument which is the ``origin`` of the inputs. It has two possible values - ``0`` -
for zero-based coordinates like numpy arrays or ``1`` - for 1-based coordinates, like
the FITS standard, or those coming from ds9.

The basic workflow is to create a WCS object calling the WCS constructor with an
`~astropy.io.fits.Header` and/or `~astropy.io.fits.HDUList` object and calling
one of the methods below::

    >>> from astropy import wcs
    >>> from astropy.io import fits
    >>> from astropy.utils.data import get_pkg_data_filename
    >>> fn = get_pkg_data_filename('data/j94f05bgq_flt.fits', package='astropy.wcs.tests')
    >>> f = fits.open(fn)
    >>> wcsobj = wcs.WCS(f[1].header)
    >>> f.close()

Optionally, if the FITS file uses any deprecated or non-standard features, you may need
to call one of the `~astropy.wcs.wcs.WCS.fix` methods on the object.

Use one of the following transformation methods.

1. Between pixels and world coordinates using all distortions:

  - `~astropy.wcs.wcs.WCS.all_pix2world`: Perform all three
    transformations in series (core WCS, SIP and table lookup
    distortions) from pixel to world coordinates.  Use this one
    if you're not sure which to use.

    >>> lon, lat = wcsobj.all_pix2world(30, 40, 0)
    >>> print(lon, lat)  # doctest: +FLOAT_CMP
    5.528442425094046 -72.05207808966726

  - `~astropy.wcs.wcs.WCS.all_world2pix`: Perform all three
     transformations (core WCS, SIP and table lookup
     distortions) from world to pixel coordinates, using an
     iterative method if necessary.

     >>> x, y = wcsobj.all_world2pix(lon, lat, 0)
     >>> print(x, y) # # doctest: +FLOAT_CMP
     30.00000214673885 39.999999958235094

 2. Performing `SIP`_ transformations only:

     - `~astropy.wcs.wcs.WCS.sip_pix2foc`: Convert from pixel to
        focal plane coordinates using the `SIP`_ polynomial
        coefficients.

        >>> xsip, ysip = wcsobj.sip_pix2foc(30, 40, 0)
        >>> print(xsip, ysip)  # doctest: +FLOAT_CMP
        -1985.8600487630586 -984.4223711273145

     - `~astropy.wcs.wcs.WCS.sip_foc2pix`: Convert from focal
        plane to pixel coordinates using the `SIP`_ polynomial
        coefficients. Note that this method only works if the
        inverse SIP distortion is specified in the header.

 3. Performing `distortion paper`_ transformations only:

     - `~astropy.wcs.wcs.WCS.p4_pix2foc`: Convert from pixel to
        focal plane coordinates using the table lookup distortion
        method described in the FITS WCS `distortion paper`_.

     - `~astropy.wcs.wcs.WCS.det2im`: Convert from detector
        coordinates to image coordinates.  Commonly used for narrow
        column correction.

Core wcslib API
^^^^^^^^^^^^^^^

The core wcslib API supports the FITS WCS standard defined in WCS
papers, I, II, III, IV. Note that distortions are not applied if
the functions in the core library are used.

1. From pixels to world coordinates:

    - `~astropy.wcs.wcs.WCS.wcs_pix2world`: Perform just the core WCS
       transformation from pixel to world coordinates.

        >>> lon, lat = wcsobj.wcs_pix2world(30, 40, 0)
        >>> print(lon, lat)  # doctest: +FLOAT_CMP
        5.527103615238458 -72.0522441352217

2. From world to pixel coordinates:

    - `~astropy.wcs.wcs.WCS.wcs_world2pix`: Perform the core WCS transformation
       from world to pixel coordinates.

        >>> x, y = wcsobj.wcs_world2pix(lon, lat, 0)
        >>> print(x, y)  # doctest: +FLOAT_CMP
        30.000000000223267 40.0000000003696
.. note that if this is changed from the default approach of using an *include*
   (in index.rst) to a separate performance page, the header needs to be changed
   from === to ***, the filename extension needs to be changed from .inc.rst to
   .rst, and a link needs to be added in the subpackage toctree

.. _astropy-time-performance:

.. Performance Tips
.. ================
..
.. Here we provide some tips and tricks for how to optimize performance of code
.. using `astropy.time`.
.. _astropy-time:

*******************************
Time and Dates (`astropy.time`)
*******************************

Introduction
============

The `astropy.time` package provides functionality for manipulating times and
dates. Specific emphasis is placed on supporting time scales (e.g., UTC, TAI,
UT1, TDB) and time representations (e.g., JD, MJD, ISO 8601) that are used in
astronomy and required to calculate, for example, sidereal times and barycentric
corrections. The `astropy.time` package is based on fast and memory efficient
PyERFA_ wrappers around the ERFA_ time and calendar routines.

All time manipulations and arithmetic operations are done internally using two
64-bit floats to represent time. Floating point algorithms from [#]_ are used so
that the |Time| object maintains sub-nanosecond precision over times spanning
the age of the universe.

.. [#] Shewchuk, 1997, Discrete & Computational Geometry 18(3):305-363

Getting Started
===============

The usual way to use `astropy.time` is to create a |Time| object by
supplying one or more input time values as well as the `time format`_ and `time
scale`_ of those values. The input time(s) can either be a single scalar like
``"2010-01-01 00:00:00"`` or a list or a ``numpy`` array of values as shown
below. In general, any output values have the same shape (scalar or array) as
the input.

Examples
--------

.. EXAMPLE START: Creating a Time Object with astropy.time

To create a |Time| object:

  >>> import numpy as np
  >>> from astropy.time import Time
  >>> times = ['1999-01-01T00:00:00.123456789', '2010-01-01T00:00:00']
  >>> t = Time(times, format='isot', scale='utc')
  >>> t
  <Time object: scale='utc' format='isot' value=['1999-01-01T00:00:00.123' '2010-01-01T00:00:00.000']>
  >>> t[1]
  <Time object: scale='utc' format='isot' value=2010-01-01T00:00:00.000>

The ``format`` argument specifies how to interpret the input values (e.g., ISO,
JD, or Unix time). The ``scale`` argument specifies the `time scale`_ for the
values (e.g., UTC, TT, or UT1). The ``scale`` argument is optional and defaults
to UTC except for `Time from Epoch Formats`_.

.. EXAMPLE END

We could have written the above as::

  >>> t = Time(times, format='isot')

When the format of the input can be unambiguously determined, the
``format`` argument is not required, so we can then simplify even further::

  >>> t = Time(times)

Now we can get the representation of these times in the JD and MJD
formats by requesting the corresponding |Time| attributes::

  >>> t.jd  # doctest: +FLOAT_CMP
  array([2451179.50000143, 2455197.5       ])
  >>> t.mjd  # doctest: +FLOAT_CMP
  array([51179.00000143, 55197.        ])

The full power of output representation is available via the
`~astropy.time.Time.to_value` method which also allows controlling the
`subformat`_. For instance, using ``numpy.longdouble`` as the output type
for higher precision::

  >>> t.to_value('mjd', 'long')  # doctest: +SKIP
  array([51179.00000143, 55197.        ], dtype=float128)

The default representation can be changed by setting the ``format`` attribute::

  >>> t.format = 'fits'
  >>> t
  <Time object: scale='utc' format='fits' value=['1999-01-01T00:00:00.123'
                                                 '2010-01-01T00:00:00.000']>
  >>> t.format = 'isot'

We can also convert to a different time scale, for instance from UTC to
TT. This uses the same attribute mechanism as above but now returns a new
|Time| object::

  >>> t2 = t.tt
  >>> t2
  <Time object: scale='tt' format='isot' value=['1999-01-01T00:01:04.307' '2010-01-01T00:01:06.184']>
  >>> t2.jd  # doctest: +FLOAT_CMP
  array([2451179.5007443 , 2455197.50076602])

Note that both the ISO (ISOT) and JD representations of ``t2`` are different
than for ``t`` because they are expressed relative to the TT time scale. Of
course, from the numbers or strings you would not be able to tell this was the
case::

  >>> print(t2.fits)
  ['1999-01-01T00:01:04.307' '2010-01-01T00:01:06.184']

You can set the time values in place using the usual ``numpy`` array setting
item syntax::

  >>> t2 = t.tt.copy()  # Copy required if transformed Time will be modified
  >>> t2[1] = '2014-12-25'
  >>> print(t2)
  ['1999-01-01T00:01:04.307' '2014-12-25T00:00:00.000']

The |Time| object also has support for missing values, which is particularly
useful for :ref:`table_operations` such as joining and stacking::

  >>> t2[0] = np.ma.masked  # Declare that first time is missing or invalid
  >>> print(t2)
  [-- '2014-12-25T00:00:00.000']

Finally, some further examples of what is possible. For details, see
the API documentation below.

  >>> dt = t[1] - t[0]
  >>> dt  # doctest: +FLOAT_CMP
  <TimeDelta object: scale='tai' format='jd' value=4018.00002172>

Here, note the conversion of the timescale to TAI. Time differences
can only have scales in which one day is always equal to 86400 seconds.

  >>> import numpy as np
  >>> t[0] + dt * np.linspace(0.,1.,12)
  <Time object: scale='utc' format='isot' value=['1999-01-01T00:00:00.123' '2000-01-01T06:32:43.930'
   '2000-12-31T13:05:27.737' '2001-12-31T19:38:11.544'
   '2003-01-01T02:10:55.351' '2004-01-01T08:43:39.158'
   '2004-12-31T15:16:22.965' '2005-12-31T21:49:06.772'
   '2007-01-01T04:21:49.579' '2008-01-01T10:54:33.386'
   '2008-12-31T17:27:17.193' '2010-01-01T00:00:00.000']>

  >>> t.sidereal_time('apparent', 'greenwich')  # doctest: +FLOAT_CMP +REMOTE_DATA
  <Longitude [6.68050179, 6.70281947] hourangle>

Using `astropy.time`
====================

Time Object Basics
------------------

In `astropy.time` a "time" is a single instant of time which is
independent of the way the time is represented (the "format") and the time
"scale" which specifies the offset and scaling relation of the unit of time.
There is no distinction made between a "date" and a "time" since both concepts
(as loosely defined in common usage) are just different representations of a
moment in time.

.. _time-format:

Time Format
^^^^^^^^^^^

The time format specifies how an instant of time is represented. The currently
available formats are can be found in the ``Time.FORMATS`` dict and are listed
in the table below. Each of these formats is implemented as a class that derives
from the base :class:`~astropy.time.TimeFormat` class. This class structure can
be adapted and extended by users for specialized time formats not supplied in
`astropy.time`.

===========  =================================================  =====================================
Format            Class                                         Example Argument
===========  =================================================  =====================================
byear        :class:`~astropy.time.TimeBesselianEpoch`          1950.0
byear_str    :class:`~astropy.time.TimeBesselianEpochString`    'B1950.0'
cxcsec       :class:`~astropy.time.TimeCxcSec`                  63072064.184
datetime     :class:`~astropy.time.TimeDatetime`                datetime(2000, 1, 2, 12, 0, 0)
decimalyear  :class:`~astropy.time.TimeDecimalYear`             2000.45
fits         :class:`~astropy.time.TimeFITS`                    '2000-01-01T00:00:00.000'
gps          :class:`~astropy.time.TimeGPS`                     630720013.0
iso          :class:`~astropy.time.TimeISO`                     '2000-01-01 00:00:00.000'
isot         :class:`~astropy.time.TimeISOT`                    '2000-01-01T00:00:00.000'
jd           :class:`~astropy.time.TimeJD`                      2451544.5
jyear        :class:`~astropy.time.TimeJulianEpoch`             2000.0
jyear_str    :class:`~astropy.time.TimeJulianEpochString`       'J2000.0'
mjd          :class:`~astropy.time.TimeMJD`                     51544.0
plot_date    :class:`~astropy.time.TimePlotDate`                730120.0003703703
unix         :class:`~astropy.time.TimeUnix`                    946684800.0
unix_tai     :class:`~astropy.time.TimeUnixTai`                 946684800.0
yday         :class:`~astropy.time.TimeYearDayTime`             2000:001:00:00:00.000
ymdhms       :class:`~astropy.time.TimeYMDHMS`                  {'year': 2010, 'month': 3, 'day': 1}
datetime64   :class:`~astropy.time.TimeDatetime64`              np.datetime64('2000-01-01T01:01:01')
===========  =================================================  =====================================

.. note:: The :class:`~astropy.time.TimeFITS` format implements most
   of the FITS standard [#]_, including support for the ``LOCAL`` timescale.
   Note, though, that FITS supports some deprecated names for timescales;
   these are translated to the formal names upon initialization. Furthermore,
   any specific realization information, such as ``UT(NIST)`` is stored only as
   long as the time scale is not changed.
.. [#] `Rots et al. 2015, A&A 574:A36 <https://ui.adsabs.harvard.edu/abs/2015A%26A...574A..36R>`_

Changing Format
"""""""""""""""

.. EXAMPLE START: Changing Time Format

The default representation can be changed by setting the ``format`` attribute::

  >>> t = Time('2000-01-02')
  >>> t.format = 'jd'
  >>> t
  <Time object: scale='utc' format='jd' value=2451545.5>

Be aware that when changing format, the current output subformat (see section
below) may not exist in the new format. In this case, the subformat will not be
preserved::

  >>> t = Time('2000-01-02', format='fits', out_subfmt='longdate')
  >>> t.value
  '+02000-01-02'
  >>> t.format = 'iso'
  >>> t.out_subfmt
  u'*'
  >>> t.format = 'fits'
  >>> t.value
  '2000-01-02T00:00:00.000'

.. EXAMPLE END

Subformat
"""""""""

Many of the available time format classes support the concept of a subformat.
This allows for variations on the basic theme of a format in both the input
parsing/validation and the output.

The table below illustrates available subformats for the string formats
 ``iso``, ``fits``, and ``yday`` formats:

========  ============ ==============================
Format    Subformat    Input / Output
========  ============ ==============================
``iso``   date_hms     2001-01-02 03:04:05.678
``iso``   date_hm      2001-01-02 03:04
``iso``   date         2001-01-02
``fits``  date_hms     2001-01-02T03:04:05.678
``fits``  longdate_hms +02001-01-02T03:04:05.678
``fits``  longdate     +02001-01-02
``yday``  date_hms     2001:032:03:04:05.678
``yday``  date_hm      2001:032:03:04
``yday``  date         2001:032
========  ============ ==============================

Numerical formats such as ``mjd``, ``jyear``, or ``cxcsec`` all support the
subformats: ``'float'``, ``'long'``, ``'decimal'``, ``'str'``, and ``'bytes'``.
Here, ``'long'`` uses ``numpy.longdouble`` for somewhat enhanced precision (with
the enhancement depending on platform), and ``'decimal'`` instances of
:class:`decimal.Decimal` for full precision. For the ``'str'`` and ``'bytes'``
subformats, the number of digits is also chosen such that time values are
represented accurately.

When used on input, these formats allow creating a time using a single input
value that accurately captures the value to the full available precision in
|Time|. Conversely, the single value on output using |Time|
`~astropy.time.Time.to_value` or |TimeDelta| `~astropy.time.TimeDelta.to_value`
can have higher precision than the standard 64-bit float::

  >>> tm = Time('51544.000000000000001', format='mjd')  # String input
  >>> tm.mjd  # float64 output loses last digit but Decimal gets it
  51544.0
  >>> tm.to_value('mjd', subfmt='decimal')  # doctest: +SKIP
  Decimal('51544.00000000000000099920072216264')
  >>> tm.to_value('mjd', subfmt='str')
  '51544.000000000000001'

The complete list of subformat options for the |Time| formats that
have them is:

================ ========================================
Format           Subformats
================ ========================================
``byear``        float, long, decimal, str, bytes
``cxcsec``       float, long, decimal, str, bytes
``datetime64``   date_hms, date_hm, date
``decimalyear``  float, long, decimal, str, bytes
``fits``         date_hms, date, longdate_hms, longdate
``gps``          float, long, decimal, str, bytes
``iso``          date_hms, date_hm, date
``isot``         date_hms, date_hm, date
``jd``           float, long, decimal, str, bytes
``jyear``        float, long, decimal, str, bytes
``mjd``          float, long, decimal, str, bytes
``plot_date``    float, long, decimal, str, bytes
``unix``         float, long, decimal, str, bytes
``unix_tai``     float, long, decimal, str, bytes
``yday``         date_hms, date_hm, date
================ ========================================

The complete list of subformat options for the |TimeDelta| formats
that have them is:

================ ========================================
Format           Subformats
================ ========================================
``jd``           float, long, decimal, str, bytes
``sec``          float, long, decimal, str, bytes
================ ========================================

Time from Epoch Formats
"""""""""""""""""""""""

The formats ``cxcsec``, ``gps``, ``unix``, and ``unix_tai`` are special in that
they provide a floating point representation of the elapsed time in seconds
since a particular reference date. These formats have a intrinsic time scale
which is used to compute the elapsed seconds since the reference date.

============ ====== ========================
Format       Scale  Reference date
============ ====== ========================
``cxcsec``   TT     ``1998-01-01 00:00:00``
``unix``     UTC    ``1970-01-01 00:00:00``
``unix_tai`` TAI    ``1970-01-01 00:00:08``
``gps``      TAI    ``1980-01-06 00:00:19``
============ ====== ========================

Unlike the other formats which default to UTC, if no ``scale`` is provided when
initializing a |Time| object then the above intrinsic scale is used.
This is done for computational efficiency.

.. _time-scale:

Time Scale
^^^^^^^^^^

The time scale (or `time standard
<https://en.wikipedia.org/wiki/Time_standard>`_) is "a specification for
measuring time: either the rate at which time passes; or points in time; or
both" [#]_, [#]_.
::

  >>> Time.SCALES
  ('tai', 'tcb', 'tcg', 'tdb', 'tt', 'ut1', 'utc', 'local')

====== =================================
Scale  Description
====== =================================
tai    International Atomic Time   (TAI)
tcb    Barycentric Coordinate Time (TCB)
tcg    Geocentric Coordinate Time  (TCG)
tdb    Barycentric Dynamical Time  (TDB)
tt     Terrestrial Time             (TT)
ut1    Universal Time              (UT1)
utc    Coordinated Universal Time  (UTC)
local  Local Time Scale          (LOCAL)
====== =================================

.. [#] Wikipedia `time standard <https://en.wikipedia.org/wiki/Time_standard>`_ article
.. [#] SOFA_ Time Scale and Calendar Tools
       `(PDF) <http://www.iausofa.org/sofa_ts_c.pdf>`_

.. note:: The ``local`` time scale is meant for free-running clocks or
   simulation times (i.e., to represent a time without a properly defined
   scale). This means it cannot be converted to any other time scale, and
   arithmetic is possible only with |Time| instances with scale ``local`` and
   with |TimeDelta| instances with scale ``local`` or `None`.

The system of transformation between supported time scales (i.e., all but
``local``) is shown in the figure below. Further details are provided in the
`Convert time scale`_ section.

.. image:: time_scale_conversion.png

Scalar or Array
^^^^^^^^^^^^^^^

A |Time| object can hold either a single time value or an array of time values.
The distinction is made entirely by the form of the input time(s). If a |Time|
object holds a single value then any format outputs will be a single scalar
value, and likewise for arrays.

Example
"""""""

.. EXAMPLE START: Time Objects Holding Scalar or Array Values

Like other arrays and lists, |Time| objects holding arrays are subscriptable,
returning scalar or array objects as appropriate::

  >>> from astropy.time import Time
  >>> t = Time(100.0, format='mjd')
  >>> t.jd
  2400100.5
  >>> t = Time([100.0, 200.0, 300.], format='mjd')
  >>> t.jd  # doctest: +FLOAT_CMP
  array([2400100.5, 2400200.5, 2400300.5])
  >>> t[:2]  # doctest: +FLOAT_CMP
  <Time object: scale='utc' format='mjd' value=[100. 200.]>
  >>> t[2]
  <Time object: scale='utc' format='mjd' value=300.0>
  >>> t = Time(np.arange(50000., 50003.)[:, np.newaxis],
  ...          np.arange(0., 1., 0.5), format='mjd')
  >>> t  # doctest: +FLOAT_CMP
  <Time object: scale='utc' format='mjd' value=[[50000.  50000.5]
   [50001.  50001.5]
   [50002.  50002.5]]>
  >>> t[0]  # doctest: +FLOAT_CMP
  <Time object: scale='utc' format='mjd' value=[50000.  50000.5]>

.. EXAMPLE END

.. _astropy-time-shape-methods:

NumPy Method Analogs and Applicable NumPy Functions
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

For |Time| instances holding arrays, many of the same methods and attributes
that work on `~numpy.ndarray` instances can be used. For example, you can
reshape |Time| instances and take specific parts using
:meth:`~astropy.time.Time.reshape`, :meth:`~astropy.time.Time.ravel`,
:meth:`~astropy.time.Time.flatten`, :attr:`~astropy.time.Time.T`,
:meth:`~astropy.time.Time.transpose`, :meth:`~astropy.time.Time.swapaxes`,
:meth:`~astropy.time.Time.diagonal`, :meth:`~astropy.time.Time.squeeze`, or
:meth:`~astropy.time.Time.take`. Corresponding functions, as well as others
that affect the shape, such as `~numpy.atleast_1d` and `~numpy.rollaxis`, work
as expected. (The relevant functions have to be explicitly enabled in
``astropy`` source code; let us know if a ``numpy`` function is not supported
that you think should work.)

Examples
""""""""

.. EXAMPLE START: Reshaping Time Instances Using NumPy Method Analogs

To reshape |Time| instances::

  >>> t.reshape(2, 3)
  <Time object: scale='utc' format='mjd' value=[[50000.  50000.5 50001. ]
   [50001.5 50002.  50002.5]]>
  >>> t.T
  <Time object: scale='utc' format='mjd' value=[[50000.  50001.  50002. ]
   [50000.5 50001.5 50002.5]]>
  >>> np.roll(t, 1, axis=0)
  <Time object: scale='utc' format='mjd' value=[[50002.  50002.5]
   [50000.  50000.5]
   [50001.  50001.5]]>

Note that similarly to the `~numpy.ndarray` methods, all but
:meth:`~astropy.time.Time.flatten` try to use new views of the data,
with the data copied only if that is impossible (as discussed, for example, in
the documentation for ``numpy`` :func:`~numpy.reshape`).

.. EXAMPLE END

Some arithmetic methods are supported as well: :meth:`~astropy.time.Time.min`,
:meth:`~astropy.time.Time.max`, :meth:`~astropy.time.Time.ptp`,
:meth:`~astropy.time.Time.sort`, :meth:`~astropy.time.Time.argmin`,
:meth:`~astropy.time.Time.argmax`, and :meth:`~astropy.time.Time.argsort`.

.. EXAMPLE START: Applying Arithmetic Methods to Time Instances

To apply arithmetic methods to |Time| instances::

  >> t.max()
  <Time object: scale='utc' format='mjd' value=50002.5>
  >> t.ptp(axis=0)  # doctest: +FLOAT_CMP
  <TimeDelta object: scale='tai' format='jd' value=[2. 2.]>

.. EXAMPLE END

.. _astropy-time-inferring-input:

Inferring Input Format
^^^^^^^^^^^^^^^^^^^^^^

The |Time| class initializer will not accept ambiguous inputs, but it will make
automatic inferences in cases where the inputs are unambiguous. This can apply
when the times are supplied as objects, inputs for ``ymdhms``, or strings. In
the latter case it is not required to specify the format because the available
string formats have no overlap. However, if the format is known in advance the
string parsing will be faster if the format is provided.

Example
"""""""

.. EXAMPLE START: Inferring Input Format in the Time Class

To infer input format::

  >>> from datetime import datetime
  >>> t = Time(datetime(2010, 1, 2, 1, 2, 3))
  >>> t.format
  'datetime'
  >>> t = Time('2010-01-02 01:02:03')
  >>> t.format
  'iso'

.. EXAMPLE END

Internal Representation
^^^^^^^^^^^^^^^^^^^^^^^

The |Time| object maintains an internal representation of time as a pair of
double precision numbers expressing Julian days. The sum of the two numbers is
the Julian Date for that time relative to the given `time scale`_. Users
requiring no better than microsecond precision over human time scales (~100
years) can safely ignore the internal representation details and skip this
section.

This representation is driven by the underlying ERFA_ C-library implementation.
The ERFA routines take care throughout to maintain overall precision of the
double pair. Users are free to choose the way in which total JD is
provided, though internally one part contains integer days and the
other the fraction of the day, as this ensures optimal accuracy for
all conversions. The internal JD pair is available via the ``jd1``
and ``jd2`` attributes::

  >>> t = Time('2010-01-01 00:00:00', scale='utc')
  >>> t.jd1, t.jd2
  (2455198.0, -0.5)
  >>> t2 = t.tai
  >>> t2.jd1, t2.jd2  # doctest: +FLOAT_CMP
  (2455198., -0.49960648148148146)

Creating a Time Object
----------------------

The allowed |Time| arguments to create a time object are listed below:

**val** : numpy ndarray, list, str, or number
    Data to initialize table.
**val2** : numpy ndarray, list, str, or number; optional
    Data to initialize table.
**format** : str, optional
    Format of input value(s).
**scale** : str, optional
    Time scale of input value(s).
**precision** : int between 0 and 9 inclusive
    Decimal precision when outputting seconds as floating point.
**in_subfmt** : str
    Unix glob to select subformats for parsing input times.
**out_subfmt** : str
    Unix glob to select subformat for output times.
**location** : |EarthLocation| or tuple, optional
    If a tuple, three |Quantity| items with length units for geocentric
    coordinates, or a longitude, latitude, and optional height for geodetic
    coordinates. Can be a single location, or one for each input time.

val
^^^

The ``val`` argument specifies the input time or times and can be a single
string or number, or it can be a Python list or ```numpy`` array of strings or
numbers. To initialize a |Time| object based on a specified time, it *must* be
present.

In most situations, you also need to specify the `time scale`_ via the
``scale`` argument. The |Time| class will never guess the `time scale`_,
so a concise example would be::

  >>> t1 = Time(50100.0, scale='tt', format='mjd')
  >>> t2 = Time('2010-01-01 00:00:00', scale='utc')

It is possible to create a new |Time| object from one or more existing time
objects. In this case, the format and scale will be inferred from the
first object unless explicitly specified.
::

  >>> Time([t1, t2])  # doctest: +FLOAT_CMP
  <Time object: scale='tt' format='mjd' value=[50100. 55197.00076602]>

val2
^^^^

The ``val2`` argument is available for those situations where high precision is
required. Recall that the internal representation of time within `astropy.time`
is two double-precision numbers that when summed give the Julian date. If
provided, the ``val2`` argument is used in combination with ``val`` to set the
second of the internal time values. The exact interpretation of ``val2`` is
determined by the input format class. All string-valued formats ignore ``val2``
and all numeric inputs effectively add the two values in a way that maintains
the highest precision. For example::

  >>> t = Time(100.0, 0.000001, format='mjd', scale='tt')
  >>> t.jd, t.jd1, t.jd2  # doctest: +FLOAT_CMP
  (2400100.500001, 2400101.0, -0.499999)

format
^^^^^^

The ```format`` argument sets the time `time format`_, and as mentioned it is
required unless the format can be unambiguously determined from the input times.


scale
^^^^^

The ``scale`` argument sets the `time scale`_ and is required except for time
formats such as ``plot_date`` (:class:`~astropy.time.TimePlotDate`) and ``unix``
(:class:`~astropy.time.TimeUnix`). These formats represent the duration
in SI seconds since a fixed instant in time is independent of time scale. See
the `Time from Epoch Formats`_ for more details.

precision
^^^^^^^^^

The ``precision`` setting affects string formats when outputting a value that
includes seconds. It must be an integer between 0 and 9. There is no effect
when inputting time values from strings. The default precision is 3. Note
that the limit of 9 digits is driven by the way that ERFA_ handles fractional
seconds. In practice this should should not be an issue.  ::

  >>> t = Time('B1950.0', precision=3)
  >>> t.byear_str
  'B1950.000'
  >>> t.precision = 0
  >>> t.byear_str
  'B1950'

in_subfmt
^^^^^^^^^

The ``in_subfmt`` argument provides a mechanism to select one or more
`subformat`_ values from the available subformats for input. Multiple
allowed subformats can be selected using Unix-style wildcard characters, in
particular ``*`` and ``?``, as documented in the Python `fnmatch
<https://docs.python.org/3/library/fnmatch.html>`_ module.

The default value for ``in_subfmt`` is ``*`` which matches any available
subformat. This allows for convenient input of values with unknown or
heterogeneous subformat::

  >>> Time(['2000:001', '2000:002:03:04', '2001:003:04:05:06.789'])
  <Time object: scale='utc' format='yday'
   value=['2000:001:00:00:00.000' '2000:002:03:04:00.000' '2001:003:04:05:06.789']>

You can explicitly specify ``in_subfmt`` in order to strictly require a
certain subformat::

  >>> t = Time('2000:002:03:04', in_subfmt='date_hm')
  >>> t = Time('2000:002', in_subfmt='date_hm')  # doctest: +SKIP
  Traceback (most recent call last):
    ...
  ValueError: Input values did not match any of the formats where the
  format keyword is optional ['astropy_time', 'datetime',
  'byear_str', 'iso', 'isot', 'jyear_str', 'yday']

out_subfmt
^^^^^^^^^^

The ``out_subfmt`` argument is similar to ``in_subfmt`` except that it applies
to output formatting. In the case of multiple matching subformats, the first
matching subformat is used.

  >>> Time('2000-01-01 02:03:04', out_subfmt='date').iso
  '2000-01-01'
  >>> Time('2000-01-01 02:03:04', out_subfmt='date_hms').iso
  '2000-01-01 02:03:04.000'
  >>> Time('2000-01-01 02:03:04', out_subfmt='date*').iso
  '2000-01-01 02:03:04.000'
  >>> Time('50814.123456789012345', format='mjd', out_subfmt='str').mjd
  '50814.123456789012345'

See also the `subformat`_ section.

location
^^^^^^^^

This optional parameter specifies the observer location, using an
|EarthLocation| object or a tuple containing any form that can initialize one:
either a tuple with geocentric coordinates (X, Y, Z), or a tuple with geodetic
coordinates (longitude, latitude, height; with height defaulting to zero).
They are used for time scales that are sensitive to observer location
(currently, only TDB, which relies on the PyERFA_ routine `erfa.dtdb` to
determine the time offset between TDB and TT), as well as for sidereal time if
no explicit longitude is given.

  >>> t = Time('2001-03-22 00:01:44.732327132980', scale='utc',
  ...          location=('120d', '40d'))
  >>> t.sidereal_time('apparent', 'greenwich')  # doctest: +FLOAT_CMP +REMOTE_DATA
  <Longitude 12. hourangle>
  >>> t.sidereal_time('apparent')  # doctest: +FLOAT_CMP +REMOTE_DATA
  <Longitude 20. hourangle>

.. note:: In future versions, we hope to add the possibility to add observatory
          objects and/or names.

Getting the Current Time
^^^^^^^^^^^^^^^^^^^^^^^^

The current time can be determined as a |Time| object using the
`~astropy.time.Time.now` class method::

  >>> nt = Time.now()
  >>> ut = Time(datetime.utcnow(), scale='utc')

The two should be very close to each other.


.. _time-fast-c-parser:

Fast C-based Date String Parser
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Time formats that are based on a date string representation of time, including
`~astropy.time.TimeISO`, `~astropy.time.TimeISOT`, and
`~astropy.time.TimeYearDayTime`, make use of a fast C-based date parser that
improves speed by a factor of 20 or more for large arrays of times.

The C parser is stricter than the Python-based parser (which relies on
`~time.strptime`). In particular fields like the month or day of year must
always have a fixed number of ASCII digits. As an example the Python parser will
accept ``2000-1-2T3:04:5.23`` while the C parser requires
``2000-01-02T03:04:05.23``

Use of the C parser is enabled by default except when the input subformat
``in_subfmt`` argument is different from the default value of ``'*'``. If the
fast C parser fails to parse the date values then the |Time| initializer will
automatically fall through to the Python parser.

In rare cases where you need to explicitly control which parser gets used there
is a configuration item ``time.conf.use_fast_parser`` that can be set. The
default is ``'True'``, which means to try the fast parser and fall through to
Python parser if needed.  Note that the configuration value is a string, not a
bool object.

For example to disable the C parser use::

  >>> from astropy.time import conf
  >>> date = '2000-1-2T3:04:5.23'
  >>> t = Time(date, format='isot')  # Succeeds by default
  >>> with conf.set_temp('use_fast_parser', 'False'):
  ...     t = Time(date, format='isot')
  ...     print(t)
  2000-01-02T03:04:05.230

To force the user of the C parser (for example in testing) use::

  >>> with conf.set_temp('use_fast_parser', 'force'):
  ...     try:
  ...          t = Time(date, format='isot')
  ...     except ValueError as err:
  ...          print(err)
  Input values did not match the format class isot:
  ValueError: fast C time string parser failed: non-digit found where digit (0-9) required

Using Time Objects
------------------

The operations available with |Time| objects include:

- Get and set time value(s) for an array-valued |Time| object.
- Set missing (masked) values.
- Get the representation of the time value(s) in a particular `time format`_.
- Get a new time object for the same time value(s) but referenced to a different
  `time scale`_.
- Calculate `sidereal time and Earth rotation angle`_ corresponding to the time value(s).
- Do time arithmetic involving |Time| and/or |TimeDelta| objects.

Get and Set Values
^^^^^^^^^^^^^^^^^^

For an existing |Time| object which is array-valued, you can use the
usual ``numpy`` array item syntax to get either a single item or a subset
of items. The returned value is a |Time| object with all the same
attributes.

Examples
""""""""

.. EXAMPLE START: Get and Set Values for Time Objects

To get an item or a subset of items::

  >>> t = Time(['2001:020', '2001:040', '2001:060', '2001:080'],
  ...          out_subfmt='date')
  >>> print(t[1])
  2001:040
  >>> print(t[1:])
  ['2001:040' '2001:060' '2001:080']
  >>> print(t[[2, 0]])
  ['2001:060' '2001:020']

You can also set values in place for an array-valued |Time| object::

  >>> t = Time(['2001:020', '2001:040', '2001:060', '2001:080'],
  ...          out_subfmt='date')
  >>> t[1] = '2010:001'
  >>> print(t)
  ['2001:020' '2010:001' '2001:060' '2001:080']
  >>> t[[2, 0]] = '1990:123'
  >>> print(t)
  ['1990:123' '2010:001' '1990:123' '2001:080']

.. EXAMPLE END

The new value (on the right hand side) when setting can be one of three
possibilities:

- Scalar string value or array of string values where each value
  is in a valid time format that can be automatically parsed and
  used to create a |Time| object.
- Value or array of values where each value has the same ``format`` as
  the |Time| object being set. For instance, a float or ``numpy`` array
  of floats for an object with ``format='unix'``.
- |Time| object with identical ``location`` (but ``scale`` and
  ``format`` need not be the same). The right side value will be
  transformed so the time ``scale`` matches.

Whenever any item is set, then the internal cache (see `Caching`_) is cleared
along with the ``delta_tdb_tt`` and/or ``delta_ut1_utc`` transformation
offsets, if they have been set.

If it is required that the |Time| object be immutable, then set the
``writeable`` attribute to `False`. In this case, attempting to set a value will
raise a ``ValueError: Time object is read-only``. See the section on
`Caching`_ for an example.

Missing Values
^^^^^^^^^^^^^^

The |Time| and |TimeDelta| objects support functionality for marking values as
missing or invalid. This is also known as masking, and is especially useful for
:ref:`table_operations` such as joining and stacking.

Example
"""""""

.. EXAMPLE START: Missing Values in Time and TimeDelta Objects

To set one or more items as missing, assign the special value
`numpy.ma.masked`::

  >>> t = Time(['2001:020', '2001:040', '2001:060', '2001:080'],
  ...          out_subfmt='date')
  >>> t[2] = np.ma.masked
  >>> print(t)
  ['2001:020' '2001:040' -- '2001:080']

.. note:: The operation of setting an array element to `numpy.ma.masked`
   (missing) *overwrites* the actual time data and therefore there is no way to
   recover the original value. In this sense, the `numpy.ma.masked` value
   behaves just like any other valid |Time| value when setting. This is
   similar to how `Pandas missing data
   <https://pandas.pydata.org/pandas-docs/stable/missing_data.html>`_ works,
   but somewhat different from `NumPy masked arrays
   <https://numpy.org/doc/stable/reference/maskedarray.html>`_ which
   maintain a separate mask array and retain the underlying data. In the
   |Time| object the ``mask`` attribute is read-only and cannot be directly set.

.. EXAMPLE END

Once one or more values in the object are masked, any operations will
propagate those values as masked, and access to format attributes such
as ``unix`` or ``value`` will return a `~numpy.ma.MaskedArray`
object::

  >>> t.unix  # doctest: +SKIP
  masked_array(data = [979948800.0 981676800.0 -- 985132800.0],
               mask = [False False  True False],
         fill_value = 1e+20)

You can view the ``mask``, but note that it is read-only and
setting the mask is always done by setting the item to `~numpy.ma.masked`.

  >>> t.mask
  array([False, False,  True, False]...)
  >>> t[:2] = np.ma.masked

.. warning:: The internal implementation of missing value support is provisional
   and may change in a subsequent release. This would impact information in the
   next section. However, the documented API for using missing values with
   |Time| and |TimeDelta| objects is stable.

Custom Format Classes and Missing Values
""""""""""""""""""""""""""""""""""""""""

For advanced users who have written a custom time format via a
`~astropy.time.TimeFormat` subclass, it may be necessary to modify your
class *if you wish to support missing values*. For applications that
do not take advantage of missing values no changes are required.

Missing values in a `~astropy.time.TimeFormat` subclass object are marked by
setting the corresponding entries of the ``jd2`` attribute to be ``numpy.nan``
(but this is never done directly by the user). For most array operations and
``numpy`` functions the ``numpy.nan`` entries are propagated as expected and
all is well. However, this is not always the case, and in particular the `ERFA`_
routines do not generally support ``numpy.nan`` values gracefully.

In cases where ``numpy.nan`` is not acceptable, format class methods should use
the ``jd2_filled`` property instead of ``jd2``. This replaces ``numpy.nan`` with
``0.0``. Since ``jd2`` is always in the range -1 to +1, substituting ``0.0``
will allow functions to return "reasonable" values which will then be masked in
any subsequent outputs. See the ``value`` property of the
`~astropy.time.TimeDecimalYear` format for any example.

Get Representation
^^^^^^^^^^^^^^^^^^

Instants of time can be represented in different ways, for instance as an
ISO-format date string (``'1999-07-23 04:31:00'``) or seconds since 1998.0
(``49091460.0``) or Modified Julian Date (``51382.187451574``).

The representation of a |Time| object in a particular format is available
by getting the object attribute corresponding to the format name. The list of
available format names is in the `time format`_ section.

  >>> t = Time('2010-01-01 00:00:00', format='iso', scale='utc')
  >>> t.jd        # JD representation of time in current scale (UTC)
  2455197.5
  >>> t.iso       # ISO representation of time in current scale (UTC)
  '2010-01-01 00:00:00.000'
  >>> t.unix      # seconds since 1970.0 (UTC)
  1262304000.0
  >>> t.datetime  # Representation as datetime.datetime object
  datetime.datetime(2010, 1, 1, 0, 0)

Example
"""""""

.. EXAMPLE START: Get Representation of a Time Object

To get the representation of a |Time| object::

  >>> import matplotlib.pyplot as plt  # doctest: +SKIP
  >>> jyear = np.linspace(2000, 2001, 20)  # doctest: +SKIP
  >>> t = Time(jyear, format='jyear')  # doctest: +SKIP
  >>> plt.plot_date(t.plot_date, jyear)  # doctest: +SKIP
  >>> plt.gcf().autofmt_xdate()  # orient date labels at a slant  # doctest: +SKIP
  >>> plt.draw()  # doctest: +SKIP

.. EXAMPLE END

Convert Time Scale
^^^^^^^^^^^^^^^^^^

A new |Time| object for the same time value(s) but referenced to a new `time
scale`_ can be created getting the object attribute corresponding to the time
scale name. The list of available time scale names is in the `time scale`_
section and in the figure below illustrating the network of time scale
transformations.

.. image:: time_scale_conversion.png

Examples
""""""""

.. EXAMPLE START: Converting Time Scales in Time Objects

To create a |Time| object with a new time scale::

  >>> t = Time('2010-01-01 00:00:00', format='iso', scale='utc')
  >>> t.tt        # TT scale
  <Time object: scale='tt' format='iso' value=2010-01-01 00:01:06.184>
  >>> t.tai
  <Time object: scale='tai' format='iso' value=2010-01-01 00:00:34.000>

In this process the ``format`` and other object attributes like ``lon``,
``lat``, and ``precision`` are also propagated to the new object.

.. EXAMPLE END

As noted in the `Time Object Basics`_ section, a |Time| object can only be
changed by explicitly setting some of its elements. The process of changing the
time scale therefore begins by making a copy of the original object and then
converting the internal time values in the copy to the new time scale. The new
|Time| object is returned by the attribute access.

Caching
^^^^^^^

The computations for transforming to different time scales or formats can be
time-consuming for large arrays. In order to avoid repeated computations, each
|Time| or |TimeDelta| instance caches such transformations internally::

  >>> t = Time(np.arange(1e6), format='unix', scale='utc')

  >>> time x = t.tt  # doctest: +SKIP
  CPU times: user 263 ms, sys: 4.02 ms, total: 267 ms
  Wall time: 267 ms

  >>> time x = t.tt  # doctest: +SKIP
  CPU times: user 28 s, sys: 9 s, total: 37 s
  Wall time: 32.9 s

Actions such as changing the output precision or subformat will clear
the cache. In order to explicitly clear the internal cache do::

  >>> del t.cache

  >>> time x = t.tt  # doctest: +SKIP
  CPU times: user 263 ms, sys: 4.02 ms, total: 267 ms
  Wall time: 267 ms

In order to ensure consistency between the transformed (and cached) version and
the original, the transformed object is set to be not writeable. For example::

  >>> x = t.tt
  >>> x[1] = '2000:001'
  Traceback (most recent call last):
    ...
  ValueError: Time object is read-only. Make a copy() or set "writeable" attribute to True.

If you require modifying the object then make a copy first, for example, ``x =
t.tt.copy()``.

Transformation Offsets
""""""""""""""""""""""

Time scale transformations that cross one of the orange circles in the image
above require an additional offset time value that is model or
observation dependent. See SOFA_ `Time Scale and Calendar Tools
<http://www.iausofa.org/sofa_ts_c.pdf>`_ for further details.

The two attributes :attr:`~astropy.time.Time.delta_ut1_utc` and
:attr:`~astropy.time.Time.delta_tdb_tt` provide a way to set
these offset times explicitly. These represent the time scale offsets
UT1 - UTC and TDB - TT, respectively. As an example::

  >>> t = Time('2010-01-01 00:00:00', format='iso', scale='utc')
  >>> t.delta_ut1_utc = 0.334  # Explicitly set one part of the transformation
  >>> t.ut1.iso    # ISO representation of time in UT1 scale
  '2010-01-01 00:00:00.334'

For the UT1 to UTC offset, you have to interpolate the observed values provided
by the `International Earth Rotation and Reference Systems (IERS) Service
<http://www.iers.org>`_. ``astropy`` will automatically download and use values
from the IERS which cover times spanning from 1973-Jan-01 through one year into
the future. In addition, the ``astropy`` package is bundled with a data table of
values provided in Bulletin B, which cover the period from 1962 to shortly
before an ``astropy`` release.

When the :attr:`~astropy.time.Time.delta_ut1_utc` attribute is not set
explicitly, IERS values will be used (initiating a download of a few Mb
file the first time). For details about how IERS values are used in ``astropy``
time and coordinates, and to understand how to control automatic downloads, see
:ref:`utils-iers`. The example below illustrates converting to the ``UT1``
scale along with the auto-download feature::

  >>> t = Time('2016:001')
  >>> t.ut1  # doctest: +SKIP
  Downloading https://maia.usno.navy.mil/ser7/finals2000A.all
  |==================================================================| 3.0M/3.0M (100.00%)         6s
  <Time object: scale='ut1' format='yday' value=2016:001:00:00:00.082>

.. note:: The :class:`~astropy.utils.iers.IERS_Auto` class contains machinery
    to ensure that the IERS table is kept up to date by auto-downloading the
    latest version as needed. This means that the IERS table is assured of
    having the state-of-the-art definitive and predictive values for Earth
    rotation. As a user it is **your responsibility** to understand the
    accuracy of IERS predictions if your science depends on that. If you
    request ``UT1-UTC`` for times beyond the range of IERS table data then the
    nearest available values will be provided.

In the case of the TDB to TT offset, most users need only provide the ``lon``
and ``lat`` values when creating the |Time| object. If the
:attr:`~astropy.time.Time.delta_tdb_tt` attribute is not explicitly set, then
the PyERFA_ routine `erfa.dtdb` will be used to compute the TDB to TT
offset. Note that if ``lon`` and ``lat`` are not explicitly initialized,
values of 0.0 degrees for both will be used.

Example
~~~~~~~

.. EXAMPLE START: Transformation Offsets in Time Objects

The following code replicates an example in the SOFA_ `Time Scale and Calendar
Tools <http://www.iausofa.org/sofa_ts_c.pdf>`_ document. It does the transform
from UTC to all supported time scales (TAI, TCB, TCG, TDB, TT, UT1, UTC). This
requires an observer location (here, latitude and longitude).
::

  >>> import astropy.units as u
  >>> t = Time('2006-01-15 21:24:37.5', format='iso', scale='utc',
  ...          location=(-155.933222*u.deg, 19.48125*u.deg))
  >>> t.utc.iso
  '2006-01-15 21:24:37.500'
  >>> t.ut1.iso  # doctest: +REMOTE_DATA
  '2006-01-15 21:24:37.834'
  >>> t.tai.iso
  '2006-01-15 21:25:10.500'
  >>> t.tt.iso
  '2006-01-15 21:25:42.684'
  >>> t.tcg.iso
  '2006-01-15 21:25:43.323'
  >>> t.tdb.iso
  '2006-01-15 21:25:42.684'
  >>> t.tcb.iso
  '2006-01-15 21:25:56.894'

.. EXAMPLE END

Hashing
^^^^^^^

A user can generate a unique hash key for scalar (0-dimensional) |Time| or
|TimeDelta| objects. The key is based on a tuple of ``jd1``,
``jd2``, ``scale``, and ``location`` (if present, ``None`` otherwise).

Note that two |Time| objects with a different ``scale`` can compare equally
but still have different hash keys. This a practical consideration driven
in by performance, but in most cases represents a desirable behavior.


Printing Time Arrays
^^^^^^^^^^^^^^^^^^^^

If your ``times`` array contains a lot of elements, the ``value`` argument will
display all the elements of the |Time| object ``t`` when it is called or
printed. To control the number of elements to be displayed, set the
``threshold`` argument with ``np.printoptions`` as follows:

    >>> many_times = np.arange(1000)
    >>> t = Time(many_times, format='cxcsec')
    >>> with np.printoptions(threshold=10):
    ...     print(repr(t))
    ...     print(t.iso)
    <Time object: scale='tt' format='cxcsec' value=[  0.   1.   2. ... 997. 998. 999.]>
    ['1998-01-01 00:00:00.000' '1998-01-01 00:00:01.000'
     '1998-01-01 00:00:02.000' ... '1998-01-01 00:16:37.000'
     '1998-01-01 00:16:38.000' '1998-01-01 00:16:39.000']

Sidereal Time and Earth Rotation Angle
--------------------------------------

Apparent or mean sidereal time can be calculated using
:meth:`~astropy.time.Time.sidereal_time`. The method returns a |Longitude|
with units of hour angle, which by default is for the longitude corresponding to
the location with which the |Time| object is initialized. Like the scale
transformations, ERFA_ C-library routines are used under the hood, which support
calculations following different IAU resolutions.

Similarly, one can calculate the Earth rotation angle with
:meth:`~astropy.time.Time.earth_rotation_angle`. Unlike sidereal time, which
is referred to the equinox and is a complicated function of both UT1 and
Terrestrial Time, the Earth rotation angle is referred to the Celestial
Intermediate Origin (CIO) and is a linear function of UT1 alone.

For the recent IAU precession models, as well as for the Earth rotation angle,
the result includes the TIO locator (s'), which positions the Terrestrial
Intermediate Origin on the equator of the Celestial Intermediate Pole (CIP)
and is rigorously corrected for polar motion.

Example
^^^^^^^

.. EXAMPLE START: Calculating Sidereal Time and Earth Rotation Angle for Time Objects

To calculate sidereal time::

  >>> t = Time('2006-01-15 21:24:37.5', scale='utc', location=('120d', '45d'))
  >>> t.sidereal_time('mean')  # doctest: +FLOAT_CMP +REMOTE_DATA
  <Longitude 13.08952187 hourangle>
  >>> t.sidereal_time('apparent')  # doctest: +FLOAT_CMP +REMOTE_DATA
  <Longitude 13.08950368 hourangle>
  >>> t.earth_rotation_angle()  # doctest: +FLOAT_CMP +REMOTE_DATA
  <Longitude 13.08436206 hourangle>
  >>> t.sidereal_time('apparent', 'greenwich')  # doctest: +FLOAT_CMP +REMOTE_DATA
  <Longitude 5.08950368 hourangle>
  >>> t.sidereal_time('apparent', '-90d')  # doctest: +FLOAT_CMP +REMOTE_DATA
  <Longitude 23.08950368 hourangle>
  >>> t.sidereal_time('apparent', '-90d', 'IAU1994')  # doctest: +FLOAT_CMP +REMOTE_DATA
  <Longitude 23.08950365 hourangle>

.. EXAMPLE END

Time Deltas
-----------

Time arithmetic is supported using the |TimeDelta| class. The following
operations are available:

- Create a |TimeDelta| explicitly by instantiating a class object.
- Create a |TimeDelta| by subtracting two |Time| objects.
- Add a |TimeDelta| to a |Time| object to get a new |Time|.
- Subtract a |TimeDelta| from a |Time| object to get a new |Time|.
- Add two |TimeDelta| objects to get a new |TimeDelta|.
- Negate a |TimeDelta| or take its absolute value.
- Multiply or divide a |TimeDelta| by a constant or array.
- Convert |TimeDelta| objects to and from time-like |Quantity|'s.

The |TimeDelta| class is derived from the |Time| class and shares many of its
properties. One difference is that the time scale has to be one for which one
day is exactly 86400 seconds. Hence, the scale cannot be UTC.

The available time formats are:

=========  ===================================================
Format     Class
=========  ===================================================
sec        :class:`~astropy.time.TimeDeltaSec`
jd         :class:`~astropy.time.TimeDeltaJD`
datetime   :class:`~astropy.time.TimeDeltaDatetime`
=========  ===================================================

Examples
^^^^^^^^

.. EXAMPLE START: Time Arithmetic Using the TimeDelta Class

Use of the |TimeDelta| object is illustrated in the few examples below::

  >>> t1 = Time('2010-01-01 00:00:00')
  >>> t2 = Time('2010-02-01 00:00:00')
  >>> dt = t2 - t1  # Difference between two Times
  >>> dt
  <TimeDelta object: scale='tai' format='jd' value=31.0>
  >>> dt.sec
  2678400.0

  >>> from astropy.time import TimeDelta
  >>> dt2 = TimeDelta(50.0, format='sec')
  >>> t3 = t2 + dt2  # Add a TimeDelta to a Time
  >>> t3.iso
  '2010-02-01 00:00:50.000'

  >>> t2 - dt2  # Subtract a TimeDelta from a Time
  <Time object: scale='utc' format='iso' value=2010-01-31 23:59:10.000>

  >>> dt + dt2  # doctest: +FLOAT_CMP
  <TimeDelta object: scale='tai' format='jd' value=31.0005787037>

  >>> import numpy as np
  >>> t1 + dt * np.linspace(0, 1, 5)
  <Time object: scale='utc' format='iso' value=['2010-01-01 00:00:00.000'
  '2010-01-08 18:00:00.000' '2010-01-16 12:00:00.000' '2010-01-24 06:00:00.000'
  '2010-02-01 00:00:00.000']>

The |TimeDelta| has a `~astropy.time.TimeDelta.to_value` method which supports
controlling the type of the output representation by providing either a format
name and optional `subformat`_ or a valid ``astropy`` unit::

  >>> dt.to_value(u.hr)
  744.0
  >>> dt.to_value('jd', 'str')
  '31.0'

.. EXAMPLE END

Time Scales for Time Deltas
^^^^^^^^^^^^^^^^^^^^^^^^^^^

We have shown in the above that the difference between two UTC times is a
|TimeDelta| with a scale of TAI. This is because a UTC time difference cannot be
uniquely defined unless the user knows the two times that were differenced
(because of leap seconds, a day does not always have 86400 seconds). For all
other time scales, the |TimeDelta| inherits the scale of the first |Time|
object.

Examples
""""""""

.. EXAMPLE START: Time Scales for Time Deltas

To get the time scale for a |TimeDelta| object::

  >>> t1 = Time('2010-01-01 00:00:00', scale='tcg')
  >>> t2 = Time('2011-01-01 00:00:00', scale='tcg')
  >>> dt = t2 - t1
  >>> dt
  <TimeDelta object: scale='tcg' format='jd' value=365.0>

When |TimeDelta| objects are added or subtracted from |Time| objects, scales
are converted appropriately, with the final scale being that of the |Time|
object::

  >>> t2 + dt
  <Time object: scale='tcg' format='iso' value=2012-01-01 00:00:00.000>
  >>> t2.tai
  <Time object: scale='tai' format='iso' value=2010-12-31 23:59:27.068>
  >>> t2.tai + dt
  <Time object: scale='tai' format='iso' value=2011-12-31 23:59:27.046>

|TimeDelta| objects can be converted only to objects with compatible scales
(i.e., scales for which it is not necessary to know the times that were
differenced)::

  >>> dt.tt  # doctest: +FLOAT_CMP
  <TimeDelta object: scale='tt' format='jd' value=364.999999746>
  >>> dt.tdb  # doctest: +IGNORE_EXCEPTION_DETAIL
  Traceback (most recent call last):
    ...
  ScaleValueError: Cannot convert TimeDelta with scale 'tcg' to scale 'tdb'

|TimeDelta| objects can also have an undefined scale, in which case it is
assumed that their scale matches that of the other |Time| or |TimeDelta|
object (or is TAI in case of a UTC time)::

  >>> t2.tai + TimeDelta(365., format='jd', scale=None)
  <Time object: scale='tai' format='iso' value=2011-12-31 23:59:27.068>

.. note:: Since internally |Time| uses floating point numbers, round-off
          errors can cause two times to be not strictly equal even if
          mathematically they should be. For times in UTC in particular, this
          can lead to surprising behavior, because when you add a
          |TimeDelta|, which cannot have a scale of UTC, the UTC time is
          first converted to TAI, then the addition is done, and finally the
          time is converted back to UTC. Hence, rounding errors can be
          incurred, which means that even expected equalities may not hold::

            >>> t = Time(2450000., 1e-6, format='jd')
            >>> t + TimeDelta(0, format='jd') == t
            False

.. EXAMPLE END

.. _time-light-travel-time:

Barycentric and Heliocentric Light Travel Time Corrections
----------------------------------------------------------

The arrival times of photons at an observatory are not particularly useful for
accurate timing work, such as eclipse/transit timing of binaries or exoplanets.
This is because the changing location of the observatory causes photons to
arrive early or late. The solution is to calculate the time the photon would
have arrived at a standard location; either the Solar System barycenter or the
heliocenter.

Example
^^^^^^^

.. EXAMPLE START: Barycentric and Heliocentric Light Travel Time Corrections

Suppose you observed the dwarf nova IP Peg from Greenwich and have a list of
times in MJD form, in the UTC timescale. You then create appropriate |Time| and
|SkyCoord| objects and calculate light travel times to the barycenter as
follows::

    >>> from astropy import time, coordinates as coord, units as u
    >>> ip_peg = coord.SkyCoord("23:23:08.55", "+18:24:59.3",
    ...                         unit=(u.hourangle, u.deg), frame='icrs')
    >>> greenwich = coord.EarthLocation.of_site('greenwich')  # doctest: +REMOTE_DATA
    >>> times = time.Time([56325.95833333, 56325.978254], format='mjd',
    ...                   scale='utc', location=greenwich)  # doctest: +REMOTE_DATA
    >>> ltt_bary = times.light_travel_time(ip_peg)  # doctest: +REMOTE_DATA
    >>> ltt_bary # doctest: +FLOAT_CMP +REMOTE_DATA
    <TimeDelta object: scale='tdb' format='jd' value=[-0.0037715  -0.00377286]>

If you desire the light travel time to the heliocenter instead, then use::

    >>> ltt_helio = times.light_travel_time(ip_peg, 'heliocentric') # doctest: +REMOTE_DATA
    >>> ltt_helio # doctest: +FLOAT_CMP +REMOTE_DATA
    <TimeDelta object: scale='tdb' format='jd' value=[-0.00376576 -0.00376712]>

The method returns an |TimeDelta| object, which can be added to
your times to give the arrival time of the photons at the barycenter or
heliocenter. Here, you should be careful with the timescales used; for more
detailed information about timescales, see :ref:`time-scale`.

.. EXAMPLE END

The heliocenter is not a fixed point, and therefore the gravity
continually changes at the heliocenter. Thus, the use of a relativistic
timescale like TDB is not particularly appropriate, and, historically,
times corrected to the heliocenter are given in the UTC timescale::

    >>> times_heliocentre = times.utc + ltt_helio  # doctest: +REMOTE_DATA

Corrections to the barycenter are more precise than the heliocenter,
because the barycenter is a fixed point where gravity is constant. For
maximum accuracy you want to have your barycentric corrected times in a
timescale that has always ticked at a uniform rate, and ideally one
whose tick rate is related to the rate that a clock would tick at the
barycenter. For this reason, barycentric corrected times normally use
the TDB timescale::

    >>> time_barycentre = times.tdb + ltt_bary  # doctest: +REMOTE_DATA

.. EXAMPLE START: Calculating Light Travel Time Using JPL Ephemerides

By default, the light travel time is calculated using the position and velocity
of Earth and the Sun from ERFA_
routines, but you can also get more precise calculations using the JPL
ephemerides (which are derived from dynamical models). An example using the JPL
ephemerides is:

.. doctest-requires:: jplephem

    >>> ltt_bary_jpl = times.light_travel_time(ip_peg, ephemeris='jpl') # doctest: +REMOTE_DATA +IGNORE_OUTPUT
    >>> ltt_bary_jpl # doctest: +REMOTE_DATA +FLOAT_CMP
    <TimeDelta object: scale='tdb' format='jd' value=[-0.0037715  -0.00377286]>
    >>> (ltt_bary_jpl - ltt_bary).to(u.ms) # doctest: +REMOTE_DATA +IGNORE_OUTPUT
    <Quantity [-0.00132325, -0.00132861] ms>

The difference between the built-in ephemerides and the JPL ephemerides is
normally of the order of 1/100th of a millisecond, so the built-in ephemerides
should be suitable for most purposes. For more details about what ephemerides
are available, including the requirements for using JPL ephemerides, see
:ref:`astropy-coordinates-solarsystem`.

.. EXAMPLE END

Interaction with time-like Quantities
-------------------------------------

Where possible, |Quantity| objects with units of time are treated as |TimeDelta|
objects with undefined scale (though necessarily with lower precision). They
can also be used as input in constructing |Time| and |TimeDelta| objects, and
|TimeDelta| objects can be converted to |Quantity| objects of arbitrary units
of time.

Examples
^^^^^^^^

.. EXAMPLE START: Time Object Interaction with time-like Quantities

To use |Quantity| objects with units of time::

  >>> import astropy.units as u
  >>> Time(10.*u.yr, format='gps')   # time-valued quantities can be used for
  ...                                # for formats requiring a time offset
  <Time object: scale='tai' format='gps' value=315576000.0>
  >>> Time(10.*u.yr, 1.*u.s, format='gps')
  <Time object: scale='tai' format='gps' value=315576001.0>
  >>> Time(2000.*u.yr, format='jyear')
  <Time object: scale='tt' format='jyear' value=2000.0>
  >>> Time(2000.*u.yr, format='byear')
  ...                                # but not for Besselian year, which implies
  ...                                # a different time scale
  ...
  Traceback (most recent call last):
    ...
  ValueError: Input values did not match the format class byear:
  ValueError: Cannot use Quantities for 'byear' format, as the interpretation would be ambiguous. Use float with Besselian year instead.

  >>> TimeDelta(10.*u.yr)            # With a quantity, no format is required
  <TimeDelta object: scale='None' format='jd' value=3652.5>

  >>> dt = TimeDelta([10., 20., 30.], format='jd')
  >>> dt.to(u.hr)                    # can convert TimeDelta to a quantity  # doctest: +FLOAT_CMP
  <Quantity [240., 480., 720.] h>
  >>> dt > 400. * u.hr               # and compare to quantities with units of time
  array([False,  True,  True]...)
  >>> dt + 1.*u.hr                   # can also add/subtract such quantities  # doctest: +FLOAT_CMP
  <TimeDelta object: scale='None' format='jd' value=[10.04166667 20.04166667 30.04166667]>
  >>> Time(50000., format='mjd', scale='utc') + 1.*u.hr  # doctest: +FLOAT_CMP
  <Time object: scale='utc' format='mjd' value=50000.0416667>
  >>> dt * 10.*u.km/u.s              # for multiplication and division with a
  ...                                # Quantity, TimeDelta is converted  # doctest: +FLOAT_CMP
  <Quantity [100., 200., 300.] d km / s>
  >>> dt * 10.*u.Unit(1)             # unless the Quantity is dimensionless  # doctest: +FLOAT_CMP
  <TimeDelta object: scale='None' format='jd' value=[100. 200. 300.]>

.. EXAMPLE END

Writing a Custom Format
-----------------------

Some applications may need a custom |Time| format, and this capability is
available by making a new subclass of the `~astropy.time.TimeFormat` class.
When such a subclass is defined in your code, the format class and
corresponding name is automatically registered in the set of available time
formats.

Examples
^^^^^^^^

.. EXAMPLE START: Writing a Custom Format with the TimeFormat Class

The key elements of a new format class are illustrated by examining the
code for the ``jd`` format (which is one of the most minimal)::

  class TimeJD(TimeFormat):
      """
      Julian Date time format.
      """
      name = 'jd'  # Unique format name

      def set_jds(self, val1, val2):
          """
          Set the internal jd1 and jd2 values from the input val1, val2.
          The input values are expected to conform to this format, as
          validated by self._check_val_type(val1, val2) during __init__.
          """
          self._check_scale(self._scale)  # Validate scale.
          self.jd1, self.jd2 = day_frac(val1, val2)

      @property
      def value(self):
          """
          Return format ``value`` property from internal jd1, jd2
          """
          return self.jd1 + self.jd2

As mentioned above, the ``_check_val_type(self, val1, val2)``
method may need to be overridden to validate the inputs as conforming to the
format specification. By default this checks for valid float, float array, or
|Quantity| inputs. In contrast, the ``iso`` format class ensures the inputs
meet the ISO format specification for strings.

.. EXAMPLE END

.. EXAMPLE START: Customizing the TimeFormat Class with Changes to Date Format

One special case that is relatively common and more convenient to implement is a
format that makes a small change to the date format. For instance, you could
insert ``T`` in the ``yday`` format with the following ``TimeYearDayTimeCustom``
class. Notice how the ``subfmts`` definition is modified slightly from the
standard `~astropy.time.TimeISO` class from which it inherits::

  >>> from astropy.time import TimeISO
  >>> class TimeYearDayTimeCustom(TimeISO):
  ...    """
  ...    Year, day-of-year and time as "<YYYY>-<DOY>T<HH>:<MM>:<SS.sss...>".
  ...    The day-of-year (DOY) goes from 001 to 365 (366 in leap years).
  ...    For example, 2000-001T00:00:00.000 is midnight on January 1, 2000.
  ...    The allowed subformats are:
  ...    - 'date_hms': date + hours, mins, secs (and optional fractional secs)
  ...    - 'date_hm': date + hours, mins
  ...    - 'date': date
  ...    """
  ...    name = 'yday_custom'  # Unique format name
  ...    subfmts = (('date_hms',
  ...                '%Y-%jT%H:%M:%S',
  ...                '{year:d}-{yday:03d}T{hour:02d}:{min:02d}:{sec:02d}'),
  ...               ('date_hm',
  ...                '%Y-%jT%H:%M',
  ...                '{year:d}-{yday:03d}T{hour:02d}:{min:02d}'),
  ...               ('date',
  ...                '%Y-%j',
  ...                '{year:d}-{yday:03d}'))


  >>> t = Time('2000-01-01')
  >>> t.yday_custom
  '2000-001T00:00:00.000'
  >>> t2 = Time('2016-001T00:00:00')
  >>> t2.iso
  '2016-01-01 00:00:00.000'

.. EXAMPLE END

.. EXAMPLE START: Customizing the TimeFormat Class with Time Since an Epoch

Another special case that is relatively common is a format that represents the
time since a particular epoch. The classic example is Unix time which is the
number of seconds since 1970-01-01 00:00:00 UTC, not counting leap seconds. What
if we wanted that value but **do** want to count leap seconds. This would be
done by using the TAI scale instead of the UTC scale. In this case we inherit
from the `~astropy.time.TimeFromEpoch` class and define a few class attributes::

  >>> from astropy.time.formats import erfa, TimeFromEpoch
  >>> class TimeUnixLeap(TimeFromEpoch):
  ...    """
  ...    Seconds from 1970-01-01 00:00:00 TAI.  Similar to Unix time
  ...    but this includes leap seconds.
  ...    """
  ...    name = 'unix_leap'
  ...    unit = 1.0 / erfa.DAYSEC  # in days (1 day == 86400 seconds)
  ...    epoch_val = '1970-01-01 00:00:00'
  ...    epoch_val2 = None
  ...    epoch_scale = 'tai'  # Scale for epoch_val class attribute
  ...    epoch_format = 'iso'  # Format for epoch_val class attribute

  >>> t = Time('2000-01-01')
  >>> t.unix_leap
  946684832.0
  >>> t.unix_leap - t.unix
  32.0

.. EXAMPLE END

Going beyond this will probably require looking at the ``astropy`` code for more
guidance, but if you get stuck, the ``astropy`` developers are more than happy
to help. If you write a format class that is widely useful we might want to
include it in the core!

Timezones
---------

When a `~astropy.time.Time` object is constructed from a timezone-aware
`~datetime.datetime`, no timezone information is saved in the
`~astropy.time.Time` object. However, `~astropy.time.Time` objects can be
converted to timezone-aware datetime objects.

Example
^^^^^^^

.. EXAMPLE START: Timezones in Time Objects

To convert a |Time| object to a timezone-aware datetime object::

  >>> from datetime import datetime
  >>> from astropy.time import Time, TimezoneInfo
  >>> import astropy.units as u
  >>> utc_plus_one_hour = TimezoneInfo(utc_offset=1*u.hour)
  >>> dt_aware = datetime(2000, 1, 1, 0, 0, 0, tzinfo=utc_plus_one_hour)
  >>> t = Time(dt_aware)  # Loses timezone info, converts to UTC
  >>> print(t)            # will return UTC
  1999-12-31 23:00:00
  >>> print(t.to_datetime(timezone=utc_plus_one_hour)) # to timezone-aware datetime
  2000-01-01 00:00:00+01:00

Timezone database packages, like `pytz <https://pythonhosted.org/pytz/>`_
for example, may be more convenient to use to create `~datetime.tzinfo`
objects used to specify timezones rather than the `~astropy.time.TimezoneInfo`
object.

.. EXAMPLE END


Example
^^^^^^^

.. EXAMPLE START: Initializing From a Timezone-aware Date

Using the `dateutil <https://dateutil.readthedocs.io/en/stable/index.html>`_ package,
you can parse times in a wide variety of supported formats to generate a
`datetime.datetime` object which can then be used to initialize a |Time| object::

  >>> from dateutil.parser import parse  # doctest: +SKIP
  >>> dtime = parse('2020-10-29T08:20:46.950+1100')  # doctest: +SKIP
  >>> Time(dtime)  # doctest: +SKIP
  <Time object: scale='utc' format='datetime' value=2020-10-28 21:20:46.950000>

.. EXAMPLE END

Custom String Formats with ``strftime`` and ``strptime``
--------------------------------------------------------

The `~astropy.time.Time` object supports output string representation
using the format specification language defined in the Python standard library
for `time.strftime`. This can be done using the `~astropy.time.Time.strftime`
method.

Examples
^^^^^^^^

.. EXAMPLE START: Custom String Formats with ``strftime`` and ``strptime``

To get output string representation using the `~astropy.time.Time.strftime`
method::

  >>> from astropy.time import Time
  >>> t = Time('2018-01-01T10:12:58')
  >>> t.strftime('%H:%M:%S %d %b %Y')
  '10:12:58 01 Jan 2018'

Conversely, to create a `~astropy.time.Time` object from a custom date string
that can be parsed with Python standard library `time.strptime` (using the same
format language linked above), use the `~astropy.time.Time.strptime` class
method::

  >>> from astropy.time import Time
  >>> t = Time.strptime('23:59:60 30 June 2015', '%H:%M:%S %d %B %Y')
  >>> t
  <Time object: scale='utc' format='isot' value=2015-06-30T23:59:60.000>

.. EXAMPLE END

.. note that if this section gets too long, it should be moved to a separate
   doc page - see the top of performance.inc.rst for the instructions on how to do
   that
.. include:: performance.inc.rst

Reference/API
=============

.. automodapi:: astropy.time
   :inherited-members:


Acknowledgments and Licenses
============================

This package makes use of the PyERFA_ wrappers of the ERFA_ ANSI C library. The copyright of the ERFA_
software belongs to the NumFOCUS Foundation. The library is made available
under the terms of the "BSD-three clauses" license.

The ERFA_ library is derived, with permission, from the International
Astronomical Union's "Standards of Fundamental Astronomy" (SOFA_) library,
available from http://www.iausofa.org.
:orphan:

Changelog
=========

This directory contains "news fragments" which are short files that contain a
small **ReST**-formatted text that will be added to the next what's new page.

Make sure to use full sentences with correct case and punctuation.

Each file should be named like ``<PULL REQUEST>.<TYPE>.rst``, where
``<PULL REQUEST>`` is a pull request number, and ``<TYPE>`` is one of:

* ``feature``: New feature.
* ``api``: API change.
* ``bugfix``: Bug fix.
* ``other``: Other changes and additions.

If the change concern a sub-package, the file should go in the sub-directory
relative to this sub-package.

It is possible to add two files with different categories (and text) if both
are relevant. For example a change may add a new feature but introduce an API
change.

So for example: ``123.feature.rst`` would have the content::

    The ``my_new_feature`` option is now available for ``my_favorite_function``.
    To use it, write ``np.my_favorite_function(..., my_new_feature=True)``.

Note the use of double-backticks for code.

If you are unsure what pull request type to use, don't hesitate to ask in your
PR.

You can install ``towncrier`` and run ``towncrier --draft --version 4.3``
if you want to get a preview of how your change will look in the final release
notes.

.. note::

    This README was adapted from the Numpy changelog readme under the terms of
    the MIT licence.
{% if top_line %}
{{ top_line }}
{{ top_underline * ((top_line)|length)}}
{% elif versiondata.name %}
{{ versiondata.name }} {{ versiondata.version }} ({{ versiondata.date }})
{{ top_underline * ((versiondata.name + versiondata.version + versiondata.date)|length + 4)}}
{% else %}
{{ versiondata.version }} ({{ versiondata.date }})
{{ top_underline * ((versiondata.version + versiondata.date)|length + 3)}}
{% endif %}

{% for category, val in definitions.items() %}

{% set underline = underlines[0] %}
{{ definitions[category]['name'] }}
{{ underline * definitions[category]['name']|length }}
{% set underline = underlines[1] %}

{% for section, _ in sections.items() %}
{% if section and category in sections[section] %}
{{section}}
{{ underline * section|length }}

{% endif %}
{% if sections[section] and category in sections[section] %}
{% if definitions[category]['showcontent'] %}
{% for text, values in sections[section][category].items() %}
- {{ text }} [{{ values|join(', ') }}]

{% endfor %}
{% else %}
- {{ sections[section][category]['']|join(', ') }}

{% endif %}
{% if sections[section][category]|length == 0 %}
No significant changes.

{% else %}
{% endif %}
{% else %}
{# No significant changes. #}
{% endif %}
{% endfor %}
{% endfor %}
Cosmological equivalency (``Cosmology.is_equivalent``) can now be extended
to any Python object that can be converted to a Cosmology, using the new
keyword argument ``format``.
This allows e.g. a properly formatted Table to be equivalent to a Cosmology.
``default_cosmology.get_cosmology_from_string`` is deprecated and will be
removed in two minor versions.
Use ``default_cosmology.get(<str>)`` instead.
Add methods ``Otot`` and ``Otot0`` to FLRW cosmologies to calculate the total
energy density of the Universe.
Register "astropy.row" into Cosmology's to/from format I/O, allowing a
Cosmology instance to be parse from or converted to an Astropy Table Row.
For converting a cosmology to a mapping, two new boolean keyword arguments are
added: ``cosmology_as_str`` for turning the the class reference to a string,
instead of the class object itself, and ``move_from_meta`` to merge the
metadata with the rest of the returned mapping instead of adding it as a
nested dictionary.
A method ``clone`` has been added to ``Parameter`` to quickly deep copy the
object and change any constructor argument.
A supporting equality method is added, and ``repr`` is enhanced to be able to
roundtrip -- ``eval(repr(Parameter()))`` -- if the Parameter's arguments can
similarly roundtrip.
Parameter's arguments are made keyword-only.
Add property ``is_flat`` to cosmologies to calculate the curvature of the Universe.

``Cosmology`` is now an abstract class and subclasses must override the
abstract property ``is_flat``.
Register format "astropy.cosmology" with Cosmology I/O.
Units of redshift are added to ``z_reion`` in built-in realizations' metadata.
Register "yaml" into Cosmology's ``to/from_format`` I/O, allowing
a Cosmology instance to be parsed from or converted to a YAML string.
Cosmology realizations (e.g. ``Planck18``) and parameter dictionaries are now
lazily loaded from source files.
In I/O, conversions of Parameters move more relevant information from the
Parameter to the Column.

The default Parameter ``format_spec`` is changed from ``".3g"`` to ``""``.
The already deprecated ``Planck18_arXiv_v2`` has been removed.
Use ``Planck18`` instead
Improve the performance of ``np.searchsorted`` by a factor of 1000 for a
bytes-type ``Column`` when the search value is ``str`` or an array of ``str``.
This happens commonly for string data stored in FITS or HDF5 format files.
A new keyword-only argument ``kind`` was added to the ``Table.sort`` method to
specify the sort algorithm. The signature of ``Table.sort`` was modified so that
the ``reverse`` argument is now keyword-only. Previously ``reverse`` could be
specified as the second positional argument.
Change the repr of the Table object to replace embedded newlines and tabs with
``r'\n'`` and ``r'\t'`` respectively. This improves the display of such tables.
A new keyword-only argument ``kind`` was added to the ``Table.sort`` method to
specify the sort algorithm.
Built-in Cosmology subclasses can now be converted to/from YAML with the
functions ``dump`` and ``load`` in ``astropy.io.misc.yaml``.
Allow serialization of model unit equivalencies.
Backward-compatible plugin ``astropy.tests.plugins.display``
has been removed; use ``pytest-astropy-header`` instead.
The following are deprecated and will be removed in a future release.
Use ``pytest`` warning and exception handling instead:

* ``astropy.io.ascii.tests.common.raises``
* ``astropy.tests.helper.catch_warnings``
* ``astropy.tests.helper.ignore_warnings``
* ``astropy.tests.helper.raises``
* ``astropy.tests.helper.enable_deprecations_as_exceptions``
* ``astropy.tests.helper.treat_deprecations_as_exceptions``
Backward-compatible import of ``astropy.tests.helper.remote_data``
has been removed; use ``pytest.mark.remote_data`` from ``pytest-remotedata``
instead.
Backward-compatible import of ``astropy.tests.disable_internet``
has been removed; use ``pytest_remotedata.disable_internet``
from ``pytest-remotedata`` instead.
Made ``astropy.modeling.fitting._fitter_to_model_params`` and ``astropy.modeling.fitting._model_to_fit_params``
public methods.
Enable direct use of the ``ignored`` feature of ``ModelBoundingBox`` by users in
addition to its use as part of enabling ``CompoundBoundingBox``.
Add option ``unit_parse_strict`` to `~astropy.io.fits.connect.read_table_fits`
to enable warnings or errors about invalid FITS units when using `~astropy.table.Table.read`.
The default for this new option is ``"warn"``, which means warnings are now raised for
columns with invalid units.
Removed deprecated ``clobber`` argument from functions in ``astropy.io.fits``.
Fixed ``io.ascii`` read and write functions for most formats to correctly handle
data fields with embedded newlines for both the fast and pure-Python readers and
writers.
Implement multiplication and division of LogQuantities and numbers
New ``doppler_redshift`` equivalency to convert between
Doppler redshift and radial velocity.
``structured_to_unstructured`` and ``unstructured_to_structured`` in
``numpy.lib.recfunctions`` now work with Quantity.
``astropy.wcs.Celprm`` and ``astropy.wcs.Prjprm`` have been added
to allow access to lower level WCSLIB functionality and to allow direct
access to the ``cel`` and ``prj`` members of ``Wcsprm``.
