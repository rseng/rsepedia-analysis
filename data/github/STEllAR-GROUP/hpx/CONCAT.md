<!-- Copyright (c) 2014 John Biddiscombe                                          -->
<!--                                                                              -->
<!-- SPDX-License-Identifier: BSL-1.0                                             -->
<!-- Distributed under the Boost Software License, Version 1.0. (See accompanying -->
<!-- file LICENSE_1_0.txt or copy at http://www.boost.org/LICENSE_1_0.txt)        -->

Not Finished yet.


The network storage benchmark comes with several files

##################################################
# What the files are for
##################################################

------------------------------
network_storage.cpp
------------------------------
This is the code for the test.

------------------------------
network_storage.bat.in
------------------------------
This is a windows batch file template which will be filled in using cmake substitution
and then copied to your build/scripts directory.
The generated script can be invoked to run the benchmark in serial or parallel.
It spawns each task on the same node so although you may run 4 copies,
they are competing for resources.
The windows version is good for testing and debugging on a single node.
adding or removing /B for the launch command in the script
controls whether each task is launched in a new console window or not
example usage (2 tasks)
scripts/network_storage.bat 2

------------------------------
slurm-test-HPX-storage.sh.in
------------------------------
This is a bash script template which will be filled in using cmake substitution
and then copied to your build/scripts directory.
When executed, the script will loop over a number of parameter combinations
and for each create a directory with a name generated from the parameters, such as
  hpx-N0002-T02048-t04-ibverbs
and inside the dir there will be a "submit-job.bash" script which contains a single
slurm job submission. You can manually submit just one job using "sbatch submit-job.bash",
or in the root of the scripts folder there will be generated another script
which is called "run_jobs.bash". When this script is run, it will loop over
the jobs that were created by the first script and submit them all, you
can then sit back and wait until they complete.
Each job will write its results into "slurm.out" (and errors to slurm.err)
in the same subdirectory in which each individual job submission script was created.

The network_storage executable will produce several lines of output, but one line
contains the condensed information needed by the plotting script.
This line begins with the text "CSVData" to indicate comma separated data values.
Whilst jobs are running or when they have completed, you can execute a command
from the test root dir, such as
  find . -name slurm.out -exec grep CSV {} \;
and a list of results generated from the jobs will be produced.
For plotting of results, the output should be directed into a file using
  find . -name slurm.out -exec grep CSV {} \; >results-bgq-1a-2014-04-01.csv
Where you use a file name applicable to your current experiment.
The generated file will be suitable for use by the plotting script.

Important Note:
By default, the slurm jobs generated by the script have the 'exclusive' flag set.
This is because the jobs are intended to test BW of the network and it is
often desirable  to have only a single job running at a time.
The 'exclusive' flag only works (under slurm) if all jobs have the same name
so in your queue you will see many identical jobs and it will take a long time to drain.
If you can afford to run many job simultaneously, the job name may be set more
appropriately (details in script) and the exclusive flag removed.

------------------------------
plot-results.py
------------------------------
This is a python script which takes results generated by the test program
as described above and plots a number of graphs for different parameter studies.
The results.csv file generated contains BW measurements, timing ,thread, parcelport,
block size, etc information for the plots.
The python script parses the results and generates arrays (maps) of the data which
can be plotted in various ways. The scripts can be invoked as
  plot-results.py results.csv
optional arguments such as the figure size can be found by looking at the script.
The output from the script will be a series of svg and png files for each of the plots
created.

## Launch notes
greina14
bin/network_storage --localMB=1024 --transferKB=16384 --hpx:localities=2 -Ihpx.parcel.tcp.enable=1 --hpx:agas=148.187.83.44:7910 --hpx:hpx=148.187.83.44:7910 -Ihpx.parcel.tcp.enable=1 -Ihpx.parcel.mpi.enable=0 -Ihpx.parcel.verbs.enable=1 -Ihpx.parcel.bootstrap=tcp

greina15
bin/network_storage --localMB=1024 --transferKB=16384 --hpx:localities=2 -Ihpx.parcel.tcp.enable=1  --hpx:agas=148.187.83.44:7910 --hpx:worker --hpx:hpx=148.187.83.45:7910 -Ihpx.parcel.tcp.enable=1 -Ihpx.parcel.mpi.enable=0 -Ihpx.parcel.verbs.enable=1 -Ihpx.parcel.bootstrap=tcp

greina1
gdb --args /home/biddisco/build/hvtkm/bin/network_storage --localMB=64 --transferKB=1024 --hpx:localities=2 -Ihpx.parcel.tcp.enable=1 --hpx:agas=192.168.3.31:7910 --hpx:hpx=192.168.3.31:7910 -Ihpx.parcel.tcp.enable=1 -Ihpx.parcel.mpi.enable=0 -Ihpx.parcel.verbs.enable=1 -Ihpx.parcel.bootstrap=tcp --hpx:threads=1

greina2
gdb --args /home/biddisco/build/hvtkm/bin/network_storage --localMB=64 --transferKB=1024 --hpx:localities=2 -Ihpx.parcel.tcp.enable=1 --hpx:agas=192.168.3.31:7910 --hpx:hpx=192.168.3.32:7910 -Ihpx.parcel.tcp.enable=1 -Ihpx.parcel.mpi.enable=0 -Ihpx.parcel.verbs.enable=1 -Ihpx.parcel.bootstrap=tcp --hpx:worker --hpx:threads=1

mpi launch
mpiexec -n 2 -host greina14,greina15 bin/network_storage --localMB=1024 --transferKB=16384 --iterations=10 --hpx:localities=2 -Ihpx.parcel.tcp.enable=1 -Ihpx.parcel.tcp.enable=0 -Ihpx.parcel.mpi.enable=1 -Ihpx.parcel.verbs.enable=0 -Ihpx.parcel.bootstrap=mpi --hpx:threads=12

test TCP greina14
bin/network_storage --localMB=1024 --transferKB=16384 --hpx:localities=2 -Ihpx.parcel.tcp.enable=1 --hpx:agas=148.187.83.44:7910 --hpx:hpx=148.187.83.44:7910 -Ihpx.parcel.tcp.enable=1 -Ihpx.parcel.mpi.enable=0 -Ihpx.parcel.verbs.enable=0 -Ihpx.parcel.bootstrap=tcp --iterations=10  --hpx:threads=12

test TCP greina15
bin/network_storage --localMB=1024 --transferKB=16384 --hpx:localities=2 -Ihpx.parcel.tcp.enable=1  --hpx:agas=148.187.83.44:7910 --hpx:worker --hpx:hpx=148.187.83.45:7910 -Ihpx.parcel.tcp.enable=1 -Ihpx.parcel.mpi.enable=0 -Ihpx.parcel.verbs.enable=0 -Ihpx.parcel.bootstrap=tcp --iterations=10  --hpx:threads=12
<!-- Copyright (c) 2017 Hartmut Kaiser                                            -->
<!--                                                                              -->
<!-- SPDX-License-Identifier: BSL-1.0                                             -->
<!-- Distributed under the Boost Software License, Version 1.0. (See accompanying -->
<!-- file LICENSE_1_0.txt or copy at http://www.boost.org/LICENSE_1_0.txt)        -->
## Support for deploying and using HPX

Welcome to [HPX](http://stellar-group.org/libraries/hpx/)! We use GitHub for
[tracking bugs and feature requests](https://github.com/STEllAR-GROUP/hpx/issues).
Please see for the resources below if you are looking for the right place to
get support for using HPX.

### Documentation

* [User Documentation](https://hpx-docs.stellar-group.org/latest/html/index.html)


### Real-time Chat

* #ste||ar on [Libera.Chat](https://libera.chat/): this is the best option
  to get in contact with us as most of the developers are available on this
  channel. Check out the [channel archives](http://irclog.cct.lsu.edu/ste~b~~b~ar)
  for past conversations. The channel can also be accessed through
  [Matrix](https://matrix.to/#/#ste||ar:libera.chat).
* Slack ([registration](https://cpplang.now.sh/)): The
  [`#hpx`](https://cpplang.slack.com/messages/C68QLPZB3) channel is another
  place where people usually offer support.


### Mailing Lists/Groups

* [hpx-users group](mailto:hpx-users@stellar-group.org), see
  [here](https://www.mail-archive.com/hpx-users@stellar-group.org) for past
  conversations. This is the main mailing list for user questions and HPX
  announcements.
* [hpx-devel group](mailto:hpx-devel@stellar-group.org), see
  [here](https://www.mail-archive.com/hpx-devel@stellar-group.org) for past
  conversations. This is the main mailing list for development discussion
  and voting.
* [hpx-pmc group](mailto:hpx-pmc@stellar-group.org). The project management
  committee (PMC) can be reached at this address.

### Latest News

* For latest news and announcements please see our
  [blog](http://hpx.stellar-group.org/blog/).

### Stack Overflow

The HPX Community is active on Stack Overflow, you can post your questions there:

* [HPX on Stack Overflow](http://stackoverflow.com/questions/tagged/hpx)

  * Here are some tips for
    [about how to ask good questions](http://stackoverflow.com/help/how-to-ask).
  * Don't forget to check to see
    [what's on topic](http://stackoverflow.com/help/on-topic).
## Expected Behavior

... Please describe the behavior you would have expected.

## Actual Behavior

... Please describe the behavior you actually observed.


## Steps to Reproduce the Problem

... Please be as specific as possible while describing how to reproduce your problem.

  1.
  1.
  1.

## Specifications

... Please describe your environment

  - HPX Version:
  - Platform (compiler, OS):

<!-- Copyright (c) 2017 Hartmut Kaiser                                            -->
<!--                                                                              -->
<!-- SPDX-License-Identifier: BSL-1.0                                             -->
<!-- Distributed under the Boost Software License, Version 1.0. (See accompanying -->
<!-- file LICENSE_1_0.txt or copy at http://www.boost.org/LICENSE_1_0.txt)        -->

# HPX Code of Conduct

Like the technical community as a whole, the HPX team and community is made up
of a mixture of professionals and volunteers from all over the world, working
on every aspect of the mission - including mentorship, teaching, and connecting
people.

Diversity is one of our huge strengths, but it can also lead to communication
issues and unhappiness. To that end, we have a few ground rules that we ask
people to adhere to. This code applies equally to founders, mentors and those
seeking help and guidance.

This isn't an exhaustive list of things that you can't do. Rather, take it in
the spirit in which it's intended - a guide to make it easier to enrich all of
us and the technical communities in which we participate.

This code of conduct applies to all spaces managed by the
[HPX](https://github.com/STEllAR-GROUP/hpx) project or
[STE||AR Group](http://stellar-group.org). This includes IRC, the mailing
lists, the issue tracker, and any other forums created by the project team
which the community uses for communication. In addition, violations of this
code outside these spaces may affect a person's ability to participate within
them.

If you believe someone is violating the code of conduct, we ask that you report
it by emailing conduct@stellar-group.org.

* **Be friendly and patient**.
* **Be welcoming.** We strive to be a community that welcomes and supports
  people of all backgrounds and identities. This includes, but is not limited
  to members of any race, ethnicity, culture, national origin, colour,
  immigration status, social and economic class, educational level, sex, sexual
  orientation, gender identity and expression, age, size, family status,
  political belief, religion, and mental and physical ability.
* **Be considerate.** Your work will be used by other people, and you in turn
  will depend on the work of others. Any decision you take will affect users
  and colleagues, and you should take those consequences into account when
  making decisions. Remember that we're a world-wide community, so you might
  not be communicating in someone else's primary language.
* **Be respectful.** Not all of us will agree all the time, but disagreement
  is no excuse for poor behavior and poor manners. We might all experience some
  frustration now and then, but we cannot allow that frustration to turn into a
  personal attack. It's important to remember that a community where people
  feel uncomfortable or threatened is not a productive one. Members of the HPX
  community should be respectful when dealing with other members as well as
  with people outside the HPX community.
* **Be careful in the words that you choose.** We are a community of
  professionals, and we conduct ourselves professionally. Be kind to others.
  Do not insult or put down other participants. Harassment and other
  exclusionary behavior aren't acceptable. This includes, but is not limited
  to:
  * Violent threats or language directed against another person.
  * Discriminatory jokes and language.
  * Posting sexually explicit or violent material.
  * Posting (or threatening to post) other people's personally identifying
    information ("doxing").
  * Personal insults, especially those using racist or sexist terms.
  * Unwelcome sexual attention.
  * Advocating for, or encouraging, any of the above behavior.
  * Repeated harassment of others. In general, if someone asks you to stop,
    then stop.

* **When we disagree, try to understand why.** Disagreements, both social and
  technical, happen all the time and HPX is no exception. It is important
  that we resolve disagreements and differing views constructively. Remember
  that we're different. The strength of HPX comes from its varied community,
  people from a wide range of backgrounds. Different people have different
  perspectives on issues. Being unable to understand why someone holds a
  viewpoint doesn't mean that they're wrong. Don't forget that it is human to
  err and blaming each other doesn't get us anywhere. Instead, focus on
  helping to resolve issues and learning from mistakes.

Original text courtesy of the [Django](https://www.djangoproject.com/conduct/) project.
<!-- Copyright (c) 2014-2020 Hartmut Kaiser                                       -->
<!--                                                                              -->
<!-- SPDX-License-Identifier: BSL-1.0                                             -->
<!-- Distributed under the Boost Software License, Version 1.0. (See accompanying -->
<!-- file LICENSE_1_0.txt or copy at http://www.boost.org/LICENSE_1_0.txt)        -->

This describes how you can contribute to [HPX](https://github.com/STEllAR-GROUP/hpx).
Great to have you here. There are a few ways you can help make HPX better!

# How to Get Involved in Developing HPX

This page describes how you can get yourself involved with the development of
HPX. Here are some easy things to do.

All of the HPX development and the related discussions happen through the
[Github ticket system](https://github.com/STEllAR-GROUP/hpx/issues). We are
looking forward to contributions submitted through the usual Github process,
i.e. pull requests.

The easiest ways to get in contact with us are listed here:

* Mailing list: [hpx-users@stellar.cct.lsu.edu](email:hpx-users@stellar.cct.lsu.edu), [hpx-devel@stellar.cct.lsu.edu](email:hpx-devel@stellar.cct.lsu.edu)
* IRC channel:  #ste||ar on irc.freenode.net
* Blog:         [stellar.cct.lsu.edu](stellar.cct.lsu.edu)

The basic approach is to find something fun you want to fix, hack it up, and
send a `git diff` as a mail attachment to [hpx-devel@stellar.cct.lsu.edu](email:hpx-devel@stellar.cct.lsu.edu)
with a Subject prefixed with 'PATCH', as well as: "made available under the Boost
Software License V1" license statement. We also need a real name for the git
commit logs if you usually use an alias. Alternatively, you can create a pull
request from your HPX repository you cloned on Github (see below).

It should be easy!

If you create new files, please use our License Header:

    //  Copyright (c) <year> <your name>
    //
    //  Distributed under the Boost Software License, Version 1.0. (See accompanying
    //  file LICENSE_1_0.txt or copy at http://www.boost.org/LICENSE_1_0.txt)

Please avoid larger reformatting of the code for the time being (except for the
tasks listed below).

If the task is really quick and easy, 'just do it'. However, if you think it
will take you some time, and/or need partitioning (eg. some big, scalable
cleanup that many people can help out with), then please:

* add a comment to the ticket that you're starting work on it
* please provide updates each week or so, otherwise someone else may take the issue
* please take only one part of the task at a time.

If a task has an owner without an update in a week, feel free to notify them
that you're taking that on yourself, and of course if you realize you can't
complete a task - please update it in the
[ticket system](https://github.com/STEllAR-GROUP/hpx/issues).

Even if you are deeply skilled, please consider doing one little easy hack, to
get used to the process. After that, you are invited to move on up to the more
difficult tasks, leaving some of the easy tasks to others so they can get
involved and achieve change themselves. The quicker you move up the pile, the
more quickly you can be making large scale, user-visible changes and
improvements to HPX - of which these easy hacks are just the tip of a very
interesting iceberg.

Before we get to the list of possible tasks, here is some additional
information to get you started.

## Getting Started

### Get a login on Github [here](https://github.com/) and fork the HPX repository to your Github account.

All new and old bugs in HPX can be found in our
[ticket system](https://github.com/STEllAR-GROUP/hpx/issues). Especially with
new incoming bugs, it is helpful to test the bug on your own computer/operating
system and comment in the bug entry whether you can or cannot confirm the bug
and under what circumstances it affects you.

### Getting a build - if necessary

Some but not all tasks require you to have built HPX. Even if that is not
required, your feedback can be helpful to us - so - please try. The master
build instructions are [here](https://stellar-group.github.io/hpx/docs/sphinx/branches/master/html/quickstart.html)
with more stuff under development.

### Hacking help

If you need to search constructs in the code, there is a code search engine at
at the top of this page.

## General info

We use the [Boost coding standards](http://www.boost.org/development/requirements.html#Guidelines)
for our work on HPX

The short version of the guidelines:

* 80-character lines.
* Absolutely no tabs (use spaces instead of tabs).
* Because we use git, UNIX line endings.
* Identifiers are C++ STL style: no CamelCase. E.g. `my_class` instead of `MyClass`.
* Use expressive identifiers.
* Exceptions for error handling instead of C-style error codes.

A more elaborate description of our coding guidelines can be found
[here](https://github.com/STEllAR-GROUP/hpx/wiki/HPX-Source-Code-Structure-and-Coding-Standards).

There is a `.editorconfig` file in the HPX root directory which can be used
for almost any widely available editor. Please see
[their webpage](http://editorconfig.org) to download plugins for your favorite
editor.

There is a `.clang-format` file in the HPX root directory which you can use to
manually format the code you contribute. This configuration file can be used
with [clang-format](https://clang.llvm.org/docs/ClangFormat.html), a tool created
by the [Clang](https://clang.llvm.org/) project.

Please follow the following guidelines for using it:

* You should use this file for creating an initial formatting for new files.
* Please separate edits which are pure formatting into isolated commits
  keeping those distinct from edits changing any of the code.
* Please do _not_ configure your editor to automatically format the source
  file while saving edits to disk.
* Please do _not_ reformat a full source file without dire need.

A few additional ones:

* Use doxygen style comments to document API functions.
* Before writing a piece of utility code, see if there is something in
  `hpx::util`, Boost or the C++ standard library that can be used to save time.

# Community

Community is an important part of all we do.

* The HPX project is a meritocratic, consensus-based community project. Anyone
  with an interest in the project can join the community, contribute to the
  project design and participate in the decision making process.
  `This document <http://hpx.stellar-group.org/documents/governance/>`_ describes
  how that participation takes place and how to set about earning merit within
  the project community.
* You can help us answer questions our users have by being around on IRC
  (#ste||ar on irc.freenode.net) or by chiming in on the
  [users mailing list](email:hpx-users@stellar.cct.lsu.edu)
* You can help write blog posts (for [stellar.cct.lsu.edu](stellar.cct.lsu.edu))
  about things you're doing with HPX. We can give you access or help with
  posting things.
* Create an example of how to use HPX in the real world by building something
  or showing what others have built.
* Write about other people's work based on HPX. Show how it is used in daily
  life. Take screenshots and make videos!


# Your first bugfix

For our project, you can talk to the following people to receive help in
working through your first bugfix and thinking through the problem:

* @hkaiser, @heller, @wash, @jbjnr

Fixes #

## Proposed Changes

  -
  -
  -

## Any background context you want to provide?

## Checklist

Not all points below apply to all pull requests.

- [ ] I have added a new feature and have added tests to go along with it.
- [ ] I have fixed a bug and have added a regression test.
- [ ] I have added a test using random numbers; I have made sure it uses a seed, and that random numbers generated are valid inputs for the tests.
<!-- Copyright (c) 2018 Thomas Heller                                             -->
<!--                                                                              -->
<!-- SPDX-License-Identifier: BSL-1.0                                             -->
<!-- Distributed under the Boost Software License, Version 1.0. (See accompanying -->
<!-- file LICENSE_1_0.txt or copy at http://www.boost.org/LICENSE_1_0.txt)        -->

This example demonstrates a pipeline split up in 3 processes:
 - emitter.cpp:
    * Process producing input for worker. This serves as the master process
 - worker.cpp:
    * Process working on inputs. There can be multiple instances of this process
 - collector.cpp:
    * Process collecting the result of the workers

When using the MPI parcelport, the example can be run like this:

```
mpirun -np 1 ./bin/emitter : -np 1 ./bin/collector : -np N-1 ./bin/worker
```

For elasticity, the applicate can be started as following:

```
host0$ ./bin/emitter --hpx:hpx=<host0> --hpx:console
host1$ ./bin/collector --hpx:hpx=<host1> --hpx:agas=<host0> --hpx:connect --hpx:run-hpx-main
host2$ ./bin/worker --hpx:hpx=<host2> --hpx:agas=<host0> --hpx:connect --hpx:run-hpx-main
host3$ ./bin/worker --hpx:hpx=<host3> --hpx:agas=<host0> --hpx:connect --hpx:run-hpx-main
...
hostN$ ./bin/worker --hpx:hpx=<hostN> --hpx:agas=<host0> --hpx:connect --hpx:run-hpx-main
```
<!-- Copyright (c) 2013 Thomas Heller                                             -->
<!--                                                                              -->
<!-- SPDX-License-Identifier: BSL-1.0                                             -->
<!-- Distributed under the Boost Software License, Version 1.0. (See accompanying -->
<!-- file LICENSE_1_0.txt or copy at http://www.boost.org/LICENSE_1_0.txt)        -->

This is a version of a Jacobi supposed to run on shared memory machines.
It based on the dataflow ideas as presented in this paper:
http://dl.acm.org/citation.cfm?id=2467126

The example consists of 2 parts, each of the part provides a jacobi smoother
implemented in HPX and OpenMP, while the OpenMP variant includes one with
static and one with dynamic scheduling policies.

The first variant smoothes a regular two-dimensional grid with a simple
5 point stencil. The parameters are the number of grid points in one dimension
and for the HPX version, a block-size parameter which determines the
granularity of the work done. The relevant executables are:
  * jacobi_hpx
  * jacobi_omp_static
  * jacobi_omp_dynamic

The second variant performs a dynamic stencil based on the neighborhood
given by a sparse matrix. The matrix input format is "Matrix Market".
An example matrix can be obtained here:
http://www.cise.ufl.edu/research/sparse/matrices/Janna/Serena.html
Other matrices from that portal work as well.
The relevant executables are:
  * jacobi_nonuniform_hpx
  * jacobi_nonuniform_omp_static
  * jacobi_nonuniform_omp_dynamic

<!-- Copyright (c) 2014 Thomas Heller                                             -->
<!--                                                                              -->
<!-- SPDX-License-Identifier: BSL-1.0                                             -->
<!-- Distributed under the Boost Software License, Version 1.0. (See accompanying -->
<!-- file LICENSE_1_0.txt or copy at http://www.boost.org/LICENSE_1_0.txt)        -->

This directory contains python script to help debugging HPX applications with
gdb. In order to use it, please add the following lines to your ~/.gdbinit:

    source $HPX_SRC_DIR/tools/gdb/hpx.py

    define hook-continue
    hpx thread restore
    end

For a list of commands see `help hpx` inside of gdb.

Notes:

 - The scripts currently only work when HPX is compiled against Boost 1.56
 - The scripts currently only work with the local priority scheduler
 - The scripts currently only work on 64 bit executables
 - The scripts currently only work with the `x86_linux_context` context
   implementation (which is the default on Linux X86_64)
 - The hook-continue is needed because we currently just overwrite the current
   frame with the selected HPX user level context.

By not doing a `hpx thread restore` before gdb continues execution, your
program will abort.
﻿---
title: 'HPX - The C++ Standard Library for Parallelism and Concurrency'
tags:
 - concurrency
 - task-based run time system
 - parallelism
 - distributed
authors:
 - name: Hartmut Kaiser
   orcid: 0000-0002-8712-2806
   affiliation: "1"
 - name: Patrick Diehl
   orcid: 0000-0003-3922-8419
   affiliation: "1"
 - name: Adrian S. Lemoine
   affiliation: "6"
 - name: Bryce Adelstein Lelbach
   orcid: 0000-0002-7995-5226
   affiliation: "5"
 - name: Parsa Amini
   orcid: 0000-0002-6439-8404
   affiliation: "1"
 - name: Agustín Berge
   affiliation: "6"
 - name: John Biddiscombe
   orcid: 0000-0002-6552-2833
   affiliation: "4"
 - name: Steven R. Brandt
   orcid: 0000-0002-7979-2906
   affiliation: "1"
 - name: Nikunj Gupta
   orcid: 0000-0003-0525-3667
   affiliation: "3"
 - name: Thomas Heller
   orcid: 0000-0003-2620-9438
   affiliation: "2"
 - name: Kevin Huck
   orcid: 0000-0001-7064-8417
   affiliation: "8"
 - name: Zahra Khatami
   orcid: 0000-0001-6654-6856
   affiliation: "7"
 - name: Alireza Kheirkhahan
   orcid: 0000-0002-4624-4647
   affiliation: "1"
 - name: Auriane Reverdell
   orcid: 0000-0002-5531-0458
   affiliation: "4"
 - name: Shahrzad Shirzad
   orcid: 0000-0001-9496-8044
   affiliation: "1"
 - name: Mikael Simberg
   orcid: 0000-0002-7238-8935
   affiliation: "4"
 - name: Bibek Wagle
   orcid: 0000-0001-6619-7115
   affiliation: "1"
 - name: Weile Wei
   orcid: 0000-0002-3065-4959
   affiliation: "1"
 - name: Tianyi Zhang
   orcid: 0000-0002-1000-4887
   affiliation: "6"
affiliations:
 - name: Center for Computation \& Technology, Louisiana State University, LA, Baton Rouge, United States of America
   index: 1
 - name: Exasol, Erlangen, Germany
   index: 2
 - name: Indian Institute of Technology, Roorkee, India
   index: 3
 - name: Swiss National Supercomputing Centre, Lugano, Switzerland
   index: 4
 - name: NVIDIA, CA, Santa Clara, United States of America
   index: 5
 - name: STE$||$AR Group
   index: 6
 - name: Oracle, CA, Redwood City, United States of America
   index: 7
 - name: Oregon Advanced Computing Institute for Science and Society (OACISS), University of Oregon, OR, Eugene, United States of America
   index: 8
date: 26.08.2020
bibliography: paper.bib
---

# Summary

The new challenges presented by exascale system architectures have resulted in
difficulty achieving the desired scalability using traditional distributed-memory
runtimes. Asynchronous many-task systems (AMT) are based on a new paradigm
showing promise in addressing these challenges, providing application
developers with a productive and performant approach to programming on next
generation systems.

HPX is a C++ Library for concurrency and parallelism that is
developed by The STE||AR Group, an international group of collaborators working
in the field of distributed and parallel programming
[@heller2017hpx;@hpx_github;@tabbal2011preliminary]. It is a runtime system
written using modern C++ techniques that are linked as part of an application.
HPX exposes extended services and functionalities supporting the implementation
of parallel, concurrent, and distributed capabilities for applications in any
domain; it has been used in scientific computing, gaming, finances, data
mining, and other fields.

The HPX AMT runtime system attempts to solve some problems the community
is facing when it comes to creating scalable parallel applications that expose
excellent parallel efficiency and a high resource utilization. First, it exposes
a C++ standards conforming API that unifies syntax and semantics for local and
remote operations. This significantly simplifies writing codes that strive to
effectively utilize different types of available parallelism in today's machines
in a coordinated way (i.e., on-node, off-node, and accelerator-based parallelism).
Second, HPX implements an asynchronous C++ standard programming model that has the
emergent property of semi-automatic parallelization of the user's code. The
provided API (especially when used in conjunction with the new C++20 `co_await`
keyword [@standard2020programming]) enables intrinsic overlap of computation and
communication, prefers moving work to data over moving data to work, and exposes
minimal overheads from its lightweight threading subsystem, ensuring efficient
fine-grained parallelization and minimal-overhead synchronization and context
switching. This programming model natively ensures high-system utilization and
perfect scalability.

A detailed comparison of HPX with various other AMTs is given in [@thoman2018taxonomy].
Some notable AMT solutions are: Uintah [@germain2000uintah], Chapel [@chamberlain2007parallel],
Charm++ [@kale1993charm], Kokkos [@edwards2014kokkos], Legion [@bauer2012legion],
and PaRSEC [@bosilca2013parsec]. Note that we only refer to distributed memory solutions,
since this is an important feature for scientific applications to run large scale simulations.
The major showpiece of HPX compared to the mentioned distributed AMTs is its future-proof C++
standards conforming API and the exposed asynchronous programming model.

HPX's main goal is to
improve efficiency and scalability of parallel applications by increasing
resource utilization and reducing synchronization overheads through providing an
asynchronous API and employing adaptive scheduling. The consequent use of
_Futures_ intrinsically enables overlap of computation and communication and
constraint-based synchronization. HPX is able to maintain a balanced load among
all the available resources resulting in significantly reducing processor
starvation and effective latencies while controlling overheads. HPX fully
conforms to the C++ ISO standards and implements the standardized concurrency
mechanisms and parallelism facilities. Further, HPX extends those facilities to
distributed use cases, thus enabling syntactic and semantic equivalence of local
and remote operations on the API level. HPX uses the concept of C++ _Futures_ to
transform sequential algorithms into wait-free asynchronous executions.
The use of _Futurization_ enables the automatic creation of dynamic data flow
execution trees of potentially millions of lightweight HPX tasks executed in the
proper order. HPX also provides a work-stealing task scheduler that takes care
of fine-grained parallelizations and automatic load balancing. Furthermore,
HPX implements functionalities proposed as part of the ongoing C++
standardization process.

![Sketch of HPX's architecture with all the components and their interactions.\label{fig:architecture}](hpx_architecture.pdf)


\autoref{fig:architecture} sketches HPX's architecture. The components of HPX
and their references are listed below:

**Threading Subsystem** [@kaiser2009parallex] The thread manager manages the
 light-weight user level threads created by HPX. These light-weight threads
 have extremely short context switching times, resulting in reduced latencies
 even for very short operations. This also ensures reduced synchronization
 overheads for coordinating execution between different threads. HPX provides
 a set of scheduling policies that enable the user to flexibly customize the
 execution of HPX threads. Work-stealing and work-sharing policies ensure
 automatic local load balancing of tasks, which is important for achieving high
 system utilization and good scalability of the user's code.

**Active Global Address Space (AGAS)** [@kaiser2014hpx;@amini2019agas]
 To support distributed objects, HPX supports a component for resolving
 global addresses that extends the Partitioned Global Address Space
 (PGAS) model, enabling dynamic runtime-based resource allocation and
 data placement.
 This layer enables HPX to expose a uniform API for local and remote
 execution. Unlike PGAS, AGAS provides the user with the ability to
 transparently move global objects in between nodes of a distributed computer
 system without changing the object's global address. This capability is
 fundamental for supporting load balancing via object migration.

**Parcel Transport Layer** [@kaiser2009parallex;@biddiscombe2017zero]
 This component is an active-message networking layer.
 The parcelport leverages AGAS in order to deliver messages to and to launch
 functions on global objects regardless of their current placement in a
 distributed system.
 Additionally, its asynchronous protocol enables the
 parcelport to implicitly overlap communication and computation.
 The parcelport is modular to support multiple communication library
 backends. By default, HPX supports TCP/IP, Message Passing Interface (MPI),
 and libfabric [@daiss2019piz].

**Performance counters** [@grubel2016dynamic]
 HPX provides its users with a uniform suite of globally accessible
 performance counters to monitor system metrics *in-situ*. These counters have
 their names registered with AGAS, which enables the users to
 easily query for different metrics at runtime.
 Additionally, HPX provides an API for users to create their
 own application-specific counters to gather information customized to their
 own application. These user-defined counters are exposed through the same
 interface as their predefined counterparts.
 By default, HPX provides performance counters for its own components, such as
 networking, AGAS operations, thread scheduling, and various statistics.

**Policy Engine/Policies** [@huck2015autonomic;@khatami2017hpx;@laberge2019scheduling]
 Often, modern applications must adapt to runtime environments
 to ensure acceptable performance. Autonomic Performance Environment for
 Exascale (APEX) enables this flexibility by measuring HPX tasks, monitoring
 system utilization, and accepting user provided policies
 that are triggered by defined events.
 In this way, features such as parcel coalescing [@wagle2018methodology] can
 adapt to the current phase of an application or even state of a system.

**Accelerator Support**
 HPX has support for several methods of integration with GPUs:
 HPXCL [@diehl2018integration; @martin_stumpf_2018_1409043] and HPX.Compute
 [@copik2017using].
 HPXCL provides users the ability to manage GPU kernels through a
 global object. This enables HPX to coordinate the launching and
 synchronization of CPU and GPU code.
 HPX.Compute [@copik2017using] aims to provide a single-source
 solution to heterogeneity by automatically generating GPU kernels
 from C++ code. This enables HPX to launch both CPU and GPU kernels
 as dictated by the current state of the system. Support for integrating
 HPX with Kokkos [@edwards2014kokkos] is currently being developed. This
 integration already has added HPX as an asynchronous backend to Kokkos and
 will expose Kokkos' accelerator functionalities through HPX's asynchronous APIs
 in a C++ standards-conforming way.

**Local Control Objects (synchronization support facilities)**
 HPX has support for many of the C++20 primitives, such as `hpx::latch`,
 `hpx::barrier`, and `hpx::counting_semaphore` to synchronize the execution of
 different threads allowing overlapping computation and communication. These
 facilities fully conform to the C++20 standard [@standard2020programming].
 For asynchronous computing HPX provides `hpx::async` and `hpx::future`; see
 the second example in the next section.

**Software Resilience**
 HPX supports software-level resilience [@gupta2020implementing] through its
 resiliency API, such as `hpx::async_replay` and `hpx::async_replicate` and
 its dataflow counterparts `hpx::dataflow_replay` and
 `hpx::dataflow_replicate`. These APIs are resilient against memory bit
 flips and other hardware errors.
 HPX provides an easy method to port codes to the resilient API by replacing
 `hpx::async` or `hpx::dataflow` with its resilient API counterparts everywhere
 in the code without making any other changes.

**C++ Standards conforming API**
 HPX implements all the C++17 parallel algorithms [@standard2017programming]
 and extends those with asynchronous versions. Here, HPX provides the
 `hpx::execution::seq` and `hpx::execution::par` execution policies, and (as an
 extension) their asynchronous equivalents
 `hpx::execution::seq(hpx::execution::task)` and
 `hpx::execution::par(hpx::execution::task)` (see the first code example
 below). HPX also implements the C++20
 concurrency facilities and APIs [@standard2020programming], such as
 `hpx::jthread`, `hpx::latch`, `hpx::barrier`, etc.

# Applications

HPX is utilized in a diverse set of applications: 

- Scientific computing
   * [Octo-Tiger](https://github.com/STEllAR-GROUP/octotiger)
   [@daiss2019piz;@heller2019harnessing;@pfander2018accelerating], an
   astrophysics code for stellar mergers.
   * [libGeoDecomp](https://github.com/gentryx/libgeodecomp)
   [@Schafer:2008:LGL:1431669.1431721], an auto-parallelizing library to speed
   up stencil-code-based computer simulations.
   * [NLMech](https://github.com/nonlocalmodels) [@diehl2018implementation], a
   simulation tool for non-local models, e.g. Peridynamics.
   * [Dynamical Cluster Approximation](https://github.com/CompFUSE/DCA) (DCA++)
   [@hahner2020dca], a high-performance research software framework to solve 
   quantum many-body problems with cutting edge quantum cluster algorithms. 

- Libraries
   * [hpxMP](https://github.com/STEllAR-GROUP/hpxMP)
   [@zhang2019introduction; @zhang2020supporting] a modern OpenMP implementation
   leveraging HPX that supports shared memory multithread programming. 
   * [Kokkos](https://github.com/kokkos/kokkos) [@10.1016/j.jpdc.2014.07.003],
   the C++ Performance Portability Programming EcoSystem. 
   * [Phylanx](https://github.com/STEllAR-GROUP/phylanx)
   [@tohid2018asynchronous;@wagle2019runtime] An Asynchronous Distributed C++
   Array Processing Toolkit.

For a updated list of applications, we refer to the
corresponding [HPX website](https://hpx.stellar-group.org/hpx-users/).

# Example code

The following is an example of HPX's parallel algorithms API using execution
policies as defined in
the C++17 standard [@standard2017programming]. HPX implements all the
parallel algorithms defined therein. The parallel algorithms extend the classic
STL algorithms by adding a first argument (called execution policy).
The `hpx::execution::seq` implies sequential execution while `hpx::execution::par`
will execute the algorithm in parallel.
HPX's parallel algorithm library API is completely standards conforming.

```cpp
#include <hpx/hpx.hpp>
#include <iostream>
#include <vector>

int main()
{
 std::vector<int> values = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10};

 // Compute the sum in a sequential fashion
 int sum1 = hpx::reduce(
 hpx::execution::seq, values.begin(), values.end(), 0);
 std::cout << sum1 << '\n'; // will print 55

 // Compute the sum in a parallel fashion based on a range of values
 int sum2 = hpx::ranges::reduce(hpx::execution::par, values, 0);
 std::cout << sum2 << '\n'; // will print 55 as well

 return 0;
}
```

Example for the HPX's concurrency API where the Taylor series for the $\sin(x)$
function is computed. The Taylor series is given by,

$$ \sin(x) \approx = \sum\limits_{n=0}^N (-1)^{n-1} \frac{x^{2n}}{(2n)!}.$$

For the concurrent computation, the interval $[0, N]$ is split in two
partitions from $[0, N/2]$ and $[(N/2)+1, N]$, and these are computed
asynchronously using `hpx::async`. Note that each asynchronous function call
returns an `hpx::future` which is needed to synchronize the collection
of the partial results. The future has a `get()` method that returns the result
once the computation of the Taylor function finished. If the result is not ready
yet, the current thread is suspended until the result is ready. Only if
`f1` and `f2` are ready, the overall result will be printed to the standard
output stream.

```cpp
#include <hpx/hpx.hpp>
#include <cmath>
#include <iostream>

// Define the partial taylor function
double taylor(size_t begin, size_t end, size_t n, double x)
{
 double denom = factorial(2 * n);
 double res = 0;
 for (size_t i = begin; i != end; ++i)
 {
 res += std::pow(-1, i - 1) * std::pow(x, 2 * n) / denom;
 }
 return res;
}

int main()
{
 // Compute the Talor series sin(2.0) for 100 iterations
 size_t n = 100;

 // Launch two concurrent computations of each partial result
 hpx::future<double> f1 = hpx::async(taylor, 0, n / 2, n, 2.);
 hpx::future<double> f2 = hpx::async(taylor, (n / 2) + 1, n, n, 2.);

 // Introduce a barrier to gather the results
 double res = f1.get() + f2.get();

 // Print the result
 std::cout << "Sin(2.) = " << res << std::endl;
}
```

Please report any bugs or feature requests on the
[HPX GitHub page](https://github.com/STEllAR-GROUP/hpx).

# Acknowledgments

We would like to acknowledge the National Science Foundation (NSF), the U.S.
Department of Energy (DoE), the Defense Technical Information Center (DTIC), the
Defense Advanced Research Projects Agency (DARPA), the Center for Computation
and Technology (CCT) at Louisiana State University (LSU), the Swiss National
Supercomputing Centre (CSCS), the Department of Computer Science 3 - Computer
Architecture at the University of Erlangen Nuremberg who fund and support our
work, and the Heterogeneous System Architecture (HSA) Foundation.

We would also like to thank the following organizations for granting us
allocations of their compute resources: LSU HPC, Louisiana Optical Network
Iniative (LONI), the Extreme Science and Engineering Discovery Environment
(XSEDE), the National Energy Research Scientific Computing Center (NERSC), the
Oak Ridge Leadership Computing Facility (OLCF), Swiss National Supercomputing
Centre (CSCS/ETHZ), the Juelich Supercomputing Centre (JSC), and the Gauss
Center for Supercomputing.

At the time the paper was written, HPX was directly funded by the following
grants:

- The National Science Foundation through awards 1339782 (STORM) and 1737785
 (Phylanx).

- The Department of Energy (DoE) through the awards DE-AC52-06NA25396 (FLeCSI)
 DE-NA0003525 (Resilience), and DE-AC05-00OR22725 (DCA++).

- The Defense Technical Information Center (DTIC) under contract
 FA8075-14-D-0002/0007.

- The Bavarian Research Foundation (Bayerische Forschungsstiftung) through the
 grant AZ-987-11.

- The European Commission's Horizon 2020 programme through the grant
 H2020-EU.1.2.2. 671603 (AllScale).


For a updated list of previous and current funding, we refer to the
corresponding [HPX website](http://hpx.stellar-group.org/funding-acknowledgements/).

# References
