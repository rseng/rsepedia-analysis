minio miniohttpswwwminioio distribut object storag server built cloud applic devop use minio distribut mode redund prerequisit understand read distribut minio quickstart guidehttpsdocsminioiodocsdistributedminioquickstartguid set minio cluster make sure set minio global variabl use templat var initi web gui avail httpcluster_namehost_domain host part minio group unit test pleas read readm rolesminiotest ansibl ansibl use setup environ set machin setup environ everi time user open bash consol window linux environ set follow command window cd path_to_emmaemma env_linuxsh env_windowssh linux cd path_to_emmaemma env_linuxsh user check role extra step setup environ exampl hadoop role requir gener extra sshkey hadoop user instal ansibl recommend version ansibl howev tri playbook align latest version ansibl instal use ubuntu packag manag aptget instal latest ansibl user follow instal instruct ansibl websitehttpdocsansiblecomansiblelatestintro_installationhtmllatestreleasesviaaptubuntu provis creat host file see hoststempl templat creat default four node cluster run follow command create_hostssh chang ansibl configur user edit ansiblecfg root directori repositori diff file etcansibleansiblecfg show addit default version use ansibl verifi login ansibl u ubuntu host ping cloud base setup skip deploy vagrant disk exampl devvdb dataloc partitionedformattedmount also set up ssh key root ansibleplaybook e datadiskdevvdb e host_namecluster_nam prepcloudplaybookyml apt auto updat playbook fail use follow command clean host kill apt process id dpkg configur role defin ansibl creat platform follow featur glusterfsglustermd miniominiomd sparksparkmd standalon cluster docker swarmdockerswarmmd jupyterhubjupyterhubmd daskrolesdaskreadmemd role deploy respect servic system node group specifi playbookyml node group defin inventori file host playbookyml defin order role execut global variabl role defin var file name role valu set run playbook templat contain default valu role variabl note variabl defin global overwrit one defin role default dir henc chang extend variabl definit use global variabl definit creat var file default valu except minio_varsyml need edit set secret access key run cd var create_vars_filessh variabl defin platform instal follow command ansibleplaybook install_platformyml platform need instal instal servic eg hadoop spark start use follow command ansibleplaybook start_platformyml shutdown platform run follow command ansibleplaybook shutdown_platformyml ansibletagshttpdocsansiblecomansibleplaybooks_tagshtml possibl fine grain control task execut know exist tag run ansibleplaybook install_platform_lightyml listtag current follow tag tag miss pleas fill issu common task role common extra_python_packag task instal python packag use pip extra_system_packag task instal system packag use aptget firewal task updat firewal minio task installstartstop servic relat minio role hadoop task installstartstop servic relat hadoop role spark task installstartstop servic relat spark role jupyterhub task installstartstop servic relat jupyterhub role jupyter_modul task instal extra modul jupyterhub dask task installstartstop servic relat dask role want updat firewal instead run entir instal could ansibleplaybook install_platformyml tag firewal hand want start platform except minio servic could ansibleplaybook start_platformyml skiptag minio updat exist platform case cluster alreadi instal user want updat hadoop spark cluster older newer version user follow edit varshadoop_varsyml set hadoop_prev_vers current_vers hadoop_vers new_vers edit varsspark_varsyml set spark_prev_vers current_vers spark_vers new_vers run instal script hadoop spark ansibleplaybook install_platformyml tag hadoopspark case user want format hdf ansibleplaybook install_platformyml tag hadoopspark skiptag hdfs_format extend exist platform case spark cluster alreadi defin user want add new node necessari recreat cluster user instal spark hadoop hdf new node add hdf data node spark worker first step creat new inventori file simpli make copi current onehttpsgithubcomnlescsherlockemmablobmasteransiblemdprovis add new node address hadoopdatanod sparkwork keep one hadoopnamenod sparkmast remov one cd emma cp host hosts_new edit hosts_new vim hosts_new second step instal system list playbook install_platform_lightyml use tag user decid system instal shutdown cluster ansibleplaybook shutdown_platformyml creat data partit run command node vagrant box ansibleplaybook hosts_new e datadiskdevvdb e host_namecluster_nam prepcloudplaybookyml instal spark hadoop ansibleplaybook hosts_new install_platform_lightyml skiptag hdfs_format spark hadoop instal new node add address origin inventori file host hadoopdatanod sparkwork done user start cluster check list data node visit hadoopnamenode_urldfshealthhtmltabdatanod spark worker visit sparkmaster_url ansibleplaybook start_platformyml final step rebal data hdf use hdf balanc done one cluster node remot user laptop latter user check instruct instal hadoop binari configur ithttpsgithubcomphenologyinfrastructureblobmasterplatformreadmemdhadoopbinari command rebal data cd hadoopbin sudo hdf balanc threshold add new modul new applic user might need instal new modul python modul use either pip aptget librari instal need list emmavarscommon_varsyaml either python_packag system_packag variabl instal user need run emma ansibl playbook pip ansibleplaybook install_platform_lightyml tag extra_python_packag aptget ansibleplaybook install_platform_lightyml tag extra_system_packag also possibl copi user defin modul udm python librari r librari scala jar cluster achiev user need copi modul respect folder emmafilespython scala r call ansibleplaybook install_platform_lightyml tag user_defined_modul udm avail path jupyterhub_modules_dir python scala r default path datalocaljupyterhubmodul remot command execut case user need run specif command one remot host restart network servic user use ansibl option interest user simpli want restart servic howev instal system packag user follow step add new modulehttpsgithubcomnlescsherlockemmablobmasteransiblemdaddnewmodul case new system user creat new ansibl rolehttpdocsansiblecomansiblelatestplaybooks_reuse_roleshtml finegrain access inventori filehttpsgithubcomnlescsherlockemmablobmasteransiblemdprovis often call host entri per node extend inventori file entri user run follow command env_linuxsh f seq calc num_host echo f echo cluster_namef host echo cluster_namehost_domain host done execut remot command user need call ansibl host_group command u ubuntu exampl user want restart network servic host number zero follow ansibl cluster_nam sudo systemctl restart networkingservic u ubuntu demo deploy demo deploy use platform set playbook done use demo sherlock project ansibleplaybook demoyml deploy websit avail httpdockerswarmmanag dockerswarmmanag defin host file describ ansiblemdansiblemd jupyterhub jupyterhub connect spark use toreehttptoreeapacheorg spark spark instal datasharedspark directori spark standalon mode master use sparksparkmast ui httpsparkmast jupyterhub httpsparkmast get shell spark cluster run sparkshel master sparksparkmast stare interact spark recommend read rdd datafram data sethttpsindatalabscomblogdataengineeringconvertsparkrddtodataframedataset program guidehttpsparkapacheorgdocslatestprogrammingguidehtml sparkml instal follow instruct sparkml use spark httpsparkapacheorgdocslatestmlguidehtml depend instal howev sinc spark prebuilt tarbal user awar netlibjava dependencyhttpsparkapacheorgdocslatestmlguidehtmldepend requir includ comgithubfommilnetlibal geotrelli instal follow inform avail geotrelli githubhttpsgithubcomlocationtechgeotrelli repositori doc dir guid use librari sparkhttpsgithubcomlocationtechgeotrellisblobmasterdocsguidesparkrst use sbt maven gradl geotrelli jar download orglocationtechgeotrellishttpsmvnrepositorycomartifactorglocationtechgeotrelli start use geotrelli recommend read core concept geotrellishttpsgeotrellisreadthedocsioenguidecoreconcept exampl python notebook unsupervis classif imageri use scikitlearnhttpnbviewerjupyterorggistomhennersccddabcf exampl show classifi imageri exampl landsat use scikitlearn mani classif method avail exampl use kmean simpl fast use numpyhttpwwwnumpyorg rasteriohttpsgithubcommapboxrasterio debug mode inform debug set spark remot debug perform tune remot debug set spark remot debug user set spark_debug_mod true emmavarsspark_varsyml reconfigur spark executor per worker emmavarsspark_varsyml user set spark_executor_cor equal spark_worker_cor spark_executor_memori equal spark_worker_memori set allow us open singl debugg per executor ie one per node default driver debug port worker debug port defin emmavarsspark_varsyml variabl worker_debug_port driver_debug_port either worker driver wait startup variabl worker_waiting_on_startup driver_waiting_on_startup set ye default set n configur take place user restart spark jupyterhub ansibleplaybook shutdown_platformyml tag sparkjupyterhub ansibleplaybook start_platformyml tag sparkjupyterhub tune garbag collect spark written scala therefor process java virtual machin jvm jvm memori manag done garbag collector gc type gc use defin variabl gc_type user want obtain gc debug inform set emmavarsspark_varsyml gc_debug xxprintflagsfin xxprintreferencegc verbosegc xxprintgcdetail xxprintgctimestamp xxprintadaptivesizepolici tune java garbag collect apach spark applicationshttpsdatabrickscomblogtuningjavagarbagecollectionforsparkapplicationshtml blog post explain memori manag jvm done tune gc spark applic hadoop platform use hadoop hdf file system spark webui access use hadoopnam node address defin inventori filehttpsgithubcomnlescsherlockemmablobmasteransiblemdprovis listen port set hadoop cluster machin master abl passwordless ssh start daemon slave henc user gener sshkey name hadoop_id_rsa key store directori file locat root directori cd file sshkeygen rsa f hadoop_id_rsa vagrant vagrant tool emul cluster machin version recommend requir user need virtualboxhttpswwwvirtualboxorgwikidownload instal version recommend instal linux system simpl packag instal enough ubuntu wget httpsreleaseshashicorpcomvagrantvagrant__x_deb sudo dpkg vagrant__x_deb window despit ubuntu environmentwindow set run ansibl vagrant need instal execut use cmd consol instal download msi file httpswwwvagrantupcomdownloadshtml sometim directori ownership issu vagrant instal solv requir click properti claim ownership directori instal proceed despit instal use cmd consol vagrantex call use ubuntu environmentwindow environ variabl need set creat env_linuxsh run env_windowssh ubuntu environmentwindow use vagrantex import make sure home directori vagrant write permiss window user window rootadministr creat edit env_linuxshtempl cp env_linuxshtempl env_linuxcmd edit vim env_linuxcmd ubuntu bash window run requir restart consol environ variabl set env_windowssh note despit ubuntu environmentwindow use run vagrant user call vagrant command vagrantex plugin vagrant need two plugin instal vagrant_hom plugin persist storag vagrantex plugin instal vagrantpersistentstorag plugin manag host vagrantex plugin instal vagranthostmanag recommend instal vbguest plugin vagrantex plugin instal vagrantvbguest vm manag window run vagrant command simpli use ubuntu bash consol creat virtual machin start use vagrantex updat guest machin etchost user vagrant alway run vagrantex hostmanag linux host machin etchost automat updat window ubuntu environmentwindow etchost guest node ip need retriev hand vagrant hostmanag run sh gethostssh halt vm vagrantex halt destroy vm vagrantex destroy case vagrant need set use privat network due issu get ip public network option networktypeprivate_network use vagrantex networktypeprivate_network use vagrant set public network default switch public privat network vice versa requir vagrantex halt vagrantex recommend use vagrantex reload check verifi login n host ssh cluster_namekey rootcluster_namehost_domain uptim ssh cluster_namekey rootcluster_namenhost_domain uptim emma doihttpszenodoorgbadgedoizenodosvghttpsdoiorgzenodo emma project creat platform develop applic spark dockerswarm cluster platform run infrastructur compos virtual machin ansibl playbook use creat storag layer process layer jupyterhubhttpsjupyternotebookreadthedocsioenlatestindexhtml servic storag layer offer two flavor storag filebas glusterfshttpswwwglusterorg hadoop distribut file system hdfshttphadoopapacheorg objectbas use miniohttpswwwminioio process layer apach spark clusterhttpsparkapacheorg docker swarmhttpsdocsdockercomengineswarm share storag instanc deploy moment deploy cluster emma test two oss environ linux window linux host os linux test use ubuntu ubuntu howev recommend use latter linux special environ setup requir deploy cluster user need clone emma repositori git clone httpsgithubcomnlescsherlockemma window host os window emma test window use embed ubuntu environ setup straight forward simpl follow step list instal guidehttpsmsdnmicrosoftcomenuscommandlinewslinstall_guid instal recommend verifi version instal user simpli need type bash search window left bottom window desktopenviron press enter bash consol open user type lsb_releas follow powershel administr enter command lxrun uninstal full lxrun instal verifi version lsb_releas instal ubuntu environ access bash command window add window execut ubuntu path add follow bashrc bash consol vim bashrc export pathpathmntcwindowssystem note c drive mount file own root file permiss set mean ssh key open ansibl henc run ansibl need call gethostssh window user need chang lineend set git configur git checkout asi commit asi git config global coreautocrlf fals deploy cluster user need clone emma repositori git clone httpsgithubcomnlescsherlockemma window subsystem linux mount c drive mntc default file permiss readwriteexecut bad use sshkey stricter permiss requir solv creat etcwslconf file follow content restart shell ini automount enabletru rootmnt optionsmetadata setup environ independ os user use follow step need done repositori clone cd emma creat edit env_linuxsh cp env_linuxshtempl env_linuxsh vim env_linuxsh linux environ also embed ubuntu environ window bash env_linuxsh key use root set passphras ask sshkeygen f cluster_namekey key use cluster_nam user sshkeygen f filescluster_namekey everi time user open bash consol window linux environ set follow command window cd path_to_emmaemma env_linuxsh env_windowssh linux cd path_to_emmaemma env_linuxsh infrastructur environ set next step setup infrastructur infrastructur physic place platform run compos set virtual machin follow characterist ubuntu os public network interfac os disk mb softwar enough room tmp passwordless login root cluster_namekey privat key xf partit mount dataloc use swapfil glusterf brick docker root python run ansibl task infrastructur collect machin must reachabl ssh machin must preparedconstruct either prepar cloud virtual machinecloudmd construct use vagrant boxesvagrantmd machin prepar server provis use ansiblehttpswwwansiblecom autom tool infrastructur role defin ansibl creat platform follow featur glusterfsglusterfsmd miniominiomd hadoophadoopmd sparksparkmd standalon cluster docker swarmdockerswarmmd jupyterhubjupyterhubmd preced platform instal user click featur understand setup requir requir fulfil user follow platform instal step list ansiblemdansiblemd cloud virtual machin instanti cloud give time kernel get updat cloud base setup skip deploy vagrant disk exampl devvdb dataloc partitionedformattedmount also set up ssh key root make sure first read ansiblemdansiblemd ansbil run first defin environ use env_linuxshtempl run env_linuxsh ansibleplaybook e datadiskdevvdb e host_namecluster_nam prepcloudplaybookyml first run fail aptget updat fail solut reboot machin log use webui follow sudo dpkg configur graphic window prompt first option select ie instal version packag manag surfsara hpc cloud guest node get dn entryhttpsdochpccloudsurfsaranlaccessyourvm vmnameprojectnamesurfhostednl use access webui instal system jupyterhub docker swarm node docker daemon run docker swarm endpoint docker_manager_ip ip address set host file howto see httpsdocsdockercomengineswarmswarmtutorialdeployservic use swarm login dockerswarmmanag host configur host file glusterf see httpglusterreadthedocsio node xf partit avail gv volum mount datashar node volum configur replicasstripestransportetc rolesglusterfstaskscreatevolumeyml file global file locat locat store file specif user run differ playbook one exampl ssk key scala modul locat locat store scala modul specif user instal run ansibleplaybook install_platform_lightyml tag user_defined_modul python modul locat locat store python modul specif user instal run ansibleplaybook install_platform_lightyml tag user_defined_modul r modul locat locat store r modul specif user instal run ansibleplaybook install_platform_lightyml tag user_defined_modul minio mount local dir bucket minio need mirror two direct option w watch chang mirror run background command run host regist alia mcconfig storag servic mc config host add host_ip access_key secret_key sv exampl mc config host add http ahrigvrkfgxjtem jdarcoixvjlzxqtwansgjmuomyjbgygyih sv let get bucket datashar mc mirror w datashar smybucket let get bucket datashar mc mirror w smybucket datashar mirror subdirectori need mirror differ bucket exampl mirror datasharedscratch mc mirror w datasharedscratch ssuperbucket mc mirror w ssuperbucket datasharedscratch configur file scmd scfg host_bas host_bucket access_key ahrigvrkfgxjtem secret_key jdarcoixvjlzxqtwansgjmuomyjbgygyih use_http fals list_md fals use_mime_mag fals make sure region one use minio bucket_loc useast exampl command scmd ls sfile scmd get sfilessonnetstxt romulotxt upload data subdirectori follow exampl cd root_dirsub_dir f ls scmd put f sroot_dirsub_dirf done unit test use testinfrahttpsgithubcomphilpeptestinfra instal depend testinfra follow sudo aptget instal pythonpip pythondev libffidev libssldev libxmldev libxsltdev libjpegdev zlibgdev sudo pip instal testinfra necessari defin sshconfig window need replac window path ubuntu environ path vagrant sshconfig rolesminiotestssshconfig run test verifi minio instal run run follow testinfra ansibleinventoryhost sshconfigrolesminiotestssshconfig connectionssh sudo rolesminioteststest_miniopi sparkshel connect sparkshel need sparkshel download data load hdf creat dir hdf login ubuntu hadoop df mkdir userubuntufil download sample_kmeans_datatxt file wget httpsrawgithubusercontentcomapachesparkmasterdatamllibsample_kmeans_datatxt upload sample_kmeans_datatxt hdf hadoop df put sample_kmeans_datatxt userubuntufilessample_kmeans_datatxt sparkshel connect sparkshel driver read need sparkshel download data load creat bucket scmd mb sfile download sonnetstxt file wget httpwwwgutenbergorgcacheepubpgtxt sonnetstxt upload sonnetstxt storag scmd put sonnetstxt sfile dask share daskhttpdaskpydataorg cluster dask schedul run daskschedul host port use python python daskdistribut import client client clientdaskschedul requir cluster run ubuntu posix user user exist firewal allow incom connect dask schedul port schedul host role variabl role follow variabl yaml port dask schedul run dask_scheduler_port work directori dask schedul dask_scheduler_dir homeubuntu work directori dask worker dask_worker_dir homeubuntu role follow host group daskschedul host dask schedul run daskwork host dask worker run depend depend ansibl role exampl playbook includ exampl use role instanc variabl pass paramet alway nice user host server role role usernamerolenam x licens apach v author inform option section role author includ contact inform websit html allow exampl manipul geotiff exampl obtain geotrelli documentationhttpgeotrellisreadthedocsioenlatestguidesparkhtmlexampleusecas stitch tile singl geotiff exampl show stich tile singl geotiffhttpsgithubcomlocationtechgeotrellisblobmasterdocsguidesparkrststichingtilesintoasinglegeotiff