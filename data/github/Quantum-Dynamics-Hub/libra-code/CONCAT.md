# Contributor Covenant Code of Conduct

## Our Pledge

In the interest of fostering an open and welcoming environment, we as
contributors and maintainers pledge to making participation in our project and
our community a harassment-free experience for everyone, regardless of age,
body size, disability, ethnicity, gender identity and expression, level of
experience, nationality, personal appearance, race, religion, or sexual
identity and orientation.

## Our Standards

Examples of behavior that contributes to creating a positive environment include:

* Using welcoming and inclusive language
* Being respectful of differing viewpoints and experiences
* Gracefully accepting constructive criticism
* Focusing on what is best for the community
* Showing empathy towards other community members

Examples of unacceptable behavior by participants include:

* The use of sexualized language or imagery and unwelcome sexual attention or advances
* Trolling, insulting/derogatory comments, and personal or political attacks
* Public or private harassment
* Publishing others' private information, such as a physical or electronic address, without explicit permission
* Other conduct which could reasonably be considered inappropriate in a professional setting

## Our Responsibilities

Project maintainers are responsible for clarifying the standards of acceptable
behavior and are expected to take appropriate and fair corrective action in
response to any instances of unacceptable behavior.

Project maintainers have the right and responsibility to remove, edit, or
reject comments, commits, code, wiki edits, issues, and other contributions
that are not aligned to this Code of Conduct, or to ban temporarily or
permanently any contributor for other behaviors that they deem inappropriate,
threatening, offensive, or harmful.

Moreover, project maintainers will strive to offer feedback and advice to
ensure quality and consistency of contributions to the code.  Contributions
from outside the group of project maintainers are strongly welcomed but the
final decision as to whether commits are merged into the codebase rests with
the team of project maintainers.

## Scope

This Code of Conduct applies both within project spaces and in public spaces
when an individual is representing the project or its community. Examples of
representing a project or community include using an official project e-mail
address, posting via an official social media account, or acting as an
appointed representative at an online or offline event. Representation of a
project may be further defined and clarified by project maintainers.

## Enforcement

Instances of abusive, harassing, or otherwise unacceptable behavior may be
reported by contacting the project team at 'alexvakimov@gmail.com'. The project team will
review and investigate all complaints, and will respond in a way that it deems
appropriate to the circumstances. The project team is obligated to maintain
confidentiality with regard to the reporter of an incident. Further details of
specific enforcement policies may be posted separately.

Project maintainers who do not follow or enforce the Code of Conduct in good
faith may face temporary or permanent repercussions as determined by other
members of the project's leadership.

## Attribution

This Code of Conduct is adapted from the [Contributor Covenant][homepage],
version 1.4, available at
[http://contributor-covenant.org/version/1/4][version]

[homepage]: http://contributor-covenant.org
[version]: http://contributor-covenant.org/version/1/4/
# Libra

[![Build Status](https://travis-ci.org/Quantum-Dynamics-Hub/libra-code.svg?branch=master)](https://travis-ci.org/Quantum-Dynamics-Hub/libra-code)


This is the main page of the computational chemistry methodology discovery library, Libra
The program website is [here](https://quantum-dynamics-hub.github.io/libra/index.html)

## Info

More:

* [Training & Usage](https://github.com/compchem-cybertraining/Tutorials_Libra)
* [Autogenerated Documentation](https://quantum-dynamics-hub.github.io/libra-code/)


Due to the increased volume of technical questions about installing and using the 
Libra, Libra-X, Pyxaid and Pyxaid2 codes, I have decided to create a convenient 
[public forum](https://groups.google.com/forum/#!forum/quantum-dynamics-hub)
for all users with the intent:

 - to share my replies with not only a single user that have a trouble, but also other
   potential users who may found that information useful;

 - so that the users/developers who have had some experience with the code would be able to share their
   knowledge and skills with others;


## Installation (as of after 5/14/2021)

### 1. Install miniconda (for Python 3.8) and activate Conda

    mkdir Conda
    cd Conda/
    wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh . 
    sh ./Miniconda3-latest-Linux-x86_64.sh -b -u -p <install_dir>

  Here, 

  * the `-b` option will accept the license agreement and will skip all the prompts
  * the `-u` option will tell the installer to do all the needed updates
  * the `-p` option followed by the installation directory path (will be created), tells
     the installed where to install the package.

  To activate the installed base environment of Conda do:

    eval "$(<path to bin/conda> shell.bash hook)"

  For instance, 

    eval "$(/projects/academic/cyberwksp21/Software/Conda/Miniconda3/bin/conda shell.bash hook)"


  You will need to run this script every time you want to use a particular Conda and the 
  corresponding environments, unless you include this command in your .bashrc or .bash_profile

  Test it is working by doing:

    which conda


### 2. Download Libra 

    mkdir libra
    cd libra
    git clone https://github.com/Quantum-Dynamics-Hub/libra-code.git .

   and switch to the correct branch or tag - usually, it would be `devel` branch

    git checkout devel


### 3. Create the environment equipped with all Libra needs 

#### 3.1. Simple way:

Just execute the following script located in the root of Libra distribution:

    sh ./libra_env_build.sh

This script will create the environment called `libra` and will install all the
needed stuff in it. You just need to activate it:

    conda activate libra

#### 3.2. Detailed instructions on what the script about does:

It first creates and activates the `libra` environment:

    conda create -n libra

Update conda install if needed (e.g. if the output suggests it)

    conda activate libra

Thne, it installs all the dependencies and tools.

Do this one by one, and in this order (should not matter too much, but who knows it)

    > To automate the below procedures, you can use `-y` option to accept prompts (sometimes this will override)
    > previous packages/conflicts, so be careful
    > 
    > You can also use `-q` to get rid of all the messages to the output, although i'd keep it to keep track of what's going on

Basic stuff

    conda install conda-build
    conda install gcc_linux-64
    conda install gxx_linux-64
    conda install make
    conda install boost
    conda install cmake
    conda install git
    conda install -c anaconda h5py
    conda install -c conda-forge/label/gcc7 eigen 
    conda install -c psi4/label/dev libint2 
    conda install -c anaconda gmp
    conda install -c conda-forge/label/gcc7 mpfr 

More, but still needed because some Python modules require those.
We need to downgrade Python version here to 3.6 to enable Psi4 installation

    conda install python=3.6
    conda install -c psi4 psi4
    conda install -c conda-forge matplotlib
    conda install -c rmg py3dmol
    conda install -c anaconda numpy
    conda install -c anaconda scipy
    conda install -c conda-forge llvm-openmp
    
You can install Jupyter notebook using the following command. This will be useful and you can set up and access the Jupyter notebook 
remotely from a cluster and load the tutorials

    conda install -c conda-forge jupyterlab
    

Used in some of the tutorials

    conda install -c conda-forge/label/gcc7 imageio


### 4. Adapt the CMakeLists.txt file according to your system

   Because we had to downgrade Python to 3.6, we need to edit the CMakeLists.txt such that 
   cmake is looking for the correct Python version

    FIND_PACKAGE(PythonLibs 3.6 REQUIRED)

   and also reflect it in the component of the Boost.Python to be found:

    FIND_PACKAGE(Boost COMPONENTS python36 regex)

   Same for Boost (see the error messages for what version of Boost the cmake can find). In my current
   case, it suggest the version 1.73.0, so be it:

    FIND_PACKAGE(Boost 1.73.0 REQUIRED)

### 5. Create the build directory and make the Makefiles

   Then in the libra directory, create the build directory:

    cd libra
    mkdir _build
    cd _build
    cmake ../

### 6. Compile the package
    
    make -j4

### 7. Setup environmental variables

   Add the following exports to your `.bash_profile` file

    export PYTHONPATH=<path to the ppackage>/libra/_build/src:$PYTHONPATH
    export LD_LIBRARY_PATH=<path to the ppackage>/libra/_build/src:$LD_LIBRARY_PATH

### 8. Restart the terminal or source the bash profile and activate libra conda environment

    source .bash_profile 
    conda activate libra
    
   And you should be ready to use Libra.


* [Old instructions](https://quantum-dynamics-hub.github.io/libra/installation.html)



## Developers and Contributors

  * Dr. Alexey Akimov (University at Buffalo, [link](https://akimovlab.github.io/index.html) )  
      The main developer and maintainer of the code

  * Mr. Brendan Smith (University at Buffalo) 
      Entangled trajectories Hamiltonian, NA-MD with spin-orbit coupling, NBRA workflows, BL-LZ NA-MD tutorials and examples, 
      Libra/DFTB+, Libra/QE, Libra/ErgoSCF, Libra/CP2K, and Libra/Gaussian interfaces
      
  * Mr. Mohammad Shakiba (Shahid Bahonar University of Kerman, Iran)
      Cube file processing scripts, Libra/CP2K and Libra/Gaussian, Libra/Libint2 interfaces

  * Mrs. Story Temen (University at Buffalo)
      Implementation and testing of the HEOM codes

  * Dr. Wei Li (Hunan Agricultural University)
      NA-MD with spin-orbit coupling

  * Dr. Kosuke Sato (Toyota Research Lab) 
      State reordering scripts, Libra/GAMESS interface (Libra-X)

  * Dr. Ekadashi Pradhan (York University) 
      Libra/QE interface, delta-SCF NA-M (Libra-X)

  * Dr. Amber Jain (Indian Institute of Technology Bombay, India)
      Implementation and testing of the HEOM codes

  * Dr. Xiang Sun (NYU Shanghai, China)
      Implementation and testing of the FGR codes


## References

This code is provided in the hope it will be useful. 

  If you use the code in your research, please cite the following paper(s):

  ### Papers that describe Libra or its features 

  * [The initial implementation](http://onlinelibrary.wiley.com/doi/10.1002/jcc.24367/full)
  Akimov, A. V. "Libra: An open-Source 'methodology discovery' library for quantum and classical dynamics simulations" 
  *J. Comput. Chem.*  **2016**  37, 1626-1649

  * [Phase correction, Ehrenfest dynamics details, basis transformations (see the SI)](https://doi.org/10.1021/acs.jpclett.8b02826)
  Akimov, A. V.; "A Simple Phase Correction Makes a Big Difference in Nonadiabatic Molecular Dynamics"
  *J. Phys. Chem. Lett.*  **2018** 9, 6096-6102

  * [Belyaev-Lebedev-Landau-Zener Surface Hopping within the Neglect of Back-Reaction Approximation](https://pubs.acs.org/doi/10.1021/acs.jpclett.9b03687)
  Smith, B.; Akimov, A. V. "Hot Electron Cooling in Silicon Nanoclusters via Landau-Zener Non-Adiabatic Molecular Dynamics: 
  Size Dependence and Role of Surface Termination" *J. Phys. Chem. Lett.* **2020** 11, 1456-1465

  * [HEOM implementation](https://pubs.acs.org/doi/10.1002/qua.26373)
  Temen, S.; Jain, A.; Akimov, A. V. "Hierarchical equations of motion in the Libra software package"
  *Int. J. Quant. Chem.* **2020**


  You may find the following papers useful examples
  ### Parers that utilize Libra

  * [Formulation of a fragment-based NA-MD](http://pubs.acs.org/doi/abs/10.1021/acs.jctc.6b00955)
  Akimov, A. V. "Nonadiabatic Molecular Dynamics with Tight-Binding Fragment Molecular Orbitals" 
  *J. Chem. Theory Comput.*  **2016** 12, 5719-5736

  * [Quasi-stochastic Hamiltonian for longer NA-MD](http://pubs.acs.org/doi/abs/10.1021/acs.jpclett.7b02185)
  Akimov, A. V.; "Stochastic and Quasi-Stochastic Hamiltonians for Long-Time Nonadiabatic Molecular Dynamics"
  *J. Phys. Chem. Lett.*  **2017** 8, 5190-5195

  * [Entrangled-trajectories Hamiltonian dynamics to capture quantum effects of nuclei](https://doi.org/10.1063/1.5022573)
  Smith, B. A.; Akimov, A. V. "Entangled trajectories Hamiltonian dynamics for treating quantum nuclear effects" 
  *J. Chem. Phys.* **2018** 148, 144106

  * [Inclusion of the Spin-orbit coupling in NA-MD](https://doi.org/10.1021/acsenergylett.8b01226)
  Li, W.; Zhou, L.; Prezhdo, O. V.; Akimov, A. V. "Spin-Orbit Interactions Greatly Accelerate Nonradiative Dynamics
  in Lead Halide Perovskites" *ACS Energy Lett.* **2018** 3, 2159-2166




# Compiling Libra's Documentation

The docs for this project are built with [Sphinx](http://www.sphinx-doc.org/en/master/).
To compile the docs, first ensure that Sphinx and the ReadTheDocs theme are installed.


```bash
conda install sphinx sphinx_rtd_theme 
```


Once installed, you can use the `Makefile` in this directory to compile static HTML pages by
```bash
make html
```

The compiled docs will be in the `build` directory and can be viewed by opening `index.html` (which may itself 
be inside a directory called `html/` depending on what version of Sphinx is installed).# Templates Doc Directory

Add any paths that contain templates here, relative to  
the `conf.py` file's directory.
They are copied after the builtin template files,
so a file named "page.html" will overwrite the builtin "page.html".

The path to this folder is set in the Sphinx `conf.py` file in the line: 
```python
html_static_path = ['_templates']
```

## Examples of file to add to this directory
* HTML extensions of stock pages like `page.html` or `layout.html`
# Static Doc Directory

Add any paths that contain custom static files (such as style sheets) here,
relative to the `conf.py` file's directory. 
They are copied after the builtin static files,
so a file named "default.css" will overwrite the builtin "default.css".

The path to this folder is set in the Sphinx `conf.py` file in the line: 
```python
templates_path = ['_static']
```

## Examples of file to add to this directory
* Custom Cascading Style Sheets
* Custom JavaScript code
* Static logo images
# Development, testing, and deployment tools

This directory contains a collection of tools for running Continuous Integration (CI) tests, 
conda installation, and other development tools not directly related to the coding process.


## Manifest

### Continuous Integration

You should test your code, but do not feel compelled to use these specific programs. You also may not need Unix and 
Windows testing if you only plan to deploy on specific platforms. These are just to help you get started

* `travis-ci`: Linux and OSX based testing through [Travis-CI](https://about.travis-ci.com/) 
  * `before_install.sh`: Pip/Miniconda pre-package installation script for Travis 
* `appveyor`: Windows based testing through [AppVeyor](https://www.appveyor.com/) (there are no files directly related to this)

### Conda Environment:

This directory contains the files to setup the Conda environment for testing purposes

* `conda-envs`: directory containing the YAML file(s) which fully describe Conda Environments, their dependencies, and those dependency provenance's
  * `test_env.yaml`: Simple test environment file with base dependencies. Channels are not specified here and therefore respect global Conda configuration
  
### Additional Scripts:

This directory contains OS agnostic helper scripts which don't fall in any of the previous categories
* `scripts`
  * `create_conda_env.py`: Helper program for spinning up new conda environments based on a starter file with Python Version and Env. Name command-line options


## How to contribute changes
- Clone the repository if you have write access to the main repo, fork the repository if you are a collaborator.
- Make a new branch with `git checkout -b {your branch name}`
- Make changes and test your code
- Ensure that the test environment dependencies (`conda-envs`) line up with the build and deploy dependencies (`conda-recipe/meta.yaml`)
- Push the branch to the repo (either the main or your fork) with `git push -u origin {your branch name}`
  * Note that `origin` is the default name assigned to the remote, yours may be different
- Make a PR on GitHub with your changes
- We'll review the changes and get your code into the repo after lively discussion!


## Checklist for updates
- [ ] Make sure there is an/are issue(s) opened for your specific update
- [ ] Create the PR, referencing the issue
- [ ] Debug the PR as needed until tests pass
- [ ] Tag the final, debugged version 
   *  `git tag -a X.Y.Z [latest pushed commit] && git push --follow-tags`
- [ ] Get the PR merged in

## Versioneer Auto-version
[Versioneer](https://github.com/warner/python-versioneer) will automatically infer what version 
is installed by looking at the `git` tags and how many commits ahead this version is. The format follows 
[PEP 440](https://www.python.org/dev/peps/pep-0440/) and has the regular expression of:
```regexp
\d+.\d+.\d+(?\+\d+-[a-z0-9]+)
```
If the version of this commit is the same as a `git` tag, the installed version is the same as the tag, 
e.g. `molecool-0.1.2`, otherwise it will be appended with `+X` where `X` is the number of commits 
ahead from the last tag, and then `-YYYYYY` where the `Y`'s are replaced with the `git` commit hash.
## Note

In this folder I will keep the Python code that used to be implemented
in Python and then got implemented in C++, so the Python version is 
no longer needed


  The following directories must be present for you to be able to run the 
  test calculation added in this example

  nosoc_nospinpol_1x1x1
  nosoc_nospinpol_2x1x1
  nosoc_nospinpol_2x2x2
  nosoc_spinpol_1x1x1
  nosoc_spinpol_2x1x1
  nosoc_spinpol_2x2x2
  soc_1x1x1
  soc_2x1x1
  soc_2x2x2


  The results of the calculations would be too big to track by the git, so please compute them yourself.
  The examples of input files for each type of calculations are given in the directories
  

  The meaning of the content in each directory is pretty much self-explanatory, but just to be sure:

  nosoc vs. soc - refers to whether spin-orbit coupling (SOC) is included or not
                  Note, the SOC calculations require fully-relativistic pseudopotential

  nospinpol vs. spinpol  - refers to whether the non-relativistic calculations are spin-polarized or not

  k1 x k2 x k3 - refers the the k-points grid used in the calculations

  
# Table of content

## Example 1

   Illustrates the basic TSH models


# Description of files

   * fgr_py.py  - an auxiliary file to streamline the calculations

   * test_nefgr_brownian.py - LVC model with Ohmic bath, to run one method/Condon vs. non-Condon combo at a time
   * plot_nefgr_brownian.plt - to plot the results of the ```test_nefgr_brownian.py``` script (with the *Test2* stuff uncommented)
   * plot_nefgr_brownian.py  - to plot the results of the ```test_nefgr_brownian.py``` script (with the *Test3* stuff uncommented)

   * test_nefgr_brownian_all.py - same as above, but runs all the combinations
   * plot_nefgr_brownian_all.py - to plot the results of the ```test_nefgr_brownian_all.py``` script

   * test_nefgr_fulvene.py - the application of FGR to Fulvene molecule
   * plot_nefgr_fulvene.plt - to plot the results of the ```test_nefgr_fulvene.py``` script


# Possible examples

##  Example 1
   * Uncomment section under "Test2"  (uncommented by default), set up parameters
   * ```python test_nefgr_brownian.py```
   * ```gnuplot nefgr_brownian.plt```

##  Example 2
   * Uncomment section under "Test3", comment the one under "Test2" (uncommented by default), set up parameters
   * ```python test_nefgr_brownian.py```
   * ```python plot_nefgr_brownian.py```

##  Example 3
   * Set up parameters in the ```test_nefgr_brownian_all.py``` file
   * ```python test_nefgr_brownian_all.py```
   * ```python plot_nefgr_brownian_all.py```

##  Example 4
   * Uncomment section under "Test2"  (uncommented by default), set up parameters
   * ```python test_nefgr_fulvene.py```
   * ```gnuplot nefgr_fulvene.plt```



# Context Tutorials


## Example 1

   This example demonstrates how to access data in the XML file using the Context class

   as well as how to save the structured data into XML


### What to pay attention to in *test_context.py*

  The following functions:

  *  ```ctx = Context()```   creates an empty Context object

  * ```ctx = Context("ctx_example.xml")```  creates a Context object *ctx* that contains information parsed from
  the XML file *ctx_example.xml*

  * ```ctx.save_xml("ctx_1.xml")```  to save the data contained in the *ctx* object into a new XML file

  * ```ctx.get_path()```  to show the current node's name (path element)

  * ```ctx.set_path("new_path")``` to rename the current node

  * ```ctx.add("param1", param1)``` to add a parameter to the current node - as it's new branch, the tree may have many
  identically-names branches.

  * ```ctx.get("param2", "Milk")```  to retrieve value of the node's brach named "param2". If such branch does not exist
  the default value supplied ("Milk") will be returned. Note that the ```get``` function can do the type conversion, to 
  the default value type, so one can also do stuff like ```ctx.get("param1a", -1)``` expecting an integer
  


## Example 2 

   This example demonstrates how to access data in the XML file using the Context class

   The xml file is generated by the QE software and contains 100 MD steps for a Si_8 cluster

### What to pay attention to

  The following functions:

  * ```ctx.set_path_separator("/")```  set the path separator to be the "/" symbol, rather than the default "." 
  This is expecially important if the key names contain "." as their integral part. With this, we can have:
  ```
  <K_point.1>
     <Wfc.1>
     </Wfc.1>
  </K_point.1>
  ```
  parsed successfully

  * ```ctx1 = ctx.get_child("step", dctx)```   creates a new Context object *ctx1*  that represents one of the 
  branches of the original *ctx* object

  *  ```steps = ctx.get_children("step")```      get all the children of the current node such that they all
  have the "step" tag. Create an array of Context objects

  *  ```all1 = ctx.get_children_all()```  similar to *get_children*, but don't cares about the name of the tag to search.
  Will create an array of Context objects that represent all the node's branches

  * ```atoms[0].show_children()``` show all the keys of the corresponding Context object - these are its branch tag names.

  * ```atoms[i].get("", "")```   to retrieve the current node's data

  *  ```atoms[i].get("<xmlattr>/index", -1.0)```  to retrieve the node's branch data, if exist. Or return the default value given
# Tutorial 1

Comparing NVE and NVT ensembles and exploring the right ways of 
computing MD
# Computing the molecular overlaps using Libint2 and CP2K

Here are 3 python test files for computing both the atomic and molecular orbitals (MOs) overlap matrices. In computing the MOs overlap, we need to make sure that the 
diagonal elements of the MO overlap matrix are all `1`. All the details are explicitly explained in the files.

`test.py` 
This file tests the Libint2 library in Libra for computing the atomic orbital overlap matrix.

`test_molden.py`
This file computes the MO overlaps from a `molden` file format printed out by CP2K. 

`test_molog.py`
This file computes the MO overlaps from a `MOLog` file printed out by CP2K.
# Lattice Monte Carlo (MC) Example

## Intro

  This example demonstrates how to run MC-base annealing of 
  a 2D lattice. The lattice is prepared with the dopants randomly distribute.
  You define the model Hamiltonian and run the Metropolis-based MC simulations
  to get the structure with the lowest possible energy.


## Files

  * ar.xyz  - defines the lattice unit cell, the atom type doesn't matter
  * elements.dat - file that defines the properties of elements in the periodic table
  * make_pics.tcl - file to generate a set of pictures for all structures along the simulation
  * model.py - the main file to run the calculations: define the lattice, Hamiltonian and 
  the simulation parameters here.
  * plot.plt - the file to plot the energy of the system as the function of annealing time

## Procedures

  * Edit the file model.py as needed
  * Create the output directory, e.g. 2D, if absent
  * Run the calculations:  `python model.py`
  * Copy plotting files to the results folder:
    `cp plot.plt 2D` and `cp make_pics.tcl 2D`
  * In the results directory, run the gnuplot:
    `gnuplot plot.plt`
  * Open VMD and its Tcl shell, navigate in the Tcl shell to 
    the results directory (2D)
  * Run the make_pics.tcl script: `source make_pics.tcl`

    
# Electronic structure calculations on Si QDs

## Intro

  This example demonstrates how run electronic structure calculations (at the EHT
  level) on various Si QDs. The postprocessing includes computing partial densities of 
  states and molecular orbitals (for visualization with VMD)


## Files

  * control_parameters_eht.dat  - defines the simulation parameters for EHT calculations (if EHT is chosen)
  * control_parameters_indo.dat  - defines the simulation parameters for INDO calculations (if INDO is chosen)
  * elements.dat - file that defines the properties of elements in the periodic table
  * muller_params - parameters for the EHT Hamiltonian
  * params_indo - parameters for the INDO Hamiltonian
  * plot_dos.plt - the file to plot pDOS of the system
  * QD_6_h.inp.xyz,  ... QD_12_h.inp.xyz - are sample input files 
  * test_qd.py - the main file to run the calculations: define which Hamiltonan to use, which system to compute,
  which projections to compute, which orbitals to plot, and all other relevant parameters here


## Procedures

  * Edit the file test_qd.py as needed
  * Create the output directory, e.g. qd6, if absent
  * Run the calculations:  `python test_qd.py`
  * The first line of the generated file dos_proj.txt will contain 
    the value of the Fermi energy - use it to adjust the x-axis of the 
    plotting script 'plot_dos.plt'
  * Run the gnuplot to compute the pDOS:
    `gnuplot plot_dos.plt`
  * Open VMD and its Tcl shell, navigate in the Tcl shell to 
    the results directory (e.g. qd6)
  * Load the .cube files and plot visualize the structure and orbitals

    

   test_verlet_ens.py - run the file to do the calculations, this is going to be 
                        a ground-state dynamics of H2 molecule with HF forces

   test_verlet_ens.plt - use this to plot the output using gnuplot

   test.py   - just a testbed to figure out how PySCF works

# Table of content

## Example 1

   Illustrates the basic use of Wfcgrid class: how to initialize the 1D grid, set up the 
   initial wavepacket, compute its properties and visualize it and other properties


## Example 2

   Illustrates how one can run fully quantum simulations for 1D models (and define them 
   in a Python script) and to print out useful information
   

## Example 3

   Illustrates how to initialize the wavefunctions to be the superpositions of HO eigenstates


## Example 4

   The analytical solution for dynamics involving superposition of HO eigenstates  

# Instructions 

  1. Create a directory "res"

  2. Run the calculations:  python run.py

  3. Plot the PES and population:   gnuplot pes.plt   and gnuplot pops.plt

  4. Copy files loop.plt and start.plt  into res folder and run gnuplot: gnuplot start.plt


# What to explore

## Explore various ways of setting up the initial wavefunction on the grid:

  1.  Case = 0  HO ground state wavefunction, |0>

  2.  Case = 1  HO 1-st excited state wavefunction, |1>

      In these examples the HO eigenstates correspond to the potential in which the present system evolves
      Therefore, there should be no overall motion of the wavepackets



## Create the initial state as a superposition of the HO eigenstates

  1.  Case = 2  a superposition of the first 2 HO eigenstates: ~ |0> + |1> (up to the overall normalization)

      Note that in this example, we will need to have 2 values of each parameter: x0, px0, alpha, initial electronic, initial vibrational state

      Because the superposition is no longer an eigenstate of the Hamiltonian, we should in principle observe some evolution


## Make the wavepackets moving 

  1.  Case = 3  the ground state wavefunction is now has a momentum, so we'll see some movement


## Displace the potential

  1.  Case = 4  we now change the minimum of the potential, so (unless we change the parameters of the 
      wavefunctions) accordingly, the initialized Gaussian is no longer an eigenfunction of the (new) Hamiltonian.
      So, we get some motion again



# Instructions 

  1. Create a directory "res"

  2. Run the calculations:  python run.py

  3. Plot the PES and population:   gnuplot pes.plt   and gnuplot pops.plt

  4. Copy files loop.plt and start.plt  into res folder and run gnuplot: gnuplot start.plt


# What to explore

## Explore three model potentials/initial conditions (which counts to many more models):

  1.  Case = 1 *Double well potential*

  2.  Case = 2 *Decay of a metastable state*

  3.  Case = 3 *Modified decay of a metastable state*

      This is a model similar to the case 2, but without infinitely deel well to the right of the barrier
      It is still sufficiently deep so that the wavefunction would have little probability of going back



## Explore two options for measuring the rate of quantum transitions:

  1.  opt = 0  *Measuring the amount of probability dissapering in the complex absorbing potential*

      Pros:
      * can use smaller grid, which can help reduce the costs

      Cons:
      * the total population and energy of the system are not conserved
      * the dynamics may be delayed because the wavepacket would need to travel to the absorbing 
        potential (maybe much later than the moment of actual barrier crossing)
      * a partial reflection from the absorbing potential is possible, such that the dynamics may be
        perturbed


  2.  opt = 1  *Measuring the amount of probability passing through a point (dividing surface)*

      Pros:
      * the total population and energy of the system are conserved
      * the dynamics is not delayed because we measure exactly what we need 
        (observing the moment of actual barrier crossing)
      * no reflection from the absorbing potential, the dynamics is not perturbed

      Cons:
      * may need larger grid to avoid reflections/boundary passing
# Computing the molecular orbital overlaps for xTB calculations

Here are the files for computing the MO overlaps for extended tight-binding calculations using CP2K. 
We first need to run an OT calculations and then use the `wfn` file for diagonalization so that we can print out the 
MOs in `molden` or `MOLog` file formats. 

Then we will use `libint2` to compute the AO overlap matrix to compute the MO overlaps.

We first need to specify the correct variables in the `es_ot_template.inp` and `es_diag_template.inp` and 
then specify the full paths to these files:

* `res_dir`
* `all_pdosfiles`
* `all_logfiles`
* `cp2k_exe`
* `cp2k_ot_input_template`
* `cp2k_diag_input_template`
* `trajectory_xyz_filename`

It is required to specify the 

* `nprocs` 
* `init_state`
* `final_state` in the `run_template.py` 

but the `istep`, `fstep`, and `njobs` is defined in `distribute_jobs.py`. 
If you want to submit the jobs (and not running as bash), set `run_slurm` to `True`.

After specifying all the variables in both files you can run the calculations using `python distribute_jobs.py`

The MO overlaps are saved as `.npz` format. 
These files are binary files by `scipy.sparse` library. In order to load the saved files you can do this:

```
import numpy as np
import scipy.sparse

# Load the sparse matrix
S = scipy.sparse.load_npz('./res/S_ks_0_re.npz')
# The dense matrix
S_dense = S.todense()

# Show the diagonal elements of the dense matrix
np.diag(S_dense)
```


# Tutorials on Normal Modes

## Example 1

### Instructions

  1.  Just run the script ```python test.py ```

  2.  Load any of the produced output .xyz files to the VMD to visualize
      You can use the pre-defined tcl scripts, e.g. ```source load_Si8.tcl``` in the 
      TkConsole of VMD

### Explanations 

  1. We use the methods ```R, V, A, M, E = QE_methods.read_md_data("x0.xml") ``` to read the 
  data of the MD simulations performed with QE and stored in the XML file. The matrices R, V, A, M
  with coordinates, velocities, accelerations, and masses are generated. In addition, we obtain a list
  E of the atom type labels (element names). This is all the info we need for further calculations

  2. We use two types of approaches to compute the normal modes and corresponding frequencies.
  One is according to [(1) Strachan, A. Normal Modes and Frequencies from Covariances in Molecular Dynamics 
  or Monte Carlo Simulation. J. Chem. Phys. 2003, 120, 1-4.] which is invoked with
  ```normal_modes.compute_cov( R, V, A, M, E, params)``` and the other is according to 
  [(2) Pereverzev, A.; Sewell, T. D. Obtaining the Hessian from the Force Covariance Matrix:
  Application to Crystalline Explosives PETN and RDX. J. Chem. Phys. 2015, 142, 134110.] which is invoked
  with ```normal_modes.compute_cov2( R, A, M, E, T, params)```  
  Each function does the following:
  - computes the frequencies
  - prints out the selected normal modes as .xyz for further visualization

  3. Note that the function *compute_cov* tries to compute the normal modes/frequencies according to
  two approaches discussed in the corresponding paper - one based on the covariances of V and R, and another
  based on covariances of A and R. The results are printed in the files with "_velocities" and "_accelerations"
  correspondingly. 

  4. The function *compute_cov2* requires the temperature in its definition, which should be chosen to be the 
  average temperature of the simulation.

  5. Each function is called with the one of the two possible values for the flag "cov_flag", which controls how the
  covariance matrices are computed. The standard definition of the covariance matrix subtracts the mean value of each 
  coordinate (so one actually looks at the covariance of the fluctuations). However, both papers define their "covariances"
  without such a correction, so I'm not totally sure which option is the best and leave it to the user to decide. 


### Systems

  1. x0.xml - Si8 molecule simulated for 100 fs

  2. x0_h2.xml - H2 molecule simulated for 500 fs

  3. x0_co2.xml - CO2 molecule simulated for 1000 fs





## Example 4

### Instructions

  1.  Just run the script ```python test.py ```

### Explanations 

  This exmple demonstrates how to get the coordinates, element names, and normal modes for a given system from
  QE (phonon) output file (.dyn). This example simply shows how to invoke the processing of two files - the first one
  produces some verbose output on how thigs are going under the hood - good for demonstrations. The other file
  deals with a much larger system, so we don't really want to produce a lot of output for it - just get the objects with
  the data we need. For this, we use the default value of verbosity flag (0 - no output). 

  The objects generated can later be used for visualization of the normal modes, with for instance the
  normal_modes.get_xyz2 function
# QSH workflow

## step2  - generate the "short trajectory" data

  * ```cd step2```
  * ```mkdir res```
  * ```python gen_data.py```

### Description:
  * gen_data.py - script to generate the short time direct Hamiltonian files used for QSH simulation for tutorial purpose
                  or user can use the their own direct Hamiltonian files calculated from ab initio MD trajectory
  * the ```res``` directory will contain the model Hamiltonian files

## step3  - extend the trajectory data to longer times, using QSH

  * ```cd ../step3```
  * ```mkdir res_qsh```
  * ```python run_step3.py```

### Description:
  * The file ```res_qsh``` will contain the QSH files generated from the original input files (direct/model simulations)

## step4  - run the TSH calculations with original data and QSH data

  * ```cd ../step4```
  * Edit the file ```run_step4.py```
  * ```python run_step4.py```

### Description:
  * The files from the ```res_qsh``` or from ```res``` directories can be used and the input to this function
  In the former case, one obtains QSH dynamics, in the latter - the direct dynamics

  This script actually runs 3 variants of the TSH calculations:
  a) the standard one
  b) file-based QSH 
  c) file-less (on-the-fly) QSH





# Step 1

  * Go to *step1* directory:  ```cd step1```

  * Edit the submit.slm file to change your email (or comment it out like this:)
   ```###SBATCH --mail-user=alexeyak@buffalo.edu```

  * Run the calculations by:   ```sbatch submit.slm```


# Step 2

  * Go to *step2* directoy:  ```cd ../step2```

  * Copy the file **x0.md.out** produced in the previous step to here: ```cp ../step1/x0.md.out .```

  * Edit the **submit_templ.slm** file to define the absolute path of the directory that will collect
    the results of this step:  e.g. ```res=/gpfs/scratch/alexeyak/example_1_Si/step2/res``` 
    Here, the *res* directory is (or will be created in) the current *step2* folder, for instance. 

  * Edit the **run_step2.py** file to define how many steps to prcess: e.g.
    ```nsteps_per_job = 20
       tot_nsteps = 100```
   
  * Run the calculations:  ```python run_step2.py```


# Step 3

  * Go to the *step3* directory:  ```cd ../step3```

  * In the **run_step3.py** edit the variable ```setup``` - select from options 1 or 2

  * Depending on the setup, create a folder *res_setup1* or *res_setup2*, e.g. ```mkdir res_setup1``` 
    the name can be different, but should be consistent with the definition in ```params["output_set_paths"]```

  * Edit other variables, if needed

  * Run the calculations:  ```python run_step3.py```


# Step 4

  * Go to the *step4* directory:  ```cd ../step4```

  * In the **run_step4.py** edit the desired variables. In particular, note the
    line containing ```params["data_set_paths"].append(...)```. If uncommented, it mimics the presence 
    of another directory (potentially for a data corresponding to another MD trajectory)

    Note the difference btween the ```params["nfiles"]``` and ```params["nsteps"]``` variables

  * Run the calculations:  ```python run_step4.py```
# Description of the modules

## common_utils.py  - Auxiliary general-purpose functions
   * check_input(params, default_params, critical_params) - checks the presenece of required parameters
   * get_matrix(nrows, ncols, filename_re, filename_im, act_sp) - reads the files into a matrix
   * orbs2spinorbs(s) - conversion from the older PYXAID style to the new style
   * find_maxima(s, logname) - finds the maxima in the data series
   * flt_stat(X) - computes statistical properties on the scalar data series
   * mat_stat(X) - computes statistical properties on the MATRIX data series
   * cmat_stat(X) - computes statistical properties on the CMATRIX data series
   * printout(t, pops, Hvib, outfile) - one of the versions to print data out
   * show_matrix_splot(X, filename) - print out of a matrix in a gnuplot format for surface plotting
   * add_printout(i, pop, filename) - one of the versions for printing out the TSH information

## compute_hprime.py - module to compute the transition dipole moment matrices
   * compute_hprime_dia(es, info, filename) 
   * hprime_py(es, info, filename)

## compute_properties.py - auxiliary module to compute some properties
   * compute_properties_dia_gamma(params, es_curr, es_next, curr_index):

## decoherence_times.py - to compute decoherence times from the data series
   * energy_gaps(Hvib) - compute the energy gaps along a single trajectory
   * energy_gaps_ave(Hvib, itimes, nsteps) - compute the energy gaps averaged over several trajectories
   * decoherence_times(Hvib, verbosity=0) - compute decoherence times based on single trajectory data
   * decoherence_times(Hvib, itimes, nsteps, verbosity=0) - compute decoherence times based on several trajectories data

## excitation_spectrum.py - to generate Hvib matrices averaged over trajectories
   * calculate(energy_prefix,energy_suffix,dip_prefix,dip_suffix,isnap,fsnap,opt,scl1,scl2,outfile,HOMO,minE,maxE,dE)
   * ham_map(prefix, isnap, fsnap, suffix, opt, scl, outfile)

## influence_spectrum.py - computes the ACF of Hamiltonian matrix elements and its FT 
   * compute(X, a, b, dt, Nfreqs, filename, logname, dw=1.0, wspan=3000.0)

## lz.py - Landau-Zener theory as applied to NA-MD (experimental module)
   * get_data(params) - read in the vibronic Hamiltonian files 
   * Belyaev_Lebedev(Hvib, dt) - compute probabilities of NA transitions along a trajectory
   * run(params) - execute the NA-MD calculations based on the LZ hopping probabilities

## mapping.py - to map the 1-electron properties onto N-electron ones
   * energy_arb(SD, e) - compute energy of a Slater determinant (simplified)
   * energy_mat_arb(SD, e, dE) - compute matrix of the energies of the SD basis, with corrections
   * sd2indx(inp,nbasis) - mapping of spin-orbitals on the orbitals
   * ovlp_arb(SD1, SD2, S) - compute the overlap of two SDs
   * ovlp_mat_arb(SD1, SD2, S) - compute a matrix of overlaps of the SD basis


## qsh.py - generates the QSH files for longer trajectory
   * compute_freqs(nstates, H_vib_re, H_vib_im, dt, Nfreqs, filename, logname, dw, wspan) - compute
     the frequencies of the matrix elements in a time-series
   * compute_qs_Hvib(Nfreqs, freqs, t, nstates, 
                 H_vib_re_ave, H_vib_re_std, dw_Hvib_re, up_Hvib_re, 
                 H_vib_im_ave, H_vib_im_std, dw_Hvib_im, up_Hvib_im, 
                 dev) - compute the quasi-stochastic Hamiltonian
   * run(params) - execute the QSH calculations


## step2.py - run QE calculations and computes the time-overlaps of the orbitals
   * run_qe(params, t, dirname0, dirname1) - execute the QE calculations and do the file management
   * read_info(params) - read the info about the QE calculation
   * read_all(params) - reade the QE calculation results - energies, orbitlas, etc.
   * read_wfc_grid(params, info0, info1) - read the wavefunction grid
   * run(params) - execute the calculations for a "step2"

## step3.py - to generate the Hvib matrices for given basis of many-electron states with needed corrections
   * get_data(params) - read in the information about the 1-electron orbitals (time-overlaps, overlaps, and energies)
   * apply_state_reordering(St, E, params) - state reordering
   * apply_phase_correction(St, params) - phase correction
   * sac_matrices(coeff) - an auxiliary function to convert the user input about SAC to the internal format
   * scale_H_vib(hvib, en_gap, dNAC, sc_nac_method) - scissor operator to the energy gap and the NACs
   * compute_Hvib(basis, St_ks, E_ks, dE, dt) - compute Hvib in the Slater determinants basis
   * run(params) - using the input of the 1-el KS  orbitals, compute the Hvib files in the specified SAC basis

## step4.py - to compute the NBRA-based NA-MD
   * get_Hvib(params) - read in the vibronic Hamiltonians along the trajectories from the files (different data sets)
   * traj_statistics(i, Coeff, istate, Hvib, itimes) - compute the averaged populations and energies from TSH data
   * printout(t, res, outfile) - print out the data to a file
   * run(params) - exectue the NA-MD for multiple trajectories 


## utils.py - various auxiliary/utility functions
   * get_value(params,key,default,typ) - check if the dictionary has the required keys/set default values
   * split_orbitals_energies(C, E) - needed for processing SOC output
   * merge_orbitals(Ca, Cb) - change the format of the PW orbitals storage
   * post_process(coeff, ene, issoc) - transform the format of orbitals
   * orthogonalize_orbitals(C) - orthogonalize orbitals, if needed
   * orthogonalize_orbitals2(Ca,Cb) - orthogonalization of the spinor wavefunction




# Roadmap of workflows

## PYXAID (old version)
   * run adiabatic MD - one time
   * step2.run()  - execute one time
   * step3.run()  - without corrections on, no spin-adaptation
   * step4.run()

## PYXAID2 
   * run adiabatic MD - potentially several trajectories
   * step2.run()  - execute several times, with available trajectories
   * step3.run()
   * step4.run()

## Quasistochastic Hamiltonian
   * run adiabatic MD - potentially several trajectories
   * step2.run()  - execute one/several times, with available trajectories
   * step3.run()  - execute or not
   * qsh.run()
   * step4.run()


# 
This page contains a list of what you can do with Libra, with the links to corresponding examples

_______

# Libra usage: By the task required


## 1. Reading and visualizing MD trajectories

### 1.1. How to visualize QE xml MD
* [Visualization of CO2 trajectory](../notebooks/Example4_md/Example2_atomistic/MD.ipynb)

### 1.2. How to compute probability distribution function
* [How to compute velocity probability distribution function](../notebooks/Example4_md/Example2_atomistic/MD.ipynb)
* [How to compute atom coordinate fluctuations](../notebooks/Example4_md/Example2_atomistic/MD.ipynb)


## 2. Working with atomic lattices

### 2.1. How to determine connectivity automatically 
* [How to autoconnect atoms in a periodic lattice](../notebooks/Example4_md/Example1_lattice/Lattice_MD.ipynb)


_______

# Libra usage: By the module/functions used

## 1. libra_core

### 1.1. DATA

#### 1.1.1. Calculate_Estimators

* [Example 1](../notebooks/Example4_md/Example2_atomistic/MD.ipynb)

#### 1.1.2. Calculate_MiniMax

* [Example 1](../notebooks/Example4_md/Example2_atomistic/MD.ipynb)

#### 1.1.3. Calculate_Distribution

* [Example 1](../notebooks/Example4_md/Example2_atomistic/MD.ipynb)



## 2. libra_py

### 2.1. QE_methods

#### 2.1.1. read_md_data

* [Example 1](../notebooks/Example4_md/Example2_atomistic/MD.ipynb)

#### 2.1.2. read_md_data_cell

* [Example 1](../notebooks/Example4_md/Example2_atomistic/MD.ipynb)


### 2.2. build

#### 2.2.1. make_xyz_mat

* [Example 1](../notebooks/Example4_md/Example2_atomistic/MD.ipynb)

#### 2.2.2. add_atom_to_system

* [Example 1](../notebooks/Example4_md/Example1_lattice/Lattice_MD.ipynb)
.. Libra documentation master file, created by
   sphinx-quickstart on Mon Jan 28 14:45:54 2019.
   You can adapt this file completely to your liking, but it should at least
   contain the root `toctree` directive.

Libra Documentation 
**************************

.. toctree::
   :maxdepth: 2
   :caption: Contents:

   reference/libra_py   


Indices and tables
==================

* :ref:`genindex`
* :ref:`modindex`
* :ref:`search`

libra_py
**************************

.. toctree::
   :maxdepth: 2
   :caption: Contents:

   libra_py/dynamics.rst
   libra_py/models.rst
   libra_py/workflows.rst

   libra_py/acf.rst
   libra_py/autoconnect.rst
   libra_py/build.rst
   libra_py/CP2K_methods.rst
   libra_py/cube_file_methods.rst
   libra_py/data_conv.rst
   libra_py/data_outs.rst
   libra_py/data_read.rst
   libra_py/data_savers.rst
   libra_py/data_stat.rst
   libra_py/DFTB_methods.rst
   libra_py/dynamics_plotting.rst
   libra_py/ERGO_methods.rst
   libra_py/fgr_py.rst
   libra_py/fit.rst
   libra_py/fix_motion.rst
   libra_py/ft.rst
   libra_py/Gaussian_methods.rst
   libra_py/hpc_utils.rst
   libra_py/hungarian.rst
   libra_py/influence_spectrum.rst
   libra_py/init_ensembles.rst
   libra_py/init_system.rst
   libra_py/LAMMPS_methods.rst
   libra_py/LoadGAFF.rst
   libra_py/LoadMMFF94.rst
   libra_py/LoadMolecule.rst
   libra_py/LoadPT.rst
   libra_py/LoadTRIPOS.rst
   libra_py/LoadUFF.rst
   libra_py/namd.rst
   libra_py/normal_modes.rst
   libra_py/nve_md.rst
   libra_py/parse_gamess.rst
   libra_py/pdos.rst
   libra_py/probabilities.rst
   libra_py/QE_methods.rst
   libra_py/QE_utils.rst
   libra_py/regexlib.rst
   libra_py/scan.rst
   libra_py/scf.rst
   libra_py/tsh.rst
   libra_py/tsh_stat.rst
   libra_py/unavoided.rst
   libra_py/units.rst
   libra_py/vesta2qe.rst



parse_gamess
*******************

.. automodule:: libra_py.parse_gamess
   :members:

data_conv
*******************

.. automodule:: libra_py.data_conv
   :members:

QE_methods
*******************

.. automodule:: libra_py.QE_methods
   :members:

LoadMMFF94
*******************

.. automodule:: libra_py.LoadMMFF94
   :members:

fgr_py
*******************

.. automodule:: libra_py.fgr_py
   :members:

init_system
*******************

.. automodule:: libra_py.init_system
   :members:

hungarian
*******************

.. automodule:: libra_py.hungarian
   :members:

ERGO_methods
*******************

.. automodule:: libra_py.ERGO_methods
   :members:

models
*******************

.. toctree::
   :maxdepth: 2
   :caption: Contents:

   models/Beswick_Jortner.rst
   models/Faist_Levine.rst
   models/Henon_Heiles.rst
   models/Holstein.rst
   models/Libra.rst
   models/LVC.rst
   models/Martens.rst
   models/SSY.rst
   models/Tully.rst

normal_modes
*******************

.. automodule:: libra_py.normal_modes
   :members:

CP2K_methods
*******************

.. automodule:: libra_py.CP2K_methods
   :members:

DFTB_methods
*******************

.. automodule:: libra_py.DFTB_methods
   :members:

data_stat
*******************

.. automodule:: libra_py.data_stat
   :members:

workflows
*******************

.. toctree::
   :maxdepth: 2
   :caption: Contents:

   workflows/nbra.rst


scf
*******************

.. automodule:: libra_py.scf
   :members:

pdos
*******************

.. automodule:: libra_py.pdos
   :members:

cube_file_methods
*******************

.. automodule:: libra_py.cube_file_methods
   :members:

acf
*******************

.. automodule:: libra_py.acf
   :members:

unavioded
*******************

.. automodule:: libra_py.unavoided
   :members:

Gaussian_methods
*******************

.. automodule:: libra_py.Gaussian_methods
   :members:

build
*******************

.. automodule:: libra_py.build
   :members:

init_ensembles
*******************

.. automodule:: libra_py.init_ensembles
   :members:

regexlib
*******************

.. automodule:: libra_py.regexlib
   :members:

LoadUFF
*******************

.. automodule:: libra_py.LoadUFF
   :members:

tsh
*******************

.. automodule:: libra_py.tsh
   :members:

vesta2qe
*******************

.. automodule:: libra_py.vesta2qe
   :members:

data_savers
*******************

.. automodule:: libra_py.data_savers
   :members:

units
*******************

.. automodule:: libra_py.units
   :members:

fit
*******************

.. automodule:: libra_py.fit
   :members:

namd
*******************

.. automodule:: libra_py.namd
   :members:

LoadPT
*******************

.. automodule:: libra_py.LoadPT
   :members:

probabilities
*******************

.. automodule:: libra_py.probabilities
   :members:

hpc_utils
*******************

.. automodule:: libra_py.hpc_utils
   :members:

dynamics
*******************

.. toctree::
   :maxdepth: 2
   :caption: Contents:

   dynamics/exact.rst
   dynamics/heom.rst
   dynamics/tsh.rst



Indices and tables
==================

* :ref:`genindex`
* :ref:`modindex`
* :ref:`search`

QE_utils
*******************

.. automodule:: libra_py.QE_utils
   :members:

scan
*******************

.. automodule:: libra_py.scan
   :members:

ft
*******************

.. automodule:: libra_py.ft
   :members:

data_outs
*******************

.. automodule:: libra_py.data_outs
   :members:

LoadMolecule
*******************

.. automodule:: libra_py.LoadMolecule
   :members:

influence_spectrum
*******************

.. automodule:: libra_py.influence_spectrum
   :members:

LoadGAFF
*******************

.. automodule:: libra_py.LoadGAFF
   :members:

fix_motion
*******************

.. automodule:: libra_py.fix_motion
   :members:

tsh_stat
*******************

.. automodule:: libra_py.tsh_stat
   :members:

autoconnect
*******************

.. automodule:: libra_py.autoconnect
   :members:

LAMMPS_methods
*******************

.. automodule:: libra_py.LAMMPS_methods
   :members:

LoadTRIPOS
*******************

.. automodule:: libra_py.LoadTRIPOS
   :members:

nve_md
*******************

.. automodule:: libra_py.nve_md
   :members:

data_read
*******************

.. automodule:: libra_py.data_read
   :members:

dynamics_plotting
*******************

.. automodule:: libra_py.dynamics_plotting
   :members:

heom
*******************

.. toctree::
   :maxdepth: 2
   :caption: Contents:


.. automodule:: libra_py.dynamics.heom.compute
   :members:

.. automodule:: libra_py.dynamics.heom.plot
   :members:

.. automodule:: libra_py.dynamics.heom.save
   :members:




exact
*******************

.. toctree::
   :maxdepth: 2
   :caption: Contents:


.. automodule:: libra_py.dynamics.exact.compute
   :members:

.. automodule:: libra_py.dynamics.exact.plot
   :members:

.. automodule:: libra_py.dynamics.exact.save
   :members:




tsh
*******************

.. toctree::
   :maxdepth: 2
   :caption: Contents:


.. automodule:: libra_py.dynamics.tsh.compute
   :members:

.. automodule:: libra_py.dynamics.tsh.plot
   :members:

.. automodule:: libra_py.dynamics.tsh.recipes_Ehrenfest
   :members:

.. automodule:: libra_py.dynamics.tsh.save
   :members:




Holstein
*******************

.. automodule:: libra_py.models.Holstein
   :members:

LVC
*******************

.. automodule:: libra_py.models.LVC
   :members:

Henon_Heiles
*******************

.. automodule:: libra_py.models.Henon_Heiles
   :members:

Martens
*******************

.. automodule:: libra_py.models.Martens
   :members:

Tully
*******************

.. automodule:: libra_py.models.Tully
   :members:

Libra
*******************

.. automodule:: libra_py.models.Libra
   :members:

Faist_Levine
*******************

.. automodule:: libra_py.models.Faist_Levine
   :members:

SSY
*******************

.. automodule:: libra_py.models.SSY
   :members:

Beswick_Jortner
*******************

.. automodule:: libra_py.models.Beswick_Jortner
   :members:

nbra
*******************

.. toctree::
   :maxdepth: 2
   :caption: Contents:

   nbra/compute_hprime.rst
   nbra/compute_properties.rst
   nbra/decoherence_times.rst
   nbra/lz.rst
   nbra/mapping.rst
   nbra/qsh.rst
   nbra/step2.rst
   nbra/step2_analysis.rst
   nbra/step2_dftb.rst
   nbra/step2_ergoscf.rst
   nbra/step2_many_body.rst
   nbra/step3.rst
   nbra/step4.rst


Indices and tables
==================

* :ref:`genindex`
* :ref:`modindex`
* :ref:`search`

step2
*******************

.. automodule:: libra_py.workflows.nbra.step2
   :members:

compute_hprime
*******************

.. automodule:: libra_py.workflows.nbra.compute_hprime
   :members:

decoherence_times
*******************

.. automodule:: libra_py.workflows.nbra.decoherence_times
   :members:

compute_properties
*******************

.. automodule:: libra_py.workflows.nbra.compute_properties
   :members:

step2_many_body
*******************

.. automodule:: libra_py.workflows.nbra.step2_many_body
   :members:

lz
*******************

.. automodule:: libra_py.workflows.nbra.lz
   :members:

qsh
*******************

.. automodule:: libra_py.workflows.nbra.qsh
   :members:

step2_analysis
*******************

.. automodule:: libra_py.workflows.nbra.step2_analysis
   :members:

step2_dftb
*******************

.. automodule:: libra_py.workflows.nbra.step2_dftb
   :members:

step2_ergoscf
*******************

.. automodule:: libra_py.workflows.nbra.step2_ergoscf
   :members:

step4
*******************

.. automodule:: libra_py.workflows.nbra.step4
   :members:

step3
*******************

.. automodule:: libra_py.workflows.nbra.step3
   :members:

mapping
*******************

.. automodule:: libra_py.workflows.nbra.mapping
   :members:

