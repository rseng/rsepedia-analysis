# Detailed descriptions of each module.

  ## Read_qc
  The metaWRAP::Read_qc module is meant to pre-process raw Illumina sequencing reads in preparation for assembly and alignment. The raw reads are trimmed based on adapted content and PHRED scored with the default setting of Trim-galore, ensuring that only high-quality sequences are left. Then reads are then aligned to the host genome (e.g. human) with bmtagger, and any host reads are removed from the metagenomic data to remove host contamination. Read pairs where only one read was aligned to the host genome are also removed. Finally, FASTQC is used to generate quality reports of the raw and final read sets in order to assess read quality improvement. The users have control over which of the above features they wish to use. 
	
  ## Assembly
  The metaWRAP::Assembly module allows the user to assemble a set of metagenomic reads with either metaSPAdes or MegaHit (default). While metaSPAdes results in a superior assembly in most samples, MegaHit is much more memory efficient, faster, and scales well with large datasets.  In addition to simplifying parameter selection for the user, this module also sorts and formats the MegaHit assembly in a way that makes it easier to inspect. The contigs are sorted by length and their naming is changed to resemble that of SPAdes, including the contig ID, length, and coverage. Finally, short scaffolds are discarded (<1000bp), and an assembly report is generated with QUAST. 
	
  ## Kraken
  The metaWRAP::Kraken module takes in any number of fastq or fasta files, classifies the contained sequences with KRAKEN, and reports the taxonomy distribution in a kronagram using KronaTools. If the sequences passed to the module belong to an assembly and follow the contig naming convention of the Assembly module, the taxonomy of each contig is weighted based on its length and coverage [weight=coverage*length]. The classifications of the sequences are then summarized in a format that KronaTools’ ktImportText function recognizes, and a final kronagram in html format is made with all the samples.
	
  ## Binning
  The metaWRAP::Binning module is meant to be a convenient wrapper around three metagenomic binning software: MaxBin2, metaBAT2, and CONCOCT. First the metagenomic assembly is indexed with bwa-index, and then paired end reads from any number of samples are aligned to it. The alignments are sorted and compressed with samtools, and library insert size statistics are also gathered at the same time (insert size average and standard deviation). metaBAT2’s jgi_summarize_bam_contig_depths function is used to generate contig adundance table, and it is then converted into the correct format for each of the three binners to take as input. After MaxBin2, metaBAT2, and CONCOCT finish binning the contigs with default settings (the user can specify which software he wants to bin with), the final bins folders are created with formatted bin fasta files for easy inspection. Optionally, the user can chose to immediately run CheckM on the bins to determine the success of the binning. CheckM’s lineage_wf function is used to predict essential genes and estimate the completion and contamination of each bin, and a custom script is used to generate an easy to view report on each bin set. 
	
  ## Bin_refinement
  The metaWRAP::Bin_refinement module utilizes a hybrid approach to take in two or three bin sets that were obtained with different software (or the same software with different parameters) and produces a consolidated, improved bin set. First, binning_refiner is used to cready hybridized bins from every possible combination of sets. If there were three bin sets: A, B, and C, then the following hybrid sets will be produced with binning_refiner: AB, BC, AC, and ABC. CheckM is then run to evaluate the completion and contamination of the bins in each of the 7 bin sets (3 originals, 4 hybridized). The bins sets are then iteratively compared to each other, and each pair is consolidated into an improved bin set. To do this, the same bin is identified within the two bin sets based on a minimum of 80% overlap in genome length, and the better bin is determined based on which bin has the higher score. The scoring function is S=Completion-5*Contamination. After all bin sets are incorporated into the consolidated bin collection, a de-replication function removes any duplicate contigs. If a contig is present in more than one bin, it is removed from all but the best bin (based on scoring function). CheckM is then run on the final bin set and a final report file is generated showing the completion, contamination, and other statistics generated by CheckM for each bin. Completion and contamination rank plots are also generated to evaluate the success of the Bin_refinement module, and compare its output to the quality of the original bins.
	
  ## Reassemble_bins
  The metaWRAP::Reassemble_bins module aims to improve a set of bins by finding reads that align to them and re-assembling them. First, bwa is used to index the entire metagenomic assembly and align fastq reads back to it. Reads mapping back to contings belonging to metagenomic bins are stored in separate fastq files. If only one read mate is aligned, the pair still gets recruited into that bin. For each bin, two sets of reads are stored – reads mapping perfectly (strict), and reads mapping with less than 3 mismatches (permissive). The each set of reads is then reassembled with SPAdes with the --carefull setting, and short contigs (<1000bp) are removed. CheckM is then used to evaluate the completion and contamination of each of the three versions of each bin – the original bin, the strict re-assembled bin, and permissive reassembled bin. The best version is chosen based on the highest score [S=Completion-5*Contamination], and added to the final bin set. The final bins set it then re-evaluated with CheckM, and summary statistics are generated. Additionally, a completion and contamination rank plots is generated to evaluate the improvements in the bin sets following reassembly. 
	
  ## Quant_bins
  The metaWRAP::Quant_bins module quickly estimates the abundance of bins across a number of samples. Salmon is used to index the entire metagenomic assembly, and then align reads from each sample back to the assembly. Coverage tables are generated estimating the abundance of each contig in each sample. The average abundance of each bin in each sample is calculated by taking the length-weighted average of the bins’s contig abundances. A final bin abundance table is made, and a clustered heatmap is generated to visualize bin abundance variation across samples.
	
  ## Blobology
  The metaWRAP::Blobology module creates blobplots (a GC vs abundance plot of all the contigs) of a metagenomic assembly, and annotates it with phylogenetic information or bin information. The assembly contigs can be randomly sub-sampled based on user preference. 
First, the taxonomy of each contig is estimated with Mega-BLAST by taking the top-most confident alignment against the NCBI_nt database. Then the assembly is indexed and the reads from any number of samples are aligned against it with bowtie2. Finally Blobology’s gc_cov_annotate.pl function is used to generate a blobplot file with the GC, coverage (in all samples), and taxonomy of each contig. If the user specified a set of bins to annotate, the contigs are also annotated with the bins they belong to. Finally, Blobology’s makeblobplot.R function is used to make the blobplots of the contigs across all the provided samples, with taxonomic annotation (at the super-kingdom, phylum, and order level) and with binning annotation (which contigs belong to which bin and the phylum taxonomy of only binned contigs). 
	
  ## Classify_bins
  The metaWRAP:: Classify_bins module is a conservative, but accurate way to assign taxonomy to a set of metagenomic bins. First, the contigs in all bins are combined into one file, and MegaBLAST is used to align the contigs to the NCBI_nt database.  The alignment results are then used by taxator-kt to estimate the most likely taxonomy of each contig. Finally, the overall most likely taxonomy of each metagenomic bin is estimated from individual contig taxonomy predictions. Taxonomy of each contig are added to a phylogenetic tree, adding weight to each branch based on the length of that contig. The tree is then traversed from the root, going down a taxonomic rank only if the weight of the next branch is >50% of the current branch, indicating a minimum confidence in that prediction. Once no further taxonomic rank can be estimated, the final taxonomy of that bin is reported.

## Annotate_bins
The metaWRAP::Annotate_bins module takes in a set of bins and quickly functionally annotates them with PROKKA v1.12. PROKKA itself utilizes a variety of software to make its predictions: BLAST, HMMER, Aragorn, Prodigal, tbl2asn, and Infernal. The annotation process is parallelized for any number of bins and threads. For each bin, the module returns the annotation file in GFF format, and two FastA files with untranslated and translated genes.

# A guide to analyzing metagenomic data with metaWRAP

Note: This pipeline is only a guide. None of metaWRAP's modules are dependant of each other, so if you want to do certain steps with another software, you are free to do so. For example, if you want to try the Reassemble_bins module on your own bins, you do not need to run the other modules just to get to that point. Or if you want to use a different assembler that metaSPAdes of MegaHit, you can do so, and then proceed to the rest of the pipeline with your own assembly.

## Step 0: Download sample metagenomic data from the metaHIT gut survey (or use your own unzipped, demultiplexed, paired-end Illumina reads). 

Download data from 3 samples:
```
wget ftp.sra.ebi.ac.uk/vol1/fastq/ERR011/ERR011347/ERR011347_1.fastq.gz
wget ftp.sra.ebi.ac.uk/vol1/fastq/ERR011/ERR011347/ERR011347_2.fastq.gz

wget ftp.sra.ebi.ac.uk/vol1/fastq/ERR011/ERR011348/ERR011348_1.fastq.gz
wget ftp.sra.ebi.ac.uk/vol1/fastq/ERR011/ERR011348/ERR011348_2.fastq.gz

wget ftp.sra.ebi.ac.uk/vol1/fastq/ERR011/ERR011349/ERR011349_1.fastq.gz
wget ftp.sra.ebi.ac.uk/vol1/fastq/ERR011/ERR011349/ERR011349_2.fastq.gz
```

Unzip the data
```
gunzip *.gz
```


Place the raw sequencing reads into a new folder
```
mkdir RAW_READS
mv *fastq RAW_READS

ls RAW_READS
ERR011347_1.fastq
ERR011347_2.fastq
ERR011348_1.fastq
ERR011348_2.fastq
ERR011349_1.fastq
ERR011349_2.fastq
```


## Step 1: Run metaWRAP-Read_qc to trim the reads and remove human contamination
Note: you will need the bmtagger hg38 index to remove the human reads - see the metaWRAP database installation instructions. You may also use another host genome to filter against with the `-x` option. Alternatively, use the `--skip-bmtagger` flag of of the ReadQC module to only do the read trimming.

Individually process each sample
```
mkdir READ_QC
metawrap read_qc -1 RAW_READS/ERR011347_1.fastq -2 RAW_READS/ERR011347_2.fastq -t 24 -o READ_QC/ERR011347
metawrap read_qc -1 RAW_READS/ERR011348_1.fastq -2 RAW_READS/ERR011348_2.fastq -t 24 -o READ_QC/ERR011348
metawrap read_qc -1 RAW_READS/ERR011349_1.fastq -2 RAW_READS/ERR011349_2.fastq -t 24 -o READ_QC/ERR011349
```

Alternatively, process all samples at the same time with a parallel for loop (especially if you have many samples):
```
for F in RAW_READS/*_1.fastq; do 
	R=${F%_*}_2.fastq
	BASE=${F##*/}
	SAMPLE=${BASE%_*}
	metawrap read_qc -1 $F -2 $R -t 1 -o READ_QC/$SAMPLE &
done	
```

Or as a one-liner: `for F in RAW_READS/*_1.fastq; do R=${F%_*}_2.fastq; BASE=${F##*/}; SAMPLE=${BASE%_*}; metawrap read_qc -1 $F -2 $R -t 1 -o READ_QC/$SAMPLE & done`


Lets have a glance at one of the output folders: `ls READ_QC/ERR011347`

These are html reports of the read quality before and after QC:
```
post-QC_report
pre-QC_report
```

Original raw reads:
![Read quality before QC](https://i.imgur.com/x8aaFWs.png)
Final QC'ed reads:
![Read quality before QC](https://i.imgur.com/drJfxC9.png)


These are the final trimmed and de-contaminated reads:
```
final_pure_reads_1.fastq
final_pure_reads_2.fastq
```

Move over the final QC'ed reads into a new folder
```
mkdir CLEAN_READS
for i in READ_QC/*; do 
	b=${i#*/}
	mv ${i}/final_pure_reads_1.fastq CLEAN_READS/${b}_1.fastq
	mv ${i}/final_pure_reads_2.fastq CLEAN_READS/${b}_2.fastq
done
```


## Step 2: Assembling the metagenomes with the metaWRAP-Assembly module
Note: Depending on your goals you may want to assemble each sample seperately, but for the purposes of analyzing the whole community across samples, we will be co-assembling our samples.

Concatinate the reads from all the samples:
```
cat CLEAN_READS/ERR*_1.fastq > CLEAN_READS/ALL_READS_1.fastq
cat CLEAN_READS/ERR*_2.fastq > CLEAN_READS/ALL_READS_2.fastq
```

Assemble the reads with the metaSPAdes option flag (usually prefered over MegaHIT unless you have a very large data set):
```
metawrap assembly -1 CLEAN_READS/ALL_READS_1.fastq -2 CLEAN_READS/ALL_READS_2.fastq -m 200 -t 96 --use-metaspades -o ASSEMBLY
```

You will find the assembly file in `ASSEMBLY/final_assembly.fasta`, and the QUAST assembly report html in `ASSEMBLY/assembly_report.html`!

Assembly statistics:
![Assembly stats](https://i.imgur.com/RbDldGU.png)


Looking at the top 10 contigs shows we got some longer contigs (considering that we are working with just 7Gbp of data)!
```
grep ">" ASSEMBLY/final_assembly.fasta | head

>NODE_1_length_196124_cov_2.427049
>NODE_2_length_176373_cov_3.889994
>NODE_3_length_163601_cov_3.070200
>NODE_4_length_142996_cov_2.771017
>NODE_5_length_109931_cov_3.516837
>NODE_6_length_106321_cov_2.842875
>NODE_7_length_99368_cov_2.860703
>NODE_8_length_95669_cov_2.506714
>NODE_9_length_91511_cov_12.466716
>NODE_10_length_88949_cov_2.730882
```

## Step 3: Run Kraken module on both reads and the assembly
Running kraken on the reads will give us an idea of the taxonomic composition of the communities in the three samples, while running kraken on the assembly will give us an idea what taxonomic groups were assembled better than others (the assembly process is heavily biased and should not be used to infer overall community composition).

Run the Kraken module on all files at once, subsetting the reads to 1M reads per sample to speed up the run
```
metawrap kraken -o KRAKEN -t 96 -s 1000000 CLEAN_READS/ERR*fastq ASSEMBLY/final_assembly.fasta
```

Lets have a look out the output folder:
```
ERR011347.kraken  ERR011348.kraken  ERR011349.kraken  final_assembly.kraken  kronagram.html
ERR011347.krona   ERR011348.krona   ERR011349.krona   final_assembly.krona
```

The .kraken files contain the KRAKEN-estimated taxonomy of each read or contig, while the `.krona` files summarize taxonomy statistics to be fed into KronaTools, which makes the kronagram.html file. The `kronagram.html` file contains all the taxonomy information from all the samples and the co-assembly. Inspecting the kronas in a web browser will show you what the community composition is like.

For example, here is the taxonomic composition of our first sample:
![Krona](https://i.imgur.com/jZiFPUV.png)



## Step 4: Bin the co-assembly with three different algorithms with the Binning module

The initial binning process with CONCOCT, MaxBin, and metaBAT will be the more time intensive steps (especially CONCOCT and MaxBin), so I would advise you to run the Binning module with each of the algorithms seperately. However, metaWRAP supports running all three together. Our dataset is reasonably small, so I will run all three binning predictions at the same time.

If you are used to a different binning software(s), feel free to run them instead. The downstream refinement process (the Bin_refinement module) takes in up to 3 different bin sets, although you can get around this by running splitting your bin sets into groups and then recursively consolidating them.

Run the binning module with all three binners - notice how I put both the F and R read files at the end of the command.
```
metawrap binning -o INITIAL_BINNING -t 96 -a ASSEMBLY/final_assembly.fasta --metabat2 --maxbin2 --concoct CLEAN_READS/ERR*fastq
```

In the output folder, we see folders with the 3 final bin sets, and a file containing the average and standard deviation of the library insert sizes (this may or may not be usefull to you).  
```
insert_sizes.txt  concoct_bins	maxbin2_bins  metabat2_bins  work_files
```
Looking inside these folders reveals that we found 47, 29, and 20 bins with concoct, metabat2, and maxbin2, respectively. But we do not know how good these bins are yet (unless you used the `--run-checkm` flag). We will find out the quality of the bins in the next step!


## Step 5: Consolidate bin sets with the Bin_refinement module
Note: make sure you downloaded the CheckM database (see metaWRAP database instrucitons)

Now what you have metabat2, maxbin2, and concoct bins, lets consolidate them into a single, stronger bin set! If you used your own binning software, feel free to use any 3 bin sets. If you have more than 3, you can run them in groups. For example if you have 5 bin sets, try consolidating 1+2+3 and 4+5, and then consolidate again between the outputs.

When you do your refinement, make sure to put some thought into the minimum compleiton (-c) and maximum contamination (-x) parameters that you enter. During refinement, metaWRAP will have to chose the best version of each bin between 7 different versions of each bin. It will dynamically adjust to prioritize the bin quality that you desire. Consider this example: bin_123 comes in four versions in terms of completion/contamination: 95/15, 90/10, 80/5, 70/5. Which one is the best version? The high completion but high contamination, or the less complete but more pure bin? This is subjective and depends on what you value in a bin and on what your purposes for bin extraction are. 

By default, the minimum completion if 70%, and maximum contamination is 5%. However, because of the relatively poor depth of these demonstration samples, we will set minimum completion to 50% and maximum contamination to 10%, but feel free to be much more picky. Parameters like `-c 90 -x 5` are not unreasonable in some data (but you will get fewer bins, of course).

Run metaWRAP's Bin_refinement module:
```
metawrap bin_refinement -o BIN_REFINEMENT -t 96 -A INITIAL_BINNING/metabat2_bins/ -B INITIAL_BINNING/maxbin2_bins/ -C INITIAL_BINNING/concoct_bins/ -c 50 -x 10
```

In the output directory, you will see the three original bin folders we fed in, as well as `metaWRAP` directory, which contains the final, consilidated bins. You will also see a `Binning_refiner` bin set - this is an internal benchmark for the bins produced by using another consolidation software (Binning_refiner). You can ignore this set - it will likely have low contamination and completion. You will also see .stats files for each one of the bin directories. 
```
concoct_bins.stats	maxbin2_bins.stats	metabat2_bins.stats	metawrap_50_10.stats	Binning_refiner.stats
concoct_bins		maxbin2_bins		metabat2_bins		metawrap_50_10_bins	Binning_refiner
```

Note that the '_50_10_' part of the naming refers to the `-c` and `-x` options you chose for the algorithm optimization. You can repeat the run with the same output directory using different options to get different results and see what works best on your sample (using '_90_5_' for example to get more near-complete genomes). The re-calculation will use the existing binning outputs, greatly reducing run time.

The .stat files contain usefull information about each bin, inscuding its completeness and contamination. For example, `cat BIN_REFINEMENT/metawrap_50_10.stats`:
```
bin	completeness	contamination	GC	lineage	N50	size	binner
bin.5	100.0	1.6	0.311	Euryarchaeota	12686	1705532	binsO.checkm
bin.4	99.32	1.342	0.408	Clostridiales	58825	2083650	binsO.checkm
bin.14	86.69	5.896	0.293	Bacteria	3754	2199676	binsO.checkm
bin.6	86.22	2.348	0.371	Clostridiales	4283	2055792	binsO.checkm
bin.8	83.16	2.516	0.446	Clostridiales	2723	1467846	binsO.checkm
bin.2	80.34	0.0	0.469	Bacteria	11936	3579466	binsO.checkm
bin.9	76.57	2.648	0.425	Selenomonadales	3155	1796524	binsO.checkm
bin.13	74.82	1.710	0.435	Bacteroidales	7456	3643185	binsO.checkm
bin.3	74.53	0.377	0.284	Clostridiales	10440	1241933	binsO.checkm
bin.10	65.78	0.0	0.263	Bacteria	3045	1159966	binsO.checkm
bin.11	64.85	3.776	0.417	Bacteroidales	2086	3103352	binsO.checkm
bin.1	57.36	0.0	0.430	Bacteria	4628	2673426	binsO.checkm
bin.7	52.94	1.724	0.501	Bacteria	3614	1465011	binsO.checkm
```

To evaluate how many "good bins" (based on out >50% comp., <10% cont. metric) metaWRAP produced, we can run
```
cat BIN_REFINEMENT/metawrap_50_10_bins.stats | awk '$2>50 && $3<10' | wc -l
13
```

By inspecting the other files, we find that metaBAT2, MaxBin2, CONCOCT, and metaWRAP produced 11, 7, 10, and 13 bins, respectively. So metaWRAP produced 2 more bins that the best single binner. Not bad! But this is just the number of bins. 

To closer compare the bin sets in terms of completion and contamination, we can look at the plots in `BIN_REFINEMENT/figures/`:
![Bin_refinement](https://i.imgur.com/m6RRJxi.jpg)

Note: This graph no longer has `Binning_refiner` in it, to reduce confusion. If you want to see Binning_refiner's performance, look at binsABC (or binsAB if you have two bin sets) in the other figure.

As you can see, the refinment process signifficantly produced the best bin set in terms of both compleiton and contamination. Keep in mind that these improvements are even more dramatic in more complex samples.


## Step 6: Visualize the community and the extracted bins with the Blobology module
Lets use the Blobology module to project the entire assembly onto a GC vs Abundance plane, and annote them with taxonomy and bin information. This will not only give us an idea of what these microbial communities are structured like, but will also show us our binning success in a more visual way. 

NOTE: In order to annotate the blobplot with bins with the `--bins` flag, you **MUST use the non-reassembled bins**! In other words, use the bins produced by the Bin_Refinment module, not the Reassemble_bins module. 

Note2: You will need R for this module.

```
metawrap blobology -a ASSEMBLY/final_assembly.fasta -t 96 -o BLOBOLOGY --bins BIN_REFINEMENT/metawrap_50_10_bins CLEAN_READS/ERR*fastq
```

You will find that the output has a number of blob plots of our communities, annotated with different levels of taxonomy or their bin membership. Note that to help with visualizing the bins, some of the plots only contain the contigs that were successfully binned (these files have `.binned.blobplot.` in their names). 

Phyla taxonomy of the entire assembled community:
![Phyla](https://i.imgur.com/VihLGWb.jpg)

Bin membership of all the contigs:
![bins](https://i.imgur.com/GDmIYe5.jpg)


## Step 7: Find the abundaces of the draft genomes (bins) across the samples
We would like to know how the extracted genomes are distributed across the samples, and in what abundances each bin is present in each sample. The Quant_bin module can give us this information. It used Salmon - a tool conventionally used for transcript quantitation - to estimate the abundance of each scaffold in each sample, and then computes the average bin abundances.

NOTE: In order to run this module, it is **highly** recomended to use the non-reassembled bins (the bins produced by the Bin_Refinment module, not the Reassemble_bins module) and provide the entire non-binned assembly with the `-a` option. This will give more accurate bin abundances that are in context of the entire community. 

Lets run the Quant_bins module:
```
metawrap quant_bins -b BIN_REFINEMENT/metawrap_50_10_bins -o QUANT_BINS -a ASSEMBLY/final_assembly.fasta CLEAN_READS/ERR*fastq
```

The output contains several usefull files. First, there is the `bin_abundance_heatmap.png` - a quick heatmap made to visualize the bin abundances across the samples. 
![heatmap](https://i.imgur.com/K1RaPUT.png)


The raw data for this plot (as you will most likely want to make your own heatmaps to analyze) is in `bin_abundance_table.tab`. Note that the abundances are expressed as "genome copies per million reads", and are calculated with Salmon in a simmilar way like TPM (transcripts per million) is calculated in RNAseq analysis. As such, they should be already standardized to the individual sample size. 

```
Genomic bins	ERR011349	ERR011348	ERR011347
bin.9	0.113912116828	35.851964987	39.8440491514
bin.10	0.273774684856	9.52869077293	39.988244574
bin.1	7.87827599808	31.3262582417	72.4475075589
bin.4	1.11852631889	100.052540293	111.213423224
bin.2	42.0242612674	69.0094806385	80.200001212
bin.5	2.16260151787	22.06396779	43.7720962538
bin.11	64.2884105466	25.3703846834	29.5444322752
bin.6	517.890689122	0.379711918465	0.834196723864
bin.7	0.499019812767	61.2121001057	82.5953338481
bin.14	5.49635966692	14.631433905	32.98399834
bin.13	0.230760165209	56.0018529273	91.6502833521
bin.8	98.4767064505	38.0691238971	22.8857472565
bin.3	349.730007621	0.0911113402849	0.196554603409
```

Finally, you can view the abundances of all the individual contigs in all the samples in the `quant_files` folder.

## Step 8: Re-assemble the consolidated bin set with the Reassemble_bins module
Now that we have our final, consilidated bin set in `BIN_REFINEMENT/metawrap_bins`, we can try to further improve it with reassembly. The Reassemble bins module will collect reads belonging to each bun, and then reassemble them sepperately with a "permissive" and a "strict" algorithm. Only the bins that improved through reassembly will be altered in the final set.

Let us run the Reassemble_bins module with all the reads we have:
```
metawrap reassemble_bins -o BIN_REASSEMBLY -1 CLEAN_READS/ALL_READS_1.fastq -2 CLEAN_READS/ALL_READS_2.fastq -t 96 -m 800 -c 50 -x 10 -b BIN_REFINEMENT/metawrap_50_10_bins
```

Looking at the output in `BIN_REASSEMBLY/reassembled_bins.stats`, we can see that 3 bins were improved though strict reassembly, 6 improved thorugh permissive reassembly, and 4 bins could not be improved (`.strict`, `.permissive`, and `.orig` bin extensions, respectively):
```
bin	completeness	contamination	GC	lineage	N50	size	binner
bin.10.orig	65.78	0.0	0.263	Bacteria	3045	1159966	NA
bin.7.strict	54.94	0.671	0.501	Clostridiales	3947	1474089	NA
bin.4.permissive	99.32	1.342	0.408	Clostridiales	72135	2088821	NA
bin.2.permissive	82.06	0.0	0.469	Bacteria	18989	3604843	NA
bin.14.strict	85.84	3.066	0.293	Bacteria	4576	2201824	NA
bin.9.permissive	76.74	2.554	0.425	Selenomonadales	3601	1802438	NA
bin.13.permissive	78.37	1.357	0.435	Bacteroidales	9887	3675176	NA
bin.11.orig	64.85	3.776	0.417	Bacteroidales	2086	3103352	NA
bin.6.permissive	88.04	1.006	0.371	Clostridiales	6288	2070146	NA
bin.5.orig	100.0	1.6	0.311	Euryarchaeota	12686	1705532	NA
bin.3.permissive	74.91	0.396	0.284	Clostridiales	16578	1243641	NA
bin.1.orig	57.36	0.0	0.430	Bacteria	4628	2673426	NA
bin.8.strict	83.89	1.342	0.446	Clostridiales	3870	1474833	NA
```

But how much did our bin set really improve? We can look at the `BIN_REASSEMBLY/reassembly_results.png` plot to compare the original and reassembled sets. We can see that the bin reassembly modestly improved the bin N50 and completion metrics, and signifficantly reduced contamination. Fantastic!
![heatmap](https://i.imgur.com/V8IosYQ.jpg)


We can also view the CheckM plot of the final bins in `BIN_REASSEMBLY/reassembled_bins.png`:
![heatmap](https://i.imgur.com/Yx00fuQ.png)



## Step 9: Determine the taxonomy of each bin with the Classify_bins module
Note: you will need the NCBI_nt and NCBI_tax databases for this module (see Database Installation section).

We already got an idea for the approximate taxonomy of each bin from CheckM in the `.stats` files in the Bin_refinment and Reassemble_bins modules. We can do better than that, however. The Classify_bins module uses Taxator-tk to accurately assign taxonomy to each contig, and then consolidates the results to estimate the taxonomy of the whole bin. Of course the success and accuracy of our predictions will rely heavily on the existing database.

Estimate the taxonomy of our final, reassembled bins with the Classify_bins module:
```
metawrap classify_bins -b BIN_REASSEMBLY/reassembled_bins -o BIN_CLASSIFICATION -t 48
```

We can view the final estimated taxonomy in `BIN_CLASSIFICATION/bin_taxonomy.tab`:
```
bin.1.orig.fa	Bacteria;Firmicutes;Clostridia;Clostridiales
bin.5.orig.fa	Archaea;Euryarchaeota;Methanobacteria;Methanobacteriales;Methanobacteriaceae;Methanobrevibacter;Methanobrevibacter smithii
bin.11.orig.fa	Bacteria;Bacteroidetes;Bacteroidia;Bacteroidales;Bacteroidaceae;Bacteroides
bin.2.permissive.fa	uncultured organism
bin.10.orig.fa	Bacteria;Firmicutes;Clostridia;Clostridiales;Clostridiaceae
bin.14.strict.fa	Bacteria;Firmicutes
bin.8.strict.fa	Bacteria
bin.9.permissive.fa	Bacteria
bin.6.permissive.fa	Bacteria;Firmicutes;Clostridia;Clostridiales
bin.3.permissive.fa	Bacteria;Firmicutes;Clostridia;Clostridiales;Clostridiaceae
bin.4.permissive.fa	Bacteria
bin.13.permissive.fa	Bacteria;Bacteroidetes;Bacteroidia;Bacteroidales;Bacteroidaceae
bin.7.strict.fa	Bacteria
```
As you can see some of the bins are annotated very deeply, while others can only be classified as "Bacteria". This method is relatively trustworthy, however it often fails to annotate organisms that are very distant from anything in the NCBI database. For these tricky bins, manually looking at marker genes (such as ribosomal proteins) can result in much more sensitive taxonomy assignment.


## Step 10: Functionally annotate bins with the Annotate_bins module
Now that we have our finalized reassembled bins, we are ready to functionally annotate them for downstream functional analysis. This module simply annotates the genes with PROKKA - it cannot do the actual functional analysis for you.

Run the functional annotation module on the final, reassembled bins: 
```
metaWRAP annotate_bins -o FUNCT_ANNOT -t 96 -b BIN_REASSEMBLY/reassembled_bins/
```

You will find the functional annotations of each bin in GFF format in the `FUNCT_ANNOT/bin_funct_annotations` folder
```
head FUNCT_ANNOT/bin_funct_annotations/bin.1.orig.gff
NODE_75_length_31799_cov_0.983871	Prodigal:2.6	CDS	2866	3645	.	-	0	ID=HMOHEJHL_00001;inference=ab initio prediction:Prodigal:2.6;locus_tag=HMOHEJHL_00001;product=hypothetical protein
NODE_75_length_31799_cov_0.983871	Prodigal:2.6	CDS	3642	4478	.	-	0	ID=HMOHEJHL_00002;inference=ab initio prediction:Prodigal:2.6;locus_tag=HMOHEJHL_00002;product=hypothetical protein
NODE_75_length_31799_cov_0.983871	Prodigal:2.6	CDS	4606	5859	.	-	0	ID=HMOHEJHL_00003;inference=ab initio prediction:Prodigal:2.6;locus_tag=HMOHEJHL_00003;product=hypothetical protein
NODE_75_length_31799_cov_0.983871	Prodigal:2.6	CDS	5856	6575	.	-	0	ID=HMOHEJHL_00004;Name=ypdB;gene=ypdB;inference=ab initio prediction:Prodigal:2.6,similar to AA sequence:UniProtKB:P0AE39;locus_tag=HMOHEJHL_00004;product=Transcriptional regulatory protein YpdB
NODE_75_length_31799_cov_0.983871	Prodigal:2.6	CDS	6603	7658	.	-	0	ID=HMOHEJHL_00005;eC_number=1.1.1.261;Name=egsA;gene=egsA;inference=ab initio prediction:Prodigal:2.6,similar to AA sequence:UniProtKB:P94527;locus_tag=HMOHEJHL_00005;product=Glycerol-1-phosphate dehydrogenase [NAD(P)+]
NODE_75_length_31799_cov_0.983871	Prodigal:2.6	CDS	7843	11946	.	-	0	ID=HMOHEJHL_00006;eC_number=3.2.1.78;inference=ab initio prediction:Prodigal:2.6,similar to AA sequence:UniProtKB:A1A278;locus_tag=HMOHEJHL_00006;product=Mannan endo-1%2C4-beta-mannosidase
NODE_75_length_31799_cov_0.983871	Prodigal:2.6	CDS	12597	13586	.	-	0	ID=HMOHEJHL_00007;eC_number=3.2.1.89;Name=ganB_1;gene=ganB_1;inference=ab initio prediction:Prodigal:2.6,similar to AA sequence:UniProtKB:Q65CX5;locus_tag=HMOHEJHL_00007;product=Arabinogalactan endo-beta-1%2C4-galactanase
NODE_75_length_31799_cov_0.983871	Prodigal:2.6	CDS	13719	14564	.	-	0	ID=HMOHEJHL_00008;Name=malG_1;gene=malG_1;inference=ab initio prediction:Prodigal:2.6,similar to AA sequence:UniProtKB:P68183;locus_tag=HMOHEJHL_00008;product=Maltose transport system permease protein MalG
NODE_75_length_31799_cov_0.983871	Prodigal:2.6	CDS	14565	15476	.	-	0	ID=HMOHEJHL_00009;Name=malF_1;gene=malF_1;inference=ab initio prediction:Prodigal:2.6,similar to AA sequence:UniProtKB:P02916;locus_tag=HMOHEJHL_00009;product=Maltose transport system permease protein MalF
NODE_75_length_31799_cov_0.983871	Prodigal:2.6	CDS	15484	16704	.	-	0	ID=HMOHEJHL_00010;Name=malE;gene=malE;inference=ab initio prediction:Prodigal:2.6,similar to AA sequence:UniProtKB:P0AEY0;locus_tag=HMOHEJHL_00010;product=Maltose-binding periplasmic protein
```

You will also find the translated and unstranslated predicted genes in fasta format in `FUNCT_ANNOT/bin_translated_genes` and `FUNCT_ANNOT/bin_untranslated_genes` folders. Finally, you can find the raw PROKKA output files in `FUNCT_ANNOT/prokka_out`.

# MetaWRAP - a flexible pipeline for genome-resolved metagenomic data analysis

 MetaWRAP aims to be an **easy-to-use metagenomic wrapper suite** that accomplishes the core tasks of metagenomic analysis from start to finish: read quality control, assembly, visualization, taxonomic profiling, extracting draft genomes (binning), and functional annotation. Additionally, metaWRAP takes bin extraction and analysis to the next level (see module overview below). While there is no single best approach for processing metagenomic data, metaWRAP is meant to be a fast and simple approach before you delve deeper into parameterization of your analysis. MetaWRAP can be applied to a variety of environments, including gut, water, and soil microbiomes (see metaWRAP paper for benchmarks). Each individual module of metaWRAP is a standalone program, which means you can use only the modules you are interested in for your data.
 
 ![General walkthrough of metaWRAP modules](https://i.imgur.com/6GqRsm3.png)

## Metagenomic bin recovery improvements

 In addition to being a tool wrapper, MetaWRAP offers a **powerful hybrid approach** for extracting high-quality draft genomes (bins) from metagenomic data by using a variety of software (metaBAT2, CONCOCT, and MaxBin2, for example, since they are already wrapped into the Binning module) and utilizing their individual strengths and minimizing their weaknesses. MetaWRAP's [bin refinement module](https://i.imgur.com/JL665Qo.png) outperforms not only individual binning approaches, but also other bin consolidation programs (Binning_refiner, DAS_Tool) in both synthetic and real datasets. I emphasize that because this module is a standalone component, I encourage you to use your favorite binning softwares for the 3 intitial predictions (they do not have to come from metaBAT2, CONCOCT and MaxBin2). These predictions can also come from different parameters of the same software.

![Bin_refinement performance comparison in different microbiome types](https://i.imgur.com/KSk3l2B.jpg)


 MetaWRAP also includes a novel [bin reassembly module](https://i.imgur.com/GUSMXl8.png), which allows to drastically improve the quality of a set of bins by extracting the reads belonging to each bin, and **reassembling the bins** with a more permissive, non-metagenomic assembler. In addition to improving the N50 of the bins, this modestly increases the completion of the bins, and drastically reduces contamination. I recommend you run the reassembly on the final bins set from the Bin_refinement module, but this can be any bin set.
 

## OVERVIEW OF METAWRAP MODULES:

#### Metagemonic data pre-processing modules:
	1) Read_QC: read trimming and host (e.g. human) read removal
	2) Assembly: metagenomic assembly and QC with metaSPAdes or MegaHit
	3) Kraken/Kraken2: taxonomy profiling and visualization or reads or contigs
	
#### Bin processing modules:
	1) Binning: initial bin extraction with MaxBin2, metaBAT2, and/or CONCOCT
	2) Bin_refinement: consolidate of multiple binning predicitons into a superior bin set
	3) Reassemble_bins: reassemble bins to improve completion and N50, and reduce contamination
	4) Quant_bins: estimate bin abundance across samples
	5) Blobology: visualize the community and extracted bins with blobplots
	6) Classify_bins: conservative but accurate taxonomy prediction for bins
	7) Annotate_bins: functionally annotate genes in a set of bins
#### For more details, please consult the [metaWRAP module descriptions](https://github.com/bxlab/metaWRAP/blob/master/Module_descriptions.md) and the [publication](https://microbiomejournal.biomedcentral.com/articles/10.1186/s40168-018-0541-1).


##  SYSTEM REQUIREMENTS
 The resource requirements for this pipeline will vary greatly based on the amount of data being processed, but due to large memory requirements of many software used (KRAKEN and metaSPAdes to name a few), I recommend at 8+ cores and 64GB+ RAM. MetaWRAP officially supports only Linux x64 systems, but may be installed on OSX manually or with docker (see below).

## INSTALLATION

#### Manual installation (this is best, if you are comfortable):
 The best way to install and manage metaWRAP is to install it directly from github, and then install all of its dependancies through conda. This is how I usually use metaWRAP, as it allows to easily update the versions of metawrap and other packages. This also works on MacOS as well as Unix.  
 
 0. Install mamba: `conda install -y mamba`. Mamba will effectively replace conda and do exactly the same thing, but _much_ faster.
 1. Download or clone this ripository: `git clone https://github.com/bxlab/metaWRAP.git`
 2. Carefully configure the `yourpath/metaWRAP/bin/config-metawrap` file to it points to your desired database locations (you can modify this later). Follow the [database configuration guide](https://github.com/bxlab/metaWRAP/blob/master/installation/database_installation.md) for details.
 3. Make metaWRAP executable by adding `yourpath/metaWRAP/bin/` directory to to your `$PATH`. Either add the line `PATH=yourpath/metaWRAP/bin/:$PATH` to your `~/.bash_profile` script, or copy over the contents of `yourpath/metaWRAP/bin/` into a location already in your `$PATH` (such as `/usr/bin/` or `/miniconda2/bin/`). 
 4. (Optional but recommended) Make a new conda environment to install and manage all dependancies:
```
mamba create -y -n metawrap-env python=2.7
conda activate metawrap-env
```
5. Install all [metaWRAP dependancies](https://github.com/bxlab/metaWRAP/blob/master/conda_pkg/meta.yaml) with conda:
 ```
conda config --add channels defaults
conda config --add channels conda-forge
conda config --add channels bioconda
conda config --add channels ursky

# Unix/Linux only
mamba install --only-deps -c ursky metawrap-mg
# `conda install --only-deps -c ursky metawrap-mg` also works, but much slower

# OR
mamba install biopython blas=2.5 blast=2.6.0 bmtagger bowtie2 bwa checkm-genome fastqc kraken=1.1 kraken=2.0 krona=2.7 matplotlib maxbin2 megahit metabat2 pandas prokka quast r-ggplot2 r-recommended salmon samtools=1.9 seaborn spades trim-galore
# Note: this last solution is more universal, but you may need to manually install concoct=1.0 and pplacer.
```

#### Express Conda/Mamba installation (the quickest but least configurable):
Directly create a metawrap-specific environment and install metawrap.
```
# install mamba (replaces conda, but much faster):
 conda install -y mamba 
 
# install metawrap:
 mamba create -y --name metawrap-env --channel ursky metawrap-mg=1.3.2
 conda activate metawrap-env

# To fix the CONCOCT endless warning messages in metaWRAP=1.2+, run
 conda install -y blas=2.5=mkl
```

#### Bioconda installation (avoid):
MetaWRAP is also available through the Bioconda channel. **However**, this distribution is not recommended for most users, as I will only push major releases to Bioconda (i.e. `v1.1`, `v1.2`). This source is meant for specific applications that require a Bioconda distribution. To get the latest version of metaWRAP with the newest patches and bug fixes, please install through the `-c ursky` channel, as seen above. 

```
# Bioconda installation (not recommended):
conda install -y mamba
mamba install -y -c bioconda metawrap

# or `conda install -y -c bioconda metawrap`
```

#### Docker installation (avoid):
If you are running on OSX and dont want to install manually, or prefer to work in containerized environments, then [Docker](https://quay.io/repository/biocontainers/metawrap?tab=info) could be the way to go. **However**, as with the Bioconda distribution, I will only push major releases to Bioconda (i.e. `v1.1`, `v1.2`). To get the latest version of metaWRAP with the newest patches and bug fixes, please install through the `-c ursky` channel, as seen above. If you still need to use Docker but run into bugs that have been fixed in the latest versions, you can manually update your scripts from this repository to apply the most recent patches. To install with Docker, run:
```
# Docker installation (not recommended and not fully supported)
docker pull quay.io/biocontainers/metawrap:1.2--0
```

 
## DATABASES

 In addition to the Conda installation, you will need to configure the paths to some databases that you downloaded onto your system. Use your favorite text editor to configure these paths in /some/path/miniconda2/bin/config-metawrap and make sure everything looks correct. If you are unsure where this config file is, run:
 ``` bash
 which config-metawrap
 ```

This is very important if you want to use any functions requiring databases, but depending on what you plan to do, the databases are not mandatory for metaWRAP (see Database section below). [Follow this guide for download and configuration instructions](https://github.com/bxlab/metaWRAP/blob/master/installation/database_installation.md).

|    Database     | Size  |  Used in module |
|:---------------:|:---------------:|:-----:| 
| Checkm_DB |1.4GB| binning, bin_refinement, reassemble_bins |
| KRAKEN standard database|161GB |  kraken |
| KRAKEN2 standard database|125GB | kraken2 |
| NCBI_nt |71GB |  blobology, classify_bins |
| NCBI_tax |283MB |  blobology, classify_bins |
| Indexed hg38  	|  20GB |  read_qc |


## DETAILED PIPELINE WALKTHROUGH

  ![Detailed pipeline walkthrough](https://i.imgur.com/HDUPeXC.png)
  Note: some features of this walkthrough are depricated since v0.7. To understand specific steps of each module, you can glance at the bash code in each script.


## USAGE

Please look at the [MetaWRAP usage tutorial](https://github.com/bxlab/metaWRAP/blob/master/Usage_tutorial.md) for detailed run instructions and examples.

Once all the dependencies are in place, running metaWRAP is relatively simple. The main metaWRAP script wraps around all of its individual modules, which you can call independently.
```
metaWRAP -h
	Usage: metawrap [module] --help
	Options:

	read_qc		Raw read QC module
	assembly	Assembly module
	binning		Binning module
	bin_refinement	Refinement of bins from binning module
	reassemble_bins Reassemble bins using metagenomic reads
	quant_bins	Quantify the abundance of each bin across samples
	blobology	Blobology module
	kraken		KRAKEN module
	kraken2		KRAKEN2 module
```

Each module is run separately. For example, to run the assembly module:
```
metawrap assembly -h

Usage: metawrap assembly [options] -1 reads_1.fastq -2 reads_2.fastq -o output_dir
Options:

	-1 STR          forward fastq reads
	-2 STR          reverse fastq reads
	-o STR          output directory
	-m INT          memory in GB (default=10)
	-t INT          number of threads (defualt=1)

	--use-megahit		assemble with megahit (default)
	--use-metaspades	assemble with metaspades instead of megahit
```

### Citing metaWRAP
If you found metaWRAP usefull in your research, please cite the publication: [MetaWRAP - a flexible pipeline for genome-resolved metagenomic data analysis](https://microbiomejournal.biomedcentral.com/articles/10.1186/s40168-018-0541-1). If certain software wrapped into metaWRAP were integral to your investigation (e.g. Salmon, MaxBin2, SPAdes, Kraken, etc.) please give them credit as well.

### Error reporting
The massive scale of the metaWRAP project unfortunately means that there are lots of opportunities for different components to fail depending on the exact environments it is installed on. Note that metaWRAP is simply a bash wrapper around other popular bioinformatics programs. If one of these other programs fails, the first thing to do is to troubleshoot the installation of that software, and not worry about metaWRAP itself until that component is fixed. If one of the components refuses to work on you environment, there may not be much I can do. Also remember that if you know a bit of bash/shell you can always see how metaWRAP calls these programs by investigating and possibly changing/tweaking the script files in `bin/metawrap-modules/`.

For errors and bugs relating to the actual metaWRAP software, please open a new Issue thread on this github page, however note that I no longer actively support metaWRAP due to moving on to other jobs. If you do report an error, please include the full output (stdout and stderr) from metaWRAP, and the version of you are using (run `metawrap -v`).

### Acknowledgements
Author of pipeline: [Gherman Uritskiy](https://github.com/ursky).

Principal Investigators: [James Taylor](http://bio.jhu.edu/directory/james-taylor/) and [Jocelyne DiRuggiero](http://bio.jhu.edu/directory/jocelyne-diruggiero/)

Institution: Johns Hopkins, [Department of Cell, Molecular, Developmental Biology, and Biophysics](http://cmdb.jhu.edu/) 
Update
------

###2015-12-16: For more features and better plots please use Dominik Laetsch's blobtools at https://drl.github.io/blobtools/


Blobology
=========

Blaxter Lab, Institute of Evolutionary Biology, University of Edinburgh

**Goal**: To create blobplots or Taxon-Annotated-GC-Coverage plots (TAGC plots) to visualise the contents of genome assembly data sets as a QC step.

This repository accompanies the paper:  
**Blobology: exploring raw genome data for contaminants, symbionts and parasites using taxon-annotated GC-coverage plots.**
*Sujai Kumar, Martin Jones, Georgios Koutsovoulos, Michael Clarke, Mark Blaxter*  
(submitted 2013-10-01 to *Frontiers in Bioinformatics and Computational Biology special issue : Quality assessment and control of high-throughput sequencing data*).

It contains bash/perl/R scripts for running the analysis presented in the paper to create a preliminary assembly, and to create and collate GC content, read coverage and taxon annotation for the preliminary assembly, which can be visualised, such as Figure 2a from the paper showing TAGC plots/blobplots for *Caenorhabditis* sp. 5:
![Figure 2a. Caenorhabditis sp. 5 prelim assembly blobplot](https://raw.github.com/blaxterlab/blobology/master/pa61-scaffolds.fa.bowtie2.txt.taxlevel_order.01.png)

**Note**: This is an update to the code at github.com/sujaikumar/assemblage which was used in my thesis. I could have updated the code in that repository, but enough things have changed (the basic file formats as well) that I thought it made sense to create a new repo. Please use this version from now on.

Installation
------------

    git clone git://github.com/blaxterlab/blobology.git
    # add this directory to your path, e.g.:
    # export PATH=$PATH:/path/to/blobology

You also need the following software in your path:

1.  samtools (tested with version 0.1.19) http://sourceforge.net/projects/samtools/files/samtools/0.1.19/
2.  R (tested with version 2.15.2)
3.  ggplot2, an R graphics package (tested with ggplot2_0.9.3.1)
4.  ABySS - optional if you already have a preliminary assembly from another assembler (tested with version 1.3.6, compiled with mpi) - http://www.bcgsc.ca/platform/bioinfo/software/abyss/releases/1.3.6
5.  fastq-mcf - not needed if you already have quality and adapter trimmed reads (from the ea-utils suite, tested with version 1.1.2-537) - https://ea-utils.googlecode.com/files/ea-utils.1.1.2-537.tar.gz  

And the following databases:

1.  NCBI nt blasted (download from ftp://ftp.ncbi.nlm.nih.gov/blast/db/ nt.??.tar.gz)

        wget "ftp://ftp.ncbi.nlm.nih.gov/blast/db/nt.??.tar.gz"
        for a in nt.*.tar.gz; do tar xzf $a; done
        # or, if you have gnu parallel installed, highly recommended:
        # parallel tar xzf ::: nt.*.tar.gz

2.  NCBI taxonomy dump (download from ftp://ftp.ncbi.nlm.nih.gov/pub/taxonomy/taxdump.tar.gz)

        wget ftp://ftp.ncbi.nlm.nih.gov/pub/taxonomy/taxdump.tar.gz
        tar xzf taxdump.tar.gz

    Only the nodes.dmp and names.dmp files are needed (nodes.dmp stores the taxon ids and their parent-child relationships, whereas names.dmp stores their common and scientific names)

Rerun example with Caenorhabditis sp. 5
-----------------------------------------

Run [blobology.bash](https://github.com/blaxterlab/blobology/blob/master/blobology.bash) from this repository. Comment out the lines that you don't need (e.g., if you prefer using a different assembler for the preliminary assembly, or a different alignment tool for mapping the reads)

Broad overview of the pipeline (Figure 1 in the paper)

<img src="blobologyMethodOverview.png" alt="Figure 1. Broad overview of pipeline" width="50%" height="50%"/>

Run the blobology pipeline for your own sequence data
-----------------------------------------------------

Run [blobology.bash](https://github.com/blaxterlab/blobology/blob/master/blobology.bash) from this repository. The only things you should really need to change are the read files in the ABySS and Bowtie 2 steps. Even these steps won't be needed if you already have a preliminary assembly and BAM files from aligning your raw reads back to this assembly.

A tab separated values (TSV) text file is created by [gc_cov_annotate.pl](https://github.com/blaxterlab/blobology/blob/master/gc_cov_annotate.pl) and the ggplot2 R

Visualise data using Blobsplorer
--------------------------------

The Blobsplorer visualiser for the TSV file created above was coded by Martin Jones, and is available at 
github.com/mojones/blobsplorer


Separate contigs and reads based on blobplot visualisations
-----------------------------------------------------------

See [separate_reads.bash](https://github.com/blaxterlab/blobology/blob/master/separate_reads.bash) from this repository for 
the commands that were used to remove contaminants from the *Caenorhabditis* sp. 5 preliminary assembly.

Your own data set will require you to devise your own positive (to keep contigs, and reads, of interest) and negative (to discard 
contigs and reads belonging to contaminants) filters.
README.md of /blobology/dev

––––––––––––––––––––––––––––––

Input formats

legacy format:

| columns |    ID   |  len |   gc  |  cov1 |  cov2 |  cov3 |     taxlevel1    |    taxlevel2   |    taxlevel3    |       taxlevel4       |
|:-------:|:-------:|:----:|:-----:|:-----:|:-----:|:-----:|:----------------:|:--------------:|:---------------:|:---------------------:|
|   type  |   str   |  int | float | float | float | float |        str       |       str      |       str       |          str          |
| example |    ID   |  len |   gc  |  cov1 |  cov2 |  cov3 | taxlevel_species | taxlevel_order | taxlevel_phylum | taxlevel_superkingdom |
|         | contig1 | 1000 |  0.56 |  100  | 150.2 |  80.3 |    Danio rerio   |  Cypriniformes |     Chordata    |       Eukaryota       |
|         | contig2 |  500 |  0.54 |  12.9 |  11.1 |  15.7 |   Not Annotated  |  Not Annotated |  Not Annotated  |     Not Annotated     |


- eval format as in gc_cov_annotate_v1.pl --evalue (requires BLAST -outfmt '6 qseqid taxid std')

| columns | ID      | len  | gc    | cov1  | cov2  | cov3  | taxlevel1        | taxlevel2      | taxlevel3       | taxlevel4             | eval       |
|---------|---------|------|-------|-------|-------|-------|------------------|----------------|-----------------|-----------------------|------------|
| type    | str     | int  | float | float | float | float | str              | str            | str             | str                   | scientific |
| example | ID      | len  | gc    | cov1  | cov2  | cov3  | taxlevel_species | taxlevel_order | taxlevel_phylum | taxlevel_superkingdom | eval       |
|         | contig1 | 1000 | 0.56  | 100   | 150.2 | 80.3  | Danio rerio      | Cypriniformes  | Chordata        | Eukaryota             | 1e-25      |
|         | contig2 | 500  | 0.54  | 12.9  | 11.1  | 15.7  | Not Annotated    | Not Annotated  | Not Annotated   | Not Annotated         | N/A        |

- novel format (to be inplemented)

| columns |    ID   |  len |   gc  |              cov              |                                            tax                                            | eval       |
|:-------:|:-------:|:----:|:-----:|:-----------------------------:|:-----------------------------------------------------------------------------------------:|------------|
|   type  |   str   |  int | float |             float             |                                            str                                            | scientific |
| example |    ID   |  len |   gc  |              cov              |                                            tax                                            | eval       |
|         | contig1 | 1000 |  0.56 | cov1=100;cov2=150.2;cov3=80.3 |       species=Danio rerio;order=Cypriniformes;phylum=Chordata;superkingdom=Eukaryota      | 1e-25      |
|         | contig2 |  500 |  0.54 | cov1=12.9;cov2=11.1;cov3=15.7 | species=Not Annotated;order=Not Annotated;phylum=Not Annotated;superkingdom=Not Annotated | N/A        |
## ALL DEPENDANCIES:
- BLAST 2.6.0+
- bmtagger 3.101
- Bowtie2 2.3.2
- bwa 0.7.15-r1140
- Checkm v1.0.7
- FastQC v0.11.5
- kraken 0.10.6
- kronatools 2.7
- megahit v1.1.1-2-g02102e1
- metabat2 2.9.1
- concoct 0.4.0
- MaxBin2 2.2.4
- perl v5.22.0
- python2.7
- quast v4.5
- R 3.3.2
- ggplot2
- samtools 1.3.1
- SPAdes v3.10.1
- trim_galore 0.4.3
- python 2.7
- seaborn 0.8.1
- salmon
- taxator-tk
- prokka


## More detailed dependancies:

### general:
- python2.7
- shell

### assembly
- SPAdes v3.10.1
- bwa 0.7.15-r1140
- megahit v1.1.1-2-g02102e1
- quast v4.5

### binning
- bwa 0.7.15-r1140
- samtools 1.3.1
- metabat2 2.9.1
- concoct 0.4.0
- MaxBin2 2.2.4

#### bin_refinement
- bwa 0.7.15-r1140
- samtools 1.3.1
- kraken 0.10.6
- kraken_db STANDARD_DATABASE
- kronatools 2.7
- checkm v1.0.7
- checkm_DB (standard)
- SPAdes v3.10.1

### quant_bins
- salmon
- seaborn 0.8.1

### blobology
- refseq_db NCBI_nt
- refseq_db NCBI_tax
- blast 2.6.0+
- bowtie2 2.3.2
- perl v5.22.0
- R 3.3.2
- ggplot2
 
### kraken
- kraken 0.10.6
- kraken_db STANDARD_DATABASE
- kronatools 2.7

### read_qc
- fastqc v0.11.5
- trim_galore 0.4.3
- bmtagger 3.101
- human_db hg38

### classify_bins
- taxator-kt
- blastn
- NCBI_nt database
- NCBI_tax database

### annotate_bins
- prokka
## Downloading the CheckM database:
``` bash
mkdir MY_CHECKM_FOLDER

# Now manually download the database:
cd MY_CHECKM_FOLDER
wget https://data.ace.uq.edu.au/public/CheckM_databases/checkm_data_2015_01_16.tar.gz
tar -xvf *.tar.gz
rm *.gz
cd ../

# Now you need to tell CheckM where to find this data before running anything:
checkm data setRoot     # CheckM will prompt to to chose your storage location
# On newer versions of CheckM, you would run:
checkm data setRoot /path/to/your/dir/MY_CHECKM_FOLDER
```
Thats it! CheckM should now use that folder and its contents as its database.



## Downloading the KRAKEN1 standard database:
Note: As of metaWRAP v1.3.2, we recomend you use Kraken2 instead of the original Kraken1 (see below).
Note: this will download the entire RefSeq database and index it, which takes a lot of computational power, storage space, and RAM. During database building, you will need >450GB of space and >250GB of RAM. With 24 cores, this will take >5 hours. Note that this is only needed if you intend on running the KRAKEN module.
``` bash
kraken-build --standard --threads 24 --db MY_KRAKEN_DATABASE
kraken-build --db MY_KRAKEN_DATABASE --clean
```
Do not forget to set the KRAKEN_DB variable in the config-metawrap file! Run `which config-metawrap` to find it.
``` bash
KRAKEN_DB=/path/to/my/database/MY_KRAKEN_DATABASE
```

## Downloading the KRAKEN2 standard database:
Note: Compared to Kraken1, the Kraken2 database is considerably more compact, making the download and indexing process much faster and less taxing on the system. You will need an estimated 120GB of RAM and ~128GB of space. Note that this is only needed if you intend on running the KRAKEN2 module.
``` bash
kraken2-build --standard --threads 24 --db MY_KRAKEN2_DB
```
Do not forget to set the KRAKEN_DB variable in the config-metawrap file! Run `which config-metawrap` to find it.
``` bash
KRAKEN2_DB=/path/to/my/database/MY_KRAKEN2_DATABASE
```

## Downloading the NCBI_nt BLAST database:
``` bash
mkdir NCBI_nt
cd  NCBI_nt
wget "ftp://ftp.ncbi.nlm.nih.gov/blast/db/nt.*.tar.gz"
for a in nt.*.tar.gz; do tar xzf $a; done
```
Note: if you are using a more recent blast verions (beyond v2.6) you will need a the newer database format: `wget "ftp://ftp.ncbi.nlm.nih.gov/blast/db/v4/nt_v4.*.tar.gz"`

Do not forget to set the BLASTDB variable in the config-metawrap file!
``` bash
BLASTDB=/your/location/of/database/NCBI_nt
```


## Downloading the NCBI taxonomy:
``` bash
mkdir NCBI_tax
cd NCBI_tax
wget ftp://ftp.ncbi.nlm.nih.gov/pub/taxonomy/taxdump.tar.gz
tar -xvf taxdump.tar.gz
```
Do not forget to set the TAXDUMP variable in the config-metawrap file! Run `which config-metawrap` to find it.
``` bash
TAXDUMP=/your/location/of/database/NCBI_tax
```


## Making host genome index for bmtagger
If you want to remove human (see end of instrutions for non-human hosts) reads from tour sequencing in the READ_QC module, you will need to dowlnoad and index the human genome. See the official bmtagger manual for detailed instructions: https://www.hmpdacc.org/hmp/doc/HumanSequenceRemoval_SOP.pdf

First, lets download and merge the human genome hg38:
``` bash 
mkdir BMTAGGER_INDEX
cd BMTAGGER_INDEX
wget ftp://hgdownload.soe.ucsc.edu/goldenPath/hg38/chromosomes/*fa.gz
gunzip *fa.gz
cat *fa > hg38.fa
rm chr*.fa
```
Now lets index the human genome. Note that the file names of the indeces must be exactly as specified for metaWRAP to recognize them! Also note that indexing will take considerable memory and time (here I pass 100GB of RAM as a -M parameter).
``` bash
bmtool -d hg38.fa -o hg38.bitmask
srprism mkindex -i hg38.fa -o hg38.srprism -M 100000
```
Note: metaWRAP looks for files hg38.bitmask and hg38.srprism - make sure they are names exactly like this.
Done! Now dont forget to specify the BMTAGGER_DB variable in the config-metawrap file! Run `which config-metawrap` to find it.
``` bash
BMTAGGER_DB=/path/to/your/index/BMTAGGER_INDEX
```

### Instructions for non-human hosts: 
For non-human hosts, the protocol for building and using the bmtagger index should be the same. In the end, you need to have th `.srprism` and `.bitmask` index files in the `BMTAGGER_INDEX` directory that you will link to in the `config-metawrap` file. When you run `metawrap read_qc`, you need to use the `-x` option to specify the prefix of your host. For example, if your `BMTAGGER_INDEX` directory has files `pig.srprism` and `pig.bitmask`, use `-x pig` as the option. 

#### To update to the latest version:
MetaWRAP is being constantly improved week to week as more bugs and issues pop up. Because of the scale of the project it is almost impossible to get a perfect working version as the dependancy software are constantly changing. I recommend to update to the newest version of metaWRAP on a monthly basis.

Before updating, back up your `config-metawrap` file so you do not have to re-do the database configurations. Then update with conda:
```
conda update -y -c ursky metawrap-mg

# or for a specific version:
conda install -y -c ursky metawrap-mg=1.2.2
```

If you are using the (recommended) manual instalation of metaWRAP, simply run `git pull` inside the metaWRAP directory.
It should also be noted that it is possible for th eupdates to produce strange behavior in complex conda environments, so if you experience issues the safest way is to just delete the old metawrap-env environment (`rm -r miniconda/envs/metawrap-env`) and re-install from scratch.  


#### Best (manual) installation:
 This is how I usually use metaWRAP. By manually installing metaWRAP you will have better control over your environment and be able to change any programs and their versions as you see fit. You can also easily get the lastest updates throught `git pull`. If you are installing on a system other than Linux64, this may be your best bet. The hardest part is to install the [relevant prerequisite programs](https://github.com/bxlab/metaWRAP/blob/master/conda_pkg/meta.yaml), but this is much easier than you would think with the use of conda. Once you have these installed in your environment, download or clone this ripository, carefully configure the `metaWRAP/bin/config-metawrap` file, and add the `metaWRAP/bin/` directory to to the `$PATH` (likely by editing the `~/.bash_profile`). Alternatively, just copy over the `metaWRAP/bin/` contents into any location with excecutbale permission such as `/usr/bin/` or `/miniconda2/bin/` (depending on your permissions). Thats it!
 
 Easiest way to quickly install the dependancies:
 ```
 # Note: ordering is important
conda config --add channels defaults
conda config --add channels conda-forge
conda config --add channels bioconda
conda config --add channels ursky

# Unix/Linux only
conda install --only-deps -c ursky metawrap-mg

# OR
conda install biopython blas=2.5 blast=2.6.0 bmtagger bowtie2 bwa checkm-genome fastqc kraken=1.1 krona=2.7 matplotlib maxbin2 megahit metabat2 pandas prokka quast r-ggplot2 r-recommended salmon samtools=1.9 seaborn spades trim-galore
# Note: this last solution is universal, but you may need to manually install concoct=1.0 and pplacer.
```

#### Basic installation:
 To start, download [miniconda2](https://repo.continuum.io/miniconda/Miniconda2-latest-Linux-x86_64.sh) and install it:
 ``` bash
 wget https://repo.continuum.io/miniconda/Miniconda2-latest-Linux-x86_64.sh
 bash Miniconda2-latest-Linux-x86_64.sh
 ```
 
 Then add channels to your conda environment, and install metaWRAP (supports Linux64):
 ``` bash
 # Note: ordering is important
 conda config --add channels defaults
 conda config --add channels conda-forge
 conda config --add channels bioconda
 conda config --add channels ursky

 conda install -y -c ursky metawrap-mg
 # Note: may take a while
 
 # To fix the CONCOCT endless warning messages in metaWRAP=1.2, run
 conda install -y blas=2.5=mkl
 ```
 
 #### Better installation:
 The conda installation of metaWRAP will install over 140 software dependancies, which may cause some conflicts with your currenly installed packages. If you already use conda, it is strongly recommended to [set up a conda custom environment](https://conda.io/docs/user-guide/tasks/manage-environments.html) and install metaWRAP only in there. That way your current conda environment and metaWRAP's environment do not not conflict.
``` bash
conda create -y -n metawrap-env python=2.7
source activate metawrap-env

# Note: ordering is important
conda config --add channels defaults
conda config --add channels conda-forge
conda config --add channels bioconda
conda config --add channels ursky

conda install -y -c ursky metawrap-mg
# Note: may take a while

 # To fix the CONCOCT endless warning messages in metaWRAP=1.2, run
 conda install -y blas=2.5=mkl
```
 

#### Bioconda installation:
MetaWRAP is also available through the Bioconda channel. **However**, this distribution is not recommended for most users, as I will only push major releases to Bioconda (i.e. `v1.1`, `v1.2`). This source is meant for specific applications that require a Bioconda distribution. To get the latest version of metaWRAP with the newest patches and bug fixes, please install through the `-c ursky` channel, as seen above. 

```
# Bioconda installation (not recommended):
conda install -y -c bioconda metawrap
```

#### Docker installation:
If you are running on OSX and dont want to install manually, or prefer to work in containerized environments, then [Docker](https://quay.io/repository/biocontainers/metawrap?tab=info) could be the way to go. **However**, as with the Bioconda distribution, I will only push major releases to Bioconda (i.e. `v1.1`, `v1.2`). To get the latest version of metaWRAP with the newest patches and bug fixes, please install through the `-c ursky` channel, as seen above. If you still need to use Docker but run into bugs that have been fixed in the latest versions, you can manually update your scripts from this repository to apply the most recent patches. To install with Docker, run:
```
# Docker installation (not recommended)
docker pull quay.io/biocontainers/metawrap:1.2--0
```
