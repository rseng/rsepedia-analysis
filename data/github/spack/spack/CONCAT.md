# v0.17.1 (2021-12-23)

### Spack Bugfixes
* Allow locks to work under high contention (#27846)
* Improve errors messages from clingo (#27707 #27970)
* Respect package permissions for sbang (#25764)
* Fix --enable-locks behavior (#24675)
* Fix log-format reporter ignoring install errors (#25961)
* Fix overloaded argparse keys (#27379)
* Allow style commands to run with targets other than "develop" (#27472)
* Log lock messages to debug level, instead of verbose level (#27408)
* Handle invalid unicode while logging (#21447)
* spack audit: fix API calls to variants (#27713)
* Provide meaningful message for empty environment installs (#28031)
* Added opensuse leap containers to spack containerize (#27837)
* Revert "patches: make re-applied patches idempotent" (#27625)
* MANPATH can use system defaults (#21682)
* Add "setdefault" subcommand to `spack module tcl` (#14686)
* Regenerate views when specs already installed (#28113)

### Package bugfixes
* Fix external package detection for OpenMPI (#27255)
* Update the UPC++ package to 2021.9.0 (#26996)
* Added py-vermin v1.3.2 (#28072)

# v0.17.0 (2021-11-05)

`v0.17.0` is a major feature release.

## Major features in this release

1. **New concretizer is now default**
   The new concretizer introduced as an experimental feature in `v0.16.0`
   is now the default (#25502). The new concretizer is based on the
   [clingo](https://github.com/potassco/clingo) logic programming system,
   and it enables us to do much higher quality and faster dependency solving
   The old concretizer is still available via the `concretizer: original`
   setting, but it is deprecated and will be removed in `v0.18.0`.

2. **Binary Bootstrapping**
   To make it easier to use the new concretizer and binary packages,
   Spack now bootstraps `clingo` and `GnuPG` from public binaries. If it
   is not able to bootstrap them from binaries, it installs them from
   source code. With these changes, you should still be able to clone Spack
   and start using it almost immediately. (#21446, #22354, #22489, #22606,
   #22720, #22720, #23677, #23946, #24003, #25138, #25607, #25964, #26029,
   #26399, #26599).

3. **Reuse existing packages (experimental)**
   The most wanted feature from our
   [2020 user survey](https://spack.io/spack-user-survey-2020/) and
   the most wanted Spack feature of all time (#25310). `spack install`,
   `spack spec`, and `spack concretize` now have a `--reuse` option, which
   causes Spack to minimize the number of rebuilds it does. The `--reuse`
   option will try to find existing installations and binary packages locally
   and in registered mirrors, and will prefer to use them over building new
   versions. This will allow users to build from source *far* less than in
   prior versions of Spack. This feature will continue to be improved, with
   configuration options and better CLI expected in `v0.17.1`. It will become
   the *default* concretization mode in `v0.18.0`.

4. **Better error messages**
   We have improved the error messages generated by the new concretizer by
   using *unsatisfiable cores*. Spack will now print a summary of the types
   of constraints that were violated to make a spec unsatisfiable (#26719).

5. **Conditional variants**
   Variants can now have a `when="<spec>"` clause, allowing them to be
   conditional based on the version or other attributes of a package (#24858).

6. **Git commit versions**
   In an environment and on the command-line, you can now provide a full,
   40-character git commit as a version for any package with a top-level
   `git` URL.  e.g., `spack install hdf5@45bb27f58240a8da7ebb4efc821a1a964d7712a8`.
   Spack will compare the commit to tags in the git repository to understand
   what versions it is ahead of or behind.

7. **Override local config and cache directories**
   You can now set `SPACK_DISABLE_LOCAL_CONFIG` to disable the `~/.spack` and
   `/etc/spack` configuration scopes. `SPACK_USER_CACHE_PATH` allows you to
   move caches out of `~/.spack`, as well (#27022, #26735). This addresses
   common problems where users could not isolate CI environments from local
   configuration.

8. **Improvements to Spack Containerize**
   For added reproducibility, you can now pin the Spack version used by
   `spack containerize` (#21910). The container build will only build
   with the Spack version pinned at build recipe creation instead of the
   latest Spack version.

9. **New commands for dealing with tags**
   The `spack tags` command allows you to list tags on packages (#26136), and you
   can list tests and filter tags with `spack test list` (#26842).

## Other new features of note

* Copy and relocate environment views as stand-alone installations (#24832)
* `spack diff` command can diff two installed specs (#22283, #25169)
* `spack -c <config>` can set one-off config parameters on CLI (#22251)
* `spack load --list` is an alias for `spack find --loaded` (#27184)
* `spack gpg` can export private key with `--secret` (#22557)
* `spack style` automatically bootstraps dependencies (#24819)
* `spack style --fix` automatically invokes `isort` (#24071)
* build dependencies can be installed from build caches with `--include-build-deps` (#19955)
* `spack audit` command for checking package constraints (#23053)
* Spack can now fetch from `CVS` repositories (yep, really) (#23212)
* `spack monitor` lets you upload analysis about installations to a
  [spack monitor server](https://github.com/spack/spack-monitor) (#23804, #24321,
  #23777, #25928))
* `spack python --path` shows which `python` Spack is using (#22006)
* `spack env activate --temp` can create temporary environments (#25388)
* `--preferred` and `--latest` options for `spack checksum` (#25830)
* `cc` is now pure posix and runs on Alpine (#26259)
* `SPACK_PYTHON` environment variable sets which `python` spack uses (#21222)
* `SPACK_SKIP_MODULES` lets you source `setup-env.sh` faster if you don't need modules (#24545)

## Major internal refactors

* `spec.yaml` files are now `spec.json`, yielding a large speed improvement (#22845)
* Splicing allows Spack specs to store mixed build provenance (#20262)
* More extensive hooks API for installations (#21930)
* New internal API for getting the active environment (#25439)

## Performance Improvements

* Parallelize separate concretization in environments; Previously 55 min E4S solve
    now takes 2.5 min (#26264)
* Drastically improve YamlFilesystemView file removal performance via batching (#24355)
* Speed up spec comparison (#21618)
* Speed up environment activation (#25633)

## Archspec improvements
* support for new generic `x86_64_v2`, `x86_64_v3`, `x86_64_v4` targets
    (see [archspec#31](https://github.com/archspec/archspec-json/pull/31))
* `spack arch --generic` lets you get the best generic architecture for
    your node (#27061)
* added support for aocc (#20124), `arm` compiler on `graviton2` (#24904)
    and on `a64fx` (#24524),

## Infrastructure, buildcaches, and services

* Add support for GCS Bucket Mirrors (#26382)
* Add `spackbot` to help package maintainers with notifications. See
  [spack.github.io/spackbot](https://spack.github.io/spackbot/)
* Reproducible pipeline builds with `spack ci rebuild` (#22887)
* Removed redundant concretizations from GitLab pipeline generation (#26622)
* Spack CI no longer generates jobs for unbuilt specs (#20435)
* Every pull request pipeline has its own buildcache (#25529)
* `--no-add` installs only specified specs and only if already present inâ€¦ (#22657)
* Add environment-aware `spack buildcache sync` command (#25470)
* Binary cache installation speedups and improvements (#19690, #20768)

## Deprecations and Removals

* `spack setup` was deprecated in v0.16.0, and has now been removed.
  Use `spack develop` and `spack dev-build`.
* Remove unused `--dependencies` flag from `spack load` (#25731)
* Remove stubs for `spack module [refresh|find|rm|loads]`, all of which
  were deprecated in 2018.

## Notable Bugfixes

* Deactivate previous env before activating new one (#25409)
* Many fixes to error codes from `spack install` (#21319, #27012, #25314)
* config add: infer type based on JSON schema validation errors (#27035)
* `spack config edit` now works even if `spack.yaml` is broken (#24689)

## Packages

* Allow non-empty version ranges like `1.1.0:1.1` (#26402)
* Remove `.99`'s from many version ranges (#26422)
* Python: use platform-specific site packages dir (#25998)
* `CachedCMakePackage` for using *.cmake initial config files (#19316)
* `lua-lang` allows swapping `lua` and `luajit` (#22492)
* Better support for `ld.gold` and `ld.lld` (#25626)
* build times are now stored as metadata in `$prefix/.spack` (#21179)
* post-install tests can be reused in smoke tests (#20298)
* Packages can use `pypi` attribute to infer `homepage`/`url`/`list_url` (#17587)
* Use gnuconfig package for `config.guess` file replacement (#26035)
* patches: make re-applied patches idempotent (#26784)

## Spack community stats

* 5969 total packages, 920 new since `v0.16.0`
    * 358 new Python packages, 175 new R packages
* 513 people contributed to this release
    * 490 committers to packages
    * 105 committers to core
* Lots of GPU updates:
    * ~77 CUDA-related commits
    * ~66 AMD-related updates
    * ~27 OneAPI-related commits
    * 30 commits from AMD toolchain support
* `spack test` usage in packages is increasing
    * 1669 packages with tests (mostly generic python tests)
    * 93 packages with their own tests


# v0.16.3 (2021-09-21)

* clang/llvm: fix version detection (#19978)
* Fix use of quotes in Python build system (#22279)
* Cray: fix extracting paths from module files (#23472)
* Use AWS CloudFront for source mirror (#23978)
* Ensure all roots of an installed environment are marked explicit in db (#24277)
* Fix fetching for Python 3.8 and 3.9 (#24686)
* locks: only open lockfiles once instead of for every lock held (#24794)
* Remove the EOL centos:6 docker image

# v0.16.2 (2021-05-22)

* Major performance improvement for `spack load` and other commands. (#23661)
* `spack fetch` is now environment-aware. (#19166)
* Numerous fixes for the new, `clingo`-based concretizer. (#23016, #23307,
  #23090, #22896, #22534, #20644, #20537, #21148)
* Supoprt for automatically bootstrapping `clingo` from source. (#20652, #20657
  #21364, #21446, #21913, #22354, #22444, #22460, #22489, #22610, #22631)
* Python 3.10 support: `collections.abc` (#20441)
* Fix import issues by using `__import__` instead of Spack package importe.
  (#23288, #23290)
* Bugfixes and `--source-dir` argument for `spack location`. (#22755, #22348,
  #22321)
* Better support for externals in shared prefixes. (#22653)
* `spack build-env` now prefers specs defined in the active environment.
  (#21642)
* Remove erroneous warnings about quotes in `from_sourcing_files`. (#22767)
* Fix clearing cache of `InternalConfigScope`. (#22609)
* Bugfix for active when pkg is already active error. (#22587)
* Make `SingleFileScope` able to repopulate the cache after clearing it.
  (#22559)
* Channelflow: Fix the package. (#22483)
* More descriptive error message for bugs in `package.py` (#21811)
* Use package-supplied `autogen.sh`. (#20319)
* Respect `-k/verify-ssl-false` in `_existing_url` method. (#21864)


# v0.16.1 (2021-02-22)

This minor release includes a new feature and associated fixes:
* intel-oneapi support through new packages (#20411, #20686, #20693, #20717,
  #20732, #20808, #21377, #21448)

This release also contains bug fixes/enhancements for:
* HIP/ROCm support (#19715, #20095)
* concretization (#19988, #20020, #20082, #20086, #20099, #20102, #20128,
  #20182, #20193, #20194, #20196, #20203, #20247, #20259, #20307, #20362,
  #20383, #20423, #20473, #20506, #20507, #20604, #20638, #20649, #20677,
  #20680, #20790)
* environment install reporting fix (#20004)
* avoid import in ABI compatibility info (#20236)
* restore ability of dev-build to skip patches (#20351)
* spack find -d spec grouping (#20028)
* spack smoke test support (#19987, #20298)
* macOS fixes (#20038, #21662)
* abstract spec comparisons (#20341)
* continuous integration (#17563)
* performance improvements for binary relocation (#19690, #20768)
* additional sanity checks for variants in builtin packages (#20373)
* do not pollute auto-generated configuration files with empty lists or
  dicts (#20526)

plus assorted documentation (#20021, #20174) and package bug fixes/enhancements
(#19617, #19933, #19986, #20006, #20097, #20198, #20794, #20906, #21411).


# v0.16.0 (2020-11-18)

`v0.16.0` is a major feature release.

## Major features in this release

1. **New concretizer (experimental)** Our new backtracking concretizer is
   now in Spack as an experimental feature. You will need to install
   `clingo@master+python` and set `concretizer: clingo` in `config.yaml`
   to use it. The original concretizer is not exhaustive and is not
   guaranteed to find a solution if one exists. We encourage you to use
   the new concretizer and to report any bugs you find with it. We
   anticipate making the new concretizer the default and including all
   required dependencies for it in Spack `v0.17`. For more details, see
   #19501.

2. **spack test (experimental)** Users can add `test()` methods to their
   packages to run smoke tests on installations with the new `spack test`
   command (the old `spack test` is now `spack unit-test`). `spack test`
   is environment-aware, so you can `spack install` an environment and
   `spack test run` smoke tests on all of its packages. Historical test
   logs can be perused with `spack test results`. Generic smoke tests for
   MPI implementations, C, C++, and Fortran compilers as well as specific
   smoke tests for 18 packages. This is marked experimental because the
   test API (`self.run_test()`) is likely to be change, but we encourage
   users to upstream tests, and we will maintain and refactor any that
   are added to mainline packages (#15702).

3. **spack develop** New `spack develop` command allows you to develop
   several packages at once within a Spack environment. Running
   `spack develop foo@v1` and `spack develop bar@v2` will check
    out specific versions of `foo` and `bar` into subdirectories, which you
    can then build incrementally with `spack install ` (#15256).

4. **More parallelism** Spack previously installed the dependencies of a
   _single_ spec in parallel. Entire environments can now be installed in
   parallel, greatly accelerating builds of large environments. get
   parallelism from individual specs. Spack now parallelizes entire
   environment builds (#18131).

5. **Customizable base images for spack containerize**
    `spack containerize` previously only output a `Dockerfile` based
    on `ubuntu`. You may now specify any base image of your choosing (#15028).

6. **more external finding** `spack external find` was added in `v0.15`,
   but only `cmake` had support. `spack external find` can now find
   `bison`, `cuda`, `findutils`, `flex`, `git`, `lustre` `m4`, `mpich`,
   `mvapich2`, `ncurses`, `openmpi`, `perl`, `spectrum-mpi`, `tar`, and
   `texinfo` on your system and add them automatically to
   `packages.yaml`.

7. **Support aocc, nvhpc, and oneapi compilers** We are aggressively
   pursuing support for the newest vendor compilers, especially those for
   the U.S. exascale and pre-exascale systems. Compiler classes and
   auto-detection for `aocc`, `nvhpc`, `oneapi` are now in Spack (#19345,
   #19294, #19330).

## Additional new features of note

* New `spack mark` command can be used to designate packages as explicitly
  installed, so that `spack gc` will not garbage-collect them (#16662).
* `install_tree` can be customized with Spack's projection format (#18341)
* `sbang` now lives in the `install_tree` so that all users can access it (#11598)
* `csh` and `tcsh` users no longer need to set `SPACK_ROOT` before
  sourcing `setup-env.csh` (#18225)
* Spec syntax now supports `variant=*` syntax for finding any package
  that has a particular variant (#19381).
* Spack respects `SPACK_GNUPGHOME` variable for custom GPG directories (#17139)
* Spack now recognizes Graviton chips

## Major refactors

* Use spawn instead of fork on Python >= 3.8 on macOS (#18205)
* Use indexes for public build caches (#19101, #19117, #19132, #19141,  #19209)
* `sbang` is an external package now (https://github.com/spack/sbang, #19582)
* `archspec` is an external package now (https://github.com/archspec/archspec, #19600)

## Deprecations and Removals

* `spack bootstrap` was deprecated in v0.14.0, and has now been removed.
* `spack setup` is deprecated as of v0.16.0.
* What was `spack test` is now called `spack unit-test`. `spack test` is
  now the smoke testing feature in (2) above.

## Bugfixes

Some of the most notable bugfixes in this release include:

* Better warning messages for deprecated syntax in `packages.yaml` (#18013)
* `buildcache list --allarch` now works properly (#17827)
* Many fixes and tests for buildcaches and binary relcoation (#15687,
  *#17455, #17418, #17455, #15687, #18110)

## Package Improvements

Spack now has 5050 total packages, 720 of which were added since `v0.15`.

* ROCm packages (`hip`, `aomp`, more) added by AMD (#19957, #19832, others)
* Many improvements for ARM support
* `llvm-flang`, `flang`, and `f18` removed, as `llvm` has real `flang`
  support since Flang was merged to LLVM mainline
* Emerging support for `spack external find` and `spack test` in packages.

## Infrastructure

* Major infrastructure improvements to pipelines on `gitlab.spack.io`
* Support for testing PRs from forks (#19248) is being enabled for all
  forks to enable rolling, up-to-date binary builds on `develop`


# v0.15.4 (2020-08-12)

This release contains one feature addition:

* Users can set `SPACK_GNUPGHOME` to override Spack's GPG path (#17139)

Several bugfixes for CUDA, binary packaging, and `spack -V`:

* CUDA package's `.libs` method searches for `libcudart` instead of `libcuda` (#18000)
* Don't set `CUDAHOSTCXX` in environments that contain CUDA (#17826)
* `buildcache create`: `NoOverwriteException` is a warning, not an error (#17832)
* Fix `spack buildcache list --allarch` (#17884)
* `spack -V` works with `releases/latest` tag and shallow clones (#17884)

And fixes for GitHub Actions and tests to ensure that CI passes on the
release branch (#15687, #17279, #17328, #17377, #17732).

# v0.15.3 (2020-07-28)

This release contains the following bugfixes:

* Fix handling of relative view paths (#17721)
* Fixes for binary relocation (#17418, #17455)
* Fix redundant printing of error messages in build environment (#17709)

It also adds a support script for Spack tutorials:

* Add a tutorial setup script to share/spack (#17705, #17722)

# v0.15.2 (2020-07-23)

This minor release includes two new features:

* Spack install verbosity is decreased, and more debug levels are added (#17546)
* The $spack/share/spack/keys directory contains public keys that may be optionally trusted for public binary mirrors (#17684)

This release also includes several important fixes:

* MPICC and related variables are now cleand in the build environment (#17450)
* LLVM flang only builds CUDA offload components when +cuda (#17466)
* CI pipelines no longer upload user environments that can contain secrets to the internet (#17545)
* CI pipelines add bootstrapped compilers to the compiler config (#17536)
* `spack buildcache list` does not exit on first failure and lists later mirrors (#17565)
* Apple's "gcc" executable that is an apple-clang compiler does not generate a gcc compiler config (#17589)
* Mixed compiler toolchains are merged more naturally across different compiler suffixes (#17590)
* Cray Shasta platforms detect the OS properly (#17467)
* Additional more minor fixes.

# v0.15.1 (2020-07-10)

This minor release includes several important fixes:

* Fix shell support on Cray (#17386)
* Fix use of externals installed with other Spack instances (#16954)
* Fix gcc+binutils build (#9024)
* Fixes for usage of intel-mpi (#17378 and #17382)
* Fixes to Autotools config.guess detection (#17333 and #17356)
* Update `spack install` message to prompt user when an environment is not
  explicitly activated (#17454)

This release also adds a mirror for all sources that are
fetched in Spack (#17077). It is expected to be useful when the
official website for a Spack package is unavailable.

# v0.15.0 (2020-06-28)

`v0.15.0` is a major feature release.

## Major Features in this release

1. **Cray support** Spack will now work properly on Cray "Cluster"
systems (non XC systems) and after a `module purge` command on Cray
systems. See #12989

2. **Virtual package configuration** Virtual packages are allowed in
packages.yaml configuration. This allows users to specify a virtual
package as non-buildable without needing to specify for each
implementation. See #14934

3. **New config subcommands** This release adds `spack config add` and
`spack config remove` commands to add to and remove from yaml
configuration files from the CLI. See #13920

4. **Environment activation** Anonymous environments are **no longer**
automatically activated in the current working directory. To activate
an environment from a `spack.yaml` file in the current directory, use
the `spack env activate .` command. This removes a concern that users
were too easily polluting their anonymous environments with accidental
installations. See #17258

5. **Apple clang compiler** The clang compiler and the apple-clang
compiler are now separate compilers in Spack. This allows Spack to
improve support for the apple-clang compiler. See #17110

6. **Finding external packages** Spack packages can now support an API
for finding external installations. This allows the `spack external
find` command to automatically add installations of those packages to
the user's configuration. See #15158


## Additional new features of note

* support for using Spack with the fish shell (#9279)
* `spack load --first` option to load first match (instead of prompting user) (#15622)
* support the Cray cce compiler both new and classic versions (#17256, #12989)
* `spack dev-build` command:
  * supports stopping before a specified phase (#14699)
  * supports automatically launching a shell in the build environment (#14887)
* `spack install --fail-fast` allows builds to fail at the first error (rather than best-effort) (#15295)
* environments: SpecList references can be dereferenced as compiler or dependency constraints (#15245)
* `spack view` command: new support for a copy/relocate view type (#16480)
* ci pipelines: see documentation for several improvements
* `spack mirror -a` command now supports excluding packages (#14154)
* `spack buildcache create` is now environment-aware (#16580)
* module generation: more flexible format for specifying naming schemes (#16629)
* lmod module generation: packages can be configured as core specs for lmod hierarchy (#16517)

## Deprecations and Removals

The following commands were deprecated in v0.13.0, and have now been removed:

* `spack configure`
* `spack build`
* `spack diy`

The following commands were deprecated in v0.14.0, and will be removed in the next major release:

* `spack bootstrap`

## Bugfixes

Some of the most notable bugfixes in this release include:

* Spack environments can now contain the string `-h` (#15429)
* The `spack install` gracefully handles being backgrounded (#15723, #14682)
* Spack uses `-isystem` instead of `-I` in cases that the underlying build system does as well (#16077)
* Spack no longer prints any specs that cannot be safely copied into a Spack command (#16462)
* Incomplete Spack environments containing python no longer cause problems (#16473)
* Several improvements to binary package relocation

## Package Improvements

The Spack project is constantly engaged in routine maintenance,
bugfixes, and improvements for the package ecosystem. Of particular
note in this release are the following:

* Spack now contains 4339 packages. There are 430 newly supported packages in v0.15.0
* GCC now builds properly on ARM architectures (#17280)
* Python: patched to support compiling mixed C/C++ python modules through distutils (#16856)
* improvements to pytorch and py-tensorflow packages
* improvements to major MPI implementations: mvapich2, mpich, openmpi, and others

## Spack Project Management:

* Much of the Spack CI infrastructure has moved from Travis to GitHub Actions (#16610, #14220, #16345)
* All merges to the `develop` branch run E4S CI pipeline (#16338)
* New `spack debug report` command makes reporting bugs easier (#15834)

# v0.14.2 (2020-04-15)

This is a minor release on the `0.14` series. It includes performance
improvements and bug fixes:

* Improvements to how `spack install` handles foreground/background (#15723)
* Major performance improvements for reading the package DB (#14693, #15777)
* No longer check for the old `index.yaml` database file (#15298)
* Properly activate environments with '-h' in the name (#15429)
* External packages have correct `.prefix` in environments/views (#15475)
* Improvements to computing env modifications from sourcing files (#15791)
* Bugfix on Cray machines when getting `TERM` env variable (#15630)
* Avoid adding spurious `LMOD` env vars to Intel modules (#15778)
* Don't output [+] for mock installs run during tests (#15609)

# v0.14.1 (2020-03-20)

This is a bugfix release on top of `v0.14.0`.  Specific fixes include:

* several bugfixes for parallel installation (#15339, #15341, #15220, #15197)
* `spack load` now works with packages that have been renamed (#14348)
* bugfix for `suite-sparse` installation (#15326)
* deduplicate identical suffixes added to module names (#14920)
* fix issues with `configure_args` during module refresh (#11084)
* increased test coverage and test fixes (#15237, #15354, #15346)
* remove some unused code (#15431)

# v0.14.0 (2020-02-23)

`v0.14.0` is a major feature release, with 3 highlighted features:

1. **Distributed builds.** Multiple Spack instances will now coordinate
   properly with each other through locks. This works on a single node
   (where you've called `spack` several times) or across multiple nodes
   with a shared filesystem. For example, with SLURM, you could build
   `trilinos` and its dependencies on 2 24-core nodes, with 3 Spack
   instances per node and 8 build jobs per instance, with `srun -N 2 -n 6
   spack install -j 8 trilinos`. This requires a filesystem with locking
   enabled, but not MPI or any other library for parallelism.

2.  **Build pipelines.** You can also build in parallel through Gitlab
   CI. Simply create a Spack environment and push it to Gitlab to build
   on Gitlab runners. Pipeline support is now integrated into a single
   `spack ci` command, so setting it up is easier than ever.  See the
   [Pipelines section](https://spack.readthedocs.io/en/v0.14.0/pipelines.html)
   in the docs.

3. **Container builds.** The new `spack containerize` command allows you
   to create a Docker or Singularity recipe from any Spack environment.
   There are options to customize the build if you need them. See the
   [Container Images section](https://spack.readthedocs.io/en/latest/containers.html)
   in the docs.

In addition, there are several other new commands, many bugfixes and
improvements, and `spack load` no longer requires modules, so you can use
it the same way on your laptop or on your supercomputer.

Spack grew by over 300 packages since our last release in November 2019,
and the project grew to over 500 contributors.  Thanks to all of you for
making yet another great release possible. Detailed notes below.

## Major new core features
* Distributed builds: spack instances coordinate and build in parallel (#13100)
* New `spack ci` command to manage CI pipelines (#12854)
* Generate container recipes from environments: `spack containerize` (#14202)
* `spack load` now works without using modules (#14062, #14628)
* Garbage collect old/unused installations with `spack gc` (#13534)
* Configuration files all set environment modifications the same way (#14372,
  [docs](https://spack.readthedocs.io/en/v0.14.0/configuration.html#environment-modifications))
* `spack commands --format=bash` auto-generates completion (#14393, #14607)
* Packages can specify alternate fetch URLs in case one fails (#13881)

## Improvements
* Improved locking for concurrency with environments (#14676, #14621, #14692)
* `spack test` sends args to `pytest`, supports better listing (#14319)
* Better support for aarch64 and cascadelake microarch (#13825, #13780, #13820)
* Archspec is now a separate library (see https://github.com/archspec/archspec)
* Many improvements to the `spack buildcache` command (#14237, #14346,
  #14466, #14467, #14639, #14642, #14659, #14696, #14698, #14714, #14732,
  #14929, #15003, #15086, #15134)

## Selected Bugfixes
* Compilers now require an exact match on version (#8735, #14730, #14752)
* Bugfix for patches that specified specific versions (#13989)
* `spack find -p` now works in environments (#10019, #13972)
* Dependency queries work correctly in `spack find` (#14757)
* Bugfixes for locking upstream Spack instances chains (#13364)
* Fixes for PowerPC clang optimization flags (#14196)
* Fix for issue with compilers and specific microarchitectures (#13733, #14798)

## New commands and options
* `spack ci` (#12854)
* `spack containerize` (#14202)
* `spack gc` (#13534)
* `spack load` accepts `--only package`, `--only dependencies` (#14062, #14628)
* `spack commands --format=bash` (#14393)
* `spack commands --update-completion` (#14607)
* `spack install --with-cache` has new option: `--no-check-signature` (#11107)
* `spack test` now has `--list`, `--list-long`, and `--list-names` (#14319)
* `spack install --help-cdash` moves CDash help out of the main help (#13704)

## Deprecations
* `spack release-jobs` has been rolled into `spack ci`
* `spack bootstrap` will be removed in a future version, as it is no longer
  needed to set up modules (see `spack load` improvements above)

## Documentation
* New section on building container images with Spack (see
  [docs](https://spack.readthedocs.io/en/latest/containers.html))
* New section on using `spack ci` command to build pipelines (see
  [docs](https://spack.readthedocs.io/en/latest/pipelines.html))
* Document how to add conditional dependencies (#14694)
* Document how to use Spack to replace Homebrew/Conda (#13083, see
  [docs](https://spack.readthedocs.io/en/latest/workflows.html#using-spack-to-replace-homebrew-conda))

## Important package changes
* 3,908 total packages (345 added since 0.13.0)
* Added first cut at a TensorFlow package (#13112)
* We now build R without "recommended" packages, manage them w/Spack (#12015)
* Elpa and OpenBLAS now leverage microarchitecture support (#13655, #14380)
* Fix `octave` compiler wrapper usage (#14726)
* Enforce that packages in `builtin` aren't missing dependencies (#13949)


# v0.13.4 (2020-02-07)

This release contains several bugfixes:

* bugfixes for invoking python in various environments (#14349, #14496, #14569)
* brought tab completion up to date (#14392)
* bugfix for removing extensions from views in order (#12961)
* bugfix for nondeterministic hashing for specs with externals (#14390)

# v0.13.3 (2019-12-23)

This release contains more major performance improvements for Spack
environments, as well as bugfixes for mirrors and a `python` issue with
RHEL8.

* mirror bugfixes: symlinks, duplicate patches, and exception handling (#13789)
* don't try to fetch `BundlePackages` (#13908)
* avoid re-fetching patches already added to a mirror (#13908)
* avoid re-fetching already added patches (#13908)
* avoid re-fetching already added patches (#13908)
* allow repeated invocations of `spack mirror create` on the same dir (#13908)
* bugfix for RHEL8 when `python` is unavailable (#14252)
* improve concretization performance in environments (#14190)
* improve installation performance in environments (#14263)

# v0.13.2 (2019-12-04)

This release contains major performance improvements for Spack environments, as
well as some bugfixes and minor changes.

* allow missing modules if they are blacklisted (#13540)
* speed up environment activation (#13557)
* mirror path works for unknown versions (#13626)
* environments: don't try to modify run-env if a spec is not installed (#13589)
* use semicolons instead of newlines in module/python command (#13904)
* verify.py: os.path.exists exception handling (#13656)
* Document use of the maintainers field (#13479)
* bugfix with config caching (#13755)
* hwloc: added 'master' version pointing at the HEAD of the master branch (#13734)
* config option to allow gpg warning suppression (#13744)
* fix for relative symlinks when relocating binary packages (#13727)
* allow binary relocation of strings in relative binaries (#13724)

# v0.13.1 (2019-11-05)

This is a bugfix release on top of `v0.13.0`.  Specific fixes include:

* `spack find` now displays variants and other spec constraints
* bugfix: uninstall should find concrete specs by DAG hash (#13598)
* environments: make shell modifications partially unconditional (#13523)
* binary distribution: relocate text files properly in relative binaries (#13578)
* bugfix: fetch prefers to fetch local mirrors over remote resources (#13545)
* environments: only write when necessary (#13546)
* bugfix: spack.util.url.join() now handles absolute paths correctly (#13488)
* sbang: use utf-8 for encoding when patching (#13490)
* Specs with quoted flags containing spaces are parsed correctly (#13521)
* targets: print a warning message before downgrading (#13513)
* Travis CI: Test Python 3.8 (#13347)
* Documentation: Database.query methods share docstrings (#13515)
* cuda: fix conflict statements for x86-64 targets (#13472)
* cpu: fix clang flags for generic x86_64 (#13491)
* syaml_int type should use int.__repr__ rather than str.__repr__ (#13487)
* elpa: prefer 2016.05.004 until sse/avx/avx2 issues are resolved (#13530)
* trilinos: temporarily constrain netcdf@:4.7.1 (#13526)

# v0.13.0 (2019-10-25)

`v0.13.0` is our biggest Spack release yet, with *many* new major features.
From facility deployment to improved environments, microarchitecture
support, and auto-generated build farms, this release has features for all of
our users.

Spack grew by over 700 packages in the past year, and the project now has
over 450 contributors.  Thanks to all of you for making this release possible.

## Major new core features
- Chaining: use dependencies from external "upstream" Spack instances
- Environments now behave more like virtualenv/conda
  - Each env has a *view*: a directory with all packages symlinked in
  - Activating an environment sets `PATH`, `LD_LIBRARY_PATH`, `CPATH`,
    `CMAKE_PREFIX_PATH`, `PKG_CONFIG_PATH`, etc. to point to this view.
- Spack detects and builds specifically for your microarchitecture
  - named, understandable targets like `skylake`, `broadwell`, `power9`, `zen2`
  - Spack knows which compilers can build for which architectures
  - Packages can easily query support for features like `avx512` and `sse3`
  - You can pick a target with, e.g. `spack install foo target=icelake`
- Spack stacks: combinatorial environments for facility deployment
  - Environments can now build cartesian products of specs (with `matrix:`)
  - Conditional syntax support to exclude certain builds from the stack
- Projections: ability to build easily navigable symlink trees environments
- Support no-source packages (BundlePackage) to aggregate related packages
- Extensions: users can write custom commands that live outside of Spack repo
- Support ARM and Fujitsu compilers

## CI/build farm support
- `spack release-jobs` can detect `package.py` changes and generate
    `.gitlab-ci.yml` to create binaries for an environment or stack
	in parallel (initial support -- will change in future release).
- Results of build pipelines can be uploaded to a CDash server.
- Spack can now upload/fetch from package mirrors in Amazon S3

## New commands/options
- `spack mirror create --all` downloads *all* package sources/resources/patches
- `spack dev-build` runs phases of the install pipeline on the working directory
- `spack deprecate` permanently symlinks an old, unwanted package to a new one
- `spack verify` chcecks that packages' files match what was originally installed
- `spack find --json` prints `JSON` that is easy to parse with, e.g. `jq`
- `spack find --format FORMAT` allows you to flexibly print package metadata
- `spack spec --json` prints JSON version of `spec.yaml`

## Selected improvements
- Auto-build requested compilers if they do not exist
- Spack automatically adds `RPATHs` needed to make executables find compiler
    runtime libraries (e.g., path to newer `libstdc++` in `icpc` or `g++`)
- setup-env.sh is now compatible with Bash, Dash, and Zsh
- Spack now caps build jobs at min(16, ncores) by default
- `spack compiler find` now also throttles number of spawned processes
- Spack now writes stage directories directly to `$TMPDIR` instead of
    symlinking stages within `$spack/var/spack/cache`.
- Improved and more powerful `spec` format strings
- You can pass a `spec.yaml` file anywhere in the CLI you can type a spec.
- Many improvements to binary caching
- Gradually supporting new features from Environment Modules v4
- `spack edit` respects `VISUAL` environment variable
- Simplified package syntax for specifying build/run environment modifications
- Numerous improvements to support for environments across Spack commands
- Concretization improvements

## Documentation
- Multi-lingual documentation (Started a Japanese translation)
- Tutorial now has its own site at spack-tutorial.readthedocs.io
  - This enables us to keep multiple versions of the tutorial around

## Deprecations
- Spack no longer supports dotkit (LLNL's homegrown, now deprecated module tool)
- `spack build`, `spack configure`, `spack diy` deprecated in favor of
    `spack dev-build` and `spack install`

## Important package changes
- 3,563 total packages (718 added since 0.12.1)
- Spack now defaults to Python 3 (previously preferred 2.7 by default)
- Much improved ARM support thanks to Fugaku (RIKEN) and SNL teams
- Support new special versions: master, trunk, and head (in addition to develop)
- Better finding logic for libraries and headers


# v0.12.1 (2018-11-13)

This is a minor bugfix release, with a minor fix in the tutorial and a `flake8` fix.

Bugfixes
* Add `r` back to regex strings in binary distribution
* Fix gcc install version in the tutorial


# v0.12.0 (2018-11-13)

## Major new features
- Spack environments
- `spack.yaml` and `spack.lock` files for tracking dependencies
- Custom configurations via command line
- Better support for linking Python packages into view directories
- Packages have more control over compiler flags via flag handlers
- Better support for module file generation
- Better support for Intel compilers, Intel MPI, etc.
- Many performance improvements, improved startup time

## License
- As of this release, all of Spack is permissively licensed under Apache-2.0 or MIT, at the user's option.
- Consents from over 300 contributors were obtained to make this relicense possible.
- Previous versions were distributed under the LGPL license, version 2.1.

## New packages
Over 2,900 packages (800 added since last year)

Spack would not be possible without our community.  Thanks to all of our
[contributors](https://github.com/spack/spack/graphs/contributors) for the
new features and packages in this release!


# v0.11.2 (2018-02-07)

This release contains the following fixes:

* Fixes for `gfortran` 7 compiler detection (#7017)
* Fixes for exceptions thrown during module generation (#7173)


# v0.11.1 (2018-01-19)

This release contains bugfixes for compiler flag handling.  There were issues in `v0.11.0` that caused some packages to be built without proper optimization.

Fixes:
* Issue #6999: FFTW installed with Spack 0.11.0 gets built without optimisations

Includes:
* PR #6415: Fixes for flag handling behavior
* PR #6960: Fix type issues with setting flag handlers
* 880e319: Upstream fixes to `list_url` in various R packages


# v0.11.0 (2018-01-17)

Spack v0.11.0 contains many improvements since v0.10.0.
Below is a summary of the major features, broken down by category.

## New packages
- Spack now has 2,178 packages (from 1,114 in v0.10.0)
- Many more Python packages (356) and R packages (471)
- 48 Exascale Proxy Apps (try `spack list -t proxy-app`)


## Core features for users
- Relocatable binary packages (`spack buildcache`, #4854)
- Spack now fully supports Python 3 (#3395)
- Packages can be tagged and searched by tags (#4786)
- Custom module file templates using Jinja (#3183)
- `spack bootstrap` command now sets up a basic module environment (#3057)
- Simplified and better organized help output (#3033)
- Improved, less redundant `spack install` output (#5714, #5950)
- Reworked `spack dependents` and `spack dependencies` commands (#4478)


## Major new features for packagers
- Multi-valued variants (#2386)
- New `conflicts()` directive (#3125)
- New dependency type: `test` dependencies (#5132)
- Packages can require their own patches on dependencies (#5476)
  - `depends_on(..., patches=<patch list>)`
- Build interface for passing linker information through Specs (#1875)
  - Major packages that use blas/lapack now use this interface
- Flag handlers allow packages more control over compiler flags (#6415)
- Package subclasses support many more build systems:
  - autotools, perl, qmake, scons, cmake, makefile, python, R, WAF
  - package-level support for installing Intel HPC products (#4300)
- `spack blame` command shows contributors to packages (#5522)
- `spack create` now guesses many more build systems (#2707)
- Better URL parsing to guess package version URLs (#2972)
- Much improved `PythonPackage` support (#3367)


## Core
- Much faster concretization (#5716, #5783)
- Improved output redirection (redirecting build output works properly #5084)
- Numerous improvements to internal structure and APIs


## Tutorials & Documentation
- Many updates to documentation
- [New tutorial material from SC17](https://spack.readthedocs.io/en/latest/tutorial.html)
  - configuration
  - build systems
  - build interface
  - working with module generation
- Documentation on docker workflows and best practices


## Selected improvements and bug fixes
- No longer build Python eggs -- installations are plain directories (#3587)
- Improved filtering of system paths from build PATHs and RPATHs (#2083, #3910)
- Git submodules are properly handled on fetch (#3956)
- Can now set default number of parallel build jobs in `config.yaml`
- Improvements to `setup-env.csh` (#4044)
- Better default compiler discovery on Mac OS X (#3427)
  - clang will automatically mix with gfortran
- Improved compiler detection on Cray machines (#3075)
- Better support for IBM XL compilers
- Better tab completion
- Resume gracefully after prematurely terminated partial installs (#4331)
- Better mesa support (#5170)


Spack would not be possible without our community.  Thanks to all of our
[contributors](https://github.com/spack/spack/graphs/contributors) for the
new features and packages in this release!


# v0.10.0 (2017-01-17)

This is Spack `v0.10.0`.  With this release, we will start to push Spack
releases more regularly.  This is the last Spack release without
automated package testing.  With the next release, we will begin to run
package tests in addition to unit tests.

Spack has grown rapidly from 422 to
[1,114 packages](https://spack.readthedocs.io/en/v0.10.0/package_list.html),
thanks to the hard work of over 100 contributors.  Below is a condensed
version of all the changes since `v0.9.1`.

### Packages
- Grew from 422 to 1,114 packages
  - Includes major updates like X11, Qt
  - Expanded HPC, R, and Python ecosystems

### Core
- Major speed improvements for spack find and concretization
- Completely reworked architecture support
  - Platforms can have front-end and back-end OS/target combinations
  - Much better support for Cray and BG/Q cross-compiled environments
- Downloads are now cached locally
- Support installations in deeply nested directories: patch long shebangs using `sbang`

### Basic usage
- Easier global configuration via config.yaml
  - customize install, stage, and cache locations
- Hierarchical configuration scopes: default, site, user
  - Platform-specific scopes allow better per-platform defaults
- Ability to set `cflags`, `cxxflags`, `fflags` on the command line
- YAML-configurable support for both Lmod and tcl modules in mainline
- `spack install` supports --dirty option for emergencies

### For developers
- Support multiple dependency types: `build`, `link`, and `run`
- Added `Package` base classes for custom build systems
  - `AutotoolsPackage`, `CMakePackage`, `PythonPackage`, etc.
  - `spack create` now guesses many more build systems
- Development environment integration with `spack setup`
- New interface to pass linking information via `spec` objects
  - Currently used for `BLAS`/`LAPACK`/`SCALAPACK` libraries
  - Polymorphic virtual dependency attributes: `spec['blas'].blas_libs`

### Testing & Documentation
- Unit tests run continuously on Travis CI for Mac and Linux
- Switched from `nose` to `pytest` for unit tests.
  - Unit tests take 1 minute now instead of 8
- Massively expanded documentation
- Docs are now hosted on [spack.readthedocs.io](https://spack.readthedocs.io)
# <img src="https://cdn.rawgit.com/spack/spack/develop/share/spack/logo/spack-logo.svg" width="64" valign="middle" alt="Spack"/> Spack

[![Unit Tests](https://github.com/spack/spack/workflows/linux%20tests/badge.svg)](https://github.com/spack/spack/actions)
[![Bootstrapping](https://github.com/spack/spack/actions/workflows/bootstrap.yml/badge.svg)](https://github.com/spack/spack/actions/workflows/bootstrap.yml)
[![macOS Builds (nightly)](https://github.com/spack/spack/workflows/macOS%20builds%20nightly/badge.svg?branch=develop)](https://github.com/spack/spack/actions?query=workflow%3A%22macOS+builds+nightly%22)
[![codecov](https://codecov.io/gh/spack/spack/branch/develop/graph/badge.svg)](https://codecov.io/gh/spack/spack)
[![Containers](https://github.com/spack/spack/actions/workflows/build-containers.yml/badge.svg)](https://github.com/spack/spack/actions/workflows/build-containers.yml)
[![Read the Docs](https://readthedocs.org/projects/spack/badge/?version=latest)](https://spack.readthedocs.io)
[![Slack](https://slack.spack.io/badge.svg)](https://slack.spack.io)

Spack is a multi-platform package manager that builds and installs
multiple versions and configurations of software. It works on Linux,
macOS, and many supercomputers. Spack is non-destructive: installing a
new version of a package does not break existing installations, so many
configurations of the same package can coexist.

Spack offers a simple "spec" syntax that allows users to specify versions
and configuration options. Package files are written in pure Python, and
specs allow package authors to write a single script for many different
builds of the same package.  With Spack, you can build your software
*all* the ways you want to.

See the
[Feature Overview](https://spack.readthedocs.io/en/latest/features.html)
for examples and highlights.

To install spack and your first package, make sure you have Python.
Then:

    $ git clone -c feature.manyFiles=true https://github.com/spack/spack.git
    $ cd spack/bin
    $ ./spack install zlib

Documentation
----------------

[**Full documentation**](https://spack.readthedocs.io/) is available, or
run `spack help` or `spack help --all`.

For a cheat sheet on Spack syntax, run `spack help --spec`.

Tutorial
----------------

We maintain a
[**hands-on tutorial**](https://spack.readthedocs.io/en/latest/tutorial.html).
It covers basic to advanced usage, packaging, developer features, and large HPC
deployments.  You can do all of the exercises on your own laptop using a
Docker container.

Feel free to use these materials to teach users at your organization
about Spack.

Community
------------------------

Spack is an open source project.  Questions, discussion, and
contributions are welcome. Contributions can be anything from new
packages to bugfixes, documentation, or even new core features.

Resources:

* **Slack workspace**: [spackpm.slack.com](https://spackpm.slack.com).
  To get an invitation, visit [slack.spack.io](https://slack.spack.io).
* **Mailing list**: [groups.google.com/d/forum/spack](https://groups.google.com/d/forum/spack)
* **Twitter**: [@spackpm](https://twitter.com/spackpm). Be sure to
  `@mention` us!

Contributing
------------------------
Contributing to Spack is relatively easy.  Just send us a
[pull request](https://help.github.com/articles/using-pull-requests/).
When you send your request, make ``develop`` the destination branch on the
[Spack repository](https://github.com/spack/spack).

Your PR must pass Spack's unit tests and documentation tests, and must be
[PEP 8](https://www.python.org/dev/peps/pep-0008/) compliant.  We enforce
these guidelines with our CI process. To run these tests locally, and for
helpful tips on git, see our
[Contribution Guide](https://spack.readthedocs.io/en/latest/contribution_guide.html).

Spack's `develop` branch has the latest contributions. Pull requests
should target `develop`, and users who want the latest package versions,
features, etc. can use `develop`.

Releases
--------

For multi-user site deployments or other use cases that need very stable
software installations, we recommend using Spack's
[stable releases](https://github.com/spack/spack/releases).

Each Spack release series also has a corresponding branch, e.g.
`releases/v0.14` has `0.14.x` versions of Spack, and `releases/v0.13` has
`0.13.x` versions. We backport important bug fixes to these branches but
we do not advance the package versions or make other changes that would
change the way Spack concretizes dependencies within a release branch.
So, you can base your Spack deployment on a release branch and `git pull`
to get fixes, without the package churn that comes with `develop`.

The latest release is always available with the `releases/latest` tag.

See the [docs on releases](https://spack.readthedocs.io/en/latest/developer_guide.html#releases)
for more details.

Code of Conduct
------------------------

Please note that Spack has a
[**Code of Conduct**](.github/CODE_OF_CONDUCT.md). By participating in
the Spack community, you agree to abide by its rules.

Authors
----------------
Many thanks go to Spack's [contributors](https://github.com/spack/spack/graphs/contributors).

Spack was created by Todd Gamblin, tgamblin@llnl.gov.

### Citing Spack

If you are referencing Spack in a publication, please cite the following paper:

 * Todd Gamblin, Matthew P. LeGendre, Michael R. Collette, Gregory L. Lee,
   Adam Moody, Bronis R. de Supinski, and W. Scott Futral.
   [**The Spack Package Manager: Bringing Order to HPC Software Chaos**](https://www.computer.org/csdl/proceedings/sc/2015/3723/00/2807623.pdf).
   In *Supercomputing 2015 (SCâ€™15)*, Austin, Texas, November 15-20 2015. LLNL-CONF-669890.

On GitHub, you can copy this citation in APA or BibTeX format via the "Cite this repository"
button. Or, see the comments in `CITATION.cff` for the raw BibTeX.

License
----------------

Spack is distributed under the terms of both the MIT license and the
Apache License (Version 2.0). Users may choose either license, at their
option.

All new contributions must be made under both the MIT and Apache-2.0
licenses.

See [LICENSE-MIT](https://github.com/spack/spack/blob/develop/LICENSE-MIT),
[LICENSE-APACHE](https://github.com/spack/spack/blob/develop/LICENSE-APACHE),
[COPYRIGHT](https://github.com/spack/spack/blob/develop/COPYRIGHT), and
[NOTICE](https://github.com/spack/spack/blob/develop/NOTICE) for details.

SPDX-License-Identifier: (Apache-2.0 OR MIT)

LLNL-CODE-811652
# Security Policy

## Supported Versions

We provide security updates for the following releases.
For more on Spack's release structure, see
[`README.md`](https://github.com/spack/spack#releases).


| Version | Supported          |
| ------- | ------------------ |
| develop | :white_check_mark: |
| 0.16.x  | :white_check_mark: |

## Reporting a Vulnerability

To report a vulnerability or other security
issue, email maintainers@spack.io.

You can expect to hear back within two days.
If your security issue is accepted, we will do
our best to release a fix within a week. If
fixing the issue will take longer than this,
we will discuss timeline options with you.
# Test data for unparser

These are test packages for testing Spack's unparser. They are used to ensure that the
canonical unparser used for Spack's package hash remains consistent across Python
versions.

All of these were copied from mainline Spack packages, and they have been renamed with
`.txt` suffixes so that they're not considered proper source files by the various
checkers used in Spack CI.

These packages were chosen for various reasons, but mainly because:

1. They're some of the more complex packages in Spack, and they exercise more unparser
   features than other packages.

2. Each of these packages has some interesting feature that was hard to unparse
   consistently across Python versions.  See docstrings in packages for details.
This directory vendors the `pluggy` module.

For a more detailed discussion for the reasons to vendoring this 
package, please see [this issue](https://github.com/pytest-dev/pytest/issues/944).

To update the current version, execute:

```
$ pip install -U pluggy==<version> --no-compile --target=_pytest/vendored_packages
```

And commit the modified files. The `pluggy-<version>.dist-info` directory 
created by `pip` should be added as well.
[![](https://github.com/archspec/archspec/workflows/Unit%20tests/badge.svg)](https://github.com/archspec/archspec/actions)
[![codecov](https://codecov.io/gh/archspec/archspec/branch/master/graph/badge.svg)](https://codecov.io/gh/archspec/archspec)
[![Documentation Status](https://readthedocs.org/projects/archspec/badge/?version=latest)](https://archspec.readthedocs.io/en/latest/?badge=latest)


# Archspec (Python bindings)

Archspec aims at providing a standard set of human-understandable labels for
various aspects of a system architecture  like CPU, network fabrics, etc. and
APIs to detect, query and compare them.

This project grew out of [Spack](https://spack.io/) and is currently under
active development. At present it supports APIs to detect and model
compatibility relationships among different CPU microarchitectures.

## Getting started with development

The `archspec` Python package needs [poetry](https://python-poetry.org/) to
be installed from VCS sources. The preferred method to install it is via
its custom installer outside of any virtual environment:
```console
$ curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python
```
You can refer to [Poetry's documentation](https://python-poetry.org/docs/#installation)
for further details or for other methods to install this tool. You'll also need `tox`
to run unit test:
```console
$ pip install --user tox
```
Finally you'll need to clone the repository:
```console
$ git clone --recursive https://github.com/archspec/archspec.git
```

### Running unit tests
Once you have your environment ready you can run `archspec` unit tests
using ``tox`` from the root of the repository:
```console
$ tox
  [ ... ]
  py27: commands succeeded
  py35: commands succeeded
  py36: commands succeeded
  py37: commands succeeded
  py38: commands succeeded
  pylint: commands succeeded
  flake8: commands succeeded
  black: commands succeeded
  congratulations :)
```

## Citing Archspec

If you are referencing `archspec` in a publication, please cite the following
paper:

 * Massimiliano Culpo, Gregory Becker, Carlos Eduardo Arango Gutierrez, Kenneth
   Hoste, and Todd Gamblin.
   [**`archspec`: A library for detecting, labeling, and reasoning about
   microarchitectures**](https://tgamblin.github.io/pubs/archspec-canopie-hpc-2020.pdf).
   In *2nd International Workshop on Containers and New Orchestration Paradigms
   for Isolated Environments in HPC (CANOPIE-HPC'20)*, Online Event, November
   12, 2020.

## License

Archspec is distributed under the terms of both the MIT license and the
Apache License (Version 2.0). Users may choose either license, at their
option.

All new contributions must be made under both the MIT and Apache-2.0
licenses.

See [LICENSE-MIT](https://github.com/archspec/archspec/blob/master/LICENSE-MIT),
[LICENSE-APACHE](https://github.com/archspec/archspec/blob/master/LICENSE-APACHE),
[COPYRIGHT](https://github.com/archspec/archspec/blob/master/COPYRIGHT), and
[NOTICE](https://github.com/archspec/archspec/blob/master/NOTICE) for details.

SPDX-License-Identifier: (Apache-2.0 OR MIT)

LLNL-CODE-811653
[![](https://github.com/archspec/archspec-json/workflows/JSON%20Validation/badge.svg)](https://github.com/archspec/archspec-json/actions)

# Archspec-json

The [archspec-json](https://github.com/archspec/archspec-json) repository is part of the
[Archspec](https://github.com/archspec) project. It contains data on various architectural
aspects of a platform stored in JSON format and is meant to be used as a base to develop
language specific APIs.

Currently the repository contains the following JSON files:
```console
.
â”œâ”€â”€ COPYRIGHT
â””â”€â”€ cpu
 Â Â  â”œâ”€â”€ microarchitectures.json         # Contains information on CPU microarchitectures
 Â Â  â””â”€â”€ microarchitectures_schema.json  # Schema for the file above
 ```


## License

Archspec is distributed under the terms of both the MIT license and the
Apache License (Version 2.0). Users may choose either license, at their
option.

All new contributions must be made under both the MIT and Apache-2.0
licenses.

See [LICENSE-MIT](https://github.com/archspec/archspec-json/blob/master/LICENSE-MIT),
[LICENSE-APACHE](https://github.com/archspec/archspec-json/blob/master/LICENSE-APACHE),
[COPYRIGHT](https://github.com/archspec/archspec-json/blob/master/COPYRIGHT), and
[NOTICE](https://github.com/archspec/archspec-json/blob/master/NOTICE) for details.

SPDX-License-Identifier: (Apache-2.0 OR MIT)

LLNL-CODE-811653
# Spack Community Code of Conduct

## Our Pledge

In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation.

## Our Standards

Examples of behavior that contributes to creating a positive environment include:

* Using welcoming and inclusive language
* Being respectful of differing viewpoints and experiences
* Gracefully accepting constructive criticism
* Focusing on what is best for the community
* Showing empathy towards other community members

Examples of unacceptable behavior by participants include:

* The use of sexualized language or imagery and unwelcome sexual attention or advances
* Trolling, insulting/derogatory comments, and personal or political attacks
* Public or private harassment
* Publishing others' private information, such as a physical or electronic address, without explicit permission
* Other conduct which could reasonably be considered inappropriate in a professional setting

## Our Responsibilities

Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.

Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.

## Scope

This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the Spack project or its community. Examples of representing the project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of the project may be further defined and clarified by Spack maintainers.

## Enforcement

Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at maintainers@spack.io. The project team will review and investigate all complaints, and will respond in a way that it deems appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.

Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.

## Attribution

This Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4, available at [http://contributor-covenant.org/version/1/4][version]

[homepage]: http://contributor-covenant.org
[version]: http://contributor-covenant.org/version/1/4/
# Contributing to Spack

All contributions to Spack must be made under both the Apache License,
Version 2.0 (Apache-2.0) and the MIT license (MIT).

Before contributing to Spack, you should read the
[Contribution Guide](https://spack.readthedocs.io/en/latest/contribution_guide.html),
which is maintained as part of Spack's documentation.
# GPG Keys

This directory contains keys that should be trusted by this installation of
Spack.  They are imported when running `spack gpg init`, but may also be
imported manually with `spack gpg trust path/to/key`.
# Mock GPG directory

This directory contains keys and data used in the testing Spack.
# Legion Spack Package Notes

The Legion Spack package follows the underlying approach used in Legion's CMake-based build infrastructure.  If you are not familiar with using Spack, we encourage you to start with the **[Spack 101 Tutorial](https://spack-tutorial.readthedocs.io/en/latest/)**.  For more background on Legion please visit the project [website](https://legion.stanford.edu). There you can find information on getting started, tutorials, and supporting documentation.

## Versions

We strongly recommend the majority of users start with the latest **stable** branch of Legion.  This will guarantee the most regularly tested and debugged version of the code.  We also strive to have quarterly tagged release throughout the calendar year starting March (e.g., legion-YY.MM.0 where MM starts at *'03'* and runs through *'12'* for any calendar year).  These versioned downloads are captured via the [Legion GitHub page](https://github.com/StanfordLegion/legion) and captured in the Spack package for individual use via the naming convention above.  The details for each release are captured in the CHANGES.txt file at the top-level of the repository.

## Spack Usage and Details

**NOTE: This version of the Legion *spackage* no longer depends upon an external GASNet spackage.  Instead, as supported and preferred by the GASNet developers, Legion internalizes the GASNet configuration and build process tailored to Legion's needs.  At present this approach should minimize both building and configuration issues that have occurred in the past.**

With a working installation of Spack following variants can be used to download, configure, and build legion.  As of its latest release Legion uses features from C++11 and can be build using either GCC or Clang (and likely any other compilers with full C++11 support).  For example,

    spack install legion %clang@10.0

will build and install Legion using version 10.0 of the clang compiler.  Additional compiler flags (beyond those automatically enabled by the underlying Legion CMake configuration) may be specified by using the `cppflags` option to Spack:

    spack install legion %clang@10.0 cppflags=-Wall

You can find more details about customizing the compiler with Spack [here](https://spack-tutorial.readthedocs.io/en/latest/tutorial_basics.html#customizing-compilers).

There are a number of configuration parameters (variants) for Legion that can be specified on the command line.  To see a complete listing of these variants you can use the following command:

    spack info legion

The default build and install of Legion is suitable for laptop/desktop development needs.  Additional variants will need to be specified to build for distributed memory systems, GPU support, and other features (e.g., debugging, profiling, etc.).  A full list of the currently supported variants is provided below.

These variants may then be used on the Spack command line to customize the build of Legion you would like to install.  For boolean-based variants the `+`, `-`, or `~` *sigils* can be used. In this case, the `+` specifies a `True` value and either `-` or `~` can be used for the `False` setting (two values for `False` are provided to avoid conflicts with shell command line parameters. For example,

    spack install legion@stable~cuda+hdf5%clang10.0

will build/install the stable version of Legion without CUDA and with HDF5 support; using Clang 10.0 as the compiler.  See below for more examples of the various package options. 

## Overview of Legion's Spack Variants


This section provides an overview of the variants that are available for builds and installations of Legion and the underlying Realm runtime libraries.  These variants range from important configuration parameters for many users, to more involved and low-level details more likely to be leveraged when debugging correctness and performance issues.  For these last set of use cases we encourage users to visit the Legion [Debugging](https://legion.stanford.edu/debugging/) and [Profiling](https://legion.stanford.edu/profiling/) pages.

Note that any variants with a default of `on` or `off` are boolean values and may be configured on the Spack command line using the `+`, or `-` (or `~`) sigils.

Finally, to build the highest performing installation of Legion requires an appropriate configuration and installation of GASNet-Ex (while we have support for an MPI transport layer it is still in testing and evaluation for achieving the best possible performance).  See the following section for details on the steps needed to build a high-performance install of Legion using Spack.

### Debugging & Internal Runtime Configurations

* **`build_type`**: This variant exposes the CMake build type setting.  Following directly from CMake these available values for this variant are [`Debug`, `RelWithDebInfo`, and `Release`]. `default=RelWithDebInfo`

* **`bounds_checks`**:  This variant supports `on` or `off` values and enables, or disables bounds checking within the runtime's data accessors.  This is helpful for debugging but  does have the potential to degrade performance (therefore it is disabled in the default build).  `default=off`

* **`privilege_checks`**: This variant enables the runtime checking of data privileges in Legion's data accessors. This can helpful in debugging applications. `default=off`

* **`spy`**: This variant enables applications to produce detailed logging information for debugging with the [Legion Spy](https://legion.stanford.edu/debugging/#legion-spy) tool.  `default=off`

* **`max_dims`**: This variant provides an integral value for the maximum number of dimensions in a logical region that are supported by the build/install.  `default=3`  *Note: Currently supported values range from 1 to 9.*

* **`max_fields`**: This variant provides an integral value for the maximum number of fields that may be stored within a logical region.  `default=512` *Note: Internally this requires a value that is a power-of-two -- if a non power-of-two value is provided the next largest power-of-two will be used.*

* **`output_level`**: This variant enables the (dynamic) debugging level for the runtime.  It can be one of the following values: [`spew`, `debug`, `info`, `print`, `warning`, `error`, `fatal`, or `none`].  `default=warning`

* **`papi`**: This variant enables PAPI performance measurements.  `default=off`

* **`redop_complex`** This variant enables support for the reduction of complex types. `default=off`  Note: will be enabled when `+bindings` is set.

* **`shared_libs`**: Build shared libraries for Legion and Realm. `default=off`

* **`enable_tls`**: This variant supports `on` or `off` and enables thread-local-storage of the Legion runtime context.  `default=off`

### Bindings & Interoperability

* **`bindings`**: This variant supports `on` or `off` and will build the language bindings for Legion.  In addition to the required C++ interface, this currently includes C, Python, and Fortran. `default=off`  Note you can use `+python` or `+fortran` to select a subset of these bindings.

* **`python`**: This variant enables Python support in terms of both bindings and runtime support (where instance(s) of the Python interpreter must be available.) `default=off`

* **`fortran`**: This variant supports `on` or `off` and enables building of Fortran language bindings for Legion. `default=off`

[//]: <> (TOOD: More details here on Kokkos interop?)

* **`kokkos`**: Enable support for interoperability with [Kokkos](https://github.com/kokkos) use in Legion tasks. `default=off`

[//]: <> (TOOD: More details here on OpenMP interop?)

* **`openmp`**: This variant enables OpenMP support within Legion tasks (and within the Realm runtime). Please note that the full OpenMP feature set (e.g. OpenMP 5.0) is not fully supported when enabling this feature.  `default=off`

* **`libdl`**: Enable support for dynamic object loading (via "libdl").  `default=on`

### Processor Architecture Support (e.g., GPUs)

* **`cuda`**: This variant supports `on` or `off` and enables CUDA support within Legion.  `default=off`

* **`cuda_arch`**: This variant specifics the specific CUDA architecture to support within the Legion build/installation.  Currently this variant must be one of [`60`, `70`, `75`, or `80`].  Where `60` is the Pascal architecture, `70` is for Volta, `75` is for Turing, and `80` is for `Volta`. `default=70`

* **`cuda_hijack`**:  This variant supports `on` or `off` and determines if the build enables performance enhancements by "*hijacking* entry points into CUDA's runtime API; thus, it obviously implies `+cuda`.This is a performance enhancement and not necessary but suggested for production use cases on NVIDIA-based systems.  `default=off`

### External Library Support

* **`hdf5`**: This variant supports `on` or `off` and enables building of HDF5 support within the runtime. `default=off`

* **`hwloc`**: Build using "libhwloc" support for numa-aware topology support within Realm (Legion's low-level runtime layer).  `default=off`

* **`zlib`**: This variant enables support for zlib. `default=on`

### Distributed Memory/Network Transport/Interconnect Support

* **`network`**: This variant specifies what network transport layer Legion/Realm should use for moving data across the distributed memories of a system.  The options are currently: [`gasnet`, `mpi`, or `none`].  The most tested and reliable interface for distributed memory systems is `gasnet`.  Via this package GASNet will be automatically configured and built. If you want to use a pre-installed version of GASNet you can use the `gasnet_root` variant; this will disable the automatic/internal installation of GASNet. We strongly encourage the use of the automatic configuration and build mechanism as it is tailored to Legion's use cases. If you would like to read more about please visit their [web page](https://gasnet.lbl.gov).  Note that the `mpi` transport layer is new and still being tuned and debugged.  Finally, if you want to run on a single system (e.g., a desktop or laptop) for development activities you may drop this parameter or specify `network=none`.  `default=none`

* **`conduit`** requires `(network=gasnet)` **only**: This variant selects the GASNet conduit to use and must be provided when `network=gasnet`.  The current choices are: [`aries`, `ibv`, `mpi`, `ucx`, `udp`, or `none`]:  defaults to `none`.
  * **aries**: Aries for Cray XC series (see [documentation](https://gasnet.lbl.gov/dist-ex/aries-conduit/README)).
  * **ibv**: OpenIB/OpenFabrics Verbs for InfiniBand (see [documentation](https://gasnet.lbl.gov/dist-ex/ibv-conduit/README)).
  * **mpi**: Portable conduit, works on any network with MPI 1.1 or newer (see [documentation](https://gasnet.lbl.gov/dist-ex/mpi-conduit/README)).
  * **ucx**: *(NOTE: EXPERIMENTAL)*  Unified Communication X framework (see [documentation](https://gasnet.lbl.gov/dist-ex/ucx-conduit/README)).
  * **udp**: Portable conduit, works on any network with a TCP/IP stack (see [documentation](https://gasnet.lbl.gov/dist-ex/udp-conduit/README)).

* **`gasnet_root`**: This variant points the package at a pre-installed version of GASNet for use when building Legion.  This skips the default behavior of embedding the GASNet build within that of Legion. It requires that `conduit` be set to match that used by the installed version of GASNet (*note that incorrectly setting `conduit` could result in failures that are not caught by the Spack package as failures will only occur during the build of Legion*). `default=/usr/local`

## Examples

This section highlights a few common configurations/installations for various common installations of Legion. 

### Laptop/Desktop Development

The default configuration will build a version of Legion without network support that is often helpful in laptop or desktop installs for development using Legion. The first example is the most straightforward and enables a CPU-only configuration of Legion:

`$ spack install legion@stable`

Given that many uses cases for laptop/desktop development involve debugging Legion you might want to consider enabling [Legion Spy](https://legion.stanford.edu/debugging/#legion-spy), bounds and privilege checks, and optionally enable debugging level output diagnostics:

`$ spack install legion@stable +spy +bounds_checks +privilege_checks [output=debug]`

Note that by specifying `+spy` the package will also install the Legion profiler.  Both will be available in the corresponding `bin` directory created by Spack's install mechanisms.

#### Adding GPU Support

To enable support for NVIDIA GPUs with Legion you can add the `+cuda` variant to the examples provided above.  In addition, you can also use the `cuda_arch` flag to enable specific GPU architectures.  For example, the follow command line installs the stable version of Legion with CUDA support for NVIDIA's Volta architecture:

`$ spack install legion@stable +cuda cuda_arch=70`

If you wanted to program tasks using Kokkos support for GPUs you can simply add the `+kokkos` option to the previous command line: 

`$ spack install legion@stable +cuda cuda_arch=70 +kokkos`

this will enable Kokkos interoperability in Legion and also build and configure a version of Kokkos with cuda and Volta GPU support.

#### Networking/Distributed Memory Support 

In order to add modify the above configuration to support a distributed memory architecture you need to add a `network` target.  For example, to build a version of Legion that uses GASNet for the data transport layer you would specify:

`$ spack install legion@stable +cuda cuda_arch=70 +kokkos network=gasnet`

Without the addition of a specific GASNet *conduit* this command will use the UDP conduit. To change this to support a Infniband-based cluster you would add:

`$ spack install legion@stable +cuda cuda_arch=70 +kokkos network=gasnet conduit=ibv`

For a pre-existing installation of GASNet you can point the package at the root directory of the installation (also often know as the installation *prefix*). For example, to point the package at a version installed under `/opt/local` (*i.e., the include files are stored under `/opt/local/include` and libraries are in `/opt/local/lib`*) you would add `gasnet_root=/opt/local` to the command line.
# ROOT Recipe Notes

ROOT has a significant number of build options. Not all of them are supported by this package recipe at this time. Relevant notes on support (or lack thereof) for various options is below.

## Fixed and unsupported options

### Missing dependencies

The following configuration options are unsupported (set to `OFF`) due to missing dependencies:

#### `alien`

Requires `libgapiUI` from ALICE.

#### `gfal`

#### `monalisa`

Monitoring with Monalisa depends on `libapmoncpp`.

#### `odbc`

#### `tcmalloc`

#### `xinetd`

### Discontinued options

Support for several options was discontinued in ROOT without prior support in the recipe:

#### `afdsmgrd`

#### `afs`

#### `bonjour`

#### `castor`

#### `chirp`

#### `geocad`

#### `glite`

#### `globus`

#### `hdfs`

#### `ios`

#### `krb5`

#### `ldap`

#### `rfio`

#### `ruby`

#### `sapdb`

#### `srp`

### Other fixed or unsupported options

#### `arrow=OFF`

#### `asimage=ON`, `astiff=ON`, `builtin_afterimage=ON`

Full control of `asimage` and `astiff` would require a package recipe for libAfterImage or Afterstep. In their absence, we use the provided and internally built version of libAfterImage.

#### `ccache=OFF`

Only useful for repeated builds.

#### `cling=OFF`

The use of Cling is assumed.

#### `cxxmodules`

This option should not be confused with `runtime_cxxmodules`. `cxxmodules` is an optimization that affects only the efficiency of building ROOT.

#### `exceptions=ON`

Support for C++ exceptions is assumed.

#### `explicitlink=ON`

Use of explicit linking when making shared libraries is assumed.

#### `fail-on-missing=ON`

Failure on missing dependencies is assumed (_vs_ automatically setting `builtin_XXX`).

#### `gnuinstall=OFF`

GNU-compliant install layout.

#### `libcxx`

This option controls use of the libC++ standard C++ library via compiler options. It is set automatically by ROOT's configuration on macOS >=10.7 when a Clang compiler is configured. Due to complexities related to compiler options and the way a compiler might have been configured, configurable support is disabled until (at least) Spack supports the standard library as a virtual dependency.

#### `pch=ON`

The use of pre-compiled headers when building is assumed.

#### `roottest=OFF`

`roottest` requires access to the network.

#### `runtime_cxxmodules=OFF`

This option tells ROOT to generate and use PCMs for its own libraries. This functionality is experimental for ROOT < 6.20, and is currently not supported by the recipe.

#### `shared=ON`

The use of shared libraries is assumed.

#### `soversion=ON`

The use of versioning for shared libraries is assumed.

#### `testing`

The building of ROOT's test suite and its availability for use by CTest is pegged to Spack's determination of whether testing is required for the current installation procedure.

## Variants and version dependent support for options

Some configuration options are version dependent---unavailable before or after a specific version. The current accounting for such options in the recipe is far from exhaustive, and a survey of the various options and the versions that support them would be useful. However, accounting for them is somewhat clumsy absent a `when` clause for variants, or similar.

### Conflicting variants since 6.18

#### `memstat`

#### `qt4`

Representing the obsolete `qt` and `qtgsi` ROOT build options.

#### `table`

### Temporarily conflicting variants

## Permanently removed variants

The following variants have been removed from the recipe as they have always been ineffective:

* `avahi`
* `kerberos`
* `ldap`
* `libcxx`
* `odbc`
* `tiff`

In addition, the `test` variant has been removed as its actions are irrelevant to the installed source---the corresponding `testing` ROOT option is enabled if and only if `self.run_tests` is set.
====================================
Development Notes on Intel Packages
====================================

These are notes for concepts and development of
lib/spack/spack/build_systems/intel.py .

For documentation on how to *use* ``IntelPackage``, see
lib/spack/docs/build_systems/intelpackage.rst .

-------------------------------------------------------------------------------
Installation and path handling as implemented in ./intel.py
-------------------------------------------------------------------------------


***************************************************************************
Prefix differences between Spack-external and Spack-internal installations
***************************************************************************


Problem summary
~~~~~~~~~~~~~~~~

For Intel packages that were installed external to Spack, ``self.prefix`` will
be a *component-specific* path (e.g. to an MKL-specific dir hierarchy), whereas
for a package installed by Spack itself, ``self.prefix`` will be a
*vendor-level* path that holds one or more components (or parts thereof), and
must be further qualified down to a particular desired component.

It is possible that a similar conceptual difference is inherent to other
package families that use a common vendor-style installer.


Description
~~~~~~~~~~~~

Spack makes packages available through two routes, let's call them A and B:

A. Packages pre-installed external to Spack and configured *for* Spack
B. Packages built and installed *by* Spack.

For a user who is interested in building end-user applications, it should not
matter through which route any of its dependent packages has been installed.
Most packages natively support a ``prefix`` concept which unifies the two
routes just fine.

Intel packages, however, are more complicated because they consist of a number
of components that are released as a suite of varying extent, like "Intel
Parallel Studio *Foo* Edition", or subsetted into products like "MKL" or "MPI",
each of which also contain libraries from other components like the compiler
runtime and multithreading libraries. For this reason, an Intel package is
"anchored" during installation at a directory level higher than just the
user-facing directory that has the conventional hierarchy of ``bin``, ``lib``,
and others relevant for the end-product.

As a result, internal to Spack, there is a conceptual difference in what
``self.prefix`` represents for the two routes.

For route A, consider MKL installed outside of Spack. It will likely be one
product component among other products, at one particular release among others
that are installed in sibling or cousin directories on the local system.
Therefore, the path given to Spack in ``packages.yaml`` should be a
*product-specific and fully version-specific* directory.  E.g., for an
``intel-mkl`` package, ``self.prefix`` should look like::

  /opt/intel/compilers_and_libraries_2018.1.163/linux/mkl

In this route, the interaction point with the user is encapsulated in an
environment variable which will be (in pseudo-code)::

    MKLROOT := {self.prefix}

For route B, a Spack-based installation of MKL will be placed in the directory
given to the ``./install.sh`` script of Intel's package distribution.  This
directory is taken to be the *vendor*-specific anchor directory, playing the
same role as the default ``/opt/intel``. In this case, ``self.prefix`` will
be::

  $SPACK_ROOT/opt/spack/linux-centos6-x86_64/gcc-4.9.3/intel-mkl-2018.1.163-<HASH>

However, now the environment variable will have to be constructed as *several
directory levels down*::

    MKLROOT := {self.prefix}/compilers_and_libraries_2018.1.163/linux/mkl

A recent post on the Spack mailing list illustrates the confusion when route A
was taken while route B was the only one that was coded in Spack:
https://groups.google.com/d/msg/spack/x28qlmqPAys/Ewx6220uAgAJ


Solution
~~~~~~~~~

Introduce a series of functions which will return the appropriate
directories, regardless of whether the Intel package has been installed
external or internal to Spack:

==========================  ==================================================
Function                    Example return values
--------------------------  --------------------------------------------------
normalize_suite_dir()       Spack-external installation:
                                /opt/intel/compilers_and_libraries_2018.1.163
                            Spack-internal installation:
                                $SPACK_ROOT/...<HASH>/compilers_and_libraries_2018.1.163
--------------------------  --------------------------------------------------
normalize_path('mkl')       <suite_dir>/linux/mkl
component_bin_dir()         <suite_dir>/linux/mkl/bin
component_lib_dir()         <suite_dir>/linux/mkl/lib/intel64
--------------------------  --------------------------------------------------
normalize_path('mpi')       <suite_dir>/linux/mpi
component_bin_dir('mpi')    <suite_dir>/linux/mpi/intel64/bin
component_lib_dir('mpi')    <suite_dir>/linux/mpi/intel64/lib
==========================  ==================================================


*********************************
Analysis of directory layouts
*********************************

Let's look at some sample directory layouts, using ``ls -lF``,
but focusing on names and symlinks only.

Spack-born installation of ``intel-mkl@2018.1.163``
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

::

  $ ls -l <prefix>

  bin/
      - compilervars.*sh (symlinked) ONLY

  compilers_and_libraries -> compilers_and_libraries_2018
      - generically-named entry point, stable across versions (one hopes)

  compilers_and_libraries_2018/
      - vaguely-versioned dirname, holding a stub hierarchy --ignorable

      $ ls -l compilers_and_libraries_2018/linux/
      bin         - actual compilervars.*sh (reg. files) ONLY
      documentation -> ../../documentation_2018/
      lib -> ../../compilers_and_libraries_2018.1.163/linux/compiler/lib/
      mkl -> ../../compilers_and_libraries_2018.1.163/linux/mkl/
      pkg_bin -> ../../compilers_and_libraries_2018.1.163/linux/bin/
      samples -> ../../samples_2018/
      tbb -> ../../compilers_and_libraries_2018.1.163/linux/tbb/

  compilers_and_libraries_2018.1.163/
      - Main "product" + a minimal set of libs from related products

      $ ls -l compilers_and_libraries_2018.1.163/linux/
      bin/        - compilervars.*sh, link_install*sh  ONLY
      mkl/        - Main Product ==> to be assigned to MKLROOT
      compiler/   - lib/intel64_lin/libiomp5*  ONLY
      tbb/        - tbb/lib/intel64_lin/gcc4.[147]/libtbb*.so* ONLY

  parallel_studio_xe_2018 -> parallel_studio_xe_2018.1.038/
  parallel_studio_xe_2018.1.038/
      - Alternate product packaging - ignorable

      $ ls -l parallel_studio_xe_2018.1.038/
      bin/               - actual psxevars.*sh (reg. files)
      compilers_and_libraries_2018 -> <full_path>/comp...aries_2018.1.163
      documentation_2018 -> <full_path_prefix>/documentation_2018
      samples_2018 -> <full_path_prefix>/samples_2018
      ...

  documentation_2018/
  samples_2018/
  lib -> compilers_and_libraries/linux/lib/
  mkl -> compilers_and_libraries/linux/mkl/
  tbb -> compilers_and_libraries/linux/tbb/
                  - auxiliaries and convenience links

Spack-external installation of Intel-MPI 2018
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For MPI, the layout is slightly different than MKL. The prefix will have to
include an architecture directory (typically ``intel64``), which then contains
bin/, lib/, ..., all without further architecture branching.  The environment
variable ``I_MPI_ROOT`` from the API documentation, however, must be the
package's top directory, not including the architecture.

FIXME: For MANPATH, need the parent dir.

::

  $ ls -lF /opt/intel/compilers_and_libraries_2018.1.163/linux/mpi/
  bin64 -> intel64/bin/
  etc64 -> intel64/etc/
  include64 -> intel64/include/
  lib64 -> intel64/lib/

  benchmarks/
  binding/
  intel64/
  man/
  test/

The package contains an MPI-2019 preview; Curiously, its release notes contain
the tag: "File structure clean-up." I could not find further documentation on
this, however, so it is unclear what, if any, changes will make it to release.

https://software.intel.com/en-us/articles/restoring-legacy-path-structure-on-intel-mpi-library-2019

::

  $ ls -lF /opt/intel/compilers_and_libraries_2018.1.163/linux/mpi_2019/
  binding/
  doc/
  imb/
  intel64/
  man/
  test/

Spack-external installation of Intel Parallel Studio 2018
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

This is the main product bundle that I actually downloaded and installed on my
system.  Its nominal installation directory mostly holds merely symlinks
to components installed in sibling dirs::

  $ ls -lF /opt/intel/parallel_studio_xe_2018.1.038/
  advisor_2018		 -> /opt/intel/advisor_2018/
  clck_2018		 -> /opt/intel/clck/2018.1/
  compilers_and_libraries_2018 -> /opt/intel/comp....aries_2018.1.163/
  documentation_2018	 -> /opt/intel/documentation_2018/
  ide_support_2018	 -> /opt/intel/ide_support_2018/
  inspector_2018		 -> /opt/intel/inspector_2018/
  itac_2018		 -> /opt/intel/itac/2018.1.017/
  man		         -> /opt/intel/man/
  samples_2018		 -> /opt/intel/samples_2018/
  vtune_amplifier_2018	 -> /opt/intel/vtune_amplifier_2018/

  psxevars.csh		 -> ./bin/psxevars.csh*
  psxevars.sh		 -> ./bin/psxevars.sh*
  bin/            - *vars.*sh scripts + sshconnectivity.exp ONLY

  licensing/
  uninstall*

The only relevant regular files are ``*vars.*sh``, but those also just churn
through the subordinate vars files of the components.

Installation model
~~~~~~~~~~~~~~~~~~~~

Intel packages come with an ``install.sh`` script that is normally run
interactively (in either text or GUI mode) but can run unattended with a
``--silent <file>`` option, which is of course what Spack uses.

Format of configuration file
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The configuration file is conventionally called ``silent.cfg`` and has a simple
``token=value`` syntax.  Before using the configuration file, the installer
calls ``<staging_dir>/pset/check.awk`` to validate it. Example paths to the
validator are::

      .../l_mkl_2018.1.163/pset/check.awk .
      .../parallel_studio_xe_2018_update1_cluster_edition/pset/check.awk

The tokens that are accepted in the configuration file vary between packages.
Tokens not supported for a given package **will cause the installer to stop
and fail.** This is particularly relevant for license-related tokens, which are
accepted only for packages that actually require a license.

Reference: [Intel's documentation](https://software.intel.com/en-us/articles/configuration-file-format)

See also:  https://software.intel.com/en-us/articles/silent-installation-guide-for-intel-parallel-studio-xe-composer-edition-for-os-x

The following is from ``.../parallel_studio_xe_2018_update1_cluster_edition/pset/check.awk``:

* Tokens valid for all packages encountered::

    ACCEPT_EULA                                  {accept, decline}
    CONTINUE_WITH_OPTIONAL_ERROR                 {yes, no}
    PSET_INSTALL_DIR                             {/opt/intel, , filepat}
    CONTINUE_WITH_INSTALLDIR_OVERWRITE           {yes, no}
    COMPONENTS                                   {ALL, DEFAULTS, , anythingpat}
    PSET_MODE                                    {install, repair, uninstall}
    NONRPM_DB_DIR                                {, filepat}

    SIGNING_ENABLED                              {yes, no}
    ARCH_SELECTED                                {IA32, INTEL64, ALL}

* Mentioned but unexplained in ``check.awk``::

    NO_VALIDATE   (?!)

* Only for licensed packages::

    ACTIVATION_SERIAL_NUMBER                     {, snpat}
    ACTIVATION_LICENSE_FILE                      {, lspat, filepat}
    ACTIVATION_TYPE                              {exist_lic, license_server,
                                                 license_file, trial_lic,

    PHONEHOME_SEND_USAGE_DATA                    {yes, no}
                                                 serial_number}

* Only for Amplifier (obviously)::

    AMPLIFIER_SAMPLING_DRIVER_INSTALL_TYPE       {build, kit}
    AMPLIFIER_DRIVER_ACCESS_GROUP                {, anythingpat, vtune}
    AMPLIFIER_DRIVER_PERMISSIONS                 {, anythingpat, 666}
    AMPLIFIER_LOAD_DRIVER                        {yes, no}
    AMPLIFIER_C_COMPILER                         {, filepat, auto, none}
    AMPLIFIER_KERNEL_SRC_DIR                     {, filepat, auto, none}
    AMPLIFIER_MAKE_COMMAND                       {, filepat, auto, none}
    AMPLIFIER_INSTALL_BOOT_SCRIPT                {yes, no}
    AMPLIFIER_DRIVER_PER_USER_MODE               {yes, no}

* Only for MKL and Studio::

    CLUSTER_INSTALL_REMOTE                       {yes, no}
    CLUSTER_INSTALL_TEMP                         {, filepat}
    CLUSTER_INSTALL_MACHINES_FILE                {, filepat}

* "backward compatibility" (?)::

    INSTALL_MODE                                 {RPM, NONRPM}
    download_only                                {yes}
    download_dir                                 {, filepat}


Details for licensing tokens
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Quoted from
https://software.intel.com/en-us/articles/configuration-file-format,
for reference:

[ed. note: As of 2018-05, the page incorrectly references ``ACTIVATION``, which
was used only until about 2012; this is corrected to ``ACTIVATION_TYPE`` here.]

    ...

    ``ACTIVATION_TYPE=exist_lic``
       This directive tells the install program to look for an existing
       license during the install process.  This is the preferred method for
       silent installs.  Take the time to register your serial number and get
       a license file (see below).  Having a license file on the system
       simplifies the process.  In addition, as an administrator it is good
       practice to know WHERE your licenses are saved on your system.
       License files are plain text files with a .lic extension.  By default
       these are saved in /opt/intel/licenses which is searched by default.
       If you save your license elsewhere, perhaps under an NFS folder, set
       environment variable **INTEL_LICENSE_FILE** to the full path to your
       license file prior to starting the installation or use the
       configuration file directive ``ACTIVATION_LICENSE_FILE`` to specify the
       full pathname to the license file.
    
       Options for ``ACTIVATION_TYPE`` are ``{ exist_lic, license_file, server_lic,
       serial_number, trial_lic }``
    
    ``exist_lic``
       directs the installer to search for a valid license on the server.
       Searches will utilize the environment variable **INTEL_LICENSE_FILE**,
       search the default license directory /opt/intel/licenses, or use the
       ``ACTIVATION_LICENSE_FILE`` directive to find a valid license file.
    
    ``license_file``
       is similar to exist_lic but directs the installer to use
       ``ACTIVATION_LICENSE_FILE`` to find the license file.
    
    ``server_lic``
       is similar to exist_lic and exist_lic but directs the installer that
       this is a client installation and a floating license server will be
       contacted to active the product.  This option will contact your
       floating license server on your network to retrieve the license
       information.  BEFORE using this option make sure your client is
       correctly set up for your network including all networking, routing,
       name service, and firewall configuration.  Insure that your client has
       direct access to your floating license server and that firewalls are
       set up to allow TCP/IP access for the 2 license server ports.
       server_lic will use **INTEL_LICENSE_FILE** containing a port@host format
       OR a client license file.  The formats for these are described here
       https://software.intel.com/en-us/articles/licensing-setting-up-the-client-floating-license
    
    ``serial_number``
       directs the installer to use directive ``ACTIVATION_SERIAL_NUMBER`` for
       activation.  This method will require the installer to contact an
       external Intel activation server over the Internet to confirm your
       serial number.  Due to user and company firewalls, this method is more
       complex and hence error prone of the available activation methods.  We
       highly recommend using a license file or license server for activation
       instead.
    
    ``trial_lic``
       is used only if you do not have an existing license and intend to
       temporarily evaluate the compiler.  This method creates a temporary
       trial license in Trusted Storage on your system.
    
    ...

*******************
vars files
*******************

Intel's product packages contain a number of shell initialization files let's call them vars files.

There are three kinds:

#. Component-specific vars files, such as `mklvars` or `tbbvars`.
#. Toplevel vars files such as "psxevars". They will scan for all
   component-specific vars files associated with the product, and source them
   if found.
#. Symbolic links to either of them. Links may appear under a different name
   for backward compatibility.

At present, IntelPackage class is only concerned with the toplevel vars files,
generally found in the product's toplevel bin/ directory.

For reference, here is an overview of the names and locations of the vars files
in the 2018 product releases, as seen for Spack-native installation. NB: May be
incomplete as some components may have been omitted during installation.

Names of vars files seen::

    $ cd opt/spack/linux-centos6-x86_64
    $ find intel* -name \*vars.sh -printf '%f\n' | sort -u | nl
     1	advixe-vars.sh
     2	amplxe-vars.sh
     3	apsvars.sh
     4	compilervars.sh
     5	daalvars.sh
     6	debuggervars.sh
     7	iccvars.sh
     8	ifortvars.sh
     9	inspxe-vars.sh
    10	ippvars.sh
    11	mklvars.sh
    12	mpivars.sh
    13	pstlvars.sh
    14	psxevars.sh
    15	sep_vars.sh
    16	tbbvars.sh

Names and locations of vars files, sorted by Spack package name::

    $ cd opt/spack/linux-centos6-x86_64
    $ find intel* -name \*vars.sh -printf '%y\t%-15f\t%h\n' \
        | cut -d/ -f1,4- \
        | sed '/iccvars\|ifortvars/d; s,/,\t\t,; s,\.sh,,; s,  */\(intel[/-]\),\1,' \
        | sort -k3,3 -k2,2 \
        | nl \
        | awk '{printf "%6i %-2s %-16s %-24s %s\n", $1, $2, $3, $4, $5}'

    --------------------------------------------------------------------------------------------------------
    item no.
       file or link
          name of vars file
                           Spack package name
                                                    dir relative to Spack install dir
    --------------------------------------------------------------------------------------------------------

     1 f  mpivars          intel                    compilers_and_libraries_2018.1.163/linux/mpi/intel64/bin
     2 f  mpivars          intel                    compilers_and_libraries_2018.1.163/linux/mpirt/bin/ia32_lin
     3 f  tbbvars          intel                    compilers_and_libraries_2018.1.163/linux/tbb/bin
     4 f  pstlvars         intel                    compilers_and_libraries_2018.1.163/linux/pstl/bin
     5 f  compilervars     intel                    compilers_and_libraries_2018.1.163/linux/bin
     6 f  compilervars     intel                    compilers_and_libraries_2018/linux/bin
     7 l  compilervars     intel                    bin
     8 f  daalvars         intel-daal               compilers_and_libraries_2018.2.199/linux/daal/bin
     9 f  psxevars         intel-daal               parallel_studio_xe_2018.2.046/bin
    10 l  psxevars         intel-daal               parallel_studio_xe_2018.2.046
    11 f  compilervars     intel-daal               compilers_and_libraries_2018.2.199/linux/bin
    12 f  compilervars     intel-daal               compilers_and_libraries_2018/linux/bin
    13 l  compilervars     intel-daal               bin
    14 f  ippvars          intel-ipp                compilers_and_libraries_2018.2.199/linux/ipp/bin
    15 f  psxevars         intel-ipp                parallel_studio_xe_2018.2.046/bin
    16 l  psxevars         intel-ipp                parallel_studio_xe_2018.2.046
    17 f  compilervars     intel-ipp                compilers_and_libraries_2018.2.199/linux/bin
    18 f  compilervars     intel-ipp                compilers_and_libraries_2018/linux/bin
    19 l  compilervars     intel-ipp                bin
    20 f  mklvars          intel-mkl                compilers_and_libraries_2018.2.199/linux/mkl/bin
    21 f  psxevars         intel-mkl                parallel_studio_xe_2018.2.046/bin
    22 l  psxevars         intel-mkl                parallel_studio_xe_2018.2.046
    23 f  compilervars     intel-mkl                compilers_and_libraries_2018.2.199/linux/bin
    24 f  compilervars     intel-mkl                compilers_and_libraries_2018/linux/bin
    25 l  compilervars     intel-mkl                bin
    26 f  mpivars          intel-mpi                compilers_and_libraries_2018.2.199/linux/mpi_2019/intel64/bin
    27 f  mpivars          intel-mpi                compilers_and_libraries_2018.2.199/linux/mpi/intel64/bin
    28 f  psxevars         intel-mpi                parallel_studio_xe_2018.2.046/bin
    29 l  psxevars         intel-mpi                parallel_studio_xe_2018.2.046
    30 f  compilervars     intel-mpi                compilers_and_libraries_2018.2.199/linux/bin
    31 f  compilervars     intel-mpi                compilers_and_libraries_2018/linux/bin
    32 l  compilervars     intel-mpi                bin
    33 f  apsvars          intel-parallel-studio    vtune_amplifier_2018.1.0.535340
    34 l  apsvars          intel-parallel-studio    performance_snapshots_2018.1.0.535340
    35 f  ippvars          intel-parallel-studio    compilers_and_libraries_2018.1.163/linux/ipp/bin
    36 f  ippvars          intel-parallel-studio    composer_xe_2015.6.233/ipp/bin
    37 f  mklvars          intel-parallel-studio    compilers_and_libraries_2018.1.163/linux/mkl/bin
    38 f  mklvars          intel-parallel-studio    composer_xe_2015.6.233/mkl/bin
    39 f  mpivars          intel-parallel-studio    compilers_and_libraries_2018.1.163/linux/mpi/intel64/bin
    40 f  mpivars          intel-parallel-studio    compilers_and_libraries_2018.1.163/linux/mpirt/bin/ia32_lin
    41 f  tbbvars          intel-parallel-studio    compilers_and_libraries_2018.1.163/linux/tbb/bin
    42 f  tbbvars          intel-parallel-studio    composer_xe_2015.6.233/tbb/bin
    43 f  daalvars         intel-parallel-studio    compilers_and_libraries_2018.1.163/linux/daal/bin
    44 f  pstlvars         intel-parallel-studio    compilers_and_libraries_2018.1.163/linux/pstl/bin
    45 f  psxevars         intel-parallel-studio    parallel_studio_xe_2018.1.038/bin
    46 l  psxevars         intel-parallel-studio    parallel_studio_xe_2018.1.038
    47 f  sep_vars         intel-parallel-studio    vtune_amplifier_2018.1.0.535340
    48 f  sep_vars         intel-parallel-studio    vtune_amplifier_2018.1.0.535340/target/android_v4.1_x86_64
    49 f  advixe-vars      intel-parallel-studio    advisor_2018.1.1.535164
    50 f  amplxe-vars      intel-parallel-studio    vtune_amplifier_2018.1.0.535340
    51 f  inspxe-vars      intel-parallel-studio    inspector_2018.1.1.535159
    52 f  compilervars     intel-parallel-studio    compilers_and_libraries_2018.1.163/linux/bin
    53 f  compilervars     intel-parallel-studio    compilers_and_libraries_2018/linux/bin
    54 l  compilervars     intel-parallel-studio    bin
    55 f  debuggervars     intel-parallel-studio    debugger_2018/bin


********************
MPI linkage
********************


Library selection
~~~~~~~~~~~~~~~~~~~~~

In the Spack code so far, the library selections for MPI are:

::

        libnames = ['libmpifort', 'libmpi']
        if 'cxx' in self.spec.last_query.extra_parameters:
            libnames = ['libmpicxx'] + libnames
        return find_libraries(libnames,
                              root=self.component_lib_dir('mpi'),
                              shared=True, recursive=False)

The problem is that there are multiple library versions under ``component_lib_dir``::

    $ cd $I_MPI_ROOT 
    $ find . -name libmpi.so | sort
    ./intel64/lib/debug/libmpi.so
    ./intel64/lib/debug_mt/libmpi.so
    ./intel64/lib/libmpi.so
    ./intel64/lib/release/libmpi.so
    ./intel64/lib/release_mt/libmpi.so

"mt" refers to multi-threading, not in the explicit sense but in the sense of being thread-safe::

    $ mpiifort -help | grep mt
       -mt_mpi         link the thread safe version of the Intel(R) MPI Library

Well, why should we not inspect what the canonical script does?  The wrapper
has its own hardcoded "prefix=..." and can thus tell us what it will do, from a
*wiped environment* no less!::

    $ env - intel64/bin/mpiicc -show hello.c | ld-unwrap-args 
    icc 'hello.c' \
        -I/opt/intel/compilers_and_libraries_2018.1.163/linux/mpi/intel64/include \
        -L/opt/intel/compilers_and_libraries_2018.1.163/linux/mpi/intel64/lib/release_mt \
        -L/opt/intel/compilers_and_libraries_2018.1.163/linux/mpi/intel64/lib \
        -Xlinker --enable-new-dtags \
        -Xlinker -rpath=/opt/intel/compilers_and_libraries_2018.1.163/linux/mpi/intel64/lib/release_mt \
        -Xlinker -rpath=/opt/intel/compilers_and_libraries_2018.1.163/linux/mpi/intel64/lib \
        -Xlinker -rpath=/opt/intel/mpi-rt/2017.0.0/intel64/lib/release_mt \
        -Xlinker -rpath=/opt/intel/mpi-rt/2017.0.0/intel64/lib \
        -lmpifort \
        -lmpi \
        -lmpigi \
        -ldl \
        -lrt \
        -lpthread


MPI Wrapper options
~~~~~~~~~~~~~~~~~~~~~

For reference, here's the wrapper's builtin help output::

    $ mpiifort -help
    Simple script to compile and/or link MPI programs.
    Usage: mpiifort [options] <files>
    ----------------------------------------------------------------------------
    The following options are supported:
       -fc=<name> | -f90=<name>
                       specify a FORTRAN compiler name: i.e. -fc=ifort
       -echo           print the scripts during their execution
       -show           show command lines without real calling
       -config=<name>  specify a configuration file: i.e. -config=ifort for mpif90-ifort.conf file
       -v              print version info of mpiifort and its native compiler
       -profile=<name> specify a profile configuration file (an MPI profiling
                       library): i.e. -profile=myprofile for the myprofile.cfg file.
                       As a special case, lib<name>.so or lib<name>.a may be used
                       if the library is found
       -check_mpi      link against the Intel(R) Trace Collector (-profile=vtmc).
       -static_mpi     link the Intel(R) MPI Library statically
       -mt_mpi         link the thread safe version of the Intel(R) MPI Library
       -ilp64          link the ILP64 support of the Intel(R) MPI Library
       -no_ilp64       disable ILP64 support explicitly
       -fast           the same as -static_mpi + pass -fast option to a compiler.
       -t or -trace
                       link against the Intel(R) Trace Collector
       -trace-imbalance
                       link against the Intel(R) Trace Collector imbalance library
                       (-profile=vtim)
       -dynamic_log    link against the Intel(R) Trace Collector dynamically
       -static         use static linkage method
       -nostrip        turn off the debug information stripping during static linking
       -O              enable optimization
       -link_mpi=<name>
                       link against the specified version of the Intel(R) MPI Library
    All other options will be passed to the compiler without changing.
    ----------------------------------------------------------------------------
    The following environment variables are used:
       I_MPI_ROOT      the Intel(R) MPI Library installation directory path
       I_MPI_F90 or MPICH_F90
                       the path/name of the underlying compiler to be used
       I_MPI_FC_PROFILE or I_MPI_F90_PROFILE or MPIF90_PROFILE
                       the name of profile file (without extension)
       I_MPI_COMPILER_CONFIG_DIR
                       the folder which contains configuration files *.conf
       I_MPI_TRACE_PROFILE
                       specify a default profile for the -trace option
       I_MPI_CHECK_PROFILE
                       specify a default profile for the -check_mpi option
       I_MPI_CHECK_COMPILER
                       enable compiler setup checks
       I_MPI_LINK      specify the version of the Intel(R) MPI Library
       I_MPI_DEBUG_INFO_STRIP
                       turn on/off the debug information stripping during static linking
       I_MPI_FCFLAGS
                       special flags needed for compilation
       I_MPI_LDFLAGS 
                       special flags needed for linking
    ----------------------------------------------------------------------------


Side Note: MPI version divergence in 2015 release
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The package `intel-parallel-studio@cluster.2015.6` contains both a full MPI
development version in `$prefix/impi` and an MPI Runtime under the
`composer_xe*` suite directory. Curiously, these have *different versions*,
with a release date nearly 1 year apart::

    $ $SPACK_ROOT/...uaxaw7/impi/5.0.3.049/intel64/bin/mpiexec --version
    Intel(R) MPI Library for Linux* OS, Version 5.0 Update 3 Build 20150804 (build id: 12452)
    Copyright (C) 2003-2015, Intel Corporation. All rights reserved.

    $ $SPACK_ROOT/...uaxaw7/composer_xe_2015.6.233/mpirt/bin/intel64/mpiexec --version
    Intel(R) MPI Library for Linux* OS, Version 5.0 Update 1 Build 20140709
    Copyright (C) 2003-2014, Intel Corporation. All rights reserved.

I'm not sure what to make of it.


**************
macOS support
**************

- On macOS, the Spack methods here only include support to integrate an
  externally installed MKL.

- URLs in child packages will be Linux-specific; macOS download packages
  are located in differently numbered dirs and are named m_*.dmg.

Plugin registration and hook calling for Python
===============================================

This is the plugin manager as used by pytest but stripped
of pytest specific details.

During the 0.x series this plugin does not have much documentation
except extensive docstrings in the pluggy.py module.


MarkupSafe
==========

MarkupSafe implements a text object that escapes characters so it is
safe to use in HTML and XML. Characters that have special meanings are
replaced so that they display as the actual characters. This mitigates
injection attacks, meaning untrusted user input can safely be displayed
on a page.


Installing
----------

Install and update using `pip`_:

.. code-block:: text

    pip install -U MarkupSafe

.. _pip: https://pip.pypa.io/en/stable/quickstart/


Examples
--------

.. code-block:: pycon

    >>> from markupsafe import Markup, escape
    >>> # escape replaces special characters and wraps in Markup
    >>> escape('<script>alert(document.cookie);</script>')
    Markup(u'&lt;script&gt;alert(document.cookie);&lt;/script&gt;')
    >>> # wrap in Markup to mark text "safe" and prevent escaping
    >>> Markup('<strong>Hello</strong>')
    Markup('<strong>hello</strong>')
    >>> escape(Markup('<strong>Hello</strong>'))
    Markup('<strong>hello</strong>')
    >>> # Markup is a text subclass (str on Python 3, unicode on Python 2)
    >>> # methods and operators escape their arguments
    >>> template = Markup("Hello <em>%s</em>")
    >>> template % '"World"'
    Markup('Hello <em>&#34;World&#34;</em>')


Donate
------

The Pallets organization develops and supports MarkupSafe and other
libraries that use it. In order to grow the community of contributors
and users, and allow the maintainers to devote more time to the
projects, `please donate today`_.

.. _please donate today: https://palletsprojects.com/donate


Links
-----

*   Website: https://palletsprojects.com/p/markupsafe/
*   Documentation: https://markupsafe.palletsprojects.com/
*   License: `BSD-3-Clause <https://github.com/pallets/markupsafe/blob/master/LICENSE.rst>`_
*   Releases: https://pypi.org/project/MarkupSafe/
*   Code: https://github.com/pallets/markupsafe
*   Issue tracker: https://github.com/pallets/markupsafe/issues
*   Test status:

    *   Linux, Mac: https://travis-ci.org/pallets/markupsafe
    *   Windows: https://ci.appveyor.com/project/pallets/markupsafe

*   Test coverage: https://codecov.io/gh/pallets/markupsafe

ruamel.yaml
===========

``ruamel.yaml`` is a YAML 1.2 loader/dumper package for Python.

* `Overview <http://yaml.readthedocs.org/en/latest/overview.html>`_
* `Installing <http://yaml.readthedocs.org/en/latest/install.html>`_
* `Details <http://yaml.readthedocs.org/en/latest/detail.html>`_
* `Examples <http://yaml.readthedocs.org/en/latest/example.html>`_
* `Differences with PyYAML <http://yaml.readthedocs.org/en/latest/pyyaml.html>`_

.. image:: https://readthedocs.org/projects/yaml/badge/?version=stable
   :target: https://yaml.readthedocs.org/en/stable

ChangeLog
=========

::

  0.11.15 (2016-XX-XX):
    - Change to prevent FutureWarning in NumPy, as reported by tgehring
    ("comparison to None will result in an elementwise object comparison in the future")

  0.11.14 (2016-07-06):
    - fix preserve_quotes missing on original Loaders (as reported
      by Leynos, bitbucket issue 38)

  0.11.13 (2016-07-06):
    - documentation only, automated linux wheels

  0.11.12 (2016-07-06):
    - added support for roundtrip of single/double quoted scalars using:
      ruamel.yaml.round_trip_load(stream, preserve_quotes=True)

  0.11.0 (2016-02-18):
    - RoundTripLoader loads 1.2 by default (no sexagesimals, 012 octals nor
      yes/no/on/off booleans
.. Copyright 2013-2022 Lawrence Livermore National Security, LLC and other
   Spack Project Developers. See the top-level COPYRIGHT file for details.

   SPDX-License-Identifier: (Apache-2.0 OR MIT)

.. _package-list:

============
Package List
============

This is a list of things you can install using Spack.  It is
automatically generated based on the packages in this Spack
version.

.. raw:: html
   :file: package_list.html
.. Copyright 2013-2022 Lawrence Livermore National Security, LLC and other
   Spack Project Developers. See the top-level COPYRIGHT file for details.

   SPDX-License-Identifier: (Apache-2.0 OR MIT)

.. _build-settings:

===================
Build Customization
===================

Spack allows you to customize how your software is built through the
``packages.yaml`` file.  Using it, you can make Spack prefer particular
implementations of virtual dependencies (e.g., MPI or BLAS/LAPACK),
or you can make it prefer to build with particular compilers.  You can
also tell Spack to use *external* software installations already
present on your system.

At a high level, the ``packages.yaml`` file is structured like this:

.. code-block:: yaml

   packages:
     package1:
       # settings for package1
     package2:
       # settings for package2
     # ...
     all:
       # settings that apply to all packages.

So you can either set build preferences specifically for *one* package,
or you can specify that certain settings should apply to *all* packages.
The types of settings you can customize are described in detail below.

Spack's build defaults are in the default
``etc/spack/defaults/packages.yaml`` file.  You can override them in
``~/.spack/packages.yaml`` or ``etc/spack/packages.yaml``. For more
details on how this works, see :ref:`configuration-scopes`.

.. _sec-external-packages:

-----------------
External Packages
-----------------

Spack can be configured to use externally-installed
packages rather than building its own packages. This may be desirable
if machines ship with system packages, such as a customized MPI
that should be used instead of Spack building its own MPI.

External packages are configured through the ``packages.yaml`` file found
in a Spack installation's ``etc/spack/`` or a user's ``~/.spack/``
directory. Here's an example of an external configuration:

.. code-block:: yaml

   packages:
     openmpi:
       externals:
       - spec: "openmpi@1.4.3%gcc@4.4.7 arch=linux-debian7-x86_64"
         prefix: /opt/openmpi-1.4.3
       - spec: "openmpi@1.4.3%gcc@4.4.7 arch=linux-debian7-x86_64+debug"
         prefix: /opt/openmpi-1.4.3-debug
       - spec: "openmpi@1.6.5%intel@10.1 arch=linux-debian7-x86_64"
         prefix: /opt/openmpi-1.6.5-intel

This example lists three installations of OpenMPI, one built with GCC,
one built with GCC and debug information, and another built with Intel.
If Spack is asked to build a package that uses one of these MPIs as a
dependency, it will use the pre-installed OpenMPI in
the given directory. Note that the specified path is the top-level
install prefix, not the ``bin`` subdirectory.

``packages.yaml`` can also be used to specify modules to load instead
of the installation prefixes.  The following example says that module
``CMake/3.7.2`` provides cmake version 3.7.2.

.. code-block:: yaml

   cmake:
     externals:
     - spec: cmake@3.7.2
       modules:
       - CMake/3.7.2

Each ``packages.yaml`` begins with a ``packages:`` attribute, followed
by a list of package names.  To specify externals, add an ``externals:``
attribute under the package name, which lists externals.
Each external should specify a ``spec:`` string that should be as
well-defined as reasonably possible.  If a
package lacks a spec component, such as missing a compiler or
package version, then Spack will guess the missing component based
on its most-favored packages, and it may guess incorrectly.

Each package version and compiler listed in an external should
have entries in Spack's packages and compiler configuration, even
though the package and compiler may not ever be built.

The packages configuration can tell Spack to use an external location
for certain package versions, but it does not restrict Spack to using
external packages.  In the above example, since newer versions of OpenMPI
are available, Spack will choose to start building and linking with the
latest version rather than continue using the pre-installed OpenMPI versions.

To prevent this, the ``packages.yaml`` configuration also allows packages
to be flagged as non-buildable.  The previous example could be modified to
be:

.. code-block:: yaml

   packages:
     openmpi:
       externals:
       - spec: "openmpi@1.4.3%gcc@4.4.7 arch=linux-debian7-x86_64"
         prefix: /opt/openmpi-1.4.3
       - spec: "openmpi@1.4.3%gcc@4.4.7 arch=linux-debian7-x86_64+debug"
         prefix: /opt/openmpi-1.4.3-debug
       - spec: "openmpi@1.6.5%intel@10.1 arch=linux-debian7-x86_64"
         prefix: /opt/openmpi-1.6.5-intel
       buildable: False

The addition of the ``buildable`` flag tells Spack that it should never build
its own version of OpenMPI, and it will instead always rely on a pre-built
OpenMPI.  Similar to ``paths``, ``buildable`` is specified as a property under
a package name.

If an external module is specified as not buildable, then Spack will load the
external module into the build environment which can be used for linking.

The ``buildable`` does not need to be paired with external packages.
It could also be used alone to forbid packages that may be
buggy or otherwise undesirable.

Virtual packages in Spack can also be specified as not buildable, and
external implementations can be provided. In the example above,
OpenMPI is configured as not buildable, but Spack will often prefer
other MPI implementations over the externally available OpenMPI. Spack
can be configured with every MPI provider not buildable individually,
but more conveniently:

.. code-block:: yaml

   packages:
     mpi:
       buildable: False
     openmpi:
       externals:
       - spec: "openmpi@1.4.3%gcc@4.4.7 arch=linux-debian7-x86_64"
         prefix: /opt/openmpi-1.4.3
       - spec: "openmpi@1.4.3%gcc@4.4.7 arch=linux-debian7-x86_64+debug"
         prefix: /opt/openmpi-1.4.3-debug
       - spec: "openmpi@1.6.5%intel@10.1 arch=linux-debian7-x86_64"
         prefix: /opt/openmpi-1.6.5-intel

Implementations can also be listed immediately under the virtual they provide:

.. code-block:: yaml

   packages:
     mpi:
       buildable: False
         openmpi@1.4.3%gcc@4.4.7 arch=linux-debian7-x86_64: /opt/openmpi-1.4.3
         openmpi@1.4.3%gcc@4.4.7 arch=linux-debian7-x86_64+debug: /opt/openmpi-1.4.3-debug
         openmpi@1.6.5%intel@10.1 arch=linux-debian7-x86_64: /opt/openmpi-1.6.5-intel
         mpich@3.3 %clang@9.0.0 arch=linux-debian7-x86_64: /opt/mpich-3.3-intel

Spack can then use any of the listed external implementations of MPI
to satisfy a dependency, and will choose depending on the compiler and
architecture.

.. _cmd-spack-external-find:

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Automatically Find External Packages
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

You can run the :ref:`spack external find <spack-external-find>` command
to search for system-provided packages and add them to ``packages.yaml``.
After running this command your ``packages.yaml`` may include new entries:

.. code-block:: yaml

   packages:
     cmake:
       externals:
       - spec: cmake@3.17.2
         prefix: /usr

Generally this is useful for detecting a small set of commonly-used packages;
for now this is generally limited to finding build-only dependencies.
Specific limitations include:

* Packages are not discoverable by default: For a package to be
  discoverable with ``spack external find``, it needs to add special
  logic. See :ref:`here <make-package-findable>` for more details.
* The current implementation only collects and examines executable files,
  so it is typically only useful for build/run dependencies (in some cases
  if a library package also provides an executable, it may be possible to
  extract a meaningful Spec by running the executable - for example the
  compiler wrappers in MPI implementations).
* The logic does not search through module files, it can only detect
  packages with executables defined in ``PATH``; you can help Spack locate
  externals which use module files by loading any associated modules for
  packages that you want Spack to know about before running
  ``spack external find``.
* Spack does not overwrite existing entries in the package configuration:
  If there is an external defined for a spec at any configuration scope,
  then Spack will not add a new external entry (``spack config blame packages``
  can help locate all external entries).

.. _concretization-preferences:

--------------------------
Concretization Preferences
--------------------------

Spack can be configured to prefer certain compilers, package
versions, dependencies, and variants during concretization.
The preferred configuration can be controlled via the
``~/.spack/packages.yaml`` file for user configurations, or the
``etc/spack/packages.yaml`` site configuration.

Here's an example ``packages.yaml`` file that sets preferred packages:

.. code-block:: yaml

   packages:
     opencv:
       compiler: [gcc@4.9]
       variants: +debug
     gperftools:
       version: [2.2, 2.4, 2.3]
     all:
       compiler: [gcc@4.4.7, 'gcc@4.6:', intel, clang, pgi]
       target: [sandybridge]
       providers:
         mpi: [mvapich2, mpich, openmpi]

At a high level, this example is specifying how packages should be
concretized.  The opencv package should prefer using GCC 4.9 and
be built with debug options.  The gperftools package should prefer version
2.2 over 2.4.  Every package on the system should prefer mvapich2 for
its MPI and GCC 4.4.7 (except for opencv, which overrides this by preferring GCC 4.9).
These options are used to fill in implicit defaults.  Any of them can be overwritten
on the command line if explicitly requested.

Each ``packages.yaml`` file begins with the string ``packages:`` and
package names are specified on the next level. The special string ``all``
applies settings to *all* packages. Underneath each package name is one
or more components: ``compiler``, ``variants``, ``version``,
``providers``, and ``target``.  Each component has an ordered list of
spec ``constraints``, with earlier entries in the list being preferred
over later entries.

Sometimes a package installation may have constraints that forbid
the first concretization rule, in which case Spack will use the first
legal concretization rule.  Going back to the example, if a user
requests gperftools 2.3 or later, then Spack will install version 2.4
as the 2.4 version of gperftools is preferred over 2.3.

An explicit concretization rule in the preferred section will always
take preference over unlisted concretizations.  In the above example,
xlc isn't listed in the compiler list.  Every listed compiler from
gcc to pgi will thus be preferred over the xlc compiler.

The syntax for the ``provider`` section differs slightly from other
concretization rules.  A provider lists a value that packages may
``depend_on`` (e.g, MPI) and a list of rules for fulfilling that
dependency.

.. _package_permissions:

-------------------
Package Permissions
-------------------

Spack can be configured to assign permissions to the files installed
by a package.

In the ``packages.yaml`` file under ``permissions``, the attributes
``read``, ``write``, and ``group`` control the package
permissions. These attributes can be set per-package, or for all
packages under ``all``. If permissions are set under ``all`` and for a
specific package, the package-specific settings take precedence.

The ``read`` and ``write`` attributes take one of ``user``, ``group``,
and ``world``.

.. code-block:: yaml

  packages:
    all:
      permissions:
        write: group
        group: spack
    my_app:
      permissions:
        read: group
        group: my_team

The permissions settings describe the broadest level of access to
installations of the specified packages. The execute permissions of
the file are set to the same level as read permissions for those files
that are executable. The default setting for ``read`` is ``world``,
and for ``write`` is ``user``. In the example above, installations of
``my_app`` will be installed with user and group permissions but no
world permissions, and owned by the group ``my_team``. All other
packages will be installed with user and group write privileges, and
world read privileges. Those packages will be owned by the group
``spack``.

The ``group`` attribute assigns a Unix-style group to a package. All
files installed by the package will be owned by the assigned group,
and the sticky group bit will be set on the install prefix and all
directories inside the install prefix. This will ensure that even
manually placed files within the install prefix are owned by the
assigned group. If no group is assigned, Spack will allow the OS
default behavior to go as expected.
.. Copyright 2013-2022 Lawrence Livermore National Security, LLC and other
   Spack Project Developers. See the top-level COPYRIGHT file for details.

   SPDX-License-Identifier: (Apache-2.0 OR MIT)

.. _repositories:

=============================
Package Repositories
=============================

Spack comes with thousands of built-in package recipes in
``var/spack/repos/builtin/``.  This is a **package repository** -- a
directory that Spack searches when it needs to find a package by name.
You may need to maintain packages for restricted, proprietary or
experimental software separately from the built-in repository. Spack
allows you to configure local repositories using either the
``repos.yaml`` or the ``spack repo`` command.

A package repository a directory structured like this::

  repo/
      repo.yaml
      packages/
          hdf5/
              package.py
          mpich/
              package.py
              mpich-1.9-bugfix.patch
          trilinos/
              package.py
          ...

The top-level ``repo.yaml`` file contains configuration metadata for the
repository, and the ``packages`` directory contains subdirectories for
each package in the repository.  Each package directory contains a
``package.py`` file and any patches or other files needed to build the
package.

Package repositories allow you to:

1. Maintain your own packages separately from Spack;

2. Share your packages (e.g., by hosting them in a shared file system),
   without committing them to the built-in Spack package repository; and

3. Override built-in Spack packages with your own implementation.

Packages in a separate repository can also *depend on* built-in Spack
packages.  So, you can leverage existing recipes without re-implementing
them in your own repository.

---------------------
``repos.yaml``
---------------------

Spack uses the ``repos.yaml`` file in ``~/.spack`` (and :ref:`elsewhere
<configuration>`) to find repositories. Note that the ``repos.yaml``
configuration file is distinct from the ``repo.yaml`` file in each
repository.  For more on the YAML format, and on how configuration file
precedence works in Spack, see :ref:`configuration <configuration>`.

The default ``etc/spack/defaults/repos.yaml`` file looks like this:

.. code-block:: yaml

  repos:
  - $spack/var/spack/repos/builtin

The file starts with ``repos:`` and contains a single ordered list of
paths to repositories. Each path is on a separate line starting with
``-``.  You can add a repository by inserting another path into the list:

.. code-block:: yaml

  repos:
  - /opt/local-repo
  - $spack/var/spack/repos/builtin

When Spack interprets a spec, e.g., ``mpich`` in ``spack install mpich``,
it searches these repositories in order (first to last) to resolve each
package name.  In this example, Spack will look for the following
packages and use the first valid file:

1. ``/opt/local-repo/packages/mpich/package.py``
2. ``$spack/var/spack/repos/builtin/packages/mpich/package.py``

.. note::

  Currently, Spack can only use repositories in the file system. We plan
  to eventually support URLs in ``repos.yaml``, so that you can easily
  point to remote package repositories, but that is not yet implemented.

---------------------
Namespaces
---------------------

Every repository in Spack has an associated **namespace** defined in its
top-level ``repo.yaml`` file.  If you look at
``var/spack/repos/builtin/repo.yaml`` in the built-in repository, you'll
see that its namespace is ``builtin``:

.. code-block:: console

  $ cat var/spack/repos/builtin/repo.yaml
  repo:
    namespace: builtin

Spack records the repository namespace of each installed package.  For
example, if you install the ``mpich`` package from the ``builtin`` repo,
Spack records its fully qualified name as ``builtin.mpich``.  This
accomplishes two things:

1. You can have packages with the same name from different namespaces
   installed at once.

1. You can easily determine which repository a package came from after it
   is installed (more :ref:`below <namespace-example>`).

.. note::

   It may seem redundant for a repository to have both a namespace and a
   path, but repository *paths* may change over time, or, as mentioned
   above, a locally hosted repository path may eventually be hosted at
   some remote URL.

   Namespaces are designed to allow *package authors* to associate a
   unique identifier with their packages, so that the package can be
   identified even if the repository moves. This is why the namespace is
   determined by the ``repo.yaml`` file in the repository rather than the
   local ``repos.yaml`` configuration: the *repository maintainer* sets
   the name.

^^^^^^^^^^^^^^^^^^^^^^^^^^^
Uniqueness
^^^^^^^^^^^^^^^^^^^^^^^^^^^

You should choose a namespace that uniquely identifies your package
repository.  For example, if you make a repository for packages written
by your organization, you could use your organization's name.  You can
also nest namespaces using periods, so you could identify a repository by
a sub-organization.  For example, LLNL might use a namespace for its
internal repositories like ``llnl``. Packages from the Physical & Life
Sciences directorate (PLS) might use the ``llnl.pls`` namespace, and
packages created by the Computation directorate might use ``llnl.comp``.

Spack cannot ensure that every repository is named uniquely, but it will
prevent you from registering two repositories with the same namespace at
the same time.  If you try to add a repository that has the same name as
an existing one, e.g., ``builtin``, Spack will print a warning message.

.. _namespace-example:

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Namespace example
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Suppose that LLNL maintains its own version of ``mpich``, separate from
Spack's built-in ``mpich`` package, and suppose you've installed both
LLNL's and Spack's ``mpich`` packages.  If you just use ``spack find``,
you won't see a difference between these two packages:

.. code-block:: console

  $ spack find
  ==> 2 installed packages.
  -- linux-rhel6-x86_64 / gcc@4.4.7 -------------
  mpich@3.2  mpich@3.2

However, if you use ``spack find -N``, Spack will display the packages
with their namespaces:

.. code-block:: console

  $ spack find -N
  ==> 2 installed packages.
  -- linux-rhel6-x86_64 / gcc@4.4.7 -------------
  builtin.mpich@3.2  llnl.comp.mpich@3.2

Now you know which one is LLNL's special version, and which one is the
built-in Spack package.  As you might guess, packages that are identical
except for their namespace will still have different hashes:

.. code-block:: console

  $ spack find -lN
  ==> 2 installed packages.
  -- linux-rhel6-x86_64 / gcc@4.4.7 -------------
  c35p3gc builtin.mpich@3.2  itoqmox llnl.comp.mpich@3.2

All Spack commands that take a package :ref:`spec <sec-specs>` can also
accept a fully qualified spec with a namespace.  This means you can use
the namespace to be more specific when designating, e.g., which package
you want to uninstall:

.. code-block:: console

  spack uninstall llnl.comp.mpich

----------------------------
Overriding built-in packages
----------------------------

Spack's search semantics mean that you can make your own implementation
of a built-in Spack package (like ``mpich``), put it in a repository, and
use it to override the built-in package.  As long as the repository
containing your ``mpich`` is earlier any other in ``repos.yaml``, any
built-in package that depends on ``mpich`` will be use the one in your
repository.

Suppose you have three repositories: the builtin Spack repo
(``builtin``), a shared repo for your institution (e.g., ``llnl``), and a
repo containing your own prototype packages (``proto``).  Suppose they
contain packages as follows:

  +--------------+------------------------------------+-----------------------------+
  | Namespace    | Path to repo                       | Packages                    |
  +==============+====================================+=============================+
  | ``proto``    | ``~/proto``                        | ``mpich``                   |
  +--------------+------------------------------------+-----------------------------+
  | ``llnl``     | ``/usr/local/llnl``                | ``hdf5``                    |
  +--------------+------------------------------------+-----------------------------+
  | ``builtin``  | ``$spack/var/spack/repos/builtin`` | ``mpich``, ``hdf5``, others |
  +--------------+------------------------------------+-----------------------------+

Suppose that ``hdf5`` depends on ``mpich``.  You can override the
built-in ``hdf5`` by adding the ``llnl`` repo to ``repos.yaml``:

.. code-block:: yaml

   repos:
   - /usr/local/llnl
   - $spack/var/spack/repos/builtin

``spack install hdf5`` will install ``llnl.hdf5 ^builtin.mpich``.

If, instead, ``repos.yaml`` looks like this:

.. code-block:: yaml

   repos:
   - ~/proto
   - /usr/local/llnl
   - $spack/var/spack/repos/builtin

``spack install hdf5`` will install ``llnl.hdf5 ^proto.mpich``.

Any unqualified package name will be resolved by searching ``repos.yaml``
from the first entry to the last.  You can force a particular
repository's package by using a fully qualified name.  For example, if
your ``repos.yaml`` is as above, and you want ``builtin.mpich`` instead
of ``proto.mpich``, you can write::

  spack install hdf5 ^builtin.mpich

which will install ``llnl.hdf5 ^builtin.mpich``.

Similarly, you can force the ``builtin.hdf5`` like this::

  spack install builtin.hdf5 ^builtin.mpich

This will not search ``repos.yaml`` at all, as the ``builtin`` repo is
specified in both cases.  It will install ``builtin.hdf5
^builtin.mpich``.

If you want to see which repositories will be used in a build *before*
you install it, you can use ``spack spec -N``:

.. code-block:: console

   $ spack spec -N hdf5
   Input spec
   --------------------------------
   hdf5

   Normalized
   --------------------------------
   hdf5
       ^zlib@1.1.2:

   Concretized
   --------------------------------
   builtin.hdf5@1.10.0-patch1%apple-clang@7.0.2+cxx~debug+fortran+mpi+shared~szip~threadsafe arch=darwin-elcapitan-x86_64
       ^builtin.openmpi@2.0.1%apple-clang@7.0.2~mxm~pmi~psm~psm2~slurm~sqlite3~thread_multiple~tm~verbs+vt arch=darwin-elcapitan-x86_64
           ^builtin.hwloc@1.11.4%apple-clang@7.0.2 arch=darwin-elcapitan-x86_64
               ^builtin.libpciaccess@0.13.4%apple-clang@7.0.2 arch=darwin-elcapitan-x86_64
                   ^builtin.libtool@2.4.6%apple-clang@7.0.2 arch=darwin-elcapitan-x86_64
                       ^builtin.m4@1.4.17%apple-clang@7.0.2+sigsegv arch=darwin-elcapitan-x86_64
                           ^builtin.libsigsegv@2.10%apple-clang@7.0.2 arch=darwin-elcapitan-x86_64
                   ^builtin.pkg-config@0.29.1%apple-clang@7.0.2+internal_glib arch=darwin-elcapitan-x86_64
                   ^builtin.util-macros@1.19.0%apple-clang@7.0.2 arch=darwin-elcapitan-x86_64
       ^builtin.zlib@1.2.8%apple-clang@7.0.2+pic arch=darwin-elcapitan-x86_64

.. warning::

   You *can* use a fully qualified package name in a ``depends_on``
   directive in a ``package.py`` file, like so::

       depends_on('proto.hdf5')

   This is *not* recommended, as it makes it very difficult for
   multiple repos to be composed and shared.  A ``package.py`` like this
   will fail if the ``proto`` repository is not registered in
   ``repos.yaml``.

.. _cmd-spack-repo:

--------------------------
``spack repo``
--------------------------

Spack's :ref:`configuration system <configuration>` allows repository
settings to come from ``repos.yaml`` files in many locations.  If you
want to see the repositories registered as a result of all configuration
files, use ``spack repo list``.

^^^^^^^^^^^^^^^^^^^
``spack repo list``
^^^^^^^^^^^^^^^^^^^

.. code-block:: console

  $ spack repo list
  ==> 2 package repositories.
  myrepo     ~/myrepo
  builtin    ~/spack/var/spack/repos/builtin

Each repository is listed with its associated namespace.  To get the raw,
merged YAML from all configuration files, use ``spack config get repos``:

.. code-block:: console

   $ spack config get repos
   repos:srepos:
   - ~/myrepo
   - $spack/var/spack/repos/builtin

Note that, unlike ``spack repo list``, this does not include the
namespace, which is read from each repo's ``repo.yaml``.

^^^^^^^^^^^^^^^^^^^^^
``spack repo create``
^^^^^^^^^^^^^^^^^^^^^

To make your own repository, you don't need to construct a directory
yourself; you can use the ``spack repo create`` command.

.. code-block:: console

  $ spack repo create myrepo
  ==> Created repo with namespace 'myrepo'.
  ==> To register it with spack, run this command:
    spack repo add ~/myrepo

  $ ls myrepo
  packages/  repo.yaml

  $ cat myrepo/repo.yaml
  repo:
    namespace: 'myrepo'

By default, the namespace of a new repo matches its directory's name.
You can supply a custom namespace with a second argument, e.g.:

.. code-block:: console

  $ spack repo create myrepo llnl.comp
  ==> Created repo with namespace 'llnl.comp'.
  ==> To register it with spack, run this command:
    spack repo add ~/myrepo

  $ cat myrepo/repo.yaml
  repo:
    namespace: 'llnl.comp'

^^^^^^^^^^^^^^^^^^
``spack repo add``
^^^^^^^^^^^^^^^^^^

Once your repository is created, you can register it with Spack with
``spack repo add``:

.. code-block:: console

   $ spack repo add ./myrepo
   ==> Added repo with namespace 'llnl.comp'.

   $ spack repo list
   ==> 2 package repositories.
   llnl.comp    ~/myrepo
   builtin      ~/spack/var/spack/repos/builtin

This simply adds the repo to your ``repos.yaml`` file.

Once a repository is registered like this, you should be able to see its
packages' names in the output of ``spack list``, and you should be able
to build them using ``spack install <name>`` as you would with any
built-in package.

^^^^^^^^^^^^^^^^^^^^^
``spack repo remove``
^^^^^^^^^^^^^^^^^^^^^

You can remove an already-registered repository with ``spack repo rm``.
This will work whether you pass the repository's namespace *or* its
path.

By namespace:

.. code-block:: console

  $ spack repo rm llnl.comp
  ==> Removed repository ~/myrepo with namespace 'llnl.comp'.

  $ spack repo list
  ==> 1 package repository.
  builtin    ~/spack/var/spack/repos/builtin

By path:

.. code-block:: console

  $ spack repo rm ~/myrepo
  ==> Removed repository ~/myrepo

  $ spack repo list
  ==> 1 package repository.
  builtin    ~/spack/var/spack/repos/builtin

--------------------------------
Repo namespaces and Python
--------------------------------

You may have noticed that namespace notation for repositories is similar
to the notation for namespaces in Python.  As it turns out, you *can*
treat Spack repositories like Python packages; this is how they are
implemented.

You could, for example, extend a ``builtin`` package in your own
repository:

.. code-block:: python

   from spack.pkg.builtin.mpich import Mpich

   class MyPackage(Mpich):
       ...

Spack repo namespaces are actually Python namespaces tacked on under
``spack.pkg``.  The search semantics of ``repos.yaml`` are actually
implemented using Python's built-in `sys.path
<https://docs.python.org/2/library/sys.html#sys.path>`_ search.  The
:py:mod:`spack.repo` module implements a custom `Python importer
<https://docs.python.org/2/library/imp.html>`_.

.. warning::

   The mechanism for extending packages is not yet extensively tested,
   and extending packages across repositories imposes inter-repo
   dependencies, which may be hard to manage.  Use this feature at your
   own risk, but let us know if you have a use case for it.
.. Copyright 2013-2022 Lawrence Livermore National Security, LLC and other
   Spack Project Developers. See the top-level COPYRIGHT file for details.

   SPDX-License-Identifier: (Apache-2.0 OR MIT)

============
Known Issues
============

This is a list of known bugs in Spack. It provides ways of getting around these
problems if you encounter them.

---------------------------------------------------
Variants are not properly forwarded to dependencies
---------------------------------------------------

**Status:** Expected to be fixed by Spack's new concretizer

Sometimes, a variant of a package can also affect how its dependencies are
built. For example, in order to build MPI support for a package, it may
require that its dependencies are also built with MPI support. In the
``package.py``, this looks like:

.. code-block:: python

   depends_on('hdf5~mpi', when='~mpi')
   depends_on('hdf5+mpi', when='+mpi')

Spack handles this situation properly for *immediate* dependencies, and
builds ``hdf5`` with the same variant you used for the package that
depends on it. However, for *indirect* dependencies (dependencies of
dependencies), Spack does not backtrack up the DAG far enough to handle
this. Users commonly run into this situation when trying to build R with
X11 support:

.. code-block:: console

   $ spack install r+X
   ...
   ==> Error: Invalid spec: 'cairo@1.14.8%gcc@6.2.1+X arch=linux-fedora25-x86_64 ^bzip2@1.0.6%gcc@6.2.1+shared arch=linux-fedora25-x86_64 ^font-util@1.3.1%gcc@6.2.1 arch=linux-fedora25-x86_64 ^fontconfig@2.12.1%gcc@6.2.1 arch=linux-fedora25-x86_64 ^freetype@2.7.1%gcc@6.2.1 arch=linux-fedora25-x86_64 ^gettext@0.19.8.1%gcc@6.2.1+bzip2+curses+git~libunistring+libxml2+tar+xz arch=linux-fedora25-x86_64 ^glib@2.53.1%gcc@6.2.1~libmount arch=linux-fedora25-x86_64 ^inputproto@2.3.2%gcc@6.2.1 arch=linux-fedora25-x86_64 ^kbproto@1.0.7%gcc@6.2.1 arch=linux-fedora25-x86_64 ^libffi@3.2.1%gcc@6.2.1 arch=linux-fedora25-x86_64 ^libpng@1.6.29%gcc@6.2.1 arch=linux-fedora25-x86_64 ^libpthread-stubs@0.4%gcc@6.2.1 arch=linux-fedora25-x86_64 ^libx11@1.6.5%gcc@6.2.1 arch=linux-fedora25-x86_64 ^libxau@1.0.8%gcc@6.2.1 arch=linux-fedora25-x86_64 ^libxcb@1.12%gcc@6.2.1 arch=linux-fedora25-x86_64 ^libxdmcp@1.1.2%gcc@6.2.1 arch=linux-fedora25-x86_64 ^libxext@1.3.3%gcc@6.2.1 arch=linux-fedora25-x86_64 ^libxml2@2.9.4%gcc@6.2.1~python arch=linux-fedora25-x86_64 ^libxrender@0.9.10%gcc@6.2.1 arch=linux-fedora25-x86_64 ^ncurses@6.0%gcc@6.2.1~symlinks arch=linux-fedora25-x86_64 ^openssl@1.0.2k%gcc@6.2.1 arch=linux-fedora25-x86_64 ^pcre@8.40%gcc@6.2.1+utf arch=linux-fedora25-x86_64 ^pixman@0.34.0%gcc@6.2.1 arch=linux-fedora25-x86_64 ^pkg-config@0.29.2%gcc@6.2.1+internal_glib arch=linux-fedora25-x86_64 ^python@2.7.13%gcc@6.2.1+shared~tk~ucs4 arch=linux-fedora25-x86_64 ^readline@7.0%gcc@6.2.1 arch=linux-fedora25-x86_64 ^renderproto@0.11.1%gcc@6.2.1 arch=linux-fedora25-x86_64 ^sqlite@3.18.0%gcc@6.2.1 arch=linux-fedora25-x86_64 ^tar^util-macros@1.19.1%gcc@6.2.1 arch=linux-fedora25-x86_64 ^xcb-proto@1.12%gcc@6.2.1 arch=linux-fedora25-x86_64 ^xextproto@7.3.0%gcc@6.2.1 arch=linux-fedora25-x86_64 ^xproto@7.0.31%gcc@6.2.1 arch=linux-fedora25-x86_64 ^xtrans@1.3.5%gcc@6.2.1 arch=linux-fedora25-x86_64 ^xz@5.2.3%gcc@6.2.1 arch=linux-fedora25-x86_64 ^zlib@1.2.11%gcc@6.2.1+pic+shared arch=linux-fedora25-x86_64'.
   Package cairo requires variant ~X, but spec asked for +X

A workaround is to explicitly activate the variants of dependencies as well:

.. code-block:: console

   $ spack install r+X ^cairo+X ^pango+X

See https://github.com/spack/spack/issues/267 and
https://github.com/spack/spack/issues/2546 for further details.

-----------------------------------------------
depends_on cannot handle recursive dependencies
-----------------------------------------------

**Status:** Not yet a work in progress

Although ``depends_on`` can handle any aspect of Spack's spec syntax,
it currently cannot handle recursive dependencies. If the ``^`` sigil
appears in a ``depends_on`` statement, the concretizer will hang.
For example, something like:

.. code-block:: python

   depends_on('mfem+cuda ^hypre+cuda', when='+cuda')


should be rewritten as:

.. code-block:: python

   depends_on('mfem+cuda', when='+cuda')
   depends_on('hypre+cuda', when='+cuda')


See https://github.com/spack/spack/issues/17660 and
https://github.com/spack/spack/issues/11160 for more details.
.. Copyright 2013-2022 Lawrence Livermore National Security, LLC and other
   Spack Project Developers. See the top-level COPYRIGHT file for details.

   SPDX-License-Identifier: (Apache-2.0 OR MIT)

.. _analyze:

=======
Analyze
=======


The analyze command is a front-end to various tools that let us analyze
package installations. Each analyzer is a module for a different kind
of analysis that can be done on a package installation, including (but not
limited to) binary, log, or text analysis. Thus, the analyze command group
allows you to take an existing package install, choose an analyzer,
and extract some output for the package using it.


-----------------
Analyzer Metadata
-----------------

For all analyzers, we write to an ``analyzers`` folder in ``~/.spack``, or the
value that you specify in your spack config at ``config:analyzers_dir``. 
For example, here we see the results of running an analysis on zlib:

.. code-block:: console

    $ tree ~/.spack/analyzers/
    â””â”€â”€ linux-ubuntu20.04-skylake
        â””â”€â”€ gcc-9.3.0
            â””â”€â”€ zlib-1.2.11-sl7m27mzkbejtkrajigj3a3m37ygv4u2
                â”œâ”€â”€ environment_variables
                â”‚Â Â  â””â”€â”€ spack-analyzer-environment-variables.json
                â”œâ”€â”€ install_files
                â”‚Â Â  â””â”€â”€ spack-analyzer-install-files.json
                â””â”€â”€ libabigail
                    â””â”€â”€ spack-analyzer-libabigail-libz.so.1.2.11.xml
    

This means that you can always find analyzer output in this folder, and it
is organized with the same logic as the package install it was run for. 
If you want to customize this top level folder, simply provide the ``--path``
argument to ``spack analyze run``. The nested organization will be maintained
within your custom root.

-----------------
Listing Analyzers
-----------------

If you aren't familiar with Spack's analyzers, you can quickly list those that 
are available:

.. code-block:: console

    $ spack analyze list-analyzers
    install_files            : install file listing read from install_manifest.json
    environment_variables    : environment variables parsed from spack-build-env.txt
    config_args              : config args loaded from spack-configure-args.txt
    libabigail               : Application Binary Interface (ABI) features for objects


In the above, the first three are fairly simple - parsing metadata files from
a package install directory to save

-------------------
Analyzing a Package
-------------------

The analyze command, akin to install, will accept a package spec to perform
an analysis for. The package must be installed. Let's walk through an example
with zlib. We first ask to analyze it. However, since we have more than one
install, we are asked to disambiguate:

.. code-block:: console

    $ spack analyze run zlib
    ==> Error: zlib matches multiple packages.
      Matching packages:
        fz2bs56 zlib@1.2.11%gcc@7.5.0 arch=linux-ubuntu18.04-skylake
        sl7m27m zlib@1.2.11%gcc@9.3.0 arch=linux-ubuntu20.04-skylake
      Use a more specific spec.


We can then specify the spec version that we want to analyze:

.. code-block:: console

    $ spack analyze run zlib/fz2bs56

If you don't provide any specific analyzer names, by default all analyzers 
(shown in the ``list-analyzers`` subcommand list) will be run. If an analyzer does not
have any result, it will be skipped. For example, here is a result running for
zlib:

.. code-block:: console

    $ ls ~/.spack/analyzers/linux-ubuntu20.04-skylake/gcc-9.3.0/zlib-1.2.11-sl7m27mzkbejtkrajigj3a3m37ygv4u2/
    spack-analyzer-environment-variables.json
    spack-analyzer-install-files.json
    spack-analyzer-libabigail-libz.so.1.2.11.xml

If you want to run a specific analyzer, ask for it with `--analyzer`. Here we run
spack analyze on libabigail (already installed) _using_ libabigail1

.. code-block:: console

    $ spack analyze run --analyzer abigail libabigail


.. _analyze_monitoring:

----------------------
Monitoring An Analysis
----------------------

For any kind of analysis, you can
use a `spack monitor <https://github.com/spack/spack-monitor>`_ "Spackmon"
as a server to upload the same run metadata to. You can
follow the instructions in the `spack monitor documentation <https://spack-monitor.readthedocs.org>`_
to first create a server along with a username and token for yourself.
You can then use this guide to interact with the server.

You should first export our spack monitor token and username to the environment:

.. code-block:: console
 
    $ export SPACKMON_TOKEN=50445263afd8f67e59bd79bff597836ee6c05438
    $ export SPACKMON_USER=spacky


By default, the host for your server is expected to be at ``http://127.0.0.1``
with a prefix of ``ms1``, and if this is the case, you can simply add the
``--monitor`` flag to the install command:

.. code-block:: console

    $ spack analyze run --monitor wget

If you need to customize the host or the prefix, you can do that as well:

.. code-block:: console

    $ spack analyze run --monitor --monitor-prefix monitor --monitor-host https://monitor-service.io wget

If your server doesn't have authentication, you can skip it:

.. code-block:: console

    $ spack analyze run --monitor --monitor-disable-auth wget
    
Regardless of your choice, when you run analyze on an installed package (whether
it was installed with ``--monitor`` or not, you'll see the results generating as they did
before, and a message that the monitor server was pinged:

.. code-block:: console

    $ spack analyze --monitor wget
    ...
    ==> Sending result for wget bin/wget to monitor.
.. Copyright 2013-2022 Lawrence Livermore National Security, LLC and other
   Spack Project Developers. See the top-level COPYRIGHT file for details.

   SPDX-License-Identifier: (Apache-2.0 OR MIT)

================
Feature Overview
================

This is a high-level overview of features that make Spack different
from other `package managers
<http://en.wikipedia.org/wiki/Package_management_system>`_ and `port
systems <http://en.wikipedia.org/wiki/Ports_collection>`_.

---------------------------
Simple package installation
---------------------------

Installing the default version of a package is simple. This will install
the latest version of the ``mpileaks`` package and all of its dependencies:

.. code-block:: console

   $ spack install mpileaks

--------------------------------
Custom versions & configurations
--------------------------------

Spack allows installation to be customized.  Users can specify the
version, build compiler, compile-time options, and cross-compile
platform, all on the command line.

.. code-block:: console

   # Install a particular version by appending @
   $ spack install mpileaks@1.1.2

   # Specify a compiler (and its version), with %
   $ spack install mpileaks@1.1.2 %gcc@4.7.3

   # Add special compile-time options by name
   $ spack install mpileaks@1.1.2 %gcc@4.7.3 debug=True

   # Add special boolean compile-time options with +
   $ spack install mpileaks@1.1.2 %gcc@4.7.3 +debug

   # Add compiler flags using the conventional names
   $ spack install mpileaks@1.1.2 %gcc@4.7.3 cppflags="-O3 -floop-block"

   # Cross-compile for a different micro-architecture with target=
   $ spack install mpileaks@1.1.2 target=icelake

Users can specify as many or few options as they care about. Spack
will fill in the unspecified values with sensible defaults. The two listed
syntaxes for variants are identical when the value is boolean.

----------------------
Customize dependencies
----------------------

Spack allows *dependencies* of a particular installation to be
customized extensively.  Suppose that ``hdf5`` depends
on ``openmpi`` and indirectly on ``hwloc``.  Using ``^``, users can add custom
configurations for the dependencies:

.. code-block:: console

   # Install hdf5 and link it with specific versions of openmpi and hwloc
   $ spack install hdf5@1.10.1 %gcc@4.7.3 +debug ^openmpi+cuda fabrics=auto ^hwloc+gl

------------------------
Non-destructive installs
------------------------

Spack installs every unique package/dependency configuration into its
own prefix, so new installs will not break existing ones.

-------------------------------
Packages can peacefully coexist
-------------------------------

Spack avoids library misconfiguration by using ``RPATH`` to link
dependencies.  When a user links a library or runs a program, it is
tied to the dependencies it was built with, so there is no need to
manipulate ``LD_LIBRARY_PATH`` at runtime.

-------------------------
Creating packages is easy
-------------------------

To create a new packages, all Spack needs is a URL for the source
archive.  The ``spack create`` command will create a boilerplate
package file, and the package authors can fill in specific build steps
in pure Python.

For example, this command:

.. code-block:: console

   $ spack create http://www.mr511.de/software/libelf-0.8.13.tar.gz

creates a simple python file:

.. code-block:: python

   from spack import *


   class Libelf(Package):
       """FIXME: Put a proper description of your package here."""

       # FIXME: Add a proper url for your package's homepage here.
       homepage = "http://www.example.com"
       url      = "http://www.mr511.de/software/libelf-0.8.13.tar.gz"

       version('0.8.13', '4136d7b4c04df68b686570afa26988ac')

       # FIXME: Add dependencies if required.
       # depends_on('foo')

       def install(self, spec, prefix):
           # FIXME: Modify the configure line to suit your build system here.
           configure('--prefix={0}'.format(prefix))

           # FIXME: Add logic to build and install here.
           make()
           make('install')

It doesn't take much python coding to get from there to a working
package:

.. literalinclude:: _spack_root/var/spack/repos/builtin/packages/libelf/package.py
   :lines: 6-

Spack also provides wrapper functions around common commands like
``configure``, ``make``, and ``cmake`` to make writing packages
simple.
.. Copyright 2013-2022 Lawrence Livermore National Security, LLC and other
   Spack Project Developers. See the top-level COPYRIGHT file for details.

   SPDX-License-Identifier: (Apache-2.0 OR MIT)

.. _mirrors:

=======
Mirrors
=======

Some sites may not have access to the internet for fetching packages.
These sites will need a local repository of tarballs from which they
can get their files.  Spack has support for this with *mirrors*.  A
mirror is a URL that points to a directory, either on the local
filesystem or on some server, containing tarballs for all of Spack's
packages.

Here's an example of a mirror's directory structure:

.. code-block:: none

   mirror/
       cmake/
           cmake-2.8.10.2.tar.gz
       dyninst/
           dyninst-8.1.1.tgz
           dyninst-8.1.2.tgz
       libdwarf/
           libdwarf-20130126.tar.gz
           libdwarf-20130207.tar.gz
           libdwarf-20130729.tar.gz
       libelf/
           libelf-0.8.12.tar.gz
           libelf-0.8.13.tar.gz
       libunwind/
           libunwind-1.1.tar.gz
       mpich/
           mpich-3.0.4.tar.gz
       mvapich2/
           mvapich2-1.9.tgz

The structure is very simple.  There is a top-level directory.  The
second level directories are named after packages, and the third level
contains tarballs for each package, named after each package.

.. note::

   Archives are **not** named exactly the way they were in the package's fetch
   URL.  They have the form ``<name>-<version>.<extension>``, where
   ``<name>`` is Spack's name for the package, ``<version>`` is the
   version of the tarball, and ``<extension>`` is whatever format the
   package's fetch URL contains.

   In order to make mirror creation reasonably fast, we copy the
   tarball in its original format to the mirror directory, but we do
   not standardize on a particular compression algorithm, because this
   would potentially require expanding and re-compressing each archive.

.. _cmd-spack-mirror:

----------------
``spack mirror``
----------------

Mirrors are managed with the ``spack mirror`` command.  The help for
``spack mirror`` looks like this:

.. command-output:: spack help mirror

The ``create`` command actually builds a mirror by fetching all of its
packages from the internet and checksumming them.

The other three commands are for managing mirror configuration.  They
control the URL(s) from which Spack downloads its packages.

.. _cmd-spack-mirror-create:

-----------------------
``spack mirror create``
-----------------------

You can create a mirror using the ``spack mirror create`` command, assuming
you're on a machine where you can access the internet.

The command will iterate through all of Spack's packages and download
the safe ones into a directory structure like the one above.  Here is
what it looks like:

.. code-block:: console

   $ spack mirror create libelf libdwarf
   ==> Created new mirror in spack-mirror-2014-06-24
   ==> Trying to fetch from http://www.mr511.de/software/libelf-0.8.13.tar.gz
   ##########################################################                81.6%
   ==> Checksum passed for libelf@0.8.13
   ==> Added libelf@0.8.13
   ==> Trying to fetch from http://www.mr511.de/software/libelf-0.8.12.tar.gz
   ######################################################################    98.6%
   ==> Checksum passed for libelf@0.8.12
   ==> Added libelf@0.8.12
   ==> Trying to fetch from http://www.prevanders.net/libdwarf-20130207.tar.gz
   ######################################################################    97.3%
   ==> Checksum passed for libdwarf@20130207
   ==> Added libdwarf@20130207
   ==> Trying to fetch from http://www.prevanders.net/libdwarf-20130126.tar.gz
   ########################################################                  78.9%
   ==> Checksum passed for libdwarf@20130126
   ==> Added libdwarf@20130126
   ==> Trying to fetch from http://www.prevanders.net/libdwarf-20130729.tar.gz
   #############################################################             84.7%
   ==> Added libdwarf@20130729
   ==> Added spack-mirror-2014-06-24/libdwarf/libdwarf-20130729.tar.gz to mirror
   ==> Added python@2.7.8.
   ==> Successfully updated mirror in spack-mirror-2015-02-24.
     Archive stats:
       0    already present
       5    added
       0    failed to fetch.

Once this is done, you can tar up the ``spack-mirror-2014-06-24`` directory and
copy it over to the machine you want it hosted on.

^^^^^^^^^^^^^^^^^^^
Custom package sets
^^^^^^^^^^^^^^^^^^^

Normally, ``spack mirror create`` downloads all the archives it has
checksums for.  If you want to only create a mirror for a subset of
packages, you can do that by supplying a list of package specs on the
command line after ``spack mirror create``.  For example, this
command:

.. code-block:: console

   $ spack mirror create libelf@0.8.12: boost@1.44:

Will create a mirror for libelf versions greater than or equal to
0.8.12 and boost versions greater than or equal to 1.44.

^^^^^^^^^^^^
Mirror files
^^^^^^^^^^^^

If you have a *very* large number of packages you want to mirror, you
can supply a file with specs in it, one per line:

.. code-block:: console

   $ cat specs.txt
   libdwarf
   libelf@0.8.12:
   boost@1.44:
   boost@1.39.0
   ...
   $ spack mirror create --file specs.txt
   ...

This is useful if there is a specific suite of software managed by
your site.

^^^^^^^^^^^^^^^^^^
Mirror environment
^^^^^^^^^^^^^^^^^^

To create a mirror of all packages required by a concerte environment, activate the environment and call ``spack mirror create -a``.
This is especially useful to create a mirror of an environment concretized on another machine.

.. code-block:: console

   [remote] $ spack env create myenv
   [remote] $ spack env activate myenv
   [remote] $ spack add ...
   [remote] $ spack concretize
   
   $ sftp remote:/spack/var/environment/myenv/spack.lock
   $ spack env create myenv spack.lock
   $ spack env activate myenv
   $ spack mirror create -a
  


.. _cmd-spack-mirror-add:

--------------------
``spack mirror add``
--------------------

Once you have a mirror, you need to let spack know about it.  This is
relatively simple.  First, figure out the URL for the mirror.  If it's
a directory, you can use a file URL like this one:

.. code-block:: none

   file://$HOME/spack-mirror-2014-06-24

That points to the directory on the local filesystem.  If it were on a
web server, you could use a URL like this one:

https://example.com/some/web-hosted/directory/spack-mirror-2014-06-24

Spack will use the URL as the root for all of the packages it fetches.
You can tell your Spack installation to use that mirror like this:

.. code-block:: console

   $ spack mirror add local_filesystem file://$HOME/spack-mirror-2014-06-24

Each mirror has a name so that you can refer to it again later.

.. _cmd-spack-mirror-list:

---------------------
``spack mirror list``
---------------------

To see all the mirrors Spack knows about, run ``spack mirror list``:

.. code-block:: console

   $ spack mirror list
   local_filesystem    file:///home/username/spack-mirror-2014-06-24

.. _cmd-spack-mirror-remove:

-----------------------
``spack mirror remove``
-----------------------

To remove a mirror by name, run:

.. code-block:: console

   $ spack mirror remove local_filesystem
   $ spack mirror list
   ==> No mirrors configured.

-----------------
Mirror precedence
-----------------

Adding a mirror really adds a line in ``~/.spack/mirrors.yaml``:

.. code-block:: yaml

   mirrors:
     local_filesystem: file:///home/username/spack-mirror-2014-06-24
     remote_server: https://example.com/some/web-hosted/directory/spack-mirror-2014-06-24

If you want to change the order in which mirrors are searched for
packages, you can edit this file and reorder the sections.  Spack will
search the topmost mirror first and the bottom-most mirror last.

.. _caching:

-------------------
Local Default Cache
-------------------

Spack caches resources that are downloaded as part of installs. The cache is
a valid spack mirror: it uses the same directory structure and naming scheme
as other Spack mirrors (so it can be copied anywhere and referenced with a URL
like other mirrors). The mirror is maintained locally (within the Spack
installation directory) at :file:`var/spack/cache/`. It is always enabled (and
is always searched first when attempting to retrieve files for an installation)
but can be cleared with :ref:`clean <cmd-spack-clean>`; the cache directory can also
be deleted manually without issue.

Caching includes retrieved tarball archives and source control repositories, but
only resources with an associated digest or commit ID (e.g. a revision number
for SVN) will be cached.
.. Copyright 2013-2022 Lawrence Livermore National Security, LLC and other
   Spack Project Developers. See the top-level COPYRIGHT file for details.

   SPDX-License-Identifier: (Apache-2.0 OR MIT)

.. _packaging-guide:

===============
Packaging Guide
===============

This guide is intended for developers or administrators who want to
package software so that Spack can install it.  It assumes that you
have at least some familiarity with Python, and that you've read the
:ref:`basic usage guide <basic-usage>`, especially the part about
:ref:`specs <sec-specs>`.

There are two key parts of Spack:

#. **Specs**: expressions for describing builds of software, and
#. **Packages**: Python modules that describe how to build
   software according to a spec.

Specs allow a user to describe a *particular* build in a way that a
package author can understand.  Packages allow the packager to
encapsulate the build logic for different versions, compilers,
options, platforms, and dependency combinations in one place.
Essentially, a package translates a spec into build logic.

Packages in Spack are written in pure Python, so you can do anything
in Spack that you can do in Python.  Python was chosen as the
implementation language for two reasons.  First, Python is becoming
ubiquitous in the scientific software community. Second, it's a modern
language and has many powerful features to help make package writing
easy.

---------------------------
Creating & editing packages
---------------------------

.. _cmd-spack-create:

^^^^^^^^^^^^^^^^
``spack create``
^^^^^^^^^^^^^^^^

The ``spack create`` command creates a directory with the package name and
generates a ``package.py`` file with a boilerplate package template. If given
a URL pointing to a tarball or other software archive, ``spack create`` is
smart enough to determine basic information about the package, including its name
and build system. In most cases, ``spack create`` plus a few modifications is
all you need to get a package working.

Here's an example:

.. code-block:: console

   $ spack create https://gmplib.org/download/gmp/gmp-6.1.2.tar.bz2

Spack examines the tarball URL and tries to figure out the name of the package
to be created. If the name contains uppercase letters, these are automatically
converted to lowercase. If the name contains underscores or periods, these are
automatically converted to dashes.

Spack also searches for *additional* versions located in the same directory of
the website. Spack prompts you to tell you how many versions it found and asks
you how many you would like to download and checksum:

.. code-block:: console

   $ spack create https://gmplib.org/download/gmp/gmp-6.1.2.tar.bz2
   ==> This looks like a URL for gmp
   ==> Found 16 versions of gmp:

     6.1.2   https://gmplib.org/download/gmp/gmp-6.1.2.tar.bz2
     6.1.1   https://gmplib.org/download/gmp/gmp-6.1.1.tar.bz2
     6.1.0   https://gmplib.org/download/gmp/gmp-6.1.0.tar.bz2
     ...
     5.0.0   https://gmplib.org/download/gmp/gmp-5.0.0.tar.bz2

   How many would you like to checksum? (default is 1, q to abort)

Spack will automatically download the number of tarballs you specify
(starting with the most recent) and checksum each of them.

You do not *have* to download all of the versions up front. You can
always choose to download just one tarball initially, and run
:ref:`cmd-spack-checksum` later if you need more versions.

Let's say you download 3 tarballs:

.. code-block:: console

	How many would you like to checksum? (default is 1, q to abort) 3
	==> Downloading...
	==> Fetching https://gmplib.org/download/gmp/gmp-6.1.2.tar.bz2
	######################################################################## 100.0%
	==> Fetching https://gmplib.org/download/gmp/gmp-6.1.1.tar.bz2
	######################################################################## 100.0%
	==> Fetching https://gmplib.org/download/gmp/gmp-6.1.0.tar.bz2
	######################################################################## 100.0%
	==> Checksummed 3 versions of gmp:
	==> This package looks like it uses the autotools build system
	==> Created template for gmp package
	==> Created package file: /Users/Adam/spack/var/spack/repos/builtin/packages/gmp/package.py

Spack automatically creates a directory in the appropriate repository,
generates a boilerplate template for your package, and opens up the new
``package.py`` in your favorite ``$EDITOR``:

.. code-block:: python
   :linenos:

   #
   # This is a template package file for Spack.  We've put "FIXME"
   # next to all the things you'll want to change. Once you've handled
   # them, you can save this file and test your package like this:
   #
   #     spack install gmp
   #
   # You can edit this file again by typing:
   #
   #     spack edit gmp
   #
   # See the Spack documentation for more information on packaging.
   # If you submit this package back to Spack as a pull request,
   # please first remove this boilerplate and all FIXME comments.
   #
   from spack import *


   class Gmp(AutotoolsPackage):
       """FIXME: Put a proper description of your package here."""

       # FIXME: Add a proper url for your package's homepage here.
       homepage = "http://www.example.com"
       url      = "https://gmplib.org/download/gmp/gmp-6.1.2.tar.bz2"

       # FIXME: Add a list of GitHub accounts to
       # notify when the package is updated.
       # maintainers = ['github_user1', 'github_user2']

       version('6.1.2', '8ddbb26dc3bd4e2302984debba1406a5')
       version('6.1.1', '4c175f86e11eb32d8bf9872ca3a8e11d')
       version('6.1.0', '86ee6e54ebfc4a90b643a65e402c4048')

       # FIXME: Add dependencies if required.
       # depends_on('foo')

       def configure_args(self):
           # FIXME: Add arguments other than --prefix
           # FIXME: If not needed delete the function
           args = []
           return args

The tedious stuff (creating the class, checksumming archives) has been
done for you. You'll notice that ``spack create`` correctly detected that
``gmp`` uses the Autotools build system. It created a new ``Gmp`` package
that subclasses the ``AutotoolsPackage`` base class. This base class
provides basic installation methods common to all Autotools packages:

.. code-block:: bash

   ./configure --prefix=/path/to/installation/directory

   make
   make check
   make install

For most Autotools packages, this is sufficient. If you need to add
additional arguments to the ``./configure`` call, add them via the
``configure_args`` function.

In the generated package, the download ``url`` attribute is already
set. All the things you still need to change are marked with
``FIXME`` labels. You can delete the commented instructions between
the license and the first import statement after reading them.
The rest of the tasks you need to do are as follows:

#. Add a description.

   Immediately inside the package class is a *docstring* in
   triple-quotes (``"""``).  It is used to generate the description
   shown when users run ``spack info``.

#. Change the ``homepage`` to a useful URL.

   The ``homepage`` is displayed when users run ``spack info`` so
   that they can learn more about your package.

#. Add a comma-separated list of maintainers.

   The ``maintainers`` field is a list of GitHub accounts of people
   who want to be notified any time the package is modified. When a
   pull request is submitted that updates the package, these people
   will be requested to review the PR. This is useful for developers
   who maintain a Spack package for their own software, as well as
   users who rely on a piece of software and want to ensure that the
   package doesn't break. It also gives users a list of people to
   contact for help when someone reports a build error with the package.

#. Add ``depends_on()`` calls for the package's dependencies.

   ``depends_on`` tells Spack that other packages need to be built
   and installed before this one. See :ref:`dependencies`.

#. Get the installation working.

   Your new package may require specific flags during ``configure``.
   These can be added via ``configure_args``. Specifics will differ
   depending on the package and its build system.
   :ref:`Implementing the install method <install-method>` is
   covered in detail later.

Passing a URL to ``spack create`` is a convenient and easy way to get
a basic package template, but what if your software is licensed and
cannot be downloaded from a URL? You can still create a boilerplate
``package.py`` by telling ``spack create`` what name you want to use:

.. code-block:: console

   $ spack create --name intel

This will create a simple ``intel`` package with an ``install()``
method that you can craft to install your package.

What if ``spack create <url>`` guessed the wrong name or build system?
For example, if your package uses the Autotools build system but does
not come with a ``configure`` script, Spack won't realize it uses
Autotools. You can overwrite the old package with ``--force`` and specify
a name with ``--name`` or a build system template to use with ``--template``:

.. code-block:: console

   $ spack create --name gmp https://gmplib.org/download/gmp/gmp-6.1.2.tar.bz2
   $ spack create --force --template autotools https://gmplib.org/download/gmp/gmp-6.1.2.tar.bz2

.. note::

   If you are creating a package that uses the Autotools build system
   but does not come with a ``configure`` script, you'll need to add an
   ``autoreconf`` method to your package that explains how to generate
   the ``configure`` script. You may also need the following dependencies:

   .. code-block:: python

      depends_on('autoconf', type='build')
      depends_on('automake', type='build')
      depends_on('libtool',  type='build')
      depends_on('m4',       type='build')

A complete list of available build system templates can be found by running
``spack create --help``.

.. _cmd-spack-edit:

^^^^^^^^^^^^^^
``spack edit``
^^^^^^^^^^^^^^

One of the easiest ways to learn how to write packages is to look at
existing ones.  You can edit a package file by name with the ``spack
edit`` command:

.. code-block:: console

   $ spack edit gmp

So, if you used ``spack create`` to create a package, then saved and
closed the resulting file, you can get back to it with ``spack edit``.
The ``gmp`` package actually lives in
``$SPACK_ROOT/var/spack/repos/builtin/packages/gmp/package.py``,
but ``spack edit`` provides a much simpler shortcut and saves you the
trouble of typing the full path.

----------------------------
Naming & directory structure
----------------------------

This section describes how packages need to be named, and where they
live in Spack's directory structure.  In general, :ref:`cmd-spack-create`
handles creating package files for you, so you can skip most of the
details here.

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
``var/spack/repos/builtin/packages``
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

A Spack installation directory is structured like a standard UNIX
install prefix (``bin``, ``lib``, ``include``, ``var``, ``opt``,
etc.).  Most of the code for Spack lives in ``$SPACK_ROOT/lib/spack``.
Packages themselves live in ``$SPACK_ROOT/var/spack/repos/builtin/packages``.

If you ``cd`` to that directory, you will see directories for each
package:

.. command-output:: cd $SPACK_ROOT/var/spack/repos/builtin/packages && ls
   :shell:
   :ellipsis: 10

Each directory contains a file called ``package.py``, which is where
all the python code for the package goes.  For example, the ``libelf``
package lives in:

.. code-block:: none

   $SPACK_ROOT/var/spack/repos/builtin/packages/libelf/package.py

Alongside the ``package.py`` file, a package may contain extra
directories or files (like patches) that it needs to build.

^^^^^^^^^^^^^
Package Names
^^^^^^^^^^^^^

Packages are named after the directory containing ``package.py``. So,
``libelf``'s ``package.py`` lives in a directory called ``libelf``.
The ``package.py`` file defines a class called ``Libelf``, which
extends Spack's ``Package`` class.  For example, here is
``$SPACK_ROOT/var/spack/repos/builtin/packages/libelf/package.py``:

.. code-block:: python
   :linenos:

   from spack import *

   class Libelf(Package):
       """ ... description ... """
       homepage = ...
       url = ...
       version(...)
       depends_on(...)

       def install():
           ...

The **directory name** (``libelf``) determines the package name that
users should provide on the command line. e.g., if you type any of
these:

.. code-block:: console

   $ spack info libelf
   $ spack versions libelf
   $ spack install libelf@0.8.13

Spack sees the package name in the spec and looks for
``libelf/package.py`` in ``var/spack/repos/builtin/packages``.
Likewise, if you run ``spack install py-numpy``, Spack looks for
``py-numpy/package.py``.

Spack uses the directory name as the package name in order to give
packagers more freedom in naming their packages. Package names can
contain letters, numbers, and dashes. Using a Python identifier
(e.g., a class name or a module name) would make it difficult to
support these options.  So, you can name a package ``3proxy`` or
``foo-bar`` and Spack won't care. It just needs to see that name
in the packages directory.

^^^^^^^^^^^^^^^^^^^
Package class names
^^^^^^^^^^^^^^^^^^^

Spack loads ``package.py`` files dynamically, and it needs to find a
special class name in the file for the load to succeed.  The **class
name** (``Libelf`` in our example) is formed by converting words
separated by ``-`` in the file name to CamelCase. If the name
starts with a number, we prefix the class name with ``_``. Here are
some examples:

=================  =================
 Module Name         Class Name
=================  =================
 ``foo-bar``         ``FooBar``
 ``3proxy``          ``_3proxy``
=================  =================

In general, you won't have to remember this naming convention because
:ref:`cmd-spack-create` and :ref:`cmd-spack-edit` handle the details for you.

-----------------
Trusted Downloads
-----------------

Spack verifies that the source code it downloads is not corrupted or
compromised; or at least, that it is the same version the author of
the Spack package saw when the package was created.  If Spack uses a
download method it can verify, we say the download method is
*trusted*.  Trust is important for *all downloads*: Spack
has no control over the security of the various sites from which it
downloads source code, and can never assume that any particular site
hasn't been compromised.

Trust is established in different ways for different download methods.
For the most common download method --- a single-file tarball --- the
tarball is checksummed.  Git downloads using ``commit=`` are trusted
implicitly, as long as a hash is specified.

Spack also provides untrusted download methods: tarball URLs may be
supplied without a checksum, or Git downloads may specify a branch or
tag instead of a hash.  If the user does not control or trust the
source of an untrusted download, it is a security risk.  Unless otherwise
specified by the user for special cases, Spack should by default use
*only* trusted download methods.

Unfortunately, Spack does not currently provide that guarantee.  It
does provide the following mechanisms for safety:

#. By default, Spack will only install a tarball package if it has a
   checksum and that checksum matches.  You can override this with
   ``spack install --no-checksum``.

#. Numeric versions are almost always tarball downloads, whereas
   non-numeric versions not named ``develop`` frequently download
   untrusted branches or tags from a version control system.  As long
   as a package has at least one numeric version, and no non-numeric
   version named ``develop``, Spack will prefer it over any
   non-numeric versions.

^^^^^^^^^
Checksums
^^^^^^^^^

For tarball downloads, Spack can currently support checksums using the
MD5, SHA-1, SHA-224, SHA-256, SHA-384, and SHA-512 algorithms.  It
determines the algorithm to use based on the hash length.

.. _versions-and-fetching:

---------------------
Versions and fetching
---------------------

The most straightforward way to add new versions to your package is to
add a line like this in the package class:

.. code-block:: python

   class Foo(Package):

       url = "http://example.com/foo-1.0.tar.gz"

       version('8.2.1', '4136d7b4c04df68b686570afa26988ac')
       version('8.2.0', '1c9f62f0778697a09d36121ead88e08e')
       version('8.1.2', 'd47dd09ed7ae6e7fd6f9a816d7f5fdf6')

Versions should be listed in descending order, from newest to oldest.

^^^^^^^^^^^^^
Date Versions
^^^^^^^^^^^^^

If you wish to use dates as versions, it is best to use the format
``@yyyy-mm-dd``.  This will ensure they sort in the correct order.

Alternately, you might use a hybrid release-version / date scheme.
For example, ``@1.3_2016-08-31`` would mean the version from the
``1.3`` branch, as of August 31, 2016.

^^^^^^^^^^^^
Version URLs
^^^^^^^^^^^^

By default, each version's URL is extrapolated from the ``url`` field
in the package.  For example, Spack is smart enough to download
version ``8.2.1`` of the ``Foo`` package above from
http://example.com/foo-8.2.1.tar.gz.

If the URL is particularly complicated or changes based on the release,
you can override the default URL generation algorithm by defining your
own ``url_for_version()`` function. For example, the download URL for
OpenMPI contains the major.minor version in one spot and the
major.minor.patch version in another:

https://www.open-mpi.org/software/ompi/v2.1/downloads/openmpi-2.1.1.tar.bz2

In order to handle this, you can define a ``url_for_version()`` function
like so:

.. literalinclude:: _spack_root/var/spack/repos/builtin/packages/openmpi/package.py
   :pyobject: Openmpi.url_for_version

With the use of this ``url_for_version()``, Spack knows to download OpenMPI ``2.1.1``
from http://www.open-mpi.org/software/ompi/v2.1/downloads/openmpi-2.1.1.tar.bz2
but download OpenMPI ``1.10.7`` from http://www.open-mpi.org/software/ompi/v1.10/downloads/openmpi-1.10.7.tar.bz2.

You'll notice that OpenMPI's ``url_for_version()`` function makes use of a special
``Version`` function called ``up_to()``. When you call ``version.up_to(2)`` on a
version like ``1.10.0``, it returns ``1.10``. ``version.up_to(1)`` would return
``1``. This can be very useful for packages that place all ``X.Y.*`` versions in
a single directory and then places all ``X.Y.Z`` versions in a sub-directory.

There are a few ``Version`` properties you should be aware of. We generally
prefer numeric versions to be separated by dots for uniformity, but not all
tarballs are named that way. For example, ``icu4c`` separates its major and minor
versions with underscores, like ``icu4c-57_1-src.tgz``. The value ``57_1`` can be
obtained with the use of the ``version.underscored`` property. Note that Python
properties don't need parentheses. There are other separator properties as well:

===================  ======
Property             Result
===================  ======
version.dotted       1.2.3
version.dashed       1-2-3
version.underscored  1_2_3
version.joined       123
===================  ======

.. note::

   Python properties don't need parentheses. ``version.dashed`` is correct.
   ``version.dashed()`` is incorrect.

In addition, these version properties can be combined with ``up_to()``.
For example:

.. code-block:: python

   >>> version = Version('1.2.3')
   >>> version.up_to(2).dashed
   Version('1-2')
   >>> version.underscored.up_to(2)
   Version('1_2')


As you can see, order is not important. Just keep in mind that ``up_to()`` and
the other version properties return ``Version`` objects, not strings.

If a URL cannot be derived systematically, or there is a special URL for one
of its versions, you can add an explicit URL for a particular version:

.. code-block:: python

   version('8.2.1', '4136d7b4c04df68b686570afa26988ac',
           url='http://example.com/foo-8.2.1-special-version.tar.gz')


When you supply a custom URL for a version, Spack uses that URL
*verbatim* and does not perform extrapolation. The order of precedence
of these methods is:

#. package-level ``url``
#. ``url_for_version()``
#. version-specific ``url``

so if your package contains a ``url_for_version()``, it can be overridden
by a version-specific ``url``.

If your package does not contain a package-level ``url`` or ``url_for_version()``,
Spack can determine which URL to download from even if only some of the versions
specify their own ``url``. Spack will use the nearest URL *before* the requested
version. This is useful for packages that have an easy to extrapolate URL, but
keep changing their URL format every few releases. With this method, you only
need to specify the ``url`` when the URL changes.

"""""""""""""""""""""""
Mirrors of the main URL
"""""""""""""""""""""""

Spack supports listing mirrors of the main URL in a package by defining
the ``urls`` attribute:

.. code-block:: python

  class Foo(Package):

    urls = [
        'http://example.com/foo-1.0.tar.gz',
        'http://mirror.com/foo-1.0.tar.gz'
    ]

instead of just a single ``url``. This attribute is a list of possible URLs that
will be tried in order when fetching packages. Notice that either one of ``url``
or ``urls`` can be present in a package, but not both at the same time.

A well-known case of packages that can be fetched from multiple mirrors is that
of GNU. For that, Spack goes a step further and defines a mixin class that
takes care of all of the plumbing and requires packagers to just define a proper
``gnu_mirror_path`` attribute:

.. literalinclude:: _spack_root/var/spack/repos/builtin/packages/autoconf/package.py
   :lines: 9-18

^^^^^^^^^^^^^^^^^^^^^^^^
Skipping the expand step
^^^^^^^^^^^^^^^^^^^^^^^^

Spack normally expands archives (e.g. ``*.tar.gz`` and ``*.zip``) automatically
into a standard stage source directory (``self.stage.source_path``) after
downloading them. If you want to skip this step (e.g., for self-extracting
executables and other custom archive types), you can add ``expand=False`` to a
``version`` directive.

.. code-block:: python

   version('8.2.1', '4136d7b4c04df68b686570afa26988ac',
           url='http://example.com/foo-8.2.1-special-version.sh', expand=False)

When ``expand`` is set to ``False``, Spack sets the current working
directory to the directory containing the downloaded archive before it
calls your ``install`` method.  Within ``install``, the path to the
downloaded archive is available as ``self.stage.archive_file``.

Here is an example snippet for packages distributed as self-extracting
archives.  The example sets permissions on the downloaded file to make
it executable, then runs it with some arguments.

.. code-block:: python

   def install(self, spec, prefix):
       set_executable(self.stage.archive_file)
       installer = Executable(self.stage.archive_file)
       installer('--prefix=%s' % prefix, 'arg1', 'arg2', 'etc.')

.. _deprecate:

^^^^^^^^^^^^^^^^^^^^^^^^
Deprecating old versions
^^^^^^^^^^^^^^^^^^^^^^^^

There are many reasons to remove old versions of software:

#. Security vulnerabilities (most serious reason)
#. Changing build systems that increase package complexity
#. Changing dependencies/patches/resources/flags that increase package complexity
#. Maintainer/developer inability/unwillingness to support old versions
#. No longer available for download (right to be forgotten)
#. Package or version rename

At the same time, there are many reasons to keep old versions of software:

#. Reproducibility
#. Requirements for older packages (e.g. some packages still rely on Qt 3)

In general, you should not remove old versions from a ``package.py``. Instead,
you should first deprecate them using the following syntax:

.. code-block:: python

   version('1.2.3', sha256='...', deprecated=True)


This has two effects. First, ``spack info`` will no longer advertise that
version. Second, commands like ``spack install`` that fetch the package will
require user approval:

.. code-block:: console

   $ spack install openssl@1.0.1e
   ==> Warning: openssl@1.0.1e is deprecated and may be removed in a future Spack release.
   ==>   Fetch anyway? [y/N]


If you use ``spack install --deprecated``, this check can be skipped.

This also applies to package recipes that are renamed or removed. You should
first deprecate all versions before removing a package. If you need to rename
it, you can deprecate the old package and create a new package at the same
time.

Version deprecations should always last at least one Spack minor release cycle
before the version is completely removed. For example, if a version is
deprecated in Spack 0.16.0, it should not be removed until Spack 0.17.0. No
version should be removed without such a deprecation process. This gives users
a chance to complain about the deprecation in case the old version is needed
for some application. If you require a deprecated version of a package, simply
submit a PR to remove ``deprecated=True`` from the package. However, you may be
asked to help maintain this version of the package if the current maintainers
are unwilling to support this older version.


^^^^^^^^^^^^^^^^
Download caching
^^^^^^^^^^^^^^^^

Spack maintains a cache (described :ref:`here <caching>`) which saves files
retrieved during package installations to avoid re-downloading in the case that
a package is installed with a different specification (but the same version) or
reinstalled on account of a change in the hashing scheme. It may (rarely) be
necessary to avoid caching for a particular version by adding ``no_cache=True``
as an option to the ``version()`` directive. Example situations would be a
"snapshot"-like Version Control System (VCS) tag, a VCS branch such as
``v6-16-00-patches``, or a URL specifying a regularly updated snapshot tarball.

^^^^^^^^^^^^^^^^^^
Version comparison
^^^^^^^^^^^^^^^^^^

Most Spack versions are numeric, a tuple of integers; for example,
``apex@0.1``, ``ferret@6.96`` or ``py-netcdf@1.2.3.1``.  Spack knows
how to compare and sort numeric versions.

Some Spack versions involve slight extensions of numeric syntax; for
example, ``py-sphinx-rtd-theme@0.1.10a0``.  In this case, numbers are
always considered to be "newer" than letters.  This is for consistency
with `RPM <https://bugzilla.redhat.com/show_bug.cgi?id=50977>`_.

Spack versions may also be arbitrary non-numeric strings, for example
``@develop``, ``@master``, ``@local``.

The order on versions is defined as follows. A version string is split
into a list of components based on delimiters such as ``.``, ``-`` etc. 
Lists are then ordered lexicographically, where components are ordered
as follows:

#. The following special strings are considered larger than any other
   numeric or non-numeric version component, and satisfy the following
   order between themselves: ``develop > main > master > head > trunk``.

#. Numbers are ordered numerically, are less than special strings, and
   larger than other non-numeric components.

#. All other non-numeric components are less than numeric components,
   and are ordered alphabetically.

The logic behind this sort order is two-fold:

#. Non-numeric versions are usually used for special cases while
   developing or debugging a piece of software.  Keeping most of them
   less than numeric versions ensures that Spack chooses numeric
   versions by default whenever possible.

#. The most-recent development version of a package will usually be
   newer than any released numeric versions.  This allows the
   ``@develop`` version to satisfy dependencies like ``depends_on(abc,
   when="@x.y.z:")``

^^^^^^^^^^^^^^^^^
Version selection
^^^^^^^^^^^^^^^^^

When concretizing, many versions might match a user-supplied spec.
For example, the spec ``python`` matches all available versions of the
package ``python``.  Similarly, ``python@3:`` matches all versions of
Python 3 and above.  Given a set of versions that match a spec, Spack
concretization uses the following priorities to decide which one to
use:

#. If the user provided a list of versions in ``packages.yaml``, the
   first matching version in that list will be used.

#. If one or more versions is specified as ``preferred=True``, in
   either ``packages.yaml`` or ``package.py``, the largest matching
   version will be used.  ("Latest" is defined by the sort order
   above).

#. If no preferences in particular are specified in the package or in
   ``packages.yaml``, then the largest matching non-develop version
   will be used.  By avoiding ``@develop``, this prevents users from
   accidentally installing a ``@develop`` version.

#. If all else fails and ``@develop`` is the only matching version, it
   will be used.


.. _cmd-spack-checksum:

^^^^^^^^^^^^^^^^^^
``spack checksum``
^^^^^^^^^^^^^^^^^^

If you want to add new versions to a package you've already created,
this is automated with the ``spack checksum`` command.  Here's an
example for ``libelf``:

.. code-block:: console

   $ spack checksum libelf
   ==> Found 16 versions of libelf.
     0.8.13    http://www.mr511.de/software/libelf-0.8.13.tar.gz
     0.8.12    http://www.mr511.de/software/libelf-0.8.12.tar.gz
     0.8.11    http://www.mr511.de/software/libelf-0.8.11.tar.gz
     0.8.10    http://www.mr511.de/software/libelf-0.8.10.tar.gz
     0.8.9     http://www.mr511.de/software/libelf-0.8.9.tar.gz
     0.8.8     http://www.mr511.de/software/libelf-0.8.8.tar.gz
     0.8.7     http://www.mr511.de/software/libelf-0.8.7.tar.gz
     0.8.6     http://www.mr511.de/software/libelf-0.8.6.tar.gz
     0.8.5     http://www.mr511.de/software/libelf-0.8.5.tar.gz
     ...
     0.5.2     http://www.mr511.de/software/libelf-0.5.2.tar.gz

   How many would you like to checksum? (default is 1, q to abort)

This does the same thing that ``spack create`` does, but it allows you
to go back and add new versions easily as you need them (e.g., as
they're released).  It fetches the tarballs you ask for and prints out
a list of ``version`` commands ready to copy/paste into your package
file:

.. code-block:: console

   ==> Checksummed new versions of libelf:
       version('0.8.13', '4136d7b4c04df68b686570afa26988ac')
       version('0.8.12', 'e21f8273d9f5f6d43a59878dc274fec7')
       version('0.8.11', 'e931910b6d100f6caa32239849947fbf')
       version('0.8.10', '9db4d36c283d9790d8fa7df1f4d7b4d9')

By default, Spack will search for new tarball downloads by scraping
the parent directory of the tarball you gave it.  So, if your tarball
is at ``http://example.com/downloads/foo-1.0.tar.gz``, Spack will look
in ``http://example.com/downloads/`` for links to additional versions.
If you need to search another path for download links, you can supply
some extra attributes that control how your package finds new
versions. See the documentation on :ref:`attribute_list_url` and
:ref:`attribute_list_depth`.

.. note::

  * This command assumes that Spack can extrapolate new URLs from an
    existing URL in the package, and that Spack can find similar URLs
    on a webpage.  If that's not possible, e.g. if the package's
    developers don't name their tarballs consistently, you'll need to
    manually add ``version`` calls yourself.

  * For ``spack checksum`` to work, Spack needs to be able to
    ``import`` your package in Python.  That means it can't have any
    syntax errors, or the ``import`` will fail.  Use this once you've
    got your package in working order.

--------------------
Finding new versions
--------------------

You've already seen the ``homepage`` and ``url`` package attributes:

.. code-block:: python
   :linenos:

   from spack import *


   class Mpich(Package):
      """MPICH is a high performance and widely portable implementation of
         the Message Passing Interface (MPI) standard."""
      homepage = "http://www.mpich.org"
      url      = "http://www.mpich.org/static/downloads/3.0.4/mpich-3.0.4.tar.gz"

These are class-level attributes used by Spack to show users
information about the package, and to determine where to download its
source code.

Spack uses the tarball URL to extrapolate where to find other tarballs
of the same package (e.g. in :ref:`cmd-spack-checksum`, but
this does not always work.  This section covers ways you can tell
Spack to find tarballs elsewhere.

.. _attribute_list_url:

^^^^^^^^^^^^
``list_url``
^^^^^^^^^^^^

When spack tries to find available versions of packages (e.g. with
:ref:`cmd-spack-checksum`), it spiders the parent directory
of the tarball in the ``url`` attribute.  For example, for libelf, the
url is:

.. code-block:: python

   url = "http://www.mr511.de/software/libelf-0.8.13.tar.gz"

Here, Spack spiders ``http://www.mr511.de/software/`` to find similar
tarball links and ultimately to make a list of available versions of
``libelf``.

For many packages, the tarball's parent directory may be unlistable,
or it may not contain any links to source code archives.  In fact,
many times additional package downloads aren't even available in the
same directory as the download URL.

For these, you can specify a separate ``list_url`` indicating the page
to search for tarballs.  For example, ``libdwarf`` has the homepage as
the ``list_url``, because that is where links to old versions are:

.. code-block:: python
   :linenos:

   class Libdwarf(Package):
       homepage = "http://www.prevanders.net/dwarf.html"
       url      = "http://www.prevanders.net/libdwarf-20130729.tar.gz"
       list_url = homepage

.. _attribute_list_depth:

^^^^^^^^^^^^^^
``list_depth``
^^^^^^^^^^^^^^

``libdwarf`` and many other packages have a listing of available
versions on a single webpage, but not all do.  For example, ``mpich``
has a tarball URL that looks like this:

.. code-block:: python

   url = "http://www.mpich.org/static/downloads/3.0.4/mpich-3.0.4.tar.gz"

But its downloads are in many different subdirectories of
``http://www.mpich.org/static/downloads/``.  So, we need to add a
``list_url`` *and* a ``list_depth`` attribute:

.. code-block:: python
   :linenos:

   class Mpich(Package):
       homepage   = "http://www.mpich.org"
       url        = "http://www.mpich.org/static/downloads/3.0.4/mpich-3.0.4.tar.gz"
       list_url   = "http://www.mpich.org/static/downloads/"
       list_depth = 1

By default, Spack only looks at the top-level page available at
``list_url``.  ``list_depth = 1`` tells it to follow up to 1 level of
links from the top-level page.  Note that here, this implies 1
level of subdirectories, as the ``mpich`` website is structured much
like a filesystem.  But ``list_depth`` really refers to link depth
when spidering the page.

.. _vcs-fetch:

-------------------------------
Fetching from code repositories
-------------------------------

For some packages, source code is provided in a Version Control System
(VCS) repository rather than in a tarball.  Spack can fetch packages
from VCS repositories. Currently, Spack supports fetching with `Git
<git-fetch_>`_, `Mercurial (hg) <hg-fetch_>`_, `Subversion (svn)
<svn-fetch_>`_, `CVS (cvs) <cvs-fetch_>`_, and `Go <go-fetch_>`_.
In all cases, the destination
is the standard stage source path.

To fetch a package from a source repository, Spack needs to know which
VCS to use and where to download from. Much like with ``url``, package
authors can specify a class-level ``git``, ``hg``, ``svn``, ``cvs``, or ``go``
attribute containing the correct download location.

Many packages developed with Git have both a Git repository as well as
release tarballs available for download. Packages can define both a
class-level tarball URL and VCS. For example:

.. code-block:: python

   class Trilinos(CMakePackage):

       homepage = "https://trilinos.org/"
       url      = "https://github.com/trilinos/Trilinos/archive/trilinos-release-12-12-1.tar.gz"
       git      = "https://github.com/trilinos/Trilinos.git"

       version('develop', branch='develop')
       version('master',  branch='master')
       version('12.12.1', 'ecd4606fa332212433c98bf950a69cc7')
       version('12.10.1', '667333dbd7c0f031d47d7c5511fd0810')
       version('12.8.1',  '9f37f683ee2b427b5540db8a20ed6b15')

If a package contains both a ``url`` and ``git`` class-level attribute,
Spack decides which to use based on the arguments to the ``version()``
directive. Versions containing a specific branch, tag, or revision are
assumed to be for VCS download methods, while versions containing a
checksum are assumed to be for URL download methods.

Like ``url``, if a specific version downloads from a different repository
than the default repo, it can be overridden with a version-specific argument.

.. note::

   In order to reduce ambiguity, each package can only have a single VCS
   top-level attribute in addition to ``url``. In the rare case that a
   package uses multiple VCS, a fetch strategy can be specified for each
   version. For example, the ``rockstar`` package contains:

   .. code-block:: python

      class Rockstar(MakefilePackage):

          homepage = "https://bitbucket.org/gfcstanford/rockstar"

          version('develop', git='https://bitbucket.org/gfcstanford/rockstar.git')
          version('yt', hg='https://bitbucket.org/MatthewTurk/rockstar')


.. _git-fetch:

^^^
Git
^^^

Git fetching supports the following parameters to ``version``:

* ``git``: URL of the git repository, if different than the class-level ``git``.
* ``branch``: Name of a branch to fetch.
* ``tag``: Name of a tag to fetch.
* ``commit``: SHA hash (or prefix) of a commit to fetch.
* ``submodules``: Also fetch submodules recursively when checking out this repository.
* ``submodules_delete``: A list of submodules to forcibly delete from the repository
  after fetching. Useful if a version in the repository has submodules that
  have disappeared/are no longer accessible.
* ``get_full_repo``: Ensure the full git history is checked out with all remote
  branch information. Normally (``get_full_repo=False``, the default), the git
  option ``--depth 1`` will be used if the version of git and the specified
  transport protocol support it, and ``--single-branch`` will be used if the
  version of git supports it.

Only one of ``tag``, ``branch``, or ``commit`` can be used at a time.

The destination directory for the clone is the standard stage source path.

Default branch
  To fetch a repository's default branch:

  .. code-block:: python

     class Example(Package):

         git = "https://github.com/example-project/example.git"

         version('develop')

  This download method is untrusted, and is not recommended. Aside from HTTPS,
  there is no way to verify that the repository has not been compromised, and
  the commit you get when you install the package likely won't be the same
  commit that was used when the package was first written. Additionally, the
  default branch may change. It is best to at least specify a branch name.

Branches
  To fetch a particular branch, use the ``branch`` parameter:

  .. code-block:: python

     version('experimental', branch='experimental')

  This download method is untrusted, and is not recommended. Branches are
  moving targets, so the commit you get when you install the package likely
  won't be the same commit that was used when the package was first written.

Tags
  To fetch from a particular tag, use ``tag`` instead:

  .. code-block:: python

     version('1.0.1', tag='v1.0.1')

  This download method is untrusted, and is not recommended. Although tags
  are generally more stable than branches, Git allows tags to be moved.
  Many developers use tags to denote rolling releases, and may move the
  tag when a bug is patched.

Commits
  Finally, to fetch a particular commit, use ``commit``:

  .. code-block:: python

     version('2014-10-08', commit='9d38cd4e2c94c3cea97d0e2924814acc')

  This doesn't have to be a full hash; you can abbreviate it as you'd
  expect with git:

  .. code-block:: python

     version('2014-10-08', commit='9d38cd')

  This download method *is trusted*.  It is the recommended way to
  securely download from a Git repository.

  It may be useful to provide a saner version for commits like this,
  e.g. you might use the date as the version, as done above. Or, if you
  know the commit at which a release was cut, you can use the release
  version. It's up to the package author to decide what makes the most
  sense. Although you can use the commit hash as the version number,
  this is not recommended, as it won't sort properly.

Submodules
  You can supply ``submodules=True`` to cause Spack to fetch submodules
  recursively along with the repository at fetch time. For more information
  about git submodules see the manpage of git: ``man git-submodule``.

  .. code-block:: python

     version('1.0.1', tag='v1.0.1', submodules=True)


.. _github-fetch:

^^^^^^
GitHub
^^^^^^

If a project is hosted on GitHub, *any* valid Git branch, tag, or hash
may be downloaded as a tarball.  This is accomplished simply by
constructing an appropriate URL.  Spack can checksum any package
downloaded this way, thereby producing a trusted download.  For
example, the following downloads a particular hash, and then applies a
checksum.

.. code-block:: python

       version('1.9.5.1.1', 'd035e4bc704d136db79b43ab371b27d2',
               url='https://www.github.com/jswhit/pyproj/tarball/0be612cc9f972e38b50a90c946a9b353e2ab140f')

.. _hg-fetch:

^^^^^^^^^
Mercurial
^^^^^^^^^

Fetching with Mercurial works much like `Git <git-fetch>`_, but you
use the ``hg`` parameter.
The destination directory is still the standard stage source path.

Default branch
  Add the ``hg`` attribute with no ``revision`` passed to ``version``:

  .. code-block:: python

     class Example(Package):

         hg = "https://bitbucket.org/example-project/example"

         version('develop')

  This download method is untrusted, and is not recommended. As with
  Git's default fetching strategy, there is no way to verify the
  integrity of the download.

Revisions
  To fetch a particular revision, use the ``revision`` parameter:

  .. code-block:: python

     version('1.0', revision='v1.0')

  Unlike ``git``, which has special parameters for different types of
  revisions, you can use ``revision`` for branches, tags, and commits
  when you fetch with Mercurial. Like Git, fetching specific branches
  or tags is an untrusted download method, and is not recommended.
  The recommended fetch strategy is to specify a particular commit
  hash as the revision.


.. _svn-fetch:

^^^^^^^^^^
Subversion
^^^^^^^^^^

To fetch with subversion, use the ``svn`` and ``revision`` parameters.
The destination directory will be the standard stage source path.

Fetching the head
  Simply add an ``svn`` parameter to the package:

  .. code-block:: python

     class Example(Package):

         svn = "https://outreach.scidac.gov/svn/example/trunk"

         version('develop')

  This download method is untrusted, and is not recommended for the
  same reasons as mentioned above.

Fetching a revision
  To fetch a particular revision, add a ``revision`` argument to the
  version directive:

  .. code-block:: python

     version('develop', revision=128)

  This download method is untrusted, and is not recommended.

  Unfortunately, Subversion has no commit hashing scheme like Git and
  Mercurial do, so there is no way to guarantee that the download you
  get is the same as the download used when the package was created.
  Use at your own risk.

Subversion branches are handled as part of the directory structure, so
you can check out a branch or tag by changing the URL. If you want to
package multiple branches, simply add a ``svn`` argument to each
version directive.

.. _cvs-fetch:

^^^
CVS
^^^

CVS (Concurrent Versions System) is an old centralized version control
system. It is a predecessor of Subversion.

To fetch with CVS, use the ``cvs``, branch, and ``date`` parameters.
The destination directory will be the standard stage source path.

Fetching the head
  Simply add a ``cvs`` parameter to the package:

  .. code-block:: python

     class Example(Package):

         cvs = ":pserver:outreach.scidac.gov/cvsroot%module=modulename"

         version('1.1.2.4')

  CVS repository locations are described using an older syntax that
  is different from today's ubiquitous URL syntax. ``:pserver:``
  denotes the transport method. CVS servers can host multiple
  repositories (called "modules") at the same location, and one needs
  to specify both the server location and the module name to access.
  Spack combines both into one string using the ``%module=modulename``
  suffix shown above.

  This download method is untrusted.

Fetching a date
  Versions in CVS are commonly specified by date. To fetch a
  particular branch or date, add a ``branch`` and/or ``date`` argument
  to the version directive:

  .. code-block:: python

     version('2021.4.22', branch='branchname', date='2021-04-22')

  Unfortunately, CVS does not identify repository-wide commits via a
  revision or hash like Subversion, Git, or Mercurial do. This makes
  it impossible to specify an exact commit to check out.

CVS has more features, but since CVS is rarely used these days, Spack
does not support all of them.

.. _go-fetch:

^^
Go
^^

Go isn't a VCS, it is a programming language with a builtin command,
`go get <https://golang.org/cmd/go/#hdr-Download_and_install_packages_and_dependencies>`_,
that fetches packages and their dependencies automatically.
The destination directory will be the standard stage source path.

This strategy can clone a Git repository, or download from another source location.
For example:

.. code-block:: python

   class ThePlatinumSearcher(Package):

       homepage = "https://github.com/monochromegane/the_platinum_searcher"
       go       = "github.com/monochromegane/the_platinum_searcher/..."

       version('head')

Go cannot be used to fetch a particular commit or branch, it always
downloads the head of the repository. This download method is untrusted,
and is not recommended. Use another fetch strategy whenever possible.

--------
Variants
--------

Many software packages can be configured to enable optional
features, which often come at the expense of additional dependencies or
longer build times. To be flexible enough and support a wide variety of
use cases, Spack allows you to expose to the end-user the ability to choose
which features should be activated in a package at the time it is installed.
The mechanism to be employed is the :py:func:`spack.directives.variant` directive.

^^^^^^^^^^^^^^^^
Boolean variants
^^^^^^^^^^^^^^^^

In their simplest form variants are boolean options specified at the package
level:

  .. code-block:: python

    class Hdf5(AutotoolsPackage):
        ...
        variant(
            'shared', default=True, description='Builds a shared version of the library'
        )

with a default value and a description of their meaning / use in the package.
*Variants can be tested in any context where a spec constraint is expected.*
In the example above the ``shared`` variant is tied to the build of shared dynamic
libraries. To pass the right option at configure time we can branch depending on
its value:

  .. code-block:: python

    def configure_args(self):
        ...
        if '+shared' in self.spec:
            extra_args.append('--enable-shared')
        else:
            extra_args.append('--disable-shared')
            extra_args.append('--enable-static-exec')

As explained in :ref:`basic-variants` the constraint ``+shared`` means
that the boolean variant is set to ``True``, while ``~shared`` means it is set
to ``False``.
Another common example is the optional activation of an extra dependency
which requires to use the variant in the ``when`` argument of
:py:func:`spack.directives.depends_on`:

  ..  code-block:: python

    class Hdf5(AutotoolsPackage):
        ...
        variant('szip', default=False, description='Enable szip support')
        depends_on('szip', when='+szip')

as shown in the snippet above where ``szip`` is modeled to be an optional
dependency of ``hdf5``.

^^^^^^^^^^^^^^^^^^^^^
Multi-valued variants
^^^^^^^^^^^^^^^^^^^^^

If need be, Spack can go beyond Boolean variants and permit an arbitrary
number of allowed values. This might be useful when modeling
options that are tightly related to each other.
The values in this case are passed to the :py:func:`spack.directives.variant`
directive as a tuple:

  .. code-block:: python

    class Blis(Package):
        ...
        variant(
            'threads', default='none', description='Multithreading support',
            values=('pthreads', 'openmp', 'none'), multi=False
        )

In the example above the argument ``multi`` is set to ``False`` to indicate
that only one among all the variant values can be active at any time. This
constraint is enforced by the parser and an error is emitted if a user
specifies two or more values at the same time:

  .. code-block:: console

    $ spack spec blis threads=openmp,pthreads
    Input spec
    --------------------------------
    blis threads=openmp,pthreads

    Concretized
    --------------------------------
    ==> Error: multiple values are not allowed for variant "threads"

Another useful note is that *Python's* ``None`` *is not allowed as a default value*
and therefore it should not be used to denote that no feature was selected.
Users should instead select another value, like ``'none'``, and handle it explicitly
within the package recipe if need be:

  .. code-block:: python

      if self.spec.variants['threads'].value == 'none':
         options.append('--no-threads')

In cases where multiple values can be selected at the same time ``multi`` should
be set to ``True``:

  .. code-block:: python

    class Gcc(AutotoolsPackage):
        ...
        variant(
            'languages', default='c,c++,fortran',
            values=('ada', 'brig', 'c', 'c++', 'fortran',
                    'go', 'java', 'jit', 'lto', 'objc', 'obj-c++'),
            multi=True,
            description='Compilers and runtime libraries to build'
        )

Within a package recipe a multi-valued variant is tested using a ``key=value`` syntax:

  .. code-block:: python

    if 'languages=jit' in spec:
        options.append('--enable-host-shared')

"""""""""""""""""""""""""""""""""""""""""""
Complex validation logic for variant values
"""""""""""""""""""""""""""""""""""""""""""
To cover complex use cases, the :py:func:`spack.directives.variant` directive
could accept as the ``values`` argument a full-fledged object which has
``default`` and other arguments of the directive embedded as attributes.

An example, already implemented in Spack's core, is :py:class:`spack.variant.DisjointSetsOfValues`.
This class is used to implement a few convenience functions, like
:py:func:`spack.variant.any_combination_of`:

  ..  code-block:: python

    class Adios(AutotoolsPackage):
        ...
        variant(
            'staging',
            values=any_combination_of('flexpath', 'dataspaces'),
            description='Enable dataspaces and/or flexpath staging transports'
        )

that allows any combination of the specified values, and also allows the
user to specify ``'none'`` (as a string) to choose none of them.
The objects returned by these functions can be modified at will by chaining
method calls to change the default value, customize the error message or
other similar operations:

  .. code-block:: python

    class Mvapich2(AutotoolsPackage):
        ...
        variant(
            'process_managers',
            description='List of the process managers to activate',
            values=disjoint_sets(
                ('auto',), ('slurm',), ('hydra', 'gforker', 'remshell')
            ).prohibit_empty_set().with_error(
                "'slurm' or 'auto' cannot be activated along with "
                "other process managers"
            ).with_default('auto').with_non_feature_values('auto'),
        )

^^^^^^^^^^^^^^^^^^^^
Conditional Variants
^^^^^^^^^^^^^^^^^^^^

The variant directive accepts a ``when`` clause. The variant will only
be present on specs that otherwise satisfy the spec listed as the
``when`` clause. For example, the following class has a variant
``bar`` when it is at version 2.0 or higher.

.. code-block:: python

   class Foo(Package):
       ...
       variant('bar', default=False, when='@2.0:', description='help message')

The ``when`` clause follows the same syntax and accepts the same
values as the ``when`` argument of
:py:func:`spack.directives.depends_on`

^^^^^^^^^^^^^^^^^^^
Overriding Variants
^^^^^^^^^^^^^^^^^^^

Packages may override variants for several reasons, most often to
change the default from a variant defined in a parent class or to
change the conditions under which a variant is present on the spec.

When a variant is defined multiple times, whether in the same package
file or in a subclass and a superclass, the last definition is used
for all attributes **except** for the ``when`` clauses. The ``when``
clauses are accumulated through all invocations, and the variant is
present on the spec if any of the accumulated conditions are
satisfied.

For example, consider the following package:

.. code-block:: python

   class Foo(Package):
       ...
       variant('bar', default=False, when='@1.0', description='help1')
       variant('bar', default=True, when='platform=darwin', description='help2')
       ...

This package ``foo`` has a variant ``bar`` when the spec satisfies
either ``@1.0`` or ``platform=darwin``, but not for other platforms at
other versions. The default for this variant, when it is present, is
always ``True``, regardless of which condition of the variant is
satisfied. This allows packages to override variants in packages or
build system classes from which they inherit, by modifying the variant
values without modifying the ``when`` clause. It also allows a package
to implement ``or`` semantics for a variant ``when`` clause by
duplicating the variant definition.

------------------------------------
Resources (expanding extra tarballs)
------------------------------------

Some packages (most notably compilers) provide optional features if additional
resources are expanded within their source tree before building. In Spack it is
possible to describe such a need with the ``resource`` directive :

  .. code-block:: python

     resource(
        name='cargo',
        git='https://github.com/rust-lang/cargo.git',
        tag='0.10.0',
        destination='cargo'
     )

Based on the keywords present among the arguments the appropriate ``FetchStrategy``
will be used for the resource. The keyword ``destination`` is relative to the source
root of the package and should point to where the resource is to be expanded.

.. _license:

-----------------
Licensed software
-----------------

In order to install licensed software, Spack needs to know a few more
details about a package. The following class attributes should be defined.

^^^^^^^^^^^^^^^^^^^^
``license_required``
^^^^^^^^^^^^^^^^^^^^

Boolean. If set to ``True``, this software requires a license. If set to
``False``, all of the following attributes will be ignored. Defaults to
``False``.

^^^^^^^^^^^^^^^^^^^
``license_comment``
^^^^^^^^^^^^^^^^^^^

String. Contains the symbol used by the license manager to denote a comment.
Defaults to ``#``.

^^^^^^^^^^^^^^^^^
``license_files``
^^^^^^^^^^^^^^^^^

List of strings. These are files that the software searches for when
looking for a license. All file paths must be relative to the installation
directory. More complex packages like Intel may require multiple
licenses for individual components. Defaults to the empty list.

^^^^^^^^^^^^^^^^
``license_vars``
^^^^^^^^^^^^^^^^

List of strings. Environment variables that can be set to tell the software
where to look for a license if it is not in the usual location. Defaults
to the empty list.

^^^^^^^^^^^^^^^
``license_url``
^^^^^^^^^^^^^^^

String. A URL pointing to license setup instructions for the software.
Defaults to the empty string.

For example, let's take a look at the package for the PGI compilers.

.. code-block:: python

   # Licensing
   license_required = True
   license_comment  = '#'
   license_files    = ['license.dat']
   license_vars     = ['PGROUPD_LICENSE_FILE', 'LM_LICENSE_FILE']
   license_url      = 'http://www.pgroup.com/doc/pgiinstall.pdf'

As you can see, PGI requires a license. Its license manager, FlexNet, uses
the ``#`` symbol to denote a comment. It expects the license file to be
named ``license.dat`` and to be located directly in the installation prefix.
If you would like the installation file to be located elsewhere, simply set
``PGROUPD_LICENSE_FILE`` or ``LM_LICENSE_FILE`` after installation. For
further instructions on installation and licensing, see the URL provided.

Let's walk through a sample PGI installation to see exactly what Spack is
and isn't capable of. Since PGI does not provide a download URL, it must
be downloaded manually. It can either be added to a mirror or located in
the current directory when ``spack install pgi`` is run. See :ref:`mirrors`
for instructions on setting up a mirror.

After running ``spack install pgi``, the first thing that will happen is
Spack will create a global license file located at
``$SPACK_ROOT/etc/spack/licenses/pgi/license.dat``. It will then open up the
file using the editor set in ``$EDITOR``, or vi if unset. It will look like
this:

.. code-block:: sh

   # A license is required to use pgi.
   #
   # The recommended solution is to store your license key in this global
   # license file. After installation, the following symlink(s) will be
   # added to point to this file (relative to the installation prefix):
   #
   #   license.dat
   #
   # Alternatively, use one of the following environment variable(s):
   #
   #   PGROUPD_LICENSE_FILE
   #   LM_LICENSE_FILE
   #
   # If you choose to store your license in a non-standard location, you may
   # set one of these variable(s) to the full pathname to the license file, or
   # port@host if you store your license keys on a dedicated license server.
   # You will likely want to set this variable in a module file so that it
   # gets loaded every time someone tries to use pgi.
   #
   # For further information on how to acquire a license, please refer to:
   #
   #   http://www.pgroup.com/doc/pgiinstall.pdf
   #
   # You may enter your license below.

You can add your license directly to this file, or tell FlexNet to use a
license stored on a separate license server. Here is an example that
points to a license server called licman1:

.. code-block:: none

   SERVER licman1.mcs.anl.gov 00163eb7fba5 27200
   USE_SERVER

If your package requires the license to install, you can reference the
location of this global license using ``self.global_license_file``.
After installation, symlinks for all of the files given in
``license_files`` will be created, pointing to this global license.
If you install a different version or variant of the package, Spack
will automatically detect and reuse the already existing global license.

If the software you are trying to package doesn't rely on license files,
Spack will print a warning message, letting the user know that they
need to set an environment variable or pointing them to installation
documentation.

.. _patching:

-------
Patches
-------

Depending on the host architecture, package version, known bugs, or
other issues, you may need to patch your software to get it to build
correctly.  Like many other package systems, spack allows you to store
patches alongside your package files and apply them to source code
after it's downloaded.

^^^^^^^^^
``patch``
^^^^^^^^^

You can specify patches in your package file with the ``patch()``
directive.  ``patch`` looks like this:

.. code-block:: python

   class Mvapich2(Package):
       ...
       patch('ad_lustre_rwcontig_open_source.patch', when='@1.9:')

The first argument can be either a URL or a filename.  It specifies a
patch file that should be applied to your source.  If the patch you
supply is a filename, then the patch needs to live within the spack
source tree.  For example, the patch above lives in a directory
structure like this:

.. code-block:: none

   $SPACK_ROOT/var/spack/repos/builtin/packages/
       mvapich2/
           package.py
           ad_lustre_rwcontig_open_source.patch

If you supply a URL instead of a filename, you need to supply a
``sha256`` checksum, like this:

.. code-block:: python

   patch('http://www.nwchem-sw.org/images/Tddft_mxvec20.patch',
         sha256='252c0af58be3d90e5dc5e0d16658434c9efa5d20a5df6c10bf72c2d77f780866')

Spack includes the hashes of patches in its versioning information, so
that the same package with different patches applied will have different
hash identifiers.  To ensure that the hashing scheme is consistent, you
must use a ``sha256`` checksum for the patch.  Patches will be fetched
from their URLs, checked, and applied to your source code.  You can use
the GNU utils ``sha256sum`` or the macOS ``shasum -a 256`` commands to
generate a checksum for a patch file.

Spack can also handle compressed patches.  If you use these, Spack needs
a little more help.  Specifically, it needs *two* checksums: the
``sha256`` of the patch and ``archive_sha256`` for the compressed
archive.  ``archive_sha256`` helps Spack ensure that the downloaded
file is not corrupted or malicious, before running it through a tool like
``tar`` or ``zip``.  The ``sha256`` of the patch is still required so
that it can be included in specs.  Providing it in the package file
ensures that Spack won't have to download and decompress patches it won't
end up using at install time.  Both the archive and patch checksum are
checked when patch archives are downloaded.

.. code-block:: python

   patch('http://www.nwchem-sw.org/images/Tddft_mxvec20.patch.gz',
         sha256='252c0af58be3d90e5dc5e0d16658434c9efa5d20a5df6c10bf72c2d77f780866',
         archive_sha256='4e8092a161ec6c3a1b5253176fcf33ce7ba23ee2ff27c75dbced589dabacd06e')

``patch`` keyword arguments are described below.

""""""""""""""""""""""""""""""
``sha256``, ``archive_sha256``
""""""""""""""""""""""""""""""

Hashes of downloaded patch and compressed archive, respectively.  Only
needed for patches fetched from URLs.

""""""""
``when``
""""""""

If supplied, this is a spec that tells spack when to apply
the patch.  If the installed package spec matches this spec, the
patch will be applied.  In our example above, the patch is applied
when mvapich is at version ``1.9`` or higher.

"""""""""
``level``
"""""""""

This tells spack how to run the ``patch`` command.  By default,
the level is 1 and spack runs ``patch -p 1``.  If level is 2,
spack will run ``patch -p 2``, and so on.

A lot of people are confused by level, so here's a primer.  If you
look in your patch file, you may see something like this:

.. code-block:: diff
   :linenos:

   --- a/src/mpi/romio/adio/ad_lustre/ad_lustre_rwcontig.c 2013-12-10 12:05:44.806417000 -0800
   +++ b/src/mpi/romio/adio/ad_lustre/ad_lustre_rwcontig.c 2013-12-10 11:53:03.295622000 -0800
   @@ -8,7 +8,7 @@
     *   Copyright (C) 2008 Sun Microsystems, Lustre group
     \*/

   -#define _XOPEN_SOURCE 600
   +//#define _XOPEN_SOURCE 600
    #include <stdlib.h>
    #include <malloc.h>
    #include "ad_lustre.h"

Lines 1-2 show paths with synthetic ``a/`` and ``b/`` prefixes.  These
are placeholders for the two ``mvapich2`` source directories that
``diff`` compared when it created the patch file.  This is git's
default behavior when creating patch files, but other programs may
behave differently.

``-p1`` strips off the first level of the prefix in both paths,
allowing the patch to be applied from the root of an expanded mvapich2
archive.  If you set level to ``2``, it would strip off ``src``, and
so on.

It's generally easier to just structure your patch file so that it
applies cleanly with ``-p1``, but if you're using a patch you didn't
create yourself, ``level`` can be handy.

"""""""""""""""
``working_dir``
"""""""""""""""

This tells spack where to run the ``patch`` command.  By default,
the working directory is the source path of the stage (``.``).
However, sometimes patches are made with respect to a subdirectory
and this is where the working directory comes in handy. Internally,
the working directory is given to ``patch`` via the ``-d`` option.
Let's take the example patch from above and assume for some reason,
it can only be downloaded in the following form:

.. code-block:: diff
   :linenos:

   --- a/romio/adio/ad_lustre/ad_lustre_rwcontig.c 2013-12-10 12:05:44.806417000 -0800
   +++ b/romio/adio/ad_lustre/ad_lustre_rwcontig.c 2013-12-10 11:53:03.295622000 -0800
   @@ -8,7 +8,7 @@
     *   Copyright (C) 2008 Sun Microsystems, Lustre group
     \*/

   -#define _XOPEN_SOURCE 600
   +//#define _XOPEN_SOURCE 600
    #include <stdlib.h>
    #include <malloc.h>
    #include "ad_lustre.h"

Hence, the patch needs to applied in the ``src/mpi`` subdirectory, and the
``working_dir='src/mpi'`` option would exactly do that.

^^^^^^^^^^^^^^^^^^^^^
Patch functions
^^^^^^^^^^^^^^^^^^^^^

In addition to supplying patch files, you can write a custom function
to patch a package's source.  For example, the ``py-pyside`` package
contains some custom code for tweaking the way the PySide build
handles ``RPATH``:

.. _pyside-patch:

.. literalinclude:: _spack_root/var/spack/repos/builtin/packages/py-pyside/package.py
   :pyobject: PyPyside.patch
   :linenos:

A ``patch`` function, if present, will be run after patch files are
applied and before ``install()`` is run.

You could put this logic in ``install()``, but putting it in a patch
function gives you some benefits.  First, spack ensures that the
``patch()`` function is run once per code checkout.  That means that
if you run install, hit ctrl-C, and run install again, the code in the
patch function is only run once.  Also, you can tell Spack to run only
the patching part of the build using the :ref:`cmd-spack-patch` command.

.. _patch_dependency_patching:

^^^^^^^^^^^^^^^^^^^
Dependency patching
^^^^^^^^^^^^^^^^^^^

So far we've covered how the ``patch`` directive can be used by a package
to patch *its own* source code. Packages can *also* specify patches to be
applied to their dependencies, if they require special modifications.  As
with all packages in Spack, a patched dependency library can coexist with
other versions of that library.  See the `section on depends_on
<dependency_dependency_patching_>`_ for more details.

.. _patch_inspecting_patches:

^^^^^^^^^^^^^^^^^^^
Inspecting patches
^^^^^^^^^^^^^^^^^^^

If you want to better understand the patches that Spack applies to your
packages, you can do that using ``spack spec``, ``spack find``, and other
query commands.  Let's look at ``m4``.  If you run ``spack spec m4``, you
can see the patches that would be applied to ``m4``::

  $ spack spec m4
  Input spec
  --------------------------------
  m4

  Concretized
  --------------------------------
  m4@1.4.18%apple-clang@9.0.0 patches=3877ab548f88597ab2327a2230ee048d2d07ace1062efe81fc92e91b7f39cd00,c0a408fbffb7255fcc75e26bd8edab116fc81d216bfd18b473668b7739a4158e,fc9b61654a3ba1a8d6cd78ce087e7c96366c290bc8d2c299f09828d793b853c8 +sigsegv arch=darwin-highsierra-x86_64
      ^libsigsegv@2.11%apple-clang@9.0.0 arch=darwin-highsierra-x86_64

You can also see patches that have been applied to installed packages
with ``spack find -v``::

  $ spack find -v m4
  ==> 1 installed package
  -- darwin-highsierra-x86_64 / apple-clang@9.0.0 -----------------
  m4@1.4.18 patches=3877ab548f88597ab2327a2230ee048d2d07ace1062efe81fc92e91b7f39cd00,c0a408fbffb7255fcc75e26bd8edab116fc81d216bfd18b473668b7739a4158e,fc9b61654a3ba1a8d6cd78ce087e7c96366c290bc8d2c299f09828d793b853c8 +sigsegv

.. _cmd-spack-resource:

In both cases above, you can see that the patches' sha256 hashes are
stored on the spec as a variant.  As mentioned above, this means that you
can have multiple, differently-patched versions of a package installed at
once.

You can look up a patch by its sha256 hash (or a short version of it)
using the ``spack resource show`` command::

  $ spack resource show 3877ab54
  3877ab548f88597ab2327a2230ee048d2d07ace1062efe81fc92e91b7f39cd00
      path:       /home/spackuser/src/spack/var/spack/repos/builtin/packages/m4/gnulib-pgi.patch
      applies to: builtin.m4

``spack resource show`` looks up downloadable resources from package
files by hash and prints out information about them.  Above, we see that
the ``3877ab54`` patch applies to the ``m4`` package.  The output also
tells us where to find the patch.

Things get more interesting if you want to know about dependency
patches. For example, when ``dealii`` is built with ``boost@1.68.0``, it
has to patch boost to work correctly.  If you didn't know this, you might
wonder where the extra boost patches are coming from::

  $ spack spec dealii ^boost@1.68.0 ^hdf5+fortran | grep '\^boost'
      ^boost@1.68.0
          ^boost@1.68.0%apple-clang@9.0.0+atomic+chrono~clanglibcpp cxxstd=default +date_time~debug+exception+filesystem+graph~icu+iostreams+locale+log+math~mpi+multithreaded~numpy patches=2ab6c72d03dec6a4ae20220a9dfd5c8c572c5294252155b85c6874d97c323199,b37164268f34f7133cbc9a4066ae98fda08adf51e1172223f6a969909216870f ~pic+program_options~python+random+regex+serialization+shared+signals~singlethreaded+system~taggedlayout+test+thread+timer~versionedlayout+wave arch=darwin-highsierra-x86_64
  $ spack resource show b37164268
  b37164268f34f7133cbc9a4066ae98fda08adf51e1172223f6a969909216870f
      path:       /home/spackuser/src/spack/var/spack/repos/builtin/packages/dealii/boost_1.68.0.patch
      applies to: builtin.boost
      patched by: builtin.dealii

Here you can see that the patch is applied to ``boost`` by ``dealii``,
and that it lives in ``dealii``'s directory in Spack's ``builtin``
package repository.

.. _handling_rpaths:

---------------
Handling RPATHs
---------------

Spack installs each package in a way that ensures that all of its
dependencies are found when it runs.  It does this using `RPATHs
<http://en.wikipedia.org/wiki/Rpath>`_.  An RPATH is a search
path, stored in a binary (an executable or library), that tells the
dynamic loader where to find its dependencies at runtime. You may be
familiar with `LD_LIBRARY_PATH
<http://tldp.org/HOWTO/Program-Library-HOWTO/shared-libraries.html>`_
on Linux or `DYLD_LIBRARY_PATH
<https://developer.apple.com/library/mac/documentation/Darwin/Reference/ManPages/man1/dyld.1.html>`_
on Mac OS X.  RPATH is similar to these paths, in that it tells
the loader where to find libraries.  Unlike them, it is embedded in
the binary and not set in each user's environment.

RPATHs in Spack are handled in one of three ways:

#. For most packages, RPATHs are handled automatically using Spack's
   :ref:`compiler wrappers <compiler-wrappers>`.  These wrappers are
   set in standard variables like ``CC``, ``CXX``, ``F77``, and ``FC``,
   so most build systems (autotools and many gmake systems) pick them
   up and use them.
#. CMake also respects Spack's compiler wrappers, but many CMake
   builds have logic to overwrite RPATHs when binaries are
   installed. Spack provides the ``std_cmake_args`` variable, which
   includes parameters necessary for CMake build use the right
   installation RPATH.  It can be used like this when ``cmake`` is
   invoked:

   .. code-block:: python

      class MyPackage(Package):
          ...
          def install(self, spec, prefix):
              cmake('..', *std_cmake_args)
              make()
              make('install')

#. If you need to modify the build to add your own RPATHs, you can
   use the ``self.rpath`` property of your package, which will
   return a list of all the RPATHs that Spack will use when it
   links.  You can see this how this is used in the :ref:`PySide
   example <pyside-patch>` above.

.. _attribute_parallel:

---------------
Parallel builds
---------------

Spack supports parallel builds on an individual package and at the
installation level.  Package-level parallelism is established by the
``--jobs`` option and its configuration and package recipe equivalents.
Installation-level parallelism is driven by the DAG(s) of the requested
package or packages.

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Package-level build parallelism
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

By default, Spack will invoke ``make()``, or any other similar tool,
with a ``-j <njobs>`` argument, so those builds run in parallel.
The parallelism is determined by the value of the ``build_jobs`` entry
in ``config.yaml`` (see :ref:`here <build-jobs>` for more details on
how this value is computed).

If a package does not build properly in parallel, you can override
this setting by adding ``parallel = False`` to your package.  For
example, OpenSSL's build does not work in parallel, so its package
looks like this:

.. code-block:: python
   :emphasize-lines: 8
   :linenos:

   class Openssl(Package):
       homepage = "http://www.openssl.org"
       url      = "http://www.openssl.org/source/openssl-1.0.1h.tar.gz"

       version('1.0.1h', '8d6d684a9430d5cc98a62a5d8fbda8cf')
       depends_on("zlib")

       parallel = False

Similarly, you can disable parallel builds only for specific make
commands, as ``libdwarf`` does:

.. code-block:: python
   :emphasize-lines: 9, 12
   :linenos:

   class Libelf(Package):
       ...

       def install(self, spec, prefix):
           configure("--prefix=" + prefix,
                     "--enable-shared",
                     "--disable-dependency-tracking",
                     "--disable-debug")
           make()

           # The mkdir commands in libelf's install can fail in parallel
           make("install", parallel=False)

The first make will run in parallel here, but the second will not.  If
you set ``parallel`` to ``False`` at the package level, then each call
to ``make()`` will be sequential by default, but packagers can call
``make(parallel=True)`` to override it.

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Install-level build parallelism
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Spack supports the concurrent installation of packages within a Spack
instance across multiple processes using file system locks.  This
parallelism is separate from the package-level achieved through build
systems' use of the ``-j <njobs>`` option.  With install-level parallelism,
processes coordinate the installation of the dependencies of specs
provided on the command line and as part of an environment build with
only **one process** being allowed to install a given package at a time.
Refer to :ref:`Dependencies` for more information on dependencies and
:ref:`installing-environment` for how to install an environment.

Concurrent processes may be any combination of interactive sessions and
batch jobs.  Which means a ``spack install`` can be running in a terminal
window while a batch job is running ``spack install`` on the same or
overlapping dependencies without any process trying to re-do the work of
another.

For example, if you are using SLURM, you could launch an installation
of ``mpich`` using the following command:

.. code-block:: console

   $ srun -N 2 -n 8 spack install -j 4 mpich@3.3.2

This will create eight concurrent, four-job installs on two different
nodes.

Alternatively, you could run the same installs on one node by entering
the following at the command line of a bash shell:

.. code-block:: console

   $ for i in {1..12}; do nohup spack install -j 4 mpich@3.3.2 >> mpich_install.txt 2>&1 &; done

.. note::

   The effective parallelism is based on the maximum number of packages
   that can be installed at the same time, which is limited by the
   number of packages with no (remaining) uninstalled dependencies.


.. _dependencies:

------------
Dependencies
------------

We've covered how to build a simple package, but what if one package
relies on another package to build?  How do you express that in a
package file?  And how do you refer to the other package in the build
script for your own package?

Spack makes this relatively easy.  Let's take a look at the
``libdwarf`` package to see how it's done:

.. code-block:: python
   :emphasize-lines: 9
   :linenos:

   class Libdwarf(Package):
       homepage = "http://www.prevanders.net/dwarf.html"
       url      = "http://www.prevanders.net/libdwarf-20130729.tar.gz"
       list_url = homepage

       version('20130729', '4cc5e48693f7b93b7aa0261e63c0e21d')
       ...

       depends_on("libelf")

       def install(self, spec, prefix):
           ...

^^^^^^^^^^^^^^^^
``depends_on()``
^^^^^^^^^^^^^^^^

The highlighted ``depends_on('libelf')`` call tells Spack that it
needs to build and install the ``libelf`` package before it builds
``libdwarf``.  This means that in your ``install()`` method, you are
guaranteed that ``libelf`` has been built and installed successfully,
so you can rely on it for your libdwarf build.

^^^^^^^^^^^^^^^^
Dependency specs
^^^^^^^^^^^^^^^^

``depends_on`` doesn't just take the name of another package. It can
take a full spec as well. This means that you can restrict the versions or
other configuration options of ``libelf`` that ``libdwarf`` will build
with. For example, suppose that in the ``libdwarf`` package you write:

.. code-block:: python

   depends_on('libelf@0.8')

Now ``libdwarf`` will require ``libelf`` at *exactly* version ``0.8``.
You can also specify a requirement for a particular variant or for
specific compiler flags:

.. code-block:: python

   depends_on('libelf@0.8+debug')
   depends_on('libelf debug=True')
   depends_on('libelf cppflags="-fPIC"')

Both users *and* package authors can use the same spec syntax to refer
to different package configurations. Users use the spec syntax on the
command line to find installed packages or to install packages with
particular constraints, and package authors can use specs to describe
relationships between packages.

^^^^^^^^^^^^^^
Version ranges
^^^^^^^^^^^^^^

Although some packages require a specific version for their dependencies,
most can be built with a range of versions. For example, if you are
writing a package for a legacy Python module that only works with Python
2.4 through 2.6, this would look like:

.. code-block:: python

   depends_on('python@2.4:2.6')

Version ranges in Spack are *inclusive*, so ``2.4:2.6`` means any version
greater than or equal to ``2.4`` and up to and including any ``2.6.x``. If
you want to specify that a package works with any version of Python 3 (or
higher), this would look like:

.. code-block:: python

   depends_on('python@3:')

Here we leave out the upper bound. If you want to say that a package
requires Python 2, you can similarly leave out the lower bound:

.. code-block:: python

   depends_on('python@:2')

Notice that we didn't use ``@:3``. Version ranges are *inclusive*, so
``@:3`` means "up to and including any 3.x version".

What if a package can only be built with Python 2.7? You might be
inclined to use:

.. code-block:: python

   depends_on('python@2.7')

However, this would be wrong. Spack assumes that all version constraints
are exact, so it would try to install Python not at ``2.7.18``, but
exactly at ``2.7``, which is a non-existent version. The correct way to
specify this would be:

.. code-block:: python

   depends_on('python@2.7.0:2.7')

A spec can contain a version list of ranges and individual versions
separated by commas. For example, if you need Boost 1.59.0 or newer,
but there are known issues with 1.64.0, 1.65.0, and 1.66.0, you can say:

.. code-block:: python

   depends_on('boost@1.59.0:1.63,1.65.1,1.67.0:')


.. _dependency-types:

^^^^^^^^^^^^^^^^
Dependency types
^^^^^^^^^^^^^^^^

Not all dependencies are created equal, and Spack allows you to specify
exactly what kind of a dependency you need. For example:

.. code-block:: python

   depends_on('cmake', type='build')
   depends_on('py-numpy', type=('build', 'run'))
   depends_on('libelf', type=('build', 'link'))
   depends_on('py-pytest', type='test')

The following dependency types are available:

* **"build"**: the dependency will be added to the ``PATH`` and
  ``PYTHONPATH`` at build-time.
* **"link"**: the dependency will be added to Spack's compiler
  wrappers, automatically injecting the appropriate linker flags,
  including ``-I``, ``-L``, and RPATH/RUNPATH handling.
* **"run"**: the dependency will be added to the ``PATH`` and
  ``PYTHONPATH`` at run-time. This is true for both ``spack load``
  and the module files Spack writes.
* **"test"**: the dependency will be added to the ``PATH`` and
  ``PYTHONPATH`` at build-time. The only difference between
  "build" and "test" is that test dependencies are only built
  if the user requests unit tests with ``spack install --test``.

One of the advantages of the ``build`` dependency type is that although the
dependency needs to be installed in order for the package to be built, it
can be uninstalled without concern afterwards. ``link`` and ``run`` disallow
this because uninstalling the dependency would break the package. Another
consequence of this is that ``build``-only dependencies do not affect the
hash of the package. The same is true for ``test`` dependencies.

If the dependency type is not specified, Spack uses a default of
``('build', 'link')``. This is the common case for compiler languages.
Non-compiled packages like Python modules commonly use
``('build', 'run')``. This means that the compiler wrappers don't need to
inject the dependency's ``prefix/lib`` directory, but the package needs to
be in ``PATH`` and ``PYTHONPATH`` during the build process and later when
a user wants to run the package.

^^^^^^^^^^^^^^^^^^^^^^^^
Conditional dependencies
^^^^^^^^^^^^^^^^^^^^^^^^

You may have a package that only requires a dependency under certain
conditions. For example, you may have a package that has optional MPI support,
- MPI is only a dependency when you want to enable MPI support for the
package. In that case, you could say something like:

.. code-block:: python

   variant('mpi', default=False, description='Enable MPI support')

   depends_on('mpi', when='+mpi')

``when`` can include constraints on the variant, version, compiler, etc. and
the :mod:`syntax<spack.spec>` is the same as for Specs written on the command
line.

If a dependency/feature of a package isn't typically used, you can save time
by making it conditional (since Spack will not build the dependency unless it
is required for the Spec).

.. _dependency_dependency_patching:

^^^^^^^^^^^^^^^^^^^
Dependency patching
^^^^^^^^^^^^^^^^^^^

Some packages maintain special patches on their dependencies, either to
add new features or to fix bugs.  This typically makes a package harder
to maintain, and we encourage developers to upstream (contribute back)
their changes rather than maintaining patches.  However, in some cases
it's not possible to upstream. Maybe the dependency's developers don't
accept changes, or maybe they just haven't had time to integrate them.

For times like these, Spack's ``depends_on`` directive can optionally
take a patch or list of patches:

.. code-block:: python

    class SpecialTool(Package):
        ...
        depends_on('binutils', patches='special-binutils-feature.patch')
        ...

Here, the ``special-tool`` package requires a special feature in
``binutils``, so it provides an extra ``patches=<filename>`` keyword
argument.  This is similar to the `patch directive <patching_>`_, with
one small difference.  Here, ``special-tool`` is responsible for the
patch, so it should live in ``special-tool``'s directory in the package
repository, not the ``binutils`` directory.

If you need something more sophisticated than this, you can simply nest a
``patch()`` directive inside of ``depends_on``:

.. code-block:: python

    class SpecialTool(Package):
        ...
        depends_on(
            'binutils',
            patches=patch('special-binutils-feature.patch',
                          level=3,
                          when='@:1.3'),   # condition on binutils
            when='@2.0:')                  # condition on special-tool
        ...

Note that there are two optional ``when`` conditions here -- one on the
``patch`` directive and the other on ``depends_on``.  The condition in
the ``patch`` directive applies to ``binutils`` (the package being
patched), while the condition in ``depends_on`` applies to
``special-tool``.  See `patch directive <patching_>`_ for details on all
the arguments the ``patch`` directive can take.

Finally, if you need *multiple* patches on a dependency, you can provide
a list for ``patches``, e.g.:

.. code-block:: python

    class SpecialTool(Package):
        ...
        depends_on(
            'binutils',
            patches=[
                'binutils-bugfix1.patch',
                'binutils-bugfix2.patch',
                patch('https://example.com/special-binutils-feature.patch',
                      sha256='252c0af58be3d90e5dc5e0d16658434c9efa5d20a5df6c10bf72c2d77f780866',
                      when='@:1.3')],
            when='@2.0:')
        ...

As with ``patch`` directives, patches are applied in the order they
appear in the package file (or in this case, in the list).

.. note::

   You may wonder whether dependency patching will interfere with other
   packages that depend on ``binutils``.  It won't.

   As described in patching_, Patching a package adds the ``sha256`` of
   the patch to the package's spec, which means it will have a
   *different* unique hash than other versions without the patch.  The
   patched version coexists with unpatched versions, and Spack's support
   for handling_rpaths_ guarantees that each installation finds the
   right version. If two packages depend on ``binutils`` patched *the
   same* way, they can both use a single installation of ``binutils``.

.. _setup-dependent-environment:

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Influence how dependents are built or run
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Spack provides a mechanism for dependencies to influence the
environment of their dependents by overriding  the
:meth:`setup_dependent_run_environment <spack.package.PackageBase.setup_dependent_run_environment>`
or the
:meth:`setup_dependent_build_environment <spack.package.PackageBase.setup_dependent_build_environment>`
methods.
The Qt package, for instance, uses this call:

.. literalinclude:: _spack_root/var/spack/repos/builtin/packages/qt/package.py
   :pyobject: Qt.setup_dependent_build_environment
   :linenos:

to set the ``QTDIR`` environment variable so that packages
that depend on a particular Qt installation will find it.
Another good example of how a dependency can influence
the build environment of dependents is the Python package:

.. literalinclude:: _spack_root/var/spack/repos/builtin/packages/python/package.py
   :pyobject: Python.setup_dependent_build_environment
   :linenos:

In the method above it is ensured that any package that depends on Python
will have the ``PYTHONPATH``, ``PYTHONHOME`` and ``PATH`` environment
variables set appropriately before starting the installation. To make things
even simpler the ``python setup.py`` command is also inserted into the module
scope of dependents by overriding a third method called
:meth:`setup_dependent_package <spack.package.PackageBase.setup_dependent_package>`
:

.. literalinclude:: _spack_root/var/spack/repos/builtin/packages/python/package.py
   :pyobject: Python.setup_dependent_package
   :linenos:

This allows most python packages to have a very simple install procedure,
like the following:

.. code-block:: python

   def install(self, spec, prefix):
       setup_py('install', '--prefix={0}'.format(prefix))

Finally the Python package takes also care of the modifications to ``PYTHONPATH``
to allow dependencies to run correctly:

.. literalinclude:: _spack_root/var/spack/repos/builtin/packages/python/package.py
    :pyobject: Python.setup_dependent_run_environment
    :linenos:


.. _packaging_conflicts:

---------
Conflicts
---------

Sometimes packages have known bugs, or limitations, that would prevent them
to build e.g. against other dependencies or with certain compilers. Spack
makes it possible to express such constraints with the ``conflicts`` directive.

Adding the following to a package:

.. code-block:: python

    conflicts('%intel', when='@:1.2',
              msg='<myNicePackage> <= v1.2 cannot be built with Intel ICC, '
                  'please use a newer release.')

we express the fact that the current package *cannot be built* with the Intel
compiler when we are trying to install a version "<=1.2". The ``when`` argument
can be omitted, in which case the conflict will always be active.
Conflicts are always evaluated after the concretization step has been performed,
and if any match is found a detailed error message is shown to the user.
You can add an additional message via the ``msg=`` parameter to a conflict that
provideds more specific instructions for users.

.. _packaging_extensions:

----------
Extensions
----------

Spack's support for package extensions is documented extensively in
:ref:`extensions`.  This section documents how to make your own
extendable packages and extensions.

To support extensions, a package needs to set its ``extendable``
property to ``True``, e.g.:

.. code-block:: python

   class Python(Package):
       ...
       extendable = True
       ...

To make a package into an extension, simply add an
``extends`` call in the package definition, and pass it the name of an
extendable package:

.. code-block:: python

   class PyNumpy(Package):
       ...
       extends('python')
       ...

Now, the ``py-numpy`` package can be used as an argument to ``spack
activate``.  When it is activated, all the files in its prefix will be
symbolically linked into the prefix of the python package.

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Adding additional constraints
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Some packages produce a Python extension, but are only compatible with
Python 3, or with Python 2.  In those cases, a ``depends_on()``
declaration should be made in addition to the ``extends()``
declaration:

.. code-block:: python

   class Icebin(Package):
       extends('python', when='+python')
       depends_on('python@3:', when='+python')

Many packages produce Python extensions for *some* variants, but not
others: they should extend ``python`` only if the appropriate
variant(s) are selected.  This may be accomplished with conditional
``extends()`` declarations:

.. code-block:: python

   class FooLib(Package):
       variant('python', default=True, description='Build the Python extension Module')
       extends('python', when='+python')
       ...

Sometimes, certain files in one package will conflict with those in
another, which means they cannot both be activated (symlinked) at the
same time.  In this case, you can tell Spack to ignore those files
when it does the activation:

.. code-block:: python

   class PySncosmo(Package):
       ...
       # py-sncosmo binaries are duplicates of those from py-astropy
       extends('python', ignore=r'bin/.*')
       depends_on('py-astropy')
       ...

The code above will prevent everything in the ``$prefix/bin/`` directory
from being linked in at activation time.

.. note::

   You can call *either* ``depends_on`` or ``extends`` on any one
   package, but not both.  For example you cannot both
   ``depends_on('python')`` and ``extends(python)`` in the same
   package.  ``extends`` implies ``depends_on``.

-----
Views
-----

As covered in :ref:`filesystem-views`, the ``spack view`` command can be
used to symlink a number of packages into a merged prefix. The methods of
``PackageViewMixin`` can be overridden to customize how packages are added
to views. Generally this can be used to create copies of specific files rather
than symlinking them when symlinking does not work. For example, ``Python``
overrides ``add_files_to_view`` in order to create a copy of the ``python``
binary since the real path of the Python executable is used to detect
extensions; as a consequence python extension packages (those inheriting from
``PythonPackage``) likewise override ``add_files_to_view`` in order to rewrite
shebang lines which point to the Python interpreter.

^^^^^^^^^^^^^^^^^^^^^^^^^
Activation & deactivation
^^^^^^^^^^^^^^^^^^^^^^^^^

Adding an extension to a view is referred to as an activation. If the view is
maintained in the Spack installation prefix of the extendee this is called a
global activation. Activations may involve updating some centralized state
that is maintained by the extendee package, so there can be additional work
for adding extensions compared with non-extension packages.

Spack's ``Package`` class has default ``activate`` and ``deactivate``
implementations that handle symbolically linking extensions' prefixes
into a specified view. Extendable packages can override these methods
to add custom activate/deactivate logic of their own.  For example,
the ``activate`` and ``deactivate`` methods in the Python class handle
symbolic linking of extensions, but they also handle details surrounding
Python's ``.pth`` files, and other aspects of Python packaging.

Spack's extensions mechanism is designed to be extensible, so that
other packages (like Ruby, R, Perl, etc.)  can provide their own
custom extension management logic, as they may not handle modules the
same way that Python does.

Let's look at Python's activate function:

.. literalinclude:: _spack_root/var/spack/repos/builtin/packages/python/package.py
   :pyobject: Python.activate
   :linenos:

This function is called on the *extendee* (Python).  It first calls
``activate`` in the superclass, which handles symlinking the
extension package's prefix into the specified view.  It then does
some special handling of the ``easy-install.pth`` file, part of
Python's setuptools.

Deactivate behaves similarly to activate, but it unlinks files:

.. literalinclude:: _spack_root/var/spack/repos/builtin/packages/python/package.py
   :pyobject: Python.deactivate
   :linenos:

Both of these methods call some custom functions in the Python
package.  See the source for Spack's Python package for details.

^^^^^^^^^^^^^^^^^^^^
Activation arguments
^^^^^^^^^^^^^^^^^^^^

You may have noticed that the ``activate`` function defined above
takes keyword arguments.  These are the keyword arguments from
``extends()``, and they are passed to both activate and deactivate.

This capability allows an extension to customize its own activation by
passing arguments to the extendee.  Extendees can likewise implement
custom ``activate()`` and ``deactivate()`` functions to suit their
needs.

The only keyword argument supported by default is the ``ignore``
argument, which can take a regex, list of regexes, or a predicate to
determine which files *not* to symlink during activation.

.. _virtual-dependencies:

--------------------
Virtual dependencies
--------------------

In some cases, more than one package can satisfy another package's
dependency.  One way this can happen is if a package depends on a
particular *interface*, but there are multiple *implementations* of
the interface, and the package could be built with any of them.  A
*very* common interface in HPC is the `Message Passing Interface (MPI)
<http://www.mcs.anl.gov/research/projects/mpi/>`_, which is used in
many large-scale parallel applications.

MPI has several different implementations (e.g., `MPICH
<http://www.mpich.org>`_, `OpenMPI <http://www.open-mpi.org>`_, and
`MVAPICH <http://mvapich.cse.ohio-state.edu>`_) and scientific
applications can be built with any one of them.  Complicating matters,
MPI does not have a standardized ABI, so a package built with one
implementation cannot simply be relinked with another implementation.
Many package managers handle interfaces like this by requiring many
similar package files, e.g., ``foo``, ``foo-mvapich``, ``foo-mpich``,
but Spack avoids this explosion of package files by providing support
for *virtual dependencies*.

^^^^^^^^^^^^
``provides``
^^^^^^^^^^^^

In Spack, ``mpi`` is handled as a *virtual package*.  A package like
``mpileaks`` can depend on it just like any other package, by
supplying a ``depends_on`` call in the package definition.  For example:

.. code-block:: python
   :linenos:
   :emphasize-lines: 7

   class Mpileaks(Package):
       homepage = "https://github.com/hpc/mpileaks"
       url = "https://github.com/hpc/mpileaks/releases/download/v1.0/mpileaks-1.0.tar.gz"

       version('1.0', '8838c574b39202a57d7c2d68692718aa')

       depends_on("mpi")
       depends_on("adept-utils")
       depends_on("callpath")

Here, ``callpath`` and ``adept-utils`` are concrete packages, but
there is no actual package file for ``mpi``, so we say it is a
*virtual* package.  The syntax of ``depends_on``, is the same for
both.  If we look inside the package file of an MPI implementation,
say MPICH, we'll see something like this:

.. code-block:: python

   class Mpich(Package):
       provides('mpi')
       ...

The ``provides("mpi")`` call tells Spack that the ``mpich`` package
can be used to satisfy the dependency of any package that
``depends_on('mpi')``.

^^^^^^^^^^^^^^^^^^^^
Versioned Interfaces
^^^^^^^^^^^^^^^^^^^^

Just as you can pass a spec to ``depends_on``, so can you pass a spec
to ``provides`` to add constraints.  This allows Spack to support the
notion of *versioned interfaces*.  The MPI standard has gone through
many revisions, each with new functions added, and each revision of
the standard has a version number.  Some packages may require a recent
implementation that supports MPI-3 functions, but some MPI versions may
only provide up to MPI-2.  Others may need MPI 2.1 or higher.  You can
indicate this by adding a version constraint to the spec passed to
``provides``:

.. code-block:: python

   provides("mpi@:2")

Suppose that the above ``provides`` call is in the ``mpich2`` package.
This says that ``mpich2`` provides MPI support *up to* version 2, but
if a package ``depends_on("mpi@3")``, then Spack will *not* build that
package with ``mpich2``.

^^^^^^^^^^^^^^^^^
``provides when``
^^^^^^^^^^^^^^^^^

The same package may provide different versions of an interface
depending on *its* version.  Above, we simplified the ``provides``
call in ``mpich`` to make the explanation easier.  In reality, this is
how ``mpich`` calls ``provides``:

.. code-block:: python

   provides('mpi@:3', when='@3:')
   provides('mpi@:1', when='@1:')

The ``when`` argument to ``provides`` allows you to specify optional
constraints on the *providing* package, or the *provider*.  The
provider only provides the declared virtual spec when *it* matches
the constraints in the when clause.  Here, when ``mpich`` is at
version 3 or higher, it provides MPI up to version 3.  When ``mpich``
is at version 1 or higher, it provides the MPI virtual package at
version 1.

The ``when`` qualifier ensures that Spack selects a suitably high
version of ``mpich`` to satisfy some other package that ``depends_on``
a particular version of MPI.  It will also prevent a user from
building with too low a version of ``mpich``.  For example, suppose
the package ``foo`` declares this:

.. code-block:: python

   class Foo(Package):
       ...
       depends_on('mpi@2')

Suppose a user invokes ``spack install`` like this:

.. code-block:: console

   $ spack install foo ^mpich@1.0

Spack will fail with a constraint violation, because the version of
MPICH requested is too low for the ``mpi`` requirement in ``foo``.

.. _abstract-and-concrete:

-------------------------
Abstract & concrete specs
-------------------------

Now that we've seen how spec constraints can be specified :ref:`on the
command line <sec-specs>` and within package definitions, we can talk
about how Spack puts all of this information together.  When you run
this:

.. code-block:: console

   $ spack install mpileaks ^callpath@1.0+debug ^libelf@0.8.11

Spack parses the command line and builds a spec from the description.
The spec says that ``mpileaks`` should be built with the ``callpath``
library at 1.0 and with the debug option enabled, and with ``libelf``
version 0.8.11.  Spack will also look at the ``depends_on`` calls in
all of these packages, and it will build a spec from that.  The specs
from the command line and the specs built from package descriptions
are then combined, and the constraints are checked against each other
to make sure they're satisfiable.

What we have after this is done is called an *abstract spec*.  An
abstract spec is partially specified.  In other words, it could
describe more than one build of a package.  Spack does this to make
things easier on the user: they should only have to specify as much of
the package spec as they care about.  Here's an example partial spec
DAG, based on the constraints above:

.. code-block:: none

   mpileaks
       ^callpath@1.0+debug
           ^dyninst
               ^libdwarf
                   ^libelf@0.8.11
           ^mpi

.. graphviz::

   digraph {
       mpileaks -> mpi
       mpileaks -> "callpath@1.0+debug" -> mpi
       "callpath@1.0+debug" -> dyninst
       dyninst  -> libdwarf -> "libelf@0.8.11"
       dyninst  -> "libelf@0.8.11"
   }

This diagram shows a spec DAG output as a tree, where successive
levels of indentation represent a depends-on relationship.  In the
above DAG, we can see some packages annotated with their constraints,
and some packages with no annotations at all.  When there are no
annotations, it means the user doesn't care what configuration of that
package is built, just so long as it works.

^^^^^^^^^^^^^^
Concretization
^^^^^^^^^^^^^^

An abstract spec is useful for the user, but you can't install an
abstract spec.  Spack has to take the abstract spec and "fill in" the
remaining unspecified parts in order to install.  This process is
called **concretization**.  Concretization happens in between the time
the user runs ``spack install`` and the time the ``install()`` method
is called.  The concretized version of the spec above might look like
this:

.. code-block:: none

   mpileaks@2.3%gcc@4.7.3 arch=linux-debian7-x86_64
       ^callpath@1.0%gcc@4.7.3+debug arch=linux-debian7-x86_64
           ^dyninst@8.1.2%gcc@4.7.3 arch=linux-debian7-x86_64
               ^libdwarf@20130729%gcc@4.7.3 arch=linux-debian7-x86_64
                   ^libelf@0.8.11%gcc@4.7.3 arch=linux-debian7-x86_64
           ^mpich@3.0.4%gcc@4.7.3 arch=linux-debian7-x86_64

.. graphviz::

   digraph {
       "mpileaks@2.3\n%gcc@4.7.3\n arch=linux-debian7-x86_64" -> "mpich@3.0.4\n%gcc@4.7.3\n arch=linux-debian7-x86_64"
       "mpileaks@2.3\n%gcc@4.7.3\n arch=linux-debian7-x86_64" -> "callpath@1.0\n%gcc@4.7.3+debug\n arch=linux-debian7-x86_64" -> "mpich@3.0.4\n%gcc@4.7.3\n arch=linux-debian7-x86_64"
       "callpath@1.0\n%gcc@4.7.3+debug\n arch=linux-debian7-x86_64" -> "dyninst@8.1.2\n%gcc@4.7.3\n arch=linux-debian7-x86_64"
       "dyninst@8.1.2\n%gcc@4.7.3\n arch=linux-debian7-x86_64" -> "libdwarf@20130729\n%gcc@4.7.3\n arch=linux-debian7-x86_64" -> "libelf@0.8.11\n%gcc@4.7.3\n arch=linux-debian7-x86_64"
       "dyninst@8.1.2\n%gcc@4.7.3\n arch=linux-debian7-x86_64" -> "libelf@0.8.11\n%gcc@4.7.3\n arch=linux-debian7-x86_64"
   }

Here, all versions, compilers, and platforms are filled in, and there
is a single version (no version ranges) for each package.  All
decisions about configuration have been made, and only after this
point will Spack call the ``install()`` method for your package.

Concretization in Spack is based on certain selection policies that
tell Spack how to select, e.g., a version, when one is not specified
explicitly.  Concretization policies are discussed in more detail in
:ref:`configuration`.  Sites using Spack can customize them to match
the preferences of their own users.

.. _cmd-spack-spec:

^^^^^^^^^^^^^^
``spack spec``
^^^^^^^^^^^^^^

For an arbitrary spec, you can see the result of concretization by
running ``spack spec``.  For example:

.. code-block:: console

   $ spack spec dyninst@8.0.1
   dyninst@8.0.1
       ^libdwarf
           ^libelf

   dyninst@8.0.1%gcc@4.7.3 arch=linux-debian7-x86_64
       ^libdwarf@20130729%gcc@4.7.3 arch=linux-debian7-x86_64
           ^libelf@0.8.13%gcc@4.7.3 arch=linux-debian7-x86_64

This is useful when you want to know exactly what Spack will do when
you ask for a particular spec.

.. _concretization-policies:

^^^^^^^^^^^^^^^^^^^^^^^^^^^
``Concretization Policies``
^^^^^^^^^^^^^^^^^^^^^^^^^^^

A user may have certain preferences for how packages should
be concretized on their system.  For example, one user may prefer packages
built with OpenMPI and the Intel compiler.  Another user may prefer
packages be built with MVAPICH and GCC.

See the :ref:`concretization-preferences` section for more details.


.. _group_when_spec:

----------------------------
Common ``when=`` constraints
----------------------------

In case a package needs many directives to share the whole ``when=``
argument, or just part of it, Spack allows you to group the common part
under a context manager:

.. code-block:: python

   class Gcc(AutotoolsPackage):

       with when('+nvptx'):
           depends_on('cuda')
           conflicts('@:6', msg='NVPTX only supported in gcc 7 and above')
           conflicts('languages=ada')
           conflicts('languages=brig')
           conflicts('languages=go')

The snippet above is equivalent to the more verbose:

.. code-block:: python

   class Gcc(AutotoolsPackage):

       depends_on('cuda', when='+nvptx')
       conflicts('@:6', when='+nvptx', msg='NVPTX only supported in gcc 7 and above')
       conflicts('languages=ada', when='+nvptx')
       conflicts('languages=brig', when='+nvptx')
       conflicts('languages=go', when='+nvptx')

Constraints stemming from the context are added to what is explicitly present in the
``when=`` argument of a directive, so:

.. code-block:: python

   with when('+elpa'):
       depends_on('elpa+openmp', when='+openmp')

is equivalent to:

.. code-block:: python

   depends_on('elpa+openmp', when='+openmp+elpa')

Constraints from nested context managers are also combined together, but they are rarely
needed or recommended.

.. _install-method:

------------------
Conflicting Specs
------------------

Suppose a user needs to install package C, which depends on packages A
and B.  Package A builds a library with a Python2 extension, and
package B builds a library with a Python3 extension.  Packages A and B
cannot be loaded together in the same Python runtime:

.. code-block:: python

    class A(Package):
        variant('python', default=True, 'enable python bindings')
        depends_on('python@2.7', when='+python')
        def install(self, spec, prefix):
            # do whatever is necessary to enable/disable python
            # bindings according to variant

    class B(Package):
        variant('python', default=True, 'enable python bindings')
        depends_on('python@3.2:', when='+python')
        def install(self, spec, prefix):
            # do whatever is necessary to enable/disable python
            # bindings according to variant

Package C needs to use the libraries from packages A and B, but does
not need either of the Python extensions.  In this case, package C
should simply depend on the ``~python`` variant of A and B:

.. code-block:: python

    class C(Package):
        depends_on('A~python')
        depends_on('B~python')

This may require that A or B be built twice, if the user wishes to use
the Python extensions provided by them: once for ``+python`` and once
for ``~python``.  Other than using a little extra disk space, that
solution has no serious problems.

.. _installation_procedure:

---------------------------------------
Implementing the installation procedure
---------------------------------------

The last element of a package is its **installation procedure**.  This is
where the real work of installation happens, and it's the main part of
the package you'll need to customize for each piece of software.

Defining an installation procedure means overriding a set of methods or attributes
that will be called at some point during the installation of the package.
The package base class, usually specialized for a given build system, determines the
actual set of entities available for overriding.
The classes that are currently provided by Spack are:

+----------------------------------------------------------+----------------------------------+
|     **Base Class**                                       |           **Purpose**            |
+==========================================================+==================================+
| :class:`~spack.package.Package`                          | General base class not           |
|                                                          | specialized for any build system |
+----------------------------------------------------------+----------------------------------+
| :class:`~spack.build_systems.makefile.MakefilePackage`   | Specialized class for packages   |
|                                                          | built invoking                   |
|                                                          | hand-written Makefiles           |
+----------------------------------------------------------+----------------------------------+
| :class:`~spack.build_systems.autotools.AutotoolsPackage` | Specialized class for packages   |
|                                                          | built using GNU Autotools        |
+----------------------------------------------------------+----------------------------------+
| :class:`~spack.build_systems.cmake.CMakePackage`         | Specialized class for packages   |
|                                                          | built using CMake                |
+----------------------------------------------------------+----------------------------------+
| :class:`~spack.build_systems.cuda.CudaPackage`           | A helper class for packages that |
|                                                          | use CUDA                         |
+----------------------------------------------------------+----------------------------------+
| :class:`~spack.build_systems.qmake.QMakePackage`         | Specialized class for packages   |
|                                                          | built using QMake                |
+----------------------------------------------------------+----------------------------------+
| :class:`~spack.build_systems.rocm.ROCmPackage`           | A helper class for packages that |
|                                                          | use ROCm                         |
+----------------------------------------------------------+----------------------------------+
| :class:`~spack.build_systems.scons.SConsPackage`         | Specialized class for packages   |
|                                                          | built using SCons                |
+----------------------------------------------------------+----------------------------------+
| :class:`~spack.build_systems.waf.WafPackage`             | Specialized class for packages   |
|                                                          | built using Waf                  |
+----------------------------------------------------------+----------------------------------+
| :class:`~spack.build_systems.r.RPackage`                 | Specialized class for            |
|                                                          | R extensions                     |
+----------------------------------------------------------+----------------------------------+
| :class:`~spack.build_systems.octave.OctavePackage`       | Specialized class for            |
|                                                          | Octave packages                  |
+----------------------------------------------------------+----------------------------------+
| :class:`~spack.build_systems.python.PythonPackage`       | Specialized class for            |
|                                                          | Python extensions                |
+----------------------------------------------------------+----------------------------------+
| :class:`~spack.build_systems.perl.PerlPackage`           | Specialized class for            |
|                                                          | Perl extensions                  |
+----------------------------------------------------------+----------------------------------+
| :class:`~spack.build_systems.intel.IntelPackage`         | Specialized class for licensed   |
|                                                          | Intel software                   |
+----------------------------------------------------------+----------------------------------+


.. note::
    Choice of the appropriate base class for a package
        In most cases packagers don't have to worry about the selection of the right base class
        for a package, as ``spack create`` will make the appropriate choice on their behalf. In those
        rare cases where manual intervention is needed we need to stress that a
        package base class depends on the *build system* being used, not the language of the package.
        For example, a Python extension installed with CMake would ``extends('python')`` and
        subclass from :class:`~spack.build_systems.cmake.CMakePackage`.

^^^^^^^^^^^^^^^^^^^^^
Installation pipeline
^^^^^^^^^^^^^^^^^^^^^

When a user runs ``spack install``, Spack:

1. Fetches an archive for the correct version of the software.
2. Expands the archive.
3. Sets the current working directory to the root directory of the expanded archive.

Then, depending on the base class of the package under consideration, it will execute
a certain number of **phases** that reflect the way a package of that type is usually built.
The name and order in which the phases will be executed can be obtained either reading the API
docs at :py:mod:`~.spack.build_systems`, or using the ``spack info`` command:

.. code-block:: console
    :emphasize-lines: 13,14

    $ spack info m4
    AutotoolsPackage:    m4
    Homepage:            https://www.gnu.org/software/m4/m4.html

    Safe versions:
        1.4.17    ftp://ftp.gnu.org/gnu/m4/m4-1.4.17.tar.gz

    Variants:
        Name       Default   Description

        sigsegv    on        Build the libsigsegv dependency

    Installation Phases:
        autoreconf    configure    build    install

    Build Dependencies:
        libsigsegv

    ...


Typically, phases have default implementations that fit most of the common cases:

.. literalinclude:: _spack_root/lib/spack/spack/build_systems/autotools.py
    :pyobject: AutotoolsPackage.configure
    :linenos:

It is thus just sufficient for a packager to override a few
build system specific helper methods or attributes to provide, for instance,
configure arguments:

.. literalinclude::  _spack_root/var/spack/repos/builtin/packages/m4/package.py
    :pyobject: M4.configure_args
    :linenos:

.. note::
    Each specific build system has a list of attributes that can be overridden to
    fine-tune the installation of a package without overriding an entire phase. To
    have more information on them the place to go is the API docs of the :py:mod:`~.spack.build_systems`
    module.

^^^^^^^^^^^^^^^^^^^^^^^^^^
Overriding an entire phase
^^^^^^^^^^^^^^^^^^^^^^^^^^

In extreme cases it may be necessary to override an entire phase. Regardless
of the build system, the signature is the same. For example, the signature
for the install phase is:

.. code-block:: python

   class Foo(Package):
       def install(self, spec, prefix):
           ...

``self``
    For those not used to Python instance methods, this is the
    package itself.  In this case it's an instance of ``Foo``, which
    extends ``Package``.  For API docs on Package objects, see
    :py:class:`Package <spack.package.Package>`.

``spec``
    This is the concrete spec object created by Spack from an
    abstract spec supplied by the user.  It describes what should be
    installed.  It will be of type :py:class:`Spec <spack.spec.Spec>`.

``prefix``
    This is the path that your install method should copy build
    targets into.  It acts like a string, but it's actually its own
    special type, :py:class:`Prefix <spack.util.prefix.Prefix>`.

The arguments ``spec`` and ``prefix`` are passed only for convenience, as they always
correspond to ``self.spec`` and ``self.spec.prefix`` respectively.

As mentioned in :ref:`install-environment`, you will usually not need to refer
to dependencies explicitly in your package file, as the compiler wrappers take care of most of
the heavy lifting here.  There will be times, though, when you need to refer to
the install locations of dependencies, or when you need to do something different
depending on the version, compiler, dependencies, etc. that your package is
built with.  These parameters give you access to this type of information.

.. _install-environment:

-----------------------
The build environment
-----------------------

In general, you should not have to do much differently in your install
method than you would when installing a package on the command line.
In fact, you may need to do *less* than you would on the command line.

Spack tries to set environment variables and modify compiler calls so
that it *appears* to the build system that you're building with a
standard system install of everything.  Obviously that's not going to
cover *all* build systems, but it should make it easy to port packages
to Spack if they use a standard build system.  Usually with autotools
or cmake, building and installing is easy.  With builds that use
custom Makefiles, you may need to add logic to modify the makefiles.

The remainder of the section covers the way Spack's build environment
works.

^^^^^^^^^^^^^^^^^^^^^
Forking ``install()``
^^^^^^^^^^^^^^^^^^^^^

To give packagers free reign over their install environment, Spack forks
a new process each time it invokes a package's ``install()`` method.
This allows packages to have a sandboxed build environment, without
impacting the environments ofother jobs that the main Spack process runs.
Packages are free to change the environment or to modify Spack internals,
because each ``install()`` call has its own dedicated process.

^^^^^^^^^^^^^^^^^^^^^
Environment variables
^^^^^^^^^^^^^^^^^^^^^

Spack sets a number of standard environment variables that serve two
purposes:

#. Make build systems use Spack's compiler wrappers for their builds.
#. Allow build systems to find dependencies more easily

The Compiler environment variables that Spack sets are:

  ============  ===============================
    Variable     Purpose
  ============  ===============================
    ``CC``       C compiler
    ``CXX``      C++ compiler
    ``F77``      Fortran 77 compiler
    ``FC``       Fortran 90 and above compiler
  ============  ===============================

Spack sets these variables so that they point to *compiler
wrappers*. These are covered in :ref:`their own section
<compiler-wrappers>` below.

All of these are standard variables respected by most build systems.
If your project uses ``Autotools`` or ``CMake``, then it should pick
them up automatically when you run ``configure`` or ``cmake`` in the
``install()`` function.  Many traditional builds using GNU Make and
BSD make also respect these variables, so they may work with these
systems.

If your build system does *not* automatically pick these variables up
from the environment, then you can simply pass them on the command
line or use a patch as part of your build process to get the correct
compilers into the project's build system.  There are also some file
editing commands you can use -- these are described later in the
`section on file manipulation <file-manipulation_>`_.

In addition to the compiler variables, these variables are set before
entering ``install()`` so that packages can locate dependencies
easily:

=====================  ====================================================
``PATH``               Set to point to ``/bin`` directories of dependencies
``CMAKE_PREFIX_PATH``  Path to dependency prefixes for CMake
``PKG_CONFIG_PATH``    Path to any pkgconfig directories for dependencies
``PYTHONPATH``         Path to site-packages dir of any python dependencies
=====================  ====================================================

``PATH`` is set up to point to dependencies ``/bin`` directories so
that you can use tools installed by dependency packages at build time.
For example, ``$MPICH_ROOT/bin/mpicc`` is frequently used by dependencies of
``mpich``.

``CMAKE_PREFIX_PATH`` contains a colon-separated list of prefixes
where ``cmake`` will search for dependency libraries and headers.
This causes all standard CMake find commands to look in the paths of
your dependencies, so you *do not* have to manually specify arguments
like ``-DDEPENDENCY_DIR=/path/to/dependency`` to ``cmake``.  More on
this is `in the CMake documentation <http://www.cmake.org/cmake/help/v3.0/variable/CMAKE_PREFIX_PATH.html>`_.

``PKG_CONFIG_PATH`` is for packages that attempt to discover
dependencies using the GNU ``pkg-config`` tool.  It is similar to
``CMAKE_PREFIX_PATH`` in that it allows a build to automatically
discover its dependencies.

If you want to see the environment that a package will build with, or
if you want to run commands in that environment to test them out, you
can use the :ref:`cmd-spack-build-env` command, documented
below.

^^^^^^^^^^^^^^^^^^^^^
Failing the build
^^^^^^^^^^^^^^^^^^^^^

Sometimes you don't want a package to successfully install unless some
condition is true.  You can explicitly cause the build to fail from
``install()`` by raising an ``InstallError``, for example:

.. code-block:: python

   if spec.architecture.startswith('darwin'):
       raise InstallError('This package does not build on Mac OS X!')

.. _shell-wrappers:

^^^^^^^^^^^^^^^^^^^^^^^
Shell command functions
^^^^^^^^^^^^^^^^^^^^^^^

Recall the install method from ``libelf``:

.. literalinclude::  _spack_root/var/spack/repos/builtin/packages/libelf/package.py
   :pyobject: Libelf.install
   :linenos:

Normally in Python, you'd have to write something like this in order
to execute shell commands:

.. code-block:: python

   import subprocess
   subprocess.check_call('configure', '--prefix={0}'.format(prefix))

We've tried to make this a bit easier by providing callable wrapper
objects for some shell commands.  By default, ``configure``,
``cmake``, and ``make`` wrappers are are provided, so you can call
them more naturally in your package files.

If you need other commands, you can use ``which`` to get them:

.. code-block:: python

   sed = which('sed')
   sed('s/foo/bar/', filename)

The ``which`` function will search the ``PATH`` for the application.

Callable wrappers also allow spack to provide some special features.
For example, in Spack, ``make`` is parallel by default, and Spack
figures out the number of cores on your machine and passes an
appropriate value for ``-j<numjobs>`` when it calls ``make`` (see the
``parallel`` `package attribute <attribute_parallel>`).  In
a package file, you can supply a keyword argument, ``parallel=False``,
to the ``make`` wrapper to disable parallel make.  In the ``libelf``
package, this allows us to avoid race conditions in the library's
build system.

^^^^^^^^^^^^^^
Compiler flags
^^^^^^^^^^^^^^

Compiler flags set by the user through the Spec object can be passed
to the build in one of three ways. By default, the build environment
injects these flags directly into the compiler commands using Spack's
compiler wrappers. In cases where the build system requires knowledge
of the compiler flags, they can be registered with the build system by
alternatively passing them through environment variables or as build
system arguments. The flag_handler method can be used to change this
behavior.

Packages can override the flag_handler method with one of three
built-in flag_handlers. The built-in flag_handlers are named
``inject_flags``, ``env_flags``, and ``build_system_flags``. The
``inject_flags`` method is the default. The ``env_flags`` method puts
all of the flags into the environment variables that ``make`` uses as
implicit variables ('CFLAGS', 'CXXFLAGS', etc.). The
``build_system_flags`` method adds the flags as
arguments to the invocation of ``configure`` or ``cmake``,
respectively.

.. warning::

   Passing compiler flags using build system arguments is only
   supported for CMake and Autotools packages. Individual packages may
   also differ in whether they properly respect these arguments.

Individual packages may also define their own ``flag_handler``
methods. The ``flag_handler`` method takes the package instance
(``self``), the name of the flag, and a list of the values of the
flag. It will be called on each of the six compiler flags supported in
Spack. It should return a triple of ``(injf, envf, bsf)`` where
``injf`` is a list of flags to inject via the Spack compiler wrappers,
``envf`` is a list of flags to set in the appropriate environment
variables, and ``bsf`` is a list of flags to pass to the build system
as arguments.

.. warning::

   Passing a non-empty list of flags to ``bsf`` for a build system
   that does not support build system arguments will result in an
   error.

Here are the definitions of the three built-in flag handlers:

.. code-block:: python

   def inject_flags(pkg, name, flags):
       return (flags, None, None)

   def env_flags(pkg, name, flags):
       return (None, flags, None)

   def build_system_flags(pkg, name, flags):
       return (None, None, flags)

.. note::

   Returning ``[]`` and ``None`` are equivalent in a ``flag_handler``
   method.

Packages can override the default behavior either by specifying one of
the built-in flag handlers,

.. code-block:: python

   flag_handler = env_flags

or by implementing the flag_handler method. Suppose for a package
``Foo`` we need to pass ``cflags``, ``cxxflags``, and ``cppflags``
through the environment, the rest of the flags through compiler
wrapper injection, and we need to add ``-lbar`` to ``ldlibs``. The
following flag handler method accomplishes that.

.. code-block:: python

   def flag_handler(self, name, flags):
       if name in ['cflags', 'cxxflags', 'cppflags']:
           return (None, flags, None)
       elif name == 'ldlibs':
           flags.append('-lbar')
       return (flags, None, None)

Because these methods can pass values through environment variables,
it is important not to override these variables unnecessarily
(E.g. setting ``env['CFLAGS']``) in other package methods when using
non-default flag handlers. In the ``setup_environment`` and
``setup_dependent_environment`` methods, use the ``append_flags``
method of the ``EnvironmentModifications`` class to append values to a
list of flags whenever the flag handler is ``env_flags``. If the
package passes flags through the environment or the build system
manually (in the install method, for example), we recommend using the
default flag handler, or removing manual references and implementing a
custom flag handler method that adds the desired flags to export as
environment variables or pass to the build system. Manual flag passing
is likely to interfere with the ``env_flags`` and
``build_system_flags`` methods.

In rare circumstances such as compiling and running small unit tests, a
package developer may need to know what are the appropriate compiler
flags to enable features like ``OpenMP``, ``c++11``, ``c++14`` and
alike. To that end the compiler classes in ``spack`` implement the
following **properties**: ``openmp_flag``, ``cxx98_flag``, ``cxx11_flag``,
``cxx14_flag``, and ``cxx17_flag``, which can be accessed in a package by
``self.compiler.cxx11_flag`` and alike. Note that the implementation is
such that if a given compiler version does not support this feature, an
error will be produced. Therefore package developers can also use these
properties to assert that a compiler supports the requested feature. This
is handy when a package supports additional variants like

.. code-block:: python

   variant('openmp', default=True, description="Enable OpenMP support.")

.. _blas_lapack_scalapack:

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Blas, Lapack and ScaLapack libraries
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Multiple packages provide implementations of ``Blas``, ``Lapack`` and ``ScaLapack``
routines.  The names of the resulting static and/or shared libraries
differ from package to package. In order to make the ``install()`` method
independent of the choice of ``Blas`` implementation, each package which
provides it implements ``@property def blas_libs(self):`` to return an object
of
`LibraryList <https://spack.readthedocs.io/en/latest/llnl.util.html#llnl.util.filesystem.LibraryList>`_
type which simplifies usage of a set of libraries.
The same applies to packages which provide ``Lapack`` and ``ScaLapack``.
Package developers are requested to use this interface. Common usage cases are:

1. Space separated list of full paths

.. code-block:: python

   lapack_blas = spec['lapack'].libs + spec['blas'].libs
   options.append(
      '--with-blas-lapack-lib={0}'.format(lapack_blas.joined())
   )

2. Names of libraries and directories which contain them

.. code-block:: python

   blas = spec['blas'].libs
   options.extend([
     '-DBLAS_LIBRARY_NAMES={0}'.format(';'.join(blas.names)),
     '-DBLAS_LIBRARY_DIRS={0}'.format(';'.join(blas.directories))
   ])

3. Search and link flags

.. code-block:: python

   math_libs = spec['scalapack'].libs + spec['lapack'].libs + spec['blas'].libs
   options.append(
     '-DMATH_LIBS:STRING={0}'.format(math_libs.ld_flags)
   )


For more information, see documentation of
`LibraryList <https://spack.readthedocs.io/en/latest/llnl.util.html#llnl.util.filesystem.LibraryList>`_
class.


.. _prefix-objects:

^^^^^^^^^^^^^^^^^^^^^
Prefix objects
^^^^^^^^^^^^^^^^^^^^^

Spack passes the ``prefix`` parameter to the install method so that
you can pass it to ``configure``, ``cmake``, or some other installer,
e.g.:

.. code-block:: python

   configure('--prefix={0}'.format(prefix))

For the most part, prefix objects behave exactly like strings.  For
packages that do not have their own install target, or for those that
implement it poorly (like ``libdwarf``), you may need to manually copy
things into particular directories under the prefix.  For this, you
can refer to standard subdirectories without having to construct paths
yourself, e.g.:

.. code-block:: python

   def install(self, spec, prefix):
       mkdirp(prefix.bin)
       install('foo-tool', prefix.bin)

       mkdirp(prefix.include)
       install('foo.h', prefix.include)

       mkdirp(prefix.lib)
       install('libfoo.a', prefix.lib)


Attributes of this object are created on the fly when you request them,
so any of the following will work:

======================  =======================
Prefix Attribute        Location
======================  =======================
``prefix.bin``          ``$prefix/bin``
``prefix.lib64``        ``$prefix/lib64``
``prefix.share.man``    ``$prefix/share/man``
``prefix.foo.bar.baz``  ``$prefix/foo/bar/baz``
======================  =======================

Of course, this only works if your file or directory is a valid Python
variable name. If your file or directory contains dashes or dots, use
``join`` instead:

.. code-block:: python

   prefix.lib.join('libz.a')


.. _spec-objects:

------------
Spec objects
------------

When ``install`` is called, most parts of the build process are set up
for you.  The correct version's tarball has been downloaded and
expanded.  Environment variables like ``CC`` and ``CXX`` are set to
point to the correct compiler and version.  An install prefix has
already been selected and passed in as ``prefix``.  In most cases this
is all you need to get ``configure``, ``cmake``, or another install
working correctly.

There will be times when you need to know more about the build
configuration.  For example, some software requires that you pass
special parameters to ``configure``, like
``--with-libelf=/path/to/libelf`` or ``--with-mpich``.  You might also
need to supply special compiler flags depending on the compiler.  All
of this information is available in the spec.

^^^^^^^^^^^^^^^^^^^^^^^^
Testing spec constraints
^^^^^^^^^^^^^^^^^^^^^^^^

You can test whether your spec is configured a certain way by using
the ``satisfies`` method.  For example, if you want to check whether
the package's version is in a particular range, you can use specs to
do that, e.g.:

.. code-block:: python

   configure_args = [
       '--prefix={0}'.format(prefix)
   ]

   if spec.satisfies('@1.2:1.4'):
       configure_args.append("CXXFLAGS='-DWITH_FEATURE'")

   configure(*configure_args)

This works for compilers, too:

.. code-block:: python

   if spec.satisfies('%gcc'):
       configure_args.append('CXXFLAGS="-g3 -O3"')
   if spec.satisfies('%intel'):
       configure_args.append('CXXFLAGS="-xSSE2 -fast"')

Or for combinations of spec constraints:

.. code-block:: python

   if spec.satisfies('@1.2%intel'):
       tty.error("Version 1.2 breaks when using Intel compiler!")

You can also do similar satisfaction tests for dependencies:

.. code-block:: python

   if spec.satisfies('^dyninst@8.0'):
       configure_args.append('CXXFLAGS=-DSPECIAL_DYNINST_FEATURE')

This could allow you to easily work around a bug in a particular
dependency version.

You can use ``satisfies()`` to test for particular dependencies,
e.g. ``foo.satisfies('^openmpi@1.2')`` or ``foo.satisfies('^mpich')``,
or you can use Python's built-in ``in`` operator:

.. code-block:: python

   if 'libelf' in spec:
       print "this package depends on libelf"

This is useful for virtual dependencies, as you can easily see what
implementation was selected for this build:

.. code-block:: python

   if 'openmpi' in spec:
       configure_args.append('--with-openmpi')
   elif 'mpich' in spec:
       configure_args.append('--with-mpich')
   elif 'mvapich' in spec:
       configure_args.append('--with-mvapich')

It's also a bit more concise than satisfies.  The difference between
the two functions is that ``satisfies()`` tests whether spec
constraints overlap at all, while ``in`` tests whether a spec or any
of its dependencies satisfy the provided spec.

^^^^^^^^^^^^^^^^^^^^^^^
Architecture specifiers
^^^^^^^^^^^^^^^^^^^^^^^

As mentioned in :ref:`support-for-microarchitectures` each node in a concretized spec
object has an architecture attribute which is a triplet of ``platform``, ``os`` and ``target``.
Each of these three items can be queried to take decisions when configuring, building or
installing a package.

""""""""""""""""""""""""""""""""""""""""""""""
Querying the platform and the operating system
""""""""""""""""""""""""""""""""""""""""""""""

Sometimes the actions to be taken to install a package might differ depending on the
platform we are installing for. If that is the case we can use conditionals:

.. code-block:: python

   if spec.platform == 'darwin':
       # Actions that are specific to Darwin
       args.append('--darwin-specific-flag')

and branch based on the current spec platform. If we need to make a package directive
conditional on the platform we can instead employ the usual spec syntax and pass the
corresponding constraint to the appropriate argument of that directive:

.. code-block:: python

   class Libnl(AutotoolsPackage):

       conflicts('platform=darwin', msg='libnl requires FreeBSD or Linux')

Similar considerations are also valid for the ``os`` part of a spec's architecture.
For instance:

.. code-block:: python

   class Glib(AutotoolsPackage)

       patch('old-kernels.patch', when='os=centos6')

will apply the patch only when the operating system is Centos 6.

.. note::

   Even though experienced Python programmers might recognize that there are other ways
   to retrieve information on the platform:

   .. code-block:: python

      if sys.platform == 'darwin':
          # Actions that are specific to Darwin
          args.append('--darwin-specific-flag')

   querying the spec architecture's platform should be considered the preferred. The key difference
   is that a query on ``sys.platform``, or anything similar, is always bound to the host on which the
   interpreter running Spack is located and as such it won't work correctly in environments where
   cross-compilation is required.

"""""""""""""""""""""""""""""""""""""
Querying the target microarchitecture
"""""""""""""""""""""""""""""""""""""

The third item of the architecture tuple is the ``target`` which abstracts the information on the
CPU microarchitecture. A list of all the targets known to Spack can be obtained via the
command line:

.. command-output:: spack arch --known-targets

Within directives each of the names above can be used to match a particular target:

.. code-block:: python

   class Julia(Package):
       # This patch is only applied on icelake microarchitectures
       patch("icelake.patch", when="target=icelake")

It's also possible to select all the architectures belonging to the same family
using an open range:

.. code-block:: python

   class Julia(Package):
       # This patch is applied on all x86_64 microarchitectures.
       # The trailing colon that denotes an open range of targets
       patch("generic_x86_64.patch", when="target=x86_64:")

in a way that resembles what was shown in :ref:`versions-and-fetching` for versions.
Where ``target`` objects really shine though is when they are used in methods
called at configure, build or install time. In that case we can test targets
for supported features, for instance:

.. code-block:: python

   if 'avx512' in spec.target:
       args.append('--with-avx512')

The snippet above will append the ``--with-avx512`` item to a list of arguments only if the corresponding
feature is supported by the current target. Sometimes we need to take different actions based
on the architecture family and not on the specific microarchitecture. In those cases
we can check the ``family`` attribute:

.. code-block:: python

   if spec.target.family == 'ppc64le':
       args.append('--enable-power')

Possible values for the ``family`` attribute are displayed by ``spack arch --known-targets``
under the "Generic architectures (families)" header.
Finally it's possible to perform actions based on whether the current microarchitecture
is compatible with a known one:

.. code-block:: python

   if spec.target > 'haswell':
       args.append('--needs-at-least-haswell')

The snippet above will add an item to a list of configure options only if the current
architecture is a superset of ``haswell`` or, said otherwise, only if the current
architecture is a later microarchitecture still compatible with ``haswell``.

.. admonition:: Using Spack on unknown microarchitectures

   If Spack is used on an unknown microarchitecture it will try to perform a best match
   of the features it detects and will select the closest microarchitecture it has
   information for. In case nothing matches, it will create on the fly a new generic
   architecture. This is done to allow users to still be able to use Spack
   for their work. The software built won't be probably as optimized as it could but just
   as you need a newer compiler to build for newer architectures, you may need newer
   versions of Spack for new architectures to be correctly labeled.

^^^^^^^^^^^^^^^^^^^^^^
Accessing Dependencies
^^^^^^^^^^^^^^^^^^^^^^

You may need to get at some file or binary that's in the installation
prefix of one of your dependencies. You can do that by sub-scripting
the spec:

.. code-block:: python

   spec['mpi']

The value in the brackets needs to be some package name, and spec
needs to depend on that package, or the operation will fail.  For
example, the above code will fail if the ``spec`` doesn't depend on
``mpi``.  The value returned is itself just another ``Spec`` object,
so you can do all the same things you would do with the package's
own spec:

.. code-block:: python

   spec['mpi'].prefix.bin
   spec['mpi'].version

.. _multimethods:

^^^^^^^^^^^^^^^^^^^^^^^^^^
Multimethods and ``@when``
^^^^^^^^^^^^^^^^^^^^^^^^^^

Spack allows you to make multiple versions of instance functions in
packages, based on whether the package's spec satisfies particular
criteria.

The ``@when`` annotation lets packages declare multiple versions of
methods like ``install()`` that depend on the package's spec.  For
example:

.. code-block:: python

   class SomePackage(Package):
       ...

       def install(self, prefix):
           # Do default install

       @when('arch=chaos_5_x86_64_ib')
       def install(self, prefix):
           # This will be executed instead of the default install if
           # the package's sys_type() is chaos_5_x86_64_ib.

       @when('arch=linux-debian7-x86_64')
       def install(self, prefix):
           # This will be executed if the package's sys_type() is
           # linux-debian7-x86_64.

In the above code there are three versions of ``install()``, two of which
are specialized for particular platforms.  The version that is called
depends on the architecture of the package spec.

Note that this works for methods other than install, as well.  So,
if you only have part of the install that is platform specific, you
could do something more like this:

.. code-block:: python

   class SomePackage(Package):
      ...
       # virtual dependence on MPI.
       # could resolve to mpich, mpich2, OpenMPI
       depends_on('mpi')

       def setup(self):
           # do nothing in the default case
           pass

       @when('^openmpi')
       def setup(self):
           # do something special when this is built with OpenMPI for
           # its MPI implementations.

       def install(self, prefix):
           # Do common install stuff
           self.setup()
           # Do more common install stuff

You can write multiple ``@when`` specs that satisfy the package's spec,
for example:

.. code-block:: python

   class SomePackage(Package):
       ...
       depends_on('mpi')

       def setup_mpi(self):
           # the default, called when no @when specs match
           pass

       @when('^mpi@3:')
       def setup_mpi(self):
           # this will be called when mpi is version 3 or higher
           pass

       @when('^mpi@2:')
       def setup_mpi(self):
           # this will be called when mpi is version 2 or higher
           pass

       @when('^mpi@1:')
       def setup_mpi(self):
           # this will be called when mpi is version 1 or higher
           pass

In situations like this, the first matching spec, in declaration order
will be called.  As before, if no ``@when`` spec matches, the default
method (the one without the ``@when`` decorator) will be called.

.. warning::

   The default version of decorated methods must **always** come
   first.  Otherwise it will override all of the platform-specific
   versions.  There's not much we can do to get around this because of
   the way decorators work.

.. _compiler-wrappers:

---------------------
Compiler wrappers
---------------------

As mentioned, ``CC``, ``CXX``, ``F77``, and ``FC`` are set to point to
Spack's compiler wrappers.  These are simply called ``cc``, ``c++``,
``f77``, and ``f90``, and they live in ``$SPACK_ROOT/lib/spack/env``.

``$SPACK_ROOT/lib/spack/env`` is added first in the ``PATH``
environment variable when ``install()`` runs so that system compilers
are not picked up instead.

All of these compiler wrappers point to a single compiler wrapper
script that figures out which *real* compiler it should be building
with.  This comes either from spec `concretization
<abstract-and-concrete>`_ or from a user explicitly asking for a
particular compiler using, e.g., ``%intel`` on the command line.

In addition to invoking the right compiler, the compiler wrappers add
flags to the compile line so that dependencies can be easily found.
These flags are added for each dependency, if they exist:

Compile-time library search paths
* ``-L$dep_prefix/lib``
* ``-L$dep_prefix/lib64``

Runtime library search paths (RPATHs)
* ``$rpath_flag$dep_prefix/lib``
* ``$rpath_flag$dep_prefix/lib64``

Include search paths
* ``-I$dep_prefix/include``

An example of this would be the ``libdwarf`` build, which has one
dependency: ``libelf``.  Every call to ``cc`` in the ``libdwarf``
build will have ``-I$LIBELF_PREFIX/include``,
``-L$LIBELF_PREFIX/lib``, and ``$rpath_flag$LIBELF_PREFIX/lib``
inserted on the command line.  This is done transparently to the
project's build system, which will just think it's using a system
where ``libelf`` is readily available.  Because of this, you **do
not** have to insert extra ``-I``, ``-L``, etc. on the command line.

Another useful consequence of this is that you often do *not* have to
add extra parameters on the ``configure`` line to get autotools to
find dependencies.  The ``libdwarf`` install method just calls
configure like this:

.. code-block:: python

   configure("--prefix=" + prefix)

Because of the ``-L`` and ``-I`` arguments, configure will
successfully find ``libdwarf.h`` and ``libdwarf.so``, without the
packager having to provide ``--with-libdwarf=/path/to/libdwarf`` on
the command line.

.. note::

    For most compilers, ``$rpath_flag`` is ``-Wl,-rpath,``. However, NAG
    passes its flags to GCC instead of passing them directly to the linker.
    Therefore, its ``$rpath_flag`` is doubly wrapped: ``-Wl,-Wl,,-rpath,``.
    ``$rpath_flag`` can be overridden on a compiler specific basis in
    ``lib/spack/spack/compilers/$compiler.py``.

The compiler wrappers also pass the compiler flags specified by the user from
the command line (``cflags``, ``cxxflags``, ``fflags``, ``cppflags``, ``ldflags``,
and/or ``ldlibs``). They do not override the canonical autotools flags with the
same names (but in ALL-CAPS) that may be passed into the build by particularly
challenging package scripts.

---------------------
MPI support in Spack
---------------------

It is common for high performance computing software/packages to use the
Message Passing Interface ( ``MPI``).  As a result of conretization, a
given package can be built using different implementations of MPI such as
``Openmpi``, ``MPICH`` or ``IntelMPI``.  That is, when your package
declares that it ``depends_on('mpi')``, it can be built with any of these
``mpi`` implementations. In some scenarios, to configure a package, one
has to provide it with appropriate MPI compiler wrappers such as
``mpicc``, ``mpic++``.  However different implementations of ``MPI`` may
have different names for those wrappers.

Spack provides an idiomatic way to use MPI compilers in your package.  To
use MPI wrappers to compile your whole build, do this in your
``install()`` method:

.. code-block:: python

   env['CC'] = spec['mpi'].mpicc
   env['CXX'] = spec['mpi'].mpicxx
   env['F77'] = spec['mpi'].mpif77
   env['FC'] = spec['mpi'].mpifc

That's all.  A longer explanation of why this works is below.

We don't try to force any particular build method on packagers.  The
decision to use MPI wrappers depends on the way the package is written,
on common practice, and on "what works".  Loosely, There are three types
of MPI builds:

  1. Some build systems work well without the wrappers and can treat MPI
     as an external library, where the person doing the build has to
     supply includes/libs/etc.  This is fairly uncommon.

  2. Others really want the wrappers and assume you're using an MPI
     "compiler" â€“ i.e., they have no mechanism to add MPI
     includes/libraries/etc.

  3. CMake's ``FindMPI`` needs the compiler wrappers, but it uses them to
     extract ``â€“I`` / ``-L`` / ``-D`` arguments, then treats MPI like a
     regular library.

Note that some CMake builds fall into case 2 because they either don't
know about or don't like CMake's ``FindMPI`` support â€“ they just assume
an MPI compiler. Also, some autotools builds fall into case 3 (e.g. `here
is an autotools version of CMake's FindMPI
<https://github.com/tgamblin/libra/blob/master/m4/lx_find_mpi.m4>`_).

Given all of this, we leave the use of the wrappers up to the packager.
Spack will support all three ways of building MPI packages.

^^^^^^^^^^^^^^^^^^^^^
Packaging Conventions
^^^^^^^^^^^^^^^^^^^^^

As mentioned above, in the ``install()`` method, ``CC``, ``CXX``,
``F77``, and ``FC`` point to Spack's wrappers around the chosen compiler.
Spack's wrappers are not the MPI compiler wrappers, though they do
automatically add ``â€“I``, ``â€“L``, and ``â€“Wl,-rpath`` args for
dependencies in a similar way.  The MPI wrappers are a bit different in
that they also add ``-l`` arguments for the MPI libraries, and some add
special ``-D`` arguments to trigger build options in MPI programs.

For case 1 above, you generally don't need to do more than patch your
Makefile or add configure args as you normally would.

For case 3, you don't need to do much of anything, as Spack puts the MPI
compiler wrappers in the PATH, and the build will find them and
interrogate them.

For case 2, things are a bit more complicated, as you'll need to tell the
build to use the MPI compiler wrappers instead of Spack's compiler
wrappers.  All it takes some lines like this:

.. code-block:: python

   env['CC'] = spec['mpi'].mpicc
   env['CXX'] = spec['mpi'].mpicxx
   env['F77'] = spec['mpi'].mpif77
   env['FC'] = spec['mpi'].mpifc

Or, if you pass CC, CXX, etc. directly to your build with, e.g.,
`--with-cc=<path>`, you'll want to substitute `spec['mpi'].mpicc` in
there instead, e.g.:

.. code-block:: python

   configure('â€”prefix=%s' % prefix,
             'â€”with-cc=%s' % spec['mpi'].mpicc)

Now, you may think that doing this will lose the includes, library paths,
and RPATHs that Spack's compiler wrapper get you, but we've actually set
things up so that the MPI compiler wrappers use Spack's compiler wrappers
when run from within Spack. So using the MPI wrappers should really be as
simple as the code above.

^^^^^^^^^^^^^^^^^^^^^
``spec['mpi']``
^^^^^^^^^^^^^^^^^^^^^

Ok, so how does all this work?

If your package has a virtual dependency like ``mpi``, then referring to
``spec['mpi']`` within ``install()`` will get you the concrete ``mpi``
implementation in your dependency DAG.  That is a spec object just like
the one passed to install, only the MPI implementations all set some
additional properties on it to help you out.  E.g., in mvapich2, you'll
find this:

.. literalinclude:: _spack_root/var/spack/repos/builtin/packages/mvapich2/package.py
   :pyobject: Mvapich2.setup_dependent_package

That code allows the mvapich2 package to associate an ``mpicc`` property
with the ``mvapich2`` node in the DAG, so that dependents can access it.
``openmpi`` and ``mpich`` do similar things.  So, no matter what MPI
you're using, spec['mpi'].mpicc gets you the location of the MPI
compilers. This allows us to have a fairly simple polymorphic interface
for information about virtual dependencies like MPI.

^^^^^^^^^^^^^^^^^^^^^
Wrapping wrappers
^^^^^^^^^^^^^^^^^^^^^

Spack likes to use its own compiler wrappers to make it easy to add
``RPATHs`` to builds, and to try hard to ensure that your builds use the
right dependencies.  This doesn't play nicely by default with MPI, so we
have to do a couple tricks.

  1. If we build MPI with Spack's wrappers, mpicc and friends will be
     installed with hard-coded paths to Spack's wrappers, and using them
     from outside of Spack will fail because they only work within Spack.
     To fix this, we patch mpicc and friends to use the regular
     compilers.  Look at the filter_compilers method in mpich, openmpi,
     or mvapich2 for details.

  2. We still want to use the Spack compiler wrappers when Spack is
     calling mpicc. Luckily, wrappers in all mainstream MPI
     implementations provide environment variables that allow us to
     dynamically set the compiler to be used by mpicc, mpicxx, etc.
     Denis pasted some code from this below â€“ Spack's build environment
     sets ``MPICC``, ``MPICXX``, etc. for mpich derivatives and
     ``OMPI_CC``, ``OMPI_CXX``, etc. for OpenMPI. This makes the MPI
     compiler wrappers use the Spack compiler wrappers so that your
     dependencies still get proper RPATHs even if you use the MPI
     wrappers.

^^^^^^^^^^^^^^^^^^^^^
MPI on Cray machines
^^^^^^^^^^^^^^^^^^^^^

The Cray programming environment notably uses ITS OWN compiler wrappers,
which function like MPI wrappers.  On Cray systems, the ``CC``, ``cc``,
and ``ftn`` wrappers ARE the MPI compiler wrappers, and it's assumed that
you'll use them for all of your builds.  So on Cray we don't bother with
``mpicc``, ``mpicxx``, etc, Spack MPI implementations set
``spec['mpi'].mpicc`` to point to Spack's wrappers, which wrap the Cray
wrappers, which wrap the regular compilers and include MPI flags.  That
may seem complicated, but for packagers, that means the same code for
using MPI wrappers will work, even on even on a Cray:

.. code-block:: python

   env['CC'] = spec['mpi'].mpicc

This is because on Cray, ``spec['mpi'].mpicc`` is just ``spack_cc``.

.. _checking_an_installation:

------------------------
Checking an installation
------------------------

A package that *appears* to install successfully does not mean
it is actually installed correctly or will continue to work indefinitely.
There are a number of possible points of failure so Spack provides
features for checking the software along the way.

Failures can occur during and after the installation process. The
build may start but the software not end up fully installed. The
installed software may not work at all or as expected. The software
may work after being installed but, due to changes on the system,
may stop working days, weeks, or months after being installed.

This section describes Spack's support for checks that can be performed
during and after its installation. The former checks are referred to as
``build-time tests`` and the latter as ``stand-alone (or smoke) tests``.

.. _build_time-tests:

^^^^^^^^^^^^^^^^
Build-time tests
^^^^^^^^^^^^^^^^

Spack infers the status of a build based on the contents of the install
prefix. Success is assumed if anything (e.g., a file, directory) is
written after ``install()`` completes. Otherwise, the build is assumed
to have failed. However, the presence of install prefix contents
is not a sufficient indicator of success.

Consider a simple autotools build using the following commands:

.. code-block:: console

   $ ./configure --prefix=/path/to/installation/prefix
   $ make
   $ make install

Standard Autotools and CMake do not write anything to the prefix from
the ``configure`` and ``make`` commands. Files are only written from
the ``make install`` after the build completes.

.. note::

   If you want to learn more about ``Autotools`` and ``CMake`` packages
   in Spack, refer to :ref:`AutotoolsPackage <autotoolspackage>` and
   :ref:`CMakePackage <cmakepackage>`, respectively.

What can you do to check that the build is progressing satisfactorily?
If there are specific files and or directories expected of a successful
installation, you can add basic, fast ``sanity checks``. You can also add
checks to be performed after one or more installation phases.

.. _sanity-checks:

""""""""""""""""""""
Adding sanity checks
""""""""""""""""""""

Unfortunately, many builds of scientific software modify the installation
prefix **before** ``make install``. Builds like this can falsely report
success when an error occurs before the installation is complete. Simple
sanity checks can be used to identify files and or directories that are
required of a successful installation. Spack checks for the presence of
the files and directories after ``install()`` runs.

If any of the listed files or directories are missing, then the build will
fail and the install prefix will be removed. If they all exist, then Spack
considers the build successful from a sanity check perspective and keeps
the prefix in place.

For example, the sanity checks for the ``reframe`` package below specify
that eight paths must exist within the installation prefix after the
``install`` method completes.

.. code-block:: python

   class Reframe(Package):
       ...

       # sanity check
       sanity_check_is_file = [join_path('bin', 'reframe')]
       sanity_check_is_dir  = ['bin', 'config', 'docs', 'reframe', 'tutorials',
                               'unittests', 'cscs-checks']

Spack will then ensure the installation created the **file**:

* ``self.prefix/bin/reframe``

It will also check for the existence of the following **directories**:

* ``self.prefix/bin``
* ``self.prefix/config``
* ``self.prefix/docs``
* ``self.prefix/reframe``
* ``self.prefix/tutorials``
* ``self.prefix/unittests``
* ``self.prefix/cscs-checks``

.. note::

   You **MUST** use ``sanity_check_is_file`` to specify required
   files and ``sanity_check_is_dir`` for required directories.

.. _install_phase-tests:

"""""""""""""""""""""""""""""""
Adding installation phase tests
"""""""""""""""""""""""""""""""

Sometimes packages appear to build "correctly" only to have run-time
behavior issues discovered at a later stage, such as after a full
software stack relying on them has been built. Checks can be performed
at different phases of the package installation to possibly avoid
these types of problems. Some checks are built-in to different build
systems, while others will need to be added to the package.

Built-in installation phase tests are provided by packages inheriting
from select :ref:`build systems <build-systems>`, where naming conventions
are used to identify typical test identifiers for those systems. In
general, you won't need to add anything to your package to take advantage
of these tests if your software's build system complies with the convention;
otherwise, you'll want or need to override the post-phase method to perform
other checks.

.. list-table:: Built-in installation phase tests
   :header-rows: 1

   * - Build System Class
     - Post-Build Phase Method (Runs)
     - Post-Install Phase Method (Runs)
   * - :ref:`AutotoolsPackage <autotoolspackage>`
     - ``check`` (``make test``, ``make check``)
     - ``installcheck`` (``make installcheck``)
   * - :ref:`CMakePackage <cmakepackage>`
     - ``check`` (``make check``, ``make test``)
     - Not applicable
   * - :ref:`MakefilePackage <makefilepackage>`
     - ``check`` (``make test``, ``make check``)
     - ``installcheck`` (``make installcheck``)
   * - :ref:`MesonPackage <mesonpackage>`
     - ``check`` (``make test``, ``make check``)
     - Not applicable
   * - :ref:`PerlPackage <perlpackage>`
     - ``check`` (``make test``)
     - Not applicable
   * - :ref:`PythonPackage <pythonpackage>`
     - Not applicable
     - ``test`` (module imports)
   * - :ref:`QMakePackage <qmakepackage>`
     - ``check`` (``make check``)
     - Not applicable
   * - :ref:`SConsPackage <sconspackage>`
     - ``build_test`` (must be overridden)
     - Not applicable
   * - :ref:`SIPPackage <sippackage>`
     - Not applicable
     - ``test`` (module imports)

For example, the ``Libelf`` package inherits from ``AutotoolsPackage``
and its ``Makefile`` has a standard ``check`` target. So Spack will
automatically run ``make check`` after the ``build`` phase when it
is installed using the ``--test`` option, such as:

.. code-block:: console

   $ spack install --test=root libelf

In addition to overriding any built-in build system installation
phase tests, you can write your own install phase tests. You will
need to use two decorators for each phase test method:

* ``run_after``
* ``on_package_attributes``

The first decorator tells Spack when in the installation process to
run your test method installation process; namely *after* the provided
installation phase. The second decorator tells Spack to only run the
checks when the ``--test`` option is provided on the command line.

.. note::

   Be sure to place the directives above your test method in the order
   ``run_after`` *then* ``on_package_attributes``.

.. note::

   You also want to be sure the package supports the phase you use
   in the ``run_after`` directive. For example, ``PackageBase`` only
   supports the ``install`` phase while the ``AutotoolsPackage`` and
   ``MakefilePackage`` support both ``install`` and ``build`` phases.

Assuming both ``build`` and ``install`` phases are available to you,
you could add additional checks to be performed after each of those
phases based on the skeleton provided below.

.. code-block:: python

   class YourMakefilePackage(MakefilePackage):
       ...

       @run_after('build')
       @on_package_attributes(run_tests=True)
       def check_build(self):
            # Add your custom post-build phase tests
            pass

       @run_after('install')
       @on_package_attributes(run_tests=True)
       def check_install(self):
            # Add your custom post-install phase tests
            pass

.. note::

    You could also schedule work to be done **before** a given phase
    using the ``run_before`` decorator.

By way of a concrete example, the ``reframe`` package mentioned
previously has a simple installation phase check that runs the
installed executable. The check is implemented as follows:

.. code-block:: python

   class Reframe(Package):
       ...

       # check if we can run reframe
       @run_after('install')
       @on_package_attributes(run_tests=True)
       def check_list(self):
            with working_dir(self.stage.source_path):
                reframe = Executable(join_path(self.prefix, 'bin', 'reframe'))
                reframe('-l')

.. warning::

    The API for adding tests is not yet considered stable and may change
    in future releases.

.. _cmd-spack-test:

^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Stand-alone (or smoke) tests
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

While build-time tests are integrated with the installation process,
stand-alone tests are independent of that process.  Consequently, such
tests can be performed days, even weeks, after the software is installed.

Stand-alone tests are checks that should run relatively quickly -- as
in on the order of at most a few minutes -- and ideally execute all
aspects of the installed software, or at least key functionality.

.. note::

    Execution speed is important because these tests are intended
    to quickly assess whether the installed software works on the
    system.

    Failing stand-alone tests indicate that there is no reason to
    proceed with more resource-intensive tests.

    Passing stand-alone (or smoke) tests can lead to more thorough
    testing, such as extensive unit or regression tests, or tests
    that run at scale. Spack support for more thorough testing is
    a work in progress.

Stand-alone tests have their own test stage directory, which can be
configured. These tests can compile or build software with the compiler
used to build the package. They can use files cached from the build for
testing the installation. Custom files, such as source, data, or expected
outputs can be added for use in these tests.

""""""""""""""""""""""""""""""""""""
Configuring the test stage directory
""""""""""""""""""""""""""""""""""""

Stand-alone tests rely on a stage directory for building, running,
and tracking results.
The default directory, ``~/.spack/test``, is defined in
:ref:`etc/spack/defaults/config.yaml <config-yaml>`.
You can configure the location in the high-level ``config`` by adding
or changing the ``test_stage`` path in the appropriate ``config.yaml``
file such that:

.. code-block:: yaml

   config:
     test_stage: /path/to/stage

The package can access this path **during test processing** using
`self.test_suite.stage`.

.. note::

   The test stage path is established for the entire suite. That
   means it is the root directory for all specs being installed
   with the same `spack test run` command. Each spec gets its
   own stage subdirectory.

"""""""""""""""""""""""""
Enabling test compilation
"""""""""""""""""""""""""

Some stand-alone tests will require access to the compiler with which
the package was built, especially for library-only packages. You must
enable loading the package's compiler configuration by setting the
``test_requires_compiler`` property to ``True`` for your package.
For example:

.. code-block:: python

   class MyPackage(Package):
       ...

       test_requires_compiler = True

Setting this property to ``True`` makes the compiler available in the
test environment through the canonical environment variables (e.g.,
``CC``, ``CXX``, ``FC``, ``F77``).

.. note::

   We recommend adding the property at the top of the package with the
   other attributes, such as ``homepage`` and ``url``.

.. _cache_extra_test_sources:

"""""""""""""""""""""""
Adding build-time files
"""""""""""""""""""""""

.. note::

    We highly recommend re-using build-time tests and input files
    for testing installed software. These files are easier to keep
    synchronized since they reside within the software's repository
    than maintaining custom install test files with the Spack package.

You can use the ``cache_extra_test_sources`` method to copy directories
and or files from the build stage directory to the package's installation
directory.

The signature for ``cache_extra_test_sources`` is:

.. code-block:: python

   def cache_extra_test_sources(self, srcs):

where ``srcs`` is a string or a list of strings corresponding to
the paths for the files and or subdirectories, relative to the staged
source, that are to be copied to the corresponding relative test path
under the prefix. All of the contents within each subdirectory will
also be copied.

For example, a package method for copying everything in the ``tests``
subdirectory plus the ``foo.c`` and ``bar.c`` files from ``examples``
can be implemented as shown below.

.. note::

   The method name ``copy_test_sources`` here is for illustration
   purposes. You are free to use a name that is more suited to your
   package.

   The key to copying the files at build time for stand-alone testing
   is use of the ``run_after`` directive, which ensures the associated
   files are copied **after** the provided build stage.

.. code-block:: python

   class MyPackage(Package):
       ...

       @run_after('install')
       def copy_test_sources(self):
           srcs = ['tests',
                   join_path('examples', 'foo.c'),
                   join_path('examples', 'bar.c')]
           self.cache_extra_test_sources(srcs)

In this case, the method copies the associated files from the build
stage **after** the software is installed to the package's metadata
directory. The result is the directory and files will be cached in
a special test subdirectory under the installation prefix.

These paths are **automatically copied** to the test stage directory
during stand-alone testing. The package's ``test`` method can access
them using the ``self.test_suite.current_test_cache_dir`` property.
In our example, the method would use the following paths to reference
the copy of each entry listed in ``srcs``, respectively:

* ``join_path(self.test_suite.current_test_cache_dir, 'tests')``
* ``join_path(self.test_suite.current_test_cache_dir, 'examples', 'foo.c')``
* ``join_path(self.test_suite.current_test_cache_dir, 'examples', 'bar.c')``

.. note::

    Library developers will want to build the associated tests
    against their **installed** libraries before running them.

.. note::

    While source and input files are generally recommended, binaries
    **may** also be cached by the build process for install testing.
    Only you, as the package writer or maintainer, know whether these
    would be appropriate for ensuring the installed software continues
    to work as the underlying system evolves.

.. _cache_custom_files:

"""""""""""""""""""
Adding custom files
"""""""""""""""""""

Some tests may require additional files not available from the build.
Examples include:

- test source files
- test input files
- test build scripts
- expected test output

These extra files should be added to the ``test`` subdirectory of the
package in the Spack repository.

Spack will **automatically copy** the contents of that directory to the
test staging directory for stand-alone testing. The ``test`` method can
access those files using the ``self.test_suite.current_test_data_dir``
property.

.. _expected_test_output_from_file:

"""""""""""""""""""""""""""""""""""
Reading expected output from a file
"""""""""""""""""""""""""""""""""""

The helper function ``get_escaped_text_output`` is available for packages
to retrieve and properly format the text from a file that contains the
output that is expected when an executable is run using ``self.run_test``.

The signature for ``get_escaped_text_output`` is:

.. code-block:: python

   def get_escaped_text_output(filename):

where ``filename`` is the path to the file containing the expected output.

The ``filename`` for a :ref:`custom file <cache_custom_files>` can be
accessed and used as illustrated by a simplified version of an ``sqlite``
package check:

.. code-block:: python

   class Sqlite(AutotoolsPackage):
       ...

       def test(self):
           test_data_dir = self.test_suite.current_test_data_dir
           db_filename = test_data_dir.join('packages.db')
           ..

           expected = get_escaped_text_output(test_data_dir.join('dump.out'))
           self.run_test('sqlite3',
                         [db_filename, '.dump'],
                         expected,
                         installed=True,
                         purpose='test: checking dump output',
                         skip_missing=False)

Expected outputs do not have to be stored with the Spack package.
Maintaining them with the source is actually preferable.

Suppose a package's source has ``examples/foo.c`` and ``examples/foo.out``
files that are copied for stand-alone test purposes using
:ref:`cache_extra_test_sources <cache_extra_test_sources>` and the
`run_test` method builds the executable ``examples/foo``. The package
can retrieve the expected output from ``examples/foo.out`` using:

.. code-block:: python

   class MyFooPackage(Package):
       ...

       def test(self):
           ..
           filename = join_path(self.test_suite.current_test_cache_dir,
                                'examples', 'foo.out')
           expected = get_escaped_text_output(filename)
           ..

Alternatively, suppose ``MyFooPackage`` installs tests in ``share/tests``
and their outputs in ``share/tests/outputs``. The expected output for
``foo``, assuming it is still called ``foo.out``, can be retrieved as
follows:

.. code-block:: python

   class MyFooPackage(Package):
       ...

       def test(self):
           ..
           filename = join_path(self.prefix.share.tests.outputs, 'foo.out')
           expected = get_escaped_text_output(filename)
           ..


""""""""""""""""""""""""
Adding stand-alone tests
""""""""""""""""""""""""

Stand-alone tests are defined in the package's ``test`` method. The
default ``test`` method is a no-op so you'll want to override it to
implement the tests.

.. note::

   Any package method named ``test`` is automatically executed by
   Spack when the ``spack test run`` command is performed.

For example, the ``MyPackage`` package below provides a skeleton for
the test method.

.. code-block:: python

   class MyPackage(Package):
       ...

       def test(self):
           # TODO: Add quick checks of the installed software
           pass

Stand-alone tests run in an environment that provides access to the
package and all of its dependencies, including ``test``-type
dependencies.

Standard python ``assert`` statements and other error reporting
mechanisms can be used in the ``test`` method. Spack will report
such errors as test failures.

You can implement multiple tests (or test parts) within the ``test``
method using the ``run_test`` method. Each invocation is run separately
in a manner that allows testing to continue after failures.

The signature for ``run_test`` is:

.. code-block:: python

   def run_test(self, exe, options=[], expected=[], status=0,
                installed=False, purpose='', skip_missing=False,
                work_dir=None):

where each argument has the following meaning:

* ``exe`` is the executable to run.

  If a name, the ``exe`` is required to be found in one of the paths
  in the ``PATH`` environment variable **unless** ``skip_missing`` is
  ``True``. Alternatively, a relative (to ``work_dir``) or fully
  qualified path for the executable can be provided in ``exe``.

  The test will fail if the resulting path is not within the prefix
  of the package being tested **unless** ``installed`` is ``False``.

* ``options`` is a list of the command line options.

  Options are a list of strings to be passed to the executable when
  it runs.

  The default is ``[]``, which means no options are provided to the
  executable.

* ``expected`` is an optional list of expected output strings.

  Spack requires every string in ``expected`` to be a regex matching
  part of the output from the test run (e.g.,
  ``expected=['completed successfully', 'converged in']``).  The
  output can also include expected failure outputs (e.g.,
  ``expected=['failed to converge']``).

  The expected output can be :ref:`read from a file
  <expected_test_output_from_file>`.

  The default is ``expected=[]``, so Spack will not check the output.

* ``status`` is the optional expected return code(s).

  A list of return codes corresponding to successful execution can
  be provided (e.g., ``status=[0,3,7]``). Support for non-zero return
  codes allows for basic **expected failure** tests as well as different
  return codes across versions of the software.

  The default is ``status=[0]``, which corresponds to **successful**
  execution in the sense that the executable does not exit with a
  failure code or raise an exception.

* ``installed`` is used to require ``exe`` to be within the package
  prefix.

  If ``True``, then the path for ``exe`` is required to be within the
  package prefix; otherwise, the path is not constrained.

  The default is ``False``, so the fully qualified path for ``exe``
  does **not** need to be within the installation directory.

* ``purpose`` is an optional heading describing the the test part.

  Output from the test is written to a test log file so this argument
  serves as a searchable heading in text logs to highlight the start
  of the test part. Having a description can be helpful when debugging
  failing tests.

* ``skip_missing`` is used to determine if the test should be skipped.

  If ``True``, then the test part should be skipped if the executable
  is missing; otherwise, the executable must exist. This option can
  be useful when test executables are removed or change as the software
  evolves in subsequent versions.

  The default is ``False``, which means the test executable must be
  present for any installable version of the software.

* ``work_dir`` is the path to the directory from which the executable
  will run.

  The default of ``None`` corresponds to the current directory (``'.'``).

"""""""""""""""""""""""""""""""""""""""""
Accessing package- and test-related files
"""""""""""""""""""""""""""""""""""""""""

You may need to access files from one or more locations when writing
stand-alone tests. This can happen if the software's repository does not
include test source files or includes files but no way to build the
executables using the installed headers and libraries. In these
cases, you may need to reference the files relative to one or more
root directory. The properties containing package- and test-related
directory paths are provided in the table below.

.. list-table:: Directory-to-property mapping
   :header-rows: 1

   * - Root Directory
     - Package Property
     - Example(s)
   * - Package Installation Files
     - ``self.prefix``
     - ``self.prefix.include``, ``self.prefix.lib``
   * - Package Dependency's Files
     - ``self.spec['<dependency-package>'].prefix``
     - ``self.spec['trilinos'].prefix.include``
   * - Test Suite Stage Files
     - ``self.test_suite.stage``
     - ``join_path(self.test_suite.stage, 'results.txt')``
   * - Staged Cached Build-time Files
     - ``self.test_suite.current_test_cache_dir``
     - ``join_path(self.test_suite.current_test_cache_dir, 'examples', 'foo.c')``
   * - Staged Custom Package Files
     - ``self.test_suite.current_test_data_dir``
     - ``join_path(self.test_suite.current_test_data_dir, 'hello.f90')``

""""""""""""""""""""""""""""
Inheriting stand-alone tests
""""""""""""""""""""""""""""

Stand-alone tests defined in parent (.e.g., :ref:`build-systems`) and
virtual (e.g., :ref:`virtual-dependencies`) packages are available to
packages that inherit from or provide interfaces for those packages,
respectively. The table below summarizes the tests that will be included
with those provided in the package itself when executing stand-alone tests.

.. list-table:: Inherited/provided stand-alone tests
   :header-rows: 1

   * - Parent/Provider Package
     - Stand-alone Tests
   * - `C
       <https://github.com/spack/spack/blob/develop/var/spack/repos/builtin/packages/c>`_
     - Compiles ``hello.c`` and runs it
   * - `Cxx
       <https://github.com/spack/spack/blob/develop/var/spack/repos/builtin/packages/cxx>`_
     - Compiles and runs several ``hello`` programs
   * - `Fortan
       <https://github.com/spack/spack/blob/develop/var/spack/repos/builtin/packages/fortran>`_
     - Compiles and runs ``hello`` programs (``F`` and ``f90``)
   * - `Mpi
       <https://github.com/spack/spack/blob/develop/var/spack/repos/builtin/packages/mpi>`_
     - Compiles and runs ``mpi_hello`` (``c``, ``fortran``)
   * - `PythonPackage <build_systems/pythonpackage>`
     - Imports installed modules

These tests are very generic so it is important that package
developers and maintainers provide additional stand-alone tests
customized to the package.

One example of a package that adds its own stand-alone (or smoke)
tests is the `Openmpi package
<https://github.com/spack/spack/blob/develop/var/spack/repos/builtin/packages/openmpi/package.py>`_.
The preliminary set of tests for the package performed the
following checks:

- installed binaries with the ``--version`` option return the expected
  version;
- outputs from (selected) installed binaries match expectations;
- ``make all`` succeeds when building examples that were copied from the
  source directory during package installation; and
- outputs from running the copied and built examples match expectations.

Below is an example of running and viewing the stand-alone tests,
where only the outputs for the first of each set are shown:

.. code-block:: console

   $ spack test run --alias openmpi-4.0.5 openmpi@4.0.5
   ==> Spack test openmpi-4.0.5
   ==> Testing package openmpi-4.0.5-eygjgve
   $ spack test results -l openmpi-4.0.5
   ==> Spack test openmpi-4.0.5
   ==> Testing package openmpi-4.0.5-eygjgve
   ==> Results for test suite 'openmpi-4.0.5':
   ==>   openmpi-4.0.5-eygjgve PASSED
   ==> Testing package openmpi-4.0.5-eygjgve
   ==> [2021-04-26-17:35:20.259650] test: ensuring version of mpiCC is 8.3.1
   ==> [2021-04-26-17:35:20.260155] '$SPACK_ROOT/opt/spack/linux-rhel7-broadwell/gcc-8.3.1/openmpi-4.0.5-eygjgvek35awfor2qaljltjind2oa67r/bin/mpiCC' '--version'
   g++ (GCC) 8.3.1 20190311 (Red Hat 8.3.1-3)
   Copyright (C) 2018 Free Software Foundation, Inc.
   This is free software; see the source for copying conditions.  There is NO
   warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

   PASSED
   ...
   ==> [2021-04-26-17:35:20.493921] test: checking mpirun output
   ==> [2021-04-26-17:35:20.494461] '$SPACK_ROOT/opt/spack/linux-rhel7-broadwell/gcc-8.3.1/openmpi-4.0.5-eygjgvek35awfor2qaljltjind2oa67r/bin/mpirun' '-n' '1' 'ls' '..'
   openmpi-4.0.5-eygjgve           repo     test_suite.lock
   openmpi-4.0.5-eygjgve-test-out.txt  results.txt
   PASSED
   ...
   ==> [2021-04-26-17:35:20.630452] test: ensuring ability to build the examples
   ==> [2021-04-26-17:35:20.630943] '/usr/bin/make' 'all'
   mpicc -g  hello_c.c  -o hello_c
   mpicc -g  ring_c.c  -o ring_c
   mpicc -g  connectivity_c.c  -o connectivity_c
   mpicc -g  spc_example.c  -o spc_example
   ...
   PASSED
   ==> [2021-04-26-17:35:23.291214] test: checking hello_c example output and status (0)
   ==> [2021-04-26-17:35:23.291841] './hello_c'
   Hello, world, I am 0 of 1, (Open MPI v4.0.5, package: Open MPI dahlgren@quartz2300 Distribution, ident: 4.0.5, repo rev: v4.0.5, Aug 26, 2020, 114)
   PASSED
   ...
   ==> [2021-04-26-17:35:24.603152] test: ensuring copied examples cleaned up
   ==> [2021-04-26-17:35:24.603807] '/usr/bin/make' 'clean'
   rm -f hello_c hello_cxx hello_mpifh hello_usempi hello_usempif08 hello_oshmem hello_oshmemcxx hello_oshmemfh Hello.class ring_c ring_cxx ring_mpifh ring_usempi ring_usempif08 ring_oshmem ring_oshmemfh Ring.class connectivity_c oshmem_shmalloc oshmem_circular_shift oshmem_max_reduction oshmem_strided_puts oshmem_symmetric_data spc_example *~ *.o
   PASSED
   ==> [2021-04-26-17:35:24.643360] test: mpicc: expect command status in [0]
   ==> [2021-04-26-17:35:24.643834] '$SPACK_ROOT/opt/spack/linux-rhel7-broadwell/gcc-8.3.1/openmpi-4.0.5-eygjgvek35awfor2qaljltjind2oa67r/bin/mpicc' '-o' 'mpi_hello_c' '$HOME/.spack/test/hyzq5eqlqfog6fawlzxwg3prqy5vjhms/openmpi-4.0.5-eygjgve/data/mpi/mpi_hello.c'
   PASSED
   ==> [2021-04-26-17:35:24.776765] test: mpirun: expect command status in [0]
   ==> [2021-04-26-17:35:24.777194] '$SPACK_ROOT/opt/spack/linux-rhel7-broadwell/gcc-8.3.1/openmpi-4.0.5-eygjgvek35awfor2qaljltjind2oa67r/bin/mpirun' '-np' '1' 'mpi_hello_c'
   Hello world! From rank 0 of 1
   PASSED
   ...


.. warning::

   The API for adding and running stand-alone tests is not yet considered
   stable and may change drastically in future releases. Packages with
   stand-alone tests will be refactored to match changes to the API.

.. _cmd-spack-test-list:

"""""""""""""""""""
``spack test list``
"""""""""""""""""""

Packages available for install testing can be found using the
``spack test list`` command. The command outputs all installed
packages that have defined ``test`` methods.

Alternatively you can use the ``--all`` option to get a list of
all packages that have defined ``test`` methods even if they are
not installed.

For more information, refer to `spack test list
<https://spack.readthedocs.io/en/latest/command_index.html#spack-test-list>`_.

.. _cmd-spack-test-run:

""""""""""""""""""
``spack test run``
""""""""""""""""""

Install tests can be run for one or more installed packages using
the ``spack test run`` command. A ``test suite`` is created from
the provided specs. If no specs are provided it will test all specs
in the active environment or all specs installed in Spack if no
environment is active.

Test suites can be named using the ``--alias`` option. Unaliased
Test suites will use the content hash of their specs as their name.

Some of the more commonly used debugging options are:

- ``--fail-fast`` stops testing each package after the first failure
- ``--fail-first`` stops testing packages after the first failure

Test output is written to a text log file by default but ``junit``
and ``cdash`` are outputs are available through the ``--log-format``
option.

For more information, refer to `spack test run
<https://spack.readthedocs.io/en/latest/command_index.html#spack-test-run>`_.


.. _cmd-spack-test-results:

""""""""""""""""""""""
``spack test results``
""""""""""""""""""""""

The ``spack test results`` command shows results for all completed
test suites. Providing the alias or content hash limits reporting
to the corresponding test suite.

The ``--logs`` option includes the output generated by the associated
test(s) to facilitate debugging.

The ``--failed`` option limits results shown to that of the failed
tests, if any, of matching packages.

For more information, refer to `spack test results
<https://spack.readthedocs.io/en/latest/command_index.html#spack-test-results>`_.

.. _cmd-spack-test-find:

"""""""""""""""""""
``spack test find``
"""""""""""""""""""

The ``spack test find`` command lists the aliases or content hashes
of all test suites whose results are available.

For more information, refer to `spack test find
<https://spack.readthedocs.io/en/latest/command_index.html#spack-test-find>`_.

.. _cmd-spack-test-remove:

"""""""""""""""""""""
``spack test remove``
"""""""""""""""""""""

The ``spack test remove`` command removes test suites to declutter
the test results directory. You are prompted to confirm the removal
of each test suite **unless** you use the ``--yes-to-all`` option.

For more information, refer to `spack test remove
<https://spack.readthedocs.io/en/latest/command_index.html#spack-test-remove>`_.

.. _file-manipulation:

---------------------------
File manipulation functions
---------------------------

Many builds are not perfect. If a build lacks an install target, or if
it does not use systems like CMake or autotools, which have standard
ways of setting compilers and options, you may need to edit files or
install some files yourself to get them working with Spack.

You can do this with standard Python code, and Python has rich
libraries with functions for file manipulation and filtering. Spack
also provides a number of convenience functions of its own to make
your life even easier. These functions are described in this section.

All of the functions in this section can be included by simply
running:

.. code-block:: python

   from spack import *

This is already part of the boilerplate for packages created with
``spack create``.

^^^^^^^^^^^^^^^^^^^
Filtering functions
^^^^^^^^^^^^^^^^^^^

:py:func:`filter_file(regex, repl, *filenames, **kwargs) <llnl.util.filesystem.filter_file>`
  Works like ``sed`` but with Python regular expression syntax.  Takes
  a regular expression, a replacement, and a set of files.  ``repl``
  can be a raw string or a callable function.  If it is a raw string,
  it can contain ``\1``, ``\2``, etc. to refer to capture groups in
  the regular expression.  If it is a callable, it is passed the
  Python ``MatchObject`` and should return a suitable replacement
  string for the particular match.

  Examples:

  #. Filtering a Makefile to force it to use Spack's compiler wrappers:

     .. code-block:: python

        filter_file(r'^\s*CC\s*=.*',  'CC = '  + spack_cc,  'Makefile')
        filter_file(r'^\s*CXX\s*=.*', 'CXX = ' + spack_cxx, 'Makefile')
        filter_file(r'^\s*F77\s*=.*', 'F77 = ' + spack_f77, 'Makefile')
        filter_file(r'^\s*FC\s*=.*',  'FC = '  + spack_fc,  'Makefile')

  #. Replacing ``#!/usr/bin/perl`` with ``#!/usr/bin/env perl`` in ``bib2xhtml``:

     .. code-block:: python

        filter_file(r'#!/usr/bin/perl',
                    '#!/usr/bin/env perl', prefix.bin.bib2xhtml)

  #. Switching the compilers used by ``mpich``'s MPI wrapper scripts from
     ``cc``, etc. to the compilers used by the Spack build:

     .. code-block:: python

        filter_file('CC="cc"', 'CC="%s"' % self.compiler.cc,
                    prefix.bin.mpicc)

        filter_file('CXX="c++"', 'CXX="%s"' % self.compiler.cxx,
                    prefix.bin.mpicxx)

:py:func:`change_sed_delimiter(old_delim, new_delim, *filenames) <llnl.util.filesystem.change_sed_delimiter>`
    Some packages, like TAU, have a build system that can't install
    into directories with, e.g. '@' in the name, because they use
    hard-coded ``sed`` commands in their build.

    ``change_sed_delimiter`` finds all ``sed`` search/replace commands
    and change the delimiter.  e.g., if the file contains commands
    that look like ``s///``, you can use this to change them to
    ``s@@@``.

    Example of changing ``s///`` to ``s@@@`` in TAU:

    .. code-block:: python

       change_sed_delimiter('@', ';', 'configure')
       change_sed_delimiter('@', ';', 'utils/FixMakefile')
       change_sed_delimiter('@', ';', 'utils/FixMakefile.sed.default')

^^^^^^^^^^^^^^
File functions
^^^^^^^^^^^^^^

:py:func:`ancestor(dir, n=1) <llnl.util.filesystem.ancestor>`
  Get the n\ :sup:`th` ancestor of the directory ``dir``.

:py:func:`can_access(path) <llnl.util.filesystem.can_access>`
  True if we can read and write to the file at ``path``.  Same as
  native python ``os.access(file_name, os.R_OK|os.W_OK)``.

:py:func:`install(src, dest) <llnl.util.filesystem.install>`
  Install a file to a particular location.  For example, install a
  header into the ``include`` directory under the install ``prefix``:

  .. code-block:: python

     install('my-header.h', prefix.include)

:py:func:`join_path(*paths) <llnl.util.filesystem.join_path>`
  An alias for ``os.path.join``. This joins paths using the OS path separator.

:py:func:`mkdirp(*paths) <llnl.util.filesystem.mkdirp>`
  Create each of the directories in ``paths``, creating any parent
  directories if they do not exist.

:py:func:`working_dir(dirname, kwargs) <llnl.util.filesystem.working_dir>`
  This is a Python `Context Manager
  <https://docs.python.org/2/library/contextlib.html>`_ that makes it
  easier to work with subdirectories in builds.  You use this with the
  Python ``with`` statement to change into a working directory, and
  when the with block is done, you change back to the original
  directory.  Think of it as a safe ``pushd`` / ``popd`` combination,
  where ``popd`` is guaranteed to be called at the end, even if
  exceptions are thrown.

  Example usage:

  #. The ``libdwarf`` build first runs ``configure`` and ``make`` in a
     subdirectory called ``libdwarf``.  It then implements the
     installation code itself.  This is natural with ``working_dir``:

     .. code-block:: python

        with working_dir('libdwarf'):
            configure("--prefix=" + prefix, "--enable-shared")
            make()
            install('libdwarf.a',  prefix.lib)

  #. Many CMake builds require that you build "out of source", that
     is, in a subdirectory.  You can handle creating and ``cd``'ing to
     the subdirectory like the LLVM package does:

     .. code-block:: python

        with working_dir('spack-build', create=True):
            cmake('..',
                  '-DLLVM_REQUIRES_RTTI=1',
                  '-DPYTHON_EXECUTABLE=/usr/bin/python',
                  '-DPYTHON_INCLUDE_DIR=/usr/include/python2.6',
                  '-DPYTHON_LIBRARY=/usr/lib64/libpython2.6.so',
                  *std_cmake_args)
            make()
            make("install")

     The ``create=True`` keyword argument causes the command to create
     the directory if it does not exist.

:py:func:`touch(path) <llnl.util.filesystem.touch>`
  Create an empty file at ``path``.

.. _make-package-findable:

----------------------------------------------------------
Making a package discoverable with ``spack external find``
----------------------------------------------------------

The simplest way to make a package discoverable with
:ref:`spack external find <cmd-spack-external-find>` is to:

1. Define the executables associated with the package
2. Implement a method to determine the versions of these executables

^^^^^^^^^^^^^^^^^
Minimal detection
^^^^^^^^^^^^^^^^^

The first step is fairly simple, as it requires only to
specify a package level ``executables`` attribute:

.. code-block:: python

   class Foo(Package):
       # Each string provided here is treated as a regular expression, and
       # would match for example 'foo', 'foobar', and 'bazfoo'.
       executables = ['foo']

This attribute must be a list of strings. Each string is a regular
expression (e.g. 'gcc' would match 'gcc', 'gcc-8.3', 'my-weird-gcc', etc.) to
determine a set of system executables that might be part or this package. Note
that to match only executables named 'gcc' the regular expression ``'^gcc$'``
must be used.

Finally to determine the version of each executable the ``determine_version``
method must be implemented:

.. code-block:: python

   @classmethod
   def determine_version(cls, exe):
       """Return either the version of the executable passed as argument
       or ``None`` if the version cannot be determined.

       Args:
           exe (str): absolute path to the executable being examined
       """

This method receives as input the path to a single executable and must return
as output its version as a string; if the user cannot determine the version
or determines that the executable is not an instance of the package, they can
return None and the exe will be discarded as a candidate.
Implementing the two steps above is mandatory, and gives the package the
basic ability to detect if a spec is present on the system at a given version.

.. note::
   Any executable for which the ``determine_version`` method returns ``None``
   will be discarded and won't appear in later stages of the workflow described below.

^^^^^^^^^^^^^^^^^^^^^^^^
Additional functionality
^^^^^^^^^^^^^^^^^^^^^^^^

Besides the two mandatory steps described above, there are also optional
methods that can be implemented to either increase the amount of details
being detected or improve the robustness of the detection logic in a package.

""""""""""""""""""""""""""""""
Variants and custom attributes
""""""""""""""""""""""""""""""

The ``determine_variants`` method can be optionally implemented in a package
to detect additional details of the spec:

.. code-block:: python

   @classmethod
   def determine_variants(cls, exes, version_str):
       """Return either a variant string, a tuple of a variant string
       and a dictionary of extra attributes that will be recorded in
       packages.yaml or a list of those items.

       Args:
           exes (list of str): list of executables (absolute paths) that
               live in the same prefix and share the same version
           version_str (str): version associated with the list of
               executables, as detected by ``determine_version``
       """

This method takes as input a list of executables that live in the same prefix and
share the same version string, and returns either:

1. A variant string
2. A tuple of a variant string and a dictionary of extra attributes
3. A list of items matching either 1 or 2 (if multiple specs are detected
   from the set of executables)

If extra attributes are returned, they will be recorded in ``packages.yaml``
and be available for later reuse. As an example, the ``gcc`` package will record
by default the different compilers found and an entry in ``packages.yaml``
would look like:

.. code-block:: yaml

   packages:
     gcc:
       externals:
       - spec: 'gcc@9.0.1 languages=c,c++,fortran'
         prefix: /usr
         extra_attributes:
           compilers:
             c: /usr/bin/x86_64-linux-gnu-gcc-9
             c++: /usr/bin/x86_64-linux-gnu-g++-9
             fortran: /usr/bin/x86_64-linux-gnu-gfortran-9

This allows us, for instance, to keep track of executables that would be named
differently if built by Spack (e.g. ``x86_64-linux-gnu-gcc-9``
instead of just ``gcc``).

.. TODO: we need to gather some more experience on overriding 'prefix'
   and other special keywords in extra attributes, but as soon as we are
   confident that this is the way to go we should document the process.
   See https://github.com/spack/spack/pull/16526#issuecomment-653783204

"""""""""""""""""""""""""""
Filter matching executables
"""""""""""""""""""""""""""

Sometimes defining the appropriate regex for the ``executables``
attribute might prove to be difficult, especially if one has to
deal with corner cases or exclude "red herrings". To help keeping
the regular expressions as simple as possible, each package can
optionally implement a ``filter_executables`` method:

.. code-block:: python

    @classmethod
    def filter_detected_exes(cls, prefix, exes_in_prefix):
        """Return a filtered list of the executables in prefix"""

which takes as input a prefix and a list of matching executables and
returns a filtered list of said executables.

Using this method has the advantage of allowing custom logic for
filtering, and does not restrict the user to regular expressions
only.  Consider the case of detecting the GNU C++ compiler. If we
try to search for executables that match ``g++``, that would have
the unwanted side effect of selecting also ``clang++`` - which is
a C++ compiler provided by another package - if present on the system.
Trying to select executables that contain ``g++`` but not ``clang``
would be quite complicated to do using regex only. Employing the
``filter_detected_exes`` method it becomes:

.. code-block:: python

   class Gcc(Package):
      executables = ['g++']

      def filter_detected_exes(cls, prefix, exes_in_prefix):
         return [x for x in exes_in_prefix if 'clang' not in x]

Another possibility that this method opens is to apply certain
filtering logic when specific conditions are met (e.g. take some
decisions on an OS and not on another).

^^^^^^^^^^^^^^^^^^
Validate detection
^^^^^^^^^^^^^^^^^^

To increase detection robustness, packagers may also implement a method
to validate the detected Spec objects:

.. code-block:: python

   @classmethod
   def validate_detected_spec(cls, spec, extra_attributes):
       """Validate a detected spec. Raise an exception if validation fails."""

This method receives a detected spec along with its extra attributes and can be
used to check that certain conditions are met by the spec. Packagers can either
use assertions or raise an ``InvalidSpecDetected`` exception when the check fails.
In case the conditions are not honored the spec will be discarded and any message
associated with the assertion or the exception will be logged as the reason for
discarding it.

As an example, a package that wants to check that the ``compilers`` attribute is
in the extra attributes can implement this method like this:

.. code-block:: python

   @classmethod
   def validate_detected_spec(cls, spec, extra_attributes):
       """Check that 'compilers' is in the extra attributes."""
       msg = ('the extra attribute "compilers" must be set for '
              'the detected spec "{0}"'.format(spec))
       assert 'compilers' in extra_attributes, msg

or like this:

.. code-block:: python

   @classmethod
   def validate_detected_spec(cls, spec, extra_attributes):
       """Check that 'compilers' is in the extra attributes."""
       if 'compilers' not in extra_attributes:
           msg = ('the extra attribute "compilers" must be set for '
                  'the detected spec "{0}"'.format(spec))
           raise InvalidSpecDetected(msg)

.. _determine_spec_details:

^^^^^^^^^^^^^^^^^^^^^^^^^
Custom detection workflow
^^^^^^^^^^^^^^^^^^^^^^^^^

In the rare case when the mechanisms described so far don't fit the
detection of a package, the implementation of all the methods above
can be disregarded and instead a custom ``determine_spec_details``
method can be implemented directly in the package class (note that
the definition of the ``executables`` attribute is still required):

.. code-block:: python

   @classmethod
   def determine_spec_details(cls, prefix, exes_in_prefix):
       # exes_in_prefix = a set of paths, each path is an executable
       # prefix = a prefix that is common to each path in exes_in_prefix

       # return None or [] if none of the exes represent an instance of
       # the package. Return one or more Specs for each instance of the
       # package which is thought to be installed in the provided prefix

This method takes as input a set of discovered executables (which match
those specified by the user) as well as a common prefix shared by all
of those executables. The function must return one or more :py:class:`spack.spec.Spec` associated
with the executables (it can also return ``None`` to indicate that no
provided executables are associated with the package).

As an example, consider a made-up package called ``foo-package`` which
builds an executable called ``foo``. ``FooPackage`` would appear as
follows:

.. code-block:: python

   class FooPackage(Package):
       homepage = "..."
       url = "..."

       version(...)

       # Each string provided here is treated as a regular expression, and
       # would match for example 'foo', 'foobar', and 'bazfoo'.
       executables = ['foo']

       @classmethod
       def determine_spec_details(cls, prefix, exes_in_prefix):
           candidates = list(x for x in exes_in_prefix
                             if os.path.basename(x) == 'foo')
           if not candidates:
               return
           # This implementation is lazy and only checks the first candidate
           exe_path = candidates[0]
           exe = Executable(exe_path)
           output = exe('--version', output=str, error=str)
           version_str = ...  # parse output for version string
           return Spec.from_detection(
               'foo-package@{0}'.format(version_str)
           )

.. _package-lifecycle:

-----------------------------
Style guidelines for packages
-----------------------------

The following guidelines are provided, in the interests of making
Spack packages work in a consistent manner:

^^^^^^^^^^^^^
Variant Names
^^^^^^^^^^^^^

Spack packages with variants similar to already-existing Spack
packages should use the same name for their variants.  Standard
variant names are:

  ======= ======== ========================
  Name    Default   Description
  ======= ======== ========================
  shared   True     Build shared libraries
  mpi      True     Use MPI
  python   False    Build Python extension
  ======= ======== ========================

If specified in this table, the corresponding default should be used
when declaring a variant.

The semantics of the `shared` variant are important. When a package is
built `~shared`, the package guarantees that no shared libraries are
built. When a package is built `+shared`, the package guarantees that
shared libraries are built, but it makes no guarantee about whether
static libraries are built.

^^^^^^^^^^^^^
Version Lists
^^^^^^^^^^^^^

Spack packages should list supported versions with the newest first.

---------------------------
Packaging workflow commands
---------------------------

When you are building packages, you will likely not get things
completely right the first time.

The ``spack install`` command performs a number of tasks before it
finally installs each package.  It downloads an archive, expands it in
a temporary directory, and only then gives control to the package's
``install()`` method.  If the build doesn't go as planned, you may
want to clean up the temporary directory, or if the package isn't
downloading properly, you might want to run *only* the ``fetch`` stage
of the build.

Spack performs best-effort installation of package dependencies by default,
which means it will continue to install as many dependencies as possible
after detecting failures.  If you are trying to install a package with a
lot of dependencies where one or more may fail to build, you might want to
try the ``--fail-fast`` option to stop the installation process on the first
failure.

A typical package workflow might look like this:

.. code-block:: console

   $ spack edit mypackage
   $ spack install --fail-fast mypackage
   ... build breaks! ...
   $ spack clean mypackage
   $ spack edit mypackage
   $ spack install --fail-fast mypackage
   ... repeat clean/install until install works ...

Below are some commands that will allow you some finer-grained
control over the install process.

.. _cmd-spack-fetch:

^^^^^^^^^^^^^^^
``spack fetch``
^^^^^^^^^^^^^^^

The first step of ``spack install``.  Takes a spec and determines the
correct download URL to use for the requested package version, then
downloads the archive, checks it against an MD5 checksum, and stores
it in a staging directory if the check was successful.  The staging
directory will be located under the first writable directory in the
``build_stage`` configuration setting.

When run after the archive has already been downloaded, ``spack
fetch`` is idempotent and will not download the archive again.

.. _cmd-spack-stage:

^^^^^^^^^^^^^^^
``spack stage``
^^^^^^^^^^^^^^^

The second step in ``spack install`` after ``spack fetch``.  Expands
the downloaded archive in its temporary directory, where it will be
built by ``spack install``.  Similar to ``fetch``, if the archive has
already been expanded,  ``stage`` is idempotent.

.. _cmd-spack-patch:

^^^^^^^^^^^^^^^
``spack patch``
^^^^^^^^^^^^^^^

After staging, Spack applies patches to downloaded packages, if any
have been specified in the package file.  This command will run the
install process through the fetch, stage, and patch phases.  Spack
keeps track of whether patches have already been applied and skips
this step if they have been.  If Spack discovers that patches didn't
apply cleanly on some previous run, then it will restage the entire
package before patching.

.. _cmd-spack-restage:

^^^^^^^^^^^^^^^^^
``spack restage``
^^^^^^^^^^^^^^^^^

Restores the source code to pristine state, as it was before building.

Does this in one of two ways:

#. If the source was fetched as a tarball, deletes the entire build
   directory and re-expands the tarball.

#. If the source was checked out from a repository, this deletes the
   build directory and checks it out again.

.. _cmd-spack-clean:

^^^^^^^^^^^^^^^
``spack clean``
^^^^^^^^^^^^^^^

Cleans up Spack's temporary and cached files.  This command can be used to
recover disk space if temporary files from interrupted or failed installs
accumulate.

When called with ``--stage`` or without arguments this removes all staged
files.

The ``--downloads`` option removes cached :ref:`cached <caching>` downloads.

You can force the removal of all install failure tracking markers using the
``--failures`` option.  Note that ``spack install`` will automatically clear
relevant failure markings prior to performing the requested installation(s).

Long-lived caches, like the virtual package index, are removed using the
``--misc-cache`` option.

The ``--python-cache`` option removes `.pyc`, `.pyo`, and `__pycache__`
folders.

To remove all of the above, the command can be called with ``--all``.

When called with positional arguments, this command cleans up temporary files
only for a particular package. If ``fetch``, ``stage``, or ``install``
are run again after this, Spack's build process will start from scratch.


^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Keeping the stage directory on success
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

By default, ``spack install`` will delete the staging area once a
package has been successfully built and installed.  Use
``--keep-stage`` to leave the build directory intact:

.. code-block:: console

   $ spack install --keep-stage <spec>

This allows you to inspect the build directory and potentially debug
the build.  You can use ``clean`` later to get rid of the
unwanted temporary files.

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Keeping the install prefix on failure
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

By default, ``spack install`` will delete any partially constructed
install prefix if anything fails during ``install()``.  If you want to
keep the prefix anyway (e.g. to diagnose a bug), you can use
``--keep-prefix``:

.. code-block:: console

   $ spack install --keep-prefix <spec>

Note that this may confuse Spack into thinking that the package has
been installed properly, so you may need to use ``spack uninstall --force``
to get rid of the install prefix before you build again:

.. code-block:: console

   $ spack uninstall --force <spec>

---------------------
Graphing dependencies
---------------------

.. _cmd-spack-graph:

^^^^^^^^^^^^^^^
``spack graph``
^^^^^^^^^^^^^^^

Spack provides the ``spack graph`` command for graphing dependencies.
The command by default generates an ASCII rendering of a spec's
dependency graph.  For example:

.. command-output:: spack graph hdf5

At the top is the root package in the DAG, with dependency edges emerging
from it.  On a color terminal, the edges are colored by which dependency
they lead to.

.. command-output:: spack graph --deptype=link hdf5

The ``deptype`` argument tells Spack what types of dependencies to graph.
By default it includes link and run dependencies but not build
dependencies.  Supplying ``--deptype=link`` will show only link
dependencies.  The default is ``--deptype=all``, which is equivalent to
``--deptype=build,link,run,test``.  Options for ``deptype`` include:

* Any combination of ``build``, ``link``, ``run``, and ``test`` separated
  by commas.
* ``all`` for all types of dependencies.

You can also use ``spack graph`` to generate graphs in the widely used
`Dot <http://www.graphviz.org/doc/info/lang.html>`_ format.  For example:

.. command-output:: spack graph --dot hdf5

This graph can be provided as input to other graphing tools, such as
those in `Graphviz <http://www.graphviz.org>`_.  If you have graphviz
installed, you can write straight to PDF like this:

.. code-block:: console

   $ spack graph --dot hdf5 | dot -Tpdf > hdf5.pdf

.. _packaging-shell-support:

-------------------------
Interactive shell support
-------------------------

Spack provides some limited shell support to make life easier for
packagers.  You can enable these commands by sourcing a setup file in
the ``share/spack`` directory.  For ``bash`` or ``ksh``, run:

.. code-block:: sh

   export SPACK_ROOT=/path/to/spack
   . $SPACK_ROOT/share/spack/setup-env.sh

For ``csh`` and ``tcsh`` run:

.. code-block:: csh

   setenv SPACK_ROOT /path/to/spack
   source $SPACK_ROOT/share/spack/setup-env.csh

``spack cd`` will then be available.

.. _cmd-spack-cd:

^^^^^^^^^^^^
``spack cd``
^^^^^^^^^^^^

``spack cd`` allows you to quickly cd to pertinent directories in Spack.
Suppose you've staged a package but you want to modify it before you
build it:

.. code-block:: console

   $ spack stage libelf
   ==> Trying to fetch from http://www.mr511.de/software/libelf-0.8.13.tar.gz
   ######################################################################## 100.0%
   ==> Staging archive: ~/spack/var/spack/stage/libelf@0.8.13%gcc@4.8.3 arch=linux-debian7-x86_64/libelf-0.8.13.tar.gz
   ==> Created stage in ~/spack/var/spack/stage/libelf@0.8.13%gcc@4.8.3 arch=linux-debian7-x86_64.
   $ spack cd libelf
   $ pwd
   ~/spack/var/spack/stage/libelf@0.8.13%gcc@4.8.3 arch=linux-debian7-x86_64/libelf-0.8.13

``spack cd`` here changed the current working directory to the
directory containing the expanded ``libelf`` source code.  There are a
number of other places you can cd to in the spack directory hierarchy:

.. command-output:: spack cd --help

Some of these change directory into package-specific locations (stage
directory, install directory, package directory) and others change to
core spack locations.  For example, ``spack cd --module-dir`` will take you to
the main python source directory of your spack install.

.. _cmd-spack-build-env:

^^^^^^^^^^^^^^^^^^^
``spack build-env``
^^^^^^^^^^^^^^^^^^^

``spack build-env`` functions much like the standard unix ``build-env``
command, but it takes a spec as an argument.  You can use it to see the
environment variables that will be set when a particular build runs,
for example:

.. code-block:: console

   $ spack build-env mpileaks@1.1%intel

This will display the entire environment that will be set when the
``mpileaks@1.1%intel`` build runs.

To run commands in a package's build environment, you can simply
provide them after the spec argument to ``spack build-env``:

.. code-block:: console

   $ spack cd mpileaks@1.1%intel
   $ spack build-env mpileaks@1.1%intel ./configure

This will cd to the build directory and then run ``configure`` in the
package's build environment.

.. _cmd-spack-location:

^^^^^^^^^^^^^^^^^^
``spack location``
^^^^^^^^^^^^^^^^^^

``spack location`` is the same as ``spack cd`` but it does not require
shell support.  It simply prints out the path you ask for, rather than
cd'ing to it.  In bash, this:

.. code-block:: console

   $ cd $(spack location --build-dir <spec>)

is the same as:

.. code-block:: console

   $ spack cd --build-dir <spec>

``spack location`` is intended for use in scripts or makefiles that
need to know where packages are installed.  e.g., in a makefile you
might write:

.. code-block:: makefile

   DWARF_PREFIX = $(spack location --install-dir libdwarf)
   CXXFLAGS += -I$DWARF_PREFIX/include
   CXXFLAGS += -L$DWARF_PREFIX/lib
.. Copyright 2013-2022 Lawrence Livermore National Security, LLC and other
   Spack Project Developers. See the top-level COPYRIGHT file for details.

   SPDX-License-Identifier: (Apache-2.0 OR MIT)

.. _binary_caches:

============
Build Caches
============

Some sites may encourage users to set up their own test environments
before carrying out central installations, or some users may prefer to set
up these environments on their own motivation. To reduce the load of
recompiling otherwise identical package specs in different installations,
installed packages can be put into build cache tarballs, uploaded to
your Spack mirror and then downloaded and installed by others.


--------------------------
Creating build cache files
--------------------------

A compressed tarball of an installed package is created. Tarballs are created
for all of its link and run dependency packages as well. Compressed tarballs are
signed with gpg and signature and tarball and put in a ``.spack`` file. Optionally,
the rpaths (and ids and deps on macOS) can be changed to paths relative to
the Spack install tree before the tarball is created.

Build caches are created via:

.. code-block:: console

    $ spack buildcache create <spec>


If you wanted to create a build cache in a local directory, you would provide
the ``-d`` argument to target that directory, again also specifying the spec.
Here is an example creating a local directory, "spack-cache" and creating
build cache files for the "ninja" spec:

.. code-block:: console

    $ mkdir -p ./spack-cache
    $ spack buildcache create -d ./spack-cache ninja
    ==> Buildcache files will be output to file:///home/spackuser/spack/spack-cache/build_cache
    gpgconf: socketdir is '/run/user/1000/gnupg'
    gpg: using "E6DF6A8BD43208E4D6F392F23777740B7DBD643D" as default secret key for signing

Note that the targeted spec must already be installed. Once you have a build cache,
you can add it as a mirror, discussed next.

---------------------------------------
Finding or installing build cache files
---------------------------------------

To find build caches or install build caches, a Spack mirror must be configured
with:

.. code-block:: console

    $ spack mirror add <name> <url>


Note that the url can be a web url _or_ a local filesystem location. In the previous
example, you might add the directory "spack-cache" and call it ``mymirror``:


.. code-block:: console

    $ spack mirror add mymirror ./spack-cache


You can see that the mirror is added with ``spack mirror list`` as follows:

.. code-block:: console


    $ spack mirror list
    mymirror           file:///home/spackuser/spack/spack-cache
    spack-public       https://spack-llnl-mirror.s3-us-west-2.amazonaws.com/


At this point, you've create a buildcache, but spack hasn't indexed it, so if
you run ``spack buildcache list`` you won't see any results. You need to index
this new build cache as follows:

.. code-block:: console

    $ spack buildcache update-index -d spack-cache/

Now you can use list:

.. code-block:: console

    $  spack buildcache list
    ==> 1 cached build.
    -- linux-ubuntu20.04-skylake / gcc@9.3.0 ------------------------
    ninja@1.10.2


Great! So now let's say you have a different spack installation, or perhaps just
a different environment for the same one, and you want to install a package from
that build cache. Let's first uninstall the actual library "ninja" to see if we can
re-install it from the cache.

.. code-block:: console

    $ spack uninstall ninja


And now reinstall from the buildcache

.. code-block:: console

    $ spack buildcache install ninja
    ==> buildcache spec(s) matching ninja 
    ==> Fetching file:///home/spackuser/spack/spack-cache/build_cache/linux-ubuntu20.04-skylake/gcc-9.3.0/ninja-1.10.2/linux-ubuntu20.04-skylake-gcc-9.3.0-ninja-1.10.2-i4e5luour7jxdpc3bkiykd4imke3mkym.spack
    ####################################################################################################################################### 100.0%
    ==> Installing buildcache for spec ninja@1.10.2%gcc@9.3.0 arch=linux-ubuntu20.04-skylake
    gpgconf: socketdir is '/run/user/1000/gnupg'
    gpg: Signature made Tue 23 Mar 2021 10:16:29 PM MDT
    gpg:                using RSA key E6DF6A8BD43208E4D6F392F23777740B7DBD643D
    gpg: Good signature from "spackuser (GPG created for Spack) <spackuser@noreply.users.github.com>" [ultimate]


It worked! You've just completed a full example of creating a build cache with
a spec of interest, adding it as a mirror, updating it's index, listing the contents,
and finally, installing from it.


Note that the above command is intended to install a particular package to a
build cache you have created, and not to install a package from a build cache.
For the latter, once a mirror is added, by default when you do ``spack install`` the ``--use-cache``
flag is set, and you will install a package from a build cache if it is available.
If you want to always use the cache, you can do:

.. code-block:: console

   $ spack install --cache-only <package>

For example, to combine all of the commands above to add the E4S build cache
and then install from it exclusively, you would do:

.. code-block:: console

    $ spack mirror add E4S https://cache.e4s.io
    $ spack buildcache keys --install --trust
    $ spack install --cache-only <package>

We use ``--install`` and ``--trust`` to say that we are installing keys to our
keyring, and trusting all downloaded keys.


^^^^^^^^^^^^^^^^^^^^^^^^^^^^
List of popular build caches
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

* `Extreme-scale Scientific Software Stack (E4S) <https://e4s-project.github.io/>`_: `build cache <https://oaciss.uoregon.edu/e4s/inventory.html>`_


----------
Relocation
----------

Initial build and later installation do not necessarily happen at the same
location. Spack provides a relocation capability and corrects for RPATHs and
non-relocatable scripts. However, many packages compile paths into binary
artifacts directly. In such cases, the build instructions of this package would
need to be adjusted for better re-locatability.

.. _cmd-spack-buildcache:

--------------------
``spack buildcache``
--------------------

^^^^^^^^^^^^^^^^^^^^^^^^^^^
``spack buildcache create``
^^^^^^^^^^^^^^^^^^^^^^^^^^^

Create tarball of installed Spack package and all dependencies.
Tarballs are checksummed and signed if gpg2 is available.
Places them in a directory ``build_cache`` that can be copied to a mirror.
Commands like ``spack buildcache install`` will search Spack mirrors for build_cache to get the list of build caches.

==============  ========================================================================================================================
Arguments       Description
==============  ========================================================================================================================
``<specs>``     list of partial specs or hashes with a leading ``/`` to match from installed packages and used for creating build caches
``-d <path>``   directory in which ``build_cache`` directory is created, defaults to ``.``
``-f``          overwrite ``.spack`` file in ``build_cache`` directory if it exists
``-k <key>``    the key to sign package with. In the case where multiple keys exist, the package will be unsigned unless ``-k`` is used.
``-r``          make paths in binaries relative before creating tarball
``-y``          answer yes to all create unsigned ``build_cache`` questions
==============  ========================================================================================================================

^^^^^^^^^^^^^^^^^^^^^^^^^
``spack buildcache list``
^^^^^^^^^^^^^^^^^^^^^^^^^

Retrieves all specs for build caches available on a Spack mirror.

==============  =====================================================================================
Arguments       Description
==============  =====================================================================================
``<specs>``     list of partial package specs to be matched against specs downloaded for build caches
==============  =====================================================================================

E.g. ``spack buildcache list gcc`` with print only commands to install ``gcc`` package(s)

^^^^^^^^^^^^^^^^^^^^^^^^^^^^
``spack buildcache install``
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Retrieves all specs for build caches available on a Spack mirror and installs build caches
with specs matching the specs input.

==============  ==============================================================================================
Arguments       Description
==============  ==============================================================================================
``<specs>``     list of partial package specs or hashes with a leading ``/`` to be installed from build caches
``-f``          remove install directory if it exists before unpacking tarball
``-y``          answer yes to all to don't verify package with gpg questions
==============  ==============================================================================================

^^^^^^^^^^^^^^^^^^^^^^^^^
``spack buildcache keys``
^^^^^^^^^^^^^^^^^^^^^^^^^

List public keys available on Spack mirror.

=========  ==============================================
Arguments  Description
=========  ==============================================
``-i``     trust the keys downloaded with prompt for each
``-y``     answer yes to all trust all keys downloaded
=========  ==============================================
.. Copyright 2013-2022 Lawrence Livermore National Security, LLC and other
   Spack Project Developers. See the top-level COPYRIGHT file for details.

   SPDX-License-Identifier: (Apache-2.0 OR MIT)

.. _environments:

============
Environments
============

An environment is used to group together a set of specs for the
purpose of building, rebuilding and deploying in a coherent fashion.
Environments provide a number of advantages over the *Ã  la carte*
approach of building and loading individual Spack modules:

#. Environments separate the steps of (a) choosing what to
   install, (b) concretizing, and (c) installing.  This allows
   Environments to remain stable and repeatable, even if Spack packages
   are upgraded: specs are only re-concretized when the user
   explicitly asks for it.  It is even possible to reliably
   transport environments between different computers running
   different versions of Spack!
#. Environments allow several specs to be built at once; a more robust
   solution than ad-hoc scripts making multiple calls to ``spack
   install``.
#. An Environment that is built as a whole can be loaded as a whole
   into the user environment. An Environment can be built to maintain
   a filesystem view of its packages, and the environment can load
   that view into the user environment at activation time. Spack can
   also generate a script to load all modules related to an
   environment.

Other packaging systems also provide environments that are similar in
some ways to Spack environments; for example, `Conda environments
<https://conda.io/docs/user-guide/tasks/manage-environments.html>`_ or
`Python Virtual Environments
<https://docs.python.org/3/tutorial/venv.html>`_.  Spack environments
provide some distinctive features:

#. A spec installed "in" an environment is no different from the same
   spec installed anywhere else in Spack.  Environments are assembled
   simply by collecting together a set of specs.
#. Spack Environments may contain more than one spec of the same
   package.

Spack uses a "manifest and lock" model similar to `Bundler gemfiles
<https://bundler.io/man/gemfile.5.html>`_ and other package
managers. The user input file is named ``spack.yaml`` and the lock
file is named ``spack.lock``

.. _environments-using:

------------------
Using Environments
------------------

Here we follow a typical use case of creating, concretizing,
installing and loading an environment.

^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Creating a named Environment
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

An environment is created by:

.. code-block:: console

   $ spack env create myenv

Spack then creates the directory ``var/spack/environments/myenv``.

.. note::

   All named environments are stored in the ``var/spack/environments`` folder.

In the ``var/spack/environments/myenv`` directory, Spack creates the
file ``spack.yaml`` and the hidden directory ``.spack-env``.

Spack stores metadata in the ``.spack-env`` directory. User
interaction will occur through the ``spack.yaml`` file and the Spack
commands that affect it. When the environment is concretized, Spack
will create a file ``spack.lock`` with the concrete information for
the environment.

In addition to being the default location for the view associated with
an Environment, the ``.spack-env`` directory also contains:

  * ``repo/``: A repo consisting of the Spack packages used in this
    environment.  This allows the environment to build the same, in
    theory, even on different versions of Spack with different
    packages!
  * ``logs/``: A directory containing the build logs for the packages
    in this Environment.

Spack Environments can also be created from either a ``spack.yaml``
manifest or a ``spack.lock`` lockfile. To create an Environment from a
``spack.yaml`` manifest:

.. code-block:: console

   $ spack env create myenv spack.yaml

To create an Environment from a ``spack.lock`` lockfile:

.. code-block:: console

   $ spack env create myenv spack.lock

Either of these commands can also take a full path to the
initialization file.

A Spack Environment created from a ``spack.yaml`` manifest is
guaranteed to have the same root specs as the original Environment,
but may concretize differently. A Spack Environment created from a
``spack.lock`` lockfile is guaranteed to have the same concrete specs
as the original Environment. Either may obviously then differ as the
user modifies it.

^^^^^^^^^^^^^^^^^^^^^^^^^
Activating an Environment
^^^^^^^^^^^^^^^^^^^^^^^^^

To activate an environment, use the following command:

.. code-block:: console

   $ spack env activate myenv

By default, the ``spack env activate`` will load the view associated
with the Environment into the user environment. The ``-v,
--with-view`` argument ensures this behavior, and the ``-V,
--without-view`` argument activates the environment without changing
the user environment variables.

The ``-p`` option to the ``spack env activate`` command modifies the
user's prompt to begin with the environment name in brackets.

.. code-block:: console

   $ spack env activate -p myenv
   [myenv] $ ...

To deactivate an environment, use the command:

.. code-block:: console

   $ spack env deactivate

or the shortcut alias

.. code-block:: console

   $ despacktivate

If the environment was activated with its view, deactivating the
environment will remove the view from the user environment.

^^^^^^^^^^^^^^^^^^^^^^
Anonymous Environments
^^^^^^^^^^^^^^^^^^^^^^

Any directory can be treated as an environment if it contains a file
``spack.yaml``. To load an anonymous environment, use:

.. code-block:: console

   $ spack env activate -d /path/to/directory

Anonymous specs can be created in place using the command:

.. code-block:: console

   $ spack env create -d .

In this case Spack simply creates a spack.yaml file in the requested
directory.

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Environment Sensitive Commands
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Spack commands are environment sensitive. For example, the ``find``
command shows only the specs in the active Environment if an
Environment has been activated. Similarly, the ``install`` and
``uninstall`` commands act on the active environment.

.. code-block:: console

  $ spack find
  ==> 0 installed packages

  $ spack install zlib@1.2.11
  ==> Installing zlib-1.2.11-q6cqrdto4iktfg6qyqcc5u4vmfmwb7iv
  ==> No binary for zlib-1.2.11-q6cqrdto4iktfg6qyqcc5u4vmfmwb7iv found: installing from source
  ==> zlib: Executing phase: 'install'
  [+] ~/spack/opt/spack/linux-rhel7-broadwell/gcc-8.1.0/zlib-1.2.11-q6cqrdto4iktfg6qyqcc5u4vmfmwb7iv

  $ spack env activate myenv

  $ spack find
  ==> In environment myenv
  ==> No root specs
  ==> 0 installed packages

  $ spack install zlib@1.2.8
  ==> Installing zlib-1.2.8-yfc7epf57nsfn2gn4notccaiyxha6z7x
  ==> No binary for zlib-1.2.8-yfc7epf57nsfn2gn4notccaiyxha6z7x found: installing from source
  ==> zlib: Executing phase: 'install'
  [+] ~/spack/opt/spack/linux-rhel7-broadwell/gcc-8.1.0/zlib-1.2.8-yfc7epf57nsfn2gn4notccaiyxha6z7x
  ==> Updating view at ~/spack/var/spack/environments/myenv/.spack-env/view

  $ spack find
  ==> In environment myenv
  ==> Root specs
  zlib@1.2.8

  ==> 1 installed package
  -- linux-rhel7-broadwell / gcc@8.1.0 ----------------------------
  zlib@1.2.8

  $ despacktivate

  $ spack find
  ==> 2 installed packages
  -- linux-rhel7-broadwell / gcc@8.1.0 ----------------------------
  zlib@1.2.8  zlib@1.2.11


Note that when we installed the abstract spec ``zlib@1.2.8``, it was
presented as a root of the Environment. All explicitly installed
packages will be listed as roots of the Environment.

All of the Spack commands that act on the list of installed specs are
Environment-sensitive in this way, including ``install``,
``uninstall``, ``activate``, ``deactivate``, ``find``, ``extensions``,
and more. In the :ref:`environment-configuration` section we will discuss
Environment-sensitive commands further.

^^^^^^^^^^^^^^^^^^^^^
Adding Abstract Specs
^^^^^^^^^^^^^^^^^^^^^

An abstract spec is the user-specified spec before Spack has applied
any defaults or dependency information.

Users can add abstract specs to an Environment using the ``spack add``
command. The most important component of an Environment is a list of
abstract specs.

Adding a spec adds to the manifest (the ``spack.yaml`` file), which is
used to define the roots of the Environment, but does not affect the
concrete specs in the lockfile, nor does it install the spec.

The ``spack add`` command is environment aware. It adds to the
currently active environment. All environment aware commands can also
be called using the ``spack -e`` flag to specify the environment.

.. code-block:: console

   $ spack env activate myenv
   $ spack add mpileaks

or

.. code-block:: console

   $ spack -e myenv add python

.. _environments_concretization:

^^^^^^^^^^^^
Concretizing
^^^^^^^^^^^^

Once some user specs have been added to an environment, they can be
concretized. *By default specs are concretized separately*, one after
the other. This mode of operation permits to deploy a full
software stack where multiple configurations of the same package
need to be installed alongside each other. Central installations done
at HPC centers by system administrators or user support groups
are a common case that fits in this behavior.
Environments *can also be configured to concretize all
the root specs in a self-consistent way* to ensure that
each package in the environment comes with a single configuration. This
mode of operation is usually what is required by software developers that
want to deploy their development environment.

Regardless of which mode of operation has been chosen, the following
command will ensure all the root specs are concretized according to the
constraints that are prescribed in the configuration:

.. code-block:: console

   [myenv]$ spack concretize

In the case of specs that are not concretized together, the command
above will concretize only the specs that were added and not yet
concretized. Forcing a re-concretization of all the specs can be done
instead with this command:

.. code-block:: console

   [myenv]$ spack concretize -f

When the ``-f`` flag is not used to reconcretize all specs, Spack
guarantees that already concretized specs are unchanged in the
environment.

The ``concretize`` command does not install any packages. For packages
that have already been installed outside of the environment, the
process of adding the spec and concretizing is identical to installing
the spec assuming it concretizes to the exact spec that was installed
outside of the environment.

The ``spack find`` command can show concretized specs separately from
installed specs using the ``-c`` (``--concretized``) flag.

.. code-block:: console

  [myenv]$ spack add zlib
  [myenv]$ spack concretize
  [myenv]$ spack find -c
  ==> In environment myenv
  ==> Root specs
  zlib

  ==> Concretized roots
  -- linux-rhel7-x86_64 / gcc@4.9.3 -------------------------------
  zlib@1.2.11

  ==> 0 installed packages


.. _installing-environment:

^^^^^^^^^^^^^^^^^^^^^^^^^
Installing an Environment
^^^^^^^^^^^^^^^^^^^^^^^^^

In addition to installing individual specs into an Environment, one
can install the entire Environment at once using the command

.. code-block:: console

   [myenv]$ spack install

If the Environment has been concretized, Spack will install the
concretized specs. Otherwise, ``spack install`` will first concretize
the Environment and then install the concretized specs.

As it installs, ``spack install`` creates symbolic links in the
``logs/`` directory in the Environment, allowing for easy inspection
of build logs related to that environment. The ``spack install``
command also stores a Spack repo containing the ``package.py`` file
used at install time for each package in the ``repos/`` directory in
the Environment.

The ``--no-add`` option can be used in a concrete environment to tell
spack to install specs already present in the environment but not to
add any new root specs to the environment.  For root specs provided
to ``spack install`` on the command line, ``--no-add`` is the default,
while for dependency specs on the other hand, it is optional.  In other
words, if there is an unambiguous match in the active concrete environment
for a root spec provided to ``spack install`` on the command line, spack
does not require you to specify the ``--no-add`` option to prevent the spec
from being added again.  At the same time, a spec that already exists in the
environment, but only as a dependency, will be added to the environment as a
root spec without the ``--no-add`` option.

^^^^^^^
Loading
^^^^^^^

Once an environment has been installed, the following creates a load
script for it:

.. code-block:: console

   $ spack env loads -r

This creates a file called ``loads`` in the environment directory.
Sourcing that file in Bash will make the environment available to the
user; and can be included in ``.bashrc`` files, etc.  The ``loads``
file may also be copied out of the environment, renamed, etc.

----------
spack.yaml
----------

Spack environments can be customized at finer granularity by editing
the ``spack.yaml`` manifest file directly.

.. _environment-configuration:

^^^^^^^^^^^^^^^^^^^^^^^^
Configuring Environments
^^^^^^^^^^^^^^^^^^^^^^^^

A variety of Spack behaviors are changed through Spack configuration
files, covered in more detail in the :ref:`configuration`
section.

Spack Environments provide an additional level of configuration scope
between the custom scope and the user scope discussed in the
configuration documentation.

There are two ways to include configuration information in a Spack Environment:

#. Inline in the ``spack.yaml`` file

#. Included in the ``spack.yaml`` file from another file.

Many Spack commands also affect configuration information in files
automatically. Those commands take a ``--scope`` argument, and the
environment can be specified by ``env:NAME`` (to affect environment
``foo``, set ``--scope env:foo``). These commands will automatically
manipulate configuration inline in the ``spack.yaml`` file.

"""""""""""""""""""""
Inline configurations
"""""""""""""""""""""

Inline Environment-scope configuration is done using the same yaml
format as standard Spack configuration scopes, covered in the
:ref:`configuration` section. Each section is contained under a
top-level yaml object with it's name. For example, a ``spack.yaml``
manifest file containing some package preference configuration (as in
a ``packages.yaml`` file) could contain:

.. code-block:: yaml

   spack:
     ...
     packages:
       all:
         compiler: [intel]
     ...

This configuration sets the default compiler for all packages to
``intel``.

"""""""""""""""""""""""
Included configurations
"""""""""""""""""""""""

Spack environments allow an ``include`` heading in their yaml
schema. This heading pulls in external configuration files and applies
them to the Environment.

.. code-block:: yaml

   spack:
     include:
     - relative/path/to/config.yaml
     - /absolute/path/to/packages.yaml

Environments can include files with either relative or absolute
paths. Inline configurations take precedence over included
configurations, so you don't have to change shared configuration files
to make small changes to an individual Environment. Included configs
listed earlier will have higher precedence, as the included configs are
applied in reverse order.

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Manually Editing the Specs List
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The list of abstract/root specs in the Environment is maintained in
the ``spack.yaml`` manifest under the heading ``specs``.

.. code-block:: yaml

   spack:
       specs:
         - ncview
         - netcdf
         - nco
         - py-sphinx

Appending to this list in the yaml is identical to using the ``spack
add`` command from the command line. However, there is more power
available from the yaml file.

"""""""""""""""""""
Spec concretization
"""""""""""""""""""

Specs can be concretized separately or together, as already
explained in :ref:`environments_concretization`. The behavior active
under any environment is determined by the ``concretization`` property:

.. code-block:: yaml

   spack:
       specs:
         - ncview
         - netcdf
         - nco
         - py-sphinx
       concretization: together

which can currently take either one of the two allowed values ``together`` or ``separately``
(the default).

.. admonition:: Re-concretization of user specs

   When concretizing specs together the entire set of specs will be
   re-concretized after any addition of new user specs, to ensure that
   the environment remains consistent. When instead the specs are concretized
   separately only the new specs will be re-concretized after any addition.

"""""""""""""
Spec Matrices
"""""""""""""

Entries in the ``specs`` list can be individual abstract specs or a
spec matrix.

A spec matrix is a yaml object containing multiple lists of specs, and
evaluates to the cross-product of those specs. Spec matrices also
contain an ``excludes`` directive, which eliminates certain
combinations from the evaluated result.

The following two Environment manifests are identical:

.. code-block:: yaml

   spack:
     specs:
       - zlib %gcc@7.1.0
       - zlib %gcc@4.9.3
       - libelf %gcc@7.1.0
       - libelf %gcc@4.9.3
       - libdwarf %gcc@7.1.0
       - cmake

   spack:
     specs:
       - matrix:
           - [zlib, libelf, libdwarf]
           - ['%gcc@7.1.0', '%gcc@4.9.3']
         exclude:
           - libdwarf%gcc@4.9.3
       - cmake

Spec matrices can be used to install swaths of software across various
toolchains.

The concretization logic for spec matrices differs slightly from the
rest of Spack. If a variant or dependency constraint from a matrix is
invalid, Spack will reject the constraint and try again without
it. For example, the following two Environment manifests will produce
the same specs:

.. code-block:: yaml

   spack:
     specs:
       - matrix:
           - [zlib, libelf, hdf5+mpi]
           - [^mvapich2@2.2, ^openmpi@3.1.0]

   spack:
     specs:
       - zlib
       - libelf
       - hdf5+mpi ^mvapich2@2.2
       - hdf5+mpi ^openmpi@3.1.0

This allows one to create toolchains out of combinations of
constraints and apply them somewhat indiscriminately to packages,
without regard for the applicability of the constraint.

""""""""""""""""""""
Spec List References
""""""""""""""""""""

The last type of possible entry in the specs list is a reference.

The Spack Environment manifest yaml schema contains an additional
heading ``definitions``. Under definitions is an array of yaml
objects. Each object has one or two fields. The one required field is
a name, and the optional field is a ``when`` clause.

The named field is a spec list. The spec list uses the same syntax as
the ``specs`` entry. Each entry in the spec list can be a spec, a spec
matrix, or a reference to an earlier named list. References are
specified using the ``$`` sigil, and are "splatted" into place
(i.e. the elements of the referent are at the same level as the
elements listed separately). As an example, the following two manifest
files are identical.

.. code-block:: yaml

   spack:
     definitions:
       - first: [libelf, libdwarf]
       - compilers: ['%gcc', '%intel']
       - second:
           - $first
           - matrix:
               - [zlib]
               - [$compilers]
     specs:
       - $second
       - cmake

   spack:
     specs:
       - libelf
       - libdwarf
       - zlib%gcc
       - zlib%intel
       - cmake

.. note::

   Named spec lists in the definitions section may only refer
   to a named list defined above itself. Order matters.

In short files like the example, it may be easier to simply list the
included specs. However for more complicated examples involving many
packages across many toolchains, separately factored lists make
Environments substantially more manageable.

Additionally, the ``-l`` option to the ``spack add`` command allows
one to add to named lists in the definitions section of the manifest
file directly from the command line.

The ``when`` directive can be used to conditionally add specs to a
named list. The ``when`` directive takes a string of Python code
referring to a restricted set of variables, and evaluates to a
boolean. The specs listed are appended to the named list if the
``when`` string evaluates to ``True``. In the following snippet, the
named list ``compilers`` is ``['%gcc', '%clang', '%intel']`` on
``x86_64`` systems and ``['%gcc', '%clang']`` on all other systems.

.. code-block:: yaml

   spack:
     definitions:
       - compilers: ['%gcc', '%clang']
       - when: arch.satisfies('x86_64:')
         compilers: ['%intel']

.. note::

   Any definitions with the same named list with true ``when``
   clauses (or absent ``when`` clauses) will be appended together

The valid variables for a ``when`` clause are:

#. ``platform``. The platform string of the default Spack
   architecture on the system.

#. ``os``. The os string of the default Spack architecture on
   the system.

#. ``target``. The target string of the default Spack
   architecture on the system.

#. ``architecture`` or ``arch``. A Spack spec satisfying the default Spack
   architecture on the system. This supports querying via the ``satisfies``
   method, as shown above.

#. ``arch_str``. The architecture string of the default Spack architecture
   on the system.

#. ``re``. The standard regex module in Python.

#. ``env``. The user environment (usually ``os.environ`` in Python).

#. ``hostname``. The hostname of the system (if ``hostname`` is an
   executable in the user's PATH).

""""""""""""""""""""""""
SpecLists as Constraints
""""""""""""""""""""""""

Dependencies and compilers in Spack can be both packages in an
environment and constraints on other packages. References to SpecLists
allow a shorthand to treat packages in a list as either a compiler or
a dependency using the ``$%`` or ``$^`` syntax respectively.

For example, the following environment has three root packages:
``gcc@8.1.0``, ``mvapich2@2.3.1 %gcc@8.1.0``, and ``hdf5+mpi
%gcc@8.1.0 ^mvapich2@2.3.1``.

.. code-block:: yaml

   spack:
     definitions:
     - compilers: [gcc@8.1.0]
     - mpis: [mvapich2@2.3.1]
     - packages: [hdf5+mpi]

     specs:
     - $compilers
     - matrix:
       - [$mpis]
       - [$%compilers]
     - matrix:
       - [$packages]
       - [$^mpis]
       - [$%compilers]

This allows for a much-needed reduction in redundancy between packages
and constraints.

^^^^^^^^^^^^^^^^^^^^^^^^^
Environment-managed Views
^^^^^^^^^^^^^^^^^^^^^^^^^

Spack Environments can define filesystem views of their software,
which are maintained as packages and can be installed and uninstalled from
the Environment. Filesystem views provide an access point for packages
from the filesystem for users who want to access those packages
directly. For more information on filesystem views, see the section
:ref:`filesystem-views`.

Spack Environment managed views are updated every time the environment
is written out to the lock file ``spack.lock``, so the concrete
environment and the view are always compatible.

.. _configuring_environment_views:

"""""""""""""""""""""""""""""
Configuring environment views
"""""""""""""""""""""""""""""

The Spack Environment manifest file has a top-level keyword
``view``. Each entry under that heading is a view descriptor, headed
by a name. The view descriptor contains the root of the view, and
optionally the projections for the view, ``select`` and
``exclude`` lists for the view and link information via ``link`` and
``link_type``. For example, in the following manifest
file snippet we define a view named ``mpis``, rooted at
``/path/to/view`` in which all projections use the package name,
version, and compiler name to determine the path for a given
package. This view selects all packages that depend on MPI, and
excludes those built with the PGI compiler at version 18.5.
All the dependencies of each root spec in the environment will be linked
in the view due to the command ``link: all`` and the files in the view will
be symlinks to the spack install directories.

.. code-block:: yaml

   spack:
     ...
     view:
       mpis:
         root: /path/to/view
         select: [^mpi]
         exclude: ['%pgi@18.5']
         projections:
           all: {name}/{version}-{compiler.name}
         link: all
         link_type: symlink

For more information on using view projections, see the section on
:ref:`adding_projections_to_views`. The default for the ``select`` and
``exclude`` values is to select everything and exclude nothing. The
default projection is the default view projection (``{}``). The ``link``
defaults to ``all`` but can also be ``roots`` when only the root specs
in the environment are desired in the view. The ``link_type`` defaults
to ``symlink`` but can also take the value of ``hardlink`` or ``copy``.

Any number of views may be defined under the ``view`` heading in a
Spack Environment.

There are two shorthands for environments with a single view. If the
environment at ``/path/to/env`` has a single view, with a root at
``/path/to/env/.spack-env/view``, with default selection and exclusion
and the default projection, we can put ``view: True`` in the
environment manifest. Similarly, if the environment has a view with a
different root, but default selection, exclusion, and projections, the
manifest can say ``view: /path/to/view``. These views are
automatically named ``default``, so that

.. code-block:: yaml

   spack:
     ...
     view: True

is equivalent to

.. code-block:: yaml

   spack:
     ...
     view:
       default:
         root: .spack-env/view

and

.. code-block:: yaml

   spack:
     ...
     view: /path/to/view

is equivalent to

.. code-block:: yaml

   spack:
     ...
     view:
       default:
         root: /path/to/view

By default, Spack environments are configured with ``view: True`` in
the manifest. Environments can be configured without views using
``view: False``. For backwards compatibility reasons, environments
with no ``view`` key are treated the same as ``view: True``.

From the command line, the ``spack env create`` command takes an
argument ``--with-view [PATH]`` that sets the path for a single, default
view. If no path is specified, the default path is used (``view:
True``). The argument ``--without-view`` can be used to create an
environment without any view configured.

The ``spack env view`` command can be used to change the manage views
of an Environment. The subcommand ``spack env view enable`` will add a
view named ``default`` to an environment. It takes an optional
argument to specify the path for the new default view. The subcommand
``spack env view disable`` will remove the view named ``default`` from
an environment if one exists. The subcommand ``spack env view
regenerate`` will regenerate the views for the environment. This will
apply any updates in the environment configuration that have not yet
been applied.

""""""""""""""""""""""""""""
Activating environment views
""""""""""""""""""""""""""""

The ``spack env activate`` command will put the default view for the
environment into the user's path, in addition to activating the
environment for Spack commands. The arguments ``-v,--with-view`` and
``-V,--without-view`` can be used to tune this behavior. The default
behavior is to activate with the environment view if there is one.

The environment variables affected by the ``spack env activate``
command and the paths that are used to update them are determined by
the :ref:`prefix inspections <customize-env-modifications>` defined in
your modules configuration; the defaults are summarized in the following
table.

=================== =========
Variable            Paths
=================== =========
PATH                bin
MANPATH             man, share/man
ACLOCAL_PATH        share/aclocal
LD_LIBRARY_PATH     lib, lib64
LIBRARY_PATH        lib, lib64
CPATH               include
PKG_CONFIG_PATH     lib/pkgconfig, lib64/pkgconfig, share/pkgconfig
CMAKE_PREFIX_PATH   .
=================== =========

Each of these paths are appended to the view root, and added to the
relevant variable if the path exists. For this reason, it is not
recommended to use non-default projections with the default view of an
environment.

The ``spack env deactivate`` command will remove the default view of
the environment from the user's path.
.. Copyright 2013-2022 Lawrence Livermore National Security, LLC and other
   Spack Project Developers. See the top-level COPYRIGHT file for details.

   SPDX-License-Identifier: (Apache-2.0 OR MIT)

.. extensions:

=================
Custom Extensions
=================

*Spack extensions* permit you to extend Spack capabilities by deploying your
own custom commands or logic in an arbitrary location on your filesystem.
This might be extremely useful e.g. to develop and maintain a command whose purpose is
too specific to be considered for reintegration into the mainline or to
evolve a command through its early stages before starting a discussion to merge
it upstream.
From Spack's point of view an extension is any path in your filesystem which
respects a prescribed naming and layout for files:

.. code-block:: console

  spack-scripting/ # The top level directory must match the format 'spack-{extension_name}'
  â”œâ”€â”€ pytest.ini # Optional file if the extension ships its own tests
  â”œâ”€â”€ scripting # Folder that may contain modules that are needed for the extension commands
  â”‚Â Â  â””â”€â”€ cmd # Folder containing extension commands
  â”‚Â Â      â””â”€â”€ filter.py # A new command that will be available
  â”œâ”€â”€ tests # Tests for this extension
  â”‚   â”œâ”€â”€ conftest.py
  â”‚   â””â”€â”€ test_filter.py
  â””â”€â”€ templates # Templates that may be needed by the extension

In the example above the extension named *scripting* adds an additional command (``filter``)
and unit tests to verify its behavior. The code for this example can be
obtained by cloning the corresponding git repository:

.. TODO: write an ad-hoc "hello world" extension and make it part of the spack organization

.. code-block:: console

   $ pwd
   /home/user
   $ mkdir tmp && cd tmp
   $ git clone https://github.com/alalazo/spack-scripting.git
   Cloning into 'spack-scripting'...
   remote: Counting objects: 11, done.
   remote: Compressing objects: 100% (7/7), done.
   remote: Total 11 (delta 0), reused 11 (delta 0), pack-reused 0
   Receiving objects: 100% (11/11), done.

As you can see by inspecting the sources, Python modules that are part of the extension
can import any core Spack module.

---------------------------------
Configure Spack to Use Extensions
---------------------------------

To make your current Spack instance aware of extensions you should add their root
paths to ``config.yaml``. In the case of our example this means ensuring that:

.. code-block:: yaml

   config:
     extensions:
     - /home/user/tmp/spack-scripting

is part of your configuration file. Once this is setup any command that the extension provides
will be available from the command line:

.. code-block:: console

   $ spack filter --help
   usage: spack filter [-h] [--installed | --not-installed]
                       [--explicit | --implicit] [--output OUTPUT]
                       ...

   filter specs based on their properties

   positional arguments:
     specs            specs to be filtered

   optional arguments:
     -h, --help       show this help message and exit
     --installed      select installed specs
     --not-installed  select specs that are not yet installed
     --explicit       select specs that were installed explicitly
     --implicit       select specs that are not installed or were installed implicitly
     --output OUTPUT  where to dump the result

The corresponding unit tests can be run giving the appropriate options
to ``spack unit-test``:

.. code-block:: console

   $ spack unit-test --extension=scripting

   ============================================================== test session starts ===============================================================
   platform linux2 -- Python 2.7.15rc1, pytest-3.2.5, py-1.4.34, pluggy-0.4.0
   rootdir: /home/mculpo/tmp/spack-scripting, inifile: pytest.ini
   collected 5 items

   tests/test_filter.py ...XX
   ============================================================ short test summary info =============================================================
   XPASS tests/test_filter.py::test_filtering_specs[flags3-specs3-expected3]
   XPASS tests/test_filter.py::test_filtering_specs[flags4-specs4-expected4]

   =========================================================== slowest 20 test durations ============================================================
   3.74s setup    tests/test_filter.py::test_filtering_specs[flags0-specs0-expected0]
   0.17s call     tests/test_filter.py::test_filtering_specs[flags3-specs3-expected3]
   0.16s call     tests/test_filter.py::test_filtering_specs[flags2-specs2-expected2]
   0.15s call     tests/test_filter.py::test_filtering_specs[flags1-specs1-expected1]
   0.13s call     tests/test_filter.py::test_filtering_specs[flags4-specs4-expected4]
   0.08s call     tests/test_filter.py::test_filtering_specs[flags0-specs0-expected0]
   0.04s teardown tests/test_filter.py::test_filtering_specs[flags4-specs4-expected4]
   0.00s setup    tests/test_filter.py::test_filtering_specs[flags4-specs4-expected4]
   0.00s setup    tests/test_filter.py::test_filtering_specs[flags3-specs3-expected3]
   0.00s setup    tests/test_filter.py::test_filtering_specs[flags1-specs1-expected1]
   0.00s setup    tests/test_filter.py::test_filtering_specs[flags2-specs2-expected2]
   0.00s teardown tests/test_filter.py::test_filtering_specs[flags2-specs2-expected2]
   0.00s teardown tests/test_filter.py::test_filtering_specs[flags1-specs1-expected1]
   0.00s teardown tests/test_filter.py::test_filtering_specs[flags0-specs0-expected0]
   0.00s teardown tests/test_filter.py::test_filtering_specs[flags3-specs3-expected3]
   ====================================================== 3 passed, 2 xpassed in 4.51 seconds =======================================================
.. Copyright 2013-2022 Lawrence Livermore National Security, LLC and other
   Spack Project Developers. See the top-level COPYRIGHT file for details.

   SPDX-License-Identifier: (Apache-2.0 OR MIT)

.. _configuration:

===================
Configuration Files
===================

Spack has many configuration files.  Here is a quick list of them, in
case you want to skip directly to specific docs:

* :ref:`compilers.yaml <compiler-config>`
* :ref:`config.yaml <config-yaml>`
* :ref:`mirrors.yaml <mirrors>`
* :ref:`modules.yaml <modules>`
* :ref:`packages.yaml <build-settings>`
* :ref:`repos.yaml <repositories>`

-----------
YAML Format
-----------

Spack configuration files are written in YAML.  We chose YAML because
it's human readable, but also versatile in that it supports dictionaries,
lists, and nested sections. For more details on the format, see `yaml.org
<http://yaml.org>`_ and `libyaml <http://pyyaml.org/wiki/LibYAML>`_.
Here is an example ``config.yaml`` file:

.. code-block:: yaml

   config:
     install_tree: $spack/opt/spack
     module_roots:
       lmod: $spack/share/spack/lmod
     build_stage:
       - $tempdir/$user/spack-stage
       - ~/.spack/stage

Each Spack configuration file is nested under a top-level section
corresponding to its name. So, ``config.yaml`` starts with ``config:``,
``mirrors.yaml`` starts with ``mirrors:``, etc.

.. _configuration-scopes:

--------------------
Configuration Scopes
--------------------

Spack pulls configuration data from files in several directories. There
are six configuration scopes. From lowest to highest:

#. **defaults**: Stored in ``$(prefix)/etc/spack/defaults/``. These are
   the "factory" settings. Users should generally not modify the settings
   here, but should override them in other configuration scopes. The
   defaults here will change from version to version of Spack.

#. **system**: Stored in ``/etc/spack/``. These are settings for this
   machine, or for all machines on which this file system is
   mounted. The site scope can be used for settings idiosyncratic to a
   particular machine, such as the locations of compilers or external
   packages. These settings are presumably controlled by someone with
   root access on the machine. They override the defaults scope.

#. **site**: Stored in ``$(prefix)/etc/spack/``. Settings here affect
   only *this instance* of Spack, and they override the defaults and system
   scopes.  The site scope can can be used for per-project settings (one
   Spack instance per project) or for site-wide settings on a multi-user
   machine (e.g., for a common Spack instance).

#. **user**: Stored in the home directory: ``~/.spack/``. These settings
   affect all instances of Spack and take higher precedence than site,
   system, or defaults scopes.

#. **custom**: Stored in a custom directory specified by ``--config-scope``.
   If multiple scopes are listed on the command line, they are ordered
   from lowest to highest precedence.

#. **environment**: When using Spack :ref:`environments`, Spack reads
   additional configuration from the environment file. See
   :ref:`environment-configuration` for further details on these
   scopes. Environment scopes can be referenced from the command line
   as ``env:name`` (to reference environment ``foo``, use
   ``env:foo``).

#. **command line**: Build settings specified on the command line take
   precedence over all other scopes.

Each configuration directory may contain several configuration files,
such as ``config.yaml``, ``compilers.yaml``, or ``mirrors.yaml``.  When
configurations conflict, settings from higher-precedence scopes override
lower-precedence settings.

Commands that modify scopes (e.g., ``spack compilers``, ``spack repo``,
etc.) take a ``--scope=<name>`` parameter that you can use to control
which scope is modified.  By default, they modify the highest-precedence
scope.

.. _custom-scopes:

^^^^^^^^^^^^^
Custom scopes
^^^^^^^^^^^^^

In addition to the ``defaults``, ``system``, ``site``, and ``user``
scopes, you may add configuration scopes directly on the command
line with the ``--config-scope`` argument, or ``-C`` for short.

For example, the following adds two configuration scopes, named
``scopea`` and ``scopeb``, to a ``spack spec`` command:

.. code-block:: console

   $ spack -C ~/myscopes/scopea -C ~/myscopes/scopeb spec ncurses

Custom scopes come *after* the ``spack`` command and *before* the
subcommand, and they specify a single path to a directory full of
configuration files. You can add the same configuration files to that
directory that you can add to any other scope (``config.yaml``,
``packages.yaml``, etc.).

If multiple scopes are provided:

#. Each must be preceded with the ``--config-scope`` or ``-C`` flag.
#. They must be ordered from lowest to highest precedence.

"""""""""""""""""""""""""""""""""""""""""""
Example: scopes for release and development
"""""""""""""""""""""""""""""""""""""""""""

Suppose that you need to support simultaneous building of release and
development versions of ``mypackage``, where ``mypackage`` -> ``A`` -> ``B``.
You could create The following files:

.. code-block:: yaml
   :caption: ~/myscopes/release/packages.yaml

   packages:
       mypackage:
           version: [1.7]
       A:
           version: [2.3]
       B:
           version: [0.8]

.. code-block:: yaml
   :caption: ~/myscopes/develop/packages.yaml

   packages:
       mypackage:
           version: [develop]
       A:
           version: [develop]
       B:
           version: [develop]

You can switch between ``release`` and ``develop`` configurations using
configuration arguments.  You would type ``spack -C ~/myscopes/release``
when you want to build the designated release versions of ``mypackage``,
``A``, and ``B``, and you would type ``spack -C ~/myscopes/develop`` when
you want to build all of these packages at the ``develop`` version.

"""""""""""""""""""""""""""""""
Example: swapping MPI providers
"""""""""""""""""""""""""""""""

Suppose that you need to build two software packages, ``packagea`` and
``packageb``. ``packagea`` is Python 2-based and ``packageb`` is Python
3-based. ``packagea`` only builds with OpenMPI and ``packageb`` only builds
with MPICH. You can create different configuration scopes for use with
``packagea`` and ``packageb``:

.. code-block:: yaml
   :caption: ~/myscopes/packgea/packages.yaml

   packages:
       python:
           version: [2.7.11]
       all:
           providers:
               mpi: [openmpi]

.. code-block:: yaml
   :caption: ~/myscopes/packageb/packages.yaml

   packages:
       python:
           version: [3.5.2]
       all:
           providers:
               mpi: [mpich]


.. _platform-scopes:

------------------------
Platform-specific Scopes
------------------------

For each scope above (excluding environment scopes), there can also be
platform-specific settings.  For example, on most platforms, GCC is
the preferred compiler.  However, on macOS (darwin), Clang often works
for more packages, and is set as the default compiler. This
configuration is set in
``$(prefix)/etc/spack/defaults/darwin/packages.yaml``. It will take
precedence over settings in the ``defaults`` scope, but can still be
overridden by settings in ``system``, ``system/darwin``, ``site``,
``site/darwin``, ``user``, ``user/darwin``, ``custom``, or
``custom/darwin``. So, the full scope precedence is:

#. ``defaults``
#. ``defaults/<platform>``
#. ``system``
#. ``system/<platform>``
#. ``site``
#. ``site/<platform>``
#. ``user``
#. ``user/<platform>``
#. ``custom``
#. ``custom/<platform>``

You can get the name to use for ``<platform>`` by running ``spack arch
--platform``. The system config scope has a ``<platform>`` section for
sites at which ``/etc`` is mounted on multiple heterogeneous machines.

----------------
Scope Precedence
----------------

When spack queries for configuration parameters, it searches in
higher-precedence scopes first. So, settings in a higher-precedence file
can override those with the same key in a lower-precedence one. For
list-valued settings, Spack *prepends* higher-precedence settings to
lower-precedence settings. Completely ignoring higher-level configuration
options is supported with the ``::`` notation for keys (see
:ref:`config-overrides` below).

^^^^^^^^^^^
Simple keys
^^^^^^^^^^^

Let's look at an example of overriding a single key in a Spack file. If
your configurations look like this:

.. code-block:: yaml
   :caption: $(prefix)/etc/spack/defaults/config.yaml

   config:
     install_tree: $spack/opt/spack
     module_roots:
       lmod: $spack/share/spack/lmod
     build_stage:
       - $tempdir/$user/spack-stage
       - ~/.spack/stage


.. code-block:: yaml
   :caption: ~/.spack/config.yaml

   config:
     install_tree: /some/other/directory


Spack will only override ``install_tree`` in the ``config`` section, and
will take the site preferences for other settings. You can see the
final, combined configuration with the ``spack config get <configtype>``
command:

.. code-block:: console
   :emphasize-lines: 3

   $ spack config get config
   config:
     install_tree: /some/other/directory
     module_roots:
       lmod: $spack/share/spack/lmod
     build_stage:
       - $tempdir/$user/spack-stage
       - ~/.spack/stage


.. _config-overrides:

^^^^^^^^^^^^^^^^^^^^^^^^^^
Overriding entire sections
^^^^^^^^^^^^^^^^^^^^^^^^^^

Above, the user ``config.yaml`` only overrides specific settings in the
default ``config.yaml``. Sometimes, it is useful to *completely*
override lower-precedence settings. To do this, you can use *two* colons
at the end of a key in a configuration file. For example:

.. code-block:: yaml
   :emphasize-lines: 1
   :caption: ~/.spack/config.yaml

   config::
     install_tree: /some/other/directory

Spack will ignore all lower-precedence configuration under the
``config::`` section:

.. code-block:: console

   $ spack config get config
   config:
     install_tree: /some/other/directory


^^^^^^^^^^^^^^^^^^^^
List-valued settings
^^^^^^^^^^^^^^^^^^^^

Let's revisit the ``config.yaml`` example one more time. The
``build_stage`` setting's value is an ordered list of directories:

.. code-block:: yaml
   :caption: $(prefix)/etc/spack/defaults/config.yaml

   build_stage:
     - $tempdir/$user/spack-stage
     - ~/.spack/stage


Suppose the user configuration adds its *own* list of ``build_stage``
paths:

.. code-block:: yaml
   :caption: ~/.spack/config.yaml

   build_stage:
     - /lustre-scratch/$user/spack
     - ~/mystage


Spack will first look at the paths in the defaults ``config.yaml``, then the
paths in the user's ``~/.spack/config.yaml``. The list in the
higher-precedence scope is *prepended* to the defaults. ``spack config
get config`` shows the result:

.. code-block:: console
   :emphasize-lines: 7-10

   $ spack config get config
   config:
     install_tree: /some/other/directory
     module_roots:
       lmod: $spack/share/spack/lmod
     build_stage:
       - /lustre-scratch/$user/spack
       - ~/mystage
       - $tempdir/$user/spack-stage
       - ~/.spack/stage


As in :ref:`config-overrides`, the higher-precedence scope can
*completely* override the lower-precedence scope using ``::``. So if the
user config looked like this:

.. code-block:: yaml
   :emphasize-lines: 1
   :caption: ~/.spack/config.yaml

   build_stage::
     - /lustre-scratch/$user/spack
     - ~/mystage


The merged configuration would look like this:

.. code-block:: console
   :emphasize-lines: 7-8

   $ spack config get config
   config:
     install_tree: /some/other/directory
     module_roots:
       lmod: $spack/share/spack/lmod
     build_stage:
       - /lustre-scratch/$user/spack
       - ~/mystage


.. _config-file-variables:

---------------------
Config File Variables
---------------------

Spack understands several variables which can be used in config file
paths wherever they appear. There are three sets of these variables:
Spack-specific variables, environment variables, and user path
variables. Spack-specific variables and environment variables are both
indicated by prefixing the variable name with ``$``. User path variables
are indicated at the start of the path with ``~`` or ``~user``.

^^^^^^^^^^^^^^^^^^^^^^^^
Spack-specific variables
^^^^^^^^^^^^^^^^^^^^^^^^

Spack understands several special variables. These are:

* ``$env``: name of the currently active :ref:`environment <environments>`
* ``$spack``: path to the prefix of this Spack installation
* ``$tempdir``: default system temporary directory (as specified in
  Python's `tempfile.tempdir
  <https://docs.python.org/2/library/tempfile.html#tempfile.tempdir>`_
  variable.
* ``$user``: name of the current user
* ``$user_cache_path``: user cache directory (``~/.spack`` unless
  :ref:`overridden <local-config-overrides>`)

Note that, as with shell variables, you can write these as ``$varname``
or with braces to distinguish the variable from surrounding characters:
``${varname}``. Their names are also case insensitive, meaning that
``$SPACK`` works just as well as ``$spack``. These special variables are
substituted first, so any environment variables with the same name will
not be used.

^^^^^^^^^^^^^^^^^^^^^
Environment variables
^^^^^^^^^^^^^^^^^^^^^

After Spack-specific variables are evaluated, environment variables are
expanded. These are formatted like Spack-specific variables, e.g.,
``${varname}``. You can use this to insert environment variables in your
Spack configuration.

^^^^^^^^^^^^^^^^^^^^^
User home directories
^^^^^^^^^^^^^^^^^^^^^

Spack performs Unix-style tilde expansion on paths in configuration
files. This means that tilde (``~``) will expand to the current user's
home directory, and ``~user`` will expand to a specified user's home
directory. The ``~`` must appear at the beginning of the path, or Spack
will not expand it.

.. _configuration_environment_variables:

-------------------------
Environment Modifications
-------------------------

Spack allows to prescribe custom environment modifications in a few places
within its configuration files. Every time these modifications are allowed
they are specified as a dictionary, like in the following example:

.. code-block:: yaml

   environment:
     set:
       LICENSE_FILE: '/path/to/license'
     unset:
     - CPATH
     - LIBRARY_PATH
     append_path:
       PATH: '/new/bin/dir'

The possible actions that are permitted are ``set``, ``unset``, ``append_path``,
``prepend_path`` and finally ``remove_path``. They all require a dictionary
of variable names mapped to the values used for the modification.
The only exception is ``unset`` that requires just a list of variable names.
No particular order is ensured on the execution of each of these modifications.

----------------------------
Seeing Spack's Configuration
----------------------------

With so many scopes overriding each other, it can sometimes be difficult
to understand what Spack's final configuration looks like.

Spack provides two useful ways to view the final "merged" version of any
configuration file: ``spack config get`` and ``spack config blame``.

.. _cmd-spack-config-get:

^^^^^^^^^^^^^^^^^^^^
``spack config get``
^^^^^^^^^^^^^^^^^^^^

``spack config get`` shows a fully merged configuration file, taking into
account all scopes. For example, to see the fully merged
``config.yaml``, you can type:

.. code-block:: console

   $ spack config get config
   config:
     debug: false
     checksum: true
     verify_ssl: true
     dirty: false
     build_jobs: 8
     install_tree: $spack/opt/spack
     template_dirs:
     - $spack/templates
     directory_layout: {architecture}/{compiler.name}-{compiler.version}/{name}-{version}-{hash}
     module_roots:
       tcl: $spack/share/spack/modules
       lmod: $spack/share/spack/lmod
     build_stage:
     - $tempdir/$user/spack-stage
     - ~/.spack/stage
     - $spack/var/spack/stage
     source_cache: $spack/var/spack/cache
     misc_cache: ~/.spack/cache
     locks: true

Likewise, this will show the fully merged ``packages.yaml``:

.. code-block:: console

   $ spack config get packages

You can use this in conjunction with the ``-C`` / ``--config-scope`` argument to
see how your scope will affect Spack's configuration:

.. code-block:: console

   $ spack -C /path/to/my/scope config get packages


.. _cmd-spack-config-blame:

^^^^^^^^^^^^^^^^^^^^^^
``spack config blame``
^^^^^^^^^^^^^^^^^^^^^^

``spack config blame`` functions much like ``spack config get``, but it
shows exactly which configuration file each preference came from. If you
do not know why Spack is behaving a certain way, this can help you track
down the problem:

.. code-block:: console

   $ spack --insecure -C ./my-scope -C ./my-scope-2 config blame config
   ==> Warning: You asked for --insecure. Will NOT check SSL certificates.
   ---                                                   config:
   _builtin                                                debug: False
   /home/myuser/spack/etc/spack/defaults/config.yaml:72    checksum: True
   command_line                                            verify_ssl: False
   ./my-scope-2/config.yaml:2                              dirty: False
   _builtin                                                build_jobs: 8
   ./my-scope/config.yaml:2                                install_tree: /path/to/some/tree
   /home/myuser/spack/etc/spack/defaults/config.yaml:23    template_dirs:
   /home/myuser/spack/etc/spack/defaults/config.yaml:24    - $spack/templates
   /home/myuser/spack/etc/spack/defaults/config.yaml:28    directory_layout: {architecture}/{compiler.name}-{compiler.version}/{name}-{version}-{hash}
   /home/myuser/spack/etc/spack/defaults/config.yaml:32    module_roots:
   /home/myuser/spack/etc/spack/defaults/config.yaml:33      tcl: $spack/share/spack/modules
   /home/myuser/spack/etc/spack/defaults/config.yaml:34      lmod: $spack/share/spack/lmod
   /home/myuser/spack/etc/spack/defaults/config.yaml:49    build_stage:
   /home/myuser/spack/etc/spack/defaults/config.yaml:50    - $tempdir/$user/spack-stage
   /home/myuser/spack/etc/spack/defaults/config.yaml:51    - ~/.spack/stage
   /home/myuser/spack/etc/spack/defaults/config.yaml:52    - $spack/var/spack/stage
   /home/myuser/spack/etc/spack/defaults/config.yaml:57    source_cache: $spack/var/spack/cache
   /home/myuser/spack/etc/spack/defaults/config.yaml:62    misc_cache: ~/.spack/cache
   /home/myuser/spack/etc/spack/defaults/config.yaml:86    locks: True

You can see above that the ``build_jobs`` and ``debug`` settings are
built in and are not overridden by a configuration file. The
``verify_ssl`` setting comes from the ``--insceure`` option on the
command line. ``dirty`` and ``install_tree`` come from the custom
scopes ``./my-scope`` and ``./my-scope-2``, and all other configuration
options come from the default configuration files that ship with Spack.

.. _local-config-overrides:

------------------------------
Overriding Local Configuration
------------------------------

Spack's ``system`` and ``user`` scopes provide ways for administrators and users to set
global defaults for all Spack instances, but for use cases where one wants a clean Spack
installation, these scopes can be undesirable. For example, users may want to opt out of
global system configuration, or they may want to ignore their own home directory
settings when running in a continuous integration environment.

Spack also, by default, keeps various caches and user data in ``~/.spack``, but
users may want to override these locations.

Spack provides three environment variables that allow you to override or opt out of
configuration locations:

* ``SPACK_USER_CONFIG_PATH``: Override the path to use for the
  ``user`` scope (``~/.spack`` by default).
* ``SPACK_SYSTEM_CONFIG_PATH``: Override the path to use for the
  ``system`` scope (``/etc/spack`` by default).
* ``SPACK_DISABLE_LOCAL_CONFIG``: set this environment variable to completely disable
  **both** the system and user configuration directories. Spack will only consider its
  own defaults and ``site`` configuration locations.

And one that allows you to move the default cache location:

* ``SPACK_USER_CACHE_PATH``: Override the default path to use for user data
  (misc_cache, tests, reports, etc.)

With these settings, if you want to isolate Spack in a CI environment, you can do this::

  export SPACK_DISABLE_LOCAL_CONFIG=true
  export SPACK_USER_CACHE_PATH=/tmp/spack
.. Copyright 2013-2022 Lawrence Livermore National Security, LLC and other
   Spack Project Developers. See the top-level COPYRIGHT file for details.

   SPDX-License-Identifier: (Apache-2.0 OR MIT)

.. _developer_guide:

===============
Developer Guide
===============

This guide is intended for people who want to work on Spack itself.
If you just want to develop packages, see the :ref:`packaging-guide`.

It is assumed that you've read the :ref:`basic-usage` and
:ref:`packaging-guide` sections, and that you're familiar with the
concepts discussed there.  If you're not, we recommend reading those
first.

--------
Overview
--------

Spack is designed with three separate roles in mind:

#. **Users**, who need to install software *without* knowing all the
   details about how it is built.
#. **Packagers** who know how a particular software package is
   built and encode this information in package files.
#. **Developers** who work on Spack, add new features, and try to
   make the jobs of packagers and users easier.

Users could be end users installing software in their home directory,
or administrators installing software to a shared directory on a
shared machine.  Packagers could be administrators who want to
automate software builds, or application developers who want to make
their software more accessible to users.

As you might expect, there are many types of users with different
levels of sophistication, and Spack is designed to accommodate both
simple and complex use cases for packages.  A user who only knows that
he needs a certain package should be able to type something simple,
like ``spack install <package name>``, and get the package that he
wants.  If a user wants to ask for a specific version, use particular
compilers, or build several versions with different configurations,
then that should be possible with a minimal amount of additional
specification.

This gets us to the two key concepts in Spack's software design:

#. **Specs**: expressions for describing builds of software, and
#. **Packages**: Python modules that build software according to a
   spec.

A package is a template for building particular software, and a spec
as a descriptor for one or more instances of that template.  Users
express the configuration they want using a spec, and a package turns
the spec into a complete build.

The obvious difficulty with this design is that users under-specify
what they want.  To build a software package, the package object needs
a *complete* specification.  In Spack, if a spec describes only one
instance of a package, then we say it is **concrete**.  If a spec
could describes many instances, (i.e. it is under-specified in one way
or another), then we say it is **abstract**.

Spack's job is to take an *abstract* spec from the user, find a
*concrete* spec that satisfies the constraints, and hand the task of
building the software off to the package object.  The rest of this
document describes all the pieces that come together to make that
happen.

-------------------
Directory Structure
-------------------

So that you can familiarize yourself with the project, we'll start
with a high level view of Spack's directory structure:

.. code-block:: none

   spack/                  <- installation root
      bin/
         spack             <- main spack executable

      etc/
         spack/            <- Spack config files.
                              Can be overridden by files in ~/.spack.

      var/
         spack/            <- build & stage directories
             repos/            <- contains package repositories
                builtin/       <- pkg repository that comes with Spack
                   repo.yaml   <- descriptor for the builtin repository
                   packages/   <- directories under here contain packages
             cache/        <- saves resources downloaded during installs

      opt/
         spack/            <- packages are installed here

      lib/
         spack/
            docs/          <- source for this documentation
            env/           <- compiler wrappers for build environment

            external/      <- external libs included in Spack distro
            llnl/          <- some general-use libraries

            spack/                <- spack module; contains Python code
               analyzers/         <- modules to run analysis on installed packages
               build_systems/     <- modules for different build systems
               cmd/               <- each file in here is a spack subcommand
               compilers/         <- compiler description files
               container/         <- module for spack containerize
               hooks/             <- hook modules to run at different points
               modules/           <- modules for lmod, tcl, etc.
               operating_systems/ <- operating system modules
               platforms/         <- different spack platforms
               reporters/         <- reporters like cdash, junit
               schema/            <- schemas to validate data structures
               solver/            <- the spack solver
               test/              <- unit test modules
               util/              <- common code

Spack is designed so that it could live within a `standard UNIX
directory hierarchy <http://linux.die.net/man/7/hier>`_, so ``lib``,
``var``, and ``opt`` all contain a ``spack`` subdirectory in case
Spack is installed alongside other software.  Most of the interesting
parts of Spack live in ``lib/spack``.

Spack has *one* directory layout and there is no install process.
Most Python programs don't look like this (they use distutils, ``setup.py``,
etc.) but we wanted to make Spack *very* easy to use.  The simple layout
spares users from the need to install Spack into a Python environment.
Many users don't have write access to a Python installation, and installing
an entire new instance of Python to bootstrap Spack would be very complicated.
Users should not have to install a big, complicated package to
use the thing that's supposed to spare them from the details of big,
complicated packages.  The end result is that Spack works out of the
box: clone it and add ``bin`` to your PATH and you're ready to go.

--------------
Code Structure
--------------

This section gives an overview of the various Python modules in Spack,
grouped by functionality.

^^^^^^^^^^^^^^^^^^^^^^^
Package-related modules
^^^^^^^^^^^^^^^^^^^^^^^

:mod:`spack.package`
  Contains the :class:`~spack.package.Package` class, which
  is the superclass for all packages in Spack.  Methods on ``Package``
  implement all phases of the :ref:`package lifecycle
  <package-lifecycle>` and manage the build process.

:mod:`spack.util.naming`
  Contains functions for mapping between Spack package names,
  Python module names, and Python class names. Functions like
  :func:`~spack.util.naming.mod_to_class` handle mapping package
  module names to class names.

:mod:`spack.directives`
  *Directives* are functions that can be called inside a package definition
  to modify the package, like :func:`~spack.directives.depends_on`
  and :func:`~spack.directives.provides`.  See :ref:`dependencies`
  and :ref:`virtual-dependencies`.

:mod:`spack.multimethod`
  Implementation of the :func:`@when <spack.multimethod.when>`
  decorator, which allows :ref:`multimethods <multimethods>` in
  packages.

^^^^^^^^^^^^^^^^^^^^
Spec-related modules
^^^^^^^^^^^^^^^^^^^^

:mod:`spack.spec`
  Contains :class:`~spack.spec.Spec` and :class:`~spack.spec.SpecParser`.
  Also implements most of the logic for normalization and concretization
  of specs.

:mod:`spack.parse`
  Contains some base classes for implementing simple recursive descent
  parsers: :class:`~spack.parse.Parser` and :class:`~spack.parse.Lexer`.
  Used by :class:`~spack.spec.SpecParser`.

:mod:`spack.concretize`
  Contains :class:`~spack.concretize.Concretizer` implementation,
  which allows site administrators to change Spack's :ref:`concretization-policies`.

:mod:`spack.version`
  Implements a simple :class:`~spack.version.Version` class with simple
  comparison semantics.  Also implements :class:`~spack.version.VersionRange`
  and :class:`~spack.version.VersionList`. All three are comparable with each
  other and offer union and intersection operations. Spack uses these classes
  to compare versions and to manage version constraints on specs. Comparison
  semantics are similar to the ``LooseVersion`` class in ``distutils`` and to
  the way RPM compares version strings.

:mod:`spack.compilers`
  Submodules contains descriptors for all valid compilers in Spack.
  This is used by the build system to set up the build environment.

  .. warning::

     Not yet implemented.  Currently has two compiler descriptions,
     but compilers aren't fully integrated with the build process
     yet.

^^^^^^^^^^^^^^^^^
Build environment
^^^^^^^^^^^^^^^^^

:mod:`spack.stage`
  Handles creating temporary directories for builds.

:mod:`spack.build_environment`
  This contains utility functions used by the compiler wrapper script,
  ``cc``.

:mod:`spack.directory_layout`
  Classes that control the way an installation directory is laid out.
  Create more implementations of this to change the hierarchy and
  naming scheme in ``$spack_prefix/opt``

^^^^^^^^^^^^^^^^^
Spack Subcommands
^^^^^^^^^^^^^^^^^

:mod:`spack.cmd`
  Each module in this package implements a Spack subcommand.  See
  :ref:`writing commands <writing-commands>` for details.

^^^^^^^^^^
Unit tests
^^^^^^^^^^

:mod:`spack.test`
  Implements Spack's test suite.  Add a module and put its name in
  the test suite in ``__init__.py`` to add more unit tests.

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Research and Monitoring Modules
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

:mod:`spack.monitor`
  Contains :class:`~spack.monitor.SpackMonitorClient`. This is accessed from
  the ``spack install`` and ``spack analyze`` commands to send build and
  package metadata up to a `Spack Monitor
  <https://github.com/spack/spack-monitor>`_ server.


:mod:`spack.analyzers`
  A module folder with a :class:`~spack.analyzers.analyzer_base.AnalyzerBase`
  that provides base functions to run, save, and (optionally) upload analysis
  results to a `Spack Monitor <https://github.com/spack/spack-monitor>`_ server.


^^^^^^^^^^^^^
Other Modules
^^^^^^^^^^^^^

:mod:`spack.url`
  URL parsing, for deducing names and versions of packages from
  tarball URLs.

:mod:`spack.error`
  :class:`~spack.error.SpackError`, the base class for
  Spack's exception hierarchy.

:mod:`llnl.util.tty`
  Basic output functions for all of the messages Spack writes to the
  terminal.

:mod:`llnl.util.tty.color`
  Implements a color formatting syntax used by ``spack.tty``.

:mod:`llnl.util`
  In this package are a number of utility modules for the rest of
  Spack.

------------
Spec objects
------------

---------------
Package objects
---------------

Most spack commands look something like this:

#. Parse an abstract spec (or specs) from the command line,
#. *Normalize* the spec based on information in package files,
#. *Concretize* the spec according to some customizable policies,
#. Instantiate a package based on the spec, and
#. Call methods (e.g., ``install()``) on the package object.

The information in Package files is used at all stages in this
process.

Conceptually, packages are overloaded.  They contain:

-------------
Stage objects
-------------


.. _writing-analyzers:

-----------------
Writing analyzers
-----------------

To write an analyzer, you should add a new python file to the
analyzers module directory at ``lib/spack/spack/analyzers`` .
Your analyzer should be a subclass of the :class:`AnalyzerBase <spack.analyzers.analyzer_base.AnalyzerBase>`. For example, if you want
to add an analyzer class ``Myanalyzer`` you would write to
``spack/analyzers/myanalyzer.py`` and import and
use the base as follows:

.. code-block:: python

    from .analyzer_base import AnalyzerBase

    class Myanalyzer(AnalyzerBase):


Note that the class name is your module file name, all lowercase
except for the first capital letter. You can  look at other analyzers in
that analyzer directory for examples. The guide here will tell you about the basic functions needed.

^^^^^^^^^^^^^^^^^^^^^^^^^
Analyzer Output Directory
^^^^^^^^^^^^^^^^^^^^^^^^^

By default, when you run ``spack analyze run`` an analyzer output directory will
be created in your spack user directory in your ``$HOME``. The reason we output here
is because the install directory might not always be writable.

.. code-block:: console

    ~/.spack/
      analyzers

Result files will be written here, organized in subfolders in the same structure
as the package, with each analyzer owning it's own subfolder. for example:


.. code-block:: console

    $ tree ~/.spack/analyzers/
    /home/spackuser/.spack/analyzers/
    â””â”€â”€ linux-ubuntu20.04-skylake
        â””â”€â”€ gcc-9.3.0
            â””â”€â”€ zlib-1.2.11-sl7m27mzkbejtkrajigj3a3m37ygv4u2
                â”œâ”€â”€ environment_variables
                â”‚Â Â  â””â”€â”€ spack-analyzer-environment-variables.json
                â”œâ”€â”€ install_files
                â”‚Â Â  â””â”€â”€ spack-analyzer-install-files.json
                â””â”€â”€ libabigail
                    â””â”€â”€ lib
                        â””â”€â”€ spack-analyzer-libabigail-libz.so.1.2.11.xml


Notice that for the libabigail analyzer, since results are generated per object,
we honor the object's folder in case there are equivalently named files in
different folders. The result files are typically written as json so they can be easily read and  uploaded in a future interaction with a monitor.


^^^^^^^^^^^^^^^^^
Analyzer Metadata
^^^^^^^^^^^^^^^^^

Your analyzer is required to have the class attributes ``name``, ``outfile``,
and ``description``. These are printed to the user with they use the subcommand
``spack analyze list-analyzers``.  Here is an example.
As we mentioned above, note that this analyzer would live in a module named
``libabigail.py`` in the analyzers folder so that the class can be discovered.


.. code-block:: python

    class Libabigail(AnalyzerBase):

        name = "libabigail"
        outfile = "spack-analyzer-libabigail.json"
        description = "Application Binary Interface (ABI) features for objects"


This means that the name and output file should be unique for your analyzer.
Note that "all" cannot be the name of an analyzer, as this key is used to indicate
that the user wants to run all analyzers.

.. _analyzer_run_function:


^^^^^^^^^^^^^^^^^^^^^^^^
An analyzer run Function
^^^^^^^^^^^^^^^^^^^^^^^^

The core of an analyzer is its ``run()`` function, which should accept no
arguments. You can assume your analyzer has the package spec of interest at ``self.spec``
and it's up to the run function to generate whatever analysis data you need,
and then return the object with a key as the analyzer name. The result data
should be a list of objects, each with a name, ``analyzer_name``, ``install_file``,
and one of ``value`` or ``binary_value``. The install file should be for a relative
path, and not the absolute path. For example, let's say we extract a metric called
``metric`` for ``bin/wget`` using our analyzer ``thebest-analyzer``.
We might have data that looks like this:

.. code-block:: python

    result = {"name": "metric", "analyzer_name": "thebest-analyzer", "value": "1", "install_file": "bin/wget"}


We'd then return it as follows - note that they key is the analyzer name at ``self.name``.

.. code-block:: python

    return {self.name: result}

This will save the complete result to the analyzer metadata folder, as described
previously. If you want support for adding a different kind of metadata (e.g.,
not associated with an install file) then the monitor server would need to be updated
to support this first.


^^^^^^^^^^^^^^^^^^^^^^^^^
An analyzer init Function
^^^^^^^^^^^^^^^^^^^^^^^^^

If you don't need any extra dependencies or checks, you can skip defining an analyzer
init function, as the base class will handle it. Typically, it will accept
a spec, and an optional output directory (if the user does not want the default
metadata folder for analyzer results). The analyzer init function should call
it's parent init, and then do any extra checks or validation that are required to
work. For example:

.. code-block:: python

    def __init__(self, spec, dirname=None):
        super(Myanalyzer, self).__init__(spec, dirname)

        # install extra dependencies, do extra preparation and checks here


At the end of the init, you will have available to you:

 - **self.spec**: the spec object
 - **self.dirname**: an optional directory name the user as provided at init to save
 - **self.output_dir**: the analyzer metadata directory, where we save by default
 - **self.meta_dir**: the path to the package metadata directory (.spack) if you need it

And can proceed to write your analyzer.


^^^^^^^^^^^^^^^^^^^^^^^
Saving Analyzer Results
^^^^^^^^^^^^^^^^^^^^^^^

The analyzer will have ``save_result`` called, with the result object generated
to save it to the filesystem, and if the user has added the ``--monitor`` flag
to upload it to a monitor server. If your result follows an accepted result
format and you don't need to parse it further, you don't need to add this
function to your class. However, if your result data is large or otherwise
needs additional parsing, you can define it. If you define the function, it
is useful to know about the ``output_dir`` property, which you can join
with your output file relative path of choice:

.. code-block:: python

    outfile = os.path.join(self.output_dir, "my-output-file.txt")


The directory will be provided by the ``output_dir`` property but it won't exist,
so you should create it:


.. code::block:: python

    # Create the output directory
    if not os.path.exists(self._output_dir):
        os.makedirs(self._output_dir)


If you are generating results that match to specific files in the package
install directory, you should try to maintain those paths in the case that
there are equivalently named files in different directories that would
overwrite one another. As an example of an analyzer with a custom save,
the Libabigail analyzer saves ``*.xml`` files to the analyzer metadata
folder in ``run()``, as they are either binaries, or as xml (text) would
usually be too big to pass in one request. For this reason, the files
are saved during ``run()`` and the filenames added to the result object,
and then when the result object is passed back into ``save_result()``,
we skip saving to the filesystem, and instead read the file and send
each one (separately) to the monitor:


.. code-block:: python

    def save_result(self, result, monitor=None, overwrite=False):
        """ABI results are saved to individual files, so each one needs to be
        read and uploaded. Result here should be the lookup generated in run(),
        the key is the analyzer name, and each value is the result file.
        We currently upload the entire xml as text because libabigail can't
        easily read gzipped xml, but this will be updated when it can.
        """
        if not monitor:
            return

        name = self.spec.package.name

        for obj, filename in result.get(self.name, {}).items():

            # Don't include the prefix
            rel_path = obj.replace(self.spec.prefix + os.path.sep, "")

            # We've already saved the results to file during run
            content = spack.monitor.read_file(filename)

            # A result needs an analyzer, value or binary_value, and name
            data = {"value": content, "install_file": rel_path, "name": "abidw-xml"}
            tty.info("Sending result for %s %s to monitor." % (name, rel_path))
            monitor.send_analyze_metadata(self.spec.package, {"libabigail": [data]})



Notice that this function, if you define it, requires a result object (generated by
``run()``, a monitor (if you want to send), and a boolean ``overwrite`` to be used
to check if a result exists first, and not write to it if the result exists and
overwrite is False. Also notice that since we already saved these files to the analyzer metadata folder, we return early if a monitor isn't defined, because this function serves to send  results to the monitor. If you haven't saved anything to the analyzer metadata folder
yet, you might want to do that here. You should also use ``tty.info`` to give
the user a message of "Writing result to $DIRNAME."


.. _writing-commands:

----------------
Writing commands
----------------

Adding a new command to Spack is easy. Simply add a ``<name>.py`` file to
``lib/spack/spack/cmd/``, where ``<name>`` is the name of the subcommand.
At the bare minimum, two functions are required in this file:

^^^^^^^^^^^^^^^^^^
``setup_parser()``
^^^^^^^^^^^^^^^^^^

Unless your command doesn't accept any arguments, a ``setup_parser()``
function is required to define what arguments and flags your command takes.
See the `Argparse documentation <https://docs.python.org/2.7/library/argparse.html>`_
for more details on how to add arguments.

Some commands have a set of subcommands, like ``spack compiler find`` or
``spack module lmod refresh``. You can add subparsers to your parser to handle
this. Check out ``spack edit --command compiler`` for an example of this.

A lot of commands take the same arguments and flags. These arguments should
be defined in ``lib/spack/spack/cmd/common/arguments.py`` so that they don't
need to be redefined in multiple commands.

^^^^^^^^^^^^
``<name>()``
^^^^^^^^^^^^

In order to run your command, Spack searches for a function with the same
name as your command in ``<name>.py``. This is the main method for your
command, and can call other helper methods to handle common tasks.

Remember, before adding a new command, think to yourself whether or not this
new command is actually necessary. Sometimes, the functionality you desire
can be added to an existing command. Also remember to add unit tests for
your command. If it isn't used very frequently, changes to the rest of
Spack can cause your command to break without sufficient unit tests to
prevent this from happening.

Whenever you add/remove/rename a command or flags for an existing command,
make sure to update Spack's `Bash tab completion script
<https://github.com/adamjstewart/spack/blob/develop/share/spack/spack-completion.bash>`_.


-------------
Writing Hooks
-------------

A hook is a callback that makes it easy to design functions that run
for different events. We do this by way of defining hook types, and then
inserting them at different places in the spack code base. Whenever a hook
type triggers by way of a function call, we find all the hooks of that type,
and run them.

Spack defines hooks by way of a module at ``lib/spack/spack/hooks`` where we can define
types of hooks in the ``__init__.py``, and then python files in that folder
can use hook functions. The files are automatically parsed, so if you write
a new file for some integration (e.g., ``lib/spack/spack/hooks/myintegration.py``
you can then write hook functions in that file that will be automatically detected,
and run whenever your hook is called. This section will cover the basic kind
of hooks, and how to write them.

^^^^^^^^^^^^^^
Types of Hooks
^^^^^^^^^^^^^^

The following hooks are currently implemented to make it easy for you,
the developer, to add hooks at different stages of a spack install or similar.
If there is a hook that you would like and is missing, you can propose to add a new one.

"""""""""""""""""""""
``pre_install(spec)``
"""""""""""""""""""""

A ``pre_install`` hook is run within an install subprocess, directly before
the install starts. It expects a single argument of a spec, and is run in
a multiprocessing subprocess. Note that if you see ``pre_install`` functions associated with packages these are not hooks
as we have defined them here, but rather callback functions associated with
a package install.


""""""""""""""""""""""
``post_install(spec)``
""""""""""""""""""""""

A ``post_install`` hook is run within an install subprocess, directly after
the install finishes, but before the build stage is removed. If you
write one of these hooks, you should expect it to accept a spec as the only
argument. This is run in a multiprocessing subprocess. This ``post_install`` is
also seen in packages, but in this context not related to the hooks described
here.


""""""""""""""""""""""""""
``on_install_start(spec)``
""""""""""""""""""""""""""

This hook is run at the beginning of ``lib/spack/spack/installer.py``,
in the install function of a ``PackageInstaller``,
and importantly is not part of a build process, but before it. This is when
we have just newly grabbed the task, and are preparing to install. If you
write a hook of this type, you should provide the spec to it.

.. code-block:: python

    def on_install_start(spec):
        """On start of an install, we want to...
        """
        print('on_install_start')


""""""""""""""""""""""""""""
``on_install_success(spec)``
""""""""""""""""""""""""""""

This hook is run on a successful install, and is also run inside the build
process, akin to ``post_install``. The main difference is that this hook
is run outside of the context of the stage directory, meaning after the
build stage has been removed and the user is alerted that the install was
successful. If you need to write a hook that is run on success of a particular
phase, you should use ``on_phase_success``.

""""""""""""""""""""""""""""
``on_install_failure(spec)``
""""""""""""""""""""""""""""

This hook is run given an install failure that happens outside of the build
subprocess, but somewhere in ``installer.py`` when something else goes wrong.
If you need to write a hook that is relevant to a failure within a build
process, you would want to instead use ``on_phase_failure``.


"""""""""""""""""""""""""""
``on_install_cancel(spec)``
"""""""""""""""""""""""""""

The same, but triggered if a spec install is cancelled for any reason.


"""""""""""""""""""""""""""""""""""""""""""""""
``on_phase_success(pkg, phase_name, log_file)``
"""""""""""""""""""""""""""""""""""""""""""""""

This hook is run within the install subprocess, and specifically when a phase
successfully finishes. Since we are interested in the package, the name of
the phase, and any output from it, we require:

 - **pkg**: the package variable, which also has the attached spec at ``pkg.spec``
 - **phase_name**: the name of the phase that was successful (e.g., configure)
 - **log_file**: the path to the file with output, in case you need to inspect or otherwise interact with it.

"""""""""""""""""""""""""""""""""""""""""""""
``on_phase_error(pkg, phase_name, log_file)``
"""""""""""""""""""""""""""""""""""""""""""""

In the case of an error during a phase, we might want to trigger some event
with a hook, and this is the purpose of this particular hook. Akin to
``on_phase_success`` we require the same variables - the package that failed,
the name of the phase, and the log file where we might find errors.

"""""""""""""""""""""""""""""""""
``on_analyzer_save(pkg, result)``
"""""""""""""""""""""""""""""""""

After an analyzer has saved some result for a package, this hook is called,
and it provides the package that we just ran the analysis for, along with
the loaded result. Typically, a result is structured to have the name
of the analyzer as key, and the result object that is defined in detail in
:ref:`analyzer_run_function`.

.. code-block:: python

    def on_analyzer_save(pkg, result):
        """given a package and a result...
        """
        print('Do something extra with a package analysis result here')


^^^^^^^^^^^^^^^^^^^^^^
Adding a New Hook Type
^^^^^^^^^^^^^^^^^^^^^^

Adding a new hook type is very simple!  In ``lib/spack/spack/hooks/__init__.py``
you can simply create a new ``HookRunner`` that is named to match your new hook.
For example, let's say you want to add a new hook called ``post_log_write``
to trigger after anything is written to a logger. You would add it as follows:

.. code-block:: python

    # pre/post install and run by the install subprocess
    pre_install = HookRunner('pre_install')
    post_install = HookRunner('post_install')

    # hooks related to logging
    post_log_write = HookRunner('post_log_write') # <- here is my new hook!


You then need to decide what arguments my hook would expect. Since this is
related to logging, let's say that you want a message and level. That means
that when you add a python file to the ``lib/spack/spack/hooks``
folder with one or more callbacks intended to be triggered by this hook. You might
use my new hook as follows:

.. code-block:: python

    def post_log_write(message, level):
        """Do something custom with the messsage and level every time we write
        to the log
        """
        print('running post_log_write!')


To use the hook, we would call it as follows somewhere in the logic to do logging.
In this example, we use it outside of a logger that is already defined:

.. code-block:: python

    import spack.hooks

    # We do something here to generate a logger and message
    spack.hooks.post_log_write(message, logger.level)


This is not to say that this would be the best way to implement an integration
with the logger (you'd probably want to write a custom logger, or you could
have the hook defined within the logger) but serves as an example of writing a hook.

----------
Unit tests
----------

------------
Unit testing
------------

---------------------
Developer environment
---------------------

.. warning::

    This is an experimental feature. It is expected to change and you should
    not use it in a production environment.


When installing a package, we currently have support to export environment
variables to specify adding debug flags to the build. By default, a package
install will build without any debug flag. However, if you want to add them,
you can export:

.. code-block:: console

   export SPACK_ADD_DEBUG_FLAGS=true
   spack install zlib


If you want to add custom flags, you should export an additional variable:

.. code-block:: console

   export SPACK_ADD_DEBUG_FLAGS=true
   export SPACK_DEBUG_FLAGS="-g"
   spack install zlib

These environment variables will eventually be integrated into spack so
they are set from the command line.

------------------
Developer commands
------------------

.. _cmd-spack-doc:

^^^^^^^^^^^^^
``spack doc``
^^^^^^^^^^^^^

.. _cmd-spack-style:

^^^^^^^^^^^^^^^
``spack style``
^^^^^^^^^^^^^^^

spack style exists to help the developer user to check imports and style with
mypy, flake8, isort, and (soon) black. To run all style checks, simply do:

.. code-block:: console

    $ spack style

To run automatic fixes for isort you can do:

.. code-block:: console

    $ spack style --fix

You do not need any of these Python packages installed on your system for
the checks to work! Spack will bootstrap install them from packages for
your use.

^^^^^^^^^^^^^^^^^^^
``spack unit-test``
^^^^^^^^^^^^^^^^^^^

See the :ref:`contributor guide section <cmd-spack-unit-test>` on
``spack unit-test``.

.. _cmd-spack-python:

^^^^^^^^^^^^^^^^
``spack python``
^^^^^^^^^^^^^^^^

``spack python`` is a command that lets you import and debug things as if
you were in a Spack interactive shell. Without any arguments, it is similar
to a normal interactive Python shell, except you can import spack and any
other Spack modules:

.. code-block:: console

   $ spack python
   Spack version 0.10.0
   Python 2.7.13, Linux x86_64
   >>> from spack.version import Version
   >>> a = Version('1.2.3')
   >>> b = Version('1_2_3')
   >>> a == b
   True
   >>> c = Version('1.2.3b')
   >>> c > a
   True
   >>>

If you prefer using an IPython interpreter, given that IPython is installed
you can specify the interpreter with ``-i``:

.. code-block:: console

   $ spack python -i ipython
   Python 3.8.3 (default, May 19 2020, 18:47:26)
   Type 'copyright', 'credits' or 'license' for more information
   IPython 7.17.0 -- An enhanced Interactive Python. Type '?' for help.


   Spack version 0.16.0
   Python 3.8.3, Linux x86_64

   In [1]:


With either interpreter you can run a single command:

.. code-block:: console

   $ spack python -c 'import distro; distro.linux_distribution()'
   ('Ubuntu', '18.04', 'Bionic Beaver')

   $ spack python -i ipython -c 'import distro; distro.linux_distribution()'
   Out[1]: ('Ubuntu', '18.04', 'Bionic Beaver')

or a file:

.. code-block:: console

   $ spack python ~/test_fetching.py
   $ spack python -i ipython ~/test_fetching.py

just like you would with the normal ``python`` command.


.. _cmd-spack-url:


^^^^^^^^^^^^^^^
``spack blame``
^^^^^^^^^^^^^^^

Spack blame is a way to quickly see contributors to packages or files
in the spack repository. You should provide a target package name or
file name to the command. Here is an example asking to see contributions
for the package "python":

.. code-block:: console

    $ spack blame python
    LAST_COMMIT  LINES  %      AUTHOR            EMAIL
    2 weeks ago  3      0.3    Mickey Mouse   <cheddar@gmouse.org>
    a month ago  927    99.7   Minnie Mouse   <swiss@mouse.org>

    2 weeks ago  930    100.0


By default, you will get a table view (shown above) sorted by date of contribution,
with the most recent contribution at the top.  If you want to sort instead
by percentage of code contribution, then add ``-p``:

.. code-block:: console

    $ spack blame -p python


And to see the git blame view, add ``-g`` instead:


.. code-block:: console

    $ spack blame -g python


Finally, to get a json export of the data, add ``--json``:

.. code-block:: console

    $ spack blame --json python


^^^^^^^^^^^^^
``spack url``
^^^^^^^^^^^^^

A package containing a single URL can be used to download several different
versions of the package. If you've ever wondered how this works, all of the
magic is in :mod:`spack.url`. This module contains methods for extracting
the name and version of a package from its URL. The name is used by
``spack create`` to guess the name of the package. By determining the version
from the URL, Spack can replace it with other versions to determine where to
download them from.

The regular expressions in ``parse_name_offset`` and ``parse_version_offset``
are used to extract the name and version, but they aren't perfect. In order
to debug Spack's URL parsing support, the ``spack url`` command can be used.

"""""""""""""""""""
``spack url parse``
"""""""""""""""""""

If you need to debug a single URL, you can use the following command:

.. command-output:: spack url parse http://cache.ruby-lang.org/pub/ruby/2.2/ruby-2.2.0.tar.gz

You'll notice that the name and version of this URL are correctly detected,
and you can even see which regular expressions it was matched to. However,
you'll notice that when it substitutes the version number in, it doesn't
replace the ``2.2`` with ``9.9`` where we would expect ``9.9.9b`` to live.
This particular package may require a ``list_url`` or ``url_for_version``
function.

This command also accepts a ``--spider`` flag. If provided, Spack searches
for other versions of the package and prints the matching URLs.

""""""""""""""""""
``spack url list``
""""""""""""""""""

This command lists every URL in every package in Spack. If given the
``--color`` and ``--extrapolation`` flags, it also colors the part of
the string that it detected to be the name and version. The
``--incorrect-name`` and ``--incorrect-version`` flags can be used to
print URLs that were not being parsed correctly.

"""""""""""""""""""""
``spack url summary``
"""""""""""""""""""""

This command attempts to parse every URL for every package in Spack
and prints a summary of how many of them are being correctly parsed.
It also prints a histogram showing which regular expressions are being
matched and how frequently:

.. command-output:: spack url summary

This command is essential for anyone adding or changing the regular
expressions that parse names and versions. By running this command
before and after the change, you can make sure that your regular
expression fixes more packages than it breaks.

---------
Profiling
---------

Spack has some limited built-in support for profiling, and can report
statistics using standard Python timing tools.  To use this feature,
supply ``--profile`` to Spack on the command line, before any subcommands.

.. _spack-p:

^^^^^^^^^^^^^^^^^^^
``spack --profile``
^^^^^^^^^^^^^^^^^^^

``spack --profile`` output looks like this:

.. command-output:: spack --profile graph hdf5
   :ellipsis: 25

The bottom of the output shows the top most time consuming functions,
slowest on top.  The profiling support is from Python's built-in tool,
`cProfile
<https://docs.python.org/2/library/profile.html#module-cProfile>`_.

.. _releases:

--------
Releases
--------

This section documents Spack's release process. It is intended for
project maintainers, as the tasks described here require maintainer
privileges on the Spack repository. For others, we hope this section at
least provides some insight into how the Spack project works.

.. _release-branches:

^^^^^^^^^^^^^^^^
Release branches
^^^^^^^^^^^^^^^^

There are currently two types of Spack releases: :ref:`major releases
<major-releases>` (``0.13.0``, ``0.14.0``, etc.) and :ref:`point releases
<point-releases>` (``0.13.1``, ``0.13.2``, ``0.13.3``, etc.). Here is a
diagram of how Spack release branches work::

    o    branch: develop  (latest version)
    |
    o    merge v0.14.1 into develop
    |\
    | o  branch: releases/v0.14, tag: v0.14.1
    o |  merge v0.14.0 into develop
    |\|
    | o  tag: v0.14.0
    |/
    o    merge v0.13.2 into develop
    |\
    | o  branch: releases/v0.13, tag: v0.13.2
    o |  merge v0.13.1 into develop
    |\|
    | o  tag: v0.13.1
    o |  merge v0.13.0 into develop
    |\|
    | o  tag: v0.13.0
    o |
    | o
    |/
    o

The ``develop`` branch has the latest contributions, and nearly all pull
requests target ``develop``.

Each Spack release series also has a corresponding branch, e.g.
``releases/v0.14`` has ``0.14.x`` versions of Spack, and
``releases/v0.13`` has ``0.13.x`` versions. A major release is the first
tagged version on a release branch. Minor releases are back-ported from
develop onto release branches. This is typically done by cherry-picking
bugfix commits off of ``develop``.

To avoid version churn for users of a release series, minor releases
**should not** make changes that would change the concretization of
packages. They should generally only contain fixes to the Spack core.
However, sometimes priorities are such that new functionality needs to
be added to a minor release.

Both major and minor releases are tagged. After each release, we merge
the release branch back into ``develop`` so that the version bump and any
other release-specific changes are visible in the mainline. As a
convenience, we also tag the latest release as ``releases/latest``,
so that users can easily check it out to get the latest
stable version. See :ref:`merging-releases` for more details.

^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Scheduling work for releases
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

We schedule work for releases by creating `GitHub projects
<https://github.com/spack/spack/projects>`_. At any time, there may be
several open release projects. For example, below are two releases (from
some past version of the page linked above):

.. image:: images/projects.png

This image shows one release in progress for ``0.15.1`` and another for
``0.16.0``. Each of these releases has a project board containing issues
and pull requests. GitHub shows a status bar with completed work in
green, work in progress in purple, and work not started yet in gray, so
it's fairly easy to see progress.

Spack's project boards are not firm commitments so we move work between
releases frequently. If we need to make a release and some tasks are not
yet done, we will simply move them to the next minor or major release, rather
than delaying the release to complete them.

For more on using GitHub project boards, see `GitHub's documentation
<https://docs.github.com/en/github/managing-your-work-on-github/about-project-boards>`_.


.. _major-releases:

^^^^^^^^^^^^^^^^^^^^^
Making major releases
^^^^^^^^^^^^^^^^^^^^^

Assuming a project board has already been created and all required work
completed, the steps to make the major release are:

#. Create two new project boards:

   * One for the next major release
   * One for the next point release

#. Move any optional tasks that are not done to one of the new project boards.

   In general, small bugfixes should go to the next point release. Major
   features, refactors, and changes that could affect concretization should
   go in the next major release.

#. Create a branch for the release, based on ``develop``:

   .. code-block:: console

      $ git checkout -b releases/v0.15 develop

   For a version ``vX.Y.Z``, the branch's name should be
   ``releases/vX.Y``. That is, you should create a ``releases/vX.Y``
   branch if you are preparing the ``X.Y.0`` release.

#. Bump the version in ``lib/spack/spack/__init__.py``.

   See `this example from 0.13.0
   <https://github.com/spack/spack/commit/8eeb64096c98b8a43d1c587f13ece743c864fba9>`_

#. Update ``CHANGELOG.md`` with major highlights in bullet form.

   Use proper markdown formatting, like `this example from 0.15.0
   <https://github.com/spack/spack/commit/d4bf70d9882fcfe88507e9cb444331d7dd7ba71c>`_.

#. Push the release branch to GitHub.

#. Make sure CI passes on the release branch, including:

   * Regular unit tests
   * Build tests
   * The E4S pipeline at `gitlab.spack.io <https://gitlab.spack.io>`_

   If CI is not passing, submit pull requests to ``develop`` as normal
   and keep rebasing the release branch on ``develop`` until CI passes.

#. Make sure the entire documentation is up to date. If documentation
   is outdated submit pull requests to ``develop`` as normal
   and keep rebasing the release branch on ``develop``.

#. Follow the steps in :ref:`publishing-releases`.

#. Follow the steps in :ref:`merging-releases`.

#. Follow the steps in :ref:`announcing-releases`.


.. _point-releases:

^^^^^^^^^^^^^^^^^^^^^
Making point releases
^^^^^^^^^^^^^^^^^^^^^

Assuming a project board has already been created and all required work
completed, the steps to make the point release are:

#. Create a new project board for the next point release.

#. Move any optional tasks that are not done to the next project board.

#. Check out the release branch (it should already exist).

    For the ``X.Y.Z`` release, the release branch is called ``releases/vX.Y``.
    For ``v0.15.1``, you would check out ``releases/v0.15``:

   .. code-block:: console

      $ git checkout releases/v0.15

#. Cherry-pick each pull request in the ``Done`` column of the release
   project board onto the release branch.

   This is **usually** fairly simple since we squash the commits from the
   vast majority of pull requests. That means there is only one commit
   per pull request to cherry-pick. For example, `this pull request
   <https://github.com/spack/spack/pull/15777>`_ has three commits, but
   they were squashed into a single commit on merge. You can see the
   commit that was created here:

   .. image:: images/pr-commit.png

   You can easily cherry pick it like this (assuming you already have the
   release branch checked out):

   .. code-block:: console

      $ git cherry-pick 7e46da7

   For pull requests that were rebased (or not squashed), you'll need to
   cherry-pick each associated commit individually.

   .. warning::

      It is important to cherry-pick commits in the order they happened,
      otherwise you can get conflicts while cherry-picking. When
      cherry-picking onto a point release, look at the merge date,
      **not** the number of the pull request or the date it was opened.

      Sometimes you may **still** get merge conflicts even if you have
      cherry-picked all the commits in order. This generally means there
      is some other intervening pull request that the one you're trying
      to pick depends on. In these cases, you'll need to make a judgment
      call regarding those pull requests.  Consider the number of affected
      files and or the resulting differences.

      1. If the dependency changes are small, you might just cherry-pick it,
         too. If you do this, add the task to the release board.

      2. If the changes are large, then you may decide that this fix is not
         worth including in a point release, in which case you should remove
         the task from the release project.

      3. You can always decide to manually back-port the fix to the release
         branch if neither of the above options makes sense, but this can
         require a lot of work. It's seldom the right choice.

#. Bump the version in ``lib/spack/spack/__init__.py``.

   See `this example from 0.14.1
   <https://github.com/spack/spack/commit/ff0abb9838121522321df2a054d18e54b566b44a>`_.

#. Update ``CHANGELOG.md`` with a list of the changes.

   This is typically a summary of the commits you cherry-picked onto the
   release branch. See `the changelog from 0.14.1
   <https://github.com/spack/spack/commit/ff0abb9838121522321df2a054d18e54b566b44a>`_.

#. Push the release branch to GitHub.

#. Make sure CI passes on the release branch, including:

   * Regular unit tests
   * Build tests
   * The E4S pipeline at `gitlab.spack.io <https://gitlab.spack.io>`_

   If CI does not pass, you'll need to figure out why, and make changes
   to the release branch until it does. You can make more commits, modify
   or remove cherry-picked commits, or cherry-pick **more** from
   ``develop`` to make this happen.

#. Follow the steps in :ref:`publishing-releases`.

#. Follow the steps in :ref:`merging-releases`.

#. Follow the steps in :ref:`announcing-releases`.


.. _publishing-releases:

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Publishing a release on GitHub
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

#. Create the release in GitHub.

   * Go to
     `github.com/spack/spack/releases <https://github.com/spack/spack/releases>`_
     and click ``Draft a new release``.

   * Set ``Tag version`` to the name of the tag that will be created.

     The name should start with ``v`` and contain *all three*
     parts of the version (e.g. ``v0.15.0`` or ``v0.15.1``).

   * Set ``Target`` to the ``releases/vX.Y`` branch (e.g., ``releases/v0.15``).

   * Set ``Release title`` to ``vX.Y.Z`` to match the tag (e.g., ``v0.15.1``).

   * Paste the latest release markdown from your ``CHANGELOG.md`` file as the text.

   * Save the draft so you can keep coming back to it as you prepare the release.

#. When you are ready to finalize the release, click ``Publish release``.

#. Immediately after publishing, go back to
   `github.com/spack/spack/releases
   <https://github.com/spack/spack/releases>`_ and download the
   auto-generated ``.tar.gz`` file for the release. It's the ``Source
   code (tar.gz)`` link.

#. Click ``Edit`` on the release you just made and attach the downloaded
   release tarball as a binary. This does two things:

   #. Makes sure that the hash of our releases does not change over time.

      GitHub sometimes annoyingly changes the way they generate tarballs
      that can result in the hashes changing if you rely on the
      auto-generated tarball links.

   #. Gets download counts on releases visible through the GitHub API.

      GitHub tracks downloads of artifacts, but *not* the source
      links. See the `releases
      page <https://api.github.com/repos/spack/spack/releases>`_ and search
      for ``download_count`` to see this.

#. Go to `readthedocs.org <https://readthedocs.org/projects/spack>`_ and
   activate the release tag.

   This builds the documentation and makes the released version
   selectable in the versions menu.


.. _merging-releases:

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Updating `releases/latest` and `develop`
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

If the new release is the **highest** Spack release yet, you should
also tag it as ``releases/latest``. For example, suppose the highest
release is currently ``0.15.3``:

* If you are releasing ``0.15.4`` or ``0.16.0``, then you should tag
  it with ``releases/latest``, as these are higher than ``0.15.3``.

* If you are making a new release of an **older** major version of
  Spack, e.g. ``0.14.4``, then you should not tag it as
  ``releases/latest`` (as there are newer major versions).

To tag ``releases/latest``, do this:

.. code-block:: console

   $ git checkout releases/vX.Y     # vX.Y is the new release's branch
   $ git tag --force releases/latest
   $ git push --force --tags

The ``--force`` argument to ``git tag`` makes ``git`` overwrite the existing
``releases/latest`` tag with the new one.

We also merge each release that we tag as ``releases/latest`` into ``develop``.
Make sure to do this with a merge commit:

.. code-block:: console

   $ git checkout develop
   $ git merge --no-ff -s ours vX.Y.Z  # vX.Y.Z is the new release's tag
   $ git push

We merge back to ``develop`` because it:

  * updates the version and ``CHANGELOG.md`` on ``develop``; and
  * ensures that your release tag is reachable from the head of
    ``develop``.

We *must* use a real merge commit (via the ``--no-ff`` option) to
ensure that the release tag is reachable from the tip of ``develop``.
This is necessary for ``spack -V`` to work properly -- it uses ``git
describe --tags`` to find the last reachable tag in the repository and
reports how far we are from it. For example:

.. code-block:: console

   $ spack -V
   0.14.2-1486-b80d5e74e5

This says that we are at commit ``b80d5e74e5``, which is 1,486 commits
ahead of the ``0.14.2`` release.

We put this step last in the process because it's best to do it only once
the release is complete and tagged. If you do it before you've tagged the
release and later decide you want to tag some later commit, you'll need
to merge again.


.. _announcing-releases:

^^^^^^^^^^^^^^^^^^^^
Announcing a release
^^^^^^^^^^^^^^^^^^^^

We announce releases in all of the major Spack communication channels.
Publishing the release takes care of GitHub. The remaining channels are
Twitter, Slack, and the mailing list. Here are the steps:

#. Announce the release on Twitter.

   * Compose the tweet on the ``@spackpm`` account per the
     ``spack-twitter`` slack channel.

   * Be sure to include a link to the release's page on GitHub.

     You can base the tweet on `this
     example <https://twitter.com/spackpm/status/1231761858182307840>`_.

#. Announce the release on Slack.

   * Compose a message in the ``#general`` Slack channel
     (`spackpm.slack.com <https://spackpm.slack.com>`_).

   * Preface the message with ``@channel`` to notify even those
     people not currently logged in.

   * Be sure to include a link to the tweet above.

   The tweet will be shown inline so that you do not have to retype
   your release announcement.

#. Announce the release on the Spack mailing list.

   * Compose an email to the Spack mailing list.

   * Be sure to include a link to the release's page on GitHub.

   * It is also helpful to include some information directly in the
     email.

   You can base your announcement on this `example
   email <https://groups.google.com/forum/#!topic/spack/WT4CT9i_X4s>`_.

Once you've completed the above steps, congratulations, you're done!
You've finished making the release!
.. Copyright 2013-2022 Lawrence Livermore National Security, LLC and other
   Spack Project Developers. See the top-level COPYRIGHT file for details.

   SPDX-License-Identifier: (Apache-2.0 OR MIT)

.. _monitoring:

==========
Monitoring
==========

You can use a `spack monitor <https://github.com/spack/spack-monitor>`_ "Spackmon"
server to store a database of your packages, builds, and associated metadata 
for provenance, research, or some other kind of development. You should
follow the instructions in the `spack monitor documentation <https://spack-monitor.readthedocs.org>`_
to first create a server along with a username and token for yourself.
You can then use this guide to interact with the server.

-------------------
Analysis Monitoring
-------------------

To read about how to monitor an analysis (meaning you want to send analysis results
to a server) see :ref:`analyze_monitoring`.

---------------------
Monitoring An Install
---------------------

Since an install is typically when you build packages, we logically want
to tell spack to monitor during this step. Let's start with an example
where we want to monitor the install of hdf5. Unless you have disabled authentication
for the server, we first want to export our spack monitor token and username to the environment:

.. code-block:: console
 
    $ export SPACKMON_TOKEN=50445263afd8f67e59bd79bff597836ee6c05438
    $ export SPACKMON_USER=spacky


By default, the host for your server is expected to be at ``http://127.0.0.1``
with a prefix of ``ms1``, and if this is the case, you can simply add the
``--monitor`` flag to the install command:

.. code-block:: console

    $ spack install --monitor hdf5


If you need to customize the host or the prefix, you can do that as well:

.. code-block:: console

    $ spack install --monitor --monitor-prefix monitor --monitor-host https://monitor-service.io hdf5


As a precaution, we cut out early in the spack client if you have not provided
authentication credentials. For example, if you run the command above without
exporting your username or token, you'll see:

.. code-block:: console

    ==> Error: You are required to export SPACKMON_TOKEN and SPACKMON_USER

This extra check is to ensure that we don't start any builds,
and then discover that you forgot to export your token. However, if 
your monitoring server has authentication disabled, you can tell this to
the client to skip this step:

.. code-block:: console

    $ spack install --monitor --monitor-disable-auth hdf5

If the service is not running, you'll cleanly exit early - the install will
not continue if you've asked it to monitor and there is no service.
For example, here is what you'll see if the monitoring service is not running:

.. code-block:: console

    [Errno 111] Connection refused


If you want to continue builds (and stop monitoring) you can set the ``--monitor-keep-going``
flag. 

.. code-block:: console

    $ spack install --monitor --monitor-keep-going hdf5

This could mean that if a request fails, you only have partial or no data
added to your monitoring database. This setting will not be applied to the
first request to check if the server is running, but to subsequent requests.
If you don't have a monitor server running and you want to build, simply
don't provide the ``--monitor`` flag! Finally, if you want to provide one or
more tags to your build, you can do:

.. code-block:: console

    # Add one tag, "pizza"
    $ spack install --monitor --monitor-tags pizza hdf5

    # Add two tags, "pizza" and "pasta"
    $ spack install --monitor --monitor-tags pizza,pasta hdf5


----------------------------
Monitoring with Containerize
----------------------------

The same argument group is available to add to a containerize command. 

^^^^^^
Docker
^^^^^^

To add monitoring to a Docker container recipe generation using the defaults,
and assuming a monitor server running on localhost, you would
start with a spack.yaml in your present working directory:

.. code-block:: yaml

   spack:
     specs:
       - samtools

And then do:

.. code-block:: console

    # preview first
    spack containerize --monitor
    
    # and then write to a Dockerfile
    spack containerize --monitor > Dockerfile
    
    
The install command will be edited to include commands for enabling monitoring.
However, getting secrets into the container for your monitor server is something
that should be done carefully. Specifically you should:

 - Never try to define secrets as ENV, ARG, or using ``--build-arg``
 - Do not try to get the secret into the container via a "temporary" file that you remove (it in fact will still exist in a layer)

Instead, it's recommended to use buildkit `as explained here <https://pythonspeed.com/articles/docker-build-secrets/>`_.
You'll need to again export environment variables for your spack monitor server:

.. code-block:: console

    $ export SPACKMON_TOKEN=50445263afd8f67e59bd79bff597836ee6c05438
    $ export SPACKMON_USER=spacky

And then use buildkit along with your build and identifying the name of the secret:

.. code-block:: console

    $ DOCKER_BUILDKIT=1 docker build --secret id=st,env=SPACKMON_TOKEN --secret id=su,env=SPACKMON_USER -t spack/container . 
    
The secrets are expected to come from your environment, and then will be temporarily mounted and available
at ``/run/secrets/<name>``. If you forget to supply them (and authentication is required) the build
will fail. If you need to build on your host (and interact with a spack monitor at localhost) you'll
need to tell Docker to use the host network:

.. code-block:: console

    $ DOCKER_BUILDKIT=1 docker build --network="host" --secret id=st,env=SPACKMON_TOKEN --secret id=su,env=SPACKMON_USER -t spack/container . 
    

^^^^^^^^^^^
Singularity
^^^^^^^^^^^

To add monitoring to a Singularity container build, the spack.yaml needs to
be modified slightly to specify wanting a different format:


.. code-block:: yaml

   spack:
     specs:
       - samtools
     container:
       format: singularity
       
       
Again, generate the recipe:


.. code-block:: console

    # preview first
    $ spack containerize --monitor
    
    # then write to a Singularity recipe
    $ spack containerize --monitor > Singularity


Singularity doesn't have a direct way to define secrets at build time, so we have
to do a bit of a manual command to add a file, source secrets in it, and remove it.
Since Singularity doesn't have layers like Docker, deleting a file will truly
remove it from the container and history. So let's say we have this file,
``secrets.sh``:

.. code-block:: console

    # secrets.sh
    export SPACKMON_USER=spack
    export SPACKMON_TOKEN=50445263afd8f67e59bd79bff597836ee6c05438


We would then generate the Singularity recipe, and add a files section,
a source of that file at the start of ``%post``, and **importantly**
a removal of the final at the end of that same section.

.. code-block::

    Bootstrap: docker
    From: spack/ubuntu-bionic:latest
    Stage: build

    %files
      secrets.sh /opt/secrets.sh 

    %post
      . /opt/secrets.sh

      # spack install commands are here
      ...
      
      # Don't forget to remove here!
      rm /opt/secrets.sh


You can then build the container as your normally would.

.. code-block:: console

    $ sudo singularity build container.sif Singularity 


------------------
Monitoring Offline
------------------

In the case that you want to save monitor results to your filesystem
and then upload them later (perhaps you are in an environment where you don't
have credentials or it isn't safe to use them) you can use the ``--monitor-save-local``
flag.

.. code-block:: console

    $ spack install --monitor --monitor-save-local hdf5 

This will save results in a subfolder, "monitor" in your designated spack
reports folder, which defaults to ``$HOME/.spack/reports/monitor``. When
you are ready to upload them to a spack monitor server:


.. code-block:: console

    $ spack monitor upload ~/.spack/reports/monitor 


You can choose the root directory of results as shown above, or a specific
subdirectory. The command accepts other arguments to specify configuration
for the monitor.
.. Copyright 2013-2022 Lawrence Livermore National Security, LLC and other
   Spack Project Developers. See the top-level COPYRIGHT file for details.

   SPDX-License-Identifier: (Apache-2.0 OR MIT)

.. _pipelines:

=========
Pipelines
=========

Spack provides commands that support generating and running automated build
pipelines designed for Gitlab CI.  At the highest level it works like this:
provide a spack environment describing the set of packages you care about,
and include within that environment file a description of how those packages
should be mapped to Gitlab runners.  Spack can then generate a ``.gitlab-ci.yml``
file containing job descriptions for all your packages that can be run by a
properly configured Gitlab CI instance.  When run, the generated pipeline will
build and deploy binaries, and it can optionally report to a CDash instance
regarding the health of the builds as they evolve over time.

------------------------------
Getting started with pipelines
------------------------------

It is fairly straightforward to get started with automated build pipelines.  At
a minimum, you'll need to set up a Gitlab instance (more about Gitlab CI
`here <https://about.gitlab.com/product/continuous-integration/>`_) and configure
at least one `runner <https://docs.gitlab.com/runner/>`_.  Then the basic steps
for setting up a build pipeline are as follows:

#. Create a repository on your gitlab instance
#. Add a ``spack.yaml`` at the root containing your pipeline environment
#. Add a ``.gitlab-ci.yml`` at the root containing two jobs (one to generate
   the pipeline dynamically, and one to run the generated jobs).
#. Push a commit containing the ``spack.yaml`` and ``.gitlab-ci.yml`` mentioned above
   to the gitlab repository

See the :ref:`functional_example` section for a minimal working example.  See also
the :ref:`custom_Workflow` section for a link to an example of a custom workflow
based on spack pipelines.

While it is possible to set up pipelines on gitlab.com, as illustrated above, the
builds there are limited to 60 minutes and generic hardware.  It is also possible to
`hook up <https://about.gitlab.com/blog/2018/04/24/getting-started-gitlab-ci-gcp>`_
Gitlab to Google Kubernetes Engine (`GKE <https://cloud.google.com/kubernetes-engine/>`_)
or Amazon Elastic Kubernetes Service (`EKS <https://aws.amazon.com/eks>`_), though those
topics are outside the scope of this document.

Spack's pipelines are now making use of the
`trigger <https://docs.gitlab.com/ee/ci/yaml/#trigger>`_ syntax to run
dynamically generated
`child pipelines <https://docs.gitlab.com/ee/ci/pipelines/parent_child_pipelines.html>`_.
Note that the use of dynamic child pipelines requires running Gitlab version
``>= 12.9``.

.. _functional_example:

------------------
Functional Example
------------------

The simplest fully functional standalone example of a working pipeline can be
examined live at this example `project <https://gitlab.com/scott.wittenburg/spack-pipeline-demo>`_
on gitlab.com.

Here's the ``.gitlab-ci.yml`` file from that example that builds and runs the
pipeline:

.. code-block:: yaml

   stages: [generate, build]

   variables:
     SPACK_REPO: https://github.com/scottwittenburg/spack.git
     SPACK_REF: pipelines-reproducible-builds

   generate-pipeline:
     stage: generate
     tags:
       - docker
     image:
       name: ghcr.io/scottwittenburg/ecpe4s-ubuntu18.04-runner-x86_64:2020-09-01
       entrypoint: [""]
     before_script:
       - git clone ${SPACK_REPO}
       - pushd spack && git checkout ${SPACK_REF} && popd
       - . "./spack/share/spack/setup-env.sh"
     script:
       - spack env activate --without-view .
       - spack -d ci generate
         --artifacts-root "${CI_PROJECT_DIR}/jobs_scratch_dir"
         --output-file "${CI_PROJECT_DIR}/jobs_scratch_dir/pipeline.yml"
     artifacts:
       paths:
         - "${CI_PROJECT_DIR}/jobs_scratch_dir"

   build-jobs:
     stage: build
     trigger:
       include:
         - artifact: "jobs_scratch_dir/pipeline.yml"
           job: generate-pipeline
       strategy: depend

The key thing to note above is that there are two jobs: The first job to run,
``generate-pipeline``, runs the ``spack ci generate`` command to generate a
dynamic child pipeline and write it to a yaml file, which is then picked up
by the second job, ``build-jobs``, and used to trigger the downstream pipeline.

And here's the spack environment built by the pipeline represented as a
``spack.yaml`` file:

.. code-block:: yaml

   spack:
     view: false
     concretization: separately

     definitions:
     - pkgs:
       - zlib
       - bzip2
     - arch:
       - '%gcc@7.5.0 arch=linux-ubuntu18.04-x86_64'

     specs:
     - matrix:
       - - $pkgs
       - - $arch

     mirrors: { "mirror": "s3://spack-public/mirror" }

     gitlab-ci:
       before_script:
         - git clone ${SPACK_REPO}
         - pushd spack && git checkout ${SPACK_CHECKOUT_VERSION} && popd
         - . "./spack/share/spack/setup-env.sh"
       script:
         - pushd ${SPACK_CONCRETE_ENV_DIR} && spack env activate --without-view . && popd
         - spack -d ci rebuild
       mappings:
         - match: ["os=ubuntu18.04"]
           runner-attributes:
             image:
               name: ghcr.io/scottwittenburg/ecpe4s-ubuntu18.04-runner-x86_64:2020-09-01
               entrypoint: [""]
             tags:
               - docker
       enable-artifacts-buildcache: True
       rebuild-index: False

The elements of this file important to spack ci pipelines are described in more
detail below, but there are a couple of things to note about the above working
example:

Normally ``enable-artifacts-buildcache`` is not recommended in production as it
results in large binary artifacts getting transferred back and forth between
gitlab and the runners.  But in this example on gitlab.com where there is no
shared, persistent file system, and where no secrets are stored for giving
permission to write to an S3 bucket, ``enabled-buildcache-artifacts`` is the only
way to propagate binaries from jobs to their dependents.

Also, it is usually a good idea to let the pipeline generate a final "rebuild the
buildcache index" job, so that subsequent pipeline generation can quickly determine
which specs are up to date and which need to be rebuilt (it's a good idea for other
reasons as well, but those are out of scope for this discussion).  In this case we
have disabled it (using ``rebuild-index: False``) because the index would only be
generated in the artifacts mirror anyway, and consequently would not be available
during subesequent pipeline runs.

.. note::
   With the addition of reproducible builds (#22887) a previously working
   pipeline will require some changes:

   * In the build jobs (``runner-attributes``), the environment location changed.
     This will typically show as a ``KeyError`` in the failing job. Be sure to
     point to ``${SPACK_CONCRETE_ENV_DIR}``.

   * When using ``include`` in your environment, be sure to make the included
     files available in the build jobs. This means adding those files to the
     artifact directory. Those files will also be missing in the reproducibility
     artifact.

   * Because the location of the environment changed, including files with
     relative path may have to be adapted to work both in the project context
     (generation job) and in the concrete env dir context (build job).

-----------------------------------
Spack commands supporting pipelines
-----------------------------------

Spack provides a ``ci`` command with a few sub-commands supporting spack
ci pipelines.  These commands are covered in more detail in this section.

.. _cmd-spack-ci:

^^^^^^^^^^^^^^^^^^
``spack ci``
^^^^^^^^^^^^^^^^^^

Super-command for functionality related to generating pipelines and executing
pipeline jobs.

.. _cmd-spack-ci-generate:

^^^^^^^^^^^^^^^^^^^^^
``spack ci generate``
^^^^^^^^^^^^^^^^^^^^^

Concretizes the specs in the active environment, stages them (as described in
:ref:`staging_algorithm`), and writes the resulting ``.gitlab-ci.yml`` to disk.
During concretization of the environment, ``spack ci generate`` also writes a
``spack.lock`` file which is then provided to generated child jobs and made
available in all generated job artifacts to aid in reproducing failed builds
in a local environment.  This means there are two artifacts that need to be
exported in your pipeline generation job (defined in your ``.gitlab-ci.yml``).
The first is the output yaml file of ``spack ci generate``, and the other is
the directory containing the concrete environment files.  In the
:ref:`functional_example` section, we only mentioned one path in the
``artifacts`` ``paths`` list because we used ``--artifacts-root`` as the
top level directory containing both the generated pipeline yaml and the
concrete environment.

Using ``--prune-dag`` or ``--no-prune-dag`` configures whether or not jobs are
generated for specs that are already up to date on the mirror.   If enabling
DAG pruning using ``--prune-dag``, more information may be required in your
``spack.yaml`` file, see the :ref:`noop_jobs` section below regarding
``service-job-attributes``.

The optional ``--check-index-only`` argument can be used to speed up pipeline
generation by telling spack to consider only remote buildcache indices when
checking the remote mirror to determine if each spec in the DAG is up to date
or not.  The default behavior is for spack to fetch the index and check it,
but if the spec is not found in the index, to also perform a direct check for
the spec on the mirror.  If the remote buildcache index is out of date, which
can easily happen if it is not updated frequently, this behavior ensures that
spack has a way to know for certain about the status of any concrete spec on
the remote mirror, but can slow down pipeline generation significantly.

The ``--optimize`` argument is experimental and runs the generated pipeline
document through a series of optimization passes designed to reduce the size
of the generated file.

The ``--dependencies`` is also experimental and disables what in Gitlab is
referred to as DAG scheduling, internally using the ``dependencies`` keyword
rather than ``needs`` to list dependency jobs.  The drawback of using this option
is that before any job can begin, all jobs in previous stages must first
complete.  The benefit is that Gitlab allows more dependencies to be listed
when using ``dependencies`` instead of ``needs``.

The optional ``--output-file`` argument should be an absolute path (including
file name) to the generated pipeline, and if not given, the default is
``./.gitlab-ci.yml``.

While optional, the ``--artifacts-root`` argument is used to determine where
the concretized environment directory should be located.  This directory will
be created by ``spack ci generate`` and will contain the ``spack.yaml`` and
generated ``spack.lock`` which are then passed to all child jobs as an
artifact.  This directory will also be the root directory for all artifacts
generated by jobs in the pipeline.

.. _cmd-spack-ci-rebuild:

^^^^^^^^^^^^^^^^^^^^^
``spack ci rebuild``
^^^^^^^^^^^^^^^^^^^^^

The purpose of the ``spack ci rebuild`` is straightforward: take its assigned
spec job, check whether the target mirror already has a binary for that spec,
and if not, build the spec from source and push the binary to the mirror.  To
accomplish this in a reproducible way, the sub-command prepares a ``spack install``
command line to build a single spec in the DAG, saves that command in a
shell script, ``install.sh``, in the current working directory, and then runs
it to install the spec.  The shell script is also exported as an artifact to
aid in reproducing the build outside of the CI environment.

If it was necessary to install the spec from source, ``spack ci rebuild`` will
also subsequently create a binary package for the spec and try to push it to the
mirror.

The ``spack ci rebuild`` sub-command mainly expects its "input" to come either
from environment variables or from the ``gitlab-ci`` section of the ``spack.yaml``
environment file.  There are two main sources of the environment variables, some
are written into ``.gitlab-ci.yml`` by ``spack ci generate``, and some are
provided by the GitLab CI runtime.

.. _cmd-spack-ci-rebuild-index:

^^^^^^^^^^^^^^^^^^^^^^^^^^
``spack ci rebuild-index``
^^^^^^^^^^^^^^^^^^^^^^^^^^

This is a convenience command to rebuild the buildcache index associated with
the mirror in the active, gitlab-enabled environment (specifying the mirror
url or name is not required).

.. _cmd-spack-ci-reproduce-build:

^^^^^^^^^^^^^^^^^^^^^^^^^^^^
``spack ci reproduce-build``
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Given the url to a gitlab pipeline rebuild job, downloads and unzips the
artifacts into a local directory (which can be specified with the optional
``--working-dir`` argument), then finds the target job in the generated
pipeline to extract details about how it was run.  Assuming the job used a
docker image, the command prints a ``docker run`` command line and some basic
instructions on how to reproduce the build locally.

Note that jobs failing in the pipeline will print messages giving the
arguments you can pass to ``spack ci reproduce-build`` in order to reproduce
a particular build locally.

------------------------------------
A pipeline-enabled spack environment
------------------------------------

Here's an example of a spack environment file that has been enhanced with
sections describing a build pipeline:

.. code-block:: yaml

   spack:
     definitions:
     - pkgs:
       - readline@7.0
     - compilers:
       - '%gcc@5.5.0'
     - oses:
       - os=ubuntu18.04
       - os=centos7
     specs:
     - matrix:
       - [$pkgs]
       - [$compilers]
       - [$oses]
     mirrors:
       cloud_gitlab: https://mirror.spack.io
     gitlab-ci:
       mappings:
         - match:
             - os=ubuntu18.04
           runner-attributes:
             tags:
               - spack-kube
             image: spack/ubuntu-bionic
         - match:
             - os=centos7
           runner-attributes:
             tags:
               - spack-kube
             image: spack/centos7
     cdash:
       build-group: Release Testing
       url: https://cdash.spack.io
       project: Spack
       site: Spack AWS Gitlab Instance

Hopefully, the ``definitions``, ``specs``, ``mirrors``, etc. sections are already
familiar, as they are part of spack :ref:`environments`.  So let's take a more
in-depth look some of the pipeline-related sections in that environment file
that might not be as familiar.

The ``gitlab-ci`` section is used to configure how the pipeline workload should be
generated, mainly how the jobs for building specs should be assigned to the
configured runners on your instance.  Each entry within the list of ``mappings``
corresponds to a known gitlab runner, where the ``match`` section is used
in assigning a release spec to one of the runners, and the ``runner-attributes``
section is used to configure the spec/job for that particular runner.

Both the top-level ``gitlab-ci`` section as well as each ``runner-attributes``
section can also contain the following keys: ``image``, ``tags``, ``variables``,
``before_script``, ``script``, and ``after_script``.  If any of these keys are
provided at the ``gitlab-ci`` level, they will be used as the defaults for any
``runner-attributes``, unless they are overridden in those sections.  Specifying
any of these keys at the ``runner-attributes`` level generally overrides the
keys specified at the higher level, with a couple exceptions.  Any ``variables``
specified at both levels result in those dictionaries getting merged in the
resulting generated job, and any duplicate variable names get assigned the value
provided in the specific ``runner-attributes``.  If ``tags`` are specified both
at the ``gitlab-ci`` level as well as the ``runner-attributes`` level, then the
lists of tags are combined, and any duplicates are removed.

See the section below on using a custom spack for an example of how these keys
could be used.

There are other pipeline options you can configure within the ``gitlab-ci`` section
as well.

The ``bootstrap`` section allows you to specify lists of specs from
your ``definitions`` that should be staged ahead of the environment's ``specs`` (this
section is described in more detail below).  The ``enable-artifacts-buildcache`` key
takes a boolean and determines whether the pipeline uses artifacts to store and
pass along the buildcaches from one stage to the next (the default if you don't
provide this option is ``False``).

The optional ``broken-specs-url`` key tells Spack to check against a list of
specs that are known to be currently broken in ``develop``. If any such specs
are found, the ``spack ci generate`` command will fail with an error message
informing the user what broken specs were encountered. This allows the pipeline
to fail early and avoid wasting compute resources attempting to build packages
that will not succeed.

The optional ``cdash`` section provides information that will be used by the
``spack ci generate`` command (invoked by ``spack ci start``) for reporting
to CDash.  All the jobs generated from this environment will belong to a
"build group" within CDash that can be tracked over time.  As the release
progresses, this build group may have jobs added or removed. The url, project,
and site are used to specify the CDash instance to which build results should
be reported.

Take a look at the
`schema <https://github.com/spack/spack/blob/develop/lib/spack/spack/schema/gitlab_ci.py>`_
for the gitlab-ci section of the spack environment file, to see precisely what
syntax is allowed there.

.. _rebuild_index:

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Note about rebuilding buildcache index
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

By default, while a pipeline job may rebuild a package, create a buildcache
entry, and push it to the mirror, it does not automatically re-generate the
mirror's buildcache index afterward.  Because the index is not needed by the
default rebuild jobs in the pipeline, not updating the index at the end of
each job avoids possible race conditions between simultaneous jobs, and it
avoids the computational expense of regenerating the index.  This potentially
saves minutes per job, depending on the number of binary packages in the
mirror.  As a result, the default is that the mirror's buildcache index may
not correctly reflect the mirror's contents at the end of a pipeline.

To make sure the buildcache index is up to date at the end of your pipeline,
spack generates a job to update the buildcache index of the target mirror
at the end of each pipeline by default.  You can disable this behavior by
adding ``rebuild-index: False`` inside the ``gitlab-ci`` section of your
spack environment.  Spack will assign the job any runner attributes found
on the ``service-job-attributes``, if you have provided that in your
``spack.yaml``.

.. _noop_jobs:

^^^^^^^^^^^^^^^^^^^^^^^
Note about "no-op" jobs
^^^^^^^^^^^^^^^^^^^^^^^

If no specs in an environment need to be rebuilt during a given pipeline run
(meaning all are already up to date on the mirror), a single succesful job
(a NO-OP) is still generated to avoid an empty pipeline (which GitLab
considers to be an error).  An optional ``service-job-attributes`` section
can be added to your ``spack.yaml`` where you can provide ``tags`` and
``image`` or ``variables`` for the generated NO-OP job.  This section also
supports providing ``before_script``, ``script``, and ``after_script``, in
case you want to take some custom actions in the case of any empty pipeline.

Following is an example of this section added to a ``spack.yaml``:

.. code-block:: yaml

   spack:
     specs:
       - openmpi
     mirrors:
       cloud_gitlab: https://mirror.spack.io
     gitlab-ci:
       mappings:
         - match:
             - os=centos8
           runner-attributes:
             tags:
               - custom
               - tag
             image: spack/centos7
       service-job-attributes:
         tags: ['custom', 'tag']
         image:
           name: 'some.image.registry/custom-image:latest'
           entrypoint: ['/bin/bash']
         script:
           - echo "Custom message in a custom script"

The example above illustrates how you can provide the attributes used to run
the NO-OP job in the case of an empty pipeline.  The only field for the NO-OP
job that might be generated for you is ``script``, but that will only happen
if you do not provide one yourself.

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Assignment of specs to runners
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The ``mappings`` section corresponds to a list of runners, and during assignment
of specs to runners, the list is traversed in order looking for matches, the
first runner that matches a release spec is assigned to build that spec.  The
``match`` section within each runner mapping section is a list of specs, and
if any of those specs match the release spec (the ``spec.satisfies()`` method
is used), then that runner is considered a match.

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Configuration of specs/jobs for a runner
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Once a runner has been chosen to build a release spec, the ``runner-attributes``
section provides information determining details of the job in the context of
the runner.  The ``runner-attributes`` section must have a ``tags`` key, which
is a list containing at least one tag used to select the runner from among the
runners known to the gitlab instance.  For Docker executor type runners, the
``image`` key is used to specify the Docker image used to build the release spec
(and could also appear as a dictionary with a ``name`` specifying the image name,
as well as an ``entrypoint`` to override whatever the default for that image is).
For other types of runners the ``variables`` key will be useful to pass any
information on to the runner that it needs to do its work (e.g. scheduler
parameters, etc.).  Any ``variables`` provided here will be added, verbatim, to
each job.

The ``runner-attributes`` section also allows users to supply custom ``script``,
``before_script``, and ``after_script`` sections to be applied to every job
scheduled on that runner.  This allows users to do any custom preparation or
cleanup tasks that fit their particular workflow, as well as completely
customize the rebuilding of a spec if they so choose.  Spack will not generate
a ``before_script`` or ``after_script`` for jobs, but if you do not provide
a custom ``script``, spack will generate one for you that assumes the concrete
environment directory is located within your ``--artifacts_root`` (or if not
provided, within your ``$CI_PROJECT_DIR``), activates that environment for
you, and invokes ``spack ci rebuild``.

.. _staging_algorithm:

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Summary of ``.gitlab-ci.yml`` generation algorithm
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

All specs yielded by the matrix (or all the specs in the environment) have their
dependencies computed, and the entire resulting set of specs are staged together
before being run through the ``gitlab-ci/mappings`` entries, where each staged
spec is assigned a runner.  "Staging" is the name given to the process of
figuring out in what order the specs should be built, taking into consideration
Gitlab CI rules about jobs/stages.  In the staging process the goal is to maximize
the number of jobs in any stage of the pipeline, while ensuring that the jobs in
any stage only depend on jobs in previous stages (since those jobs are guaranteed
to have completed already).  As a runner is determined for a job, the information
in the ``runner-attributes`` is used to populate various parts of the job
description that will be used by Gitlab CI. Once all the jobs have been assigned
a runner, the ``.gitlab-ci.yml`` is written to disk.

The short example provided above would result in the ``readline``, ``ncurses``,
and ``pkgconf`` packages getting staged and built on the runner chosen by the
``spack-k8s`` tag.  In this example, spack assumes the runner is a Docker executor
type runner, and thus certain jobs will be run in the ``centos7`` container,
and others in the ``ubuntu-18.04`` container.  The resulting ``.gitlab-ci.yml``
will contain 6 jobs in three stages.  Once the jobs have been generated, the
presence of a ``SPACK_CDASH_AUTH_TOKEN`` environment variable during the
``spack ci generate`` command would result in all of the jobs being put in a
build group on CDash called "Release Testing" (that group will be created if
it didn't already exist).

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Optional compiler bootstrapping
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Spack pipelines also have support for bootstrapping compilers on systems that
may not already have the desired compilers installed. The idea here is that
you can specify a list of things to bootstrap in your ``definitions``, and
spack will guarantee those will be installed in a phase of the pipeline before
your release specs, so that you can rely on those packages being available in
the binary mirror when you need them later on in the pipeline.  At the moment
the only viable use-case for bootstrapping is to install compilers.

Here's an example of what bootstrapping some compilers might look like:

.. code-block:: yaml

   spack:
     definitions:
     - compiler-pkgs:
       - 'llvm+clang@6.0.1 os=centos7'
       - 'gcc@6.5.0 os=centos7'
       - 'llvm+clang@6.0.1 os=ubuntu18.04'
       - 'gcc@6.5.0 os=ubuntu18.04'
     - pkgs:
       - readline@7.0
     - compilers:
       - '%gcc@5.5.0'
       - '%gcc@6.5.0'
       - '%gcc@7.3.0'
       - '%clang@6.0.0'
       - '%clang@6.0.1'
     - oses:
       - os=ubuntu18.04
       - os=centos7
     specs:
     - matrix:
       - [$pkgs]
       - [$compilers]
       - [$oses]
       exclude:
         - '%gcc@7.3.0 os=centos7'
         - '%gcc@5.5.0 os=ubuntu18.04'
     gitlab-ci:
       bootstrap:
         - name: compiler-pkgs
           compiler-agnostic: true
       mappings:
         # mappings similar to the example higher up in this description
         ...

The example above adds a list to the ``definitions`` called ``compiler-pkgs``
(you can add any number of these), which lists compiler packages that should
be staged ahead of the full matrix of release specs (in this example, only
readline).  Then within the ``gitlab-ci`` section, note the addition of a
``bootstrap`` section, which can contain a list of items, each referring to
a list in the ``definitions`` section.  These items can either
be a dictionary or a string.  If you supply a dictionary, it must have a name
key whose value must match one of the lists in definitions and it can have a
``compiler-agnostic`` key whose value is a boolean.  If you supply a string,
then it needs to match one of the lists provided in ``definitions``.  You can
think of the bootstrap list as an ordered list of pipeline "phases" that will
be staged before your actual release specs.  While this introduces another
layer of bottleneck in the pipeline (all jobs in all stages of one phase must
complete before any jobs in the next phase can begin), it also means you are
guaranteed your bootstrapped compilers will be available when you need them.

The ``compiler-agnostic`` key can be provided with each item in the
bootstrap list. It tells the ``spack ci generate`` command that any jobs staged
from that particular list should have the compiler removed from the spec, so
that any compiler available on the runner where the job is run can be used to
build the package.

When including a bootstrapping phase as in the example above, the result is that
the bootstrapped compiler packages will be pushed to the binary mirror (and the
local artifacts mirror) before the actual release specs are built. In this case,
the jobs corresponding to subsequent release specs are configured to
``install_missing_compilers``, so that if spack is asked to install a package
with a compiler it doesn't know about, it can be quickly installed from the
binary mirror first.

Since bootstrapping compilers is optional, those items can be left out of the
environment/stack file, and in that case no bootstrapping will be done (only the
specs will be staged for building) and the runners will be expected to already
have all needed compilers installed and configured for spack to use.

-------------------------------------
Using a custom spack in your pipeline
-------------------------------------

If your runners will not have a version of spack ready to invoke, or if for some
other reason you want to use a custom version of spack to run your pipelines,
this section provides an example of how you could take advantage of
user-provided pipeline scripts to accomplish this fairly simply.  First, consider
specifying the source and version of spack you want to use with variables, either
written directly into your ``.gitlab-ci.yml``, or provided by CI variables defined
in the gitlab UI or from some upstream pipeline.  Let's say you choose the variable
names ``SPACK_REPO`` and ``SPACK_REF`` to refer to the particular fork of spack
and branch you want for running your pipeline.  You can then refer to those in a
custom shell script invoked both from your pipeline generation job and your rebuild
jobs.  Here's the ``generate-pipeline`` job from the top of this document,
updated to clone and source a custom spack:

.. code-block:: yaml

   generate-pipeline:
     tags:
       - <some-other-tag>
   before_script:
     - git clone ${SPACK_REPO}
     - pushd spack && git checkout ${SPACK_REF} && popd
     - . "./spack/share/spack/setup-env.sh"
   script:
     - spack env activate --without-view .
     - spack ci generate --check-index-only
       --artifacts-root "${CI_PROJECT_DIR}/jobs_scratch_dir"
       --output-file "${CI_PROJECT_DIR}/jobs_scratch_dir/pipeline.yml"
   after_script:
     - rm -rf ./spack
   artifacts:
     paths:
       - "${CI_PROJECT_DIR}/jobs_scratch_dir"

That takes care of getting the desired version of spack when your pipeline is
generated by ``spack ci generate``.  You also want your generated rebuild jobs
(all of them) to clone that version of spack, so next you would update your
``spack.yaml`` from above as follows:

.. code-block:: yaml

   spack:
     ...
     gitlab-ci:
       mappings:
         - match:
             - os=ubuntu18.04
           runner-attributes:
             tags:
               - spack-kube
             image: spack/ubuntu-bionic
             before_script:
               - git clone ${SPACK_REPO}
               - pushd spack && git checkout ${SPACK_REF} && popd
               - . "./spack/share/spack/setup-env.sh"
             script:
               - spack env activate --without-view ${SPACK_CONCRETE_ENV_DIR}
               - spack -d ci rebuild
             after_script:
               - rm -rf ./spack

Now all of the generated rebuild jobs will use the same shell script to clone
spack before running their actual workload.

Now imagine you have long pipelines with many specs to be built, and you
are pointing to a spack repository and branch that has a tendency to change
frequently, such as the main repo and its ``develop`` branch.  If each child
job checks out the ``develop`` branch, that could result in some jobs running
with one SHA of spack, while later jobs run with another.  To help avoid this
issue, the pipeline generation process saves global variables called
``SPACK_VERSION`` and ``SPACK_CHECKOUT_VERSION`` that capture the version
of spack used to generate the pipeline.  While the ``SPACK_VERSION`` variable
simply contains the human-readable value produced by ``spack -V`` at pipeline
generation time, the ``SPACK_CHECKOUT_VERSION`` variable can be used in a
``git checkout`` command to make sure all child jobs checkout the same version
of spack used to generate the pipeline.  To take advantage of this, you could
simply replace ``git checkout ${SPACK_REF}`` in the example ``spack.yaml``
above with ``git checkout ${SPACK_CHECKOUT_VERSION}``.

On the other hand, if you're pointing to a spack repository and branch under your
control, there may be no benefit in using the captured ``SPACK_CHECKOUT_VERSION``,
and you can instead just clone using the variables you define (``SPACK_REPO``
and ``SPACK_REF`` in the example aboves).

.. _custom_workflow:

---------------
Custom Workflow
---------------

There are many ways to take advantage of spack CI pipelines to achieve custom
workflows for building packages or other resources.  One example of a custom
pipelines workflow is the spack tutorial container
`repo <https://github.com/spack/spack-tutorial-container>`_.  This project uses
GitHub (for source control), GitLab (for automated spack ci pipelines), and
DockerHub automated builds to build Docker images (complete with fully populate
binary mirror) used by instructors and participants of a spack tutorial.

Take a look a the repo to see how it is accomplished using spack CI pipelines,
and see the following markdown files at the root of the repository for
descriptions and documentation describing the workflow: ``DESCRIPTION.md``,
``DOCKERHUB_SETUP.md``, ``GITLAB_SETUP.md``, and ``UPDATING.md``.

.. _ci_environment_variables:

--------------------------------------------------
Environment variables affecting pipeline operation
--------------------------------------------------

Certain secrets and some other information should be provided to the pipeline
infrastructure via environment variables, usually for reasons of security, but
in some cases to support other pipeline use cases such as PR testing.  The
environment variables used by the pipeline infrastructure are described here.

^^^^^^^^^^^^^^^^^
AWS_ACCESS_KEY_ID
^^^^^^^^^^^^^^^^^

Optional.  Only needed when binary mirror is an S3 bucket.

^^^^^^^^^^^^^^^^^^^^^
AWS_SECRET_ACCESS_KEY
^^^^^^^^^^^^^^^^^^^^^

Optional.  Only needed when binary mirror is an S3 bucket.

^^^^^^^^^^^^^^^
S3_ENDPOINT_URL
^^^^^^^^^^^^^^^

Optional.  Only needed when binary mirror is an S3 bucket that is *not* on AWS.

^^^^^^^^^^^^^^^^^
CDASH_AUTH_TOKEN
^^^^^^^^^^^^^^^^^

Optional. Only needed in order to report build groups to CDash.

^^^^^^^^^^^^^^^^^
SPACK_SIGNING_KEY
^^^^^^^^^^^^^^^^^

Optional.  Only needed if you want ``spack ci rebuild`` to trust the key you
store in this variable, in which case, it will subsequently be used to sign and
verify binary packages (when installing or creating buildcaches).  You could
also have already trusted a key spack know about, or if no key is present anywhere,
spack will install specs using ``--no-check-signature`` and create buildcaches
using ``-u`` (for unsigned binaries).
.. Copyright 2013-2022 Lawrence Livermore National Security, LLC and other
   Spack Project Developers. See the top-level COPYRIGHT file for details.

   SPDX-License-Identifier: (Apache-2.0 OR MIT)

.. _contribution-guide:

==================
Contribution Guide
==================

This guide is intended for developers or administrators who want to
contribute a new package, feature, or bugfix to Spack.
It assumes that you have at least some familiarity with Git VCS and Github.
The guide will show a few examples of contributing workflows and discuss
the granularity of pull-requests (PRs). It will also discuss the tests your
PR must pass in order to be accepted into Spack.

First, what is a PR? Quoting `Bitbucket's tutorials <https://www.atlassian.com/git/tutorials/making-a-pull-request/>`_:

  Pull requests are a mechanism for a developer to notify team members that
  they have **completed a feature**. The pull request is more than just a
  notificationâ€”itâ€™s a dedicated forum for discussing the proposed feature.

Important is **completed feature**. The changes one proposes in a PR should
correspond to one feature/bugfix/extension/etc. One can create PRs with
changes relevant to different ideas, however reviewing such PRs becomes tedious
and error prone. If possible, try to follow the **one-PR-one-package/feature** rule.

--------
Branches
--------

Spack's ``develop`` branch has the latest contributions. Nearly all pull
requests should start from ``develop`` and target ``develop``.

There is a branch for each major release series. Release branches
originate from ``develop`` and have tags for each point release in the
series. For example, ``releases/v0.14`` has tags for ``0.14.0``,
``0.14.1``, ``0.14.2``, etc. versions of Spack. We backport important bug
fixes to these branches, but we do not advance the package versions or
make other changes that would change the way Spack concretizes
dependencies. Currently, the maintainers manage these branches by
cherry-picking from ``develop``. See :ref:`releases` for more
information.

----------------------
Continuous Integration
----------------------

Spack uses `Github Actions <https://docs.github.com/en/actions>`_ for Continuous Integration
testing. This means that every time you submit a pull request, a series of tests will
be run to make sure you didn't accidentally introduce any bugs into Spack. **Your PR
will not be accepted until it passes all of these tests.** While you can certainly wait
for the results of these tests after submitting a PR, we recommend that you run them
locally to speed up the review process.

.. note::

   Oftentimes, CI will fail for reasons other than a problem with your PR.
   For example, apt-get, pip, or homebrew will fail to download one of the
   dependencies for the test suite, or a transient bug will cause the unit tests
   to timeout. If any job fails, click the "Details" link and click on the test(s)
   that is failing. If it doesn't look like it is failing for reasons related to
   your PR, you have two options. If you have write permissions for the Spack
   repository, you should see a "Restart workflow" button on the right-hand side. If
   not, you can close and reopen your PR to rerun all of the tests. If the same
   test keeps failing, there may be a problem with your PR. If you notice that
   every recent PR is failing with the same error message, it may be that an issue
   occurred with the CI infrastructure or one of Spack's dependencies put out a
   new release that is causing problems. If this is the case, please file an issue.


We currently test against Python 2.7 and 3.5-3.9 on both macOS and Linux and
perform 3 types of tests:

.. _cmd-spack-unit-test:

^^^^^^^^^^
Unit Tests
^^^^^^^^^^

Unit tests ensure that core Spack features like fetching or spec resolution are
working as expected. If your PR only adds new packages or modifies existing ones,
there's very little chance that your changes could cause the unit tests to fail.
However, if you make changes to Spack's core libraries, you should run the unit
tests to make sure you didn't break anything.

Since they test things like fetching from VCS repos, the unit tests require
`git <https://git-scm.com/>`_, `mercurial <https://www.mercurial-scm.org/>`_,
and `subversion <https://subversion.apache.org/>`_ to run. Make sure these are
installed on your system and can be found in your ``PATH``. All of these can be
installed with Spack or with your system package manager.

To run *all* of the unit tests, use:

.. code-block:: console

   $ spack unit-test

These tests may take several minutes to complete. If you know you are
only modifying a single Spack feature, you can run subsets of tests at a
time.  For example, this would run all the tests in
``lib/spack/spack/test/architecture.py``:

.. code-block:: console

   $ spack unit-test lib/spack/spack/test/architecture.py

And this would run the ``test_platform`` test from that file:

.. code-block:: console

   $ spack unit-test lib/spack/spack/test/architecture.py::test_platform

This allows you to develop iteratively: make a change, test that change,
make another change, test that change, etc.  We use `pytest
<http://pytest.org/>`_ as our tests framework, and these types of
arguments are just passed to the ``pytest`` command underneath. See `the
pytest docs
<http://doc.pytest.org/en/latest/usage.html#specifying-tests-selecting-tests>`_
for more details on test selection syntax.

``spack unit-test`` has a few special options that can help you
understand what tests are available.  To get a list of all available
unit test files, run:

.. command-output:: spack unit-test --list
   :ellipsis: 5

To see a more detailed list of available unit tests, use ``spack
unit-test --list-long``:

.. command-output:: spack unit-test --list-long
   :ellipsis: 10

And to see the fully qualified names of all tests, use ``--list-names``:

.. command-output:: spack unit-test --list-names
   :ellipsis: 5

You can combine these with ``pytest`` arguments to restrict which tests
you want to know about.  For example, to see just the tests in
``architecture.py``:

.. command-output:: spack unit-test --list-long lib/spack/spack/test/architecture.py

You can also combine any of these options with a ``pytest`` keyword
search.  See the `pytest usage docs
<https://docs.pytest.org/en/stable/usage.html#specifying-tests-selecting-tests>`_:
for more details on test selection syntax. For example, to see the names of all tests that have "spec"
or "concretize" somewhere in their names:

.. command-output:: spack unit-test --list-names -k "spec and concretize"

By default, ``pytest`` captures the output of all unit tests, and it will
print any captured output for failed tests. Sometimes it's helpful to see
your output interactively, while the tests run (e.g., if you add print
statements to a unit tests).  To see the output *live*, use the ``-s``
argument to ``pytest``:

.. code-block:: console

   $ spack unit-test -s --list-long lib/spack/spack/test/architecture.py::test_platform

Unit tests are crucial to making sure bugs aren't introduced into
Spack. If you are modifying core Spack libraries or adding new
functionality, please add new unit tests for your feature, and consider
strengthening existing tests.  You will likely be asked to do this if you
submit a pull request to the Spack project on GitHub.  Check out the
`pytest docs <http://pytest.org/>`_ and feel free to ask for guidance on
how to write tests!

.. note::

   You may notice the ``share/spack/qa/run-unit-tests`` script in the
   repository.  This script is designed for CI.  It runs the unit
   tests and reports coverage statistics back to Codecov. If you want to
   run the unit tests yourself, we suggest you use ``spack unit-test``.

^^^^^^^^^^^^
Style Tests
^^^^^^^^^^^^

Spack uses `Flake8 <http://flake8.pycqa.org/en/latest/>`_ to test for
`PEP 8 <https://www.python.org/dev/peps/pep-0008/>`_ conformance and
`mypy <https://mypy.readthedocs.io/en/stable/>` for type checking. PEP 8 is
a series of style guides for Python that provide suggestions for everything
from variable naming to indentation. In order to limit the number of PRs that
were mostly style changes, we decided to enforce PEP 8 conformance. Your PR
needs to comply with PEP 8 in order to be accepted, and if it modifies the
spack library it needs to successfully type-check with mypy as well.

Testing for compliance with spack's style is easy. Simply run the ``spack style``
command:

.. code-block:: console

   $ spack style

``spack style`` has a couple advantages over running the tools by hand:

#. It only tests files that you have modified since branching off of
   ``develop``.

#. It works regardless of what directory you are in.

#. It automatically adds approved exemptions from the ``flake8``
   checks. For example, URLs are often longer than 80 characters, so we
   exempt them from line length checks. We also exempt lines that start
   with "homepage", "url", "version", "variant", "depends_on", and
   "extends" in ``package.py`` files.  This is now also possible when directly
   running flake8 if you can use the ``spack`` formatter plugin included with
   spack.

More approved flake8 exemptions can be found
`here <https://github.com/spack/spack/blob/develop/.flake8>`_.

If all is well, you'll see something like this:

.. code-block:: console

   $ run-flake8-tests
   Dependencies found.
   =======================================================
   flake8: running flake8 code checks on spack.

   Modified files:

     var/spack/repos/builtin/packages/hdf5/package.py
     var/spack/repos/builtin/packages/hdf/package.py
     var/spack/repos/builtin/packages/netcdf/package.py
   =======================================================
   Flake8 checks were clean.

However, if you aren't compliant with PEP 8, flake8 will complain:

.. code-block:: console

   var/spack/repos/builtin/packages/netcdf/package.py:26: [F401] 'os' imported but unused
   var/spack/repos/builtin/packages/netcdf/package.py:61: [E303] too many blank lines (2)
   var/spack/repos/builtin/packages/netcdf/package.py:106: [E501] line too long (92 > 79 characters)
   Flake8 found errors.

Most of the error messages are straightforward, but if you don't understand what
they mean, just ask questions about them when you submit your PR. The line numbers
will change if you add or delete lines, so simply run ``spack style`` again
to update them.

.. tip::

   Try fixing flake8 errors in reverse order. This eliminates the need for
   multiple runs of ``spack style`` just to re-compute line numbers and
   makes it much easier to fix errors directly off of the CI output.

.. warning::

   Flake8 and ``pep8-naming`` require a number of dependencies in order
   to run.  If you installed ``py-flake8`` and ``py-pep8-naming``, the
   easiest way to ensure the right packages are on your ``PYTHONPATH`` is
   to run::

     spack activate py-flake8
     spack activate pep8-naming

   so that all of the dependencies are symlinked to a central
   location. If you see an error message like:

   .. code-block:: console

      Traceback (most recent call last):
        File: "/usr/bin/flake8", line 5, in <module>
          from pkg_resources import load_entry_point
      ImportError: No module named pkg_resources

   that means Flake8 couldn't find setuptools in your ``PYTHONPATH``.

^^^^^^^^^^^^^^^^^^^
Documentation Tests
^^^^^^^^^^^^^^^^^^^

Spack uses `Sphinx <http://www.sphinx-doc.org/en/stable/>`_ to build its
documentation. In order to prevent things like broken links and missing imports,
we added documentation tests that build the documentation and fail if there
are any warning or error messages.

Building the documentation requires several dependencies:

* sphinx
* sphinxcontrib-programoutput
* sphinx-rtd-theme
* graphviz
* git
* mercurial
* subversion

All of these can be installed with Spack, e.g.

.. code-block:: console

   $ spack install py-sphinx py-sphinxcontrib-programoutput py-sphinx-rtd-theme graphviz git mercurial subversion

.. warning::

   Sphinx has `several required dependencies <https://github.com/spack/spack/blob/develop/var/spack/repos/builtin/packages/py-sphinx/package.py>`_.
   If you're using a ``python`` from Spack and you installed
   ``py-sphinx`` and friends, you need to make them available to your
   ``python``. The easiest way to do this is to run:

   .. code-block:: console

      $ spack activate py-sphinx
      $ spack activate py-sphinx-rtd-theme
      $ spack activate py-sphinxcontrib-programoutput

   so that all of the dependencies are symlinked into that Python's
   tree.  Alternatively, you could arrange for their library
   directories to be added to PYTHONPATH.  If you see an error message
   like:

   .. code-block:: console

      Extension error:
      Could not import extension sphinxcontrib.programoutput (exception: No module named sphinxcontrib.programoutput)
      make: *** [html] Error 1

   that means Sphinx couldn't find ``py-sphinxcontrib-programoutput`` in your
   ``PYTHONPATH``.

Once all of the dependencies are installed, you can try building the documentation:

.. code-block:: console

   $ cd path/to/spack/lib/spack/docs/
   $ make clean
   $ make

If you see any warning or error messages, you will have to correct those before
your PR is accepted.

If you are editing the documentation, you should obviously be running the
documentation tests. But even if you are simply adding a new package, your
changes could cause the documentation tests to fail:

.. code-block:: console

   package_list.rst:8745: WARNING: Block quote ends without a blank line; unexpected unindent.

At first, this error message will mean nothing to you, since you didn't edit
that file. Until you look at line 8745 of the file in question:

.. code-block:: rst

   Description:
      NetCDF is a set of software libraries and self-describing, machine-
     independent data formats that support the creation, access, and sharing
     of array-oriented scientific data.

Our documentation includes :ref:`a list of all Spack packages <package-list>`.
If you add a new package, its docstring is added to this page. The problem in
this case was that the docstring looked like:

.. code-block:: python

   class Netcdf(Package):
       """
       NetCDF is a set of software libraries and self-describing,
       machine-independent data formats that support the creation,
       access, and sharing of array-oriented scientific data.
       """

Docstrings cannot start with a newline character, or else Sphinx will complain.
Instead, they should look like:

.. code-block:: python

   class Netcdf(Package):
       """NetCDF is a set of software libraries and self-describing,
       machine-independent data formats that support the creation,
       access, and sharing of array-oriented scientific data."""

Documentation changes can result in much more obfuscated warning messages.
If you don't understand what they mean, feel free to ask when you submit
your PR.

--------
Coverage
--------

Spack uses `Codecov <https://codecov.io/>`_ to generate and report unit test
coverage. This helps us tell what percentage of lines of code in Spack are
covered by unit tests. Although code covered by unit tests can still contain
bugs, it is much less error prone than code that is not covered by unit tests.

Codecov provides `browser extensions <https://github.com/codecov/sourcegraph-codecov>`_
for Google Chrome and Firefox. These extensions integrate with GitHub
and allow you to see coverage line-by-line when viewing the Spack repository.
If you are new to Spack, a great way to get started is to write unit tests to
increase coverage!

Unlike with CI on Github Actions Codecov tests are not required to pass in order for your
PR to be merged. If you modify core Spack libraries, we would greatly
appreciate unit tests that cover these changed lines. Otherwise, we have no
way of knowing whether or not your changes introduce a bug. If you make
substantial changes to the core, we may request unit tests to increase coverage.

.. note::

   If the only files you modified are package files, we do not care about
   coverage on your PR. You may notice that the Codecov tests fail even though
   you didn't modify any core files. This means that Spack's overall coverage
   has increased since you branched off of develop. This is a good thing!
   If you really want to get the Codecov tests to pass, you can rebase off of
   the latest develop, but again, this is not required.


-------------
Git Workflows
-------------

Spack is still in the beta stages of development. Most of our users run off of
the develop branch, and fixes and new features are constantly being merged. So
how do you keep up-to-date with upstream while maintaining your own local
differences and contributing PRs to Spack?

^^^^^^^^^
Branching
^^^^^^^^^

The easiest way to contribute a pull request is to make all of your changes on
new branches. Make sure your ``develop`` is up-to-date and create a new branch
off of it:

.. code-block:: console

   $ git checkout develop
   $ git pull upstream develop
   $ git branch <descriptive_branch_name>
   $ git checkout <descriptive_branch_name>

Here we assume that the local ``develop`` branch tracks the upstream develop
branch of Spack. This is not a requirement and you could also do the same with
remote branches. But for some it is more convenient to have a local branch that
tracks upstream.

Normally we prefer that commits pertaining to a package ``<package-name>`` have
a message ``<package-name>: descriptive message``. It is important to add
descriptive message so that others, who might be looking at your changes later
(in a year or maybe two), would understand the rationale behind them.

Now, you can make your changes while keeping the ``develop`` branch pure.
Edit a few files and commit them by running:

.. code-block:: console

   $ git add <files_to_be_part_of_the_commit>
   $ git commit --message <descriptive_message_of_this_particular_commit>

Next, push it to your remote fork and create a PR:

.. code-block:: console

   $ git push origin <descriptive_branch_name> --set-upstream

GitHub provides a `tutorial <https://help.github.com/articles/about-pull-requests/>`_
on how to file a pull request. When you send the request, make ``develop`` the
destination branch.

If you need this change immediately and don't have time to wait for your PR to
be merged, you can always work on this branch. But if you have multiple PRs,
another option is to maintain a Frankenstein branch that combines all of your
other branches:

.. code-block:: console

   $ git co develop
   $ git branch <your_modified_develop_branch>
   $ git checkout <your_modified_develop_branch>
   $ git merge <descriptive_branch_name>

This can be done with each new PR you submit. Just make sure to keep this local
branch up-to-date with upstream ``develop`` too.

^^^^^^^^^^^^^^
Cherry-Picking
^^^^^^^^^^^^^^

What if you made some changes to your local modified develop branch and already
committed them, but later decided to contribute them to Spack? You can use
cherry-picking to create a new branch with only these commits.

First, check out your local modified develop branch:

.. code-block:: console

   $ git checkout <your_modified_develop_branch>

Now, get the hashes of the commits you want from the output of:

.. code-block:: console

   $ git log

Next, create a new branch off of upstream ``develop`` and copy the commits
that you want in your PR:

.. code-block:: console

   $ git checkout develop
   $ git pull upstream develop
   $ git branch <descriptive_branch_name>
   $ git checkout <descriptive_branch_name>
   $ git cherry-pick <hash>
   $ git push origin <descriptive_branch_name> --set-upstream

Now you can create a PR from the web-interface of GitHub. The net result is as
follows:

#. You patched your local version of Spack and can use it further.
#. You "cherry-picked" these changes in a stand-alone branch and submitted it
   as a PR upstream.

Should you have several commits to contribute, you could follow the same
procedure by getting hashes of all of them and cherry-picking to the PR branch.

.. note::

   It is important that whenever you change something that might be of
   importance upstream, create a pull request as soon as possible. Do not wait
   for weeks/months to do this, because:

   #. you might forget why you modified certain files
   #. it could get difficult to isolate this change into a stand-alone clean PR.

^^^^^^^^
Rebasing
^^^^^^^^

Other developers are constantly making contributions to Spack, possibly on the
same files that your PR changed. If their PR is merged before yours, it can
create a merge conflict. This means that your PR can no longer be automatically
merged without a chance of breaking your changes. In this case, you will be
asked to rebase on top of the latest upstream ``develop``.

First, make sure your develop branch is up-to-date:

.. code-block:: console

   $ git checkout develop
   $ git pull upstream develop

Now, we need to switch to the branch you submitted for your PR and rebase it
on top of develop:

.. code-block:: console

   $ git checkout <descriptive_branch_name>
   $ git rebase develop

Git will likely ask you to resolve conflicts. Edit the file that it says can't
be merged automatically and resolve the conflict. Then, run:

.. code-block:: console

   $ git add <file_that_could_not_be_merged>
   $ git rebase --continue

You may have to repeat this process multiple times until all conflicts are resolved.
Once this is done, simply force push your rebased branch to your remote fork:

.. code-block:: console

   $ git push --force origin <descriptive_branch_name>

^^^^^^^^^^^^^^^^^^^^^^^^^
Rebasing with cherry-pick
^^^^^^^^^^^^^^^^^^^^^^^^^

You can also perform a rebase using ``cherry-pick``. First, create a temporary
backup branch:

.. code-block:: console

   $ git checkout <descriptive_branch_name>
   $ git branch tmp

If anything goes wrong, you can always go back to your ``tmp`` branch.
Now, look at the logs and save the hashes of any commits you would like to keep:

.. code-block:: console

   $ git log

Next, go back to the original branch and reset it to ``develop``.
Before doing so, make sure that you local ``develop`` branch is up-to-date
with upstream:

.. code-block:: console

   $ git checkout develop
   $ git pull upstream develop
   $ git checkout <descriptive_branch_name>
   $ git reset --hard develop

Now you can cherry-pick relevant commits:

.. code-block:: console

   $ git cherry-pick <hash1>
   $ git cherry-pick <hash2>

Push the modified branch to your fork:

.. code-block:: console

   $ git push --force origin <descriptive_branch_name>

If everything looks good, delete the backup branch:

.. code-block:: console

   $ git branch --delete --force tmp

^^^^^^^^^^^^^^^^^^
Re-writing History
^^^^^^^^^^^^^^^^^^

Sometimes you may end up on a branch that has diverged so much from develop
that it cannot easily be rebased. If the current commits history is more of
an experimental nature and only the net result is important, you may rewrite
the history.

First, merge upstream ``develop`` and reset you branch to it. On the branch
in question, run:

.. code-block:: console

   $ git merge develop
   $ git reset develop

At this point your branch will point to the same commit as develop and
thereby the two are indistinguishable. However, all the files that were
previously modified will stay as such. In other words, you do not lose the
changes you made. Changes can be reviewed by looking at diffs:

.. code-block:: console

   $ git status
   $ git diff

The next step is to rewrite the history by adding files and creating commits:

.. code-block:: console

   $ git add <files_to_be_part_of_commit>
   $ git commit --message <descriptive_message>

After all changed files are committed, you can push the branch to your fork
and create a PR:

.. code-block:: console

   $ git push origin --set-upstream
.. Copyright 2013-2022 Lawrence Livermore National Security, LLC and other
   Spack Project Developers. See the top-level COPYRIGHT file for details.

   SPDX-License-Identifier: (Apache-2.0 OR MIT)

.. _basic-usage:

===========
Basic Usage
===========

The ``spack`` command has many *subcommands*.  You'll only need a
small subset of them for typical usage.

Note that Spack colorizes output.  ``less -R`` should be used with
Spack to maintain this colorization.  E.g.:

.. code-block:: console

    $ spack find | less -R

It is recommended that the following be put in your ``.bashrc`` file:

.. code-block:: sh

    alias less='less -R'

If you do not see colorized output when using ``less -R`` it is because color
is being disabled in the piped output. In this case, tell spack to force
colorized output with a flag

.. code-block:: console

    $ spack --color always find | less -R

or an environment variable

.. code-block:: console

   $ SPACK_COLOR=always spack find | less -R

--------------------------
Listing available packages
--------------------------

To install software with Spack, you need to know what software is
available.  You can see a list of available package names at the
:ref:`package-list` webpage, or using the ``spack list`` command.

.. _cmd-spack-list:

^^^^^^^^^^^^^^
``spack list``
^^^^^^^^^^^^^^

The ``spack list`` command prints out a list of all of the packages Spack
can install:

.. command-output:: spack list
   :ellipsis: 10

There are thousands of them, so we've truncated the output above, but you
can find a :ref:`full list here <package-list>`.
Packages are listed by name in alphabetical order.
A pattern to match with no wildcards, ``*`` or ``?``,
will be treated as though it started and ended with
``*``, so ``util`` is equivalent to ``*util*``.  All patterns will be treated
as case-insensitive. You can also add the ``-d`` to search the description of
the package in addition to the name.  Some examples:

All packages whose names contain "sql":

.. command-output:: spack list sql

All packages whose names or descriptions contain documentation:

.. command-output:: spack list --search-description documentation

.. _cmd-spack-info:

^^^^^^^^^^^^^^
``spack info``
^^^^^^^^^^^^^^

To get more information on a particular package from `spack list`, use
`spack info`.  Just supply the name of a package:

.. command-output:: spack info mpich

Most of the information is self-explanatory.  The *safe versions* are
versions that Spack knows the checksum for, and it will use the
checksum to verify that these versions download without errors or
viruses.

:ref:`Dependencies <sec-specs>` and :ref:`virtual dependencies
<sec-virtual-dependencies>` are described in more detail later.

.. _cmd-spack-versions:

^^^^^^^^^^^^^^^^^^
``spack versions``
^^^^^^^^^^^^^^^^^^

To see *more* available versions of a package, run ``spack versions``.
For example:

.. command-output:: spack versions libelf

There are two sections in the output.  *Safe versions* are versions
for which Spack has a checksum on file.  It can verify that these
versions are downloaded correctly.

In many cases, Spack can also show you what versions are available out
on the web---these are *remote versions*.  Spack gets this information
by scraping it directly from package web pages.  Depending on the
package and how its releases are organized, Spack may or may not be
able to find remote versions.

---------------------------
Installing and uninstalling
---------------------------

.. _cmd-spack-install:

^^^^^^^^^^^^^^^^^
``spack install``
^^^^^^^^^^^^^^^^^

``spack install`` will install any package shown by ``spack list``.
For example, To install the latest version of the ``mpileaks``
package, you might type this:

.. code-block:: console

   $ spack install mpileaks

If ``mpileaks`` depends on other packages, Spack will install the
dependencies first.  It then fetches the ``mpileaks`` tarball, expands
it, verifies that it was downloaded without errors, builds it, and
installs it in its own directory under ``$SPACK_ROOT/opt``. You'll see
a number of messages from Spack, a lot of build output, and a message
that the package is installed.

.. code-block:: console

   $ spack install mpileaks
   ... dependency build output ...
   ==> Installing mpileaks-1.0-ph7pbnhl334wuhogmugriohcwempqry2
   ==> No binary for mpileaks-1.0-ph7pbnhl334wuhogmugriohcwempqry2 found: installing from source
   ==> mpileaks: Executing phase: 'autoreconf'
   ==> mpileaks: Executing phase: 'configure'
   ==> mpileaks: Executing phase: 'build'
   ==> mpileaks: Executing phase: 'install'
   [+] ~/spack/opt/linux-rhel7-broadwell/gcc-8.1.0/mpileaks-1.0-ph7pbnhl334wuhogmugriohcwempqry2

The last line, with the ``[+]``, indicates where the package is
installed.

Add the Spack debug option (one or more times) -- ``spack -d install
mpileaks`` -- to get additional (and even more verbose) output.

^^^^^^^^^^^^^^^^^^^^^^^^^^^
Building a specific version
^^^^^^^^^^^^^^^^^^^^^^^^^^^

Spack can also build *specific versions* of a package.  To do this,
just add ``@`` after the package name, followed by a version:

.. code-block:: console

   $ spack install mpich@3.0.4

Any number of versions of the same package can be installed at once
without interfering with each other.  This is good for multi-user
sites, as installing a version that one user needs will not disrupt
existing installations for other users.

In addition to different versions, Spack can customize the compiler,
compile-time options (variants), compiler flags, and platform (for
cross compiles) of an installation.  Spack is unique in that it can
also configure the *dependencies* a package is built with.  For example,
two configurations of the same version of a package, one built with boost
1.39.0, and the other version built with version 1.43.0, can coexist.

This can all be done on the command line using the *spec* syntax.
Spack calls the descriptor used to refer to a particular package
configuration a **spec**.  In the commands above, ``mpileaks`` and
``mpileaks@3.0.4`` are both valid *specs*.  We'll talk more about how
you can use them to customize an installation in :ref:`sec-specs`.

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Reusing installed dependencies
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. warning::

   The ``--reuse`` option described here is experimental, and it will
   likely be replaced with a different option and configuration settings
   in the next Spack release.

By default, when you run ``spack install``, Spack tries to build a new
version of the package you asked for, along with updated versions of
its dependencies.  This gets you the latest versions and configurations,
but it can result in unwanted rebuilds if you update Spack frequently.

If you want Spack to try hard to reuse existing installations as dependencies,
you can add the ``--reuse`` option:

.. code-block:: console

   $ spack install --reuse mpich

This will not do anything if ``mpich`` is already installed.  If ``mpich``
is not installed, but dependencies like ``hwloc`` and ``libfabric`` are,
the ``mpich`` will be build with the installed versions, if possible.
You can use the :ref:`spack spec -I <cmd-spack-spec>` command to see what
will be reused and what will be built before you install.

.. _cmd-spack-uninstall:

^^^^^^^^^^^^^^^^^^^
``spack uninstall``
^^^^^^^^^^^^^^^^^^^

To uninstall a package, type ``spack uninstall <package>``.  This will ask
the user for confirmation before completely removing the directory
in which the package was installed.

.. code-block:: console

   $ spack uninstall mpich

If there are still installed packages that depend on the package to be
uninstalled, spack will refuse to uninstall it.

To uninstall a package and every package that depends on it, you may give the
``--dependents`` option.

.. code-block:: console

   $ spack uninstall --dependents mpich

will display a list of all the packages that depend on ``mpich`` and, upon
confirmation, will uninstall them in the right order.

A command like

.. code-block:: console

   $ spack uninstall mpich

may be ambiguous if multiple ``mpich`` configurations are installed.
For example, if both ``mpich@3.0.2`` and ``mpich@3.1`` are installed,
``mpich`` could refer to either one. Because it cannot determine which
one to uninstall, Spack will ask you either to provide a version number
to remove the ambiguity or use the ``--all`` option to uninstall all of
the matching packages.

You may force uninstall a package with the ``--force`` option

.. code-block:: console

   $ spack uninstall --force mpich

but you risk breaking other installed packages. In general, it is safer to
remove dependent packages *before* removing their dependencies or use the
``--dependents`` option.


.. _nondownloadable:

^^^^^^^^^^^^^^^^^^
Garbage collection
^^^^^^^^^^^^^^^^^^

When Spack builds software from sources, if often installs tools that are needed
just to build or test other software. These are not necessary at runtime.
To support cases where removing these tools can be a benefit Spack provides
the ``spack gc`` ("garbage collector") command, which will uninstall all unneeded packages:

.. code-block:: console

   $ spack find
   ==> 24 installed packages
   -- linux-ubuntu18.04-broadwell / gcc@9.0.1 ----------------------
   autoconf@2.69    findutils@4.6.0  libiconv@1.16        libszip@2.1.1  m4@1.4.18    openjpeg@2.3.1  pkgconf@1.6.3  util-macros@1.19.1
   automake@1.16.1  gdbm@1.18.1      libpciaccess@0.13.5  libtool@2.4.6  mpich@3.3.2  openssl@1.1.1d  readline@8.0   xz@5.2.4
   cmake@3.16.1     hdf5@1.10.5      libsigsegv@2.12      libxml2@2.9.9  ncurses@6.1  perl@5.30.0     texinfo@6.5    zlib@1.2.11

   $ spack gc
   ==> The following packages will be uninstalled:

       -- linux-ubuntu18.04-broadwell / gcc@9.0.1 ----------------------
       vn47edz autoconf@2.69    6m3f2qn findutils@4.6.0  ubl6bgk libtool@2.4.6  pksawhz openssl@1.1.1d  urdw22a readline@8.0
       ki6nfw5 automake@1.16.1  fklde6b gdbm@1.18.1      b6pswuo m4@1.4.18      k3s2csy perl@5.30.0     lp5ya3t texinfo@6.5
       ylvgsov cmake@3.16.1     5omotir libsigsegv@2.12  leuzbbh ncurses@6.1    5vmfbrq pkgconf@1.6.3   5bmv4tg util-macros@1.19.1

   ==> Do you want to proceed? [y/N] y

   [ ... ]

   $ spack find
   ==> 9 installed packages
   -- linux-ubuntu18.04-broadwell / gcc@9.0.1 ----------------------
   hdf5@1.10.5  libiconv@1.16  libpciaccess@0.13.5  libszip@2.1.1  libxml2@2.9.9  mpich@3.3.2  openjpeg@2.3.1  xz@5.2.4  zlib@1.2.11

In the example above Spack went through all the packages in the package database
and removed everything that is not either:

1. A package installed upon explicit request of the user
2. A ``link`` or ``run`` dependency, even transitive, of one of the packages at point 1.

You can check :ref:`cmd-spack-find-metadata` to see how to query for explicitly installed packages
or :ref:`dependency-types` for a more thorough treatment of dependency types.

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Marking packages explicit or implicit
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

By default, Spack will mark packages a user installs as explicitly installed,
while all of its dependencies will be marked as implicitly installed. Packages
can be marked manually as explicitly or implicitly installed by using
``spack mark``. This can be used in combination with ``spack gc`` to clean up
packages that are no longer required.

.. code-block:: console

  $ spack install m4
  ==> 29005: Installing libsigsegv
  [...]
  ==> 29005: Installing m4
  [...]

  $ spack install m4 ^libsigsegv@2.11
  ==> 39798: Installing libsigsegv
  [...]
  ==> 39798: Installing m4
  [...]

  $ spack find -d
  ==> 4 installed packages
  -- linux-fedora32-haswell / gcc@10.1.1 --------------------------
  libsigsegv@2.11

  libsigsegv@2.12

  m4@1.4.18
      libsigsegv@2.12

  m4@1.4.18
      libsigsegv@2.11

  $ spack gc
  ==> There are no unused specs. Spack's store is clean.

  $ spack mark -i m4 ^libsigsegv@2.11
  ==> m4@1.4.18 : marking the package implicit

  $ spack gc
  ==> The following packages will be uninstalled:

      -- linux-fedora32-haswell / gcc@10.1.1 --------------------------
      5fj7p2o libsigsegv@2.11  c6ensc6 m4@1.4.18

  ==> Do you want to proceed? [y/N]

In the example above, we ended up with two versions of ``m4`` since they depend
on different versions of ``libsigsegv``. ``spack gc`` will not remove any of
the packages since both versions of ``m4`` have been installed explicitly
and both versions of ``libsigsegv`` are required by the ``m4`` packages.

``spack mark`` can also be used to implement upgrade workflows. The following
example demonstrates how the ``spack mark`` and ``spack gc`` can be used to
only keep the current version of a package installed.

When updating Spack via ``git pull``, new versions for either ``libsigsegv``
or ``m4`` might be introduced. This will cause Spack to install duplicates.
Since we only want to keep one version, we mark everything as implicitly
installed before updating Spack. If there is no new version for either of the
packages, ``spack install`` will simply mark them as explicitly installed and
``spack gc`` will not remove them.

.. code-block:: console

  $ spack install m4
  ==> 62843: Installing libsigsegv
  [...]
  ==> 62843: Installing m4
  [...]

  $ spack mark -i -a
  ==> m4@1.4.18 : marking the package implicit

  $ git pull
  [...]

  $ spack install m4
  [...]
  ==> m4@1.4.18 : marking the package explicit
  [...]

  $ spack gc
  ==> There are no unused specs. Spack's store is clean.

When using this workflow for installations that contain more packages, care
has to be taken to either only mark selected packages or issue ``spack install``
for all packages that should be kept.

You can check :ref:`cmd-spack-find-metadata` to see how to query for explicitly
or implicitly installed packages.

^^^^^^^^^^^^^^^^^^^^^^^^^
Non-Downloadable Tarballs
^^^^^^^^^^^^^^^^^^^^^^^^^

The tarballs for some packages cannot be automatically downloaded by
Spack.  This could be for a number of reasons:

#. The author requires users to manually accept a license agreement
   before downloading (``jdk`` and ``galahad``).

#. The software is proprietary and cannot be downloaded on the open
   Internet.

To install these packages, one must create a mirror and manually add
the tarballs in question to it (see :ref:`mirrors`):

#. Create a directory for the mirror.  You can create this directory
   anywhere you like, it does not have to be inside ``~/.spack``:

   .. code-block:: console

       $ mkdir ~/.spack/manual_mirror

#. Register the mirror with Spack by creating ``~/.spack/mirrors.yaml``:

   .. code-block:: yaml

       mirrors:
         manual: file://~/.spack/manual_mirror

#. Put your tarballs in it.  Tarballs should be named
   ``<package>/<package>-<version>.tar.gz``.  For example:

   .. code-block:: console

       $ ls -l manual_mirror/galahad

       -rw-------. 1 me me 11657206 Jun 21 19:25 galahad-2.60003.tar.gz

#. Install as usual:

   .. code-block:: console

       $ spack install galahad


-------------------------
Seeing installed packages
-------------------------

We know that ``spack list`` shows you the names of available packages,
but how do you figure out which are already installed?

.. _cmd-spack-find:

^^^^^^^^^^^^^^
``spack find``
^^^^^^^^^^^^^^

``spack find`` shows the *specs* of installed packages.  A spec is
like a name, but it has a version, compiler, architecture, and build
options associated with it.  In spack, you can have many installations
of the same package with different specs.

Running ``spack find`` with no arguments lists installed packages:

.. code-block:: console

   $ spack find
   ==> 74 installed packages.
   -- linux-debian7-x86_64 / gcc@4.4.7 --------------------------------
   ImageMagick@6.8.9-10  libdwarf@20130729  py-dateutil@2.4.0
   adept-utils@1.0       libdwarf@20130729  py-ipython@2.3.1
   atk@2.14.0            libelf@0.8.12      py-matplotlib@1.4.2
   boost@1.55.0          libelf@0.8.13      py-nose@1.3.4
   bzip2@1.0.6           libffi@3.1         py-numpy@1.9.1
   cairo@1.14.0          libmng@2.0.2       py-pygments@2.0.1
   callpath@1.0.2        libpng@1.6.16      py-pyparsing@2.0.3
   cmake@3.0.2           libtiff@4.0.3      py-pyside@1.2.2
   dbus@1.8.6            libtool@2.4.2      py-pytz@2014.10
   dbus@1.9.0            libxcb@1.11        py-setuptools@11.3.1
   dyninst@8.1.2         libxml2@2.9.2      py-six@1.9.0
   fontconfig@2.11.1     libxml2@2.9.2      python@2.7.8
   freetype@2.5.3        llvm@3.0           qhull@1.0
   gdk-pixbuf@2.31.2     memaxes@0.5        qt@4.8.6
   glib@2.42.1           mesa@8.0.5         qt@5.4.0
   graphlib@2.0.0        mpich@3.0.4        readline@6.3
   gtkplus@2.24.25       mpileaks@1.0       sqlite@3.8.5
   harfbuzz@0.9.37       mrnet@4.1.0        stat@2.1.0
   hdf5@1.8.13           ncurses@5.9        tcl@8.6.3
   icu@54.1              netcdf@4.3.3       tk@src
   jpeg@9a               openssl@1.0.1h     vtk@6.1.0
   launchmon@1.0.1       pango@1.36.8       xcb-proto@1.11
   lcms@2.6              pixman@0.32.6      xz@5.2.0
   libdrm@2.4.33         py-dateutil@2.4.0  zlib@1.2.8

   -- linux-debian7-x86_64 / gcc@4.9.2 --------------------------------
   libelf@0.8.10  mpich@3.0.4

Packages are divided into groups according to their architecture and
compiler.  Within each group, Spack tries to keep the view simple, and
only shows the version of installed packages.

.. _cmd-spack-find-metadata:

""""""""""""""""""""""""""""""""
Viewing more metadata
""""""""""""""""""""""""""""""""

``spack find`` can filter the package list based on the package name,
spec, or a number of properties of their installation status.  For
example, missing dependencies of a spec can be shown with
``--missing``, deprecated packages can be included with
``--deprecated``, packages which were explicitly installed with
``spack install <package>`` can be singled out with ``--explicit`` and
those which have been pulled in only as dependencies with
``--implicit``.

In some cases, there may be different configurations of the *same*
version of a package installed.  For example, there are two
installations of ``libdwarf@20130729`` above.  We can look at them
in more detail using ``spack find --deps``, and by asking only to show
``libdwarf`` packages:

.. code-block:: console

   $ spack find --deps libdwarf
   ==> 2 installed packages.
   -- linux-debian7-x86_64 / gcc@4.4.7 --------------------------------
       libdwarf@20130729-d9b90962
           ^libelf@0.8.12
       libdwarf@20130729-b52fac98
           ^libelf@0.8.13

Now we see that the two instances of ``libdwarf`` depend on
*different* versions of ``libelf``: 0.8.12 and 0.8.13.  This view can
become complicated for packages with many dependencies.  If you just
want to know whether two packages' dependencies differ, you can use
``spack find --long``:

.. code-block:: console

   $ spack find --long libdwarf
   ==> 2 installed packages.
   -- linux-debian7-x86_64 / gcc@4.4.7 --------------------------------
   libdwarf@20130729-d9b90962  libdwarf@20130729-b52fac98

Now the ``libdwarf`` installs have hashes after their names.  These are
hashes over all of the dependencies of each package.  If the hashes
are the same, then the packages have the same dependency configuration.

If you want to know the path where each package is installed, you can
use ``spack find --paths``:

.. code-block:: console

   $ spack find --paths
   ==> 74 installed packages.
   -- linux-debian7-x86_64 / gcc@4.4.7 --------------------------------
       ImageMagick@6.8.9-10  ~/spack/opt/linux-debian7-x86_64/gcc@4.4.7/ImageMagick@6.8.9-10-4df950dd
       adept-utils@1.0       ~/spack/opt/linux-debian7-x86_64/gcc@4.4.7/adept-utils@1.0-5adef8da
       atk@2.14.0            ~/spack/opt/linux-debian7-x86_64/gcc@4.4.7/atk@2.14.0-3d09ac09
       boost@1.55.0          ~/spack/opt/linux-debian7-x86_64/gcc@4.4.7/boost@1.55.0
       bzip2@1.0.6           ~/spack/opt/linux-debian7-x86_64/gcc@4.4.7/bzip2@1.0.6
       cairo@1.14.0          ~/spack/opt/linux-debian7-x86_64/gcc@4.4.7/cairo@1.14.0-fcc2ab44
       callpath@1.0.2        ~/spack/opt/linux-debian7-x86_64/gcc@4.4.7/callpath@1.0.2-5dce4318
   ...

You can restrict your search to a particular package by supplying its
name:

.. code-block:: console

   $ spack find --paths libelf
   -- linux-debian7-x86_64 / gcc@4.4.7 --------------------------------
       libelf@0.8.11  ~/spack/opt/linux-debian7-x86_64/gcc@4.4.7/libelf@0.8.11
       libelf@0.8.12  ~/spack/opt/linux-debian7-x86_64/gcc@4.4.7/libelf@0.8.12
       libelf@0.8.13  ~/spack/opt/linux-debian7-x86_64/gcc@4.4.7/libelf@0.8.13

""""""""""""""""""""""""""""""""
Spec queries
""""""""""""""""""""""""""""""""

``spack find`` actually does a lot more than this.  You can use
*specs* to query for specific configurations and builds of each
package. If you want to find only libelf versions greater than version
0.8.12, you could say:

.. code-block:: console

   $ spack find libelf@0.8.12:
   -- linux-debian7-x86_64 / gcc@4.4.7 --------------------------------
       libelf@0.8.12  libelf@0.8.13

Finding just the versions of libdwarf built with a particular version
of libelf would look like this:

.. code-block:: console

   $ spack find --long libdwarf ^libelf@0.8.12
   ==> 1 installed packages.
   -- linux-debian7-x86_64 / gcc@4.4.7 --------------------------------
   libdwarf@20130729-d9b90962

We can also search for packages that have a certain attribute. For example,
``spack find libdwarf +debug`` will show only installations of libdwarf
with the 'debug' compile-time option enabled.

The full spec syntax is discussed in detail in :ref:`sec-specs`.


""""""""""""""""""""""""""""""""
Machine-readable output
""""""""""""""""""""""""""""""""

If you only want to see very specific things about installed packages,
Spack has some options for you.  ``spack find --format`` can be used to
output only specific fields:

.. code-block:: console

   $ spack find --format "{name}-{version}-{hash}"
   autoconf-2.69-icynozk7ti6h4ezzgonqe6jgw5f3ulx4
   automake-1.16.1-o5v3tc77kesgonxjbmeqlwfmb5qzj7zy
   bzip2-1.0.6-syohzw57v2jfag5du2x4bowziw3m5p67
   bzip2-1.0.8-zjny4jwfyvzbx6vii3uuekoxmtu6eyuj
   cmake-3.15.1-7cf6onn52gywnddbmgp7qkil4hdoxpcb
   ...

or:

.. code-block:: console

   $ spack find --format "{hash:7}"
   icynozk
   o5v3tc7
   syohzw5
   zjny4jw
   7cf6onn
   ...

This uses the same syntax as described in documentation for
:meth:`~spack.spec.Spec.format` -- you can use any of the options there.
This is useful for passing metadata about packages to other command-line
tools.

Alternately, if you want something even more machine readable, you can
output each spec as JSON records using ``spack find --json``.  This will
output metadata on specs and all dependencies as json:

.. code-block:: console

    $ spack find --json sqlite@3.28.0
    [
     {
      "name": "sqlite",
      "hash": "3ws7bsihwbn44ghf6ep4s6h4y2o6eznv",
      "version": "3.28.0",
      "arch": {
       "platform": "darwin",
       "platform_os": "mojave",
       "target": "x86_64"
      },
      "compiler": {
       "name": "apple-clang",
       "version": "10.0.0"
      },
      "namespace": "builtin",
      "parameters": {
       "fts": true,
       "functions": false,
       "cflags": [],
       "cppflags": [],
       "cxxflags": [],
       "fflags": [],
       "ldflags": [],
       "ldlibs": []
      },
      "dependencies": {
       "readline": {
        "hash": "722dzmgymxyxd6ovjvh4742kcetkqtfs",
        "type": [
         "build",
         "link"
        ]
       }
      }
     },
     ...
    ]

You can use this with tools like `jq <https://stedolan.github.io/jq/>`_ to quickly create JSON records
structured the way you want:

.. code-block:: console

    $ spack find --json sqlite@3.28.0 | jq -C '.[] | { name, version, hash }'
    {
      "name": "sqlite",
      "version": "3.28.0",
      "hash": "3ws7bsihwbn44ghf6ep4s6h4y2o6eznv"
    }
    {
      "name": "readline",
      "version": "7.0",
      "hash": "722dzmgymxyxd6ovjvh4742kcetkqtfs"
    }
    {
      "name": "ncurses",
      "version": "6.1",
      "hash": "zvaa4lhlhilypw5quj3akyd3apbq5gap"
    }


^^^^^^^^^^^^^^
``spack diff``
^^^^^^^^^^^^^^

It's often the case that you have two versions of a spec that you need to
disambiguate. Let's say that we've installed two variants of zlib, one with
and one without the optimize variant:

.. code-block:: console

   $ spack install zlib
   $ spack install zlib -optimize

When we do ``spack find`` we see the two versions.

.. code-block:: console

    $ spack find zlib
    ==> 2 installed packages
    -- linux-ubuntu20.04-skylake / gcc@9.3.0 ------------------------
    zlib@1.2.11  zlib@1.2.11


Let's now say that we want to uninstall zlib. We run the command, and hit a problem
real quickly since we have two!

.. code-block:: console

    $ spack uninstall zlib
    ==> Error: zlib matches multiple packages:

        -- linux-ubuntu20.04-skylake / gcc@9.3.0 ------------------------
        efzjziy zlib@1.2.11  sl7m27m zlib@1.2.11

    ==> Error: You can either:
        a) use a more specific spec, or
        b) specify the spec by its hash (e.g. `spack uninstall /hash`), or
        c) use `spack uninstall --all` to uninstall ALL matching specs.

Oh no! We can see from the above that we have two different versions of zlib installed,
and the only difference between the two is the hash. This is a good use case for
``spack diff``, which can easily show us the "diff" or set difference
between properties for two packages. Let's try it out.
Since the only difference we see in the ``spack find`` view is the hash, let's use
``spack diff`` to look for more detail. We will provide the two hashes:

.. code-block:: console

    $ spack diff /efzjziy /sl7m27m
    ==> Warning: This interface is subject to change.

    --- zlib@1.2.11efzjziyc3dmb5h5u5azsthgbgog5mj7g
    +++ zlib@1.2.11sl7m27mzkbejtkrajigj3a3m37ygv4u2
    @@ variant_value @@
    -  zlib optimize False
    +  zlib optimize True


The output is colored, and written in the style of a git diff. This means that you
can copy and paste it into a GitHub markdown as a code block with language "diff"
and it will render nicely! Here is an example:

.. code-block:: md

    ```diff
    --- zlib@1.2.11/efzjziyc3dmb5h5u5azsthgbgog5mj7g
    +++ zlib@1.2.11/sl7m27mzkbejtkrajigj3a3m37ygv4u2
    @@ variant_value @@
    -  zlib optimize False
    +  zlib optimize True
    ```

Awesome! Now let's read the diff. It tells us that our first zlib was built with ``~optimize``
(``False``) and the second was built with ``+optimize`` (``True``). You can't see it in the docs
here, but the output above is also colored based on the content being an addition (+) or
subtraction (-).

This is a small example, but you will be able to see differences for any attributes on the
installation spec. Running ``spack diff A B`` means we'll see which spec attributes are on
``B`` but not on ``A`` (green) and which are on ``A`` but not on ``B`` (red). Here is another
example with an additional difference type, ``version``:

.. code-block:: console

    $ spack diff python@2.7.8 python@3.8.11
    ==> Warning: This interface is subject to change.

    --- python@2.7.8/tsxdi6gl4lihp25qrm4d6nys3nypufbf
    +++ python@3.8.11/yjtseru4nbpllbaxb46q7wfkyxbuvzxx
    @@ variant_value @@
    -  python patches a8c52415a8b03c0e5f28b5d52ae498f7a7e602007db2b9554df28cd5685839b8
    +  python patches 0d98e93189bc278fbc37a50ed7f183bd8aaf249a8e1670a465f0db6bb4f8cf87
    @@ version @@
    -  openssl 1.0.2u
    +  openssl 1.1.1k
    -  python 2.7.8
    +  python 3.8.11

Let's say that we were only interested in one kind of attribute above, ``version``.
We can ask the command to only output this attribute.  To do this, you'd add
the ``--attribute`` for attribute parameter, which defaults to all. Here is how you
would filter to show just versions:

.. code-block:: console

    $ spack diff --attribute version python@2.7.8 python@3.8.11
    ==> Warning: This interface is subject to change.

    --- python@2.7.8/tsxdi6gl4lihp25qrm4d6nys3nypufbf
    +++ python@3.8.11/yjtseru4nbpllbaxb46q7wfkyxbuvzxx
    @@ version @@
    -  openssl 1.0.2u
    +  openssl 1.1.1k
    -  python 2.7.8
    +  python 3.8.11

And you can add as many attributes as you'd like with multiple `--attribute` arguments
(for lots of attributes, you can use ``-a`` for short). Finally, if you want to view the
data as json (and possibly pipe into an output file) just add ``--json``:


.. code-block:: console

    $ spack diff --json python@2.7.8 python@3.8.11


This data will be much longer because along with the differences for ``A`` vs. ``B`` and
``B`` vs. ``A``, the JSON output also showsthe intersection.


------------------------
Using installed packages
------------------------

There are several different ways to use Spack packages once you have
installed them. As you've seen, spack packages are installed into long
paths with hashes, and you need a way to get them into your path. The
easiest way is to use :ref:`spack load <cmd-spack-load>`, which is
described in the next section.

Some more advanced ways to use Spack packages include:

* :ref:`environments <environments>`, which you can use to bundle a
  number of related packages to "activate" all at once, and
* :ref:`environment modules <modules>`, which are commonly used on
  supercomputing clusters. Spack generates module files for every
  installation automatically, and you can customize how this is done.

.. _cmd-spack-load:

^^^^^^^^^^^^^^^^^^^^^^^
``spack load / unload``
^^^^^^^^^^^^^^^^^^^^^^^

If you have :ref:`shell support <shell-support>` enabled you can use the
``spack load`` command to quickly get a package on your ``PATH``.

For example this will add the ``mpich`` package built with ``gcc`` to
your path:

.. code-block:: console

   $ spack install mpich %gcc@4.4.7

   # ... wait for install ...

   $ spack load mpich %gcc@4.4.7
   $ which mpicc
   ~/spack/opt/linux-debian7-x86_64/gcc@4.4.7/mpich@3.0.4/bin/mpicc

These commands will add appropriate directories to your ``PATH``,
``MANPATH``, ``CPATH``, and ``LD_LIBRARY_PATH`` according to the
:ref:`prefix inspections <customize-env-modifications>` defined in your
modules configuration.
When you no longer want to use a package, you can type unload or
unuse similarly:

.. code-block:: console

   $ spack unload mpich %gcc@4.4.7


"""""""""""""""
Ambiguous specs
"""""""""""""""

If a spec used with load/unload or is ambiguous (i.e. more than one
installed package matches it), then Spack will warn you:

.. code-block:: console

   $ spack load libelf
   ==> Error: libelf matches multiple packages.
   Matching packages:
     qmm4kso libelf@0.8.13%gcc@4.4.7 arch=linux-debian7-x86_64
     cd2u6jt libelf@0.8.13%intel@15.0.0 arch=linux-debian7-x86_64
   Use a more specific spec

You can either type the ``spack load`` command again with a fully
qualified argument, or you can add just enough extra constraints to
identify one package.  For example, above, the key differentiator is
that one ``libelf`` is built with the Intel compiler, while the other
used ``gcc``.  You could therefore just type:

.. code-block:: console

   $ spack load libelf %intel

To identify just the one built with the Intel compiler. If you want to be
*very* specific, you can load it by its hash. For example, to load the
first ``libelf`` above, you would run:

.. code-block:: console

   $ spack load /qmm4kso

To see which packages that you have loaded to your enviornment you would
use ``spack find --loaded``.

.. code-block:: console

    $ spack find --loaded
    ==> 2 installed packages
    -- linux-debian7 / gcc@4.4.7 ------------------------------------
    libelf@0.8.13

    -- linux-debian7 / intel@15.0.0 ---------------------------------
    libelf@0.8.13

You can also use ``spack load --list`` to get the same output, but it
does not have the full set of query options that ``spack find`` offers.

We'll learn more about Spack's spec syntax in the next section.


.. _sec-specs:

--------------------
Specs & dependencies
--------------------

We know that ``spack install``, ``spack uninstall``, and other
commands take a package name with an optional version specifier.  In
Spack, that descriptor is called a *spec*.  Spack uses specs to refer
to a particular build configuration (or configurations) of a package.
Specs are more than a package name and a version; you can use them to
specify the compiler, compiler version, architecture, compile options,
and dependency options for a build.  In this section, we'll go over
the full syntax of specs.

Here is an example of a much longer spec than we've seen thus far:

.. code-block:: none

   mpileaks @1.2:1.4 %gcc@4.7.5 +debug -qt target=x86_64 ^callpath @1.1 %gcc@4.7.2

If provided to ``spack install``, this will install the ``mpileaks``
library at some version between ``1.2`` and ``1.4`` (inclusive),
built using ``gcc`` at version 4.7.5 for a generic ``x86_64`` architecture,
with debug options enabled, and without Qt support.  Additionally, it
says to link it with the ``callpath`` library (which it depends on),
and to build callpath with ``gcc`` 4.7.2.  Most specs will not be as
complicated as this one, but this is a good example of what is
possible with specs.

More formally, a spec consists of the following pieces:

* Package name identifier (``mpileaks`` above)
* ``@`` Optional version specifier (``@1.2:1.4``)
* ``%`` Optional compiler specifier, with an optional compiler version
  (``gcc`` or ``gcc@4.7.3``)
* ``+`` or ``-`` or ``~`` Optional variant specifiers (``+debug``,
  ``-qt``, or ``~qt``) for boolean variants
* ``name=<value>`` Optional variant specifiers that are not restricted to
  boolean variants
* ``name=<value>`` Optional compiler flag specifiers. Valid flag names are
  ``cflags``, ``cxxflags``, ``fflags``, ``cppflags``, ``ldflags``, and ``ldlibs``.
* ``target=<value> os=<value>`` Optional architecture specifier
  (``target=haswell os=CNL10``)
* ``^`` Dependency specs (``^callpath@1.1``)

There are two things to notice here.  The first is that specs are
recursively defined.  That is, each dependency after ``^`` is a spec
itself.  The second is that everything is optional *except* for the
initial package name identifier.  Users can be as vague or as specific
as they want about the details of building packages, and this makes
spack good for beginners and experts alike.

To really understand what's going on above, we need to think about how
software is structured.  An executable or a library (these are
generally the artifacts produced by building software) depends on
other libraries in order to run.  We can represent the relationship
between a package and its dependencies as a graph.  Here is the full
dependency graph for ``mpileaks``:

.. graphviz::

   digraph {
       mpileaks -> mpich
       mpileaks -> callpath -> mpich
       callpath -> dyninst
       dyninst  -> libdwarf -> libelf
       dyninst  -> libelf
   }

Each box above is a package and each arrow represents a dependency on
some other package.  For example, we say that the package ``mpileaks``
*depends on* ``callpath`` and ``mpich``.  ``mpileaks`` also depends
*indirectly* on ``dyninst``, ``libdwarf``, and ``libelf``, in that
these libraries are dependencies of ``callpath``.  To install
``mpileaks``, Spack has to build all of these packages.  Dependency
graphs in Spack have to be acyclic, and the *depends on* relationship
is directional, so this is a *directed, acyclic graph* or *DAG*.

The package name identifier in the spec is the root of some dependency
DAG, and the DAG itself is implicit.  Spack knows the precise
dependencies among packages, but users do not need to know the full
DAG structure. Each ``^`` in the full spec refers to some dependency
of the root package. Spack will raise an error if you supply a name
after ``^`` that the root does not actually depend on (e.g. ``mpileaks
^emacs@23.3``).

Spack further simplifies things by only allowing one configuration of
each package within any single build.  Above, both ``mpileaks`` and
``callpath`` depend on ``mpich``, but ``mpich`` appears only once in
the DAG.  You cannot build an ``mpileaks`` version that depends on one
version of ``mpich`` *and* on a ``callpath`` version that depends on
some *other* version of ``mpich``.  In general, such a configuration
would likely behave unexpectedly at runtime, and Spack enforces this
to ensure a consistent runtime environment.

The point of specs is to abstract this full DAG from Spack users.  If
a user does not care about the DAG at all, she can refer to mpileaks
by simply writing ``mpileaks``.  If she knows that ``mpileaks``
indirectly uses ``dyninst`` and she wants a particular version of
``dyninst``, then she can refer to ``mpileaks ^dyninst@8.1``.  Spack
will fill in the rest when it parses the spec; the user only needs to
know package names and minimal details about their relationship.

When spack prints out specs, it sorts package names alphabetically to
normalize the way they are displayed, but users do not need to worry
about this when they write specs.  The only restriction on the order
of dependencies within a spec is that they appear *after* the root
package.  For example, these two specs represent exactly the same
configuration:

.. code-block:: none

   mpileaks ^callpath@1.0 ^libelf@0.8.3
   mpileaks ^libelf@0.8.3 ^callpath@1.0

You can put all the same modifiers on dependency specs that you would
put on the root spec.  That is, you can specify their versions,
compilers, variants, and architectures just like any other spec.
Specifiers are associated with the nearest package name to their left.
For example, above, ``@1.1`` and ``%gcc@4.7.2`` associates with the
``callpath`` package, while ``@1.2:1.4``, ``%gcc@4.7.5``, ``+debug``,
``-qt``, and ``target=haswell os=CNL10`` all associate with the ``mpileaks`` package.

In the diagram above, ``mpileaks`` depends on ``mpich`` with an
unspecified version, but packages can depend on other packages with
*constraints* by adding more specifiers.  For example, ``mpileaks``
could depend on ``mpich@1.2:`` if it can only build with version
``1.2`` or higher of ``mpich``.

Below are more details about the specifiers that you can add to specs.

^^^^^^^^^^^^^^^^^
Version specifier
^^^^^^^^^^^^^^^^^

A version specifier comes somewhere after a package name and starts
with ``@``.  It can be a single version, e.g. ``@1.0``, ``@3``, or
``@1.2a7``.  Or, it can be a range of versions, such as ``@1.0:1.5``
(all versions between ``1.0`` and ``1.5``, inclusive).  Version ranges
can be open, e.g. ``:3`` means any version up to and including ``3``.
This would include ``3.4`` and ``3.4.2``.  ``4.2:`` means any version
above and including ``4.2``.  Finally, a version specifier can be a
set of arbitrary versions, such as ``@1.0,1.5,1.7`` (``1.0``, ``1.5``,
or ``1.7``).  When you supply such a specifier to ``spack install``,
it constrains the set of versions that Spack will install.

If the version spec is not provided, then Spack will choose one
according to policies set for the particular spack installation.  If
the spec is ambiguous, i.e. it could match multiple versions, Spack
will choose a version within the spec's constraints according to
policies set for the particular Spack installation.

Details about how versions are compared and how Spack determines if
one version is less than another are discussed in the developer guide.

^^^^^^^^^^^^^^^^^^
Compiler specifier
^^^^^^^^^^^^^^^^^^

A compiler specifier comes somewhere after a package name and starts
with ``%``.  It tells Spack what compiler(s) a particular package
should be built with.  After the ``%`` should come the name of some
registered Spack compiler.  This might include ``gcc``, or ``intel``,
but the specific compilers available depend on the site.  You can run
``spack compilers`` to get a list; more on this below.

The compiler spec can be followed by an optional *compiler version*.
A compiler version specifier looks exactly like a package version
specifier.  Version specifiers will associate with the nearest package
name or compiler specifier to their left in the spec.

If the compiler spec is omitted, Spack will choose a default compiler
based on site policies.


.. _basic-variants:

^^^^^^^^
Variants
^^^^^^^^

Variants are named options associated with a particular package. They are
optional, as each package must provide default values for each variant it
makes available. Variants can be specified using
a flexible parameter syntax ``name=<value>``. For example,
``spack install mercury debug=True`` will install mercury built with debug
flags. The names of particular variants available for a package depend on
what was provided by the package author. ``spack info <package>`` will
provide information on what build variants are available.

For compatibility with earlier versions, variants which happen to be
boolean in nature can be specified by a syntax that represents turning
options on and off. For example, in the previous spec we could have
supplied ``mercury +debug`` with the same effect of enabling the debug
compile time option for the libelf package.

Depending on the package a variant may have any default value.  For
``mercury`` here, ``debug`` is ``False`` by default, and we turned it on
with ``debug=True`` or ``+debug``.  If a variant is ``True`` by default
you can turn it off by either adding ``-name`` or ``~name`` to the spec.

There are two syntaxes here because, depending on context, ``~`` and
``-`` may mean different things.  In most shells, the following will
result in the shell performing home directory substitution:

.. code-block:: sh

   mpileaks ~debug   # shell may try to substitute this!
   mpileaks~debug    # use this instead

If there is a user called ``debug``, the ``~`` will be incorrectly
expanded.  In this situation, you would want to write ``libelf
-debug``.  However, ``-`` can be ambiguous when included after a
package name without spaces:

.. code-block:: sh

   mpileaks-debug     # wrong!
   mpileaks -debug    # right

Spack allows the ``-`` character to be part of package names, so the
above will be interpreted as a request for the ``mpileaks-debug``
package, not a request for ``mpileaks`` built without ``debug``
options.  In this scenario, you should write ``mpileaks~debug`` to
avoid ambiguity.

When spack normalizes specs, it prints them out with no spaces boolean
variants using the backwards compatibility syntax and uses only ``~``
for disabled boolean variants.  The ``-`` and spaces on the command
line are provided for convenience and legibility.

^^^^^^^^^^^^^^
Compiler Flags
^^^^^^^^^^^^^^

Compiler flags are specified using the same syntax as non-boolean variants,
but fulfill a different purpose. While the function of a variant is set by
the package, compiler flags are used by the compiler wrappers to inject
flags into the compile line of the build. Additionally, compiler flags are
inherited by dependencies. ``spack install libdwarf cppflags="-g"`` will
install both libdwarf and libelf with the ``-g`` flag injected into their
compile line.

Notice that the value of the compiler flags must be quoted if it
contains any spaces. Any of ``cppflags=-O3``, ``cppflags="-O3"``,
``cppflags='-O3'``, and ``cppflags="-O3 -fPIC"`` are acceptable, but
``cppflags=-O3 -fPIC`` is not. Additionally, if the value of the
compiler flags is not the last thing on the line, it must be followed
by a space. The command ``spack install libelf cppflags="-O3"%intel``
will be interpreted as an attempt to set ``cppflags="-O3%intel"``.

The six compiler flags are injected in the order of implicit make commands
in GNU Autotools. If all flags are set, the order is
``$cppflags $cflags|$cxxflags $ldflags <command> $ldlibs`` for C and C++ and
``$fflags $cppflags $ldflags <command> $ldlibs`` for Fortran.

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Compiler environment variables and additional RPATHs
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Sometimes compilers require setting special environment variables to
operate correctly. Spack handles these cases by allowing custom environment
modifications in the ``environment`` attribute of the compiler configuration
section. See also the :ref:`configuration_environment_variables` section
of the configuration files docs for more information.

It is also possible to specify additional ``RPATHs`` that the
compiler will add to all executables generated by that compiler.  This is
useful for forcing certain compilers to RPATH their own runtime libraries, so
that executables will run without the need to set ``LD_LIBRARY_PATH``.

.. code-block:: yaml

  compilers:
    - compiler:
        spec: gcc@4.9.3
        paths:
          cc: /opt/gcc/bin/gcc
          c++: /opt/gcc/bin/g++
          f77: /opt/gcc/bin/gfortran
          fc: /opt/gcc/bin/gfortran
        environment:
          unset:
            - BAD_VARIABLE
          set:
            GOOD_VARIABLE_NUM: 1
            GOOD_VARIABLE_STR: good
          prepend_path:
            PATH: /path/to/binutils
          append_path:
            LD_LIBRARY_PATH: /opt/gcc/lib
        extra_rpaths:
        - /path/to/some/compiler/runtime/directory
        - /path/to/some/other/compiler/runtime/directory


^^^^^^^^^^^^^^^^^^^^^^^
Architecture specifiers
^^^^^^^^^^^^^^^^^^^^^^^

Each node in the dependency graph of a spec has an architecture attribute.
This attribute is a triplet of platform, operating system and processor.
You can specify the elements either separately, by using
the reserved keywords ``platform``, ``os`` and ``target``:

.. code-block:: console

   $ spack install libelf platform=linux
   $ spack install libelf os=ubuntu18.04
   $ spack install libelf target=broadwell

or together by using the reserved keyword ``arch``:

.. code-block:: console

   $ spack install libelf arch=cray-CNL10-haswell

Normally users don't have to bother specifying the architecture if they
are installing software for their current host, as in that case the
values will be detected automatically.  If you need fine-grained control
over which packages use which targets (or over *all* packages' default
target), see :ref:`concretization-preferences`.

.. admonition:: Cray machines

  The situation is a little bit different for Cray machines and a detailed
  explanation on how the architecture can be set on them can be found at :ref:`cray-support`

.. _support-for-microarchitectures:

"""""""""""""""""""""""""""""""""""""""
Support for specific microarchitectures
"""""""""""""""""""""""""""""""""""""""

Spack knows how to detect and optimize for many specific microarchitectures
(including recent Intel, AMD and IBM chips) and encodes this information in
the ``target`` portion of the architecture specification. A complete list of
the microarchitectures known to Spack can be obtained in the following way:

.. command-output:: spack arch --known-targets

When a spec is installed Spack matches the compiler being used with the
microarchitecture being targeted to inject appropriate optimization flags
at compile time. Giving a command such as the following:

.. code-block:: console

   $ spack install zlib%gcc@9.0.1 target=icelake

will produce compilation lines similar to:

.. code-block:: console

   $ /usr/bin/gcc-9 -march=icelake-client -mtune=icelake-client -c ztest10532.c
   $ /usr/bin/gcc-9 -march=icelake-client -mtune=icelake-client -c -fPIC -O2 ztest10532.
   ...

where the flags ``-march=icelake-client -mtune=icelake-client`` are injected
by Spack based on the requested target and compiler.

If Spack knows that the requested compiler can't optimize for the current target
or can't build binaries for that target at all, it will exit with a meaningful error message:

.. code-block:: console

   $ spack install zlib%gcc@5.5.0 target=icelake
   ==> Error: cannot produce optimized binary for micro-architecture "icelake" with gcc@5.5.0 [supported compiler versions are 8:]

When instead an old compiler is selected on a recent enough microarchitecture but there is
no explicit ``target`` specification, Spack will optimize for the best match it can find instead
of failing:

.. code-block:: console

   $ spack arch
   linux-ubuntu18.04-broadwell

   $ spack spec zlib%gcc@4.8
   Input spec
   --------------------------------
   zlib%gcc@4.8

   Concretized
   --------------------------------
   zlib@1.2.11%gcc@4.8+optimize+pic+shared arch=linux-ubuntu18.04-haswell

   $ spack spec zlib%gcc@9.0.1
   Input spec
   --------------------------------
   zlib%gcc@9.0.1

   Concretized
   --------------------------------
   zlib@1.2.11%gcc@9.0.1+optimize+pic+shared arch=linux-ubuntu18.04-broadwell

In the snippet above, for instance, the microarchitecture was demoted to ``haswell`` when
compiling with ``gcc@4.8`` since support to optimize for ``broadwell`` starts from ``gcc@4.9:``.

Finally, if Spack has no information to match compiler and target, it will
proceed with the installation but avoid injecting any microarchitecture
specific flags.

.. warning::

   Currently, Spack doesn't print any warning to the user if it has no information
   on which optimization flags should be used for a given compiler. This behavior
   might change in the future.

.. _sec-virtual-dependencies:

--------------------
Virtual dependencies
--------------------

The dependency graph for ``mpileaks`` we saw above wasn't *quite*
accurate.  ``mpileaks`` uses MPI, which is an interface that has many
different implementations.  Above, we showed ``mpileaks`` and
``callpath`` depending on ``mpich``, which is one *particular*
implementation of MPI.  However, we could build either with another
implementation, such as ``openmpi`` or ``mvapich``.

Spack represents interfaces like this using *virtual dependencies*.
The real dependency DAG for ``mpileaks`` looks like this:

.. graphviz::

   digraph {
       mpi [color=red]
       mpileaks -> mpi
       mpileaks -> callpath -> mpi
       callpath -> dyninst
       dyninst  -> libdwarf -> libelf
       dyninst  -> libelf
   }

Notice that ``mpich`` has now been replaced with ``mpi``. There is no
*real* MPI package, but some packages *provide* the MPI interface, and
these packages can be substituted in for ``mpi`` when ``mpileaks`` is
built.

You can see what virtual packages a particular package provides by
getting info on it:

.. command-output:: spack info mpich

Spack is unique in that its virtual packages can be versioned, just
like regular packages.  A particular version of a package may provide
a particular version of a virtual package, and we can see above that
``mpich`` versions ``1`` and above provide all ``mpi`` interface
versions up to ``1``, and ``mpich`` versions ``3`` and above provide
``mpi`` versions up to ``3``.  A package can *depend on* a particular
version of a virtual package, e.g. if an application needs MPI-2
functions, it can depend on ``mpi@2:`` to indicate that it needs some
implementation that provides MPI-2 functions.

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Constraining virtual packages
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

When installing a package that depends on a virtual package, you can
opt to specify the particular provider you want to use, or you can let
Spack pick.  For example, if you just type this:

.. code-block:: console

   $ spack install mpileaks

Then spack will pick a provider for you according to site policies.
If you really want a particular version, say ``mpich``, then you could
run this instead:

.. code-block:: console

   $ spack install mpileaks ^mpich

This forces spack to use some version of ``mpich`` for its
implementation.  As always, you can be even more specific and require
a particular ``mpich`` version:

.. code-block:: console

   $ spack install mpileaks ^mpich@3

The ``mpileaks`` package in particular only needs MPI-1 commands, so
any MPI implementation will do.  If another package depends on
``mpi@2`` and you try to give it an insufficient MPI implementation
(e.g., one that provides only ``mpi@:1``), then Spack will raise an
error.  Likewise, if you try to plug in some package that doesn't
provide MPI, Spack will raise an error.

^^^^^^^^^^^^^^^^^^^^^^^^
Specifying Specs by Hash
^^^^^^^^^^^^^^^^^^^^^^^^

Complicated specs can become cumbersome to enter on the command line,
especially when many of the qualifications are necessary to distinguish
between similar installs. To avoid this, when referencing an existing spec,
Spack allows you to reference specs by their hash. We previously
discussed the spec hash that Spack computes. In place of a spec in any
command, substitute ``/<hash>`` where ``<hash>`` is any amount from
the beginning of a spec hash.

For example, lets say that you accidentally installed two different
``mvapich2`` installations. If you want to uninstall one of them but don't
know what the difference is, you can run:

.. code-block:: console

   $ spack find --long mvapich2
   ==> 2 installed packages.
   -- linux-centos7-x86_64 / gcc@6.3.0 ----------
   qmt35td mvapich2@2.2%gcc
   er3die3 mvapich2@2.2%gcc


You can then uninstall the latter installation using:

.. code-block:: console

   $ spack uninstall /er3die3


Or, if you want to build with a specific installation as a dependency,
you can use:

.. code-block:: console

   $ spack install trilinos ^/er3die3


If the given spec hash is sufficiently long as to be unique, Spack will
replace the reference with the spec to which it refers. Otherwise, it will
prompt for a more qualified hash.

Note that this will not work to reinstall a dependency uninstalled by
``spack uninstall --force``.

.. _cmd-spack-providers:

^^^^^^^^^^^^^^^^^^^
``spack providers``
^^^^^^^^^^^^^^^^^^^

You can see what packages provide a particular virtual package using
``spack providers``.  If you wanted to see what packages provide
``mpi``, you would just run:

.. command-output:: spack providers mpi

And if you *only* wanted to see packages that provide MPI-2, you would
add a version specifier to the spec:

.. command-output:: spack providers mpi@2

Notice that the package versions that provide insufficient MPI
versions are now filtered out.


-----------------------------
Deprecating insecure packages
-----------------------------

``spack deprecate`` allows for the removal of insecure packages with
minimal impact to their dependents.

.. warning::

  The ``spack deprecate`` command is designed for use only in
  extraordinary circumstances. This is a VERY big hammer to be used
  with care.

The ``spack deprecate`` command will remove one package and replace it
with another by replacing the deprecated package's prefix with a link
to the deprecator package's prefix.

.. warning::

  The ``spack deprecate`` command makes no promises about binary
  compatibility. It is up to the user to ensure the deprecator is
  suitable for the deprecated package.

Spack tracks concrete deprecated specs and ensures that no future packages
concretize to a deprecated spec.

The first spec given to the ``spack deprecate`` command is the package
to deprecate. It is an abstract spec that must describe a single
installed package. The second spec argument is the deprecator
spec. By default it must be an abstract spec that describes a single
installed package, but with the ``-i/--install-deprecator`` it can be
any abstract spec that Spack will install and then use as the
deprecator. The ``-I/--no-install-deprecator`` option will ensure
the default behavior.

By default, ``spack deprecate`` will deprecate all dependencies of the
deprecated spec, replacing each by the dependency of the same name in
the deprecator spec. The ``-d/--dependencies`` option will ensure the
default, while the ``-D/--no-dependencies`` option will deprecate only
the root of the deprecate spec in favor of the root of the deprecator
spec.

``spack deprecate`` can use symbolic links or hard links. The default
behavior is symbolic links, but the ``-l/--link-type`` flag can take
options ``hard`` or ``soft``.

-----------------------
Verifying installations
-----------------------

The ``spack verify`` command can be used to verify the validity of
Spack-installed packages any time after installation.

At installation time, Spack creates a manifest of every file in the
installation prefix. For links, Spack tracks the mode, ownership, and
destination. For directories, Spack tracks the mode, and
ownership. For files, Spack tracks the mode, ownership, modification
time, hash, and size. The Spack verify command will check, for every
file in each package, whether any of those attributes have changed. It
will also check for newly added files or deleted files from the
installation prefix. Spack can either check all installed packages
using the `-a,--all` or accept specs listed on the command line to
verify.

The ``spack verify`` command can also verify for individual files that
they haven't been altered since installation time. If the given file
is not in a Spack installation prefix, Spack will report that it is
not owned by any package. To check individual files instead of specs,
use the ``-f,--files`` option.

Spack installation manifests are part of the tarball signed by Spack
for binary package distribution. When installed from a binary package,
Spack uses the packaged installation manifest instead of creating one
at install time.

The ``spack verify`` command also accepts the ``-l,--local`` option to
check only local packages (as opposed to those used transparently from
``upstream`` spack instances) and the ``-j,--json`` option to output
machine-readable json data for any errors.


.. _extensions:

---------------------------
Extensions & Python support
---------------------------

Spack's installation model assumes that each package will live in its
own install prefix.  However, certain packages are typically installed
*within* the directory hierarchy of other packages.  For example,
`Python <https://www.python.org>`_ packages are typically installed in the
``$prefix/lib/python-2.7/site-packages`` directory.

Spack has support for this type of installation as well.  In Spack,
a package that can live inside the prefix of another package is called
an *extension*.  Suppose you have Python installed like so:

.. code-block:: console

   $ spack find python
   ==> 1 installed packages.
   -- linux-debian7-x86_64 / gcc@4.4.7 --------------------------------
   python@2.7.8

.. _cmd-spack-extensions:

^^^^^^^^^^^^^^^^^^^^
``spack extensions``
^^^^^^^^^^^^^^^^^^^^

You can find extensions for your Python installation like this:

.. code-block:: console

   $ spack extensions python
   ==> python@2.7.8%gcc@4.4.7 arch=linux-debian7-x86_64-703c7a96
   ==> 36 extensions:
   geos          py-ipython     py-pexpect    py-pyside            py-sip
   py-basemap    py-libxml2     py-pil        py-pytz              py-six
   py-biopython  py-mako        py-pmw        py-rpy2              py-sympy
   py-cython     py-matplotlib  py-pychecker  py-scientificpython  py-virtualenv
   py-dateutil   py-mpi4py      py-pygments   py-scikit-learn
   py-epydoc     py-mx          py-pylint     py-scipy
   py-gnuplot    py-nose        py-pyparsing  py-setuptools
   py-h5py       py-numpy       py-pyqt       py-shiboken

   ==> 12 installed:
   -- linux-debian7-x86_64 / gcc@4.4.7 --------------------------------
   py-dateutil@2.4.0    py-nose@1.3.4       py-pyside@1.2.2
   py-dateutil@2.4.0    py-numpy@1.9.1      py-pytz@2014.10
   py-ipython@2.3.1     py-pygments@2.0.1   py-setuptools@11.3.1
   py-matplotlib@1.4.2  py-pyparsing@2.0.3  py-six@1.9.0

   ==> None activated.

The extensions are a subset of what's returned by ``spack list``, and
they are packages like any other.  They are installed into their own
prefixes, and you can see this with ``spack find --paths``:

.. code-block:: console

   $ spack find --paths py-numpy
   ==> 1 installed packages.
   -- linux-debian7-x86_64 / gcc@4.4.7 --------------------------------
       py-numpy@1.9.1  ~/spack/opt/linux-debian7-x86_64/gcc@4.4.7/py-numpy@1.9.1-66733244

However, even though this package is installed, you cannot use it
directly when you run ``python``:

.. code-block:: console

   $ spack load python
   $ python
   Python 2.7.8 (default, Feb 17 2015, 01:35:25)
   [GCC 4.4.7 20120313 (Red Hat 4.4.7-11)] on linux2
   Type "help", "copyright", "credits" or "license" for more information.
   >>> import numpy
   Traceback (most recent call last):
     File "<stdin>", line 1, in <module>
   ImportError: No module named numpy
   >>>

^^^^^^^^^^^^^^^^
Using Extensions
^^^^^^^^^^^^^^^^

There are four ways to get ``numpy`` working in Python.  The first is
to use :ref:`shell-support`.  You can simply ``load`` the extension,
and it will be added to the ``PYTHONPATH`` in your current shell:

.. code-block:: console

   $ spack load python
   $ spack load py-numpy

Now ``import numpy`` will succeed for as long as you keep your current
session open.
The loaded packages can be checked using ``spack find --loaded``

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Loading Extensions via Modules
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Instead of using Spack's environment modification capabilities through
the ``spack load`` command, you can load numpy through your
environment modules (using ``environment-modules`` or ``lmod``). This
will also add the extension to the ``PYTHONPATH`` in your current
shell.

.. code-block:: console

   $ module load <name of numpy module>

If you do not know the name of the specific numpy module you wish to
load, you can use the ``spack module tcl|lmod loads`` command to get
the name of the module from the Spack spec.

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Activating Extensions in a View
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Another way to use extensions is to create a view, which merges the
python installation along with the extensions into a single prefix.
See :ref:`filesystem-views` for a more in-depth description of views and
:ref:`cmd-spack-view` for usage of the ``spack view`` command.

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Activating Extensions Globally
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

As an alternative to creating a merged prefix with Python and its extensions,
and prior to support for views, Spack has provided a means to install the
extension into the Spack installation prefix for the extendee. This has
typically been useful since extendable packages typically search their own
installation path for addons by default.

Global activations are performed with the ``spack activate`` command:

.. _cmd-spack-activate:

^^^^^^^^^^^^^^^^^^
``spack activate``
^^^^^^^^^^^^^^^^^^

.. code-block:: console

   $ spack activate py-numpy
   ==> Activated extension py-setuptools@11.3.1%gcc@4.4.7 arch=linux-debian7-x86_64-3c74eb69 for python@2.7.8%gcc@4.4.7.
   ==> Activated extension py-nose@1.3.4%gcc@4.4.7 arch=linux-debian7-x86_64-5f70f816 for python@2.7.8%gcc@4.4.7.
   ==> Activated extension py-numpy@1.9.1%gcc@4.4.7 arch=linux-debian7-x86_64-66733244 for python@2.7.8%gcc@4.4.7.

Several things have happened here.  The user requested that
``py-numpy`` be activated in the ``python`` installation it was built
with.  Spack knows that ``py-numpy`` depends on ``py-nose`` and
``py-setuptools``, so it activated those packages first.  Finally,
once all dependencies were activated in the ``python`` installation,
``py-numpy`` was activated as well.

If we run ``spack extensions`` again, we now see the three new
packages listed as activated:

.. code-block:: console

   $ spack extensions python
   ==> python@2.7.8%gcc@4.4.7  arch=linux-debian7-x86_64-703c7a96
   ==> 36 extensions:
   geos          py-ipython     py-pexpect    py-pyside            py-sip
   py-basemap    py-libxml2     py-pil        py-pytz              py-six
   py-biopython  py-mako        py-pmw        py-rpy2              py-sympy
   py-cython     py-matplotlib  py-pychecker  py-scientificpython  py-virtualenv
   py-dateutil   py-mpi4py      py-pygments   py-scikit-learn
   py-epydoc     py-mx          py-pylint     py-scipy
   py-gnuplot    py-nose        py-pyparsing  py-setuptools
   py-h5py       py-numpy       py-pyqt       py-shiboken

   ==> 12 installed:
   -- linux-debian7-x86_64 / gcc@4.4.7 --------------------------------
   py-dateutil@2.4.0    py-nose@1.3.4       py-pyside@1.2.2
   py-dateutil@2.4.0    py-numpy@1.9.1      py-pytz@2014.10
   py-ipython@2.3.1     py-pygments@2.0.1   py-setuptools@11.3.1
   py-matplotlib@1.4.2  py-pyparsing@2.0.3  py-six@1.9.0

   ==> 3 currently activated:
   -- linux-debian7-x86_64 / gcc@4.4.7 --------------------------------
   py-nose@1.3.4  py-numpy@1.9.1  py-setuptools@11.3.1

Now, when a user runs python, ``numpy`` will be available for import
*without* the user having to explicitly load it.  ``python@2.7.8`` now
acts like a system Python installation with ``numpy`` installed inside
of it.

Spack accomplishes this by symbolically linking the *entire* prefix of
the ``py-numpy`` package into the prefix of the ``python`` package.  To the
python interpreter, it looks like ``numpy`` is installed in the
``site-packages`` directory.

The only limitation of global activation is that you can only have a *single*
version of an extension activated at a time.  This is because multiple
versions of the same extension would conflict if symbolically linked
into the same prefix.  Users who want a different version of a package
can still get it by using environment modules or views, but they will have to
explicitly load their preferred version.

^^^^^^^^^^^^^^^^^^^^^^^^^^
``spack activate --force``
^^^^^^^^^^^^^^^^^^^^^^^^^^

If, for some reason, you want to activate a package *without* its
dependencies, you can use ``spack activate --force``:

.. code-block:: console

   $ spack activate --force py-numpy
   ==> Activated extension py-numpy@1.9.1%gcc@4.4.7 arch=linux-debian7-x86_64-66733244 for python@2.7.8%gcc@4.4.7.

.. _cmd-spack-deactivate:

^^^^^^^^^^^^^^^^^^^^
``spack deactivate``
^^^^^^^^^^^^^^^^^^^^

We've seen how activating an extension can be used to set up a default
version of a Python module.  Obviously, you may want to change that at
some point.  ``spack deactivate`` is the command for this.  There are
several variants:

* ``spack deactivate <extension>`` will deactivate a single
  extension.  If another activated extension depends on this one,
  Spack will warn you and exit with an error.
* ``spack deactivate --force <extension>`` deactivates an extension
  regardless of packages that depend on it.
* ``spack deactivate --all <extension>`` deactivates an extension and
  all of its dependencies.  Use ``--force`` to disregard dependents.
* ``spack deactivate --all <extendee>`` deactivates *all* activated
  extensions of a package.  For example, to deactivate *all* python
  extensions, use:

  .. code-block:: console

     $ spack deactivate --all python

-----------------------
Filesystem requirements
-----------------------

By default, Spack needs to be run from a filesystem that supports
``flock`` locking semantics.  Nearly all local filesystems and recent
versions of NFS support this, but parallel filesystems or NFS volumes may
be configured without ``flock`` support enabled.  You can determine how
your filesystems are mounted with ``mount``.  The output for a Lustre
filesystem might look like this:

.. code-block:: console

   $ mount | grep lscratch
   mds1-lnet0@o2ib100:/lsd on /p/lscratchd type lustre (rw,nosuid,lazystatfs,flock)
   mds2-lnet0@o2ib100:/lse on /p/lscratche type lustre (rw,nosuid,lazystatfs,flock)

Note the ``flock`` option on both Lustre mounts.

If you do not see this or a similar option for your filesystem, you have
a few options. First, you can move your Spack installation to a
filesystem that supports locking. Second, you could ask your system
administrator to enable ``flock`` for your filesystem.

If none of those work, you can disable locking in one of two ways:

  1. Run Spack with the ``-L`` or ``--disable-locks`` option to disable
     locks on a call-by-call basis.
  2. Edit :ref:`config.yaml <config-yaml>` and set the ``locks`` option
     to ``false`` to always disable locking.

.. warning::

   If you disable locking, concurrent instances of Spack will have no way
   to avoid stepping on each other.  You must ensure that there is only
   **one** instance of Spack running at a time.  Otherwise, Spack may end
   up with a corrupted database file, or you may not be able to see all
   installed packages in commands like ``spack find``.

   If you are unfortunate enough to run into this situation, you may be
   able to fix it by running ``spack reindex``.

This issue typically manifests with the error below:

.. code-block:: console

   $ ./spack find
   Traceback (most recent call last):
   File "./spack", line 176, in <module>
     main()
   File "./spack", line 154,' in main
     return_val = command(parser, args)
   File "./spack/lib/spack/spack/cmd/find.py", line 170, in find
     specs = set(spack.installed_db.query(\**q_args))
   File "./spack/lib/spack/spack/database.py", line 551, in query
     with self.read_transaction():
   File "./spack/lib/spack/spack/database.py", line 598, in __enter__
     if self._enter() and self._acquire_fn:
   File "./spack/lib/spack/spack/database.py", line 608, in _enter
     return self._db.lock.acquire_read(self._timeout)
   File "./spack/lib/spack/llnl/util/lock.py", line 103, in acquire_read
     self._lock(fcntl.LOCK_SH, timeout)   # can raise LockError.
   File "./spack/lib/spack/llnl/util/lock.py", line 64, in _lock
     fcntl.lockf(self._fd, op | fcntl.LOCK_NB)
   IOError: [Errno 38] Function not implemented

A nicer error message is TBD in future versions of Spack.

---------------
Troubleshooting
---------------

The ``spack audit`` command:

.. command-output:: spack audit -h

can be used to detect a number of configuration issues. This command detects
configuration settings which might not be strictly wrong but are not likely
to be useful outside of special cases.

It can also be used to detect dependency issues with packages - for example
cases where a package constrains a dependency with a variant that doesn't
exist (in this case Spack could report the problem ahead of time but
automatically performing the check would slow down most runs of Spack).

A detailed list of the checks currently implemented for each subcommand can be
printed with:

.. command-output:: spack -v audit list

Depending on the use case, users might run the appropriate subcommands to obtain
diagnostics. Issues, if found, are reported to stdout:

.. code-block:: console

   % spack audit packages lammps
   PKG-DIRECTIVES: 1 issue found
   1. lammps: wrong variant in "conflicts" directive
       the variant 'adios' does not exist
       in /home/spack/spack/var/spack/repos/builtin/packages/lammps/package.py


------------
Getting Help
------------

.. _cmd-spack-help:

^^^^^^^^^^^^^^
``spack help``
^^^^^^^^^^^^^^

If you don't find what you need here, the ``help`` subcommand will
print out out a list of *all* of spack's options and subcommands:

.. command-output:: spack help

Adding an argument, e.g. ``spack help <subcommand>``, will print out
usage information for a particular subcommand:

.. command-output:: spack help install

Alternately, you can use ``spack --help`` in place of ``spack help``, or
``spack <subcommand> --help`` to get help on a particular subcommand.
.. Copyright 2013-2022 Lawrence Livermore National Security, LLC and other
   Spack Project Developers. See the top-level COPYRIGHT file for details.

   SPDX-License-Identifier: (Apache-2.0 OR MIT)

.. _getting_started:

===============
Getting Started
===============

--------------------
System Prerequisites
--------------------

Spack has the following minimum system requirements, which are assumed to
be present on the machine where Spack is run:

.. csv-table:: System prerequisites for Spack
   :file: tables/system_prerequisites.csv
   :header-rows: 1

These requirements can be easily installed on most modern Linux systems;
on macOS, XCode is required.  Spack is designed to run on HPC
platforms like Cray.  Not all packages should be expected
to work on all platforms.  A build matrix showing which packages are
working on which systems is planned but not yet available.

------------
Installation
------------

Getting Spack is easy.  You can clone it from the `github repository
<https://github.com/spack/spack>`_ using this command:

.. code-block:: console

   $ git clone -c feature.manyFiles=true https://github.com/spack/spack.git

This will create a directory called ``spack``.

.. _shell-support:

^^^^^^^^^^^^^
Shell support
^^^^^^^^^^^^^

Once you have cloned Spack, we recommend sourcing the appropriate script
for your shell:

.. code-block:: console

   # For bash/zsh/sh
   $ . spack/share/spack/setup-env.sh

   # For tcsh/csh
   $ source spack/share/spack/setup-env.csh

   # For fish
   $ . spack/share/spack/setup-env.fish

That's it! You're ready to use Spack.

Sourcing these files will put the ``spack`` command in your ``PATH``, set
up your ``MODULEPATH`` to use Spack's packages, and add other useful
shell integration for :ref:`certain commands <packaging-shell-support>`,
:ref:`environments <environments>`, and :ref:`modules <modules>`. For
``bash`` and ``zsh``, it also sets up tab completion.

In order to know which directory to add to your ``MODULEPATH``, these scripts
query the ``spack`` command. On shared filesystems, this can be a bit slow,
especially if you log in frequently. If you don't use modules, or want to set
``MODULEPATH`` manually instead, you can set the ``SPACK_SKIP_MODULES``
environment variable to skip this step and speed up sourcing the file.

If you do not want to use Spack's shell support, you can always just run
the ``spack`` command directly from ``spack/bin/spack``.

When the ``spack`` command is executed it searches for an appropriate
Python interpreter to use, which can be explicitly overridden by setting
the ``SPACK_PYTHON`` environment variable.  When sourcing the appropriate shell
setup script, ``SPACK_PYTHON`` will be set to the interpreter found at
sourcing time, ensuring future invocations of the ``spack`` command will
continue to use the same consistent python version regardless of changes in
the environment.

^^^^^^^^^^^^^^^^^^^^
Bootstrapping clingo
^^^^^^^^^^^^^^^^^^^^

Spack uses ``clingo`` under the hood to resolve optimal versions and variants of
dependencies when installing a package. Since ``clingo`` itself is a binary,
Spack has to install it on initial use, which is called bootstrapping.

Spack provides two ways of bootstrapping ``clingo``: from pre-built binaries
(default), or from sources. The fastest way to get started is to bootstrap from
pre-built binaries.

.. note::

   When bootstrapping from pre-built binaries, Spack currently requires 
   ``patchelf`` on Linux and ``otool`` on macOS. If ``patchelf`` is not in the
   ``PATH``, Spack will build it from sources, and a C++ compiler is required.

The first time you concretize a spec, Spack will bootstrap in the background:

.. code-block:: console

   $ time spack spec zlib
   Input spec
   --------------------------------
   zlib

   Concretized
   --------------------------------
   zlib@1.2.11%gcc@7.5.0+optimize+pic+shared arch=linux-ubuntu18.04-zen

   real	0m20.023s
   user	0m18.351s
   sys	0m0.784s

After this command you'll see that ``clingo`` has been installed for Spack's own use:

.. code-block:: console

   $ spack find -b
   ==> Showing internal bootstrap store at "/root/.spack/bootstrap/store"
   ==> 3 installed packages
   -- linux-rhel5-x86_64 / gcc@9.3.0 -------------------------------
   clingo-bootstrap@spack  python@3.6

   -- linux-ubuntu18.04-zen / gcc@7.5.0 ----------------------------
   patchelf@0.13

Subsequent calls to the concretizer will then be much faster:

.. code-block:: console

   $ time spack spec zlib
   [ ... ]
   real	0m0.490s
   user	0m0.431s
   sys	0m0.041s


If for security concerns you cannot bootstrap ``clingo`` from pre-built
binaries, you have to mark this bootstrapping method as untrusted. This makes
Spack fall back to bootstrapping from sources:

.. code-block:: console

   $ spack bootstrap untrust github-actions
   ==> "github-actions" is now untrusted and will not be used for bootstrapping

You can verify that the new settings are effective with:

.. code-block:: console

   $ spack bootstrap list
   Name: github-actions UNTRUSTED

     Type: buildcache

     Info:
       url: https://mirror.spack.io/bootstrap/github-actions/v0.1
       homepage: https://github.com/alalazo/spack-bootstrap-mirrors
       releases: https://github.com/alalazo/spack-bootstrap-mirrors/releases

     Description:
       Buildcache generated from a public workflow using Github Actions.
       The sha256 checksum of binaries is checked before installation.


   Name: spack-install TRUSTED

     Type: install

     Description:
       Specs built from sources by Spack. May take a long time.

.. note::

   When bootstrapping from sources, Spack requires a full install of Python
   including header files (e.g. ``python3-dev`` on Debian), and a compiler
   with support for C++14 (GCC on Linux, Apple Clang on macOS) and static C++
   standard libraries on Linux.

Spack will build the required software on the first request to concretize a spec:

.. code-block:: console

   $ spack spec zlib
   [+] /usr (external bison-3.0.4-wu5pgjchxzemk5ya2l3ddqug2d7jv6eb)
   [+] /usr (external cmake-3.19.4-a4kmcfzxxy45mzku4ipmj5kdiiz5a57b)
   [+] /usr (external python-3.6.9-x4fou4iqqlh5ydwddx3pvfcwznfrqztv)
   ==> Installing re2c-1.2.1-e3x6nxtk3ahgd63ykgy44mpuva6jhtdt
   [ ... ]
   zlib@1.2.11%gcc@10.1.0+optimize+pic+shared arch=linux-ubuntu18.04-broadwell

"""""""""""""""""""
The Bootstrap Store
"""""""""""""""""""

All the tools Spack needs for its own functioning are installed in a separate store, which lives
under the ``${HOME}/.spack`` directory. The software installed there can be queried with:

.. code-block:: console

   $ spack find --bootstrap
   ==> Showing internal bootstrap store at "/home/spack/.spack/bootstrap/store"
   ==> 3 installed packages
   -- linux-ubuntu18.04-x86_64 / gcc@10.1.0 ------------------------
   clingo-bootstrap@spack  python@3.6.9  re2c@1.2.1

In case it's needed the bootstrap store can also be cleaned with:

.. code-block:: console

   $ spack clean -b
   ==> Removing software in "/home/spack/.spack/bootstrap/store"

^^^^^^^^^^^^^^^^^^
Check Installation
^^^^^^^^^^^^^^^^^^

With Spack installed, you should be able to run some basic Spack
commands.  For example:

.. command-output:: spack spec netcdf-c

In theory, Spack doesn't need any additional installation; just
download and run!  But in real life, additional steps are usually
required before Spack can work in a practical sense.  Read on...

^^^^^^^^^^^^^^^^^
Clean Environment
^^^^^^^^^^^^^^^^^

Many packages' installs can be broken by changing environment
variables.  For example, a package might pick up the wrong build-time
dependencies (most of them not specified) depending on the setting of
``PATH``.  ``GCC`` seems to be particularly vulnerable to these issues.

Therefore, it is recommended that Spack users run with a *clean
environment*, especially for ``PATH``.  Only software that comes with
the system, or that you know you wish to use with Spack, should be
included.  This procedure will avoid many strange build errors.

^^^^^^^^^^^^^^^^^^^^^^^^^^
Optional: Alternate Prefix
^^^^^^^^^^^^^^^^^^^^^^^^^^

You may want to run Spack out of a prefix other than the git repository
you cloned.  The ``spack clone`` command provides this
functionality.  To install spack in a new directory, simply type:

.. code-block:: console

   $ spack clone /my/favorite/prefix

This will install a new spack script in ``/my/favorite/prefix/bin``,
which you can use just like you would the regular spack script.  Each
copy of spack installs packages into its own ``$PREFIX/opt``
directory.


.. _compiler-config:

----------------------
Compiler configuration
----------------------

Spack has the ability to build packages with multiple compilers and
compiler versions. Compilers can be made available to Spack by
specifying them manually in ``compilers.yaml``, or automatically by
running ``spack compiler find``, but for convenience Spack will
automatically detect compilers the first time it needs them.

.. _cmd-spack-compilers:

^^^^^^^^^^^^^^^^^^^
``spack compilers``
^^^^^^^^^^^^^^^^^^^

You can see which compilers are available to Spack by running ``spack
compilers`` or ``spack compiler list``:

.. code-block:: console

   $ spack compilers
   ==> Available compilers
   -- gcc ---------------------------------------------------------
       gcc@4.9.0  gcc@4.8.0  gcc@4.7.0  gcc@4.6.2  gcc@4.4.7
       gcc@4.8.2  gcc@4.7.1  gcc@4.6.3  gcc@4.6.1  gcc@4.1.2
   -- intel -------------------------------------------------------
       intel@15.0.0  intel@14.0.0  intel@13.0.0  intel@12.1.0  intel@10.0
       intel@14.0.3  intel@13.1.1  intel@12.1.5  intel@12.0.4  intel@9.1
       intel@14.0.2  intel@13.1.0  intel@12.1.3  intel@11.1
       intel@14.0.1  intel@13.0.1  intel@12.1.2  intel@10.1
   -- clang -------------------------------------------------------
       clang@3.4  clang@3.3  clang@3.2  clang@3.1
   -- pgi ---------------------------------------------------------
       pgi@14.3-0   pgi@13.2-0  pgi@12.1-0   pgi@10.9-0  pgi@8.0-1
       pgi@13.10-0  pgi@13.1-1  pgi@11.10-0  pgi@10.2-0  pgi@7.1-3
       pgi@13.6-0   pgi@12.8-0  pgi@11.1-0   pgi@9.0-4   pgi@7.0-6

Any of these compilers can be used to build Spack packages.  More on
how this is done is in :ref:`sec-specs`.

.. _cmd-spack-compiler-add:

^^^^^^^^^^^^^^^^^^^^^^
``spack compiler add``
^^^^^^^^^^^^^^^^^^^^^^

An alias for ``spack compiler find``.

.. _cmd-spack-compiler-find:

^^^^^^^^^^^^^^^^^^^^^^^
``spack compiler find``
^^^^^^^^^^^^^^^^^^^^^^^

Lists the compilers currently available to Spack. If you do not see
a compiler in this list, but you want to use it with Spack, you can
simply run ``spack compiler find`` with the path to where the
compiler is installed.  For example:

.. code-block:: console

   $ spack compiler find /usr/local/tools/ic-13.0.079
   ==> Added 1 new compiler to ~/.spack/linux/compilers.yaml
       intel@13.0.079

Or you can run ``spack compiler find`` with no arguments to force
auto-detection.  This is useful if you do not know where compilers are
installed, but you know that new compilers have been added to your
``PATH``.  For example, you might load a module, like this:

.. code-block:: console

   $ module load gcc-4.9.0
   $ spack compiler find
   ==> Added 1 new compiler to ~/.spack/linux/compilers.yaml
       gcc@4.9.0

This loads the environment module for gcc-4.9.0 to add it to
``PATH``, and then it adds the compiler to Spack.

.. note::

   By default, spack does not fill in the ``modules:`` field in the
   ``compilers.yaml`` file.  If you are using a compiler from a
   module, then you should add this field manually.
   See the section on :ref:`compilers-requiring-modules`.

.. _cmd-spack-compiler-info:

^^^^^^^^^^^^^^^^^^^^^^^
``spack compiler info``
^^^^^^^^^^^^^^^^^^^^^^^

If you want to see specifics on a particular compiler, you can run
``spack compiler info`` on it:

.. code-block:: console

   $ spack compiler info intel@15
   intel@15.0.0:
     paths:
       cc  = /usr/local/bin/icc-15.0.090
       cxx = /usr/local/bin/icpc-15.0.090
       f77 = /usr/local/bin/ifort-15.0.090
       fc  = /usr/local/bin/ifort-15.0.090
     modules = []
     operating_system = centos6
   ...

This shows which C, C++, and Fortran compilers were detected by Spack.
Notice also that we didn't have to be too specific about the
version. We just said ``intel@15``, and information about the only
matching Intel compiler was displayed.

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Manual compiler configuration
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

If auto-detection fails, you can manually configure a compiler by
editing your ``~/.spack/<platform>/compilers.yaml`` file.  You can do this by running
``spack config edit compilers``, which will open the file in your ``$EDITOR``.

Each compiler configuration in the file looks like this:

.. code-block:: yaml

   compilers:
   - compiler:
       modules: []
       operating_system: centos6
       paths:
         cc: /usr/local/bin/icc-15.0.024-beta
         cxx: /usr/local/bin/icpc-15.0.024-beta
         f77: /usr/local/bin/ifort-15.0.024-beta
         fc: /usr/local/bin/ifort-15.0.024-beta
       spec: intel@15.0.0

For compilers that do not support Fortran (like ``clang``), put
``None`` for ``f77`` and ``fc``:

.. code-block:: yaml

   compilers:
   - compiler:
       modules: []
       operating_system: centos6
       paths:
         cc: /usr/bin/clang
         cxx: /usr/bin/clang++
         f77: None
         fc: None
       spec: clang@3.3svn

Once you save the file, the configured compilers will show up in the
list displayed by ``spack compilers``.

You can also add compiler flags to manually configured compilers. These
flags should be specified in the ``flags`` section of the compiler
specification. The valid flags are ``cflags``, ``cxxflags``, ``fflags``,
``cppflags``, ``ldflags``, and ``ldlibs``. For example:

.. code-block:: yaml

   compilers:
   - compiler:
       modules: []
       operating_system: centos6
       paths:
         cc: /usr/bin/gcc
         cxx: /usr/bin/g++
         f77: /usr/bin/gfortran
         fc: /usr/bin/gfortran
       flags:
         cflags: -O3 -fPIC
         cxxflags: -O3 -fPIC
         cppflags: -O3 -fPIC
       spec: gcc@4.7.2

These flags will be treated by spack as if they were entered from
the command line each time this compiler is used. The compiler wrappers
then inject those flags into the compiler command. Compiler flags
entered from the command line will be discussed in more detail in the
following section.

Some compilers also require additional environment configuration.
Examples include Intels oneAPI and AMDs AOCC compiler suites,
which have custom scripts for loading environment variables and setting paths.
These variables should be specified in the ``environment`` section of the compiler
specification. The operations available to modify the environment are ``set``, ``unset``,
``prepend_path``, ``append_path``, and ``remove_path``. For example:

.. code-block:: yaml

   compilers:
   - compiler:
       modules: []
       operating_system: centos6
       paths:
         cc: /opt/intel/oneapi/compiler/latest/linux/bin/icx
         cxx: /opt/intel/oneapi/compiler/latest/linux/bin/icpx
         f77: /opt/intel/oneapi/compiler/latest/linux/bin/ifx
         fc: /opt/intel/oneapi/compiler/latest/linux/bin/ifx
       spec: oneapi@latest
       environment:
         set:
           MKL_ROOT: "/path/to/mkl/root"
         unset: # A list of environment variables to unset
           - CC
         prepend_path: # Similar for append|remove_path
           LD_LIBRARY_PATH: /ld/paths/added/by/setvars/sh


^^^^^^^^^^^^^^^^^^^^^^^
Build Your Own Compiler
^^^^^^^^^^^^^^^^^^^^^^^

If you are particular about which compiler/version you use, you might
wish to have Spack build it for you.  For example:

.. code-block:: console

   $ spack install gcc@4.9.3

Once that has finished, you will need to add it to your
``compilers.yaml`` file.  You can then set Spack to use it by default
by adding the following to your ``packages.yaml`` file:

.. code-block:: yaml

   packages:
     all:
       compiler: [gcc@4.9.3]

.. _compilers-requiring-modules:

^^^^^^^^^^^^^^^^^^^^^^^^^^^
Compilers Requiring Modules
^^^^^^^^^^^^^^^^^^^^^^^^^^^

Many installed compilers will work regardless of the environment they
are called with.  However, some installed compilers require
``$LD_LIBRARY_PATH`` or other environment variables to be set in order
to run; this is typical for Intel and other proprietary compilers.

In such a case, you should tell Spack which module(s) to load in order
to run the chosen compiler (If the compiler does not come with a
module file, you might consider making one by hand).  Spack will load
this module into the environment ONLY when the compiler is run, and
NOT in general for a package's ``install()`` method.  See, for
example, this ``compilers.yaml`` file:

.. code-block:: yaml

   compilers:
   - compiler:
       modules: [other/comp/gcc-5.3-sp3]
       operating_system: SuSE11
       paths:
         cc: /usr/local/other/SLES11.3/gcc/5.3.0/bin/gcc
         cxx: /usr/local/other/SLES11.3/gcc/5.3.0/bin/g++
         f77: /usr/local/other/SLES11.3/gcc/5.3.0/bin/gfortran
         fc: /usr/local/other/SLES11.3/gcc/5.3.0/bin/gfortran
       spec: gcc@5.3.0

Some compilers require special environment settings to be loaded not just
to run, but also to execute the code they build, breaking packages that
need to execute code they just compiled.  If it's not possible or
practical to use a better compiler, you'll need to ensure that
environment settings are preserved for compilers like this (i.e., you'll
need to load the module or source the compiler's shell script).

By default, Spack tries to ensure that builds are reproducible by
cleaning the environment before building.  If this interferes with your
compiler settings, you CAN use ``spack install --dirty`` as a workaround.
Note that this MAY interfere with package builds.

.. _licensed-compilers:

^^^^^^^^^^^^^^^^^^
Licensed Compilers
^^^^^^^^^^^^^^^^^^

Some proprietary compilers require licensing to use.  If you need to
use a licensed compiler (eg, PGI), the process is similar to a mix of
build your own, plus modules:

#. Create a Spack package (if it doesn't exist already) to install
   your compiler.  Follow instructions on installing :ref:`license`.

#. Once the compiler is installed, you should be able to test it by
   using Spack to load the module it just created, and running simple
   builds (eg: ``cc helloWorld.c && ./a.out``)

#. Add the newly-installed compiler to ``compilers.yaml`` as shown
   above.

.. _mixed-toolchains:

^^^^^^^^^^^^^^^^
Mixed Toolchains
^^^^^^^^^^^^^^^^

Modern compilers typically come with related compilers for C, C++ and
Fortran bundled together.  When possible, results are best if the same
compiler is used for all languages.

In some cases, this is not possible.  For example, starting with macOS El
Capitan (10.11), many packages no longer build with GCC, but XCode
provides no Fortran compilers.  The user is therefore forced to use a
mixed toolchain: XCode-provided Clang for C/C++ and GNU ``gfortran`` for
Fortran.

#. You need to make sure that Xcode is installed. Run the following command:

   .. code-block:: console

      $ xcode-select --install


   If the Xcode command-line tools are already installed, you will see an
   error message:

   .. code-block:: none

      xcode-select: error: command line tools are already installed, use "Software Update" to install updates


#. For most packages, the Xcode command-line tools are sufficient. However,
   some packages like ``qt`` require the full Xcode suite. You can check
   to see which you have installed by running:

   .. code-block:: console

      $ xcode-select -p


   If the output is:

   .. code-block:: none

      /Applications/Xcode.app/Contents/Developer


   you already have the full Xcode suite installed. If the output is:

   .. code-block:: none

      /Library/Developer/CommandLineTools


   you only have the command-line tools installed. The full Xcode suite can
   be installed through the App Store. Make sure you launch the Xcode
   application and accept the license agreement before using Spack.
   It may ask you to install additional components. Alternatively, the license
   can be accepted through the command line:

   .. code-block:: console

      $ sudo xcodebuild -license accept


   Note: the flag is ``-license``, not ``--license``.

#. Run ``spack compiler find`` to locate Clang.

#. There are different ways to get ``gfortran`` on macOS. For example, you can
   install GCC with Spack (``spack install gcc``), with Homebrew (``brew install
   gcc``), or from a `DMG installer
   <https://github.com/fxcoudert/gfortran-for-macOS/releases>`_.

#. The only thing left to do is to edit ``~/.spack/darwin/compilers.yaml`` to provide
   the path to ``gfortran``:

   .. code-block:: yaml

      compilers:
      - compiler:
        ...
        paths:
          cc: /usr/bin/clang
          cxx: /usr/bin/clang++
          f77: /path/to/bin/gfortran
          fc: /path/to/bin/gfortran
        spec: apple-clang@11.0.0


   If you used Spack to install GCC, you can get the installation prefix by
   ``spack location -i gcc`` (this will only work if you have a single version
   of GCC installed). Whereas for Homebrew, GCC is installed in
   ``/usr/local/Cellar/gcc/x.y.z``. With the DMG installer, the correct path
   will be ``/usr/local/gfortran``.

^^^^^^^^^^^^^^^^^^^^^
Compiler Verification
^^^^^^^^^^^^^^^^^^^^^

You can verify that your compilers are configured properly by installing a
simple package.  For example:

.. code-block:: console

   $ spack install zlib%gcc@5.3.0


.. _vendor-specific-compiler-configuration:

--------------------------------------
Vendor-Specific Compiler Configuration
--------------------------------------

With Spack, things usually "just work" with GCC.  Not so for other
compilers.  This section provides details on how to get specific
compilers working.

^^^^^^^^^^^^^^^
Intel Compilers
^^^^^^^^^^^^^^^

Intel compilers are unusual because a single Intel compiler version
can emulate multiple GCC versions.  In order to provide this
functionality, the Intel compiler needs GCC to be installed.
Therefore, the following steps are necessary to successfully use Intel
compilers:

#. Install a version of GCC that implements the desired language
   features (``spack install gcc``).

#. Tell the Intel compiler how to find that desired GCC.  This may be
   done in one of two ways:

      "By default, the compiler determines which version of ``gcc`` or ``g++``
      you have installed from the ``PATH`` environment variable.

      If you want use a version of ``gcc`` or ``g++`` other than the default
      version on your system, you need to use either the ``-gcc-name``
      or ``-gxx-name`` compiler option to specify the path to the version of
      ``gcc`` or ``g++`` that you want to use."

      -- `Intel Reference Guide <https://software.intel.com/en-us/node/522750>`_

Intel compilers may therefore be configured in one of two ways with
Spack: using modules, or using compiler flags.

""""""""""""""""""""""""""
Configuration with Modules
""""""""""""""""""""""""""

One can control which GCC is seen by the Intel compiler with modules.
A module must be loaded both for the Intel Compiler (so it will run)
and GCC (so the compiler can find the intended GCC).  The following
configuration in ``compilers.yaml`` illustrates this technique:

.. code-block:: yaml

   compilers:
   - compiler:
       modules: [gcc-4.9.3, intel-15.0.24]
       operating_system: centos7
       paths:
         cc: /opt/intel-15.0.24/bin/icc-15.0.24-beta
         cxx: /opt/intel-15.0.24/bin/icpc-15.0.24-beta
         f77: /opt/intel-15.0.24/bin/ifort-15.0.24-beta
         fc: /opt/intel-15.0.24/bin/ifort-15.0.24-beta
       spec: intel@15.0.24.4.9.3


.. note::

   The version number on the Intel compiler is a combination of
   the "native" Intel version number and the GNU compiler it is
   targeting.

""""""""""""""""""""""""""
Command Line Configuration
""""""""""""""""""""""""""

One can also control which GCC is seen by the Intel compiler by adding
flags to the ``icc`` command:

#. Identify the location of the compiler you just installed:

   .. code-block:: console

       $ spack location --install-dir gcc
       ~/spack/opt/spack/linux-centos7-x86_64/gcc-4.9.3-iy4rw...

#. Set up ``compilers.yaml``, for example:

   .. code-block:: yaml

       compilers:
       - compiler:
           modules: [intel-15.0.24]
           operating_system: centos7
           paths:
             cc: /opt/intel-15.0.24/bin/icc-15.0.24-beta
             cxx: /opt/intel-15.0.24/bin/icpc-15.0.24-beta
             f77: /opt/intel-15.0.24/bin/ifort-15.0.24-beta
             fc: /opt/intel-15.0.24/bin/ifort-15.0.24-beta
           flags:
             cflags: -gcc-name ~/spack/opt/spack/linux-centos7-x86_64/gcc-4.9.3-iy4rw.../bin/gcc
             cxxflags: -gxx-name ~/spack/opt/spack/linux-centos7-x86_64/gcc-4.9.3-iy4rw.../bin/g++
             fflags: -gcc-name ~/spack/opt/spack/linux-centos7-x86_64/gcc-4.9.3-iy4rw.../bin/gcc
           spec: intel@15.0.24.4.9.3


^^^
PGI
^^^

PGI comes with two sets of compilers for C++ and Fortran,
distinguishable by their names.  "Old" compilers:

.. code-block:: yaml

    cc:  /soft/pgi/15.10/linux86-64/15.10/bin/pgcc
    cxx: /soft/pgi/15.10/linux86-64/15.10/bin/pgCC
    f77: /soft/pgi/15.10/linux86-64/15.10/bin/pgf77
    fc:  /soft/pgi/15.10/linux86-64/15.10/bin/pgf90

"New" compilers:

.. code-block:: yaml

    cc:  /soft/pgi/15.10/linux86-64/15.10/bin/pgcc
    cxx: /soft/pgi/15.10/linux86-64/15.10/bin/pgc++
    f77: /soft/pgi/15.10/linux86-64/15.10/bin/pgfortran
    fc:  /soft/pgi/15.10/linux86-64/15.10/bin/pgfortran

Older installations of PGI contains just the old compilers; whereas
newer installations contain the old and the new.  The new compiler is
considered preferable, as some packages
(``hdf``) will not build with the old compiler.

When auto-detecting a PGI compiler, there are cases where Spack will
find the old compilers, when you really want it to find the new
compilers.  It is best to check this ``compilers.yaml``; and if the old
compilers are being used, change ``pgf77`` and ``pgf90`` to
``pgfortran``.

Other issues:

* There are reports that some packages will not build with PGI,
  including ``libpciaccess`` and ``openssl``.  A workaround is to
  build these packages with another compiler and then use them as
  dependencies for PGI-build packages.  For example:

  .. code-block:: console

     $ spack install openmpi%pgi ^libpciaccess%gcc


* PGI requires a license to use; see :ref:`licensed-compilers` for more
  information on installation.

.. note::

   It is believed the problem with HDF 4 is that everything is
   compiled with the ``F77`` compiler, but at some point some Fortran
   90 code slipped in there. So compilers that can handle both FORTRAN
   77 and Fortran 90 (``gfortran``, ``pgfortran``, etc) are fine.  But
   compilers specific to one or the other (``pgf77``, ``pgf90``) won't
   work.


^^^
NAG
^^^

The Numerical Algorithms Group provides a licensed Fortran compiler. Like Clang,
this requires you to set up a :ref:`mixed-toolchains`. It is recommended to use
GCC for your C/C++ compilers.

The NAG Fortran compilers are a bit more strict than other compilers, and many
packages will fail to install with error messages like:

.. code-block:: none

   Error: mpi_comm_spawn_multiple_f90.f90: Argument 3 to MPI_COMM_SPAWN_MULTIPLE has data type DOUBLE PRECISION in reference from MPI_COMM_SPAWN_MULTIPLEN and CHARACTER in reference from MPI_COMM_SPAWN_MULTIPLEA

In order to convince the NAG compiler not to be too picky about calling conventions,
you can use ``FFLAGS=-mismatch`` and ``FCFLAGS=-mismatch``. This can be done through
the command line:

.. code-block:: console

   $ spack install openmpi fflags="-mismatch"

Or it can be set permanently in your ``compilers.yaml``:

.. code-block:: yaml

   - compiler:
    modules: []
    operating_system: centos6
    paths:
      cc: /soft/spack/opt/spack/linux-x86_64/gcc-5.3.0/gcc-6.1.0-q2zosj3igepi3pjnqt74bwazmptr5gpj/bin/gcc
      cxx: /soft/spack/opt/spack/linux-x86_64/gcc-5.3.0/gcc-6.1.0-q2zosj3igepi3pjnqt74bwazmptr5gpj/bin/g++
      f77: /soft/spack/opt/spack/linux-x86_64/gcc-4.4.7/nag-6.1-jt3h5hwt5myezgqguhfsan52zcskqene/bin/nagfor
      fc: /soft/spack/opt/spack/linux-x86_64/gcc-4.4.7/nag-6.1-jt3h5hwt5myezgqguhfsan52zcskqene/bin/nagfor
    flags:
      fflags: -mismatch
    spec: nag@6.1


---------------
System Packages
---------------

Once compilers are configured, one needs to determine which
pre-installed system packages, if any, to use in builds.  This is
configured in the file ``~/.spack/packages.yaml``.  For example, to use
an OpenMPI installed in /opt/local, one would use:

.. code-block:: yaml

    packages:
        openmpi:
            externals:
            - spec: openmpi@1.10.1
              prefix: /opt/local
            buildable: False

In general, Spack is easier to use and more reliable if it builds all of
its own dependencies.  However, there are several packages for which one
commonly needs to use system versions:

^^^
MPI
^^^

On supercomputers, sysadmins have already built MPI versions that take
into account the specifics of that computer's hardware.  Unless you
know how they were built and can choose the correct Spack variants,
you are unlikely to get a working MPI from Spack.  Instead, use an
appropriate pre-installed MPI.

If you choose a pre-installed MPI, you should consider using the
pre-installed compiler used to build that MPI; see above on
``compilers.yaml``.

^^^^^^^
OpenSSL
^^^^^^^

The ``openssl`` package underlies much of modern security in a modern
OS; an attacker can easily "pwn" any computer on which they can modify SSL.
Therefore, any ``openssl`` used on a system should be created in a
"trusted environment" --- for example, that of the OS vendor.

OpenSSL is also updated by the OS vendor from time to time, in
response to security problems discovered in the wider community.  It
is in everyone's best interest to use any newly updated versions as
soon as they come out.  Modern Linux installations have standard
procedures for security updates without user involvement.

Spack running at user-level is not a trusted environment, nor do Spack
users generally keep up-to-date on the latest security holes in SSL.  For
these reasons, a Spack-installed OpenSSL should likely not be trusted.

As long as the system-provided SSL works, you can use it instead.  One
can check if it works by trying to download an ``https://``.  For
example:

.. code-block:: console

    $ curl -O https://github.com/ImageMagick/ImageMagick/archive/7.0.2-7.tar.gz

To tell Spack to use the system-supplied OpenSSL, first determine what
version you have:

.. code-block:: console

   $ openssl version
   OpenSSL 1.0.2g  1 Mar 2016

Then add the following to ``~/.spack/packages.yaml``:

.. code-block:: yaml

    packages:
        openssl:
            externals:
            - spec: openssl@1.0.2g
              prefix: /usr
            buildable: False


^^^^^^^^^^^^^
BLAS / LAPACK
^^^^^^^^^^^^^

The recommended way to use system-supplied BLAS / LAPACK packages is
to add the following to ``packages.yaml``:

.. code-block:: yaml

    packages:
        netlib-lapack:
            externals:
            - spec: netlib-lapack@3.6.1
              prefix: /usr
            buildable: False
        all:
            providers:
                blas: [netlib-lapack]
                lapack: [netlib-lapack]

.. note::

   Above we pretend that the system-provided BLAS / LAPACK is ``netlib-lapack``
   only because it is the only BLAS / LAPACK provider which use standard names
   for libraries (as opposed to, for example, ``libopenblas.so``).

   Although we specify external package in ``/usr``, Spack is smart enough not
   to add ``/usr/lib`` to RPATHs, where it could cause unrelated system
   libraries to be used instead of their Spack equivalents. ``usr/bin`` will be
   present in PATH, however it will have lower precedence compared to paths
   from other dependencies. This ensures that binaries in Spack dependencies
   are preferred over system binaries.

^^^
Git
^^^

Some Spack packages use ``git`` to download, which might not work on
some computers.  For example, the following error was
encountered on a Macintosh during ``spack install julia@master``:

.. code-block:: console

   ==> Cloning git repository:
     https://github.com/JuliaLang/julia.git
     on branch master
   Cloning into 'julia'...
   fatal: unable to access 'https://github.com/JuliaLang/julia.git/':
       SSL certificate problem: unable to get local issuer certificate

This problem is related to OpenSSL, and in some cases might be solved
by installing a new version of ``git`` and ``openssl``:

#. Run ``spack install git``
#. Add the output of ``spack module tcl loads git`` to your ``.bashrc``.

If this doesn't work, it is also possible to disable checking of SSL
certificates by using:

.. code-block:: console

   $ spack --insecure install

Using ``--insecure`` makes Spack disable SSL checking when fetching
from websites and from git.

.. warning::

   This workaround should be used ONLY as a last resort!  Without SSL
   certificate verification, spack and git will download from sites you
   wouldn't normally trust.  The code you download and run may then be
   compromised!  While this is not a major issue for archives that will
   be checksummed, it is especially problematic when downloading from
   name Git branches or tags, which relies entirely on trusting a
   certificate for security (no verification).

-----------------------
Utilities Configuration
-----------------------

Although Spack does not need installation *per se*, it does rely on
other packages to be available on its host system.  If those packages
are out of date or missing, then Spack will not work.  Sometimes, an
appeal to the system's package manager can fix such problems.  If not,
the solution is have Spack install the required packages, and then
have Spack use them.

For example, if ``curl`` doesn't work, one could use the following steps
to provide Spack a working ``curl``:

.. code-block:: console

    $ spack install curl
    $ spack load curl

or alternately:

.. code-block:: console

    $ spack module tcl loads curl >>~/.bashrc

or if environment modules don't work:

.. code-block:: console

    $ export PATH=`spack location --install-dir curl`/bin:$PATH


External commands are used by Spack in two places: within core Spack,
and in the package recipes. The bootstrapping procedure for these two
cases is somewhat different, and is treated separately below.

^^^^^^^^^^^^^^^^^^^^
Core Spack Utilities
^^^^^^^^^^^^^^^^^^^^

Core Spack uses the following packages, mainly to download and unpack
source code: ``curl``, ``env``, ``git``, ``go``, ``hg``, ``svn``,
``tar``, ``unzip``, ``patch``

As long as the user's environment is set up to successfully run these
programs from outside of Spack, they should work inside of Spack as
well.  They can generally be activated as in the ``curl`` example above;
or some systems might already have an appropriate hand-built
environment module that may be loaded.  Either way works.

A few notes on specific programs in this list:

""""""""""""""""""""""""""
cURL, git, Mercurial, etc.
""""""""""""""""""""""""""

Spack depends on cURL to download tarballs, the format that most
Spack-installed packages come in.  Your system's cURL should always be
able to download unencrypted ``http://``.  However, the cURL on some
systems has problems with SSL-enabled ``https://`` URLs, due to
outdated / insecure versions of OpenSSL on those systems.  This will
prevent Spack from installing any software requiring ``https://``
until a new cURL has been installed, using the technique above.

.. warning::

   remember that if you install ``curl`` via Spack that it may rely on a
   user-space OpenSSL that is not upgraded regularly.  It may fall out of
   date faster than your system OpenSSL.

Some packages use source code control systems as their download method:
``git``, ``hg``, ``svn`` and occasionally ``go``.  If you had to install
a new ``curl``, then chances are the system-supplied version of these
other programs will also not work, because they also rely on OpenSSL.
Once ``curl`` has been installed, you can similarly install the others.


^^^^^^^^^^^^^^^^^
Package Utilities
^^^^^^^^^^^^^^^^^

Spack may also encounter bootstrapping problems inside a package's
``install()`` method.  In this case, Spack will normally be running
inside a *sanitized build environment*.  This includes all of the
package's dependencies, but none of the environment Spack inherited
from the user: if you load a module or modify ``$PATH`` before
launching Spack, it will have no effect.

In this case, you will likely need to use the ``--dirty`` flag when
running ``spack install``, causing Spack to **not** sanitize the build
environment.  You are now responsible for making sure that environment
does not do strange things to Spack or its installs.

Another way to get Spack to use its own version of something is to add
that something to a package that needs it.  For example:

.. code-block:: python

   depends_on('binutils', type='build')

This is considered best practice for some common build dependencies,
such as ``autotools`` (if the ``autoreconf`` command is needed) and
``cmake`` --- ``cmake`` especially, because different packages require
a different version of CMake.

""""""""
binutils
""""""""

.. https://groups.google.com/forum/#!topic/spack/i_7l_kEEveI

Sometimes, strange error messages can happen while building a package.
For example, ``ld`` might crash.  Or one receives a message like:

.. code-block:: console

   ld: final link failed: Nonrepresentable section on output


or:

.. code-block:: console

   ld: .../_fftpackmodule.o: unrecognized relocation (0x2a) in section `.text'

These problems are often caused by an outdated ``binutils`` on your
system.  Unlike CMake or Autotools, adding ``depends_on('binutils')`` to
every package is not considered a best practice because every package
written in C/C++/Fortran would need it.  A potential workaround is to
load a recent ``binutils`` into your environment and use the ``--dirty``
flag.

-----------
GPG Signing
-----------

.. _cmd-spack-gpg:

^^^^^^^^^^^^^
``spack gpg``
^^^^^^^^^^^^^

Spack has support for signing and verifying packages using GPG keys. A
separate keyring is used for Spack, so any keys available in the user's home
directory are not used.

^^^^^^^^^^^^^^^^^^
``spack gpg init``
^^^^^^^^^^^^^^^^^^

When Spack is first installed, its keyring is empty. Keys stored in
:file:`var/spack/gpg` are the default keys for a Spack installation. These
keys may be imported by running ``spack gpg init``. This will import the
default keys into the keyring as trusted keys.

^^^^^^^^^^^^^
Trusting keys
^^^^^^^^^^^^^

Additional keys may be added to the keyring using
``spack gpg trust <keyfile>``. Once a key is trusted, packages signed by the
owner of they key may be installed.

^^^^^^^^^^^^^
Creating keys
^^^^^^^^^^^^^

You may also create your own key so that you may sign your own packages using
``spack gpg create <name> <email>``. By default, the key has no expiration,
but it may be set with the ``--expires <date>`` flag (see the ``gnupg2``
documentation for accepted date formats). It is also recommended to add a
comment as to the use of the key using the ``--comment <comment>`` flag. The
public half of the key can also be exported for sharing with others so that
they may use packages you have signed using the ``--export <keyfile>`` flag.
Secret keys may also be later exported using the
``spack gpg export <location> [<key>...]`` command.

.. note::

   Key creation speed
      The creation of a new GPG key requires generating a lot of random numbers.
      Depending on the entropy produced on your system, the entire process may
      take a long time (*even appearing to hang*). Virtual machines and cloud
      instances are particularly likely to display this behavior.

      To speed it up you may install tools like ``rngd``, which is
      usually available as a package in the host OS.  On e.g. an
      Ubuntu machine you need to give the following commands:

      .. code-block:: console

         $ sudo apt-get install rng-tools
         $ sudo rngd -r /dev/urandom

      before generating the keys.

      Another alternative is ``haveged``, which can be installed on
      RHEL/CentOS machines as follows:

      .. code-block:: console

         $ sudo yum install haveged
         $ sudo chkconfig haveged on

      `This Digital Ocean tutorial
      <https://www.digitalocean.com/community/tutorials/how-to-setup-additional-entropy-for-cloud-servers-using-haveged>`_
      provides a good overview of sources of randomness.

Here is an example of creating a key. Note that we provide a name for the key first
(which we can use to reference the key later) and an email address:

.. code-block:: console

    $ spack gpg create dinosaur dinosaur@thedinosaurthings.com


If you want to export the key as you create it:


.. code-block:: console

    $ spack gpg create --export key.pub dinosaur dinosaur@thedinosaurthings.com

Or the private key:


.. code-block:: console

    $ spack gpg create --export-secret key.priv dinosaur dinosaur@thedinosaurthings.com


You can include both ``--export`` and ``--export-secret``, each with
an output file of choice, to export both.


^^^^^^^^^^^^
Listing keys
^^^^^^^^^^^^

In order to list the keys available in the keyring, the
``spack gpg list`` command will list trusted keys with the ``--trusted`` flag
and keys available for signing using ``--signing``. If you would like to
remove keys from your keyring, ``spack gpg untrust <keyid>``. Key IDs can be
email addresses, names, or (best) fingerprints. Here is an example of listing
the key that we just created:

.. code-block:: console

    gpgconf: socketdir is '/run/user/1000/gnupg'
    /home/spackuser/spack/opt/spack/gpg/pubring.kbx
    ----------------------------------------------------------
    pub   rsa4096 2021-03-25 [SC]
          60D2685DAB647AD4DB54125961E09BB6F2A0ADCB
    uid           [ultimate] dinosaur (GPG created for Spack) <dinosaur@thedinosaurthings.com>


Note that the name "dinosaur" can be seen under the uid, which is the unique
id. We might need this reference if we want to export or otherwise reference the key.


^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Signing and Verifying Packages
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

In order to sign a package, ``spack gpg sign <file>`` should be used. By
default, the signature will be written to ``<file>.asc``, but that may be
changed by using the ``--output <file>`` flag. If there is only one signing
key available, it will be used, but if there is more than one, the key to use
must be specified using the ``--key <keyid>`` flag. The ``--clearsign`` flag
may also be used to create a signed file which contains the contents, but it
is not recommended. Signed packages may be verified by using
``spack gpg verify <file>``.


^^^^^^^^^^^^^^
Exporting Keys
^^^^^^^^^^^^^^

You likely might want to export a public key, and that looks like this. Let's
use the previous example and ask spack to export the key with uid "dinosaur."
We will provide an output location (typically a `*.pub` file) and the name of
the key.

.. code-block:: console

    $ spack gpg export dinosaur.pub dinosaur

You can then look at the created file, `dinosaur.pub`, to see the exported key.
If you want to include the private key, then just add `--secret`:

.. code-block:: console

    $ spack gpg export --secret dinosaur.priv dinosaur

This will write the private key to the file `dinosaur.priv`.

.. warning::

    You should be very careful about exporting private keys. You likely would
    only want to do this in the context of moving your spack installation to
    a different server, and wanting to preserve keys for a buildcache. If you
    are unsure about exporting, you can ask your local system administrator
    or for help on an issue or the Spack slack.


.. _cray-support:

-------------
Spack on Cray
-------------

Spack differs slightly when used on a Cray system. The architecture spec
can differentiate between the front-end and back-end processor and operating system.
For example, on Edison at NERSC, the back-end target processor
is "Ivy Bridge", so you can specify to use the back-end this way:

.. code-block:: console

   $ spack install zlib target=ivybridge

You can also use the operating system to build against the back-end:

.. code-block:: console

   $ spack install zlib os=CNL10

Notice that the name includes both the operating system name and the major
version number concatenated together.

Alternatively, if you want to build something for the front-end,
you can specify the front-end target processor. The processor for a login node
on Edison is "Sandy bridge" so we specify on the command line like so:

.. code-block:: console

   $ spack install zlib target=sandybridge

And the front-end operating system is:

.. code-block:: console

   $ spack install zlib os=SuSE11

^^^^^^^^^^^^^^^^^^^^^^^
Cray compiler detection
^^^^^^^^^^^^^^^^^^^^^^^

Spack can detect compilers using two methods. For the front-end, we treat
everything the same. The difference lies in back-end compiler detection.
Back-end compiler detection is made via the Tcl module avail command.
Once it detects the compiler it writes the appropriate PrgEnv and compiler
module name to compilers.yaml and sets the paths to each compiler with Cray\'s
compiler wrapper names (i.e. cc, CC, ftn). During build time, Spack will load
the correct PrgEnv and compiler module and will call appropriate wrapper.

The compilers.yaml config file will also differ. There is a
modules section that is filled with the compiler's Programming Environment
and module name. On other systems, this field is empty []:

.. code-block:: yaml

   - compiler:
       modules:
         - PrgEnv-intel
         - intel/15.0.109

As mentioned earlier, the compiler paths will look different on a Cray system.
Since most compilers are invoked using cc, CC and ftn, the paths for each
compiler are replaced with their respective Cray compiler wrapper names:

.. code-block:: yaml

     paths:
       cc: cc
       cxx: CC
       f77: ftn
       fc: ftn

As opposed to an explicit path to the compiler executable. This allows Spack
to call the Cray compiler wrappers during build time.

For more on compiler configuration, check out :ref:`compiler-config`.

Spack sets the default Cray link type to dynamic, to better match other
other platforms. Individual packages can enable static linking (which is the
default outside of Spack on cray systems) using the ``-static`` flag.

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Setting defaults and using Cray modules
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

If you want to use default compilers for each PrgEnv and also be able
to load cray external modules, you will need to set up a ``packages.yaml``.

Here's an example of an external configuration for cray modules:

.. code-block:: yaml

   packages:
     mpich:
       externals:
       - spec: "mpich@7.3.1%gcc@5.2.0 arch=cray_xc-haswell-CNL10"
         modules:
         - cray-mpich
       - spec: "mpich@7.3.1%intel@16.0.0.109 arch=cray_xc-haswell-CNL10"
         modules:
         - cray-mpich
     all:
       providers:
         mpi: [mpich]

This tells Spack that for whatever package that depends on mpi, load the
cray-mpich module into the environment. You can then be able to use whatever
environment variables, libraries, etc, that are brought into the environment
via module load.

.. note::

    For Cray-provided packages, it is best to use ``modules:`` instead of ``prefix:``
    in ``packages.yaml``, because the Cray Programming Environment heavily relies on
    modules (e.g., loading the ``cray-mpich`` module adds MPI libraries to the
    compiler wrapper link line).

You can set the default compiler that Spack can use for each compiler type.
If you want to use the Cray defaults, then set them under ``all:`` in packages.yaml.
In the compiler field, set the compiler specs in your order of preference.
Whenever you build with that compiler type, Spack will concretize to that version.

Here is an example of a full packages.yaml used at NERSC

.. code-block:: yaml

   packages:
     mpich:
       externals:
       - spec: "mpich@7.3.1%gcc@5.2.0 arch=cray_xc-CNL10-ivybridge"
         modules:
         - cray-mpich
       - spec: "mpich@7.3.1%intel@16.0.0.109 arch=cray_xc-SuSE11-ivybridge"
         modules:
         - cray-mpich
       buildable: False
     netcdf:
       externals:
       - spec: "netcdf@4.3.3.1%gcc@5.2.0 arch=cray_xc-CNL10-ivybridge"
         modules:
         - cray-netcdf
       - spec: "netcdf@4.3.3.1%intel@16.0.0.109 arch=cray_xc-CNL10-ivybridge"
         modules:
         - cray-netcdf
       buildable: False
     hdf5:
       externals:
       - spec: "hdf5@1.8.14%gcc@5.2.0 arch=cray_xc-CNL10-ivybridge"
         modules:
         - cray-hdf5
       - spec: "hdf5@1.8.14%intel@16.0.0.109 arch=cray_xc-CNL10-ivybridge"
         modules:
         - cray-hdf5
       buildable: False
     all:
       compiler: [gcc@5.2.0, intel@16.0.0.109]
       providers:
         mpi: [mpich]

Here we tell spack that whenever we want to build with gcc use version 5.2.0 or
if we want to build with intel compilers, use version 16.0.0.109. We add a spec
for each compiler type for each cray modules. This ensures that for each
compiler on our system we can use that external module.

For more on external packages check out the section :ref:`sec-external-packages`.

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Using Linux containers on Cray machines
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Spack uses environment variables particular to the Cray programming
environment to determine which systems are Cray platforms. These
environment variables may be propagated into containers that are not
using the Cray programming environment.

To ensure that Spack does not autodetect the Cray programming
environment, unset the environment variable ``MODULEPATH``. This
will cause Spack to treat a linux container on a Cray system as a base
linux distro.
.. Copyright 2013-2022 Lawrence Livermore National Security, LLC and other
   Spack Project Developers. See the top-level COPYRIGHT file for details.

   SPDX-License-Identifier: (Apache-2.0 OR MIT)

.. _config-yaml:

==============
Basic Settings
==============

Spack's basic configuration options are set in ``config.yaml``.  You can
see the default settings by looking at
``etc/spack/defaults/config.yaml``:

.. literalinclude:: _spack_root/etc/spack/defaults/config.yaml
   :language: yaml

These settings can be overridden in ``etc/spack/config.yaml`` or
``~/.spack/config.yaml``.  See :ref:`configuration-scopes` for details.

--------------------
``install_tree``
--------------------

The location where Spack will install packages and their dependencies.
Default is ``$spack/opt/spack``.

---------------------------------------------------
``install_hash_length`` and ``install_path_scheme``
---------------------------------------------------

The default Spack installation path can be very long and can create problems
for scripts with hardcoded shebangs. Additionally, when using the Intel
compiler, and if there is also a long list of dependencies, the compiler may
segfault. If you see the following:

     .. code-block:: console

       : internal error: ** The compiler has encountered an unexpected problem.
       ** Segmentation violation signal raised. **
       Access violation or stack overflow. Please contact Intel Support for assistance.

it may be because variables containing dependency specs may be too long. There
are two parameters to help with long path names. Firstly, the
``install_hash_length`` parameter can set the length of the hash in the
installation path from 1 to 32. The default path uses the full 32 characters.

Secondly, it is also possible to modify the entire installation
scheme. By default Spack uses
``{architecture}/{compiler.name}-{compiler.version}/{name}-{version}-{hash}``
where the tokens that are available for use in this directive are the
same as those understood by the :meth:`~spack.spec.Spec.format`
method. Using this parameter it is possible to use a different package
layout or reduce the depth of the installation paths. For example

     .. code-block:: yaml

       config:
         install_path_scheme: '{name}/{version}/{hash:7}'

would install packages into sub-directories using only the package
name, version and a hash length of 7 characters.

When using either parameter to set the hash length it only affects the
representation of the hash in the installation directory. You
should be aware that the smaller the hash length the more likely
naming conflicts will occur. These parameters are independent of those
used to configure module names.

.. warning:: Modifying the installation hash length or path scheme after
   packages have been installed will prevent Spack from being
   able to find the old installation directories.

--------------------
``module_roots``
--------------------

Controls where Spack installs generated module files.  You can customize
the location for each type of module.  e.g.:

.. code-block:: yaml

   module_roots:
     tcl:    $spack/share/spack/modules
     lmod:   $spack/share/spack/lmod

See :ref:`modules` for details.

--------------------
``build_stage``
--------------------

Spack is designed to run out of a user home directory, and on many
systems the home directory is a (slow) network file system.  On most systems,
building in a temporary file system is faster.  Usually, there is also more
space available in the temporary location than in the home directory.  If the
username is not already in the path, Spack will append the value of ``$user`` to
the selected ``build_stage`` path.

.. warning:: We highly recommend specifying ``build_stage`` paths that
   distinguish between staging and other activities to ensure
   ``spack clean`` does not inadvertently remove unrelated files.
   Spack prepends ``spack-stage-`` to temporary staging directory names to
   reduce this risk.  Using a combination of ``spack`` and or ``stage`` in
   each specified path, as shown in the default settings and documented
   examples, will add another layer of protection.

By default, Spack's ``build_stage`` is configured like this:

.. code-block:: yaml

   build_stage:
    - $tempdir/$user/spack-stage
    - ~/.spack/stage

This can be an ordered list of paths that Spack should search when trying to
find a temporary directory for the build stage.  The list is searched in
order, and Spack will use the first directory to which it has write access.

Specifying `~/.spack/stage` first will ensure each user builds in their home
directory.  The historic Spack stage path `$spack/var/spack/stage` will build
directly inside the Spack instance.  See :ref:`config-file-variables` for more
on ``$tempdir`` and ``$spack``.

When Spack builds a package, it creates a temporary directory within the
``build_stage``.  After the package is successfully installed, Spack deletes
the temporary directory it used to build.  Unsuccessful builds are not
deleted, but you can manually purge them with :ref:`spack clean --stage
<cmd-spack-clean>`.

.. note::

   The build will fail if there is no writable directory in the ``build_stage``
   list, where any user- and site-specific setting will be searched first.

--------------------
``source_cache``
--------------------

Location to cache downloaded tarballs and repositories.  By default these
are stored in ``$spack/var/spack/cache``.  These are stored indefinitely
by default. Can be purged with :ref:`spack clean --downloads
<cmd-spack-clean>`.

--------------------
``misc_cache``
--------------------

Temporary directory to store long-lived cache files, such as indices of
packages available in repositories.  Defaults to ``~/.spack/cache``.  Can
be purged with :ref:`spack clean --misc-cache <cmd-spack-clean>`.

--------------------
``verify_ssl``
--------------------

When set to ``true`` (default) Spack will verify certificates of remote
hosts when making ``ssl`` connections.  Set to ``false`` to disable, and
tools like ``curl`` will use their ``--insecure`` options.  Disabling
this can expose you to attacks.  Use at your own risk.

--------------------
``checksum``
--------------------

When set to ``true``, Spack verifies downloaded source code using a
checksum, and will refuse to build packages that it cannot verify.  Set
to ``false`` to disable these checks.  Disabling this can expose you to
attacks.  Use at your own risk.

--------------------
``locks``
--------------------

When set to ``true``, concurrent instances of Spack will use locks to
avoid modifying the install tree, database file, etc. If false, Spack
will disable all locking, but you must **not** run concurrent instances
of Spack.  For file systems that don't support locking, you should set
this to ``false`` and run one Spack at a time, but otherwise we recommend
enabling locks.

--------------------
``dirty``
--------------------

By default, Spack unsets variables in your environment that can change
the way packages build. This includes ``LD_LIBRARY_PATH``, ``CPATH``,
``LIBRARY_PATH``, ``DYLD_LIBRARY_PATH``, and others.

By default, builds are ``clean``, but on some machines, compilers and
other tools may need custom ``LD_LIBRARY_PATH`` settings to run.  You can
set ``dirty`` to ``true`` to skip the cleaning step and make all builds
"dirty" by default.  Be aware that this will reduce the reproducibility
of builds.

.. _build-jobs:

--------------
``build_jobs``
--------------

Unless overridden in a package or on the command line, Spack builds all
packages in parallel. The default parallelism is equal to the number of
cores available to the process, up to 16 (the default of ``build_jobs``).
For a build system that uses Makefiles, this ``spack install`` runs:

- ``make -j<build_jobs>``, when ``build_jobs`` is less than the number of
  cores available
- ``make -j<ncores>``, when ``build_jobs`` is greater or equal to the
  number of cores available

If you work on a shared login node or have a strict ulimit, it may be
necessary to set the default to a lower value. By setting ``build_jobs``
to 4, for example, commands like ``spack install`` will run ``make -j4``
instead of hogging every core. To build all software in serial,
set ``build_jobs`` to 1.

Note that specifying the number of jobs on the command line always takes
priority, so that ``spack install -j<n>`` always runs `make -j<n>`, even
when that exceeds the number of cores available.

--------------------
``ccache``
--------------------

When set to ``true`` Spack will use ccache to cache compiles. This is
useful specifically in two cases: (1) when using ``spack dev-build``, and (2)
when building the same package with many different variants. The default is
``false``.

When enabled, Spack will look inside your ``PATH`` for a ``ccache``
executable and stop if it is not found. Some systems come with
``ccache``, but it can also be installed using ``spack install
ccache``. ``ccache`` comes with reasonable defaults for cache size
and location. (See the *Configuration settings* section of ``man
ccache`` to learn more about the default settings and how to change
them). Please note that we currently disable ccache's ``hash_dir``
feature to avoid an issue with the stage directory (see
https://github.com/LLNL/spack/pull/3761#issuecomment-294352232).

------------------
``shared_linking``
------------------

Control whether Spack embeds ``RPATH`` or ``RUNPATH`` attributes in ELF binaries
so that they can find their dependencies. Has no effect on macOS.
Two options are allowed:

 1. ``rpath`` uses ``RPATH`` and forces the ``--disable-new-tags`` flag to be passed to the linker
 2. ``runpath`` uses ``RUNPATH`` and forces the ``--enable-new-tags`` flag to be passed to the linker

``RPATH`` search paths have higher precedence than ``LD_LIBRARY_PATH``
and ld.so will search for libraries in transitive ``RPATHs`` of
parent objects.

``RUNPATH`` search paths have lower precedence than ``LD_LIBRARY_PATH``,
and ld.so will ONLY search for dependencies in the ``RUNPATH`` of
the loading object.

DO NOT MIX the two options within the same install tree.

----------------------
``terminal_title``
----------------------

By setting this option to ``true``, Spack will update the terminal's title to
provide information about its current progress as well as the current and
total package numbers.

To work properly, this requires your terminal to reset its title after
Spack has finished its work, otherwise Spack's status information will
remain in the terminal's title indefinitely. Most terminals should already
be set up this way and clear Spack's status information.
.. Copyright 2013-2022 Lawrence Livermore National Security, LLC and other
   Spack Project Developers. See the top-level COPYRIGHT file for details.

   SPDX-License-Identifier: (Apache-2.0 OR MIT)

.. chain:

============================
Chaining Spack Installations
============================

You can point your Spack installation to another installation to use any
packages that are installed there. To register the other Spack instance,
you can add it as an entry to ``upstreams.yaml``:

.. code-block:: yaml

  upstreams:
    spack-instance-1:
      install_tree: /path/to/other/spack/opt/spack
    spack-instance-2:
      install_tree: /path/to/another/spack/opt/spack

``install_tree`` must point to the ``opt/spack`` directory inside of the
Spack base directory.

Once the upstream Spack instance has been added, ``spack find`` will
automatically check the upstream instance when querying installed packages,
and new package installations for the local Spack install will use any
dependencies that are installed in the upstream instance.

This other instance of Spack has no knowledge of the local Spack instance
and may not have the same permissions or ownership as the local Spack instance.
This has the following consequences:

#. Upstream Spack instances are not locked. Therefore it is up to users to
   make sure that the local instance is not using an upstream instance when it
   is being modified.

#. Users should not uninstall packages from the upstream instance. Since the
   upstream instance doesn't know about the local instance, it cannot prevent
   the uninstallation of packages which the local instance depends on.

Other details about upstream installations:

#. If a package is installed both locally and upstream, the local installation
   will always be used as a dependency. This can occur if the local Spack
   installs a package which is not present in the upstream, but later on the
   upstream Spack instance also installs that package.

#. If an upstream Spack instance registers and installs an external package,
   the local Spack instance will treat this the same as a Spack-installed
   package. This feature will only work if the upstream Spack instance
   includes the upstream functionality (i.e. if its commit is after March
   27, 2019).

---------------------------------------
Using Multiple Upstream Spack Instances
---------------------------------------

A single Spack instance can use multiple upstream Spack installations. Spack
will search upstream instances in the order you list them in your
configuration. If your installation refers to instances X and Y, in that order,
then instance X must list Y as an upstream in its own ``upstreams.yaml``.

-----------------------------------
Using Modules for Upstream Packages
-----------------------------------

The local Spack instance does not generate modules for packages which are
installed upstream. The local Spack instance can be configured to use the
modules generated by the upstream Spack instance.

There are two requirements to use the modules created by an upstream Spack
instance: firstly the upstream instance must do a ``spack module tcl refresh``,
which generates an index file that maps installed packages to their modules;
secondly, the local Spack instance must add a ``modules`` entry to the
configuration:

.. code-block:: yaml

  upstreams:
    spack-instance-1:
      install_tree: /path/to/other/spack/opt/spack
      modules:
        tcl: /path/to/other/spack/share/spack/modules

Each time new packages are installed in the upstream Spack instance, the
upstream Spack maintainer should run ``spack module tcl refresh`` (or the
corresponding command for the type of module they intend to use).

.. note::

   Spack can generate modules that :ref:`automatically load
   <autoloading-dependencies>` the modules of dependency packages. Spack cannot
   currently do this for modules in upstream packages.
.. Copyright 2013-2022 Lawrence Livermore National Security, LLC and other
   Spack Project Developers. See the top-level COPYRIGHT file for details.

   SPDX-License-Identifier: (Apache-2.0 OR MIT)

.. _modules:

=======
Modules
=======

The use of module systems to manage user environment in a controlled way
is a common practice at HPC centers that is often embraced also by
individual programmers on their development machines. To support this
common practice Spack integrates with `Environment Modules
<http://modules.sourceforge.net/>`_ and `LMod
<http://lmod.readthedocs.io/en/latest/>`_ by providing post-install hooks
that generate module files and commands to manipulate them.

Modules are one of several ways you can use Spack packages. For other
options that may fit your use case better, you should also look at
:ref:`spack load <spack-load>` and :ref:`environments <environments>`.

----------------------------
Using module files via Spack
----------------------------

If you have installed a supported module system you should be able to
run either ``module avail`` or ``use -l spack`` to see what module
files have been installed.  Here is sample output of those programs,
showing lots of installed packages:

.. code-block:: console

   $ module avail

   --------------------------------------------------------------- ~/spack/share/spack/modules/linux-ubuntu14-x86_64 ---------------------------------------------------------------
   autoconf-2.69-gcc-4.8-qextxkq       hwloc-1.11.6-gcc-6.3.0-akcisez             m4-1.4.18-gcc-4.8-ev2znoc                   openblas-0.2.19-gcc-6.3.0-dhkmed6        py-setuptools-34.2.0-gcc-6.3.0-fadur4s
   automake-1.15-gcc-4.8-maqvukj       isl-0.18-gcc-4.8-afi6taq                   m4-1.4.18-gcc-6.3.0-uppywnz                 openmpi-2.1.0-gcc-6.3.0-go2s4z5          py-six-1.10.0-gcc-6.3.0-p4dhkaw
   binutils-2.28-gcc-4.8-5s7c6rs       libiconv-1.15-gcc-4.8-at46wg3              mawk-1.3.4-gcc-4.8-acjez57                  openssl-1.0.2k-gcc-4.8-dkls5tk           python-2.7.13-gcc-6.3.0-tyehea7
   bison-3.0.4-gcc-4.8-ek4luo5         libpciaccess-0.13.4-gcc-6.3.0-gmufnvh      mawk-1.3.4-gcc-6.3.0-ostdoms                openssl-1.0.2k-gcc-6.3.0-gxgr5or         readline-7.0-gcc-4.8-xhufqhn
   bzip2-1.0.6-gcc-4.8-iffrxzn         libsigsegv-2.11-gcc-4.8-pp2cvte            mpc-1.0.3-gcc-4.8-g5mztc5                   pcre-8.40-gcc-4.8-r5pbrxb                readline-7.0-gcc-6.3.0-zzcyicg
   bzip2-1.0.6-gcc-6.3.0-bequudr       libsigsegv-2.11-gcc-6.3.0-7enifnh          mpfr-3.1.5-gcc-4.8-o7xm7az                  perl-5.24.1-gcc-4.8-dg5j65u              sqlite-3.8.5-gcc-6.3.0-6zoruzj
   cmake-3.7.2-gcc-6.3.0-fowuuby       libtool-2.4.6-gcc-4.8-7a523za              mpich-3.2-gcc-6.3.0-dmvd3aw                 perl-5.24.1-gcc-6.3.0-6uzkpt6            tar-1.29-gcc-4.8-wse2ass
   curl-7.53.1-gcc-4.8-3fz46n6         libtool-2.4.6-gcc-6.3.0-n7zmbzt            ncurses-6.0-gcc-4.8-dcpe7ia                 pkg-config-0.29.2-gcc-4.8-ib33t75        tcl-8.6.6-gcc-4.8-tfxzqbr
   expat-2.2.0-gcc-4.8-mrv6bd4         libxml2-2.9.4-gcc-4.8-ryzxnsu              ncurses-6.0-gcc-6.3.0-ucbhcdy               pkg-config-0.29.2-gcc-6.3.0-jpgubk3      util-macros-1.19.1-gcc-6.3.0-xorz2x2
   flex-2.6.3-gcc-4.8-yf345oo          libxml2-2.9.4-gcc-6.3.0-rltzsdh            netlib-lapack-3.6.1-gcc-6.3.0-js33dog       py-appdirs-1.4.0-gcc-6.3.0-jxawmw7       xz-5.2.3-gcc-4.8-mew4log
   gcc-6.3.0-gcc-4.8-24puqve           lmod-7.4.1-gcc-4.8-je4srhr                 netlib-scalapack-2.0.2-gcc-6.3.0-5aidk4l    py-numpy-1.12.0-gcc-6.3.0-oemmoeu        xz-5.2.3-gcc-6.3.0-3vqeuvb
   gettext-0.19.8.1-gcc-4.8-yymghlh    lua-5.3.4-gcc-4.8-im75yaz                  netlib-scalapack-2.0.2-gcc-6.3.0-hjsemcn    py-packaging-16.8-gcc-6.3.0-i2n3dtl      zip-3.0-gcc-4.8-rwar22d
   gmp-6.1.2-gcc-4.8-5ub2wu5           lua-luafilesystem-1_6_3-gcc-4.8-wkey3nl    netlib-scalapack-2.0.2-gcc-6.3.0-jva724b    py-pyparsing-2.1.10-gcc-6.3.0-tbo6gmw    zlib-1.2.11-gcc-4.8-pgxsxv7
   help2man-1.47.4-gcc-4.8-kcnqmau     lua-luaposix-33.4.0-gcc-4.8-mdod2ry        netlib-scalapack-2.0.2-gcc-6.3.0-rgqfr6d    py-scipy-0.19.0-gcc-6.3.0-kr7nat4        zlib-1.2.11-gcc-6.3.0-7cqp6cj

The names should look familiar, as they resemble the output from ``spack find``.
You *can* use the modules here directly.  For example, you could type either of these commands
to load the ``cmake`` module:

.. code-block:: console

   $ use cmake-3.7.2-gcc-6.3.0-fowuuby

.. code-block:: console

   $ module load cmake-3.7.2-gcc-6.3.0-fowuuby

Neither of these is particularly pretty, easy to remember, or easy to
type. Luckily, Spack offers many facilities for customizing the module
scheme used at your site.

-------------------------
Module file customization
-------------------------

Module files are generated by post-install hooks after the successful
installation of a package.

.. note::

   Spack only generates modulefiles when a package is installed. If
   you attempt to install a package and it is already installed, Spack
   will not regenerate modulefiles for the package. This may to
   inconsistent modulefiles if the Spack module configuration has
   changed since the package was installed, either by editing a file
   or changing scopes or environments.

   Later in this section there is a subsection on :ref:`regenerating
   modules <cmd-spack-module-refresh>` that will allow you to bring
   your modules to a consistent state.

The table below summarizes the essential information associated with
the different file formats that can be generated by Spack:


  +-----------------------------+--------------------+-------------------------------+----------------------------------------------+----------------------+
  |                             | **Hook name**      |  **Default root directory**   | **Default template file**                    | **Compatible tools** |
  +=============================+====================+===============================+==============================================+======================+
  |  **TCL - Non-Hierarchical** | ``tcl``            |  share/spack/modules          | share/spack/templates/modules/modulefile.tcl | Env. Modules/LMod    |
  +-----------------------------+--------------------+-------------------------------+----------------------------------------------+----------------------+
  |  **Lua - Hierarchical**     | ``lmod``           |  share/spack/lmod             | share/spack/templates/modules/modulefile.lua | LMod                 |
  +-----------------------------+--------------------+-------------------------------+----------------------------------------------+----------------------+


Spack ships with sensible defaults for the generation of module files, but
you can customize many aspects of it to accommodate package or site specific needs.
In general you can override or extend the default behavior by:

 1. overriding certain callback APIs in the Python packages
 2. writing specific rules in the ``modules.yaml`` configuration file
 3. writing your own templates to override or extend the defaults

The former method let you express changes in the run-time environment
that are needed to use the installed software properly, e.g. injecting variables
from language interpreters into their extensions. The latter two instead permit to
fine tune the filesystem layout, content and creation of module files to meet
site specific conventions.

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Override API calls in ``package.py``
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

There are two methods that you can override in any ``package.py`` to affect the
content of the module files generated by Spack. The first one:

.. code-block:: python

   def setup_run_environment(self, env):
       pass

can alter the content of the module file associated with the same package where it is overridden.
The second method:

.. code-block:: python

   def setup_dependent_run_environment(self, env, dependent_spec):
       pass

can instead inject run-time environment modifications in the module files of packages
that depend on it. In both cases you need to fill ``run_env`` with the desired
list of environment modifications.

.. admonition:: The ``r`` package and callback APIs

  An example in which it is crucial to override both methods
  is given by the ``r`` package. This package installs libraries and headers
  in non-standard locations and it is possible to prepend the appropriate directory
  to the corresponding environment variables:

  ================== =================================
   LD_LIBRARY_PATH    ``self.prefix/rlib/R/lib``
   PKG_CONFIG_PATH    ``self.prefix/rlib/pkgconfig``
  ================== =================================

  with the following snippet:

  .. literalinclude:: _spack_root/var/spack/repos/builtin/packages/r/package.py
     :pyobject: R.setup_run_environment

  The ``r`` package also knows which environment variable should be modified
  to make language extensions provided by other packages available, and modifies
  it appropriately in the override of the second method:

  .. literalinclude:: _spack_root/var/spack/repos/builtin/packages/r/package.py
     :pyobject: R.setup_dependent_run_environment

.. _modules-yaml:

^^^^^^^^^^^^^^^^^^^^^^^^^^
Write a configuration file
^^^^^^^^^^^^^^^^^^^^^^^^^^

The configuration files that control module generation behavior
are named ``modules.yaml``. The default configuration:

.. literalinclude:: _spack_root/etc/spack/defaults/modules.yaml
   :language: yaml

activates the hooks to generate ``tcl`` module files and inspects
the installation folder of each package for the presence of a set of subdirectories
(``bin``, ``man``, ``share/man``, etc.). If any is found its full path is prepended
to the environment variables listed below the folder name.

Spack modules can be configured for multiple module sets. The default
module set is named ``default``. All Spack commands which operate on
modules default to apply the ``default`` module set, but can be
applied to any module set in the configuration. Settings applied at
the root of the configuration (e.g. ``modules:enable`` rather than
``modules:default:enable``) are applied to the default module set for
backwards compatibility.

"""""""""""""""""""""""""
Changing the modules root
"""""""""""""""""""""""""

As shown in the table above, the default module root for ``lmod`` is
``$spack/share/spack/lmod`` and the default root for ``tcl`` is
``$spack/share/spack/modules``. This can be overridden for any module
set by changing the ``roots`` key of the configuration.

.. code-block:: yaml

   modules:
     default:
       roots:
         tcl: /path/to/install/tcl/modules
     my_custom_lmod_modules:
       roots:
         lmod: /path/to/install/custom/lmod/modules
         ...

This configuration will create two module sets. The default module set
will install its ``tcl`` modules to ``/path/to/install/tcl/modules``
(and still install its lmod modules, if any, to the default
location). The set ``my_custom_lmod_modules`` will install its lmod
modules to ``/path/to/install/custom/lmod/modules`` (and still install
its tcl modules, if any, to the default location).

By default, an architecture-specific directory is added to the root
directory. A module set may override that behavior by setting the
``arch_folder`` config value to ``False``.

.. code-block:: yaml

   modules:
     default:
       roots:
         tcl: /path/to/install/tcl/modules
       arch_folder: false

Obviously, having multiple module sets install modules to the default
location could be confusing to users of your modules. In the next
section, we will discuss enabling and disabling module types (module
file generators) for each module set.

""""""""""""""""""""
Activate other hooks
""""""""""""""""""""

Any other module file generator shipped with Spack can be activated adding it to the
list under the ``enable`` key in the module file. Currently the only generator that
is not active by default is ``lmod``, which produces hierarchical lua module files.

Each module system can then be configured separately. In fact, you should list configuration
options that affect a particular type of module files under a top level key corresponding
to the generator being customized:

.. code-block:: yaml

   modules:
     default:
       enable:
         - tcl
         - lmod
       tcl:
         # contains environment modules specific customizations
       lmod:
         # contains lmod specific customizations

In general, the configuration options that you can use in ``modules.yaml`` will
either change the layout of the module files on the filesystem, or they will affect
their content. For the latter point it is possible to use anonymous specs
to fine tune the set of packages on which the modifications should be applied.

.. _anonymous_specs:

""""""""""""""""""""""""""""
Selection by anonymous specs
""""""""""""""""""""""""""""

In the configuration file you can use *anonymous specs* (i.e. specs
that **are not required to have a root package** and are thus used just
to express constraints) to apply certain modifications on a selected set
of the installed software. For instance, in the snippet below:

.. code-block:: yaml

   modules:
     default:
       tcl:
         # The keyword `all` selects every package
         all:
           environment:
             set:
               BAR: 'bar'
         # This anonymous spec selects any package that
         # depends on openmpi. The double colon at the
         # end clears the set of rules that matched so far.
         ^openmpi::
           environment:
             set:
               BAR: 'baz'
         # Selects any zlib package
         zlib:
           environment:
             prepend_path:
               LD_LIBRARY_PATH: 'foo'
         # Selects zlib compiled with gcc@4.8
         zlib%gcc@4.8:
           environment:
             unset:
             - FOOBAR

you are instructing Spack to set the environment variable ``BAR=bar`` for every module,
unless the associated spec satisfies ``^openmpi`` in which case ``BAR=baz``.
In addition in any spec that satisfies ``zlib`` the value ``foo`` will be
prepended to ``LD_LIBRARY_PATH`` and in any spec that satisfies ``zlib%gcc@4.8``
the variable ``FOOBAR`` will be unset.

.. note::
   Order does matter
     The modifications associated with the ``all`` keyword are always evaluated
     first, no matter where they appear in the configuration file. All the other
     spec constraints are instead evaluated top to bottom.

""""""""""""""""""""""""""""""""""""""""""""
Blacklist or whitelist specific module files
""""""""""""""""""""""""""""""""""""""""""""

You can use anonymous specs also to prevent module files from being written or
to force them to be written. Consider the case where you want to hide from users
all the boilerplate software that you had to build in order to bootstrap a new
compiler. Suppose for instance that ``gcc@4.4.7`` is the compiler provided by
your system. If you write a configuration file like:

.. code-block:: yaml

   modules:
     default:
       tcl:
         whitelist: ['gcc', 'llvm']  # Whitelist will have precedence over blacklist
         blacklist: ['%gcc@4.4.7']   # Assuming gcc@4.4.7 is the system compiler

you will prevent the generation of module files for any package that
is compiled with ``gcc@4.4.7``, with the only exception of any ``gcc``
or any ``llvm`` installation.


.. _modules-projections:

"""""""""""""""""""""""""""""""
Customize the naming of modules
"""""""""""""""""""""""""""""""

The names of environment modules generated by spack are not always easy to
fully comprehend due to the long hash in the name. There are three module
configuration options to help with that. The first is a global setting to
adjust the hash length. It can be set anywhere from 0 to 32 and has a default
length of 7. This is the representation of the hash in the module file name and
does not affect the size of the package hash. Be aware that the smaller the
hash length the more likely naming conflicts will occur. The following snippet
shows how to set hash length in the module file names:

.. code-block:: yaml

   modules:
     default:
       tcl:
         hash_length: 7

To help make module names more readable, and to help alleviate name conflicts
with a short hash, one can use the ``suffixes`` option in the modules
configuration file. This option will add strings to modules that match a spec.
For instance, the following config options,

.. code-block:: yaml

   modules:
     default:
       tcl:
         all:
           suffixes:
             ^python@2.7.12: 'python-2.7.12'
             ^openblas: 'openblas'

will add a ``python-2.7.12`` version string to any packages compiled with
python matching the spec, ``python@2.7.12``. This is useful to know which
version of python a set of python extensions is associated with. Likewise, the
``openblas`` string is attached to any program that has openblas in the spec,
most likely via the ``+blas`` variant specification.

The most heavyweight solution to module naming is to change the entire
naming convention for module files. This uses the projections format
covered in :ref:`adding_projections_to_views`.

.. code-block:: yaml

  modules:
    default:
      tcl:
        projections:
          all: '{name}/{version}-{compiler.name}-{compiler.version}-module'
          ^mpi: '{name}/{version}-{^mpi.name}-{^mpi.version}-{compiler.name}-{compiler.version}-module'

will create module files that are nested in directories by package
name, contain the version and compiler name and version, and have the
word ``module`` before the hash for all specs that do not depend on
mpi, and will have the same information plus the MPI implementation
name and version for all packages that depend on mpi.

When specifying module names by projection for Lmod modules, we
recommend NOT including names of dependencies (e.g., MPI, compilers)
that are already in the LMod hierarchy.



.. note::
   TCL modules
     TCL modules also allow for explicit conflicts between modulefiles.

     .. code-block:: yaml

        modules:
          default:
            enable:
              - tcl
            tcl:
              projections:
                all: '{name}/{version}-{compiler.name}-{compiler.version}'
              all:
                conflict:
                  - '{name}'
                  - 'intel/14.0.1'

     will create module files that will conflict with ``intel/14.0.1`` and with the
     base directory of the same module, effectively preventing the possibility to
     load two or more versions of the same software at the same time. The tokens
     that are available for use in this directive are the same understood by
     the :meth:`~spack.spec.Spec.format` method.


.. note::
   LMod hierarchical module files
     When ``lmod`` is activated Spack will generate a set of hierarchical lua module
     files that are understood by LMod. The hierarchy will always contain the
     two layers ``Core`` / ``Compiler`` but can be further extended to
     any of the virtual dependencies present in Spack. A case that could be useful in
     practice is for instance:

     .. code-block:: yaml

       modules:
         default:
           enable:
             - lmod
           lmod:
             core_compilers:
               - 'gcc@4.8'
             core_specs:
               - 'python'
             hierarchy:
               - 'mpi'
               - 'lapack'

     that will generate a hierarchy in which the ``lapack`` and ``mpi`` layer can be switched
     independently. This allows a site to build the same libraries or applications against different
     implementations of ``mpi`` and ``lapack``, and let LMod switch safely from one to the
     other.

     All packages built with a compiler in ``core_compilers`` and all
     packages that satisfy a spec in ``core_specs`` will be put in the
     ``Core`` hierarchy of the lua modules.

.. warning::
  Consistency of Core packages
   The user is responsible for maintining consistency among core packages, as ``core_specs``
   bypasses the hierarchy that allows LMod to safely switch between coherent software stacks.

.. warning::
  Deep hierarchies and ``lmod spider``
   For hierarchies that are deeper than three layers ``lmod spider`` may have some issues.
   See `this discussion on the LMod project <https://github.com/TACC/Lmod/issues/114>`_.

""""""""""""""""""""""
Select default modules
""""""""""""""""""""""

By default, when multiple modules of the same name share a directory,
the highest version number will be the default module. This behavior
of the ``module`` command can be overridden with a symlink named
``default`` to the desired default module. If you wish to configure
default modules with Spack, add a ``defaults`` key to your modules
configuration:

.. code-block:: yaml

  modules:
    my-module-set:
      tcl:
        defaults:
        - gcc@10.2.1
        - hdf5@1.2.10+mpi+hl%gcc

These defaults may be arbitrarily specific. For any package that
satisfies a default, Spack will generate the module file in the
appropriate path, and will generate a default symlink to the module
file as well.

.. warning:: 
  If Spack is configured to generate multiple default packages in the
  same directory, the last modulefile to be generated will be the
  default module.

.. _customize-env-modifications:

"""""""""""""""""""""""""""""""""""
Customize environment modifications
"""""""""""""""""""""""""""""""""""

You can control which prefixes in a Spack package are added to
environment variables with the ``prefix_inspections`` section; this
section maps relative prefixes to the list of environment variables
which should be updated with those prefixes.

The ``prefix_inspections`` configuration is different from other
settings in that a ``prefix_inspections`` configuration at the
``modules`` level of the configuration file applies to all module
sets. This allows users to make general overrides to the default
inspections and customize them per-module-set.

.. code-block:: yaml

  modules:
    prefix_inspections:
      bin:
        - PATH
      lib:
        - LIBRARY_PATH
      '':
        - CMAKE_PREFIX_PATH

Prefix inspections are only applied if the relative path inside the
installation prefix exists. In this case, for a Spack package ``foo``
installed to ``/spack/prefix/foo``, if ``foo`` installs executables to
``bin`` but no libraries in ``lib``, the generated module file for
``foo`` would update ``PATH`` to contain ``/spack/prefix/foo/bin`` and
``CMAKE_PREFIX_PATH`` to contain ``/spack/prefix/foo``, but would not
update ``LIBRARY_PATH``.

There is a special case for prefix inspections relative to environment
views. If all of the following conditions hold for a module set
configuration:

#. The configuration is for an :ref:`environment <environments>` and
   will never be applied outside the environment,
#. The environment in question is configured to use a :ref:`view
   <filesystem-views>`,
#. The :ref:`environment view is configured
   <configuring_environment_views>` with a projection that ensures
   every package is linked to a unique directory,

then the module set may be configured to create modules relative to
the environment view. This is specified by the ``use_view``
configuration option in the module set. If ``True``, the module set is
constructed relative to the default view of the
environment. Otherwise, the value must be the name of the environment
view relative to which to construct modules, or ``False-ish`` to
disable the feature explicitly (the default is ``False``).

If the ``use_view`` value is set in the config, then the prefix
inspections for the package are done relative to the package's path in
the view.

.. code-block:: yaml

   spack:
     modules:
       view_relative_modules:
         use_view: my_view
       prefix_inspections:
         bin:
           - PATH
     view:
       my_view:
         projections:
           root: /path/to/my/view
           all:  '{name}-{hash}'

The ``spack`` key is relevant to :ref:`environment <environments>`
configuration, and the view key is discussed in detail in the section
on :ref:`Configuring environment views
<configuring_environment_views>`. With this configuration the
generated module for package ``foo`` would set ``PATH`` to include
``/path/to/my/view/foo-<hash>/bin`` instead of
``/spack/prefix/foo/bin``.

The ``use_view`` option is useful when deploying a large software
stack to users who are likely to inspect the modules to find full
paths to software, when it is desirable to present the users with a
simpler set of paths than those generated by the Spack install tree.

""""""""""""""""""""""""""""""""""""
Filter out environment modifications
""""""""""""""""""""""""""""""""""""

Modifications to certain environment variables in module files are there by
default, for instance because they are generated by prefix inspections.
If you want to prevent modifications to some environment variables, you can
do so by using the environment blacklist:

.. code-block:: yaml

   modules:
     default:
       tcl:
         all:
           filter:
             # Exclude changes to any of these variables
             environment_blacklist: ['CPATH', 'LIBRARY_PATH']

The configuration above will generate module files that will not contain
modifications to either ``CPATH`` or ``LIBRARY_PATH``.


.. _autoloading-dependencies:

"""""""""""""""""""""
Autoload dependencies
"""""""""""""""""""""

Often it is required for a module to have its (transient) dependencies loaded as well.
One example where this is useful is when one package needs to use executables provided
by its dependency; when the dependency is autoloaded, the executable will be in the
PATH. Similarly for scripting languages such as Python, packages and their dependencies
have to be loaded together.

Autoloading is enabled by default for LMod, as it has great builtin support for through
the ``depends_on`` function. For Environment Modules it is disabled by default.

Autoloading can also be enabled conditionally:

.. code-block:: yaml

    modules:
      default:
        tcl:
          all:
            autoload: none
          ^python:
            autoload: direct

The configuration file above will produce module files that will
load their direct dependencies if the package installed depends on ``python``.
The allowed values for the ``autoload`` statement are either ``none``,
``direct`` or ``all``.

.. note::
  TCL prerequisites
     In the ``tcl`` section of the configuration file it is possible to use
     the ``prerequisites`` directive that accepts the same values as
     ``autoload``. It will produce module files that have a ``prereq``
     statement, which can be used to autoload dependencies in some versions
     of Environment Modules.

------------------------
Maintaining Module Files
------------------------

Each type of module file has a command with the same name associated
with it. The actions these commands permit are usually associated
with the maintenance of a production environment. Here's, for instance,
a sample of the features of the ``spack module tcl`` command:

.. command-output:: spack module tcl --help

.. _cmd-spack-module-refresh:

^^^^^^^^^^^^^^^^^^^^^^^^^^
Refresh the set of modules
^^^^^^^^^^^^^^^^^^^^^^^^^^

The subcommand that regenerates module files to update their content or
their layout is ``refresh``:

.. command-output:: spack module tcl refresh --help

A set of packages can be selected using anonymous specs for the optional
``constraint`` positional argument. Optionally the entire tree can be deleted
before regeneration if the change in layout is radical.

.. _cmd-spack-module-rm:

^^^^^^^^^^^^^^^^^^^
Delete module files
^^^^^^^^^^^^^^^^^^^

If instead what you need is just to delete a few module files, then the right
subcommand is ``rm``:

.. command-output:: spack module tcl rm --help

.. note::
  We care about your module files!
   Every modification done on modules
   that are already existing will ask for a confirmation by default. If
   the command is used in a script it is possible though to pass the
   ``-y`` argument, that will skip this safety measure.


.. _modules-in-shell-scripts:

------------------------------------
Using Spack modules in shell scripts
------------------------------------

The easiest To enable additional Spack commands for loading and unloading
module files, and to add the correct path to ``MODULEPATH``, you need to
source the appropriate setup file. Assuming Spack is installed in
``$SPACK_ROOT``, run the appropriate command for your shell:

.. code-block:: console

   # For bash/zsh/sh
   $ . $SPACK_ROOT/share/spack/setup-env.sh

   # For tcsh/csh
   $ source $SPACK_ROOT/share/spack/setup-env.csh

   # For fish
   $ . $SPACK_ROOT/share/spack/setup-env.fish

If you want to have Spack's shell support available on the command line
at any login you can put this source line in one of the files that are
sourced at startup (like ``.profile``, ``.bashrc`` or ``.cshrc``). Be
aware that the shell startup time may increase slightly as a result.

.. _cmd-spack-module-loads:

^^^^^^^^^^^^^^^^^^^^^^^^^^
``spack module tcl loads``
^^^^^^^^^^^^^^^^^^^^^^^^^^

In some cases, it is desirable to use a Spack-generated module, rather
than relying on Spack's built-in user-environment modification
capabilities. To translate a spec into a module name, use ``spack
module tcl loads`` or ``spack module lmod loads`` depending on the
module system desired.


To load not just a module, but also all the modules it depends on, use
the ``--dependencies`` option. This is not required for most modules
because Spack builds binaries with RPATH support.  However, not all
packages use RPATH to find their dependencies: this can be true in
particular for Python extensions, which are currently *not* built with
RPATH.

Scripts to load modules recursively may be made with the command:

.. code-block:: console

    $ spack module tcl loads --dependencies <spec>

An equivalent alternative using `process substitution <http://tldp.org/LDP/abs/html/process-sub.html>`_ is:

.. code-block:: console

    $ source <( spack module tcl loads --dependencies <spec> )


^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Module Commands for Shell Scripts
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Although Spack is flexible, the ``module`` command is much faster.
This could become an issue when emitting a series of ``spack load``
commands inside a shell script.  By adding the ``--dependencies`` flag,
``spack module tcl loads`` may also be used to generate code that can be
cut-and-pasted into a shell script.  For example:

.. code-block:: console

    $ spack module tcl loads --dependencies py-numpy git
    # bzip2@1.0.6%gcc@4.9.3=linux-x86_64
    module load bzip2-1.0.6-gcc-4.9.3-ktnrhkrmbbtlvnagfatrarzjojmkvzsx
    # ncurses@6.0%gcc@4.9.3=linux-x86_64
    module load ncurses-6.0-gcc-4.9.3-kaazyneh3bjkfnalunchyqtygoe2mncv
    # zlib@1.2.8%gcc@4.9.3=linux-x86_64
    module load zlib-1.2.8-gcc-4.9.3-v3ufwaahjnviyvgjcelo36nywx2ufj7z
    # sqlite@3.8.5%gcc@4.9.3=linux-x86_64
    module load sqlite-3.8.5-gcc-4.9.3-a3eediswgd5f3rmto7g3szoew5nhehbr
    # readline@6.3%gcc@4.9.3=linux-x86_64
    module load readline-6.3-gcc-4.9.3-se6r3lsycrwxyhreg4lqirp6xixxejh3
    # python@3.5.1%gcc@4.9.3=linux-x86_64
    module load python-3.5.1-gcc-4.9.3-5q5rsrtjld4u6jiicuvtnx52m7tfhegi
    # py-setuptools@20.5%gcc@4.9.3=linux-x86_64
    module load py-setuptools-20.5-gcc-4.9.3-4qr2suj6p6glepnedmwhl4f62x64wxw2
    # py-nose@1.3.7%gcc@4.9.3=linux-x86_64
    module load py-nose-1.3.7-gcc-4.9.3-pwhtjw2dvdvfzjwuuztkzr7b4l6zepli
    # openblas@0.2.17%gcc@4.9.3+shared=linux-x86_64
    module load openblas-0.2.17-gcc-4.9.3-pw6rmlom7apfsnjtzfttyayzc7nx5e7y
    # py-numpy@1.11.0%gcc@4.9.3+blas+lapack=linux-x86_64
    module load py-numpy-1.11.0-gcc-4.9.3-mulodttw5pcyjufva4htsktwty4qd52r
    # curl@7.47.1%gcc@4.9.3=linux-x86_64
    module load curl-7.47.1-gcc-4.9.3-ohz3fwsepm3b462p5lnaquv7op7naqbi
    # autoconf@2.69%gcc@4.9.3=linux-x86_64
    module load autoconf-2.69-gcc-4.9.3-bkibjqhgqm5e3o423ogfv2y3o6h2uoq4
    # cmake@3.5.0%gcc@4.9.3~doc+ncurses+openssl~qt=linux-x86_64
    module load cmake-3.5.0-gcc-4.9.3-x7xnsklmgwla3ubfgzppamtbqk5rwn7t
    # expat@2.1.0%gcc@4.9.3=linux-x86_64
    module load expat-2.1.0-gcc-4.9.3-6pkz2ucnk2e62imwakejjvbv6egncppd
    # git@2.8.0-rc2%gcc@4.9.3+curl+expat=linux-x86_64
    module load git-2.8.0-rc2-gcc-4.9.3-3bib4hqtnv5xjjoq5ugt3inblt4xrgkd

The script may be further edited by removing unnecessary modules.


^^^^^^^^^^^^^^^
Module Prefixes
^^^^^^^^^^^^^^^

On some systems, modules are automatically prefixed with a certain
string; ``spack module tcl loads`` needs to know about that prefix when it
issues ``module load`` commands.  Add the ``--prefix`` option to your
``spack module tcl loads`` commands if this is necessary.

For example, consider the following on one system:

.. code-block:: console

    $ module avail
    linux-SuSE11-x86_64/antlr-2.7.7-gcc-5.3.0-bdpl46y

    $ spack module tcl loads antlr    # WRONG!
    # antlr@2.7.7%gcc@5.3.0~csharp+cxx~java~python arch=linux-SuSE11-x86_64
    module load antlr-2.7.7-gcc-5.3.0-bdpl46y

    $ spack module tcl loads --prefix linux-SuSE11-x86_64/ antlr
    # antlr@2.7.7%gcc@5.3.0~csharp+cxx~java~python arch=linux-SuSE11-x86_64
    module load linux-SuSE11-x86_64/antlr-2.7.7-gcc-5.3.0-bdpl46y
.. Copyright 2013-2022 Lawrence Livermore National Security, LLC and other
   Spack Project Developers. See the top-level COPYRIGHT file for details.

   SPDX-License-Identifier: (Apache-2.0 OR MIT)

.. Spack documentation master file, created by
   sphinx-quickstart on Mon Dec  9 15:32:41 2013.
   You can adapt this file completely to your liking, but it should at least
   contain the root `toctree` directive.

===================
Spack
===================

.. epigraph::

   `These are docs for the Spack package manager. For sphere packing, see` `pyspack <https://pyspack.readthedocs.io>`_.

Spack is a package management tool designed to support multiple
versions and configurations of software on a wide variety of platforms
and environments.  It was designed for large supercomputing centers,
where many users and application teams share common installations of
software on clusters with exotic architectures, using libraries that
do not have a standard ABI.  Spack is non-destructive: installing a
new version does not break existing installations, so many
configurations can coexist on the same system.

Most importantly, Spack is *simple*.  It offers a simple *spec* syntax
so that users can specify versions and configuration options
concisely.  Spack is also simple for package authors: package files
are written in pure Python, and specs allow package authors to
maintain a single file for many different builds of the same package.

See the :doc:`features` for examples and highlights.

Get spack from the `github repository
<https://github.com/spack/spack>`_ and install your first
package:

.. code-block:: console

   $ git clone -c feature.manyFiles=true https://github.com/spack/spack.git
   $ cd spack/bin
   $ ./spack install libelf

If you're new to spack and want to start using it, see :doc:`getting_started`,
or refer to the full manual below.


.. toctree::
   :maxdepth: 2
   :caption: Basics

   features
   getting_started
   basic_usage
   workflows
   Tutorial: Spack 101 <https://spack-tutorial.readthedocs.io>
   known_issues

.. toctree::
   :maxdepth: 2
   :caption: Reference

   configuration
   config_yaml
   build_settings
   environments
   containers
   monitoring
   mirrors
   module_file_support
   repositories
   binary_caches
   command_index
   package_list
   chain
   extensions
   pipelines

.. toctree::
   :maxdepth: 2
   :caption: Research

   analyze

.. toctree::
   :maxdepth: 2
   :caption: Contributing

   contribution_guide
   packaging_guide
   build_systems
   developer_guide

.. toctree::
   :maxdepth: 2
   :caption: API Docs

   Spack API Docs <spack>
   LLNL API Docs <llnl>

==================
Indices and tables
==================

* :ref:`genindex`
* :ref:`modindex`
* :ref:`search`
.. Copyright 2013-2022 Lawrence Livermore National Security, LLC and other
   Spack Project Developers. See the top-level COPYRIGHT file for details.

   SPDX-License-Identifier: (Apache-2.0 OR MIT)

=========
Workflows
=========

The process of using Spack involves building packages, running
binaries from those packages, and developing software that depends on
those packages.  For example, one might use Spack to build the
``netcdf`` package, use ``spack load`` to run the ``ncdump`` binary, and
finally, write a small C program to read/write a particular NetCDF file.

Spack supports a variety of workflows to suit a variety of situations
and user preferences, there is no single way to do all these things.
This chapter demonstrates different workflows that have been
developed, pointing out the pros and cons of them.

-----------
Definitions
-----------

First some basic definitions.

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Package, Concrete Spec, Installed Package
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

In Spack, a package is an abstract recipe to build one piece of software.
Spack packages may be used to build, in principle, any version of that
software with any set of variants.  Examples of packages include
``curl`` and ``zlib``.

A package may be *instantiated* to produce a concrete spec; one
possible realization of a particular package, out of combinatorially
many other realizations.  For example, here is a concrete spec
instantiated from ``curl``:

.. command-output:: spack spec curl

Spack's core concretization algorithm generates concrete specs by
instantiating packages from its repo, based on a set of "hints",
including user input and the ``packages.yaml`` file.  This algorithm
may be accessed at any time with the ``spack spec`` command.

Every time Spack installs a package, that installation corresponds to
a concrete spec.  Only a vanishingly small fraction of possible
concrete specs will be installed at any one Spack site.

^^^^^^^^^^^^^^^
Consistent Sets
^^^^^^^^^^^^^^^

A set of Spack specs is said to be *consistent* if each package is
only instantiated one way within it --- that is, if two specs in the
set have the same package, then they must also have the same version,
variant, compiler, etc.  For example, the following set is consistent:

.. code-block:: console

   curl@7.50.1%gcc@5.3.0 arch=linux-SuSE11-x86_64
       ^openssl@1.0.2k%gcc@5.3.0 arch=linux-SuSE11-x86_64
       ^zlib@1.2.8%gcc@5.3.0 arch=linux-SuSE11-x86_64
   zlib@1.2.8%gcc@5.3.0 arch=linux-SuSE11-x86_64

The following set is not consistent:

.. code-block:: console

   curl@7.50.1%gcc@5.3.0 arch=linux-SuSE11-x86_64
       ^openssl@1.0.2k%gcc@5.3.0 arch=linux-SuSE11-x86_64
       ^zlib@1.2.8%gcc@5.3.0 arch=linux-SuSE11-x86_64
   zlib@1.2.7%gcc@5.3.0 arch=linux-SuSE11-x86_64

The compatibility of a set of installed packages determines what may
be done with it.  It is always possible to ``spack load`` any set of
installed packages, whether or not they are consistent, and run their
binaries from the command line.  However, a set of installed packages
can only be linked together in one binary if it is consistent.

If the user produces a series of ``spack spec`` or ``spack load``
commands, in general there is no guarantee of consistency between
them.  Spack's concretization procedure guarantees that the results of
any *single* ``spack spec`` call will be consistent.  Therefore, the
best way to ensure a consistent set of specs is to create a Spack
package with dependencies, and then instantiate that package.  We will
use this technique below.

-----------------
Building Packages
-----------------

Suppose you are tasked with installing a set of software packages on a
system in order to support one application -- both a core application
program, plus software to prepare input and analyze output.  The
required software might be summed up as a series of ``spack install``
commands placed in a script.  If needed, this script can always be run
again in the future.  For example:

.. code-block:: sh

   #!/bin/sh
   spack install modele-utils
   spack install emacs
   spack install ncview
   spack install nco
   spack install modele-control
   spack install py-numpy

In most cases, this script will not correctly install software
according to your specific needs: choices need to be made for
variants, versions and virtual dependency choices may be needed.  It
*is* possible to specify these choices by extending specs on the
command line; however, the same choices must be specified repeatedly.
For example, if you wish to use ``openmpi`` to satisfy the ``mpi``
dependency, then ``^openmpi`` will have to appear on *every* ``spack
install`` line that uses MPI.  It can get repetitive fast.

Customizing Spack installation options is easier to do in the
``~/.spack/packages.yaml`` file.  In this file, you can specify
preferred versions and variants to use for packages.  For example:

.. code-block:: yaml

   packages:
       python:
           version: [3.5.1]
       modele-utils:
           version: [cmake]

       everytrace:
           version: [develop]
       eigen:
           variants: ~suitesparse
       netcdf:
           variants: +mpi

       all:
           compiler: [gcc@5.3.0]
           providers:
               mpi: [openmpi]
               blas: [openblas]
               lapack: [openblas]


This approach will work as long as you are building packages for just
one application.

^^^^^^^^^^^^^^^^^^^^^
Multiple Applications
^^^^^^^^^^^^^^^^^^^^^

Suppose instead you're building multiple inconsistent applications.
For example, users want package A to be built with ``openmpi`` and
package B with ``mpich`` --- but still share many other lower-level
dependencies.  In this case, a single ``packages.yaml`` file will not
work.  Plans are to implement *per-project* ``packages.yaml`` files.
In the meantime, one could write shell scripts to switch
``packages.yaml`` between multiple versions as needed, using symlinks.

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Combinatorial Sets of Installs
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Suppose that you are now tasked with systematically building many
incompatible versions of packages.  For example, you need to build
``petsc`` 9 times for 3 different MPI implementations on 3 different
compilers, in order to support user needs.  In this case, you will
need to either create 9 different ``packages.yaml`` files; or more
likely, create 9 different ``spack install`` command lines with the
correct options in the spec.  Here is a real-life example of this kind
of usage:

.. code-block:: sh

   #!/bin/bash

   compilers=(
       %gcc
       %intel
       %pgi
   )

   mpis=(
       openmpi+psm~verbs
       openmpi~psm+verbs
       mvapich2+psm~mrail
       mvapich2~psm+mrail
       mpich+verbs
   )

   for compiler in "${compilers[@]}"
   do
       # Serial installs
       spack install szip           $compiler
       spack install hdf            $compiler
       spack install hdf5           $compiler
       spack install netcdf         $compiler
       spack install netcdf-fortran $compiler
       spack install ncview         $compiler

       # Parallel installs
       for mpi in "${mpis[@]}"
       do
           spack install $mpi            $compiler
           spack install hdf5~cxx+mpi    $compiler ^$mpi
           spack install parallel-netcdf $compiler ^$mpi
       done
   done

------------------------------
Running Binaries from Packages
------------------------------

Once Spack packages have been built, the next step is to use them.  As
with building packages, there are many ways to use them, depending on
the use case.

^^^^^^^^^^^^
Find and Run
^^^^^^^^^^^^

The simplest way to run a Spack binary is to find it and run it!
In many cases, nothing more is needed because Spack builds binaries
with RPATHs.  Spack installation directories may be found with ``spack
location --install-dir`` commands.  For example:

.. code-block:: console

   $ spack location --install-dir cmake
   ~/spack/opt/spack/linux-SuSE11-x86_64/gcc-5.3.0/cmake-3.6.0-7cxrynb6esss6jognj23ak55fgxkwtx7

This gives the root of the Spack package; relevant binaries may be
found within it.  For example:

.. code-block:: console

   $ CMAKE=`spack location --install-dir cmake`/bin/cmake


Standard UNIX tools can find binaries as well.  For example:

.. code-block:: console

   $ find ~/spack/opt -name cmake | grep bin
   ~/spack/opt/spack/linux-SuSE11-x86_64/gcc-5.3.0/cmake-3.6.0-7cxrynb6esss6jognj23ak55fgxkwtx7/bin/cmake

These methods are suitable, for example, for setting up build
processes or GUIs that need to know the location of particular tools.
However, other more powerful methods are generally preferred for user
environments.


^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Using ``spack load`` to Manage the User Environment
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Suppose that Spack has been used to install a set of command-line
programs, which users now wish to use.  One can in principle put a
number of ``spack load`` commands into ``.bashrc``, for example, to
load a set of Spack packages:

.. code-block:: sh

   spack load modele-utils
   spack load emacs
   spack load ncview
   spack load nco
   spack load modele-control

Although simple load scripts like this are useful in many cases, they
have some drawbacks:

1. The set of packages loaded by them will in general not be
   consistent.  They are a decent way to load commands to be called
   from command shells.  See below for better ways to assemble a
   consistent set of packages for building application programs.

2. The ``spack spec`` and ``spack install`` commands use a
   sophisticated concretization algorithm that chooses the "best"
   among several options, taking into account ``packages.yaml`` file.
   The ``spack load`` and ``spack module tcl loads`` commands, on the
   other hand, are not very smart: if the user-supplied spec matches
   more than one installed package, then ``spack module tcl loads`` will
   fail. This default behavior may change in the future.  For now,
   the workaround is to either be more specific on any failing ``spack load``
   commands or to use ``spack load --first`` to allow spack to load the
   first matching spec.


""""""""""""""""""""""
Generated Load Scripts
""""""""""""""""""""""

Another problem with using `spack load` is, it can be slow; a typical
user environment could take several seconds to load, and would not be
appropriate to put into ``.bashrc`` directly.  This is because it
requires the full start-up overhead of python/Spack for each command.
In some circumstances it is preferable to use a series of ``spack
module tcl loads`` (or ``spack module lmod loads``) commands to
pre-compute which modules to load.  This will generate the modulenames
to load the packages using environment modules, rather than Spack's
built-in support for environment modifications. These can be put in a
script that is run whenever installed Spack packages change.  For
example:

.. code-block:: sh

   #!/bin/sh
   #
   # Generate module load commands in ~/env/spackenv

   cat <<EOF | /bin/sh >$HOME/env/spackenv
   FIND='spack module tcl loads --prefix linux-SuSE11-x86_64/'

   \$FIND modele-utils
   \$FIND emacs
   \$FIND ncview
   \$FIND nco
   \$FIND modele-control
   EOF

The output of this file is written in ``~/env/spackenv``:

.. code-block:: sh

   # binutils@2.25%gcc@5.3.0+gold~krellpatch~libiberty arch=linux-SuSE11-x86_64
   module load linux-SuSE11-x86_64/binutils-2.25-gcc-5.3.0-6w5d2t4
   # python@2.7.12%gcc@5.3.0~tk~ucs4 arch=linux-SuSE11-x86_64
   module load linux-SuSE11-x86_64/python-2.7.12-gcc-5.3.0-2azoju2
   # ncview@2.1.7%gcc@5.3.0 arch=linux-SuSE11-x86_64
   module load linux-SuSE11-x86_64/ncview-2.1.7-gcc-5.3.0-uw3knq2
   # nco@4.5.5%gcc@5.3.0 arch=linux-SuSE11-x86_64
   module load linux-SuSE11-x86_64/nco-4.5.5-gcc-5.3.0-7aqmimu
   # modele-control@develop%gcc@5.3.0 arch=linux-SuSE11-x86_64
   module load linux-SuSE11-x86_64/modele-control-develop-gcc-5.3.0-7rddsij
   # zlib@1.2.8%gcc@5.3.0 arch=linux-SuSE11-x86_64
   module load linux-SuSE11-x86_64/zlib-1.2.8-gcc-5.3.0-fe5onbi
   # curl@7.50.1%gcc@5.3.0 arch=linux-SuSE11-x86_64
   module load linux-SuSE11-x86_64/curl-7.50.1-gcc-5.3.0-4vlev55
   # hdf5@1.10.0-patch1%gcc@5.3.0+cxx~debug+fortran+mpi+shared~szip~threadsafe arch=linux-SuSE11-x86_64
   module load linux-SuSE11-x86_64/hdf5-1.10.0-patch1-gcc-5.3.0-pwnsr4w
   # netcdf@4.4.1%gcc@5.3.0~hdf4+mpi arch=linux-SuSE11-x86_64
   module load linux-SuSE11-x86_64/netcdf-4.4.1-gcc-5.3.0-rl5canv
   # netcdf-fortran@4.4.4%gcc@5.3.0 arch=linux-SuSE11-x86_64
   module load linux-SuSE11-x86_64/netcdf-fortran-4.4.4-gcc-5.3.0-stdk2xq
   # modele-utils@cmake%gcc@5.3.0+aux+diags+ic arch=linux-SuSE11-x86_64
   module load linux-SuSE11-x86_64/modele-utils-cmake-gcc-5.3.0-idyjul5
   # everytrace@develop%gcc@5.3.0+fortran+mpi arch=linux-SuSE11-x86_64
   module load linux-SuSE11-x86_64/everytrace-develop-gcc-5.3.0-p5wmb25

Users may now put ``source ~/env/spackenv`` into ``.bashrc``.

.. note ::

   Some module systems put a prefix on the names of modules created
   by Spack.  For example, that prefix is ``linux-SuSE11-x86_64/`` in
   the above case.  If a prefix is not needed, you may omit the
   ``--prefix`` flag from ``spack module tcl loads``.


"""""""""""""""""""""""
Transitive Dependencies
"""""""""""""""""""""""

In the script above, each ``spack module tcl loads`` command generates a
*single* ``module load`` line.  Transitive dependencies do not usually
need to be loaded, only modules the user needs in ``$PATH``.  This is
because Spack builds binaries with RPATH.  Spack's RPATH policy has
some nice features:

#. Modules for multiple inconsistent applications may be loaded
   simultaneously.  In the above example (Multiple Applications),
   package A and package B can coexist together in the user's $PATH,
   even though they use different MPIs.

#. RPATH eliminates a whole class of strange errors that can happen
   in non-RPATH binaries when the wrong ``LD_LIBRARY_PATH`` is
   loaded.

#. Recursive module systems such as LMod are not necessary.

#. Modules are not needed at all to execute binaries.  If a path to a
   binary is known, it may be executed.  For example, the path for a
   Spack-built compiler can be given to an IDE without requiring the
   IDE to load that compiler's module.

Unfortunately, Spack's RPATH support does not work in every case.  For example:

#. Software comes in many forms --- not just compiled ELF binaries,
   but also as interpreted code in Python, R, JVM bytecode, etc.
   Those systems almost universally use an environment variable
   analogous to ``LD_LIBRARY_PATH`` to dynamically load libraries.

#. Although Spack generally builds binaries with RPATH, it does not
   currently do so for compiled Python extensions (for example,
   ``py-numpy``).  Any libraries that these extensions depend on
   (``blas`` in this case, for example) must be specified in the
   ``LD_LIBRARY_PATH``.`

#. In some cases, Spack-generated binaries end up without a
   functional RPATH for no discernible reason.

In cases where RPATH support doesn't make things "just work," it can
be necessary to load a module's dependencies as well as the module
itself.  This is done by adding the ``--dependencies`` flag to the
``spack module tcl loads`` command.  For example, the following line,
added to the script above, would be used to load SciPy, along with
Numpy, core Python, BLAS/LAPACK and anything else needed:

.. code-block:: sh

   spack module tcl loads --dependencies py-scipy

^^^^^^^^^^^^^^
Dummy Packages
^^^^^^^^^^^^^^

As an alternative to a series of ``module load`` commands, one might
consider dummy packages as a way to create a *consistent* set of
packages that may be loaded as one unit.  The idea here is pretty
simple:

#. Create a package (say, ``mydummy``) with no URL and no
   ``install()`` method, just dependencies.

#. Run ``spack install mydummy`` to install.

An advantage of this method is the set of packages produced will be
consistent.  This means that you can reliably build software against
it.  A disadvantage is the set of packages will be consistent; this
means you cannot load up two applications this way if they are not
consistent with each other.

.. _filesystem-views:

^^^^^^^^^^^^^^^^
Filesystem Views
^^^^^^^^^^^^^^^^

Filesystem views offer an alternative to environment modules, another
way to assemble packages in a useful way and load them into a user's
environment.

A single-prefix filesystem view is a single directory tree that is the
union of the directory hierarchies of a number of installed packages;
it is similar to the directory hierarchy that might exist under
``/usr/local``.  The files of the view's installed packages are
brought into the view by symbolic or hard links, referencing the
original Spack installation.

A combinatorial filesystem view can contain more software than a
single-prefix view. Combinatorial filesystem views are created by
defining a projection for each spec or set of specs. The syntax for
this will be discussed in the section for the ``spack view`` command
under `adding_projections_to_views`_.

The projection for a spec or set of specs specifies the naming scheme
for the directory structure under the root of the view into which the
package will be linked. For example, the spec ``zlib@1.2.8%gcc@4.4.7``
could be projected to ``MYVIEW/zlib-1.2.8-gcc``.

When software is built and installed, absolute paths are frequently
"baked into" the software, making it non-relocatable.  This happens
not just in RPATHs, but also in shebangs, configuration files, and
assorted other locations.

Therefore, programs run out of a Spack view will typically still look
in the original Spack-installed location for shared libraries and
other resources.  This behavior is not easily changed; in general,
there is no way to know where absolute paths might be written into an
installed package, and how to relocate it.  Therefore, the original
Spack tree must be kept in place for a filesystem view to work, even
if the view is built with hardlinks.

.. FIXME: reference the relocation work of Hegner and Gartung (PR #1013)

.. _cmd-spack-view:

""""""""""""""
``spack view``
""""""""""""""

A filesystem view is created, and packages are linked in, by the ``spack
view`` command's ``symlink`` and ``hardlink`` sub-commands.  The
``spack view remove`` command can be used to unlink some or all of the
filesystem view.

The following example creates a filesystem view based
on an installed ``cmake`` package and then removes from the view the
files in the ``cmake`` package while retaining its dependencies.

.. code-block:: console

   $ spack view --verbose symlink myview cmake@3.5.2
   ==> Linking package: "ncurses"
   ==> Linking package: "zlib"
   ==> Linking package: "openssl"
   ==> Linking package: "cmake"

   $ ls myview/
   bin  doc  etc  include  lib  share

   $ ls myview/bin/
   captoinfo  clear  cpack     ctest    infotocap        openssl  tabs  toe   tset
   ccmake     cmake  c_rehash  infocmp  ncurses6-config  reset    tic   tput

   $ spack view --verbose --dependencies false rm myview cmake@3.5.2
   ==> Removing package: "cmake"

   $ ls myview/bin/
   captoinfo  c_rehash  infotocap        openssl  tabs  toe   tset
   clear      infocmp   ncurses6-config  reset    tic   tput

.. note::

    If the set of packages being included in a view is inconsistent,
    then it is possible that two packages will provide the same file.  Any
    conflicts of this type are handled on a first-come-first-served basis,
    and a warning is printed.

.. note::

    When packages are removed from a view, empty directories are
    purged.

.. _adding_projections_to_views:

""""""""""""""""""""""""""""
Controlling View Projections
""""""""""""""""""""""""""""

The default projection into a view is to link every package into the
root of the view. This can be changed by adding a ``projections.yaml``
configuration file to the view. The projection configuration file for
a view located at ``/my/view`` is stored in
``/my/view/.spack/projections.yaml``.

When creating a view, the projection configuration file can also be
specified from the command line using the ``--projection-file`` option
to the ``spack view`` command.

The projections configuration file is a mapping of partial specs to
spec format strings, defined by the :meth:`~spack.spec.Spec.format`
function, as shown in the example below.

.. code-block:: yaml

   projections:
     zlib: {name}-{version}
     ^mpi: {name}-{version}/{^mpi.name}-{^mpi.version}-{compiler.name}-{compiler.version}
     all: {name}-{version}/{compiler.name}-{compiler.version}

The entries in the projections configuration file must all be either
specs or the keyword ``all``. For each spec, the projection used will
be the first non-``all`` entry that the spec satisfies, or ``all`` if
there is an entry for ``all`` and no other entry is satisfied by the
spec. Where the keyword ``all`` appears in the file does not
matter. Given the example above, any spec satisfying ``zlib@1.2.8``
will be linked into ``/my/view/zlib-1.2.8/``, any spec satisfying
``hdf5@1.8.10+mpi %gcc@4.9.3 ^mvapich2@2.2`` will be linked into
``/my/view/hdf5-1.8.10/mvapich2-2.2-gcc-4.9.3``, and any spec
satisfying ``hdf5@1.8.10~mpi %gcc@4.9.3`` will be linked into
``/my/view/hdf5-1.8.10/gcc-4.9.3``.

If the keyword ``all`` does not appear in the projections
configuration file, any spec that does not satisfy any entry in the
file will be linked into the root of the view as in a single-prefix
view. Any entries that appear below the keyword ``all`` in the
projections configuration file will not be used, as all specs will use
the projection under ``all`` before reaching those entries.

""""""""""""""""""
Fine-Grain Control
""""""""""""""""""

The ``--exclude`` and ``--dependencies`` option flags allow for
fine-grained control over which packages and dependencies do or not
get included in a view.  For example, suppose you are developing the
``appsy`` package.  You wish to build against a view of all ``appsy``
dependencies, but not ``appsy`` itself:

.. code-block:: console

   $ spack view --dependencies yes --exclude appsy symlink /path/to/MYVIEW/ appsy

Alternately, you wish to create a view whose purpose is to provide
binary executables to end users.  You only need to include
applications they might want, and not those applications'
dependencies.  In this case, you might use:

.. code-block:: console

   $ spack view --dependencies no symlink /path/to/MYVIEW/ cmake


"""""""""""""""""""""""
Hybrid Filesystem Views
"""""""""""""""""""""""

Although filesystem views are usually created by Spack, users are free
to add to them by other means.  For example, imagine a filesystem
view, created by Spack, that looks something like:

.. code-block:: console

   /path/to/MYVIEW/bin/programA -> /path/to/spack/.../bin/programA
   /path/to/MYVIEW/lib/libA.so -> /path/to/spack/.../lib/libA.so

Now, the user may add to this view by non-Spack means; for example, by
running a classic install script.  For example:

.. code-block:: console

   $ tar -xf B.tar.gz
   $ cd B/
   $ ./configure --prefix=/path/to/MYVIEW \
               --with-A=/path/to/MYVIEW
   $ make && make install

The result is a hybrid view:

.. code-block:: console

   /path/to/MYVIEW/bin/programA -> /path/to/spack/.../bin/programA
   /path/to/MYVIEW/bin/programB
   /path/to/MYVIEW/lib/libA.so -> /path/to/spack/.../lib/libA.so
   /path/to/MYVIEW/lib/libB.so

In this case, real files coexist, interleaved with the "view"
symlinks.  At any time one can delete ``/path/to/MYVIEW`` or use
``spack view`` to manage it surgically.  None of this will affect the
real Spack install area.

^^^^^^^^^^^^^^^^^^
Global Activations
^^^^^^^^^^^^^^^^^^

:ref:`cmd-spack-activate` may be used as an alternative to loading
Python (and similar systems) packages directly or creating a view.
If extensions are globally activated, then ``spack load python`` will
also load all the extensions activated for the given ``python``.
This reduces the need for users to load a large number of packages.

However, Spack global activations have two potential drawbacks:

#. Activated packages that involve compiled C extensions may still
   need their dependencies to be loaded manually.  For example,
   ``spack load openblas`` might be required to make ``py-numpy``
   work.

#. Global activations "break" a core feature of Spack, which is that
   multiple versions of a package can co-exist side-by-side.  For example,
   suppose you wish to run a Python package in two different
   environments but the same basic Python --- one with
   ``py-numpy@1.7`` and one with ``py-numpy@1.8``.  Spack extensions
   will not support this potential debugging use case.

^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Discussion: Running Binaries
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Modules, extension packages and filesystem views are all ways to
assemble sets of Spack packages into a useful environment.  They are
all semantically similar, in that conflicting installed packages
cannot simultaneously be loaded, activated or included in a view.

With all of these approaches, there is no guarantee that the
environment created will be consistent.  It is possible, for example,
to simultaneously load application A that uses OpenMPI and application
B that uses MPICH.  Both applications will run just fine in this
inconsistent environment because they rely on RPATHs, not the
environment, to find their dependencies.

In general, environments set up using modules vs. views will work
similarly.  Both can be used to set up ephemeral or long-lived
testing/development environments.  Operational differences between the
two approaches can make one or the other preferable in certain
environments:

* Filesystem views do not require environment module infrastructure.
  Although Spack can install ``environment-modules``, users might be
  hostile to its use.  Filesystem views offer a good solution for
  sysadmins serving users who just "want all the stuff I need in one
  place" and don't want to hear about Spack.

* Although modern build systems will find dependencies wherever they
  might be, some applications with hand-built make files expect their
  dependencies to be in one place.  One common problem is makefiles
  that assume that ``netcdf`` and ``netcdf-fortran`` are installed in
  the same tree.  Or, one might use an IDE that requires tedious
  configuration of dependency paths; and it's easier to automate that
  administration in a view-building script than in the IDE itself.
  For all these cases, a view will be preferable to other ways to
  assemble an environment.

* On systems with I-node quotas, modules might be preferable to views
  and extension packages.

* Views and activated extensions maintain state that is semantically
  equivalent to the information in a ``spack module tcl loads`` script.
  Administrators might find things easier to maintain without the
  added "heavyweight" state of a view.

-------------------------------------
Using Spack to Replace Homebrew/Conda
-------------------------------------

Spack is an incredibly powerful package manager, designed for supercomputers
where users have diverse installation needs. But Spack can also be used to
handle simple single-user installations on your laptop. Most macOS users are
already familiar with package managers like Homebrew and Conda, where all
installed packages are symlinked to a single central location like ``/usr/local``.
In this section, we will show you how to emulate the behavior of Homebrew/Conda
using :ref:`environments`!

^^^^^
Setup
^^^^^

First, let's create a new environment. We'll assume that Spack is already set up
correctly, and that you've already sourced the setup script for your shell.
To create a new environment, simply run:

.. code-block:: console

   $ spack env create myenv
   ==> Updating view at /Users/me/spack/var/spack/environments/myenv/.spack-env/view
   ==> Created environment 'myenv' in /Users/me/spack/var/spack/environments/myenv
   $ spack env activate myenv

Here, *myenv* can be anything you want to name your environment. Next, we can add
a list of packages we would like to install into our environment. Let's say we
want a newer version of Bash than the one that comes with macOS, and we want a
few Python libraries. We can run:

.. code-block:: console

   $ spack add bash
   ==> Adding bash to environment myenv
   ==> Updating view at /Users/me/spack/var/spack/environments/myenv/.spack-env/view
   $ spack add python@3:
   ==> Adding python@3: to environment myenv
   ==> Updating view at /Users/me/spack/var/spack/environments/myenv/.spack-env/view
   $ spack add py-numpy py-scipy py-matplotlib
   ==> Adding py-numpy to environment myenv
   ==> Adding py-scipy to environment myenv
   ==> Adding py-matplotlib to environment myenv
   ==> Updating view at /Users/me/spack/var/spack/environments/myenv/.spack-env/view

Each package can be listed on a separate line, or combined into a single line.
Notice that we're explicitly asking for Python 3 here. You can use any spec
you would normally use on the command line with other Spack commands.

Next, we want to manually configure a couple of things. In the ``myenv``
directory, we can find the ``spack.yaml`` that actually defines our environment.

.. code-block:: console

   $ vim ~/spack/var/spack/environments/myenv/spack.yaml

.. code-block:: yaml

   # This is a Spack Environment file.
   #
   # It describes a set of packages to be installed, along with
   # configuration settings.
   spack:
     # add package specs to the `specs` list
     specs: [bash, 'python@3:', py-numpy, py-scipy, py-matplotlib]
     view:
       default:
         root: /Users/me/spack/var/spack/environments/myenv/.spack-env/view
         projections: {}
     config: {}
     mirrors: {}
     modules:
       enable: []
     packages: {}
     repos: []
     upstreams: {}
     definitions: []
     concretization: separately

You can see the packages we added earlier in the ``specs:`` section. If you
ever want to add more packages, you can either use ``spack add`` or manually
edit this file.

We also need to change the ``concretization:`` option. By default, Spack
concretizes each spec *separately*, allowing multiple versions of the same
package to coexist. Since we want a single consistent environment, we want to
concretize all of the specs *together*.

Here is what your ``spack.yaml`` looks like with these new settings, and with
some of the sections we don't plan on using removed:

.. code-block:: diff

   spack:
   -  specs: [bash, 'python@3:', py-numpy, py-scipy, py-matplotlib]
   +  specs:
   +  - bash
   +  - 'python@3:'
   +  - py-numpy
   +  - py-scipy
   +  - py-matplotlib
   -  view:
   -    default:
   -      root: /Users/me/spack/var/spack/environments/myenv/.spack-env/view
   -      projections: {}
   +  view: /Users/me/spack/var/spack/environments/myenv/.spack-env/view
   -  config: {}
   -  mirrors: {}
   -  modules:
   -    enable: []
   -  packages: {}
   -  repos: []
   -  upstreams: {}
   -  definitions: []
   +  concretization: together
   -  concretization: separately

""""""""""""""""
Symlink location
""""""""""""""""

In the ``spack.yaml`` file above, you'll notice that by default, Spack symlinks
all installations to ``/Users/me/spack/var/spack/environments/myenv/.spack-env/view``.
You can actually change this to any directory you want. For example, Homebrew
uses ``/usr/local``, while Conda uses ``/Users/me/anaconda``. In order to access
files in these locations, you need to update ``PATH`` and other environment variables
to point to them. Activating the Spack environment does this automatically, but
you can also manually set them in your ``.bashrc``.

.. warning::

   There are several reasons why you shouldn't use ``/usr/local``:

   1. If you are on macOS 10.11+ (El Capitan and newer), Apple makes it hard
      for you. You may notice permissions issues on ``/usr/local`` due to their
      `System Integrity Protection <https://support.apple.com/en-us/HT204899>`_.
      By default, users don't have permissions to install anything in ``/usr/local``,
      and you can't even change this using ``sudo chown`` or ``sudo chmod``.
   2. Other package managers like Homebrew will try to install things to the
      same directory. If you plan on using Homebrew in conjunction with Spack,
      don't symlink things to ``/usr/local``.
   3. If you are on a shared workstation, or don't have sudo privileges, you
      can't do this.

   If you still want to do this anyway, there are several ways around SIP.
   You could disable SIP by booting into recovery mode and running
   ``csrutil disable``, but this is not recommended, as it can open up your OS
   to security vulnerabilities. Another technique is to run ``spack concretize``
   and ``spack install`` using ``sudo``. This is also not recommended.

   The safest way I've found is to create your installation directories using
   sudo, then change ownership back to the user like so:

   .. code-block:: bash

      for directory in .spack bin contrib include lib man share
      do
          sudo mkdir -p /usr/local/$directory
          sudo chown $(id -un):$(id -gn) /usr/local/$directory
      done

   Depending on the packages you install in your environment, the exact list of
   directories you need to create may vary. You may also find some packages
   like Java libraries that install a single file to the installation prefix
   instead of in a subdirectory. In this case, the action is the same, just replace
   ``mkdir -p`` with ``touch`` in the for-loop above.

   But again, it's safer just to use the default symlink location.


^^^^^^^^^^^^
Installation
^^^^^^^^^^^^

To actually concretize the environment, run:

.. code-block:: console

   $ spack concretize

This will tell you which if any packages are already installed, and alert you
to any conflicting specs.

To actually install these packages and symlink them to your ``view:``
directory, simply run:

.. code-block:: console

   $ spack install

Now, when you type ``which python3``, it should find the one you just installed.

In order to change the default shell to our newer Bash installation, we first
need to add it to this list of acceptable shells. Run:

.. code-block:: console

   $ sudo vim /etc/shells

and add the absolute path to your bash executable. Then run:

.. code-block:: console

   $ chsh -s /path/to/bash

Now, when you log out and log back in, ``echo $SHELL`` should point to the
newer version of Bash.

^^^^^^^^^^^^^^^^^^^^^^^^^^^
Updating Installed Packages
^^^^^^^^^^^^^^^^^^^^^^^^^^^

Let's say you upgraded to a new version of macOS, or a new version of Python
was released, and you want to rebuild your entire software stack. To do this,
simply run the following commands:

.. code-block:: console

   $ spack env activate myenv
   $ spack concretize --force
   $ spack install

The ``--force`` flag tells Spack to overwrite its previous concretization
decisions, allowing you to choose a new version of Python. If any of the new
packages like Bash are already installed, ``spack install`` won't re-install
them, it will keep the symlinks in place.

^^^^^^^^^^^^^^
Uninstallation
^^^^^^^^^^^^^^

If you decide that Spack isn't right for you, uninstallation is simple.
Just run:

.. code-block:: console

   $ spack env activate myenv
   $ spack uninstall --all

This will uninstall all packages in your environment and remove the symlinks.

------------------------
Using Spack on Travis-CI
------------------------

Spack can be deployed as a provider for userland software in
`Travis-CI <https://http://travis-ci.org>`_.

A starting-point for a ``.travis.yml`` file can look as follows.
It uses `caching <https://docs.travis-ci.com/user/caching/>`_ for
already built environments, so make sure to clean the Travis cache if
you run into problems.

The main points that are implemented below:

#. Travis is detected as having up to 34 cores available, but only 2
   are actually allocated for the user. We limit the parallelism of
   the spack builds in the config.
   (The Travis yaml parser is a bit buggy on the echo command.)

#. Without control for the user, Travis jobs will run on various
   ``x86_64`` microarchitectures. If you plan to cache build results,
   e.g. to accelerate dependency builds, consider building for the
   generic ``x86_64`` target only.
   Limiting the microarchitecture will also find more packages when
   working with the
   `E4S Spack build cache <https://oaciss.uoregon.edu/e4s/e4s_buildcache_inventory.html>`_.

#. Builds over 10 minutes need to be prefixed with ``travis_wait``.
   Alternatively, generate output once with ``spack install -v``.

#. Travis builds are non-interactive. This prevents using bash
   aliases and functions for modules. We fix that by sourcing
   ``/etc/profile`` first (or running everything in a subshell with
   ``bash -l -c '...'``).

.. code-block:: yaml

   language: cpp
   sudo: false
   dist: trusty

   cache:
     apt: true
     directories:
       - $HOME/.cache

   addons:
     apt:
       sources:
         - ubuntu-toolchain-r-test
       packages:
         - g++-4.9
         - environment-modules

   env:
     global:
       - SPACK_ROOT: $HOME/.cache/spack
       - PATH: $PATH:$HOME/.cache/spack/bin

   before_install:
     - export CXX=g++-4.9
     - export CC=gcc-4.9
     - export FC=gfortran-4.9
     - export CXXFLAGS="-std=c++11"

   install:
     - |
       if ! which spack >/dev/null; then
         mkdir -p $SPACK_ROOT &&
         git clone --depth 50 https://github.com/spack/spack.git $SPACK_ROOT &&
         printf "config:\n  build_jobs: 2\n" > $SPACK_ROOT/etc/spack/config.yaml &&
         printf "packages:\n  all:\n    target: ['x86_64']\n" \
                 > $SPACK_ROOT/etc/spack/packages.yaml;
       fi
     - travis_wait spack install cmake@3.7.2~openssl~ncurses
     - travis_wait spack install boost@1.62.0~graph~iostream~locale~log~wave
     - spack clean -a
     - source /etc/profile &&
       source $SPACK_ROOT/share/spack/setup-env.sh
     - spack load cmake
     - spack load boost

   script:
     - mkdir -p $HOME/build
     - cd $HOME/build
     - cmake $TRAVIS_BUILD_DIR
     - make -j 2
     - make test

------------------
Upstream Bug Fixes
------------------

It is not uncommon to discover a bug in an upstream project while
trying to build with Spack.  Typically, the bug is in a package that
serves a dependency to something else.  This section describes
procedure to work around and ultimately resolve these bugs, while not
delaying the Spack user's main goal.

^^^^^^^^^^^^^^^^^
Buggy New Version
^^^^^^^^^^^^^^^^^

Sometimes, the old version of a package works fine, but a new version
is buggy.  For example, it was once found that `Adios did not build
with hdf5@1.10 <https://github.com/spack/spack/issues/1683>`_.  If the
old version of ``hdf5`` will work with ``adios``, the suggested
procedure is:

#. Revert ``adios`` to the old version of ``hdf5``.  Put in its
   ``adios/package.py``:

   .. code-block:: python

      # Adios does not build with HDF5 1.10
      # See: https://github.com/spack/spack/issues/1683
      depends_on('hdf5@:1.9')

#. Determine whether the problem is with ``hdf5`` or ``adios``, and
   report the problem to the appropriate upstream project.  In this
   case, the problem was with ``adios``.

#. Once a new version of ``adios`` comes out with the bugfix, modify
   ``adios/package.py`` to reflect it:

   .. code-block:: python

      # Adios up to v1.10.0 does not build with HDF5 1.10
      # See: https://github.com/spack/spack/issues/1683
      depends_on('hdf5@:1.9', when='@:1.10.0')
      depends_on('hdf5', when='@1.10.1:')

^^^^^^^^^^^^^^^^
No Version Works
^^^^^^^^^^^^^^^^

Sometimes, *no* existing versions of a dependency work for a build.
This typically happens when developing a new project: only then does
the developer notice that existing versions of a dependency are all
buggy, or the non-buggy versions are all missing a critical feature.

In the long run, the upstream project will hopefully fix the bug and
release a new version.  But that could take a while, even if a bugfix
has already been pushed to the project's repository.  In the meantime,
the Spack user needs things to work.

The solution is to create an unofficial Spack release of the project,
as soon as the bug is fixed in *some* repository.  A study of the `Git
history <https://github.com/citibeth/spack/commits/efischer/develop/var/spack/repos/builtin/packages/py-proj/package.py>`_
of ``py-proj/package.py`` is instructive here:

#. On `April 1 <https://github.com/citibeth/spack/commit/44a1d6a96706affe6ef0a11c3a780b91d21d105a>`_, an initial bugfix was identified for the PyProj project
   and a pull request submitted to PyProj.  Because the upstream
   authors had not yet fixed the bug, the ``py-proj`` Spack package
   downloads from a forked repository, set up by the package's author.
   A non-numeric version number is used to make it easy to upgrade the
   package without recomputing checksums; however, this is an
   untrusted download method and should not be distributed.  The
   package author has now become, temporarily, a maintainer of the
   upstream project:

   .. code-block:: python

      # We need the benefits of this PR
      # https://github.com/jswhit/pyproj/pull/54
      version('citibeth-latlong2',
          git='https://github.com/citibeth/pyproj.git',
          branch='latlong2')


#. By May 14, the upstream project had accepted a pull request with
   the required bugfix.  At this point, the forked repository was
   deleted.  However, the upstream project still had not released a
   new version with a bugfix.  Therefore, a Spack-only release was
   created by specifying the desired hash in the main project
   repository.  The version number ``@1.9.5.1.1`` was chosen for this
   "release" because it's a descendent of the officially released
   version ``@1.9.5.1``.  This is a trusted download method, and can
   be released to the Spack community:

   .. code-block:: python

      # This is not a tagged release of pyproj.
      # The changes in this "version" fix some bugs, especially with Python3 use.
      version('1.9.5.1.1', 'd035e4bc704d136db79b43ab371b27d2',
          url='https://www.github.com/jswhit/pyproj/tarball/0be612cc9f972e38b50a90c946a9b353e2ab140f')

   .. note::

      It would have been simpler to use Spack's Git download method,
      which is also a trusted download in this case:

      .. code-block:: python

         # This is not a tagged release of pyproj.
         # The changes in this "version" fix some bugs, especially with Python3 use.
         version('1.9.5.1.1',
              git='https://github.com/jswhit/pyproj.git',
              commit='0be612cc9f972e38b50a90c946a9b353e2ab140f')

   .. note::

      In this case, the upstream project fixed the bug in its
      repository in a relatively timely manner.  If that had not been
      the case, the numbered version in this step could have been
      released from the forked repository.


#. The author of the Spack package has now become an unofficial
   release engineer for the upstream project.  Depending on the
   situation, it may be advisable to put ``preferred=True`` on the
   latest *officially released* version.

#. As of August 31, the upstream project still had not made a new
   release with the bugfix.  In the meantime, Spack-built ``py-proj``
   provides the bugfix needed by packages depending on it.  As long as
   this works, there is no particular need for the upstream project to
   make a new official release.

#. If the upstream project releases a new official version with the
   bugfix, then the unofficial ``version()`` line should be removed
   from the Spack package.

^^^^^^^
Patches
^^^^^^^

Spack's source patching mechanism provides another way to fix bugs in
upstream projects.  This has advantages and disadvantages compared to the procedures above.

Advantages:

 1. It can fix bugs in existing released versions, and (probably)
    future releases as well.

 2. It is lightweight, does not require a new fork to be set up.

Disadvantages:

 1. It is harder to develop and debug a patch, vs. a branch in a
    repository.  The user loses the automation provided by version
    control systems.

 2. Although patches of a few lines work OK, large patch files can be
    hard to create and maintain.
.. Copyright 2013-2022 Lawrence Livermore National Security, LLC and other
   Spack Project Developers. See the top-level COPYRIGHT file for details.

   SPDX-License-Identifier: (Apache-2.0 OR MIT)


.. _build-systems:

=============
Build Systems
=============

Spack defines a number of classes which understand how to use common
`build systems  <https://en.wikipedia.org/wiki/List_of_build_automation_software>`_
(Makefiles, CMake, etc.). Spack package definitions can inherit these
classes in order to streamline their builds.

This guide provides information specific to each particular build system.
It assumes that you've read the :ref:`packaging-guide` and expands
on these ideas for each distinct build system that Spack supports:

.. toctree::
   :maxdepth: 1
   :caption: Make-based

   build_systems/makefilepackage

.. toctree::
   :maxdepth: 1
   :caption: Make-incompatible

   build_systems/mavenpackage
   build_systems/sconspackage
   build_systems/wafpackage

.. toctree::
   :maxdepth: 1
   :caption: Build-script generation

   build_systems/autotoolspackage
   build_systems/cmakepackage
   build_systems/mesonpackage
   build_systems/qmakepackage
   build_systems/sippackage

.. toctree::
   :maxdepth: 1
   :caption: Language-specific

   build_systems/octavepackage
   build_systems/perlpackage
   build_systems/pythonpackage
   build_systems/rpackage
   build_systems/rubypackage

.. toctree::
   :maxdepth: 1
   :caption: Other

   build_systems/bundlepackage
   build_systems/cudapackage
   build_systems/inteloneapipackage
   build_systems/intelpackage
   build_systems/rocmpackage
   build_systems/custompackage
   build_systems/multiplepackage

For reference, the :py:mod:`Build System API docs <spack.build_systems>`
provide a list of build systems and methods/attributes that can be
overridden. If you are curious about the implementation of a particular
build system, you can view the source code by running:

.. code-block:: console

   $ spack edit --build-system autotools


This will open up the ``AutotoolsPackage`` definition in your favorite
editor. In addition, if you are working with a less common build system
like QMake, SCons, or Waf, it may be useful to see examples of other
packages. You can quickly find examples by running:

.. code-block:: console

   $ cd var/spack/repos/builtin/packages
   $ grep -l QMakePackage */package.py


You can then view these packages with ``spack edit``.

This guide is intended to supplement the
:py:mod:`Build System API docs <spack.build_systems>` with examples of
how to override commonly used methods. It also provides rules of thumb
and suggestions for package developers who are unfamiliar with a
particular build system.
.. Copyright 2013-2022 Lawrence Livermore National Security, LLC and other
   Spack Project Developers. See the top-level COPYRIGHT file for details.

   SPDX-License-Identifier: (Apache-2.0 OR MIT)

.. _containers:

================
Container Images
================

Spack :ref:`environments` are a great tool to create container images, but
preparing one that is suitable for production requires some more boilerplate
than just:

.. code-block:: docker

   COPY spack.yaml /environment
   RUN spack -e /environment install

Additional actions may be needed to minimize the size of the
container, or to update the system software that is installed in the base
image, or to set up a proper entrypoint to run the image. These tasks are
usually both necessary and repetitive, so Spack comes with a command
to generate recipes for container images starting from a ``spack.yaml``.

--------------------
A Quick Introduction
--------------------

Consider having a Spack environment like the following:

.. code-block:: yaml

   spack:
     specs:
     - gromacs+mpi
     - mpich

Producing a ``Dockerfile`` from it is as simple as moving to the directory
where the ``spack.yaml`` file is stored and giving the following command:

.. code-block:: console

   $ spack containerize > Dockerfile

The ``Dockerfile`` that gets created uses multi-stage builds and
other techniques to minimize the size of the final image:

.. code-block:: docker

   # Build stage with Spack pre-installed and ready to be used
   FROM spack/ubuntu-bionic:latest as builder

   # What we want to install and how we want to install it
   # is specified in a manifest file (spack.yaml)
   RUN mkdir /opt/spack-environment \
   &&  (echo "spack:" \
   &&   echo "  specs:" \
   &&   echo "  - gromacs+mpi" \
   &&   echo "  - mpich" \
   &&   echo "  concretization: together" \
   &&   echo "  config:" \
   &&   echo "    install_tree: /opt/software" \
   &&   echo "  view: /opt/view") > /opt/spack-environment/spack.yaml

   # Install the software, remove unnecessary deps
   RUN cd /opt/spack-environment && spack env activate . && spack install --fail-fast && spack gc -y

   # Strip all the binaries
   RUN find -L /opt/view/* -type f -exec readlink -f '{}' \; | \
       xargs file -i | \
       grep 'charset=binary' | \
       grep 'x-executable\|x-archive\|x-sharedlib' | \
       awk -F: '{print $1}' | xargs strip -s

   # Modifications to the environment that are necessary to run
   RUN cd /opt/spack-environment && \
       spack env activate --sh -d . >> /etc/profile.d/z10_spack_environment.sh

   # Bare OS image to run the installed executables
   FROM ubuntu:18.04

   COPY --from=builder /opt/spack-environment /opt/spack-environment
   COPY --from=builder /opt/software /opt/software
   COPY --from=builder /opt/view /opt/view
   COPY --from=builder /etc/profile.d/z10_spack_environment.sh /etc/profile.d/z10_spack_environment.sh

   ENTRYPOINT ["/bin/bash", "--rcfile", "/etc/profile", "-l"]

The image itself can then be built and run in the usual way, with any of the
tools suitable for the task. For instance, if we decided to use ``docker``:

.. code-block:: bash

   $ spack containerize > Dockerfile
   $ docker build -t myimage .
   [ ... ]
   $ docker run -it myimage

The various components involved in the generation of the recipe and their
configuration are discussed in details in the sections below.

.. _container_spack_images:

--------------------------
Spack Images on Docker Hub
--------------------------

Docker images with Spack preinstalled and ready to be used are
built on `Docker Hub <https://hub.docker.com/u/spack>`_
at every push to ``develop`` or to a release branch. The OS that
are currently supported are summarized in the table below:

.. _containers-supported-os:

.. list-table:: Supported operating systems
   :header-rows: 1

   * - Operating System
     - Base Image
     - Spack Image
   * - Ubuntu 16.04
     - ``ubuntu:16.04``
     - ``spack/ubuntu-xenial``
   * - Ubuntu 18.04
     - ``ubuntu:18.04``
     - ``spack/ubuntu-bionic``
   * - CentOS 7
     - ``centos:7``
     - ``spack/centos7``
   * - openSUSE Leap
     - ``opensuse/leap``
     - ``spack/leap15``

All the images are tagged with the corresponding release of Spack:

.. image:: dockerhub_spack.png

with the exception of the ``latest`` tag that points to the HEAD
of the ``develop`` branch. These images are available for anyone
to use and take care of all the repetitive tasks that are necessary
to setup Spack within a container. The container recipes generated
by Spack use them as default base images for their ``build`` stage,
even though handles to use custom base images provided by users are
available to accommodate complex use cases.

---------------------------------
Creating Images From Environments
---------------------------------

Any Spack Environment can be used for the automatic generation of container
recipes. Sensible defaults are provided for things like the base image or the
version of Spack used in the image.
If a finer tuning is needed it can be obtained by adding the relevant metadata
under the ``container`` attribute of environments:

.. code-block:: yaml

   spack:
     specs:
     - gromacs+mpi
     - mpich

     container:
       # Select the format of the recipe e.g. docker,
       # singularity or anything else that is currently supported
       format: docker

       # Sets the base images for the stages where Spack builds the
       # software or where the software gets installed after being built..
       images:
         os: "centos:7"
         spack: develop

       # Whether or not to strip binaries
       strip: true

       # Additional system packages that are needed at runtime
       os_packages:
         final:
         - libgomp

       # Extra instructions
       extra_instructions:
         final: |
           RUN echo 'export PS1="\[$(tput bold)\]\[$(tput setaf 1)\][gromacs]\[$(tput setaf 2)\]\u\[$(tput sgr0)\]:\w $ "' >> ~/.bashrc

       # Labels for the image
       labels:
         app: "gromacs"
         mpi: "mpich"

A detailed description of the options available can be found in the
:ref:`container_config_options` section.

-------------------
Setting Base Images
-------------------

The ``images`` subsection is used to select both the image where
Spack builds the software and the image where the built software
is installed. This attribute can be set in different ways and
which one to use depends on the use case at hand.

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Use Official Spack Images From Dockerhub
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

To generate a recipe that uses an official Docker image from the
Spack organization to build the software and the corresponding official OS image
to install the built software, all the user has to do is specify:

1. An operating system under ``images:os``
2. A Spack version under ``images:spack``

Any combination of these two values that can be mapped to one of the images
discussed in :ref:`container_spack_images` is allowed. For instance, the
following ``spack.yaml``:

.. code-block:: yaml

   spack:
     specs:
     - gromacs+mpi
     - mpich

     container:
       images:
         os: centos:7
         spack: 0.15.4

uses ``spack/centos7:0.15.4``  and ``centos:7`` for the stages where the
software is respectively built and installed:

.. code-block:: docker

   # Build stage with Spack pre-installed and ready to be used
   FROM spack/centos7:0.15.4 as builder

   # What we want to install and how we want to install it
   # is specified in a manifest file (spack.yaml)
   RUN mkdir /opt/spack-environment \
   &&  (echo "spack:" \
   &&   echo "  specs:" \
   &&   echo "  - gromacs+mpi" \
   &&   echo "  - mpich" \
   &&   echo "  concretization: together" \
   &&   echo "  config:" \
   &&   echo "    install_tree: /opt/software" \
   &&   echo "  view: /opt/view") > /opt/spack-environment/spack.yaml
   [ ... ]
   # Bare OS image to run the installed executables
   FROM centos:7

   COPY --from=builder /opt/spack-environment /opt/spack-environment
   COPY --from=builder /opt/software /opt/software
   COPY --from=builder /opt/view /opt/view
   COPY --from=builder /etc/profile.d/z10_spack_environment.sh /etc/profile.d/z10_spack_environment.sh

   ENTRYPOINT ["/bin/bash", "--rcfile", "/etc/profile", "-l"]

This is the simplest available method of selecting base images, and we advise
to use it whenever possible. There are cases though where using Spack official
images is not enough to fit production needs. In these situations users can
extend the recipe to start with the bootstrapping of Spack at a certain pinned
version or manually select which base image to start from in the recipe,
as we'll see next.

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Use a Bootstrap Stage for Spack
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

In some cases users may want to pin the commit sha that is used for Spack, to ensure later
reproducibility, or start from a fork of the official Spack repository to try a bugfix or
a feature in the early stage of development. This is possible by being just a little more
verbose when specifying information about Spack in the ``spack.yaml`` file:

.. code-block:: yaml

   images:
     os: amazonlinux:2
     spack:
       # URL of the Spack repository to be used in the container image
       url: <to-use-a-fork>
       # Either a commit sha, a branch name or a tag
       ref: <sha/tag/branch>
       # If true turn a branch name or a tag into the corresponding commit
       # sha at the time of recipe generation
       resolve_sha: <true/false>

``url`` specifies the URL from which to clone Spack and defaults to https://github.com/spack/spack.
The ``ref`` attribute can be either a commit sha, a branch name or a tag. The default value in
this case is to use the ``develop`` branch, but it may change in the future to point to the latest stable
release. Finally ``resolve_sha`` transform branch names or tags into the corresponding commit
shas at the time of recipe generation, to allow for a greater reproducibility of the results
at a later time.

The list of operating systems that can be used to bootstrap Spack can be
obtained with:

.. command-output:: spack containerize --list-os

.. note::

   The ``resolve_sha`` option uses ``git rev-parse`` under the hood and thus it requires
   to checkout the corresponding Spack repository in a temporary folder before generating
   the recipe. Recipe generation may take longer when this option is set to true because
   of this additional step.


^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Use Custom Images Provided by Users
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Consider, as an example, building a production grade image for a CUDA
application. The best strategy would probably be to build on top of
images provided by the vendor and regard CUDA as an external package.

Spack doesn't currently provide an official image with CUDA configured
this way, but users can build it on their own and then configure the
environment to explicitly pull it. This requires users to:

1. Specify the image used to build the software under ``images:build``
2. Specify the image used to install the built software under ``images:final``

A ``spack.yaml`` like the following:

.. code-block:: yaml

   spack:
     specs:
     - gromacs@2019.4+cuda build_type=Release
     - mpich
     - fftw precision=float
     packages:
       cuda:
         buildable: False
         externals:
         - spec: cuda%gcc
           prefix: /usr/local/cuda

     container:
       images:
         build: custom/cuda-10.1-ubuntu18.04:latest
         final: nvidia/cuda:10.1-base-ubuntu18.04

produces, for instance, the following ``Dockerfile``:

.. code-block:: docker

   # Build stage with Spack pre-installed and ready to be used
   FROM custom/cuda-10.1-ubuntu18.04:latest as builder

   # What we want to install and how we want to install it
   # is specified in a manifest file (spack.yaml)
   RUN mkdir /opt/spack-environment \
   &&  (echo "spack:" \
   &&   echo "  specs:" \
   &&   echo "  - gromacs@2019.4+cuda build_type=Release" \
   &&   echo "  - mpich" \
   &&   echo "  - fftw precision=float" \
   &&   echo "  packages:" \
   &&   echo "    cuda:" \
   &&   echo "      buildable: false" \
   &&   echo "      externals:" \
   &&   echo "      - spec: cuda%gcc" \
   &&   echo "        prefix: /usr/local/cuda" \
   &&   echo "  concretization: together" \
   &&   echo "  config:" \
   &&   echo "    install_tree: /opt/software" \
   &&   echo "  view: /opt/view") > /opt/spack-environment/spack.yaml

   # Install the software, remove unnecessary deps
   RUN cd /opt/spack-environment && spack env activate . && spack install --fail-fast && spack gc -y

   # Strip all the binaries
   RUN find -L /opt/view/* -type f -exec readlink -f '{}' \; | \
       xargs file -i | \
       grep 'charset=binary' | \
       grep 'x-executable\|x-archive\|x-sharedlib' | \
       awk -F: '{print $1}' | xargs strip -s

   # Modifications to the environment that are necessary to run
   RUN cd /opt/spack-environment && \
       spack env activate --sh -d . >> /etc/profile.d/z10_spack_environment.sh

   # Bare OS image to run the installed executables
   FROM nvidia/cuda:10.1-base-ubuntu18.04

   COPY --from=builder /opt/spack-environment /opt/spack-environment
   COPY --from=builder /opt/software /opt/software
   COPY --from=builder /opt/view /opt/view
   COPY --from=builder /etc/profile.d/z10_spack_environment.sh /etc/profile.d/z10_spack_environment.sh

   ENTRYPOINT ["/bin/bash", "--rcfile", "/etc/profile", "-l"]

where the base images for both stages are completely custom.

This second mode of selection for base images is more flexible than just
choosing an operating system and a Spack version, but is also more demanding.
Users may need to generate by themselves their base images and it's also their
responsibility to ensure that:

1. Spack is available in the ``build`` stage and set up correctly to install the required software
2. The artifacts produced in the ``build`` stage can be executed in the ``final`` stage

Therefore we don't recommend its use in cases that can be otherwise
covered by the simplified mode shown first.

----------------------------
Singularity Definition Files
----------------------------

In addition to producing recipes in ``Dockerfile`` format Spack can produce
Singularity Definition Files by just changing the value of the ``format``
attribute:

.. code-block:: console

   $ cat spack.yaml
   spack:
     specs:
     - hdf5~mpi
     container:
       format: singularity

   $ spack containerize > hdf5.def
   $ sudo singularity build hdf5.sif hdf5.def

The minimum version of Singularity required to build a SIF (Singularity Image Format)
image from the recipes generated by Spack is ``3.5.3``.

.. _container_config_options:

-----------------------
Configuration Reference
-----------------------

The tables below describe all the configuration options that are currently supported
to customize the generation of container recipes:

.. list-table:: General configuration options for the ``container`` section of ``spack.yaml``
   :header-rows: 1

   * - Option Name
     - Description
     - Allowed Values
     - Required
   * - ``format``
     - The format of the recipe
     - ``docker`` or ``singularity``
     - Yes
   * - ``images:os``
     - Operating system used as a base for the image
     - See :ref:`containers-supported-os`
     - Yes, if using constrained selection of base images
   * - ``images:spack``
     - Version of Spack use in the ``build`` stage
     - Valid tags for ``base:image``
     - Yes, if using constrained selection of base images
   * - ``images:spack:url``
     - Repository from which Spack is cloned
     - Any fork of Spack
     - No
   * - ``images:spack:ref``
     - Reference for the checkout of Spack
     - Either a commit sha, a branch name or a tag
     - No
   * - ``images:spack:resolve_sha``
     - Resolve branches and tags in ``spack.yaml`` to commits in the generated recipe
     - True or False (default: False)
     - No
   * - ``images:build``
     - Image to be used in the ``build`` stage
     - Any valid container image
     - Yes, if using custom selection of base images
   * - ``images:final``
     - Image to be used in the ``build`` stage
     - Any valid container image
     - Yes, if using custom selection of base images
   * - ``strip``
     - Whether to strip binaries
     - ``true`` (default) or ``false``
     - No
   * - ``os_packages:command``
     - Tool used to manage system packages
     - ``apt``, ``yum``
     - Only with custom base images
   * - ``os_packages:update``
     - Whether or not to update the list of available packages
     - True or False (default: True)
     - No
   * - ``os_packages:build``
     - System packages needed at build-time
     - Valid packages for the current OS
     - No
   * - ``os_packages:final``
     - System packages needed at run-time
     - Valid packages for the current OS
     - No
   * - ``extra_instructions:build``
     - Extra instructions (e.g. `RUN`, `COPY`, etc.) at the end of the ``build`` stage
     - Anything understood by the current ``format``
     - No
   * - ``extra_instructions:final``
     - Extra instructions (e.g. `RUN`, `COPY`, etc.) at the end of the ``final`` stage
     - Anything understood by the current ``format``
     - No
   * - ``labels``
     - Labels to tag the image
     - Pairs of key-value strings
     - No

.. list-table:: Configuration options specific to Singularity
   :header-rows: 1

   * - Option Name
     - Description
     - Allowed Values
     - Required
   * - ``singularity:runscript``
     - Content of ``%runscript``
     - Any valid script
     - No
   * - ``singularity:startscript``
     - Content of ``%startscript``
     - Any valid script
     - No
   * - ``singularity:test``
     - Content of ``%test``
     - Any valid script
     - No
   * - ``singularity:help``
     - Description of the image
     - Description string
     - No

--------------
Best Practices
--------------

^^^
MPI
^^^
Due to the dependency on Fortran for OpenMPI, which is the spack default
implementation, consider adding ``gfortran`` to the ``apt-get install`` list.

Recent versions of OpenMPI will require you to pass ``--allow-run-as-root``
to your ``mpirun`` calls if started as root user inside Docker.

For execution on HPC clusters, it can be helpful to import the docker
image into Singularity in order to start a program with an *external*
MPI. Otherwise, also add ``openssh-server`` to the ``apt-get install`` list.

^^^^
CUDA
^^^^
Starting from CUDA 9.0, Nvidia provides minimal CUDA images based on
Ubuntu. Please see `their instructions <https://hub.docker.com/r/nvidia/cuda/>`_.
Avoid double-installing CUDA by adding, e.g.

.. code-block:: yaml

   packages:
     cuda:
       externals:
       - spec: "cuda@9.0.176%gcc@5.4.0 arch=linux-ubuntu16-x86_64"
         prefix: /usr/local/cuda
       buildable: False

to your ``spack.yaml``.

Users will either need ``nvidia-docker`` or e.g. Singularity to *execute*
device kernels.

^^^^^^^^^^^^^^^^^^^^^^^^^
Docker on Windows and OSX
^^^^^^^^^^^^^^^^^^^^^^^^^

On Mac OS and Windows, docker runs on a hypervisor that is not allocated much
memory by default, and some spack packages may fail to build due to lack of
memory. To work around this issue, consider configuring your docker installation
to use more of your host memory. In some cases, you can also ease the memory
pressure on parallel builds by limiting the parallelism in your config.yaml.

.. code-block:: yaml

   config:
     build_jobs: 2

.. Copyright 2013-2022 Lawrence Livermore National Security, LLC and other
   Spack Project Developers. See the top-level COPYRIGHT file for details.

   SPDX-License-Identifier: (Apache-2.0 OR MIT)

.. _bundlepackage:

-------------
BundlePackage
-------------

``BundlePackage`` represents a set of packages that are expected to work well
together, such as a collection of commonly used software libraries.  The
associated software is specified as bundle dependencies.


^^^^^^^^
Creation
^^^^^^^^

Be sure to specify the ``bundle`` template if you are using ``spack create``
to generate a package from the template.  For example, use the following
command to create a bundle package whose class name will be ``Mybundle``:

.. code-block:: console

    $ spack create --template bundle --name mybundle



^^^^^^
Phases
^^^^^^

The ``BundlePackage`` base class does not provide any phases by default
since the bundle does not represent a build system.


^^^
URL
^^^

The ``url`` property does not have meaning since there is no package-specific
code to fetch.


^^^^^^^
Version
^^^^^^^

At least one ``version`` must be specified in order for the package to
build.
.. Copyright 2013-2022 Lawrence Livermore National Security, LLC and other
   Spack Project Developers. See the top-level COPYRIGHT file for details.

   SPDX-License-Identifier: (Apache-2.0 OR MIT)

.. _autotoolspackage:

----------------
AutotoolsPackage
----------------

Autotools is a GNU build system that provides a build-script generator.
By running the platform-independent ``./configure`` script that comes
with the package, you can generate a platform-dependent Makefile.

^^^^^^
Phases
^^^^^^

The ``AutotoolsPackage`` base class comes with the following phases:

#. ``autoreconf`` - generate the configure script
#. ``configure`` - generate the Makefiles
#. ``build`` - build the package
#. ``install`` - install the package

Most of the time, the ``autoreconf`` phase will do nothing, but if the
package is missing a ``configure`` script, ``autoreconf`` will generate
one for you.

The other phases run:

.. code-block:: console

   $ ./configure --prefix=/path/to/installation/prefix
   $ make
   $ make check  # optional
   $ make install
   $ make installcheck  # optional


Of course, you may need to add a few arguments to the ``./configure``
line.

^^^^^^^^^^^^^^^
Important files
^^^^^^^^^^^^^^^

The most important file for an Autotools-based package is the ``configure``
script. This script is automatically generated by Autotools and generates
the appropriate Makefile when run.

.. warning::

   Watch out for fake Autotools packages!

   Autotools is a very popular build system, and many people are used to the
   classic steps to install a package:

   .. code-block:: console

      $ ./configure
      $ make
      $ make install


   For this reason, some developers will write their own ``configure``
   scripts that have nothing to do with Autotools. These packages may
   not accept the same flags as other Autotools packages, so it is
   better to use the ``Package`` base class and create a
   :ref:`custom build system <custompackage>`. You can tell if a package
   uses Autotools by running ``./configure --help`` and comparing the output
   to other known Autotools packages. You should also look for files like:

   * ``configure.ac``
   * ``configure.in``
   * ``Makefile.am``

   Packages that don't use Autotools aren't likely to have these files.

^^^^^^^^^^^^^^^^^^^^^^^^^
Build system dependencies
^^^^^^^^^^^^^^^^^^^^^^^^^

Whether or not your package requires Autotools to install depends on
how the source code is distributed. Most of the time, when developers
distribute tarballs, they will already contain the ``configure`` script
necessary for installation. If this is the case, your package does not
require any Autotools dependencies.

However, a basic rule of version control systems is to never commit
code that can be generated. The source code repository itself likely
does not have a ``configure`` script. Developers typically write
(or auto-generate) a ``configure.ac`` script that contains configuration
preferences and a ``Makefile.am`` script that contains build instructions.
Then, ``autoconf`` is used to convert ``configure.ac`` into ``configure``,
while ``automake`` is used to convert ``Makefile.am`` into ``Makefile.in``.
``Makefile.in`` is used by ``configure`` to generate a platform-dependent
``Makefile`` for you. The following diagram provides a high-level overview
of the process:

.. figure:: Autoconf-automake-process.*
   :target: https://commons.wikimedia.org/w/index.php?curid=15581407

   `GNU autoconf and automake process for generating makefiles <https://commons.wikimedia.org/wiki/File:Autoconf-automake-process.svg>`_
   by `Jdthood` under `CC BY-SA 3.0 <https://creativecommons.org/licenses/by-sa/3.0/deed.en>`_

If a ``configure`` script is not present in your tarball, you will
need to generate one yourself. Luckily, Spack already has an ``autoreconf``
phase to do most of the work for you. By default, the ``autoreconf``
phase runs:

.. code-block:: console

   $ autoreconf --install --verbose --force -I <aclocal-prefix>/share/aclocal

In case you need to add more arguments, override ``autoreconf_extra_args``
in your ``package.py`` on class scope like this:

.. code-block:: python

   autoreconf_extra_args = ["-Im4"]

All you need to do is add a few Autotools dependencies to the package.
Most stable releases will come with a ``configure`` script, but if you
check out a commit from the ``master`` branch, you would want to add:

.. code-block:: python

   depends_on('autoconf', type='build', when='@master')
   depends_on('automake', type='build', when='@master')
   depends_on('libtool',  type='build', when='@master')

It is typically redundant to list the ``m4`` macro processor package as a
dependency, since ``autoconf`` already depends on it.

"""""""""""""""""""""""""""""""
Using a custom autoreconf phase
"""""""""""""""""""""""""""""""

In some cases, it might be needed to replace the default implementation
of the autoreconf phase with one running a script interpreter. In this
example, the ``bash`` shell is used to run the ``autogen.sh`` script.

.. code-block:: python

   def autoreconf(self, spec, prefix):
       which('bash')('autogen.sh')

"""""""""""""""""""""""""""""""""""""""
patching configure or Makefile.in files
"""""""""""""""""""""""""""""""""""""""

In some cases, developers might need to distribute a patch that modifies
one of the files used to generate ``configure`` or ``Makefile.in``.
In this case, these scripts will need to be regenerated. It is
preferable to regenerate these manually using the patch, and then
create a new patch that directly modifies ``configure``. That way,
Spack can use the secondary patch and additional build system
dependencies aren't necessary.

""""""""""""""""""""""""""""
Old Autotools helper scripts
""""""""""""""""""""""""""""

Autotools based tarballs come with helper scripts such as ``config.sub`` and
``config.guess``. It is the responsibility of the developers to keep these files
up to date so that they run on every platform, but for very old software
releases this is impossible. In these cases Spack can help to replace these
files with newer ones, without having to add the heavy dependency on
``automake``.

Automatic helper script replacement is currently enabled by default on
``ppc64le`` and ``aarch64``, as these are the known cases where old scripts fail.
On these targets, ``AutotoolsPackage`` adds a build dependency on ``gnuconfig``,
which is a very light-weight package with newer versions of the helper files.
Spack then tries to run all the helper scripts it can find in the release, and
replaces them on failure with the helper scripts from ``gnuconfig``.

To opt out of this feature, use the following setting:

.. code-block:: python

   patch_config_files = False

To enable it conditionally on different architectures, define a property and
make the package depend on ``gnuconfig`` as a build dependency:

.. code-block

   depends_on('gnuconfig', when='@1.0:')

   @property
   def patch_config_files(self):
      return self.spec.satisfies("@1.0:")

.. note::

    On some exotic architectures it is necessary to use system provided
    ``config.sub`` and ``config.guess`` files. In this case, the most
    transparent solution is to mark the ``gnuconfig`` package as external and
    non-buildable, with a prefix set to the directory containing the files:

   .. code-block:: yaml

       gnuconfig:
         buildable: false
         externals:
         - spec: gnuconfig@master
           prefix: /usr/share/configure_files/


""""""""""""""""
force_autoreconf
""""""""""""""""

If for whatever reason you really want to add the original patch
and tell Spack to regenerate ``configure``, you can do so using the
following setting:

.. code-block:: python

   force_autoreconf = True

This line tells Spack to wipe away the existing ``configure`` script
and generate a new one. If you only need to do this for a single
version, this can be done like so:

.. code-block:: python

   @property
   def force_autoreconf(self):
       return self.version == Version('1.2.3')

^^^^^^^^^^^^^^^^^^^^^^^
Finding configure flags
^^^^^^^^^^^^^^^^^^^^^^^

Once you have a ``configure`` script present, the next step is to
determine what option flags are available. These flags can be found
by running:

.. code-block:: console

   $ ./configure --help

``configure`` will display a list of valid flags separated into
some or all of the following sections:

* Configuration
* Installation directories
* Fine tuning of the installation directories
* Program names
* X features
* System types
* **Optional Features**
* **Optional Packages**
* **Some influential environment variables**

For the most part, you can ignore all but the last 3 sections.
The "Optional Features" sections lists flags that enable/disable
features you may be interested in. The "Optional Packages" section
often lists dependencies and the flags needed to locate them. The
"environment variables" section lists environment variables that the
build system uses to pass flags to the compiler and linker.

^^^^^^^^^^^^^^^^^^^^^^^^^^
Addings flags to configure
^^^^^^^^^^^^^^^^^^^^^^^^^^

For most of the flags you encounter, you will want a variant to
optionally enable/disable them. You can then optionally pass these
flags to the ``configure`` call by overriding the ``configure_args``
function like so:

.. code-block:: python

   def configure_args(self):
       args = []

       if '+mpi' in self.spec:
           args.append('--enable-mpi')
       else:
           args.append('--disable-mpi')

       return args

Note that we are explicitly disabling MPI support if it is not
requested. This is important, as many Autotools packages will enable
options by default if the dependencies are found, and disable them
otherwise. We want Spack installations to be as deterministic as possible.
If two users install a package with the same variants, the goal is that
both installations work the same way. See `here <https://www.linux.com/news/best-practices-autotools>`__
and `here <https://wiki.gentoo.org/wiki/Project:Quality_Assurance/Automagic_dependencies>`__
for a rationale as to why these so-called "automagic" dependencies
are a problem.

By default, Autotools installs packages to ``/usr``. We don't want this,
so Spack automatically adds ``--prefix=/path/to/installation/prefix``
to your list of ``configure_args``. You don't need to add this yourself.

^^^^^^^^^^^^^^^^
Helper functions
^^^^^^^^^^^^^^^^

You may have noticed that most of the Autotools flags are of the form
``--enable-foo``, ``--disable-bar``, ``--with-baz=<prefix>``, or
``--without-baz``. Since these flags are so common, Spack provides a
couple of helper functions to make your life easier.

"""""""""""""""""
enable_or_disable
"""""""""""""""""

Autotools flags for simple boolean variants can be automatically
generated by calling the ``enable_or_disable`` method. This is
typically used to enable or disable some feature within the package.

.. code-block:: python

   variant(
       'memchecker',
       default=False,
       description='Memchecker support for debugging [degrades performance]'
   )
   config_args.extend(self.enable_or_disable('memchecker'))

In this example, specifying the variant ``+memchecker`` will generate
the following configuration options:

.. code-block:: console

   --enable-memchecker

"""""""""""""""
with_or_without
"""""""""""""""

Autotools flags for more complex variants, including boolean variants
and multi-valued variants, can be automatically generated by calling
the ``with_or_without`` method.

.. code-block:: python

   variant(
       'schedulers',
       values=disjoint_sets(
           ('auto',), ('alps', 'lsf', 'tm', 'slurm', 'sge', 'loadleveler')
       ).with_non_feature_values('auto', 'none'),
       description="List of schedulers for which support is enabled; "
       "'auto' lets openmpi determine",
   )
   if 'schedulers=auto' not in spec:
       config_args.extend(self.with_or_without('schedulers'))

In this example, specifying the variant ``schedulers=slurm,sge`` will
generate the following configuration options:

.. code-block:: console

   --with-slurm --with-sge

``enable_or_disable`` is actually functionally equivalent with
``with_or_without``, and accepts the same arguments and variant types;
but idiomatic autotools packages often follow these naming
conventions.

""""""""""""""""
activation_value
""""""""""""""""

Autotools parameters that require an option can still be automatically
generated, using the ``activation_value`` argument to
``with_or_without`` (or, rarely, ``enable_or_disable``).

.. code-block:: python

   variant(
      'fabrics',
       values=disjoint_sets(
           ('auto',), ('psm', 'psm2', 'verbs', 'mxm', 'ucx', 'libfabric')
       ).with_non_feature_values('auto', 'none'),
       description="List of fabrics that are enabled; "
       "'auto' lets openmpi determine",
   )
   if 'fabrics=auto' not in spec:
       config_args.extend(self.with_or_without('fabrics',
           activation_value='prefix'))

``activation_value`` accepts a callable that generates the configure
parameter value given the variant value; but the special value
``prefix`` tells Spack to automatically use the dependenency's
installation prefix, which is the most common use for such
parameters. In this example, specifying the variant
``fabrics=libfabric`` will generate the following configuration
options:

.. code-block:: console

   --with-libfabric=</path/to/libfabric>

"""""""""""""""""""""""
The ``variant`` keyword
"""""""""""""""""""""""

When Spack variants and configure flags do not correspond one-to-one, the
``variant`` keyword can be passed to ``with_or_without`` and
``enable_or_disable``. For example:

.. code-block:: python

   variant('debug_tools', default=False)
   config_args += self.enable_or_disable('debug-tools', variant='debug_tools')

Or when one variant controls multiple flags:

.. code-block:: python

   variant('debug_tools', default=False)
   config_args += self.with_or_without('memchecker', variant='debug_tools')
   config_args += self.with_or_without('profiler', variant='debug_tools')


""""""""""""""""""""
Conditional variants
""""""""""""""""""""

When a variant is conditional and its condition is not met on the concrete spec, the
``with_or_without`` and ``enable_or_disable`` methods will simply return an empty list.

For example:

.. code-block:: python

   variant('profiler', when='@2.0:')
   config_args += self.with_or_without('profiler)

will neither add ``--with-profiler`` nor ``--without-profiler`` when the version is
below ``2.0``.

""""""""""""""""""""
Activation overrides
""""""""""""""""""""

Finally, the behavior of either ``with_or_without`` or
``enable_or_disable`` can be overridden for specific variant
values. This is most useful for multi-values variants where some of
the variant values require atypical behavior.

.. code-block:: python

   def with_or_without_verbs(self, activated):
       # Up through version 1.6, this option was named --with-openib.
       # In version 1.7, it was renamed to be --with-verbs.
       opt = 'verbs' if self.spec.satisfies('@1.7:') else 'openib'
       if not activated:
           return '--without-{0}'.format(opt)
       return '--with-{0}={1}'.format(opt, self.spec['rdma-core'].prefix)

Defining ``with_or_without_verbs`` overrides the behavior of a
``fabrics=verbs`` variant, changing the configure-time option to
``--with-openib`` for older versions of the package and specifying an
alternative dependency name:

.. code-block::

   --with-openib=</path/to/rdma-core>

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Configure script in a sub-directory
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Occasionally, developers will hide their source code and ``configure``
script in a subdirectory like ``src``. If this happens, Spack won't
be able to automatically detect the build system properly when running
``spack create``. You will have to manually change the package base
class and tell Spack where the ``configure`` script resides. You can
do this like so:

.. code-block:: python

   configure_directory = 'src'

^^^^^^^^^^^^^^^^^^^^^^
Building out of source
^^^^^^^^^^^^^^^^^^^^^^

Some packages like ``gcc`` recommend building their software in a
different directory than the source code to prevent build pollution.
This can be done using the ``build_directory`` variable:

.. code-block:: python

   build_directory = 'spack-build'

By default, Spack will build the package in the same directory that
contains the ``configure`` script

^^^^^^^^^^^^^^^^^^^^^^^^^
Build and install targets
^^^^^^^^^^^^^^^^^^^^^^^^^

For most Autotools packages, the usual:

.. code-block:: console

   $ configure
   $ make
   $ make install

is sufficient to install the package. However, if you need to run
make with any other targets, for example, to build an optional
library or build the documentation, you can add these like so:

.. code-block:: python

   build_targets = ['all', 'docs']
   install_targets = ['install', 'docs']

^^^^^^^
Testing
^^^^^^^

Autotools-based packages typically provide unit testing via the
``check`` and ``installcheck`` targets. If you build your software
with ``spack install --test=root``, Spack will check for the presence
of a ``check`` or ``test`` target in the Makefile and run
``make check`` for you. After installation, it will check for an
``installcheck`` target and run ``make installcheck`` if it finds one.

^^^^^^^^^^^^^^^^^^^^^^
External documentation
^^^^^^^^^^^^^^^^^^^^^^

For more information on the Autotools build system, see:
https://www.gnu.org/software/automake/manual/html_node/Autotools-Introduction.html
.. Copyright 2013-2022 Lawrence Livermore National Security, LLC and other
   Spack Project Developers. See the top-level COPYRIGHT file for details.

   SPDX-License-Identifier: (Apache-2.0 OR MIT)

.. _pythonpackage:

-------------
PythonPackage
-------------

Python packages and modules have their own special build system. This
documentation covers everything you'll need to know in order to write
a Spack build recipe for a Python library.

^^^^^^^^^^^
Terminology
^^^^^^^^^^^

In the Python ecosystem, there are a number of terms that are
important to understand.

**PyPI**
   The `Python Package Index <https://pypi.org/>`_, where most Python
   libraries are hosted.

**sdist**
   Source distributions, distributed as tarballs (.tar.gz) and zip
   files (.zip). Contain the source code of the package.

**bdist**
   Built distributions, distributed as wheels (.whl). Contain the
   pre-built library.

**wheel**
   A binary distribution format common in the Python ecosystem. This
   file is actually just a zip file containing specific metadata and
   code. See the
   `documentation <https://packaging.python.org/en/latest/specifications/binary-distribution-format/>`_
   for more details.

**build frontend**
   Command-line tools used to build and install wheels. Examples
   include `pip <https://pip.pypa.io/>`_,
   `build <https://pypa-build.readthedocs.io/>`_, and
   `installer <https://installer.readthedocs.io/>`_.

**build backend**
   Libraries used to define how to build a wheel. Examples
   include `setuptools <https://setuptools.pypa.io/>`__,
   `flit <https://flit.readthedocs.io/>`_, and
   `poetry <https://python-poetry.org/>`_.

^^^^^^^^^^^
Downloading
^^^^^^^^^^^

The first step in packaging a Python library is to figure out where
to download it from. The vast majority of Python packages are hosted
on `PyPI <https://pypi.org/>`_, which is
:ref:`preferred over GitHub <pypi-vs-github>` for downloading
packages. Search for the package name on PyPI to find the project
page. The project page is usually located at::

   https://pypi.org/project/<package-name>

On the project page, there is a "Download files" tab containing
download URLs. Whenever possible, we prefer to build Spack packages
from source. If PyPI only has wheels, check to see if the project is
hosted on GitHub and see if GitHub has source distributions. The
project page usually has a "Homepage" and/or "Source code" link for
this. If the project is closed-source, it may only have wheels
available. For example, ``py-azureml-sdk`` is closed-source and can
be downloaded from::

   https://pypi.io/packages/py3/a/azureml_sdk/azureml_sdk-1.11.0-py3-none-any.whl

Once you've found a URL to download the package from, run:

.. code-block:: console

   $ spack create <url>


to create a new package template.

.. _pypi-vs-github:

"""""""""""""""
PyPI vs. GitHub
"""""""""""""""

Many packages are hosted on PyPI, but are developed on GitHub or
another version control system hosting service. The source code can
be downloaded from either location, but PyPI is preferred for the
following reasons:

#. PyPI contains the bare minimum number of files needed to install
   the package.

   You may notice that the tarball you download from PyPI does not
   have the same checksum as the tarball you download from GitHub.
   When a developer uploads a new release to PyPI, it doesn't contain
   every file in the repository, only the files necessary to install
   the package. PyPI tarballs are therefore smaller.

#. PyPI is the official source for package managers like ``pip``.

   Let's be honest, ``pip`` is much more popular than Spack. If the
   GitHub tarball contains a file not present in the PyPI tarball that
   causes a bug, the developers may not realize this for quite some
   time. If the bug was in a file contained in the PyPI tarball, users
   would notice the bug much more quickly.

#. GitHub release may be a beta version.

   When a developer releases a new version of a package on GitHub,
   it may not be intended for most users. Until that release also
   makes its way to PyPI, it should be assumed that the release is
   not yet ready for general use.

#. The checksum for a GitHub release may change.

   Unfortunately, some developers have a habit of patching releases
   without incrementing the version number. This results in a change
   in tarball checksum. Package managers like Spack that use checksums
   to verify the integrity of a download tarball grind to a halt when
   the checksum for a known version changes. Most of the time, the
   change is intentional, and contains a needed bug fix. However,
   sometimes the change indicates a download source that has been
   compromised, and a tarball that contains a virus. If this happens,
   you must contact the developers to determine which is the case.
   PyPI is nice because it makes it physically impossible to
   re-release the same version of a package with a different checksum.

The only reason to use GitHub instead of PyPI is if PyPI only has
wheels or if the PyPI sdist is missing a file needed to build the
package. If this is the case, please add a comment above the ``url``
explaining this.

^^^^
PyPI
^^^^

Since PyPI is so commonly used to host Python libraries, the
``PythonPackage`` base class has a ``pypi`` attribute that can be
set. Once set, ``pypi`` will be used to define the ``homepage``,
``url``, and ``list_url``. For example, the following:

.. code-block:: python

   homepage = 'https://pypi.org/project/setuptools/'
   url      = 'https://pypi.org/packages/source/s/setuptools/setuptools-49.2.0.zip'
   list_url = 'https://pypi.org/simple/setuptools/'


is equivalent to:

.. code-block:: python

   pypi = 'setuptools/setuptools-49.2.0.zip'


If a package has a different homepage listed on PyPI, you can
override it by setting your own ``homepage``.

^^^^^^^^^^^
Description
^^^^^^^^^^^

The top of the PyPI project page contains a short description of the
package. The "Project description" tab may also contain a longer
description of the package. Either of these can be used to populate
the package docstring.

^^^^^^^^^^^^^
Build backend
^^^^^^^^^^^^^

Once you've determined the basic metadata for a package, the next
step is to determine the build backend. ``PythonPackage`` uses
`pip <https://pip.pypa.io/>`_ to install the package, but pip
requires a backend to actually build the package.

To determine the build backend, look for a ``pyproject.toml`` file.
If there is no ``pyproject.toml`` file and only a ``setup.py`` or
``setup.cfg`` file, you can assume that the project uses
:ref:`setuptools`. If there is a ``pyproject.toml`` file, see if it
contains a ``[build-system]`` section. For example:

.. code-block:: toml

   [build-system]
   requires = [
       "setuptools>=42",
       "wheel",
   ]
   build-backend = "setuptools.build_meta"


This section does two things: the ``requires`` key lists build
dependencies of the project, and the ``build-backend`` key defines
the build backend. All of these build dependencies should be added as
dependencies to your package:

.. code-block:: python

   depends_on('py-setuptools@42:', type='build')


Note that ``py-wheel`` is already listed as a build dependency in the
``PythonPackage`` base class, so you don't need to add it unless you
need to specify a specific version requirement or change the
dependency type.

See `PEP 517 <https://www.python.org/dev/peps/pep-0517/>`_ and
`PEP 518 <https://www.python.org/dev/peps/pep-0518/>`_ for more
information on the design of ``pyproject.toml``.

Depending on which build backend a project uses, there are various
places that run-time dependencies can be listed.

"""""""""
distutils
"""""""""

Before the introduction of setuptools and other build backends,
Python packages had to rely on the built-in distutils library.
Distutils is missing many of the features that setuptools and other
build backends offer, and users are encouraged to use setuptools
instead. In fact, distutils was deprecated in Python 3.10 and will be
removed in Python 3.12. Because of this, pip actually replaces all
imports of distutils with setuptools. If a package uses distutils,
you should instead add a build dependency on setuptools. Check for a
``requirements.txt`` file that may list dependencies of the project.

.. _setuptools:

""""""""""
setuptools
""""""""""

If the ``pyproject.toml`` lists ``setuptools.build_meta`` as a
``build-backend``, or if the package has a ``setup.py`` that imports
``setuptools``, or if the package has a ``setup.cfg`` file, then it
uses setuptools to build. Setuptools is a replacement for the
distutils library, and has almost the exact same API. Dependencies
can be listed in the ``setup.py`` or ``setup.cfg`` file. Look for the
following arguments:

* ``python_requires``

  This specifies the version of Python that is required.

* ``setup_requires``

  These packages are usually only needed at build-time, so you can
  add them with ``type='build'``.

* ``install_requires``

  These packages are required for building and installation. You can
  add them with ``type=('build', 'run')``.

* ``extras_require``

  These packages are optional dependencies that enable additional
  functionality. You should add a variant that optionally adds these
  dependencies. This variant should be False by default.

* ``tests_require``

  These are packages that are required to run the unit tests for the
  package. These dependencies can be specified using the
  ``type='test'`` dependency type. However, the PyPI tarballs rarely
  contain unit tests, so there is usually no reason to add these.

See https://setuptools.pypa.io/en/latest/userguide/dependency_management.html
for more information on how setuptools handles dependency management.
See `PEP 440 <https://www.python.org/dev/peps/pep-0440/#version-specifiers>`_
for documentation on version specifiers in setuptools.

""""
flit
""""

There are actually two possible ``build-backend`` for flit, ``flit``
and ``flit_core``. If you see these in the ``pyproject.toml``, add a
build dependency to your package. With flit, all dependencies are
listed directly in the ``pyproject.toml`` file. Older versions of
flit used to store this info in a ``flit.ini`` file, so check for
this too.

Either of these files may contain keys like:

* ``requires-python``

  This specifies the version of Python that is required

* ``dependencies`` or ``requires``

  These packages are required for building and installation. You can
  add them with ``type=('build', 'run')``.

* ``project.optional-dependencies`` or ``requires-extra``

  This section includes keys with lists of optional dependencies
  needed to enable those features. You should add a variant that
  optionally adds these dependencies. This variant should be False
  by default.

See https://flit.readthedocs.io/en/latest/pyproject_toml.html for
more information.

""""""
poetry
""""""

Like flit, poetry also has two possible ``build-backend``, ``poetry``
and ``poetry_core``. If you see these in the ``pyproject.toml``, add
a build dependency to your package. With poetry, all dependencies are
listed directly in the ``pyproject.toml`` file. Dependencies are
listed in a ``[tool.poetry.dependencies]`` section, and use a
`custom syntax <https://python-poetry.org/docs/dependency-specification/#version-constraints>`_
for specifying the version requirements. Note that ``~=`` works
differently in poetry than in setuptools and flit for versions that
start with a zero.

""""""
wheels
""""""

Some Python packages are closed-source and are distributed as Python
wheels. For example, ``py-azureml-sdk`` downloads a ``.whl`` file. This
file is simply a zip file, and can be extracted using:

.. code-block:: console

   $ unzip *.whl


The zip file will not contain a ``setup.py``, but it will contain a
``METADATA`` file which contains all the information you need to
write a ``package.py`` build recipe. Check for lines like::

   Requires-Python: >=3.5,<4
   Requires-Dist: azureml-core (~=1.11.0)
   Requires-Dist: azureml-dataset-runtime[fuse] (~=1.11.0)
   Requires-Dist: azureml-train (~=1.11.0)
   Requires-Dist: azureml-train-automl-client (~=1.11.0)
   Requires-Dist: azureml-pipeline (~=1.11.0)
   Provides-Extra: accel-models
   Requires-Dist: azureml-accel-models (~=1.11.0); extra == 'accel-models'
   Provides-Extra: automl
   Requires-Dist: azureml-train-automl (~=1.11.0); extra == 'automl'


``Requires-Python`` is equivalent to ``python_requires`` and
``Requires-Dist`` is equivalent to ``install_requires``.
``Provides-Extra`` is used to name optional features (variants) and
a ``Requires-Dist`` with ``extra == 'foo'`` will list any
dependencies needed for that feature.

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Passing arguments to setup.py
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The default install phase should be sufficient to install most
packages. However, the installation instructions for a package may
suggest passing certain flags to the ``setup.py`` call. The
``PythonPackage`` class has two techniques for doing this.

""""""""""""""
Global options
""""""""""""""

These flags are added directly after ``setup.py`` when pip runs
``python setup.py install``. For example, the ``py-pyyaml`` package
has an optional dependency on ``libyaml`` that can be enabled like so:

.. code-block:: python

   def global_options(self, spec, prefix):
       options = []
       if '+libyaml' in spec:
           options.append('--with-libyaml')
       else:
           options.append('--without-libyaml')
       return options


"""""""""""""""
Install options
"""""""""""""""

These flags are added directly after ``install`` when pip runs
``python setup.py install``. For example, the ``py-pyyaml`` package
allows you to specify the directories to search for ``libyaml``:

.. code-block:: python

   def install_options(self, spec, prefix):
       options = []
       if '+libyaml' in spec:
           options.extend([
               spec['libyaml'].libs.search_flags,
               spec['libyaml'].headers.include_flags,
           ])
       return options


^^^^^^^
Testing
^^^^^^^

``PythonPackage`` provides a couple of options for testing packages
both during and after the installation process.

""""""""""""
Import tests
""""""""""""

Just because a package successfully built does not mean that it built
correctly. The most reliable test of whether or not the package was
correctly installed is to attempt to import all of the modules that
get installed. To get a list of modules, run the following command
in the source directory:

.. code-block:: console

   $ python
   >>> import setuptools
   >>> setuptools.find_packages()
   ['numpy', 'numpy._build_utils', 'numpy.compat', 'numpy.core', 'numpy.distutils', 'numpy.doc', 'numpy.f2py', 'numpy.fft', 'numpy.lib', 'numpy.linalg', 'numpy.ma', 'numpy.matrixlib', 'numpy.polynomial', 'numpy.random', 'numpy.testing', 'numpy.core.code_generators', 'numpy.distutils.command', 'numpy.distutils.fcompiler']


Large, complex packages like ``numpy`` will return a long list of
packages, while other packages like ``six`` will return an empty list.
``py-six`` installs a single ``six.py`` file. In Python packaging lingo,
a "package" is a directory containing files like:

.. code-block:: none

   foo/__init__.py
   foo/bar.py
   foo/baz.py


whereas a "module" is a single Python file.

The ``PythonPackage`` base class automatically detects these package
and module names for you. If, for whatever reason, the module names
detected are wrong, you can provide the names yourself by overriding
``import_modules`` like so:

.. code-block:: python

   import_modules = ['six']


Sometimes the list of module names to import depends on how the
package was built. For example, the ``py-pyyaml`` package has a
``+libyaml`` variant that enables the build of a faster optimized
version of the library. If the user chooses ``~libyaml``, only the
``yaml`` library will be importable. If the user chooses ``+libyaml``,
both the ``yaml`` and ``yaml.cyaml`` libraries will be available.
This can be expressed like so:

.. code-block:: python

   @property
   def import_modules(self):
       modules = ['yaml']
       if '+libyaml' in self.spec:
           modules.append('yaml.cyaml')
       return modules


These tests often catch missing dependencies and non-RPATHed
libraries. Make sure not to add modules/packages containing the word
"test", as these likely won't end up in the installation directory,
or may require test dependencies like pytest to be installed.

Import tests can be run during the installation using ``spack install
--test=root`` or at any time after the installation using
``spack test run``.

""""""""""
Unit tests
""""""""""

The package may have its own unit or regression tests. Spack can
run these tests during the installation by adding test methods after
installation.

For example, ``py-numpy`` adds the following as a check to run
after the ``install`` phase:

.. code-block:: python

   @run_after('install')
   @on_package_attributes(run_tests=True)
   def install_test(self):
       with working_dir('spack-test', create=True):
           python('-c', 'import numpy; numpy.test("full", verbose=2)')


when testing is enabled during the installation (i.e., ``spack install
--test=root``).

.. note::

   Additional information is available on :ref:`install phase tests
   <install_phase-tests>`.

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Setup file in a sub-directory
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Many C/C++ libraries provide optional Python bindings in a
subdirectory. To tell pip which directory to build from, you can
override the ``build_directory`` attribute. For example, if a package
provides Python bindings in a ``python`` directory, you can use:

.. code-block:: python

   build_directory = 'python'


^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
PythonPackage vs. packages that use Python
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

There are many packages that make use of Python, but packages that depend
on Python are not necessarily ``PythonPackage``'s.

"""""""""""""""""""""""
Choosing a build system
"""""""""""""""""""""""

First of all, you need to select a build system. ``spack create``
usually does this for you, but if for whatever reason you need to do
this manually, choose ``PythonPackage`` if and only if the package
contains one of the following files:

* ``pyproject.toml``
* ``setup.py``
* ``setup.cfg``

"""""""""""""""""""""""
Choosing a package name
"""""""""""""""""""""""

Selecting the appropriate package name is a little more complicated
than choosing the build system. By default, ``spack create`` will
prepend ``py-`` to the beginning of the package name if it detects
that the package uses the ``PythonPackage`` build system. However, there
are occasionally packages that use ``PythonPackage`` that shouldn't
start with ``py-``. For example:

* awscli
* aws-parallelcluster
* busco
* easybuild
* httpie
* mercurial
* scons
* snakemake

The thing these packages have in common is that they are command-line
tools that just so happen to be written in Python. Someone who wants
to install ``mercurial`` with Spack isn't going to realize that it is
written in Python, and they certainly aren't going to assume the package
is called ``py-mercurial``. For this reason, we manually renamed the
package to ``mercurial``.

Likewise, there are occasionally packages that don't use the
``PythonPackage`` build system but should still be prepended with ``py-``.
For example:

* py-genders
* py-py2cairo
* py-pygobject
* py-pygtk
* py-pyqt
* py-pyserial
* py-sip
* py-xpyb

These packages are primarily used as Python libraries, not as
command-line tools. You may see C/C++ packages that have optional
Python language-bindings, such as:

* antlr
* cantera
* conduit
* pagmo
* vtk

Don't prepend these kind of packages with ``py-``. When in doubt,
think about how this package will be used. Is it primarily a Python
library that will be imported in other Python scripts? Or is it a
command-line tool, or C/C++/Fortran program with optional Python
modules? The former should be prepended with ``py-``, while the
latter should not.

""""""""""""""""""""""
extends vs. depends_on
""""""""""""""""""""""

This is very similar to the naming dilemma above, with a slight twist.
As mentioned in the :ref:`Packaging Guide <packaging_extensions>`,
``extends`` and ``depends_on`` are very similar, but ``extends`` adds
the ability to *activate* the package. Activation involves symlinking
everything in the installation prefix of the package to the installation
prefix of Python. This allows the user to import a Python module without
having to add that module to ``PYTHONPATH``.

When deciding between ``extends`` and ``depends_on``, the best rule of
thumb is to check the installation prefix. If Python libraries are
installed to ``<prefix>/lib/pythonX.Y/site-packages``, then you
should use ``extends``. If Python libraries are installed elsewhere
or the only files that get installed reside in ``<prefix>/bin``, then
don't use ``extends``, as symlinking the package wouldn't be useful.

^^^^^^^^^^^^^^^^^^^^^
Alternatives to Spack
^^^^^^^^^^^^^^^^^^^^^

PyPI has hundreds of thousands of packages that are not yet in Spack,
and ``pip`` may be a perfectly valid alternative to using Spack. The
main advantage of Spack over ``pip`` is its ability to compile
non-Python dependencies. It can also build cythonized versions of a
package or link to an optimized BLAS/LAPACK library like MKL,
resulting in calculations that run orders of magnitudes faster.
Spack does not offer a significant advantage over other python-management
systems for installing and using tools like flake8 and sphinx.
But if you need packages with non-Python dependencies like
numpy and scipy, Spack will be very valuable to you.

Anaconda is another great alternative to Spack, and comes with its own
``conda`` package manager. Like Spack, Anaconda is capable of compiling
non-Python dependencies. Anaconda contains many Python packages that
are not yet in Spack, and Spack contains many Python packages that are
not yet in Anaconda. The main advantage of Spack over Anaconda is its
ability to choose a specific compiler and BLAS/LAPACK or MPI library.
Spack also has better platform support for supercomputers, and can build
optimized binaries for your specific microarchitecture. On the other hand,
Anaconda offers Windows support.

^^^^^^^^^^^^^^^^^^^^^^
External documentation
^^^^^^^^^^^^^^^^^^^^^^

For more information on Python packaging, see:

* https://packaging.python.org/

For more information on build and installation frontend tools, see:

* pip: https://pip.pypa.io/
* build: https://pypa-build.readthedocs.io/
* installer: https://installer.readthedocs.io/

For more information on build backend tools, see:

* setuptools: https://setuptools.pypa.io/
* flit: https://flit.readthedocs.io/
* poetry: https://python-poetry.org/
.. Copyright 2013-2022 Lawrence Livermore National Security, LLC and other
   Spack Project Developers. See the top-level COPYRIGHT file for details.

   SPDX-License-Identifier: (Apache-2.0 OR MIT)

.. _mavenpackage:

------------
MavenPackage
------------

Apache Maven is a general-purpose build system that does not rely
on Makefiles to build software. It is designed for building and
managing and Java-based project.

^^^^^^
Phases
^^^^^^

The ``MavenPackage`` base class comes with the following phases:

#. ``build`` - compile code and package into a JAR file
#. ``install`` - copy to installation prefix

By default, these phases run:

.. code-block:: console

   $ mvn package
   $ install . <prefix>


^^^^^^^^^^^^^^^
Important files
^^^^^^^^^^^^^^^

Maven packages can be identified by the presence of a ``pom.xml`` file.
This file lists dependencies and other metadata about the project.
There may also be configuration files in the ``.mvn`` directory.

^^^^^^^^^^^^^^^^^^^^^^^^^
Build system dependencies
^^^^^^^^^^^^^^^^^^^^^^^^^

Maven requires the ``mvn`` executable to build the project. It also
requires Java at both build- and run-time. Because of this, the base
class automatically adds the following dependencies:

.. code-block:: python

   depends_on('java', type=('build', 'run'))
   depends_on('maven', type='build')


In the ``pom.xml`` file, you may see sections like:

.. code-block:: xml

   <requireJavaVersion>
      <version>[1.7,)</version>
   </requireJavaVersion>
   <requireMavenVersion>
      <version>[3.5.4,)</version>
   </requireMavenVersion>


This specifies the versions of Java and Maven that are required to
build the package. See
https://docs.oracle.com/middleware/1212/core/MAVEN/maven_version.htm#MAVEN402
for a description of this version range syntax. In this case, you
should add:

.. code-block:: python

   depends_on('java@7:', type='build')
   depends_on('maven@3.5.4:', type='build')


^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Passing arguments to the build phase
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The default build and install phases should be sufficient to install
most packages. However, you may want to pass additional flags to
the build phase. For example:

.. code-block:: python

   def build_args(self):
       return [
           '-Pdist,native',
           '-Dtar',
           '-Dmaven.javadoc.skip=true'
       ]


^^^^^^^^^^^^^^^^^^^^^^
External documentation
^^^^^^^^^^^^^^^^^^^^^^

For more information on the Maven build system, see:
https://maven.apache.org/index.html
.. Copyright 2013-2022 Lawrence Livermore National Security, LLC and other
   Spack Project Developers. See the top-level COPYRIGHT file for details.

   SPDX-License-Identifier: (Apache-2.0 OR MIT)

.. _rocmpackage:

-----------
ROCmPackage
-----------

The ``ROCmPackage`` is not a build system but a helper package. Like ``CudaPackage``,
it provides standard variants, dependencies, and conflicts to facilitate building
packages using GPUs though for AMD in this case.

You can find the source for this package (and suggestions for setting up your
``compilers.yaml`` and ``packages.yaml`` files) at
`<https://github.com/spack/spack/blob/develop/lib/spack/spack/build_systems/rocm.py>`__.

^^^^^^^^
Variants
^^^^^^^^

This package provides the following variants:

* **rocm**

  This variant is used to enable/disable building with ``rocm``.  
  The default is disabled (or ``False``).

* **amdgpu_target**

  This variant supports the optional specification of the AMD GPU architecture.
  Valid values are the names of the GPUs (e.g., ``gfx701``), which are maintained
  in the ``amdgpu_targets`` property.

^^^^^^^^^^^^
Dependencies
^^^^^^^^^^^^

This package defines basic ``rocm`` dependencies, including ``llvm`` and ``hip``.

^^^^^^^^^
Conflicts
^^^^^^^^^

Conflicts are used to prevent builds with known bugs or issues. This package
already requires that the ``amdgpu_target`` always be specified for ``rocm``
builds. It also defines a conflict that prevents builds with an ``amdgpu_target``
when ``rocm`` is disabled.

Refer to `Conflicts <https://spack.readthedocs.io/en/latest/packaging_guide.html?highlight=conflicts#conflicts>`__
for more information on package conflicts.

^^^^^^^
Methods
^^^^^^^

This package provides one custom helper method, which is used to build
standard AMD hip compiler flags.

**hip_flags**

    This built-in static method returns the appropriately formatted
    ``--amdgpu-target`` build option for ``hipcc``.

    This method must be explicitly called when you are creating the
    arguments for your build in order to use the values.

^^^^^
Usage
^^^^^

This helper package can be added to your package by adding it as a base
class of your package.  For example, you can add it to your
:ref:`CMakePackage <cmakepackage>`-based package as follows:

.. code-block:: python
   :emphasize-lines: 1,3-7,14-25

    class MyRocmPackage(CMakePackage, ROCmPackage):
        ...
        # Ensure +rocm and amdgpu_targets are passed to dependencies
        depends_on('mydeppackage', when='+rocm')
        for val in ROCmPackage.amdgpu_targets:
            depends_on('mydeppackage amdgpu_target={0}'.format(val),
                       when='amdgpu_target={0}'.format(val))
        ...

        def cmake_args(self):
            spec = self.spec
            args = []
            ...
            if '+rocm' in spec:
                # Set up the hip macros needed by the build
                args.extend([
                    '-DENABLE_HIP=ON',
                    '-DHIP_ROOT_DIR={0}'.format(spec['hip'].prefix])
                rocm_archs = spec.variants['amdgpu_target'].value
                if 'none' not in rocm_archs:
                    args.append('-DHIP_HIPCC_FLAGS=--amdgpu-target={0}'
                                .format(",".join(rocm_archs)))
            else:
                # Ensure build with hip is disabled
                args.append('-DENABLE_HIP=OFF')
            ...
            return args
        ...

assuming only on the ``ENABLE_HIP``, ``HIP_ROOT_DIR``, and ``HIP_HIPCC_FLAGS``
macros are required to be set and the only dependency needing rocm options
is ``mydeppackage``. You will need to customize the flags as needed for your
build.

This example also illustrates how to check for the ``rocm`` variant using
``self.spec`` and how to retrieve the ``amdgpu_target`` variant's value
using ``self.spec.variants['amdgpu_target'].value``.

All five packages using ``ROCmPackage`` as of January 2021 also use the
:ref:`CudaPackage <cudapackage>`. So it is worth looking at those packages
to get ideas for creating a package that can support both ``cuda`` and
``rocm``.
.. Copyright 2013-2022 Lawrence Livermore National Security, LLC and other
   Spack Project Developers. See the top-level COPYRIGHT file for details.

   SPDX-License-Identifier: (Apache-2.0 OR MIT)

.. _qmakepackage:

------------
QMakePackage
------------

Much like Autotools and CMake, QMake is a build-script generator
designed by the developers of Qt. In its simplest form, Spack's
``QMakePackage`` runs the following steps:

.. code-block:: console

   $ qmake
   $ make
   $ make check  # optional
   $ make install


QMake does not appear to have a standardized way of specifying
the installation directory, so you may have to set environment
variables or edit ``*.pro`` files to get things working properly.

^^^^^^
Phases
^^^^^^

The ``QMakePackage`` base class comes with the following phases:

#. ``qmake`` - generate Makefiles
#. ``build`` - build the project
#. ``install`` - install the project

By default, these phases run:

.. code-block:: console

   $ qmake
   $ make
   $ make install


Any of these phases can be overridden in your package as necessary.
There is also a ``check`` method that looks for a ``check`` target
in the Makefile. If a ``check`` target exists and the user runs:

.. code-block:: console

   $ spack install --test=root <qmake-package>


Spack will run ``make check`` after the build phase.

^^^^^^^^^^^^^^^
Important files
^^^^^^^^^^^^^^^

Packages that use the QMake build system can be identified by the
presence of a ``<project-name>.pro`` file. This file declares things
like build instructions and dependencies.

One thing to look for is the ``minQtVersion`` function:

.. code-block:: none

   minQtVersion(5, 6, 0)


This means that Qt 5.6.0 is the earliest release that will work.
You should specify this in a ``depends_on`` statement.

^^^^^^^^^^^^^^^^^^^^^^^^^
Build system dependencies
^^^^^^^^^^^^^^^^^^^^^^^^^

At the bare minimum, packages that use the QMake build system need a
``qt`` dependency. Since this is always the case, the ``QMakePackage``
base class already contains:

.. code-block:: python

   depends_on('qt', type='build')


If you want to specify a particular version requirement, or need to
link to the ``qt`` libraries, you can override this in your package:

.. code-block:: python

   depends_on('qt@5.6.0:')

^^^^^^^^^^^^^^^^^^^^^^^^^^
Passing arguments to qmake
^^^^^^^^^^^^^^^^^^^^^^^^^^

If you need to pass any arguments to the ``qmake`` call, you can
override the ``qmake_args`` method like so:

.. code-block:: python

   def qmake_args(self):
       return ['-recursive']


This method can be used to pass flags as well as variables.

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
``*.pro`` file in a sub-directory
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

If the ``*.pro`` file used to tell QMake how to build the package is
found in a sub-directory, you can tell Spack to run all phases in this
sub-directory by adding the following to the package:

.. code-block:: python

   build_directory = 'src'


^^^^^^^^^^^^^^^^^^^^^^
External documentation
^^^^^^^^^^^^^^^^^^^^^^

For more information on the QMake build system, see:
http://doc.qt.io/qt-5/qmake-manual.html
.. Copyright 2013-2022 Lawrence Livermore National Security, LLC and other
   Spack Project Developers. See the top-level COPYRIGHT file for details.

   SPDX-License-Identifier: (Apache-2.0 OR MIT)

.. _intelpackage:

------------
IntelPackage
------------

.. contents::

^^^^^^^^^^^^^^^^^^^^^^^^
Intel packages in Spack
^^^^^^^^^^^^^^^^^^^^^^^^

Spack can install and use several software development products offered by Intel.
Some of these are available under no-cost terms, others require a paid license.
All share the same basic steps for configuration, installation, and, where
applicable, license management. The Spack Python class ``IntelPackage`` implements
these steps.

Spack interacts with Intel tools in several routes, like it does for any
other package:

.. _`route 1`:

1. Accept system-provided tools after you declare them to Spack as *external packages*.

.. _`route 2`:

2. Install the products for you as *internal packages* in Spack.

.. _`route 3`:

3. *Use* the packages, regardless of installation route, to install what we'll
   call *client packages* for you, this being Spack's primary purpose.

An auxiliary route follows from route 2, as it would for most Spack
packages, namely:

.. _`route 4`:

4. Make Spack-installed Intel tools available outside of Spack for ad-hoc use,
   typically through Spack-managed modulefiles.

This document covers routes 1 through 3.


""""""""""""""""""""""""""""""""""
Packages under no-cost license
""""""""""""""""""""""""""""""""""

Intel's standalone performance library products, notably MKL and MPI, are
available for use under a `simplified license
<https://software.intel.com/en-us/license/intel-simplified-software-license>`_
since 2017 [fn1]_. They are packaged in Spack as:

* ``intel-mkl`` -- Math Kernel Library (linear algebra and FFT),
* ``intel-mpi`` -- The Intel-MPI implementation (derived from MPICH),
* ``intel-ipp`` -- Primitives for image-, signal-, and data-processing,
* ``intel-daal`` -- Machine learning and data analytics.

Some earlier versions of these libraries were released under a paid license.
For these older versions, the license must be available at installation time of
the products and during compilation of client packages.

The library packages work well with the Intel compilers but do not require them
-- those packages can just as well be used with other compilers.  The Intel
compiler invocation commands offer custom options to simplify linking Intel
libraries (sometimes considerably), but Spack always uses fairly explicit
linkage anyway.


""""""""""""""""""
Licensed packages
""""""""""""""""""

Intel's core software development products that provide compilers, analyzers,
and optimizers do require a paid license.  In Spack, they are packaged as:

* ``intel-parallel-studio`` -- the entire suite of compilers and libraries,
* ``intel`` -- a subset containing just the compilers and the Intel-MPI runtime [fn2]_.

..
    TODO: Confirm and possible change(!) the scope of MPI components (runtime
    vs. devel) in current (and previous?) *cluster/professional/composer*
    editions, i.e., presence in downloads, possibly subject to license
    coverage(!); see `disussion in PR #4300
    <https://github.com/spack/spack/pull/4300#issuecomment-305582898>`_.  [NB:
    An "mpi" subdirectory is not indicative of the full MPI SDK being present
    (i.e., ``mpicc``, ..., and header files).  The directory may just as well
    contain only the MPI runtime (``mpirun`` and shared libraries) .]
    See also issue #8632.

The license is needed at installation time and to compile client packages, but
never to merely run any resulting binaries. The license status for a given
Spack package is normally specified in the *package code* through directives like
`license_required` (see :ref:`Licensed software <license>`).
For the Intel packages, however, the *class code* provides these directives (in
exchange of forfeiting a measure of OOP purity) and takes care of idiosyncasies
like historic version dependence.

The libraries that are provided in the standalone packages are also included in the
all-encompassing ``intel-parallel-studio``. To complicate matters a bit, that
package is sold in 3 "editions", of which only the upper-tier ``cluster``
edition supports *compiling* MPI applications, and hence only that edition can
provide the ``mpi`` virtual package.  (As mentioned [fn2]_, all editions
provide support for *running* MPI applications.)

The edition forms the leading part of the version number for Spack's
``intel*`` packages discussed here. This differs from the primarily numeric
version numbers seen with most other Spack packages. For example, we have:


.. code-block:: console

   $ spack info intel-parallel-studio
   ...
   Preferred version:
       professional.2018.3    http:...

   Safe versions:
       professional.2018.3    http:...
       ...
       composer.2018.3        http:...
       ...
       cluster.2018.3         http:...
       ...
   ...

The full studio suite, capable of compiling MPI applications, currently
requires about 12 GB of disk space when installed (see section `Install steps
for packages with compilers and libraries`_ for detailed instructions).
If you need to save disk space or installation time, you could install the
``intel`` compilers-only subset (0.6 GB) and just the library packages you
need, for example ``intel-mpi`` (0.5 GB) and ``intel-mkl`` (2.5 GB).

.. _intel-unrelated-packages:

""""""""""""""""""""
Unrelated packages
""""""""""""""""""""

The following packages do not use the Intel installer and are not in class ``IntelPackage``
that is discussed here:

* ``intel-gpu-tools`` -- Test suite and low-level tools for the Linux `Direct
  Rendering Manager <https://en.wikipedia.org/wiki/Direct_Rendering_Manager>`_
* ``intel-mkl-dnn`` -- Math Kernel Library for Deep Neural Networks (``CMakePackage``)
* ``intel-xed`` -- X86 machine instructions encoder/decoder
* ``intel-tbb`` -- Standalone version of Intel Threading Building Blocks. Note that
  a TBB runtime version is included with ``intel-mkl``, and development
  versions are provided by the packages ``intel-parallel-studio`` (all
  editions) and its ``intel`` subset.

""""""""""""""""""""""""""""""""""""""""""
Configuring Spack to use Intel licenses
""""""""""""""""""""""""""""""""""""""""""

If you wish to integrate licensed Intel products into Spack as external packages
(`route 1`_ above) we assume that their license configuration is in place and
is working [fn3]_. In this case, skip to section `Integration of Intel tools
installed external to Spack`_.

If you plan to have Spack install licensed products for you (`route 2`_ above),
the Intel product installer that Spack will run underneath must have access to
a license that is either provided by a *license server* or as a *license file*.
The installer may be able to locate a license that is already configured on
your system.  If it cannot, you must configure Spack to provide either the
server location or the license file.

For authoritative information on Intel licensing, see:

* https://software.intel.com/en-us/faq/licensing
* https://software.intel.com/en-us/articles/how-do-i-manage-my-licenses

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Pointing to an existing license server
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Installing and configuring a license server is outside the scope of Spack. We
assume that:

* Your system administrator has a license server running.
* The license server offers valid licenses for the Intel packages of interest.
* You can access these licenses under the user id running Spack.

Be aware of the difference between (a) installing and configuring a license
server, and (b) configuring client software to *use* a server's
so-called floating licenses.  We are concerned here with (b) only. The
process of obtaining a license from a server for temporary use is called
"checking out a license".  For that, a client application such as the Intel
package installer or a compiler needs to know the host name and port number of
one or more license servers that it may query [fn4]_.

Follow one of three methods to `point client software to a floating license server
<https://software.intel.com/en-us/articles/licensing-setting-up-the-client-floating-license>`_.
Ideally, your license administrator will already have implemented one that can
be used unchanged in Spack: Look for the environment variable
``INTEL_LICENSE_FILE`` or for files
``/opt/intel/licenses/*.lic`` that contain::

  SERVER  hostname  hostid_or_ANY  portnum
  USE_SERVER

The relevant tokens, among possibly others, are the ``USE_SERVER`` line,
intended specifically for clients, and one or more ``SERVER`` lines above it
which give the network address.

If you cannot find pre-existing ``/opt/intel/licenses/*.lic`` files and the
``INTEL_LICENSE_FILE`` environment variable is not set (even after you loaded
any relevant modulefiles), ask your license administrator for the server
address(es) and place them in a "global" license file within your Spack
directory tree `as shown below <Spack-managed file_>`_).


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Installing a standalone license file
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If you purchased a user-specific license, follow `Intel's instructions
<https://software.intel.com/en-us/faq/licensing#license-management>`_
to "activate" it for your serial number, then download the resulting license file.
If needed, `request to have the file re-sent
<https://software.intel.com/en-us/articles/resend-license-file>`_ to you.

Intel's license files are text files that contain tokens in the proprietary
"FLEXlm" format and whose name ends in ``.lic``.
Intel installers and compilers look for license files in several locations when they run.
Place your license by one of the following means, in order of decreasing preference:

* Default directory

  Install your license file in the directory ``/opt/intel/licenses/`` if you
  have write permission to it. This directory is inspected by all Intel tools
  and is therefore preferred, as no further configuration will be needed.
  Create the directory if it does not yet exist.  For the file name, either
  keep the downloaded name or use another suitably plain yet descriptive
  name that ends in ``.lic``. Adjust file permissions for access by licensed
  users.


* Directory given in environment variable

  If you cannot use the default directory, but your system already has set the
  environment variable ``INTEL_LICENSE_FILE`` independent from Spack [fn5]_,
  then, if you have the necessary write permissions, place your license file in
  one of the directories mentioned in this environment variable.  Adjust file
  permissions to match licensed users.

  .. tip::

      If your system has not yet set and used the environment variable
      ``INTEL_LICENSE_FILE``, you could start using it with the ``spack
      install`` stage of licensed tools and subsequent client packages. You
      would, however, be in a bind to always set that variable in the same
      manner, across updates and re-installations, and perhaps accommodate
      additions to it. As this may be difficult in the long run, we recommend
      that you do *not* attempt to start using the variable solely for Spack.

.. _`Spack-managed file`:

* Spack-managed file

  The first time Spack encounters an Intel package that requires a license, it
  will initialize a Spack-global Intel-specific license file for you, as a
  template with instructional comments, and bring up an editor [fn6]_.  Spack
  will do this *even if you have a working license elsewhere* on the system.

  * To proceed with an externally configured license, leave the newly templated
    file as is (containing comments only) and close the editor. You do not need
    to touch the file again.

  * To configure your own standalone license, copy the contents of your
    downloaded license file into the opened file, save it, and close the editor.

  * To use a license server (i.e., a floating network license) that is not
    already configured elsewhere on the system, supply your license server
    address(es) in the form of ``SERVER`` and ``USE_SERVER`` lines at the
    *beginning of the file* [fn7]_, in the format shown in section `Pointing to
    an existing license server`_. Save the file and close the editor.

  To revisit and manually edit this file, such as prior to a subsequent
  installation attempt, find it at
  ``$SPACK_ROOT/etc/spack/licenses/intel/intel.lic`` .

  Spack will place symbolic links to this file in each directory where licensed
  Intel binaries were installed.  If you kept the template unchanged, Intel tools
  will simply ignore it.


.. _integrate-external-intel:

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Integration of Intel tools installed *external* to Spack
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

This section discusses `route 1`_ from the introduction.

A site that already uses Intel tools, especially licensed ones, will likely
have some versions already installed on the system, especially at a time when
Spack is just being introduced. It will be useful to make such previously
installed tools available for use by Spack as they are. How to do this varies
depending on the type of the tools:

""""""""""""""""""""""""""""""""""
Integrating external compilers
""""""""""""""""""""""""""""""""""

For Spack to use external Intel compilers, you must tell it both *where* to
find them and *when* to use them.  The present section documents the "where"
aspect, involving ``compilers.yaml`` and, in most cases, long absolute paths.
The "when" aspect actually relates to `route 3`_ and requires explicitly
stating the compiler as a spec component (in the form ``foo %intel`` or ``foo
%intel@compilerversion``) when installing client packages or altering Spack's
compiler default in ``packages.yaml``.
See section `Selecting Intel compilers <Selecting Intel compilers_>`_ for details.

To integrate a new set of externally installed Intel compilers into Spack
follow section
:ref:`Compiler configuration <compiler-config>`.
Briefly, prepare your shell environment like you would if you were to use these
compilers normally, i.e., typically by a ``module load ...`` or a shell
``source ...`` command, then use ``spack compiler find`` to make Spack aware of
these compilers.  This will create a new entry in a suitably scoped and possibly new
``compilers.yaml`` file. You could certainly create such a compiler entry
manually, but this is error-prone due to the indentation and different data
types involved.

The Intel compilers need and use the system's native GCC compiler (``gcc`` on
most systems, ``clang`` on macOS) to provide certain functionality, notably to
support C++. To provide a different GCC compiler for the Intel tools, or more
generally set persistent flags for all invocations of the Intel compilers, locate
the ``compilers.yaml`` entry that defines your Intel compiler, and, using a
text editor, change one or both of the following:

1. At the ``modules:`` tag, add a ``gcc`` module to the list.
2. At the ``flags:`` tag, add ``cflags:``, ``cxxflags:``, and ``fflags:`` key-value entries.

Consult the examples under
:ref:`Compiler configuration <compiler-config>`
and
:ref:`Vendor-Specific Compiler Configuration <vendor-specific-compiler-configuration>`
in the Spack documentation.
When done, validate your compiler definition by running
``spack compiler info intel@compilerversion`` (replacing ``compilerversion`` by
the version that you defined).

Be aware that both the GCC integration and persistent compiler flags can also be
affected by an advanced third method:

3. A modulefile that provides the Intel compilers for you
   could, for the benefit of users outside of Spack, implicitly
   integrate a specific ``gcc`` version via compiler flag environment variables
   or (hopefully not) via a sneaky extra ``PATH`` addition.

Next, visit section `Selecting Intel Compilers`_ to learn how to tell
Spack to use the newly configured compilers.

.. _intel-integrating-external-libraries:

""""""""""""""""""""""""""""""""""
Integrating external libraries
""""""""""""""""""""""""""""""""""

Configure external library-type packages (as opposed to compilers)
in the files ``$SPACK_ROOT/etc/spack/packages.yaml`` or
``~/.spack/packages.yaml``, following the Spack documentation under
:ref:`External Packages <sec-external-packages>`.

Similar to ``compilers.yaml``, the ``packages.yaml`` files define a package
external to Spack in terms of a Spack spec and resolve each such spec via
either the ``paths`` or ``modules`` tokens to a specific pre-installed package
version on the system.  Since Intel tools generally need environment variables
to interoperate, which cannot be conveyed in a mere ``paths`` specification,
the ``modules`` token will be more sensible to use. It resolves the Spack-side
spec to a modulefile generated and managed outside of Spack's purview,
which Spack will load internally and transiently when the corresponding spec is
called upon to compile client packages.

Unlike for compilers, where ``spack find compilers [spec]`` generates an entry
in an existing or new ``compilers.yaml`` file, Spack does not offer a command
to generate an entirely new ``packages.yaml`` entry.  You must create
new entries yourself in a text editor, though the command ``spack config
[--scope=...] edit packages`` can help with selecting the proper file.
See section
:ref:`Configuration Scopes <configuration-scopes>`
for an explanation about the different files
and section
:ref:`Build customization <build-settings>`
for specifics and examples for ``packages.yaml`` files.

.. If your system administrator did not provide modules for pre-installed Intel
   tools, you could do well to ask for them, because installing multiple copies
   of the Intel tools, as is wont to happen once Spack is in the picture, is
   bound to stretch disk space and patience thin. If you *are* the system
   administrator and are still new to modules, then perhaps it's best to follow
   the `next section <Installing Intel tools within Spack_>`_ and install the tools
   solely within Spack.

The following example integrates packages embodied by hypothetical
external modulefiles ``intel-mkl/18/...`` into
Spack as packages ``intel-mkl@...``:

.. code-block:: console

   $ spack config edit packages

Make sure the file begins with:

.. code-block:: yaml

   packages:

Adapt the following example. Be sure to maintain the indentation:

.. code-block:: yaml

   # other content ...

     intel-mkl:
       externals:
       - spec: "intel-mkl@2018.2.199  arch=linux-centos6-x86_64"
         modules:
         -  intel-mkl/18/18.0.2
       - spec: "intel-mkl@2018.3.222  arch=linux-centos6-x86_64"
         modules:
         -  intel-mkl/18/18.0.3

The version numbers for the ``intel-mkl`` specs defined here correspond to file
and directory names that Intel uses for its products because they were adopted
and declared as such within Spack's package repository. You can inspect the
versions known to your current Spack installation by:

.. code-block:: console

   $ spack info intel-mkl

Using the same version numbers for external packages as for packages known
internally is useful for clarity, but not strictly necessary.  Moreover, with a
``packages.yaml`` entry, you can go beyond internally known versions.

.. _compiler-neutral-package:

Note that the Spack spec in the example does not contain a compiler
specification. This is intentional, as the Intel library packages can be used
unmodified with different compilers.

A slightly more advanced example illustrates how to provide
:ref:`variants <basic-variants>`
and how to use the ``buildable: False`` directive to prevent Spack from installing
other versions or variants of the named package through its normal internal
mechanism.

.. code-block:: yaml

   packages:
     intel-parallel-studio:
       externals:
       - spec: "intel-parallel-studio@cluster.2018.2.199 +mkl+mpi+ipp+tbb+daal  arch=linux-centos6-x86_64"
         modules:
         -  intel/18/18.0.2
       - spec: "intel-parallel-studio@cluster.2018.3.222 +mkl+mpi+ipp+tbb+daal  arch=linux-centos6-x86_64"
         modules:
         -  intel/18/18.0.3
       buildable: False

One additional example illustrates the use of ``prefix:`` instead of
``modules:``, useful when external modulefiles are not available or not
suitable:

.. code-block:: yaml

   packages:
     intel-parallel-studio:
       externals:
       - spec: "intel-parallel-studio@cluster.2018.2.199 +mkl+mpi+ipp+tbb+daal"
         prefix: /opt/intel
       - spec: "intel-parallel-studio@cluster.2018.3.222 +mkl+mpi+ipp+tbb+daal"
         prefix: /opt/intel
       buildable: False

Note that for the Intel packages discussed here, the directory values in the
``prefix:`` entries must be the high-level and typically version-less
"installation directory" that has been used by Intel's product installer.
Such a directory will typically accumulate various product versions.  Amongst
them, Spack will select the correct version-specific product directory based on
the ``@version`` spec component that each path is being defined for.

For further background and details, see
:ref:`External Packages <sec-external-packages>`.


^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Installing Intel tools *within* Spack
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

This section discusses `route 2`_ from the introduction.

When a system does not yet have Intel tools installed already, or the installed
versions are undesirable, Spack can install these tools like any regular Spack
package for you and, with appropriate pre- and post-install configuration, use its
compilers and/or libraries to install client packages.

.. _intel-install-studio:

""""""""""""""""""""""""""""""""""""""""""""""""""""""""""
Install steps for packages with compilers and libraries
""""""""""""""""""""""""""""""""""""""""""""""""""""""""""

The packages ``intel-parallel-studio`` and ``intel`` (which is a subset of the
former) are many-in-one products that contain both compilers and a set of
library packages whose scope depends on the edition.
Because they are general products geared towards shell environments,
it can be somewhat involved to integrate these packages at their full extent
into Spack.

Note: To install library-only packages like ``intel-mkl``, ``intel-mpi``, and ``intel-daal``
follow `the next section <intel-install-libs_>`_ instead.

1. Review the section `Configuring spack to use intel licenses`_.

.. _intel-compiler-anticipation:

2. To install a version of ``intel-parallel-studio`` that provides Intel
   compilers at a version that you have *not yet declared in Spack*,
   the following preparatory steps are recommended:

   A. Determine the compiler spec that the new ``intel-parallel-studio`` package
      will provide, as follows: From the package version, combine the last two
      digits of the version year, a literal "0" (zero), and the version component
      that immediately follows the year.

      ==========================================  ======================
      Package version                             Compiler spec provided
      ------------------------------------------  ----------------------
       ``intel-parallel-studio@edition.YYyy.u``   ``intel@yy.0.u``
      ==========================================  ======================

      Example: The package ``intel-parallel-studio@cluster.2018.3`` will provide
      the compiler with spec ``intel@18.0.3``.

   .. _`config-compiler-anticipated`:

   B. Add a new compiler section with the newly anticipated version at the
      end of a ``compilers.yaml`` file in a suitable scope.  For example, run:

      .. code-block:: console

         $ spack config --scope=user/linux edit compilers

      and append a stub entry:

      .. code-block:: yaml

         - compiler:
             target:     x86_64
             operating_system:   centos6
             modules:    []
             spec:       intel@18.0.3
             paths:
               cc:       /usr/bin/true
               cxx:      /usr/bin/true
               f77:      /usr/bin/true
               fc:       /usr/bin/true

      Replace ``18.0.3`` with the version that you determined in the preceding
      step. The exact contents under ``paths:`` do not matter yet, but the paths must exist.

   This temporary stub is required such that the ``intel-parallel-studio`` package
   can be installed for the ``intel`` compiler (which the package itself is going
   to provide after the installation) rather than an arbitrary system compiler.
   The paths given in ``cc``, ``cxx``, ``f77``, ``fc`` must exist, but will
   never be used to build anything during the installation of ``intel-parallel-studio``.

   The reason for this stub is that ``intel-parallel-studio`` also provides the
   ``mpi`` and ``mkl`` packages and when concretizing a spec, Spack ensures
   strong consistency of the used compiler across all dependencies:  [fn8]_.
   Installing a package ``foo +mkl %intel`` will make Spack look for a package
   ``mkl %intel``, which can be provided by ``intel-parallel-studio+mkl %intel``,
   but not by ``intel-parallel-studio+mkl %gcc``.

   Failure to do so may result in additional installations of ``mkl``, ``intel-mpi`` or
   even ``intel-parallel-studio`` as dependencies for other packages.

   .. _`verify-compiler-anticipated`:

3. Verify that the compiler version provided by the new ``studio`` version
   would be used as expected if you were to compile a client package:

   .. code-block:: console

      $ spack spec zlib %intel

   If the version does not match, explicitly state the anticipated compiler version, e.g.:

   .. code-block:: console

      $ spack spec zlib %intel@18.0.3

   if there are problems, review and correct the compiler's ``compilers.yaml``
   entry, be it still in stub form or already complete (as it would be for a
   re-installation).

4. Install the new ``studio`` package using Spack's regular ``install``
   command.
   It may be wise to provide the anticipated compiler (`see above
   <verify-compiler-anticipated_>`_) as an explicit concretization
   element:

   .. code-block:: console

      $ spack install intel-parallel-studio@cluster.2018.3  %intel@18.0.3

5. Follow the same steps as under `Integrating external compilers`_ to tell
   Spack the minutiae for actually using those compilers with client packages.
   If you placed a stub entry in a ``compilers.yaml`` file, now is the time to
   edit it and fill in the particulars.

   * Under ``paths:``, give the full paths to the actual compiler binaries (``icc``,
     ``ifort``, etc.) located within the Spack installation tree, in all their
     unsightly length [fn9]_.

     To determine the full path to the C compiler, adapt and run:

     .. code-block:: console

        $ find `spack location -i intel-parallel-studio@cluster.2018.3` \
               -name icc -type f -ls

     If you get hits for both ``intel64`` and ``ia32``, you almost certainly will
     want to use the ``intel64`` variant.  The ``icpc`` and ``ifort`` compilers
     will be located in the same directory as ``icc``.

   * Make sure to specify ``modules: ['intel-parallel-studio-cluster2018.3-intel-18.0.3-HASH']``
     (with ``HASH`` being the short hash as displayed when running
     ``spack find -l intel-parallel-studio@cluster.2018.3`` and the versions adapted accordingly)
     to ensure that the correct and complete environment for the Intel compilers gets
     loaded when running them. With modern versions of the Intel compiler you may otherwise see
     issues about missing libraries. Please also note that module name must exactly match
     the name as returned by ``module avail`` (and shown in the example above).

   * Use the ``modules:`` and/or ``cflags:`` tokens to further specify a suitable accompanying
     ``gcc`` version to help pacify picky client packages that ask for C++
     standards more recent than supported by your system-provided ``gcc`` and its
     ``libstdc++.so``.

   * If you specified a custom variant (for example ``+vtune``) you may want to add this as your
     preferred variant in the packages configuration for the ``intel-parallel-studio`` package
     as described in :ref:`concretization-preferences`. Otherwise you will have to specify
     the variant everytime ``intel-parallel-studio`` is being used as ``mkl``, ``fftw`` or ``mpi``
     implementation to avoid pulling in a different variant.

   * To set the Intel compilers for default use in Spack, instead of the usual ``%gcc``,
     follow section `Selecting Intel compilers`_.

.. tip::

   Compiler packages like ``intel-parallel-studio`` can easily be above 10 GB
   in size, which can tax the disk space available for temporary files on
   small, busy, or restricted systems (like virtual machines). The Intel
   installer will stop and report insufficient space as::

       ==> './install.sh' '--silent' 'silent.cfg'
       ...
       Missing critical prerequisite
       -- Not enough disk space

   As first remedy, clean Spack's existing staging area:

   .. code-block:: console

      $ spack clean --stage

   then retry installing the large package. Spack normally cleans staging
   directories but certain failures may prevent it from doing so.

   If the error persists, tell Spack to use an alternative location for
   temporary files:

   1. Run ``df -h`` to identify an alternative location on your system.

   2. Tell Spack to use that location for staging. Do **one** of the following:

      * Run Spack with the environment variable ``TMPDIR`` altered for just a
        single command. For example, to use your ``$HOME`` directory:

        .. code-block:: console

           $ TMPDIR="$HOME/spack-stage"  spack install ....

        This example uses Bourne shell syntax. Adapt for other shells as needed.

      * Alternatively, customize
        Spack's ``build_stage`` :ref:`configuration setting <config-overrides>`.

        .. code-block:: console

           $ spack config edit config

        Append:

        .. code-block:: yaml

           config:
             build_stage:
             - /home/$user/spack-stage

        Do not duplicate the ``config:`` line if it already is present.
        Adapt the location, which here is the same as in the preceding example.

   3. Retry installing the large package.


.. _intel-install-libs:

""""""""""""""""""""""""""""""""""""""""""""""""""""""""
Install steps for library-only packages
""""""""""""""""""""""""""""""""""""""""""""""""""""""""

To install library-only packages like ``intel-mkl``, ``intel-mpi``, and ``intel-daal``
follow the steps given here.
For packages that contain a compiler, follow `the previous section
<intel-install-studio_>`_ instead.

1. For pre-2017 product releases, review the section `Configuring Spack to use Intel licenses`_.

2. Inspect the package spec. Specify an explicit compiler if necessary, e.g.:

   .. code-block:: console

      $ spack spec intel-mpi@2018.3.199
      $ spack spec intel-mpi@2018.3.199  %intel

   Check that the package will use the compiler flavor and version that you expect.

3. Install the package normally within Spack. Use the same spec as in the
   previous command, i.e., as general or as specific as needed:

   .. code-block:: console

      $ spack install intel-mpi@2018.3.199
      $ spack install intel-mpi@2018.3.199  %intel@18

4. To prepare the new packages for use with client packages,
   follow `Selecting libraries to satisfy virtual packages`_.


""""""""""""""""
Debug notes
""""""""""""""""

* You can trigger a wall of additional diagnostics using Spack options, e.g.:

  .. code-block:: console

     $ spack --debug -v install intel-mpi

  The ``--debug`` option can also be useful while installing client
  packages `(see below) <Using Intel tools in Spack to install client
  packages_>`_ to confirm the integration of the Intel tools in Spack, notably
  MKL and MPI.

* The ``.spack/`` subdirectory of an installed ``IntelPackage`` will contain,
  besides Spack's usual archival items, a copy of the ``silent.cfg`` file that
  was passed to the Intel installer:

  .. code-block:: console

     $ grep COMPONENTS ...intel-mpi...<hash>/.spack/silent.cfg
     COMPONENTS=ALL

* If an installation error occurs, Spack will normally clean up and remove a
  partially installed target directory. You can direct Spack to keep it using
  ``--keep-prefix``, e.g.:

  .. code-block:: console

     $ spack install --keep-prefix  intel-mpi

  You must, however, *remove such partial installations* prior to subsequent
  installation attempts. Otherwise, the Intel installer will behave
  incorrectly.


^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Using Intel tools in Spack to install client packages
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Finally, this section pertains to `route 3`_ from the introduction.

Once Intel tools are installed within Spack as external or internal packages
they can be used as intended for installing client packages.


.. _`select-intel-compilers`:

""""""""""""""""""""""""""
Selecting Intel compilers
""""""""""""""""""""""""""

Select Intel compilers to compile client packages, like any compiler in Spack,
by one of the following means:

* Request the Intel compilers explicitly in the client spec, e.g.:

  .. code-block:: console

     $ spack install libxc@3.0.0%intel


* Alternatively, request Intel compilers implicitly by concretization preferences.
  Configure the order of compilers in the appropriate ``packages.yaml`` file,
  under either an ``all:`` or client-package-specific entry, in a
  ``compiler:`` list. Consult the Spack documentation for
  `Configuring Package Preferences <https://spack-tutorial.readthedocs.io/en/latest/tutorial_configuration.html#configuring-package-preferences>`_
  and
  :ref:`Concretization Preferences <concretization-preferences>`.

Example: ``etc/spack/packages.yaml`` might simply contain:

.. code-block:: yaml

  packages:
    all:
      compiler: [ intel, gcc, ]

To be more specific, you can state partial or full compiler version numbers,
for example:

.. code-block:: yaml

  packages:
    all:
      compiler: [ intel@18, intel@17, gcc@4.4.7, gcc@4.9.3, gcc@7.3.0, ]


.. _intel-virtual-packages:

""""""""""""""""""""""""""""""""""""""""""""""""
Selecting libraries to satisfy virtual packages
""""""""""""""""""""""""""""""""""""""""""""""""

Intel packages, whether integrated into Spack as external packages or
installed within Spack, can be called upon to satisfy the requirement of a
client package for a library that is available from different providers.
The relevant virtual packages for Intel are ``blas``, ``lapack``,
``scalapack``, and ``mpi``.

In both integration routes, Intel packages can have optional
:ref:`variants <basic-variants>`
which alter the list of virtual packages they can satisfy.  For Spack-external
packages, the active variants are a combination of the defaults declared in
Spack's package repository and the spec it is declared as in ``packages.yaml``.
Needless to say, those should match the components that are actually present in
the external product installation. Likewise, for Spack-internal packages, the
active variants are determined, persistently at installation time, from the
defaults in the repository and the spec selected to be installed.

To have Intel packages satisfy virtual package requests for all or selected
client packages, edit the ``packages.yaml`` file.  Customize, either in the
``all:`` or a more specific entry, a ``providers:`` dictionary whose keys are
the virtual packages and whose values are the Spack specs that satisfy the
virtual package, in order of decreasing preference.  To learn more about the
``providers:`` settings, see the Spack tutorial for
`Configuring Package Preferences <https://spack-tutorial.readthedocs.io/en/latest/tutorial_configuration.html#configuring-package-preferences>`_
and the section
:ref:`Concretization Preferences <concretization-preferences>`.

Example: The following fairly minimal example for ``packages.yaml`` shows how
to exclusively use the standalone ``intel-mkl`` package for all the linear
algebra virtual packages in Spack, and ``intel-mpi`` as the preferred MPI
implementation. Other providers can still be chosen on a per-package basis.

.. code-block:: yaml

  packages:
    all:
      providers:
        mpi:       [intel-mpi]
        blas:      [intel-mkl]
        lapack:    [intel-mkl]
        scalapack: [intel-mkl]

If you have access to the ``intel-parallel-studio@cluster`` edition, you can
use instead:

.. code-block:: yaml

    all:
      providers:
        mpi:       [intel-parallel-studio+mpi]
        # Note: +mpi vs. +mkl
        blas:      [intel-parallel-studio+mkl]
        lapack:    [intel-parallel-studio+mkl]
        scalapack: [intel-parallel-studio+mkl]

If you installed ``intel-parallel-studio`` within Spack ("`route 2`_"), make
sure you followed the `special installation step
<intel-compiler-anticipation_>`_ to ensure that its virtual packages match the
compilers it provides.


""""""""""""""""""""""""""""""""""""""""""""
Using Intel tools as explicit dependency
""""""""""""""""""""""""""""""""""""""""""""

With the proper installation as detailed above, no special steps should be
required when a client package specifically (and thus deliberately) requests an
Intel package as dependency, this being one of the target use cases for Spack.

.. _using-mkl-tips:

"""""""""""""""""""""""""""""""""""""""""""""""
Tips for configuring client packages to use MKL
"""""""""""""""""""""""""""""""""""""""""""""""

The Math Kernel Library (MKL) is provided by several Intel packages, currently
``intel-parallel-studio`` when variant ``+mkl`` is active (it is by default)
and the standalone ``intel-mkl``. Because of these different provider packages,
a *virtual* ``mkl`` package is declared in Spack.

* To use MKL-specific APIs in a client package:

  Declare a dependency on ``mkl``, rather than a specific provider like
  ``intel-mkl``.  Declare the dependency either absolutely or conditionally
  based on variants that your package might have declared:

  .. code-block:: python

     # Examples for absolute and conditional dependencies:
     depends_on('mkl')
     depends_on('mkl', when='+mkl')
     depends_on('mkl', when='fftw=mkl')

  The ``MKLROOT`` environment variable (part of the documented API) will be set
  during all stages of client package installation, and is available to both
  the Spack packaging code and the client code.

* To use MKL as provider for BLAS, LAPACK, or ScaLAPACK:

  The packages that provide ``mkl`` also provide the narrower
  virtual ``blas``, ``lapack``, and ``scalapack`` packages.
  See the relevant :ref:`Packaging Guide section <blas_lapack_scalapack>`
  for an introduction.
  To portably use these virtual packages, construct preprocessor and linker
  option strings in your package configuration code using the package functions
  ``.headers`` and ``.libs`` in conjunction with utility functions from the
  following classes:

  * :py:class:`llnl.util.filesystem.FileList`,
  * :py:class:`llnl.util.filesystem.HeaderList`,
  * :py:class:`llnl.util.filesystem.LibraryList`.

  .. tip::
     *Do not* use constructs like ``.prefix.include`` or ``.prefix.lib``, with
     Intel or any other implementation of ``blas``, ``lapack``, and
     ``scalapack``.

  For example, for an
  :ref:`AutotoolsPackage <autotoolspackage>`
  use ``.libs.ld_flags`` to transform the library file list into linker options
  passed to ``./configure``:

  .. code-block:: python

      def configure_args(self):
          args = []
          ...
          args.append('--with-blas=%s' % self.spec['blas'].libs.ld_flags)
          args.append('--with-lapack=%s' % self.spec['lapack'].libs.ld_flags)
          ...

  .. tip::
     Even though ``.ld_flags`` will return a string of multiple words, *do not*
     use quotes for options like ``--with-blas=...`` because Spack passes them
     to ``./configure`` without invoking a shell.

  Likewise, in a
  :ref:`MakefilePackage <makefilepackage>`
  or similar package that does not use AutoTools you may need to provide include
  and link options for use on command lines or in environment variables.
  For example, to generate an option string of the form ``-I<dir>``, use:

  .. code-block:: python

    self.spec['blas'].headers.include_flags

  and to generate linker options (``-L<dir> -llibname ...``), use the same as above,

  .. code-block:: python

    self.spec['blas'].libs.ld_flags

  See
  :ref:`MakefilePackage <makefilepackage>`
  and more generally the
  :ref:`Packaging Guide <blas_lapack_scalapack>`
  for background and further examples.


^^^^^^^^^^
Footnotes
^^^^^^^^^^

.. [fn1] Strictly speaking, versions from ``2017.2`` onward.

.. [fn2] The package ``intel`` intentionally does not have a ``+mpi`` variant since
   it is meant to be small. The native installer will always add MPI *runtime*
   components because it follows defaults defined in the download package, even
   when ``intel-parallel-studio ~mpi`` has been requested.

   For ``intel-parallel-studio +mpi``, the class function
   :py:func:``.IntelPackage.pset_components``
   will include ``"intel-mpi intel-imb"`` in a list of component patterns passed
   to the Intel installer. The installer will extend each pattern word with an
   implied glob-like ``*`` to resolve it to package names that are
   *actually present in the product BOM*.
   As a side effect, this pattern approach accommodates occasional package name
   changes, e.g., capturing both ``intel-mpirt`` and ``intel-mpi-rt`` .

.. [fn3] How could the external installation have succeeded otherwise?

.. [fn4] According to Intel's documentation, there is supposedly a way to install a
   product using a network license even `when a FLEXlm server is not running
   <https://software.intel.com/en-us/articles/licensing-setting-up-the-client-floating-license>`_:
   Specify the license in the form ``port@serverhost`` in the
   ``INTEL_LICENSE_FILE`` environment variable. All other means of specifying a
   network license require that the license server be up.

.. [fn5]  Despite the name, ``INTEL_LICENSE_FILE`` can hold several and diverse entries.
   They  can be either directories (presumed to contain ``*.lic`` files), file
   names, or network locations in the form ``port@host`` (on Linux and Mac),
   with all items separated by ":" (on Linux and Mac).

.. [fn6] Should said editor turn out to be ``vi``, you better be in a position
   to know how to use it.

.. [fn7] Comment lines in FLEXlm files, indicated by ``#`` as the first
   non-whitespace character on the line, are generally allowed anywhere in the file.
   There `have been reports <https://github.com/spack/spack/issues/6534>`_,
   however, that as of 2018, ``SERVER`` and ``USE_SERVER`` lines must precede
   any comment lines.

..
    .. [fnX] The name component ``intel`` of the compiler spec is separate from (in
       a different namespace than) the names of the Spack packages
       ``intel-parallel-studio`` and ``intel``. Both of the latter provide the former.

.. [fn8] Spack's close coupling of installed packages to compilers, which both
   necessitates the detour for installing ``intel-parallel-studio``, and
   largely limits any of its provided virtual packages to a single compiler, heavily
   favors `recommending to install Intel Parallel Studio outside of Spack
   <integrate-external-intel_>`_ and declare it for Spack in ``packages.yaml``
   by a `compiler-less spec <compiler-neutral-package_>`_.

.. [fn9] With some effort, you can convince Spack to use shorter paths.

   .. warning:: Altering the naming scheme means that Spack will lose track of
      all packages it has installed for you so far.
      That said, the time is right for this kind of customization
      when you are defining a new set of compilers.

   The relevant tunables are:

   1. Set the ``install_tree`` location in ``config.yaml``
      (:ref:`see doc <config-yaml>`).
   2. Set the hash length in ``install-path-scheme``, also in ``config.yaml``
      (:ref:`q.v. <config-yaml>`).
   3. You will want to set the *same* hash length for
      :ref:`module files <modules-projections>`
      if you have Spack produce them for you, under ``projections`` in
      ``modules.yaml``.
.. Copyright 2013-2022 Lawrence Livermore National Security, LLC and other
   Spack Project Developers. See the top-level COPYRIGHT file for details.

   SPDX-License-Identifier: (Apache-2.0 OR MIT)

.. _makefilepackage:

---------------
MakefilePackage
---------------

The most primitive build system a package can use is a plain Makefile.
Makefiles are simple to write for small projects, but they usually
require you to edit the Makefile to set platform and compiler-specific
variables.

^^^^^^
Phases
^^^^^^

The ``MakefilePackage`` base class comes with 3 phases:

#. ``edit`` - edit the Makefile
#. ``build`` - build the project
#. ``install`` - install the project

By default, ``edit`` does nothing, but you can override it to replace
hard-coded Makefile variables. The ``build`` and ``install`` phases
run:

.. code-block:: console

   $ make
   $ make install


^^^^^^^^^^^^^^^
Important files
^^^^^^^^^^^^^^^

The main file that matters for a ``MakefilePackage`` is the Makefile.
This file will be named one of the following ways:

* GNUmakefile (only works with GNU Make)
* Makefile (most common)
* makefile

Some Makefiles also *include* other configuration files. Check for an
``include`` directive in the Makefile.

^^^^^^^^^^^^^^^^^^^^^^^^^
Build system dependencies
^^^^^^^^^^^^^^^^^^^^^^^^^

Spack assumes that the operating system will have a valid ``make`` utility
installed already, so you don't need to add a dependency on ``make``.
However, if the package uses a ``GNUmakefile`` or the developers recommend
using GNU Make, you should add a dependency on ``gmake``:

.. code-block:: python

   depends_on('gmake', type='build')


^^^^^^^^^^^^^^^^^^^^^^^^^^
Types of Makefile packages
^^^^^^^^^^^^^^^^^^^^^^^^^^

Most of the work involved in packaging software that uses Makefiles
involves overriding or replacing hard-coded variables. Many packages
make the mistake of hard-coding compilers, usually for GCC or Intel.
This is fine if you happen to be using that particular compiler, but
Spack is designed to work with *any* compiler, and you need to ensure
that this is the case.

Depending on how the Makefile is designed, there are 4 common strategies
that can be used to set or override the appropriate variables:

"""""""""""""""""""""
Environment variables
"""""""""""""""""""""

Make has multiple types of
`assignment operators <https://www.gnu.org/software/make/manual/make.html#Setting>`_.
Some Makefiles use ``=`` to assign variables. The only way to override
these variables is to edit the Makefile or override them on the
command-line. However, Makefiles that use ``?=`` for assignment honor
environment variables. Since Spack already sets ``CC``, ``CXX``, ``F77``,
and ``FC``, you won't need to worry about setting these variables. If
there are any other variables you need to set, you can do this in the
``edit`` method:

.. code-block:: python

   def edit(self, spec, prefix):
       env['PREFIX'] = prefix
       env['BLASLIB'] = spec['blas'].libs.ld_flags


`cbench <https://github.com/spack/spack/blob/develop/var/spack/repos/builtin/packages/cbench/package.py>`_
is a good example of a simple package that does this, while
`esmf <https://github.com/spack/spack/blob/develop/var/spack/repos/builtin/packages/esmf/package.py>`_
is a good example of a more complex package.

""""""""""""""""""""""
Command-line arguments
""""""""""""""""""""""

If the Makefile ignores environment variables, the next thing to try
is command-line arguments. You can do this by overriding the
``build_targets`` attribute. If you don't need access to the spec,
you can do this like so:

.. code-block:: python

   build_targets = ['CC=cc']


If you do need access to the spec, you can create a property like so:

.. code-block:: python

   @property
   def build_targets(self):
       spec = self.spec

       return [
           'CC=cc',
           'BLASLIB={0}'.format(spec['blas'].libs.ld_flags),
       ]


`cloverleaf <https://github.com/spack/spack/blob/develop/var/spack/repos/builtin/packages/cloverleaf/package.py>`_
is a good example of a package that uses this strategy.

"""""""""""""
Edit Makefile
"""""""""""""

Some Makefiles are just plain stubborn and will ignore command-line
variables. The only way to ensure that these packages build correctly
is to directly edit the Makefile. Spack provides a ``FileFilter`` class
and a ``filter_file`` method to help with this. For example:

.. code-block:: python

   def edit(self, spec, prefix):
       makefile = FileFilter('Makefile')

       makefile.filter(r'^\s*CC\s*=.*',  'CC = '  + spack_cc)
       makefile.filter(r'^\s*CXX\s*=.*', 'CXX = ' + spack_cxx)
       makefile.filter(r'^\s*F77\s*=.*', 'F77 = ' + spack_f77)
       makefile.filter(r'^\s*FC\s*=.*',  'FC = '  + spack_fc)


`stream <https://github.com/spack/spack/blob/develop/var/spack/repos/builtin/packages/stream/package.py>`_
is a good example of a package that involves editing a Makefile to set
the appropriate variables.

"""""""""""
Config file
"""""""""""

More complex packages often involve Makefiles that *include* a
configuration file. These configuration files are primarily composed
of variables relating to the compiler, platform, and the location of
dependencies or names of libraries. Since these config files are
dependent on the compiler and platform, you will often see entire
directories of examples for common compilers and architectures. Use
these examples to help determine what possible values to use.

If the config file is long and only contains one or two variables
that need to be modified, you can use the technique above to edit
the config file. However, if you end up needing to modify most of
the variables, it may be easier to write a new file from scratch.

If each variable is independent of each other, a dictionary works
well for storing variables:

.. code-block:: python

   def edit(self, spec, prefix):
       config = {
           'CC': 'cc',
           'MAKE': 'make',
       }

       if '+blas' in spec:
           config['BLAS_LIBS'] = spec['blas'].libs.joined()

       with open('make.inc', 'w') as inc:
           for key in config:
               inc.write('{0} = {1}\n'.format(key, config[key]))


`elk <https://github.com/spack/spack/blob/develop/var/spack/repos/builtin/packages/elk/package.py>`_
is a good example of a package that uses a dictionary to store
configuration variables.

If the order of variables is important, it may be easier to store
them in a list:

.. code-block:: python

   def edit(self, spec, prefix):
       config = [
           'INSTALL_DIR = {0}'.format(prefix),
           'INCLUDE_DIR = $(INSTALL_DIR)/include',
           'LIBRARY_DIR = $(INSTALL_DIR)/lib',
       ]

       with open('make.inc', 'w') as inc:
           for var in config:
               inc.write('{0}\n'.format(var))


`hpl <https://github.com/spack/spack/blob/develop/var/spack/repos/builtin/packages/hpl/package.py>`_
is a good example of a package that uses a list to store
configuration variables.

^^^^^^^^^^^^^^^^^^^^^^^^^^
Variables to watch out for
^^^^^^^^^^^^^^^^^^^^^^^^^^

The following is a list of common variables to watch out for. The first
two sections are
`implicit variables <https://www.gnu.org/software/make/manual/html_node/Implicit-Variables.html>`_
defined by Make and will always use the same name, while the rest are
user-defined variables and may vary from package to package.

* **Compilers**

  This includes variables such as ``CC``, ``CXX``, ``F77``, ``F90``,
  and ``FC``, as well as variables related to MPI compiler wrappers,
  like ``MPICC`` and friends.

* **Compiler flags**

  This includes variables for specific compilers, like ``CFLAGS``,
  ``CXXFLAGS``, ``F77FLAGS``, ``F90FLAGS``, ``FCFLAGS``, and ``CPPFLAGS``.
  These variables are often hard-coded to contain flags specific to a
  certain compiler. If these flags don't work for every compiler,
  you may want to consider filtering them.

* **Variables that enable or disable features**

  This includes variables like ``MPI``, ``OPENMP``, ``PIC``, and
  ``DEBUG``. These flags often require you to create a variant
  so that you can either build with or without MPI support, for
  example. These flags are often compiler-dependent. You should
  replace them with the appropriate compiler flags, such as
  ``self.compiler.openmp_flag`` or ``self.compiler.pic_flag``.

* **Platform flags**

  These flags control the type of architecture that the executable
  is compiler for. Watch out for variables like ``PLAT`` or ``ARCH``.

* **Dependencies**

  Look out for variables that sound like they could be used to
  locate dependencies, such as ``JAVA_HOME``, ``JPEG_ROOT``, or
  ``ZLIBDIR``. Also watch out for variables that control linking,
  such as ``LIBS``, ``LDFLAGS``, and ``INCLUDES``. These variables
  need to be set to the installation prefix of a dependency, or
  to the correct linker flags to link to that dependency.

* **Installation prefix**

  If your Makefile has an ``install`` target, it needs some way of
  knowing where to install. By default, many packages install to
  ``/usr`` or ``/usr/local``. Since many Spack users won't have
  sudo privileges, it is imperative that each package is installed
  to the proper prefix. Look for variables like ``PREFIX`` or
  ``INSTALL``.

^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Makefiles in a sub-directory
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Not every package places their Makefile in the root of the package
tarball. If the Makefile is in a sub-directory like ``src``, you
can tell Spack where to locate it like so:

.. code-block:: python

   build_directory = 'src'


^^^^^^^^^^^^^^^^^^^
Manual installation
^^^^^^^^^^^^^^^^^^^

Not every Makefile includes an ``install`` target. If this is the
case, you can override the default ``install`` method to manually
install the package:

.. code-block:: python

   def install(self, spec, prefix):
       mkdir(prefix.bin)
       install('foo', prefix.bin)
       install_tree('lib', prefix.lib)


^^^^^^^^^^^^^^^^^^^^^^
External documentation
^^^^^^^^^^^^^^^^^^^^^^

For more information on reading and writing Makefiles, see:
https://www.gnu.org/software/make/manual/make.html
.. Copyright 2013-2022 Lawrence Livermore National Security, LLC and other
   Spack Project Developers. See the top-level COPYRIGHT file for details.

   SPDX-License-Identifier: (Apache-2.0 OR MIT)

.. _custompackage:

--------------------
Custom Build Systems
--------------------

While the built-in build systems should meet your needs for the
vast majority of packages, some packages provide custom build scripts.
This guide is intended for the following use cases:

* Packaging software with its own custom build system
* Adding support for new build systems

If you want to add support for a new build system, a good place to
start is to look at the definitions of other build systems. This guide
focuses mostly on how Spack's build systems work.

In this guide, we will be using the
`perl <https://github.com/spack/spack/blob/develop/var/spack/repos/builtin/packages/perl/package.py>`_ and
`cmake <https://github.com/spack/spack/blob/develop/var/spack/repos/builtin/packages/cmake/package.py>`_
packages as examples. ``perl``'s build system is a hand-written
``Configure`` shell script, while ``cmake`` bootstraps itself during
installation. Both of these packages require custom build systems.

^^^^^^^^^^
Base class
^^^^^^^^^^

If your package does not belong to any of the built-in build
systems that Spack already supports, you should inherit from the
``Package`` base class. ``Package`` is a simple base class with a
single phase: ``install``. If your package is simple, you may be able
to simply write an ``install`` method that gets the job done. However,
if your package is more complex and installation involves multiple
steps, you should add separate phases as mentioned in the next section.

If you are creating a new build system base class, you should inherit
from ``PackageBase``. This is the superclass for all build systems in
Spack.

^^^^^^
Phases
^^^^^^

The most important concept in Spack's build system support is the idea
of phases. Each build system defines a set of phases that are necessary
to install the package. They usually follow some sort of "configure",
"build", "install" guideline, but any of those phases may be missing
or combined with another phase.

If you look at the ``perl`` package, you'll see:

.. code-block:: python

   phases = ['configure', 'build', 'install']

Similarly, ``cmake`` defines:

.. code-block:: python

   phases = ['bootstrap', 'build', 'install']

If we look at the ``cmake`` example, this tells Spack's ``PackageBase``
class to run the ``bootstrap``, ``build``, and ``install`` functions
in that order. It is now up to you to define these methods.

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Phase and phase_args functions
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

If we look at ``perl``, we see that it defines a ``configure`` method:

.. code-block:: python

   def configure(self, spec, prefix):
       configure = Executable('./Configure')
       configure(*self.configure_args())

There is also a corresponding ``configure_args`` function that handles
all of the arguments to pass to ``Configure``, just like in
``AutotoolsPackage``. Comparatively, the ``build`` and ``install``
phases are pretty simple:

.. code-block:: python

   def build(self, spec, prefix):
       make()

   def install(self, spec, prefix):
       make('install')

The ``cmake`` package looks very similar, but with a ``bootstrap``
function instead of ``configure``:

.. code-block:: python

   def bootstrap(self, spec, prefix):
       bootstrap = Executable('./bootstrap')
       bootstrap(*self.bootstrap_args())

   def build(self, spec, prefix):
       make()

   def install(self, spec, prefix):
       make('install')

Again, there is a ``boostrap_args`` function that determines the
correct bootstrap flags to use.

^^^^^^^^^^^^^^^^^^^^
run_before/run_after
^^^^^^^^^^^^^^^^^^^^

Occasionally, you may want to run extra steps either before or after
a given phase. This applies not just to custom build systems, but to
existing build systems as well. You may need to patch a file that is
generated by configure, or install extra files in addition to what
``make install`` copies to the installation prefix. This is where
``@run_before`` and ``@run_after`` come in.

These Python decorators allow you to write functions that are called
before or after a particular phase. For example, in ``perl``, we see:

.. code-block:: python

   @run_after('install')
   def install_cpanm(self):
       spec = self.spec

       if '+cpanm' in spec:
           with working_dir(join_path('cpanm', 'cpanm')):
               perl = spec['perl'].command
               perl('Makefile.PL')
               make()
               make('install')

This extra step automatically installs ``cpanm`` in addition to the
base Perl installation.

^^^^^^^^^^^^^^^^^^^^^
on_package_attributes
^^^^^^^^^^^^^^^^^^^^^

The ``run_before``/``run_after`` logic discussed above becomes
particularly powerful when combined with the ``@on_package_attributes``
decorator. This decorator allows you to conditionally run certain
functions depending on the attributes of that package. The most
common example is conditional testing. Many unit tests are prone to
failure, even when there is nothing wrong with the installation.
Unfortunately, non-portable unit tests and tests that are
"supposed to fail" are more common than we would like. Instead of
always running unit tests on installation, Spack lets users
conditionally run tests with the ``--test=root`` flag.

If we wanted to define a function that would conditionally run
if and only if this flag is set, we would use the following line:

.. code-block:: python

   @on_package_attributes(run_tests=True)

^^^^^^^
Testing
^^^^^^^

Let's put everything together and add unit tests to be optionally run
during the installation of our package.
In the ``perl`` package, we can see:

.. code-block:: python

   @run_after('build')
   @on_package_attributes(run_tests=True)
   def test(self):
       make('test')

As you can guess, this runs ``make test`` *after* building the package,
if and only if testing is requested. Again, this is not specific to
custom build systems, it can be added to existing build systems as well.

.. warning::

   The order of decorators matters. The following ordering:

   .. code-block:: python

      @run_after('install')
      @on_package_attributes(run_tests=True)

   works as expected. However, if you reverse the ordering:

   .. code-block:: python

      @on_package_attributes(run_tests=True)
      @run_after('install')

   the tests will always be run regardless of whether or not
   ``--test=root`` is requested. See https://github.com/spack/spack/issues/3833
   for more information

Ideally, every package in Spack will have some sort of test to ensure
that it was built correctly. It is up to the package authors to make
sure this happens. If you are adding a package for some software and
the developers list commands to test the installation, please add these
tests to your ``package.py``.

For more information on other forms of package testing, refer to
:ref:`Checking an installation <checking_an_installation>`.
.. Copyright 2013-2022 Lawrence Livermore National Security, LLC and other
   Spack Project Developers. See the top-level COPYRIGHT file for details.

   SPDX-License-Identifier: (Apache-2.0 OR MIT)

.. _wafpackage:

----------
WafPackage
----------

Like SCons, Waf is a general-purpose build system that does not rely
on Makefiles to build software.

^^^^^^
Phases
^^^^^^

The ``WafPackage`` base class comes with the following phases:

#. ``configure`` - configure the project
#. ``build`` - build the project
#. ``install`` - install the project

By default, these phases run:

.. code-block:: console

   $ python waf configure --prefix=/path/to/installation/prefix
   $ python waf build
   $ python waf install


Each of these are standard Waf commands and can be found by running:

.. code-block:: console

   $ python waf --help


Each phase provides a ``<phase>`` function that runs:

.. code-block:: console

   $ python waf -j<jobs> <phase>


where ``<jobs>`` is the number of parallel jobs to build with. Each phase
also has a ``<phase_args>`` function that can pass arguments to this call.
All of these functions are empty. The ``configure`` phase
automatically adds  ``--prefix=/path/to/installation/prefix``, so you
don't need to add that in the ``configure_args``.

^^^^^^^
Testing
^^^^^^^

``WafPackage`` also provides ``test`` and ``installtest`` methods,
which are run after the ``build`` and ``install`` phases, respectively.
By default, these phases do nothing, but you can override them to
run package-specific unit tests. For example, the
`py-py2cairo <https://github.com/spack/spack/blob/develop/var/spack/repos/builtin/packages/py-py2cairo/package.py>`_
package uses:

.. code-block:: python

   def installtest(self):
       with working_dir('test'):
           pytest = which('py.test')
           pytest()


^^^^^^^^^^^^^^^
Important files
^^^^^^^^^^^^^^^

Each Waf package comes with a custom ``waf`` build script, written in
Python. This script contains instructions to build the project.

The package also comes with a ``wscript`` file. This file is used to
override the default ``configure``, ``build``, and ``install`` phases
to customize the Waf project. It also allows developers to override
the default ``./waf --help`` message. Check this file to find useful
information about dependencies and the minimum versions that are
supported.

^^^^^^^^^^^^^^^^^^^^^^^^^
Build system dependencies
^^^^^^^^^^^^^^^^^^^^^^^^^

``WafPackage`` does not require ``waf`` to build. ``waf`` is only
needed to create the ``./waf`` script. Since ``./waf`` is a Python
script, Python is needed to build the project. ``WafPackage`` adds
the following dependency automatically:

.. code-block:: python

   depends_on('python@2.5:', type='build')


Waf only supports Python 2.5 and up.

^^^^^^^^^^^^^^^^^^^^^^^^
Passing arguments to waf
^^^^^^^^^^^^^^^^^^^^^^^^

As previously mentioned, each phase comes with a ``<phase_args>``
function that can be used to pass arguments to that particular
phase. For example, if you need to pass arguments to the build
phase, you can use:

.. code-block:: python

   def build_args(self, spec, prefix):
       args = []

       if self.run_tests:
           args.append('--test')

       return args


A list of valid options can be found by running ``./waf --help``.

^^^^^^^^^^^^^^^^^^^^^^
External documentation
^^^^^^^^^^^^^^^^^^^^^^

For more information on the Waf build system, see:
https://waf.io/book/
.. Copyright 2013-2022 Lawrence Livermore National Security, LLC and other
   Spack Project Developers. See the top-level COPYRIGHT file for details.

   SPDX-License-Identifier: (Apache-2.0 OR MIT)

.. _octavepackage:

-------------
OctavePackage
-------------

Octave has its own build system for installing packages.

^^^^^^
Phases
^^^^^^

The ``OctavePackage`` base class has a single phase:

#. ``install`` - install the package

By default, this phase runs the following command:

.. code-block:: console

   $ octave '--eval' 'pkg prefix <prefix>; pkg install <archive_file>'


Beware that uninstallation is not implemented at the moment. After uninstalling
a package via Spack, you also need to manually uninstall it from Octave via
``pkg uninstall <package_name>``.

^^^^^^^^^^^^^^^^^^^^^^^
Finding Octave packages
^^^^^^^^^^^^^^^^^^^^^^^

Most Octave packages are listed at https://octave.sourceforge.io/packages.php.

^^^^^^^^^^^^
Dependencies
^^^^^^^^^^^^

Usually, the homepage of a package will list dependencies, i.e.
``Dependencies:	Octave >= 3.6.0 struct >= 1.0.12``. The same information should
be available in the ``DESCRIPTION`` file in the root of each archive.

^^^^^^^^^^^^^^^^^^^^^^
External Documentation
^^^^^^^^^^^^^^^^^^^^^^

For more information on the Octave build system, see:
https://octave.org/doc/v4.4.0/Installing-and-Removing-Packages.html
.. Copyright 2013-2022 Lawrence Livermore National Security, LLC and other
   Spack Project Developers. See the top-level COPYRIGHT file for details.

   SPDX-License-Identifier: (Apache-2.0 OR MIT)

.. _cmakepackage:

------------
CMakePackage
------------

Like Autotools, CMake is a widely-used build-script generator. Designed
by Kitware, CMake is the most popular build system for new C, C++, and
Fortran projects, and many older projects are switching to it as well.

Unlike Autotools, CMake can generate build scripts for builders other
than Make: Ninja, Visual Studio, etc. It is therefore cross-platform,
whereas Autotools is Unix-only.

^^^^^^
Phases
^^^^^^

The ``CMakePackage`` base class comes with the following phases:

#. ``cmake`` - generate the Makefile
#. ``build`` - build the package
#. ``install`` - install the package

By default, these phases run:

.. code-block:: console

   $ mkdir spack-build
   $ cd spack-build
   $ cmake .. -DCMAKE_INSTALL_PREFIX=/path/to/installation/prefix
   $ make
   $ make test  # optional
   $ make install


A few more flags are passed to ``cmake`` by default, including flags
for setting the build type and flags for locating dependencies. Of
course, you may need to add a few arguments yourself.

^^^^^^^^^^^^^^^
Important files
^^^^^^^^^^^^^^^

A CMake-based package can be identified by the presence of a
``CMakeLists.txt`` file. This file defines the build flags that can be
passed to the cmake invocation, as well as linking instructions. If
you are familiar with CMake, it can prove very useful for determining
dependencies and dependency version requirements.

One thing to look for is the ``cmake_minimum_required`` function:

.. code-block:: cmake

   cmake_minimum_required(VERSION 2.8.12)


This means that CMake 2.8.12 is the earliest release that will work.
You should specify this in a ``depends_on`` statement.

CMake-based packages may also contain ``CMakeLists.txt`` in subdirectories.
This modularization helps to manage complex builds in a hierarchical
fashion. Sometimes these nested ``CMakeLists.txt`` require additional
dependencies not mentioned in the top-level file.

There's also usually a ``cmake`` or ``CMake`` directory containing
additional macros, find scripts, etc. These may prove useful in
determining dependency version requirements.

^^^^^^^^^^^^^^^^^^^^^^^^^
Build system dependencies
^^^^^^^^^^^^^^^^^^^^^^^^^

Every package that uses the CMake build system requires a ``cmake``
dependency. Since this is always the case, the ``CMakePackage`` base
class already contains:

.. code-block:: python

   depends_on('cmake', type='build')


If you need to specify a particular version requirement, you can
override this in your package:

.. code-block:: python

   depends_on('cmake@2.8.12:', type='build')


^^^^^^^^^^^^^^^^^^^
Finding cmake flags
^^^^^^^^^^^^^^^^^^^

To get a list of valid flags that can be passed to ``cmake``, run the
following command in the directory that contains ``CMakeLists.txt``:

.. code-block:: console

   $ cmake . -LAH


CMake will start by checking for compilers and dependencies. Eventually
it will begin to list build options. You'll notice that most of the
build options at the top are prefixed with ``CMAKE_``. You can safely
ignore most of these options as Spack already sets them for you. This
includes flags needed to locate dependencies, RPATH libraries, set the
installation directory, and set the build type.

The rest of the flags are the ones you should consider adding to your
package. They often include flags to enable/disable support for certain
features and locate specific dependencies. One thing you'll notice that
makes CMake different from Autotools is that CMake has an understanding
of build flag hierarchy. That is, certain flags will not display unless
their parent flag has been selected. For example, flags to specify the
``lib`` and ``include`` directories for a package might not appear
unless CMake found the dependency it was looking for. You may need to
manually specify certain flags to explore the full depth of supported
build flags, or check the ``CMakeLists.txt`` yourself.

^^^^^^^^^^^^^^^^^^^^^
Adding flags to cmake
^^^^^^^^^^^^^^^^^^^^^

To add additional flags to the ``cmake`` call, simply override the
``cmake_args`` function. The following example defines values for the flags
``WHATEVER``, ``ENABLE_BROKEN_FEATURE``, ``DETECT_HDF5``, and ``THREADS`` with
and without the :meth:`~spack.build_systems.cmake.CMakePackage.define` and
:meth:`~spack.build_systems.cmake.CMakePackage.define_from_variant` helper functions:

.. code-block:: python

   def cmake_args(self):
       args = [
           '-DWHATEVER:STRING=somevalue',
           self.define('ENABLE_BROKEN_FEATURE', False),
           self.define_from_variant('DETECT_HDF5', 'hdf5'),
           self.define_from_variant('THREADS'), # True if +threads
       ]

       return args

Spack supports CMake defines from conditional variants too. Whenever the condition on
the variant is not met, ``define_from_variant()`` will simply return an empty string,
and CMake simply ignores the empty command line argument. For example the following

.. code-block:: python

   variant('example', default=True, when='@2.0:')

   def cmake_args(self):
      return [self.define_from_variant('EXAMPLE', 'example')]

will generate ``'cmake' '-DEXAMPLE=ON' ...`` when `@2.0: +example` is met, but will
result in ``'cmake' '' ...`` when the spec version is below ``2.0``.


^^^^^^^^^^
Generators
^^^^^^^^^^

CMake and Autotools are build-script generation tools; they "generate"
the Makefiles that are used to build a software package. CMake actually
supports multiple generators, not just Makefiles. Another common
generator is Ninja. To switch to the Ninja generator, simply add:

.. code-block:: python

   generator = 'Ninja'


``CMakePackage`` defaults to "Unix Makefiles". If you switch to the
Ninja generator, make sure to add:

.. code-block:: python

   depends_on('ninja', type='build')

to the package as well. Aside from that, you shouldn't need to do
anything else. Spack will automatically detect that you are using
Ninja and run:

.. code-block:: console

   $ cmake .. -G Ninja
   $ ninja
   $ ninja install

Spack currently only supports "Unix Makefiles" and "Ninja" as valid
generators, but it should be simple to add support for alternative
generators. For more information on CMake generators, see:
https://cmake.org/cmake/help/latest/manual/cmake-generators.7.html

^^^^^^^^^^^^^^^^
CMAKE_BUILD_TYPE
^^^^^^^^^^^^^^^^

Every CMake-based package accepts a ``-DCMAKE_BUILD_TYPE`` flag to
dictate which level of optimization to use. In order to ensure
uniformity across packages, the ``CMakePackage`` base class adds
a variant to control this:

.. code-block:: python

   variant('build_type', default='RelWithDebInfo',
           description='CMake build type',
           values=('Debug', 'Release', 'RelWithDebInfo', 'MinSizeRel'))

However, not every CMake package accepts all four of these options.
Grep the ``CMakeLists.txt`` file to see if the default values are
missing or replaced. For example, the
`dealii <https://github.com/spack/spack/blob/develop/var/spack/repos/builtin/packages/dealii/package.py>`_
package overrides the default variant with:

.. code-block:: python

   variant('build_type', default='DebugRelease',
           description='The build type to build',
           values=('Debug', 'Release', 'DebugRelease'))

For more information on ``CMAKE_BUILD_TYPE``, see:
https://cmake.org/cmake/help/latest/variable/CMAKE_BUILD_TYPE.html

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
CMakeLists.txt in a sub-directory
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Occasionally, developers will hide their source code and ``CMakeLists.txt``
in a subdirectory like ``src``. If this happens, Spack won't
be able to automatically detect the build system properly when running
``spack create``. You will have to manually change the package base
class and tell Spack where ``CMakeLists.txt`` resides. You can do this
like so:

.. code-block:: python

   root_cmakelists_dir = 'src'


Note that this path is relative to the root of the extracted tarball,
not to the ``build_directory``. It defaults to the current directory.

^^^^^^^^^^^^^^^^^^^^^^
Building out of source
^^^^^^^^^^^^^^^^^^^^^^

By default, Spack builds every ``CMakePackage`` in a ``spack-build``
sub-directory. If, for whatever reason, you would like to build in a
different sub-directory, simply override ``build_directory`` like so:

.. code-block:: python

   build_directory = 'my-build'

^^^^^^^^^^^^^^^^^^^^^^^^^
Build and install targets
^^^^^^^^^^^^^^^^^^^^^^^^^

For most CMake packages, the usual:

.. code-block:: console

   $ cmake
   $ make
   $ make install

is sufficient to install the package. However, if you need to run
make with any other targets, for example, to build an optional
library or build the documentation, you can add these like so:

.. code-block:: python

   build_targets = ['all', 'docs']
   install_targets = ['install', 'docs']

^^^^^^^
Testing
^^^^^^^

CMake-based packages typically provide unit testing via the
``test`` target. If you build your software with ``--test=root``,
Spack will check for the presence of a ``test`` target in the
Makefile and run ``make test`` for you. If you want to run a
different test instead, simply override the ``check`` method.

^^^^^^^^^^^^^^^^^^^^^^
External documentation
^^^^^^^^^^^^^^^^^^^^^^

For more information on the CMake build system, see:
https://cmake.org/cmake/help/latest/
.. Copyright 2013-2022 Lawrence Livermore National Security, LLC and other
   Spack Project Developers. See the top-level COPYRIGHT file for details.

   SPDX-License-Identifier: (Apache-2.0 OR MIT)

.. _rpackage:

--------
RPackage
--------

Like Python, R has its own built-in build system.

The R build system is remarkably uniform and well-tested.
This makes it one of the easiest build systems to create
new Spack packages for.

^^^^^^
Phases
^^^^^^

The ``RPackage`` base class has a single phase:

#. ``install`` - install the package

By default, this phase runs the following command:

.. code-block:: console

   $ R CMD INSTALL --library=/path/to/installation/prefix/rlib/R/library .


^^^^^^^^^^^^^^^^^^
Finding R packages
^^^^^^^^^^^^^^^^^^

The vast majority of R packages are hosted on CRAN - The Comprehensive
R Archive Network. If you are looking for a particular R package, search
for "CRAN <package-name>" and you should quickly find what you want.
If it isn't on CRAN, try Bioconductor, another common R repository.

For the purposes of this tutorial, we will be walking through
`r-caret <https://github.com/spack/spack/blob/develop/var/spack/repos/builtin/packages/r-caret/package.py>`_
as an example. If you search for "CRAN caret", you will quickly find what
you are looking for at https://cran.r-project.org/package=caret.
https://cran.r-project.org is the main CRAN website. However, CRAN also
has a https://cloud.r-project.org site that automatically redirects to
`mirrors around the world <https://cloud.r-project.org/mirrors.html>`_.
For stability and performance reasons, we will use https://cloud.r-project.org/package=caret.
If you search for "Package source", you will find the download URL for
the latest release. Use this URL with ``spack create`` to create a new
package.

^^^^^^^^^^^^
Package name
^^^^^^^^^^^^

The first thing you'll notice is that Spack prepends ``r-`` to the front
of the package name. This is how Spack separates R package extensions
from the rest of the packages in Spack. Without this, we would end up
with package name collisions more frequently than we would like. For
instance, there are already packages for both:

* ``ape`` and ``r-ape``
* ``curl`` and ``r-curl``
* ``gmp`` and ``r-gmp``
* ``jpeg`` and ``r-jpeg``
* ``openssl`` and ``r-openssl``
* ``uuid`` and ``r-uuid``
* ``xts`` and ``r-xts``

Many popular programs written in C/C++ are later ported to R as a
separate project.

^^^^^^^^^^^
Description
^^^^^^^^^^^

The first thing you'll need to add to your new package is a description.
The top of the homepage for ``caret`` lists the following description:

   Classification and Regression Training

   Misc functions for training and plotting classification and regression models.

The first line is a short description (title) and the second line is a long
description. In this case the description is only one line but often the
description is several lines. Spack makes use of both short and long
descriptions and convention is to use both when creating an R  package.

^^^^^^^^
Homepage
^^^^^^^^

If you look at the bottom of the page, you'll see:

   Linking:

   Please use the canonical form https://CRAN.R-project.org/package=caret to link to this page.

Please uphold the wishes of the CRAN admins and use
https://cloud.r-project.org/package=caret as the homepage instead of
https://cloud.r-project.org/web/packages/caret/index.html. The latter may
change without notice.

^^^
URL
^^^

As previously mentioned, the download URL for the latest release can be
found by searching "Package source" on the homepage.

^^^^^^^^
List URL
^^^^^^^^

CRAN maintains a single webpage containing the latest release of every
single package: https://cloud.r-project.org/src/contrib/

Of course, as soon as a new release comes out, the version you were using
in your package is no longer available at that URL. It is moved to an
archive directory. If you search for "Old sources", you will find:
https://cloud.r-project.org/src/contrib/Archive/caret

If you only specify the URL for the latest release, your package will
no longer be able to fetch that version as soon as a new release comes
out. To get around this, add the archive directory as a ``list_url``.

^^^^^^^^^^^^^^^^^^^^^
Bioconductor packages
^^^^^^^^^^^^^^^^^^^^^

Bioconductor packages are set up in a similar way to CRAN packages, but there
are some very important distinctions. Bioconductor packages can be found at:
https://bioconductor.org/. Bioconductor packages are R packages and so follow
the same packaging scheme as CRAN packages. What is different is that
Bioconductor itself is versioned and released. This scheme, using the
Bioconductor package installer, allows further specification of the minimum
version of R as well as further restrictions on the dependencies between
packages than what is possible with the native R packaging system. Spack can
not replicate these extra features and thus Bioconductor packages in Spack need
to be managed as a group during updates in order to maintain package
consistency with Bioconductor itself.

Another key difference is that, while previous versions of packages are
available, they are not available from a site that can be programmatically set,
thus a ``list_url`` attribute can not be used. However, each package is also
available in a git repository, with branches corresponding to each Bioconductor
release. Thus, it is always possible to retrieve the version of any package
corresponding to a Bioconductor release simply by fetching the branch that
corresponds to the Bioconductor release of the package repository. For this
reason, spack Bioconductor R packages use the git repository, with the commit
of the respective branch used in the ``version()`` attribute of the package.

^^^^^^^^^^^^^^^^^^^^^^^^
cran and bioc attributes
^^^^^^^^^^^^^^^^^^^^^^^^

Much like the ``pypi`` attribute for python packages, due to the fact that R
packages are obtained from specific repositories, it is possible to set up shortcut
attributes that can be used to set ``homepage``, ``url``, ``list_url``, and
``git``. For example, the following ``cran`` attribute:

.. code-block:: python

   cran = 'caret'

is equivalent to:

.. code-block:: python

   homepage = 'https://cloud.r-project.org/package=caret'
   url      = 'https://cloud.r-project.org/src/contrib/caret_6.0-86.tar.gz'
   list_url = 'https://cloud.r-project.org/src/contrib/Archive/caret'

Likewise, the following ``bioc`` attribute:

.. code-block:: python

   bioc = 'BiocVersion'

is equivalent to:

.. code-block:: python

   homepage = 'https://bioconductor.org/packages/BiocVersion/'
   git      = 'https://git.bioconductor.org/packages/BiocVersion'


^^^^^^^^^^^^^^^^^^^^^^^^^
Build system dependencies
^^^^^^^^^^^^^^^^^^^^^^^^^

As an extension of the R ecosystem, your package will obviously depend
on R to build and run. Normally, we would use ``depends_on`` to express
this, but for R packages, we use ``extends``. ``extends`` is similar to
``depends_on``, but adds an additional feature: the ability to "activate"
the package by symlinking it to the R installation directory. Since
every R package needs this, the ``RPackage`` base class contains:

.. code-block:: python

   extends('r')


Take a close look at the homepage for ``caret``. If you look at the
"Depends" section, you'll notice that ``caret`` depends on "R (â‰¥ 3.2.0)".
You should add this to your package like so:

.. code-block:: python

   depends_on('r@3.2.0:', type=('build', 'run'))


^^^^^^^^^^^^^^
R dependencies
^^^^^^^^^^^^^^

R packages are often small and follow the classic Unix philosophy
of doing one thing well. They are modular and usually depend on
several other packages. You may find a single package with over a
hundred dependencies. Luckily, R packages are well-documented
and list all of their dependencies in the following sections:

* Depends
* Imports
* LinkingTo

As far as Spack is concerned, all 3 of these dependency types
correspond to ``type=('build', 'run')``, so you don't have to worry
about the details. If you are curious what they mean,
https://github.com/spack/spack/issues/2951 has a pretty good summary:

   ``Depends`` is required and will cause those R packages to be *attached*,
   that is, their APIs are exposed to the user. ``Imports`` *loads* packages
   so that *the package* importing these packages can access their APIs,
   while *not* being exposed to the user. When a user calls ``library(foo)``
   s/he *attaches* package ``foo`` and all of the packages under ``Depends``.
   Any function in one of these package can be called directly as ``bar()``.
   If there are conflicts, user can also specify ``pkgA::bar()`` and
   ``pkgB::bar()`` to distinguish between them. Historically, there was only
   ``Depends`` and ``Suggests``, hence the confusing names. Today, maybe
   ``Depends`` would have been named ``Attaches``.

   The ``LinkingTo`` is not perfect and there was recently an extensive
   discussion about API/ABI among other things on the R-devel mailing
   list among very skilled R developers:

   * https://stat.ethz.ch/pipermail/r-devel/2016-December/073505.html
   * https://stat.ethz.ch/pipermail/r-devel/2017-January/073647.html

Some packages also have a fourth section:

* Suggests

These are optional, rarely-used dependencies that a user might find
useful. You should **NOT** add these dependencies to your package.
R packages already have enough dependencies as it is, and adding
optional dependencies can really slow down the concretization
process. They can also introduce circular dependencies.

A fifth rarely used section is:

* Enhances

This means that the package can be used as an optional dependency
for another package. Again, these packages should **NOT** be listed
as dependencies.

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Core, recommended, and non-core packages
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

If you look at "Depends", "Imports", and "LinkingTo", you will notice
3 different types of packages:

"""""""""""""
Core packages
"""""""""""""

If you look at the ``caret`` homepage, you'll notice a few dependencies
that don't have a link to the package, like ``methods``, ``stats``, and
``utils``. These packages are part of the core R distribution and are
tied to the R version installed. You can basically consider these to be
"R itself". These are so essential to R that it would not make sense for
them to be updated via CRAN. If you did, you would basically get a different
version of R. Thus, they're updated when R is updated.

You can find a list of these core libraries at:
https://github.com/wch/r-source/tree/trunk/src/library

""""""""""""""""""""
Recommended packages
""""""""""""""""""""

When you install R, there is an option called ``--with-recommended-packages``.
This flag causes the R installation to include a few "Recommended" packages
(legacy term). They are for historical reasons quite tied to the core R
distribution, developed by the R core team or people closely related to it.
The R core distribution "knows" about these package, but they are indeed
distributed via CRAN. Because they're distributed via CRAN, they can also be
updated between R version releases.

Spack explicitly adds the ``--without-recommended-packages`` flag to prevent
the installation of these packages. Due to the way Spack handles package
activation (symlinking packages to the R installation directory),
pre-existing recommended packages will cause conflicts for already-existing
files. We could either not include these recommended packages in Spack and
require them to be installed through ``--with-recommended-packages``, or
we could not install them with R and let users choose the version of the
package they want to install. We chose the latter.

Since these packages are so commonly distributed with the R system, many
developers may assume these packages exist and fail to list them as
dependencies. Watch out for this.

You can find a list of these recommended packages at:
https://github.com/wch/r-source/blob/trunk/share/make/vars.mk

"""""""""""""""""
Non-core packages
"""""""""""""""""

These are packages that are neither "core" nor "recommended". There are more
than 10,000 of these packages hosted on CRAN alone.

For each of these package types, if you see that a specific version is
required, for example, "lattice (â‰¥ 0.20)", please add this information to
the dependency:

.. code-block:: python

   depends_on('r-lattice@0.20:', type=('build', 'run'))


^^^^^^^^^^^^^^^^^^
Non-R dependencies
^^^^^^^^^^^^^^^^^^

Some packages depend on non-R libraries for linking. Check out the
`r-stringi <https://github.com/spack/spack/blob/develop/var/spack/repos/builtin/packages/r-stringi/package.py>`_
package for an example: https://cloud.r-project.org/package=stringi.
If you search for the text "SystemRequirements", you will see:

   ICU4C (>= 52, optional)

This is how non-R dependencies are listed. Make sure to add these
dependencies. The default dependency type should suffice.

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Passing arguments to the installation
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Some R packages provide additional flags that can be passed to
``R CMD INSTALL``, often to locate non-R dependencies.
`r-rmpi <https://github.com/spack/spack/blob/develop/var/spack/repos/builtin/packages/r-rmpi/package.py>`_
is an example of this, and flags for linking to an MPI library. To pass
these to the installation command, you can override ``configure_args``
like so:

.. code-block:: python

   def configure_args(self):
       mpi_name = self.spec['mpi'].name

       # The type of MPI. Supported values are:
       # OPENMPI, LAM, MPICH, MPICH2, or CRAY
       if mpi_name == 'openmpi':
           Rmpi_type = 'OPENMPI'
       elif mpi_name == 'mpich':
           Rmpi_type = 'MPICH2'
       else:
           raise InstallError('Unsupported MPI type')

       return [
           '--with-Rmpi-type={0}'.format(Rmpi_type),
           '--with-mpi={0}'.format(spec['mpi'].prefix),
       ]


There is a similar ``configure_vars`` function that can be overridden
to pass variables to the build.

^^^^^^^^^^^^^^^^^^^^^
Alternatives to Spack
^^^^^^^^^^^^^^^^^^^^^

CRAN hosts over 10,000 R packages, most of which are not in Spack. Many
users may not need the advanced features of Spack, and may prefer to
install R packages the normal way:

.. code-block:: console

   $ R
   > install.packages("ggplot2")


R will search CRAN for the ``ggplot2`` package and install all necessary
dependencies for you. If you want to update all installed R packages to
the latest release, you can use:

.. code-block:: console

   > update.packages(ask = FALSE)


This works great for users who have internet access, but those on an
air-gapped cluster will find it easier to let Spack build a download
mirror and install these packages for you.

Where Spack really shines is its ability to install non-R dependencies
and link to them properly, something the R installation mechanism
cannot handle.

^^^^^^^^^^^^^^^^^^^^^^
External documentation
^^^^^^^^^^^^^^^^^^^^^^

For more information on installing R packages, see:
https://stat.ethz.ch/R-manual/R-devel/library/utils/html/INSTALL.html

For more information on writing R packages, see:
https://cloud.r-project.org/doc/manuals/r-release/R-exts.html

In particular,
https://cloud.r-project.org/doc/manuals/r-release/R-exts.html#Package-Dependencies
has a great explanation of the difference between Depends, Imports,
and LinkingTo.
.. Copyright 2013-2022 Lawrence Livermore National Security, LLC and other
   Spack Project Developers. See the top-level COPYRIGHT file for details.

   SPDX-License-Identifier: (Apache-2.0 OR MIT)

.. _sconspackage:

------------
SConsPackage
------------

SCons is a general-purpose build system that does not rely on
Makefiles to build software. SCons is written in Python, and handles
all building and linking itself.

As far as build systems go, SCons is very non-uniform. It provides a
common framework for developers to write build scripts, but the build
scripts themselves can vary drastically. Some developers add subcommands
like:

.. code-block:: console

   $ scons clean
   $ scons build
   $ scons test
   $ scons install


Others don't add any subcommands. Some have configuration options that
can be specified through variables on the command line. Others don't.

^^^^^^
Phases
^^^^^^

As previously mentioned, SCons allows developers to add subcommands like
``build`` and ``install``, but by default, installation usually looks like:

.. code-block:: console

   $ scons
   $ scons install


To facilitate this, the ``SConsPackage`` base class provides the
following phases:

#. ``build`` - build the package
#. ``install`` - install the package

Package developers often add unit tests that can be invoked with
``scons test`` or ``scons check``. Spack provides a ``test`` method
to handle this. Since we don't know which one the package developer
chose, the ``test`` method does nothing by default, but can be easily
overridden like so:

.. code-block:: python

   def test(self):
       scons('check')


^^^^^^^^^^^^^^^
Important files
^^^^^^^^^^^^^^^

SCons packages can be identified by their ``SConstruct`` files. These
files handle everything from setting up subcommands and command-line
options to linking and compiling.

One thing to look for is the ``EnsureSConsVersion`` function:

.. code-block:: none

   EnsureSConsVersion(2, 3, 0)


This means that SCons 2.3.0 is the earliest release that will work.
You should specify this in a ``depends_on`` statement.

^^^^^^^^^^^^^^^^^^^^^^^^^
Build system dependencies
^^^^^^^^^^^^^^^^^^^^^^^^^

At the bare minimum, packages that use the SCons build system need a
``scons`` dependency. Since this is always the case, the ``SConsPackage``
base class already contains:

.. code-block:: python

   depends_on('scons', type='build')


If you want to specify a particular version requirement, you can override
this in your package:

.. code-block:: python

   depends_on('scons@2.3.0:', type='build')


^^^^^^^^^^^^^^^^^^^^^^^^^
Finding available options
^^^^^^^^^^^^^^^^^^^^^^^^^

The first place to start when looking for a list of valid options to
build a package is ``scons --help``. Some packages like
`kahip <https://github.com/spack/spack/blob/develop/var/spack/repos/builtin/packages/kahip/package.py>`_
don't bother overwriting the default SCons help message, so this isn't
very useful, but other packages like
`serf <https://github.com/spack/spack/blob/develop/var/spack/repos/builtin/packages/serf/package.py>`_
print a list of valid command-line variables:

.. code-block:: console

   $ scons --help
   scons: Reading SConscript files ...
   Checking for GNU-compatible C compiler...yes
   scons: done reading SConscript files.

   PREFIX: Directory to install under ( /path/to/PREFIX )
       default: /usr/local
       actual: /usr/local

   LIBDIR: Directory to install architecture dependent libraries under ( /path/to/LIBDIR )
       default: $PREFIX/lib
       actual: /usr/local/lib

   APR: Path to apr-1-config, or to APR's install area ( /path/to/APR )
       default: /usr
       actual: /usr

   APU: Path to apu-1-config, or to APR's install area ( /path/to/APU )
       default: /usr
       actual: /usr

   OPENSSL: Path to OpenSSL's install area ( /path/to/OPENSSL )
       default: /usr
       actual: /usr

   ZLIB: Path to zlib's install area ( /path/to/ZLIB )
       default: /usr
       actual: /usr

   GSSAPI: Path to GSSAPI's install area ( /path/to/GSSAPI )
       default: None
       actual: None

   DEBUG: Enable debugging info and strict compile warnings (yes|no)
       default: False
       actual: False

   APR_STATIC: Enable using a static compiled APR (yes|no)
       default: False
       actual: False

   CC: Command name or path of the C compiler
       default: None
       actual: gcc

   CFLAGS: Extra flags for the C compiler (space-separated)
       default: None
       actual:

   LIBS: Extra libraries passed to the linker, e.g. "-l<library1> -l<library2>" (space separated)
       default: None
       actual: None

   LINKFLAGS: Extra flags for the linker (space-separated)
       default: None
       actual:

   CPPFLAGS: Extra flags for the C preprocessor (space separated)
       default: None
       actual: None

   Use scons -H for help about command-line options.


More advanced packages like
`cantera <https://github.com/spack/spack/blob/develop/var/spack/repos/builtin/packages/cantera/package.py>`_
use ``scons --help`` to print a list of subcommands:

.. code-block:: console

   $ scons --help
   scons: Reading SConscript files ...

   SCons build script for Cantera

   Basic usage:
       'scons help' - print a description of user-specifiable options.

       'scons build' - Compile Cantera and the language interfaces using
                       default options.

       'scons clean' - Delete files created while building Cantera.

       '[sudo] scons install' - Install Cantera.

       '[sudo] scons uninstall' - Uninstall Cantera.

       'scons test' - Run all tests which did not previously pass or for which the
                      results may have changed.

       'scons test-reset' - Reset the passing status of all tests.

       'scons test-clean' - Delete files created while running the tests.

       'scons test-help' - List available tests.

       'scons test-NAME' - Run the test named "NAME".

       'scons <command> dump' - Dump the state of the SCons environment to the
                                screen instead of doing <command>, e.g.
                                'scons build dump'. For debugging purposes.

       'scons samples' - Compile the C++ and Fortran samples.

       'scons msi' - Build a Windows installer (.msi) for Cantera.

       'scons sphinx' - Build the Sphinx documentation

       'scons doxygen' - Build the Doxygen documentation


You'll notice that cantera provides a ``scons help`` subcommand. Running
``scons help`` prints a list of valid command-line variables.

^^^^^^^^^^^^^^^^^^^^^^^^^^
Passing arguments to scons
^^^^^^^^^^^^^^^^^^^^^^^^^^

Now that you know what arguments the project accepts, you can add them to
the package build phase. This is done by overriding ``build_args`` like so:

.. code-block:: python

   def build_args(self, spec, prefix):
       args = [
         'PREFIX={0}'.format(prefix),
         'ZLIB={0}'.format(spec['zlib'].prefix),
       ]

       if '+debug' in spec:
           args.append('DEBUG=yes')
       else:
           args.append('DEBUG=no')

       return args


``SConsPackage`` also provides an ``install_args`` function that you can
override to pass additional arguments to ``scons install``.

^^^^^^^^^^^^^^^^^
Compiler wrappers
^^^^^^^^^^^^^^^^^

By default, SCons builds all packages in a separate execution environment,
and doesn't pass any environment variables from the user environment.
Even changes to ``PATH`` are not propagated unless the package developer
does so.

This is particularly troublesome for Spack's compiler wrappers, which depend
on environment variables to manage dependencies and linking flags. In many
cases, SCons packages are not compatible with Spack's compiler wrappers,
and linking must be done manually.

First of all, check the list of valid options for anything relating to
environment variables. For example, cantera has the following option:

.. code-block:: none

   * env_vars: [ string ]
       Environment variables to propagate through to SCons. Either the
       string "all" or a comma separated list of variable names, e.g.
       'LD_LIBRARY_PATH,HOME'.
       - default: 'LD_LIBRARY_PATH,PYTHONPATH'


In the case of cantera, using ``env_vars=all`` allows us to use
Spack's compiler wrappers. If you don't see an option related to
environment variables, try using Spack's compiler wrappers by passing
``spack_cc``, ``spack_cxx``, and ``spack_fc`` via the ``CC``, ``CXX``,
and ``FC`` arguments, respectively. If you pass them to the build and
you see an error message like:

.. code-block:: none

   Spack compiler must be run from Spack! Input 'SPACK_PREFIX' is missing.


you'll know that the package isn't compatible with Spack's compiler
wrappers. In this case, you'll have to use the path to the actual
compilers, which are stored in ``self.compiler.cc`` and friends.
Note that this may involve passing additional flags to the build to
locate dependencies, a task normally done by the compiler wrappers.
serf is an example of a package with this limitation.

^^^^^^^^^^^^^^^^^^^^^^
External documentation
^^^^^^^^^^^^^^^^^^^^^^

For more information on the SCons build system, see:
http://scons.org/documentation.html
.. Copyright 2013-2022 Lawrence Livermore National Security, LLC and other
   Spack Project Developers. See the top-level COPYRIGHT file for details.

   SPDX-License-Identifier: (Apache-2.0 OR MIT)

.. _inteloneapipackage:


====================
 IntelOneapiPackage
====================


.. contents::


oneAPI packages in Spack
========================

Spack can install and use the Intel oneAPI products. You may either
use spack to install the oneAPI tools or use the `Intel
installers`_. After installation, you may use the tools directly, or
use Spack to build packages with the tools.

The Spack Python class ``IntelOneapiPackage`` is a base class that is
used by ``IntelOneapiCompilers``, ``IntelOneapiMkl``,
``IntelOneapiTbb`` and other classes to implement the oneAPI
packages. See the :ref:`package-list` for the full list of available
oneAPI packages or use::

  spack list -d oneAPI

For more information on a specific package, do::

  spack info <package-name>

Intel no longer releases new versions of Parallel Studio, which can be
used in Spack via the :ref:`intelpackage`. All of its components can
now be found in oneAPI. 

Examples
========

Building a Package With icx
---------------------------

In this example, we build patchelf with ``icc`` and ``icx``. The
compilers are installed with spack.

Install the oneAPI compilers::

  spack install intel-oneapi-compilers

Add the compilers to your ``compilers.yaml`` so spack can use them::

  spack compiler add `spack location -i intel-oneapi-compilers`/compiler/latest/linux/bin/intel64
  spack compiler add `spack location -i intel-oneapi-compilers`/compiler/latest/linux/bin

Verify that the compilers are available::

  spack compiler list

The ``intel-oneapi-compilers`` package includes 2 families of
compilers:

* ``intel``: ``icc``, ``icpc``, ``ifort``. Intel's *classic*
  compilers.
* ``oneapi``: ``icx``, ``icpx``, ``ifx``. Intel's new generation of
  compilers based on LLVM.

To build the ``patchelf`` Spack package with ``icc``, do::

  spack install patchelf%intel

To build with with ``icx``, do ::

  spack install patchelf%oneapi

Using oneAPI MPI to Satisfy a Virtual Dependence
------------------------------------------------------

The ``hdf5`` package works with any compatible MPI implementation. To
build ``hdf5`` with Intel oneAPI MPI do::

  spack install hdf5 +mpi ^intel-oneapi-mpi

Using an Externally Installed oneAPI
====================================

Spack can also use oneAPI tools that are manually installed with
`Intel Installers`_.  The procedures for configuring Spack to use
external compilers and libraries are different.

Compilers
---------

To use the compilers, add some information about the installation to
``compilers.yaml``. For most users, it is sufficient to do::

  spack compiler add /opt/intel/oneapi/compiler/latest/linux/bin/intel64
  spack compiler add /opt/intel/oneapi/compiler/latest/linux/bin

Adapt the paths above if you did not install the tools in the default
location. After adding the compilers, using them is the same
as if you had installed the ``intel-oneapi-compilers`` package.
Another option is to manually add the configuration to
``compilers.yaml`` as described in :ref:`Compiler configuration
<compiler-config>`.

Libraries
---------

If you want Spack to use MKL that you have installed without Spack in
the default location, then add the following to
``~/.spack/packages.yaml``, adjusting the version as appropriate::

  intel-oneapi-mkl:
    externals:
    - spec: intel-oneapi-mkl@2021.1.1
      prefix: /opt/intel/oneapi/


Using oneAPI Tools Installed by Spack
=====================================

Spack can be a convenient way to install and configure compilers and
libaries, even if you do not intend to build a Spack package. If you
want to build a Makefile project using Spack-installed oneAPI compilers,
then use spack to configure your environment::

  spack load intel-oneapi-compilers

And then you can build with::

  CXX=icpx make

You can also use Spack-installed libraries. For example::

  spack load intel-oneapi-mkl

Will update your environment CPATH, LIBRARY_PATH, and other
environment variables for building an application with MKL.

More information
================

This section describes basic use of oneAPI, especially if it has
changed compared to Parallel Studio. See :ref:`intelpackage` for more
information on :ref:`intel-virtual-packages`,
:ref:`intel-unrelated-packages`,
:ref:`intel-integrating-external-libraries`, and
:ref:`using-mkl-tips`.


.. _`Intel installers`: https://software.intel.com/content/www/us/en/develop/documentation/installation-guide-for-intel-oneapi-toolkits-linux/top.html
.. Copyright 2013-2022 Lawrence Livermore National Security, LLC and other
   Spack Project Developers. See the top-level COPYRIGHT file for details.

   SPDX-License-Identifier: (Apache-2.0 OR MIT)

.. _perlpackage:

-----------
PerlPackage
-----------

Much like Octave, Perl has its own language-specific
build system.

^^^^^^
Phases
^^^^^^

The ``PerlPackage`` base class comes with 3 phases that can be overridden:

#. ``configure`` - configure the package
#. ``build`` - build the package
#. ``install`` - install the package

Perl packages have 2 common modules used for module installation:

"""""""""""""""""""""""
``ExtUtils::MakeMaker``
"""""""""""""""""""""""

The ``ExtUtils::MakeMaker`` module is just what it sounds like, a module
designed to generate Makefiles. It can be identified by the presence of
a ``Makefile.PL`` file, and has the following installation steps:

.. code-block:: console

   $ perl Makefile.PL INSTALL_BASE=/path/to/installation/prefix
   $ make
   $ make test  # optional
   $ make install


"""""""""""""""""
``Module::Build``
"""""""""""""""""

The ``Module::Build`` module is a pure-Perl build system, and can be
identified by the presence of a ``Build.PL`` file. It has the following
installation steps:

.. code-block:: console

   $ perl Build.PL --install_base /path/to/installation/prefix
   $ ./Build
   $ ./Build test  # optional
   $ ./Build install


If both ``Makefile.PL`` and ``Build.PL`` files exist in the package,
Spack will use ``Makefile.PL`` by default. If your package uses a
different module, ``PerlPackage`` will need to be extended to support
it.

``PerlPackage`` automatically detects which build steps to use, so there
shouldn't be much work on the package developer's side to get things
working.

^^^^^^^^^^^^^^^^^^^^^
Finding Perl packages
^^^^^^^^^^^^^^^^^^^^^

Most Perl modules are hosted on CPAN - The Comprehensive Perl Archive
Network. If you need to find a package for ``XML::Parser``, for example,
you should search for "CPAN XML::Parser".

Some CPAN pages are versioned. Check for a link to the
"Latest Release" to make sure you have the latest version.

^^^^^^^^^^^^
Package name
^^^^^^^^^^^^

When you use ``spack create`` to create a new Perl package, Spack will
automatically prepend ``perl-`` to the front of the package name. This
helps to keep Perl modules separate from other packages. The same
naming scheme is used for other language extensions, like Python and R.

^^^^^^^^^^^
Description
^^^^^^^^^^^

Most CPAN pages have a short description under "NAME" and a longer
description under "DESCRIPTION". Use whichever you think is more
useful while still being succinct.

^^^^^^^^
Homepage
^^^^^^^^

In the top-right corner of the CPAN page, you'll find a "permalink"
for the package. This should be used instead of the current URL, as
it doesn't contain the version number and will always link to the
latest release.

^^^
URL
^^^

If you haven't found it already, the download URL is on the right
side of the page below the permalink. Search for "Download".

^^^^^^^^^^^^^^^^^^^^^^^^^
Build system dependencies
^^^^^^^^^^^^^^^^^^^^^^^^^

Every ``PerlPackage`` obviously depends on Perl at build and run-time,
so ``PerlPackage`` contains:

.. code-block:: python

   extends('perl')


If your package requires a specific version of Perl, you should
specify this.

Although newer versions of Perl include ``ExtUtils::MakeMaker`` and
``Module::Build`` as "core" modules, you may want to add dependencies
on ``perl-extutils-makemaker`` and ``perl-module-build`` anyway. Many
people add Perl as an external package, and we want the build to work
properly. If your package uses ``Makefile.PL`` to build, add:

.. code-block:: python

   depends_on('perl-extutils-makemaker', type='build')


If your package uses ``Build.PL`` to build, add:

.. code-block:: python

   depends_on('perl-module-build', type='build')


^^^^^^^^^^^^^^^^^
Perl dependencies
^^^^^^^^^^^^^^^^^

Below the download URL, you will find a "Dependencies" link, which
takes you to a page listing all of the dependencies of the package.
Packages listed as "Core module" don't need to be added as dependencies,
but all direct dependencies should be added. Don't add dependencies of
dependencies. These should be added as dependencies to the dependency,
not to your package.

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Passing arguments to configure
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Packages that have non-Perl dependencies often use command-line
variables to specify their installation directory. You can pass
arguments to ``Makefile.PL`` or ``Build.PL`` by overriding
``configure_args`` like so:

.. code-block:: python

   def configure_args(self):
       expat = self.spec['expat'].prefix

       return [
           'EXPATLIBPATH={0}'.format(expat.lib),
           'EXPATINCPATH={0}'.format(expat.include),
       ]


^^^^^^^^^^^^^^^^^^^^^
Alternatives to Spack
^^^^^^^^^^^^^^^^^^^^^

If you need to maintain a stack of Perl modules for a user and don't
want to add all of them to Spack, a good alternative is ``cpanm``.
If Perl is already installed on your system, it should come with a
``cpan`` executable. To install ``cpanm``, run the following command:

.. code-block:: console

   $ cpan App::cpanminus


Now, you can install any Perl module you want by running:

.. code-block:: console

   $ cpanm Module::Name


Obviously, these commands can only be run if you have root privileges.
Furthermore, ``cpanm`` is not capable of installing non-Perl dependencies.
If you need to install to your home directory or need to install a module
with non-Perl dependencies, Spack is a better option.

^^^^^^^^^^^^^^^^^^^^^^
External documentation
^^^^^^^^^^^^^^^^^^^^^^

You can find more information on installing Perl modules from source
at: http://www.perlmonks.org/?node_id=128077

More generic Perl module installation instructions can be found at:
http://www.cpan.org/modules/INSTALL.html
.. Copyright 2013-2022 Lawrence Livermore National Security, LLC and other
   Spack Project Developers. See the top-level COPYRIGHT file for details.

   SPDX-License-Identifier: (Apache-2.0 OR MIT)

.. _sippackage:

----------
SIPPackage
----------

SIP is a tool that makes it very easy to create Python bindings for C and C++
libraries. It was originally developed to create PyQt, the Python bindings for
the Qt toolkit, but can be used to create bindings for any C or C++ library.

SIP comprises a code generator and a Python module. The code generator
processes a set of specification files and generates C or C++ code which is
then compiled to create the bindings extension module. The SIP Python module
provides support functions to the automatically generated code.

^^^^^^
Phases
^^^^^^

The ``SIPPackage`` base class comes with the following phases:

#. ``configure`` - configure the package
#. ``build`` - build the package
#. ``install`` - install the package

By default, these phases run:

.. code-block:: console

   $ python configure.py --bindir ... --destdir ...
   $ make
   $ make install


^^^^^^^^^^^^^^^
Important files
^^^^^^^^^^^^^^^

Each SIP package comes with a custom ``configure.py`` build script,
written in Python. This script contains instructions to build the project.

^^^^^^^^^^^^^^^^^^^^^^^^^
Build system dependencies
^^^^^^^^^^^^^^^^^^^^^^^^^

``SIPPackage`` requires several dependencies. Python is needed to run
the ``configure.py`` build script, and to run the resulting Python
libraries. Qt is needed to provide the ``qmake`` command. SIP is also
needed to build the package. All of these dependencies are automatically
added via the base class

.. code-block:: python

   extends('python')

   depends_on('qt', type='build')

   depends_on('py-sip', type='build')

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Passing arguments to ``configure.py``
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Each phase comes with a ``<phase_args>`` function that can be used to pass
arguments to that particular phase. For example, if you need to pass
arguments to the configure phase, you can use:

.. code-block:: python

   def configure_args(self, spec, prefix):
       return ['--no-python-dbus']


A list of valid options can be found by running ``python configure.py --help``.

^^^^^^^
Testing
^^^^^^^

Just because a package successfully built does not mean that it built
correctly. The most reliable test of whether or not the package was
correctly installed is to attempt to import all of the modules that
get installed. To get a list of modules, run the following command
in the site-packages directory:

.. code-block:: console

   $ python
   >>> import setuptools
   >>> setuptools.find_packages()
   [
       'PyQt5', 'PyQt5.QtCore', 'PyQt5.QtGui', 'PyQt5.QtHelp',
       'PyQt5.QtMultimedia', 'PyQt5.QtMultimediaWidgets', 'PyQt5.QtNetwork',
       'PyQt5.QtOpenGL', 'PyQt5.QtPrintSupport', 'PyQt5.QtQml',
       'PyQt5.QtQuick', 'PyQt5.QtSvg', 'PyQt5.QtTest', 'PyQt5.QtWebChannel',
       'PyQt5.QtWebSockets', 'PyQt5.QtWidgets', 'PyQt5.QtXml',
       'PyQt5.QtXmlPatterns'
    ]


Large, complex packages like ``py-pyqt5`` will return a long list of
packages, while other packages may return an empty list. These packages
only install a single ``foo.py`` file. In Python packaging lingo,
a "package" is a directory containing files like:

.. code-block:: none

   foo/__init__.py
   foo/bar.py
   foo/baz.py


whereas a "module" is a single Python file.

The ``SIPPackage`` base class automatically detects these module
names for you. If, for whatever reason, the module names detected
are wrong, you can provide the names yourself by overriding
``import_modules`` like so:

.. code-block:: python

   import_modules = ['PyQt5']


These tests often catch missing dependencies and non-RPATHed
libraries. Make sure not to add modules/packages containing the word
"test", as these likely won't end up in the installation directory,
or may require test dependencies like pytest to be installed.

These tests can be triggered by running ``spack install --test=root``
or by running ``spack test run`` after the installation has finished.

^^^^^^^^^^^^^^^^^^^^^^
External documentation
^^^^^^^^^^^^^^^^^^^^^^

For more information on the SIP build system, see:

* https://www.riverbankcomputing.com/software/sip/intro
* https://www.riverbankcomputing.com/static/Docs/sip/
* https://wiki.python.org/moin/SIP
.. Copyright 2013-2022 Lawrence Livermore National Security, LLC and other
   Spack Project Developers. See the top-level COPYRIGHT file for details.

   SPDX-License-Identifier: (Apache-2.0 OR MIT)

.. _mesonpackage:

------------
MesonPackage
------------

Much like Autotools and CMake, Meson is a build system.  But it is
meant to be both fast and as user friendly as possible.  GNOME's goal
is to port modules to use the Meson build system.

^^^^^^
Phases
^^^^^^

The ``MesonPackage`` base class comes with the following phases:

#. ``meson`` - generate ninja files
#. ``build`` - build the project
#. ``install`` - install the project

By default, these phases run:

.. code-block:: console

   $ mkdir spack-build
   $ cd spack-build
   $ meson .. --prefix=/path/to/installation/prefix
   $ ninja
   $ ninja test  # optional
   $ ninja install


Any of these phases can be overridden in your package as necessary.
There is also a ``check`` method that looks for a ``test`` target
in the build file. If a ``test`` target exists and the user runs:

.. code-block:: console

   $ spack install --test=root <meson-package>


Spack will run ``ninja test`` after the build phase.

^^^^^^^^^^^^^^^
Important files
^^^^^^^^^^^^^^^

Packages that use the Meson build system can be identified by the
presence of a ``meson.build`` file. This file declares things
like build instructions and dependencies.

One thing to look for is the ``meson_version`` key that gets passed
to the ``project`` function:

.. code-block:: none
   :emphasize-lines: 10

   project('gtk+', 'c',
        version: '3.94.0',
        default_options: [
          'buildtype=debugoptimized',
          'warning_level=1',
          # We only need c99, but glib needs GNU-specific features
          # https://github.com/mesonbuild/meson/issues/2289
          'c_std=gnu99',
        ],
        meson_version: '>= 0.43.0',
        license: 'LGPLv2.1+')


This means that Meson 0.43.0 is the earliest release that will work.
You should specify this in a ``depends_on`` statement.

^^^^^^^^^^^^^^^^^^^^^^^^^
Build system dependencies
^^^^^^^^^^^^^^^^^^^^^^^^^

At the bare minimum, packages that use the Meson build system need
``meson`` and ```ninja``` dependencies. Since this is always the case,
the ``MesonPackage`` base class already contains:

.. code-block:: python

   depends_on('meson', type='build')
   depends_on('ninja', type='build')


If you need to specify a particular version requirement, you can
override this in your package:

.. code-block:: python

   depends_on('meson@0.43.0:', type='build')
   depends_on('ninja', type='build')


^^^^^^^^^^^^^^^^^^^
Finding meson flags
^^^^^^^^^^^^^^^^^^^

To get a list of valid flags that can be passed to ``meson``, run the
following command in the directory that contains ``meson.build``:

.. code-block:: console

   $ meson setup --help


^^^^^^^^^^^^^^^^^^^^^^^^^^
Passing arguments to meson
^^^^^^^^^^^^^^^^^^^^^^^^^^

If you need to pass any arguments to the ``meson`` call, you can
override the ``meson_args`` method like so:

.. code-block:: python

   def meson_args(self):
       return ['--warnlevel=3']


This method can be used to pass flags as well as variables.

Note that the ``MesonPackage`` base class already defines variants for
``buildtype``, ``default_library`` and ``strip``, which are mapped to default
Meson arguments, meaning that you don't have to specify these.

^^^^^^^^^^^^^^^^^^^^^^
External documentation
^^^^^^^^^^^^^^^^^^^^^^

For more information on the Meson build system, see:
https://mesonbuild.com/index.html
.. Copyright 2013-2022 Lawrence Livermore National Security, LLC and other
   Spack Project Developers. See the top-level COPYRIGHT file for details.

   SPDX-License-Identifier: (Apache-2.0 OR MIT)

.. _multiplepackage:

----------------------
Multiple Build Systems
----------------------

Quite frequently, a package will change build systems from one version to the
next. For example, a small project that once used a single Makefile to build
may now require Autotools to handle the increased number of files that need to
be compiled. Or, a package that once used Autotools may switch to CMake for
Windows support. In this case, it becomes a bit more challenging to write a
single build recipe for this package in Spack.

There are several ways that this can be handled in Spack:

#. Subclass the new build system, and override phases as needed (preferred)
#. Subclass ``Package`` and implement ``install`` as needed
#. Create separate ``*-cmake``, ``*-autotools``, etc. packages for each build system
#. Rename the old package to ``*-legacy`` and create a new package
#. Move the old package to a ``legacy`` repository and create a new package
#. Drop older versions that only support the older build system

Of these options, 1 is preferred, and will be demonstrated in this
documentation. Options 3-5 have issues with concretization, so shouldn't be
used. Options 4-5 also don't support more than two build systems. Option 6 only
works if the old versions are no longer needed. Option 1 is preferred over 2
because it makes it easier to drop the old build system entirely.

The exact syntax of the package depends on which build systems you need to
support. Below are a couple of common examples.

^^^^^^^^^^^^^^^^^^^^^
Makefile -> Autotools
^^^^^^^^^^^^^^^^^^^^^

Let's say we have the following package:

.. code-block:: python

   class Foo(MakefilePackage):
       version("1.2.0", sha256="...")

       def edit(self, spec, prefix):
           filter_file("CC=", "CC=" + spack_cc, "Makefile")

       def install(self, spec, prefix):
           install_tree(".", prefix)


The package subclasses from :ref:`makefilepackage`, which has three phases:

#. ``edit`` (does nothing by default)
#. ``build`` (runs ``make`` by default)
#. ``install`` (runs ``make install`` by default)

In this case, the ``install`` phase needed to be overridden because the
Makefile did not have an install target. We also modify the Makefile to use
Spack's compiler wrappers. The default ``build`` phase is not changed.

Starting with version 1.3.0, we want to use Autotools to build instead.
:ref:`autotoolspackage` has four phases:

#. ``autoreconf`` (does not if a configure script already exists)
#. ``configure`` (runs ``./configure --prefix=...`` by default)
#. ``build`` (runs ``make`` by default)
#. ``install`` (runs ``make install`` by default)

If the only version we need to support is 1.3.0, the package would look as
simple as:

.. code-block:: python

   class Foo(AutotoolsPackage):
       version("1.3.0", sha256="...")

       def configure_args(self):
           return ["--enable-shared"]


In this case, we use the default methods for each phase and only override
``configure_args`` to specify additional flags to pass to ``./configure``.

If we wanted to write a single package that supports both versions 1.2.0 and
1.3.0, it would look something like:

.. code-block:: python

   class Foo(AutotoolsPackage):
       version("1.3.0", sha256="...")
       version("1.2.0", sha256="...", deprecated=True)

       def configure_args(self):
           return ["--enable-shared"]

       # Remove the following once version 1.2.0 is dropped
       @when("@:1.2")
       def patch(self):
           filter_file("CC=", "CC=" + spack_cc, "Makefile")

       @when("@:1.2")
       def autoreconf(self, spec, prefix):
           pass

       @when("@:1.2")
       def configure(self, spec, prefix):
           pass

       @when("@:1.2")
       def install(self, spec, prefix):
           install_tree(".", prefix)


There are a few interesting things to note here:

* We added ``deprecated=True`` to version 1.2.0. This signifies that version
  1.2.0 is deprecated and shouldn't be used. However, if a user still relies
  on version 1.2.0, it's still there and builds just fine.
* We moved the contents of the ``edit`` phase to the ``patch`` function. Since
  ``AutotoolsPackage`` doesn't have an ``edit`` phase, the only way for this
  step to be executed is to move it to the ``patch`` function, which always
  gets run.
* The ``autoreconf`` and ``configure`` phases become no-ops. Since the old
  Makefile-based build system doesn't use these, we ignore these phases when
  building ``foo@1.2.0``.
* The ``@when`` decorator is used to override these phases only for older
  versions. The default methods are used for ``foo@1.3:``.

Once a new Spack release comes out, version 1.2.0 and everything below the
comment can be safely deleted. The result is the same as if we had written a
package for version 1.3.0 from scratch.

^^^^^^^^^^^^^^^^^^
Autotools -> CMake
^^^^^^^^^^^^^^^^^^

Let's say we have the following package:

.. code-block:: python

   class Bar(AutotoolsPackage):
       version("1.2.0", sha256="...")

       def configure_args(self):
           return ["--enable-shared"]


The package subclasses from :ref:`autotoolspackage`, which has four phases:

#. ``autoreconf`` (does not if a configure script already exists)
#. ``configure`` (runs ``./configure --prefix=...`` by default)
#. ``build`` (runs ``make`` by default)
#. ``install`` (runs ``make install`` by default)

In this case, we use the default methods for each phase and only override
``configure_args`` to specify additional flags to pass to ``./configure``.

Starting with version 1.3.0, we want to use CMake to build instead.
:ref:`cmakepackage` has three phases:

#. ``cmake`` (runs ``cmake ...`` by default)
#. ``build`` (runs ``make`` by default)
#. ``install`` (runs ``make install`` by default)

If the only version we need to support is 1.3.0, the package would look as
simple as:

.. code-block:: python

   class Bar(CMakePackage):
       version("1.3.0", sha256="...")

       def cmake_args(self):
           return [self.define("BUILD_SHARED_LIBS", True)]


In this case, we use the default methods for each phase and only override
``cmake_args`` to specify additional flags to pass to ``cmake``.

If we wanted to write a single package that supports both versions 1.2.0 and
1.3.0, it would look something like:

.. code-block:: python

   class Bar(CMakePackage):
       version("1.3.0", sha256="...")
       version("1.2.0", sha256="...", deprecated=True)

       def cmake_args(self):
           return [self.define("BUILD_SHARED_LIBS", True)]

       # Remove the following once version 1.2.0 is dropped
       def configure_args(self):
           return ["--enable-shared"]

       @when("@:1.2")
       def cmake(self, spec, prefix):
           configure("--prefix=" + prefix, *self.configure_args())


There are a few interesting things to note here:

* We added ``deprecated=True`` to version 1.2.0. This signifies that version
  1.2.0 is deprecated and shouldn't be used. However, if a user still relies
  on version 1.2.0, it's still there and builds just fine.
* Since CMake and Autotools are so similar, we only need to override the
  ``cmake`` phase, we can use the default ``build`` and ``install`` phases.
* We override ``cmake`` to run ``./configure`` for older versions.
  ``configure_args`` remains the same.
* The ``@when`` decorator is used to override these phases only for older
  versions. The default methods are used for ``bar@1.3:``.

Once a new Spack release comes out, version 1.2.0 and everything below the
comment can be safely deleted. The result is the same as if we had written a
package for version 1.3.0 from scratch.

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Multiple build systems for the same version
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

During the transition from one build system to another, developers often
support multiple build systems at the same time. Spack can only use a single
build system for a single version. To decide which build system to use for a
particular version, take the following things into account:

1. If the developers explicitly state that one build system is preferred over
   another, use that one.
2. If one build system is considered "experimental" while another is considered
   "stable", use the stable build system.
3. Otherwise, use the newer build system.

The developer preference for which build system to use can change over time as
a newer build system becomes stable/recommended.

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Dropping support for old build systems
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

When older versions of a package don't support a newer build system, it can be
tempting to simply delete them from a package. This significantly reduces
package complexity and makes the build recipe much easier to maintain. However,
other packages or Spack users may rely on these older versions. The recommended
approach is to first support both build systems (as demonstrated above),
:ref:`deprecate <deprecate>` versions that rely on the old build system, and
remove those versions and any phases that needed to be overridden in the next
Spack release.

^^^^^^^^^^^^^^^^^^^^^^^^^^^
Three or more build systems
^^^^^^^^^^^^^^^^^^^^^^^^^^^

In rare cases, a package may change build systems multiple times. For example,
a package may start with Makefiles, then switch to Autotools, then switch to
CMake. The same logic used above can be extended to any number of build systems.
For example:

.. code-block:: python

   class Baz(CMakePackage):
       version("1.4.0", sha256="...")  # CMake
       version("1.3.0", sha256="...")  # Autotools
       version("1.2.0", sha256="...")  # Makefile

       def cmake_args(self):
           return [self.define("BUILD_SHARED_LIBS", True)]

       # Remove the following once version 1.3.0 is dropped
       def configure_args(self):
           return ["--enable-shared"]

       @when("@1.3")
       def cmake(self, spec, prefix):
           configure("--prefix=" + prefix, *self.configure_args())

       # Remove the following once version 1.2.0 is dropped
       @when("@:1.2")
       def patch(self):
           filter_file("CC=", "CC=" + spack_cc, "Makefile")

       @when("@:1.2")
       def cmake(self, spec, prefix):
           pass

       @when("@:1.2")
       def install(self, spec, prefix):
           install_tree(".", prefix)


^^^^^^^^^^^^^^^^^^^
Additional examples
^^^^^^^^^^^^^^^^^^^

When writing new packages, it often helps to see examples of existing packages.
Here is an incomplete list of existing Spack packages that have changed build
systems before:

================  =====================  ================
Package           Previous Build System  New Build System
================  =====================  ================
amber             custom                 CMake
arpack-ng         Autotools              CMake
atk               Autotools              Meson
blast             None                   Autotools
dyninst           Autotools              CMake
evtgen            Autotools              CMake
fish              Autotools              CMake
gdk-pixbuf        Autotools              Meson
glib              Autotools              Meson
glog              Autotools              CMake
gmt               Autotools              CMake
gtkplus           Autotools              Meson
hpl               Makefile               Autotools
interproscan      Perl                   Maven
jasper            Autotools              CMake
kahip             SCons                  CMake
kokkos            Makefile               CMake
kokkos-kernels    Makefile               CMake
leveldb           Makefile               CMake
libdrm            Autotools              Meson
libjpeg-turbo     Autotools              CMake
mesa              Autotools              Meson
metis             None                   CMake
mpifileutils      Autotools              CMake
muparser          Autotools              CMake
mxnet             Makefile               CMake
nest              Autotools              CMake
neuron            Autotools              CMake
nsimd             CMake                  nsconfig
opennurbs         Makefile               CMake
optional-lite     None                   CMake
plasma            Makefile               CMake
preseq            Makefile               Autotools
protobuf          Autotools              CMake
py-pygobject      Autotools              Python
singularity       Autotools              Makefile
span-lite         None                   CMake
ssht              Makefile               CMake
string-view-lite  None                   CMake
superlu           Makefile               CMake
superlu-dist      Makefile               CMake
uncrustify        Autotools              CMake
================  =====================  ================

Packages that support multiple build systems can be a bit confusing to write.
Don't hesitate to open an issue or draft pull request and ask for advice from
other Spack developers!
.. Copyright 2013-2022 Lawrence Livermore National Security, LLC and other
   Spack Project Developers. See the top-level COPYRIGHT file for details.

   SPDX-License-Identifier: (Apache-2.0 OR MIT)

.. _cudapackage:

-----------
CudaPackage
-----------

Different from other packages, ``CudaPackage`` does not represent a build system.
Instead its goal is to simplify and unify usage of ``CUDA`` in other packages by providing a `mixin-class <https://en.wikipedia.org/wiki/Mixin>`_.

You can find source for the package at
`<https://github.com/spack/spack/blob/develop/lib/spack/spack/build_systems/cuda.py>`__.

^^^^^^^^
Variants
^^^^^^^^

This package provides the following variants:

* **cuda**

  This variant is used to enable/disable building with ``CUDA``. The default
  is disabled (or ``False``).

* **cuda_arch**

  This variant supports the optional specification of the architecture.
  Valid values are maintained in the ``cuda_arch_values`` property and
  are the numeric character equivalent of the compute capability version
  (e.g., '10' for version 1.0). Each provided value affects associated
  ``CUDA`` dependencies and compiler conflicts.

  GPUs and their compute capability versions are listed at
  https://developer.nvidia.com/cuda-gpus .

^^^^^^^^^
Conflicts
^^^^^^^^^

Conflicts are used to prevent builds with known bugs or issues. While
base ``CUDA`` conflicts have been included with this package, you may
want to add more for your software.

For example, if your package requires ``cuda_arch`` to be specified when
``cuda`` is enabled, you can add the following conflict to your package
to terminate such build attempts with a suitable message:

.. code-block:: python

    conflicts('cuda_arch=none', when='+cuda',
              msg='CUDA architecture is required')

Similarly, if your software does not support all versions of the property,
you could add ``conflicts`` to your package for those versions.  For example,
suppose your software does not work with CUDA compute capability versions
prior to SM 5.0 (``50``).  You can add the following code to display a
custom message should a user attempt such a build:

.. code-block:: python

    unsupported_cuda_archs = [
        '10', '11', '12', '13',
        '20', '21',
        '30', '32', '35', '37'
    ]
    for value in unsupported_cuda_archs:
        conflicts('cuda_arch={0}'.format(value), when='+cuda',
                  msg='CUDA architecture {0} is not supported'.format(value))

^^^^^^^
Methods
^^^^^^^

This package provides one custom helper method, which is used to build
standard CUDA compiler flags.

**cuda_flags**

    This built-in static method returns a list of command line flags 
    for the chosen ``cuda_arch`` value(s).  The flags are intended to
    be passed to the CUDA compiler driver (i.e., ``nvcc``).

    This method must be explicitly called when you are creating the
    arguments for your build in order to use the values.

^^^^^
Usage
^^^^^

This helper package can be added to your package by adding it as a base
class of your package.  For example, you can add it to your
:ref:`CMakePackage <cmakepackage>`-based package as follows:

.. code-block:: python
   :emphasize-lines: 1,7-16

    class MyCudaPackage(CMakePackage, CudaPackage):
        ...
        def cmake_args(self):
            spec = self.spec
            args = []
            ...
            if '+cuda' in spec:
                # Set up the cuda macros needed by the build
                args.append('-DWITH_CUDA=ON')
                cuda_arch_list = spec.variants['cuda_arch'].value
                cuda_arch = cuda_arch_list[0]
                if cuda_arch != 'none':
                    args.append('-DCUDA_FLAGS=-arch=sm_{0}'.format(cuda_arch))
            else:
                # Ensure build with cuda is disabled
                args.append('-DWITH_CUDA=OFF')
            ...
            return args

assuming only the ``WITH_CUDA`` and ``CUDA_FLAGS`` flags are required.
You will need to customize options as needed for your build.

This example also illustrates how to check for the ``cuda`` variant using
``self.spec`` and how to retrieve the ``cuda_arch`` variant's value, which
is a list, using ``self.spec.variants['cuda_arch'].value``.

With over 70 packages using ``CudaPackage`` as of January 2021 there are
lots of examples to choose from to get more ideas for using this package.
.. Copyright 2013-2022 Lawrence Livermore National Security, LLC and other
   Spack Project Developers. See the top-level COPYRIGHT file for details.

   SPDX-License-Identifier: (Apache-2.0 OR MIT)

.. _rubypackage:

-----------
RubyPackage
-----------

Like Perl, Python, and R, Ruby has its own build system for
installing Ruby gems.

^^^^^^
Phases
^^^^^^

The ``RubyPackage`` base class provides the following phases that
can be overridden:

#. ``build`` - build everything needed to install
#. ``install`` - install everything from build directory

For packages that come with a ``*.gemspec`` file, these phases run:

.. code-block:: console

   $ gem build *.gemspec
   $ gem install *.gem


For packages that come with a ``Rakefile`` file, these phases run:

.. code-block:: console

   $ rake package
   $ gem install *.gem


For packages that come pre-packaged as a ``*.gem`` file, the build
phase is skipped and the install phase runs:

.. code-block:: console

   $ gem install *.gem


These are all standard ``gem`` commands and can be found by running:

.. code-block:: console

   $ gem help commands


For packages that only distribute ``*.gem`` files, these files can be
downloaded with the ``expand=False`` option in the ``version`` directive.
The build phase will be automatically skipped.

^^^^^^^^^^^^^^^
Important files
^^^^^^^^^^^^^^^

When building from source, Ruby packages can be identified by the
presence of any of the following files:

* ``*.gemspec``
* ``Rakefile``
* ``setup.rb`` (not yet supported)

However, not all Ruby packages are released as source code. Some are only
released as ``*.gem`` files. These files can be extracted using:

.. code-block:: console

   $ gem unpack *.gem


^^^^^^^^^^^
Description
^^^^^^^^^^^

The ``*.gemspec`` file may contain something like:

.. code-block:: ruby

   summary = 'An implementation of the AsciiDoc text processor and publishing toolchain'
   description = 'A fast, open source text processor and publishing toolchain for converting AsciiDoc content to HTML 5, DocBook 5, and other formats.'


Either of these can be used for the description of the Spack package.

^^^^^^^^
Homepage
^^^^^^^^

The ``*.gemspec`` file may contain something like:

.. code-block:: ruby

   homepage = 'https://asciidoctor.org'


This should be used as the official homepage of the Spack package.

^^^^^^^^^^^^^^^^^^^^^^^^^
Build system dependencies
^^^^^^^^^^^^^^^^^^^^^^^^^

All Ruby packages require Ruby at build and run-time. For this reason,
the base class contains:

.. code-block:: python

   extends('ruby')


The ``*.gemspec`` file may contain something like:

.. code-block:: ruby

   required_ruby_version = '>= 2.3.0'


This can be added to the Spack package using:

.. code-block:: python

   depends_on('ruby@2.3.0:', type=('build', 'run'))


^^^^^^^^^^^^^^^^^
Ruby dependencies
^^^^^^^^^^^^^^^^^

When you install a package with ``gem``, it reads the ``*.gemspec``
file in order to determine the dependencies of the package.
If the dependencies are not yet installed, ``gem`` downloads them
and installs them for you. This may sound convenient, but Spack
cannot rely on this behavior for two reasons:

#. Spack needs to be able to install packages on air-gapped networks.

   If there is no internet connection, ``gem`` can't download the
   package dependencies. By explicitly listing every dependency in
   the ``package.py``, Spack knows what to download ahead of time.

#. Duplicate installations of the same dependency may occur.

   Spack supports *activation* of Ruby extensions, which involves
   symlinking the package installation prefix to the Ruby installation
   prefix. If your package is missing a dependency, that dependency
   will be installed to the installation directory of the same package.
   If you try to activate the package + dependency, it may cause a
   problem if that package has already been activated.

For these reasons, you must always explicitly list all dependencies.
Although the documentation may list the package's dependencies,
often the developers assume people will use ``gem`` and won't have to
worry about it. Always check the ``*.gemspec`` file to find the true
dependencies.

Check for the following clues in the ``*.gemspec`` file:

* ``add_runtime_dependency``

  These packages are required for installation.

* ``add_dependency``

  This is an alias for ``add_runtime_dependency``

* ``add_development_dependency``

  These packages are optional dependencies used for development.
  They should not be added as dependencies of the package.

^^^^^^^^^^^^^^^^^^^^^^
External documentation
^^^^^^^^^^^^^^^^^^^^^^

For more information on Ruby packaging, see:
https://guides.rubygems.org/
============================================
The packages.spack.io Package Index REST API
============================================

This directory provides the docker recipe for the Spack package index on https://packages.spack.io

On each merge to ``develop``, DockerHub builds a new image ``spack/packages.spack.io`` which is configured in:
  https://cloud.docker.com/u/spack/repository/docker/spack/packages.spack.io/builds/edit

------------
The REST API
------------

The API is a simple, file-based JSON index.
A specific package can be queried via the URI syntax:
``https://packages.spack.io/api/:firstLetter/:packageName.json``
which will return a HTTP status code ``200`` with a JSON file for all valid packages (content from ``spack list --format version_json``) and HTTP status code ``404`` for all other package names.

Examples:

- https://packages.spack.io/api/a/adios2.json
- https://packages.spack.io/api/p/py-pandas.json

There is also the full index available at once under https://packages.spack.io/api/packages.json

Current down-stream dependencies are, e.g. the https://shields.io service:

- https://shields.io/category/version
- https://github.com/badges/shields/pull/3536

--------------------
Local Build and Test
--------------------

Execute in your local Spack source root directory:

.. code-block:: bash

   docker build -t spack/packages.spack.io:latest -f share/spack/docker/package-index/Dockerfile .

Startup a local HTTP server on http://localhost:8080 via:

.. code-block:: bash

   docker run -p 8080:80 spack/packages.spack.io:latest
